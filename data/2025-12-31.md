<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 50]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.CL](#cs.CL) [Total: 66]
- [cs.CV](#cs.CV) [Total: 179]
- [cs.DC](#cs.DC) [Total: 29]
- [cs.LG](#cs.LG) [Total: 155]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.PL](#cs.PL) [Total: 8]
- [cs.RO](#cs.RO) [Total: 43]
- [cs.SE](#cs.SE) [Total: 24]
- [q-bio.NC](#q-bio.NC) [Total: 6]
- [stat.ML](#stat.ML) [Total: 11]
- [math.NA](#math.NA) [Total: 2]
- [cs.DL](#cs.DL) [Total: 3]
- [math.DS](#math.DS) [Total: 1]
- [cs.MA](#cs.MA) [Total: 6]
- [math.AT](#math.AT) [Total: 1]
- [stat.CO](#stat.CO) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [stat.ME](#stat.ME) [Total: 1]
- [math.OC](#math.OC) [Total: 5]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [econ.EM](#econ.EM) [Total: 2]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [eess.SP](#eess.SP) [Total: 5]
- [eess.IV](#eess.IV) [Total: 8]
- [cs.CR](#cs.CR) [Total: 11]
- [cs.CE](#cs.CE) [Total: 1]
- [q-fin.GN](#q-fin.GN) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 2]
- [eess.SY](#eess.SY) [Total: 5]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.DB](#cs.DB) [Total: 3]
- [math.ST](#math.ST) [Total: 1]
- [cs.HC](#cs.HC) [Total: 6]
- [econ.GN](#econ.GN) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 2]
- [cs.ET](#cs.ET) [Total: 1]
- [q-fin.PR](#q-fin.PR) [Total: 1]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.GT](#cs.GT) [Total: 2]
- [cs.SD](#cs.SD) [Total: 3]
- [eess.AS](#eess.AS) [Total: 1]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [physics.app-ph](#physics.app-ph) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Bidirectional RAG: Safe Self-Improving Retrieval-Augmented Generation Through Multi-Stage Validation](https://arxiv.org/abs/2512.22199)
*Teja Chinthala*

Main category: cs.AI

TL;DR: Bidirectional RAG introduces a system for RAG models that evolves via validated write-back of high-quality AI-generated responses while maintaining safe corpus expansion.


<details>
  <summary>Details</summary>
Motivation: Conventional RAG systems rely on static corpora, limiting their capacity for evolving and improving based on real-world user interactions.

Method: A multi-stage acceptance layer is designed using grounding verification, NLI-based entailment, attribution checking, and novelty detection to ensure safe knowledge accumulation.

Result: Bidirectional RAG nearly doubles the coverage of standard RAG systems (40.58% vs 20.33%) while introducing fewer documents (72% less write-back).

Conclusion: Self-improving RAG systems are feasible and safe with rigorous validation measures, paving a practical path for dynamic RAG architectures that learn over time.

Abstract: Retrieval-Augmented Generation RAG systems enhance large language models by grounding responses in external knowledge bases, but conventional RAG architectures operate with static corpora that cannot evolve from user interactions. We introduce Bidirectional RAG, a novel RAG architecture that enables safe corpus expansion through validated write back of high quality generated responses. Our system employs a multi stage acceptance layer combining grounding verification (NLI based entailment, attribution checking, and novelty detection to prevent hallucination pollution while enabling knowledge accumulation. Across four datasets Natural Questions, TriviaQA, HotpotQA, Stack Overflow with three random seeds 12 experiments per system, Bidirectional RAG achieves 40.58% average coverage nearly doubling Standard RAG 20.33% while adding 72% fewer documents than naive write back 140 vs 500. Our work demonstrates that self improving RAG is feasible and safe when governed by rigorous validation, offering a practical path toward RAG systems that learn from deployment.

</details>


### [2] [Emergent Persuasion: Will LLMs Persuade Without Being Prompted?](https://arxiv.org/abs/2512.22201)
*Vincent Chang,Thee Ho,Sunishchal Dev,Kevin Zhu,Shi Feng,Kellin Pelrine,Matthew Kowal*

Main category: cs.AI

TL;DR: The paper investigates the emergence of unprompted persuasion in AI language models, finding that supervised fine-tuning increases persuasive tendencies even on harmful topics.


<details>
  <summary>Details</summary>
Motivation: Despite the potential benefits of conversational AI, concerns have risen about their ability to unintentionally persuade users into harmful beliefs or actions, particularly as models grow in scale.

Method: The authors examined unprompted persuasion by testing two scenarios: 1) steering models with internal activation techniques towards persona traits, and 2) supervised fine-tuning (SFT) models to exhibit specific traits using benign persuasion datasets.

Result: The study found that while persona steering does not reliably increase unprompted persuasion, supervised fine-tuning not only increases persuasive tendencies but also introduces risks of promoting harmful and controversial topics.

Conclusion: Supervised fine-tuning makes language models more prone to unprompted persuasion, including on harmful topics, highlighting the need for further study and mitigation efforts regarding emergent persuasion risks.

Abstract: With the wide-scale adoption of conversational AI systems, AI are now able to exert unprecedented influence on human opinion and beliefs. Recent work has shown that many Large Language Models (LLMs) comply with requests to persuade users into harmful beliefs or actions when prompted and that model persuasiveness increases with model scale. However, this prior work looked at persuasion from the threat model of $\textit{misuse}$ (i.e., a bad actor asking an LLM to persuade). In this paper, we instead aim to answer the following question: Under what circumstances would models persuade $\textit{without being explicitly prompted}$, which would shape how concerned we should be about such emergent persuasion risks. To achieve this, we study unprompted persuasion under two scenarios: (i) when the model is steered (through internal activation steering) along persona traits, and (ii) when the model is supervised-finetuned (SFT) to exhibit the same traits. We showed that steering towards traits, both related to persuasion and unrelated, does not reliably increase models' tendency to persuade unprompted, however, SFT does. Moreover, SFT on general persuasion datasets containing solely benign topics admits a model that has a higher propensity to persuade on controversial and harmful topics--showing that emergent harmful persuasion can arise and should be studied further.

</details>


### [3] [GamiBench: Evaluating Spatial Reasoning and 2D-to-3D Planning Capabilities of MLLMs with Origami Folding Tasks](https://arxiv.org/abs/2512.22207)
*Ryan Spencer,Roey Yaari,Ritvik Vemavarapu,Joyce Yang,Steven Ngo,Utkarsh Sharma*

Main category: cs.AI

TL;DR: GamiBench is a new benchmark testing spatial reasoning in multimodal large language models (MLLMs) through origami-inspired folding tasks.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the gap in evaluating spatial reasoning capabilities in MLLMs, which is crucial for tasks involving sequential and viewpoint-dependent skills.

Method: The benchmark, GamiBench, incorporates origami folding tasks with 2D crease patterns and 3D shapes across six viewpoints to test various spatial reasoning abilities.

Result: Leading MLLMs such as GPT-5 and Gemini-2.5-Pro demonstrated considerable challenges in handling even single-step spatial reasoning tasks.

Conclusion: GamiBench provides a comprehensive framework to evaluate spatial reasoning and geometric understanding in MLLMs, helping to highlight current model limitations and guide future developments.

Abstract: Multimodal large language models (MLLMs) are proficient in perception and instruction-following, but they still struggle with spatial reasoning: the ability to mentally track and manipulate objects across multiple views and over time. Spatial reasoning is a key component of human intelligence, but most existing benchmarks focus on static images or final outputs, failing to account for the sequential and viewpoint-dependent nature of this skill. To close this gap, we introduce GamiBench, a benchmark designed to evaluate spatial reasoning and 2D-to-3D planning in MLLMs through origami-inspired folding tasks. GamiBench includes 186 regular and 186 impossible 2D crease patterns paired with their corresponding 3D folded shapes, produced from six distinct viewpoints across three visual question-answering (VQA) tasks: predicting 3D fold configurations, distinguishing valid viewpoints, and detecting impossible patterns. Unlike previous benchmarks that assess only final predictions, GamiBench holistically evaluates the entire reasoning process--measuring cross-view consistency, physical feasibility through impossible-fold detection, and interpretation of intermediate folding steps. It further introduces new diagnostic metrics--viewpoint consistency (VC) and impossible fold selection rate (IFSR)--to measure how well models handle folds of varying complexity. Our experiments show that even leading models such as GPT-5 and Gemini-2.5-Pro struggle on single-step spatial understanding. These contributions establish a standardized framework for evaluating geometric understanding and spatial reasoning in MLLMs. Dataset and code: https://github.com/stvngo/GamiBench.

</details>


### [4] [Toward Equitable Recovery: A Fairness-Aware AI Framework for Prioritizing Post-Flood Aid in Bangladesh](https://arxiv.org/abs/2512.22210)
*Farjana Yesmin,Romana Akter*

Main category: cs.AI

TL;DR: The paper introduces a fairness-aware AI framework to prioritize post-flood aid distribution in Bangladesh fairly, reducing biases and improving equitable aid allocation.


<details>
  <summary>Details</summary>
Motivation: To address systematic biases in post-disaster aid allocation, which often disadvantages vulnerable regions and perpetuates inequities in developing nations.

Method: The authors develop an adversarial debiasing model using fairness-aware representation learning with a gradient reversal layer to predict flood vulnerability while minimizing biases against marginalized districts and rural areas.

Result: The model reduced statistical parity difference by 41.6%, decreased regional fairness gaps by 43.2%, and achieved strong predictive accuracy with an R-squared of 0.784 while generating actionable aid allocation priorities.

Conclusion: Algorithmic fairness techniques can be effectively applied to humanitarian efforts to support more equitable disaster recovery strategies for vulnerable populations.

Abstract: Post-disaster aid allocation in developing nations often suffers from systematic biases that disadvantage vulnerable regions, perpetuating historical inequities. This paper presents a fairness-aware artificial intelligence framework for prioritizing post-flood aid distribution in Bangladesh, a country highly susceptible to recurring flood disasters. Using real data from the 2022 Bangladesh floods that affected 7.2 million people and caused 405.5 million US dollars in damages, we develop an adversarial debiasing model that predicts flood vulnerability while actively removing biases against marginalized districts and rural areas. Our approach adapts fairness-aware representation learning techniques from healthcare AI to disaster management, employing a gradient reversal layer that forces the model to learn bias-invariant representations. Experimental results on 87 upazilas across 11 districts demonstrate that our framework reduces statistical parity difference by 41.6 percent, decreases regional fairness gaps by 43.2 percent, and maintains strong predictive accuracy (R-squared=0.784 vs baseline 0.811). The model generates actionable priority rankings ensuring aid reaches the most vulnerable populations based on genuine need rather than historical allocation patterns. This work demonstrates how algorithmic fairness techniques can be effectively applied to humanitarian contexts, providing decision-makers with tools to implement more equitable disaster recovery strategies.

</details>


### [5] [With Great Capabilities Come Great Responsibilities: Introducing the Agentic Risk & Capability Framework for Governing Agentic AI Systems](https://arxiv.org/abs/2512.22211)
*Shaun Khoo,Jessica Foo,Roy Ka-Wei Lee*

Main category: cs.AI

TL;DR: The paper introduces the ARC Framework, a technical governance tool for managing risks associated with agentic AI systems.


<details>
  <summary>Details</summary>
Motivation: The rising risks and complexities linked to autonomous, agentic AI systems due to their independent actions like code execution and internet interaction require robust organizational governance tools.

Method: The ARC Framework adopts a capability-centric analysis, identifies three primary risk sources (components, design, capabilities), links these to materialized risks and technical controls, and outlines practical implementation.

Result: A robust and adaptable framework that distills risks and provides mechanisms for responsible governance of agentic AI systems was developed.

Conclusion: The ARC Framework is a pioneering tool aiding organizations in responsibly leveraging agentic AI while managing associated risks, ensuring secure innovation.

Abstract: Agentic AI systems present both significant opportunities and novel risks due to their capacity for autonomous action, encompassing tasks such as code execution, internet interaction, and file modification. This poses considerable challenges for effective organizational governance, particularly in comprehensively identifying, assessing, and mitigating diverse and evolving risks. To tackle this, we introduce the Agentic Risk \& Capability (ARC) Framework, a technical governance framework designed to help organizations identify, assess, and mitigate risks arising from agentic AI systems. The framework's core contributions are: (1) it develops a novel capability-centric perspective to analyze a wide range of agentic AI systems; (2) it distills three primary sources of risk intrinsic to agentic AI systems - components, design, and capabilities; (3) it establishes a clear nexus between each risk source, specific materialized risks, and corresponding technical controls; and (4) it provides a structured and practical approach to help organizations implement the framework. This framework provides a robust and adaptable methodology for organizations to navigate the complexities of agentic AI, enabling rapid and effective innovation while ensuring the safe, secure, and responsible deployment of agentic AI systems. Our framework is open-sourced \href{https://govtech-responsibleai.github.io/agentic-risk-capability-framework/}{here}.

</details>


### [6] [We are not able to identify AI-generated images](https://arxiv.org/abs/2512.22236)
*Adrien Pavão*

Main category: cs.AI

TL;DR: The paper studies humans' ability to differentiate between real and AI-generated images, finding their accuracy only slightly better than random guesses.


<details>
  <summary>Details</summary>
Motivation: To assess whether humans can reliably identify between AI-generated and real images and understand the need for guidelines as AI media becomes increasingly convincing.

Method: Participants classified 20 images (real or AI-generated) from a curated dataset through an interactive web experiment.

Result: Participants' average accuracy was 54% — minimally above random guessing, with minimal improvement after repeated attempts. Some AI images were consistently deceptive.

Conclusion: Humans struggle to distinguish real from AI-generated images, highlighting the need for ethical guidelines and awareness as AI media improves.

Abstract: AI-generated images are now pervasive online, yet many people believe they can easily tell them apart from real photographs. We test this assumption through an interactive web experiment where participants classify 20 images as real or AI-generated. Our dataset contains 120 difficult cases: real images sampled from CC12M, and carefully curated AI-generated counterparts produced with MidJourney. In total, 165 users completed 233 sessions. Their average accuracy was 54%, only slightly above random guessing, with limited improvement across repeated attempts. Response times averaged 7.3 seconds, and some images were consistently more deceptive than others. These results indicate that, even on relatively simple portrait images, humans struggle to reliably detect AI-generated content. As synthetic media continues to improve, human judgment alone is becoming insufficient for distinguishing real from artificial data. These findings highlight the need for greater awareness and ethical guidelines as AI-generated media becomes increasingly indistinguishable from reality.

</details>


### [7] [Shape of Thought: When Distribution Matters More than Correctness in Reasoning Tasks](https://arxiv.org/abs/2512.22255)
*Abhranil Chandra,Ayush Agrawal,Arian Hosseini,Sebastian Fischmeister,Rishabh Agarwal,Navin Goyal,Aaron Courville*

Main category: cs.AI

TL;DR: The research shows that language model reasoning can improve by training on synthetic datasets with chain-of-thought (CoT) traces from larger models, even when they lead to incorrect answers, surpassing human-annotated datasets in reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: The study aims to explore alternative methods to improve reasoning capabilities in language models, as traditional reliance on human-annotated datasets might not be optimal.

Method: The authors trained language models using synthetic CoT data from stronger models, analyzed the impact of paraphrased human traces, and experimented with flawed traces across various datasets and models.

Result: The findings indicate better reasoning performance using synthetic datasets with CoT traces closer to the model's distribution, even when these traces include flaws. Models were tolerant of flawed reasoning and still learned legitimate steps.

Conclusion: Training data distribution alignment matters significantly for model performance, and a correct answer does not always signal faithful reasoning. These insights open new directions for model training strategies.

Abstract: We present the surprising finding that a language model's reasoning capabilities can be improved by training on synthetic datasets of chain-of-thought (CoT) traces from more capable models, even when all of those traces lead to an incorrect final answer. Our experiments show this approach can yield better performance on reasoning tasks than training on human-annotated datasets. We hypothesize that two key factors explain this phenomenon: first, the distribution of synthetic data is inherently closer to the language model's own distribution, making it more amenable to learning. Second, these `incorrect' traces are often only partially flawed and contain valid reasoning steps from which the model can learn. To further test the first hypothesis, we use a language model to paraphrase human-annotated traces -- shifting their distribution closer to the model's own distribution -- and show that this improves performance. For the second hypothesis, we introduce increasingly flawed CoT traces and study to what extent models are tolerant to these flaws. We demonstrate our findings across various reasoning domains like math, algorithmic reasoning and code generation using MATH, GSM8K, Countdown and MBPP datasets on various language models ranging from 1.5B to 9B across Qwen, Llama, and Gemma models. Our study shows that curating datasets that are closer to the model's distribution is a critical aspect to consider. We also show that a correct final answer is not always a reliable indicator of a faithful reasoning process.

</details>


### [8] [Lessons from Neuroscience for AI: How integrating Actions, Compositional Structure and Episodic Memory could enable Safe, Interpretable and Human-Like AI](https://arxiv.org/abs/2512.22568)
*Rajesh P. N. Rao,Vishwas Sathish,Linxing Preston Jiang,Matthew Bryan,Prashant Rangarajan*

Main category: cs.AI

TL;DR: This paper proposes integrating actions, compositional generative architectures, and episodic memory into large language models (LLMs) to address deficiencies like hallucinations, lack of grounding, and inefficiency while drawing inspiration from brain science.


<details>
  <summary>Details</summary>
Motivation: To address fundamental shortcomings of current foundation models, such as hallucinations, lack of interpretability, and energy inefficiency, and to create AI systems that are safer, more human-like, and more aligned with cognitive neuroscience principles.

Method: The authors propose incorporating concepts from neuroscience—specifically the integration of actions at varying abstractions, hierarchical generative structure, and episodic memory—into foundation models. They analyze these components conceptually and compare them to existing solutions like chain-of-thought reasoning and retrieval-augmented generation.

Result: The paper suggests that integrating these brain-inspired components could resolve existing issues in LLMs, such as hallucinations, lack of agency, trustworthiness concerns, and inefficiencies, by fostering better grounding and enhanced interpretability.

Conclusion: Brain-inspired architecture and interdisciplinary collaboration between neuroscience and AI research can lead to safer, interpretable, and energy-efficient human-centered AI systems.

Abstract: The phenomenal advances in large language models (LLMs) and other foundation models over the past few years have been based on optimizing large-scale transformer models on the surprisingly simple objective of minimizing next-token prediction loss, a form of predictive coding that is also the backbone of an increasingly popular model of brain function in neuroscience and cognitive science. However, current foundation models ignore three other important components of state-of-the-art predictive coding models: tight integration of actions with generative models, hierarchical compositional structure, and episodic memory. We propose that to achieve safe, interpretable, energy-efficient, and human-like AI, foundation models should integrate actions, at multiple scales of abstraction, with a compositional generative architecture and episodic memory. We present recent evidence from neuroscience and cognitive science on the importance of each of these components. We describe how the addition of these missing components to foundation models could help address some of their current deficiencies: hallucinations and superficial understanding of concepts due to lack of grounding, a missing sense of agency/responsibility due to lack of control, threats to safety and trustworthiness due to lack of interpretability, and energy inefficiency. We compare our proposal to current trends, such as adding chain-of-thought (CoT) reasoning and retrieval-augmented generation (RAG) to foundation models, and discuss new ways of augmenting these models with brain-inspired components. We conclude by arguing that a rekindling of the historically fruitful exchange of ideas between brain science and AI will help pave the way towards safe and interpretable human-centered AI.

</details>


### [9] [Logic Sketch Prompting (LSP): A Deterministic and Interpretable Prompting Method](https://arxiv.org/abs/2512.22258)
*Satvik Tripathi*

Main category: cs.AI

TL;DR: The paper introduces Logic Sketch Prompting (LSP), a prompting framework designed to enhance accuracy and reliability in language models for tasks requiring rule compliance and determinism. It shows significantly higher performance compared to other methods.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) struggle with tasks requiring strict rules, determinism, and auditability despite excelling at natural language reasoning. There is a need for a method to make LLMs more reliable and interpretable for critical applications.

Method: Logic Sketch Prompting (LSP) utilizes typed variables, deterministic condition evaluators, and a rule-based validator to generate outputs that are traceable and repeatable. It is tested across pharmacologic compliance tasks using three open weight models.

Result: Across two tasks and three models, LSP achieved significantly higher accuracy (0.83-0.89) and F1 score (0.83-0.89) compared to zero shot prompting, concise prompting, and chain of thought prompting. Statistical tests confirmed the significance of these improvements.

Conclusion: LSP reliably improves the performance, determinism, and interpretability of LLMs, making it suitable for use in regulated and safety-critical systems like clinical decision support.

Abstract: Large language models (LLMs) excel at natural language reasoning but remain unreliable on tasks requiring strict rule adherence, determinism, and auditability. Logic Sketch Prompting (LSP) is a lightweight prompting framework that introduces typed variables, deterministic condition evaluators, and a rule based validator that produces traceable and repeatable outputs. Using two pharmacologic logic compliance tasks, we benchmark LSP against zero shot prompting, chain of thought prompting, and concise prompting across three open weight models: Gemma 2, Mistral, and Llama 3. Across both tasks and all models, LSP consistently achieves the highest accuracy (0.83 to 0.89) and F1 score (0.83 to 0.89), substantially outperforming zero shot prompting (0.24 to 0.60), concise prompts (0.16 to 0.30), and chain of thought prompting (0.56 to 0.75). McNemar tests show statistically significant gains for LSP across nearly all comparisons (p < 0.01). These results demonstrate that LSP improves determinism, interpretability, and consistency without sacrificing performance, supporting its use in clinical, regulated, and safety critical decision support systems.

</details>


### [10] [SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence](https://arxiv.org/abs/2512.22334)
*Yiheng Wang,Yixin Chen,Shuo Li,Yifan Zhou,Bo Liu,Hengjian Gao,Jiakang Yuan,Jia Bu,Wanghan Xu,Yuhao Zhou,Xiangyu Zhao,Zhiwang Zhou,Fengxiang Wang,Haodong Duan,Songyang Zhang,Jun Yao,Han Deng,Yizhou Wang,Jiabei Xiao,Jiaqi Liu,Encheng Su,Yujie Liu,Weida Wang,Junchi Yao,Shenghe Zheng,Haoran Sun,Runmin Ma,Xiangchao Yan,Bo Zhang,Dongzhan Zhou,Shufei Zhang,Peng Ye,Xiaosong Wang,Shixiang Tang,Wenlong Zhang,Lei Bai*

Main category: cs.AI

TL;DR: SciEvalKit is a benchmarking toolkit for AI in science, enabling evaluation across disciplines and scientific tasks, with open-source, expert-curated datasets.


<details>
  <summary>Details</summary>
Motivation: To address the lack of tailored evaluation tools for scientific intelligence in AI by providing a standardized benchmarking framework.

Method: It incorporates expert-grade benchmarks from real-world datasets and supports diverse scientific domains and task types through an extensible evaluation pipeline.

Result: SciEvalKit delivers reproducible, transparent, and customizable results for AI model evaluation in scientific disciplines.

Conclusion: SciEvalKit advances AI4Science by standardizing evaluation practices and enabling community-driven improvements in scientific AI models.

Abstract: We introduce SciEvalKit, a unified benchmarking toolkit designed to evaluate AI models for science across a broad range of scientific disciplines and task capabilities. Unlike general-purpose evaluation platforms, SciEvalKit focuses on the core competencies of scientific intelligence, including Scientific Multimodal Perception, Scientific Multimodal Reasoning, Scientific Multimodal Understanding, Scientific Symbolic Reasoning, Scientific Code Generation, Science Hypothesis Generation and Scientific Knowledge Understanding. It supports six major scientific domains, spanning from physics and chemistry to astronomy and materials science. SciEvalKit builds a foundation of expert-grade scientific benchmarks, curated from real-world, domain-specific datasets, ensuring that tasks reflect authentic scientific challenges. The toolkit features a flexible, extensible evaluation pipeline that enables batch evaluation across models and datasets, supports custom model and dataset integration, and provides transparent, reproducible, and comparable results. By bridging capability-based evaluation and disciplinary diversity, SciEvalKit offers a standardized yet customizable infrastructure to benchmark the next generation of scientific foundation models and intelligent agents. The toolkit is open-sourced and actively maintained to foster community-driven development and progress in AI4Science.

</details>


### [11] [Agent2World: Learning to Generate Symbolic World Models via Adaptive Multi-Agent Feedback](https://arxiv.org/abs/2512.22336)
*Mengkang Hu,Bowei Xia,Yuran Wu,Ailing Yu,Yude Zou,Qiguang Chen,Shijian Wang,Jiarui Jin,Kexin Li,Wenxiang Jiao,Yuan Lu,Ping Luo*

Main category: cs.AI

TL;DR: Agent2World proposes a novel multi-agent framework to enhance world-model generation and validation, achieving state-of-the-art results and significant performance improvements after fine-tuning.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitation in training LLMs to generate symbolic world models due to a lack of large-scale, verifiable supervision and shortcomings of static validation methods.

Method: Agent2World operates through a three-stage pipeline: (i) a Deep Researcher synthesizes knowledge from web searches to address specification gaps, (ii) a Model Developer creates executable world models, and (iii) a Testing Team conducts adaptive validation through simulation and unit testing.

Result: Agent2World achieves superior world-model generation accuracy across multiple benchmarks, with fine-tuned models showing an average improvement of 30.95% in performance.

Conclusion: The framework enhances real-time generation of symbolic world models and enables iterative learning through interactive multi-agent feedback, providing a highly effective solution for model-based planning.

Abstract: Symbolic world models (e.g., PDDL domains or executable simulators) are central to model-based planning, but training LLMs to generate such world models is limited by the lack of large-scale verifiable supervision. Current approaches rely primarily on static validation methods that fail to catch behavior-level errors arising from interactive execution. In this paper, we propose Agent2World, a tool-augmented multi-agent framework that achieves strong inference-time world-model generation and also serves as a data engine for supervised fine-tuning, by grounding generation in multi-agent feedback. Agent2World follows a three-stage pipeline: (i) A Deep Researcher agent performs knowledge synthesis by web searching to address specification gaps; (ii) A Model Developer agent implements executable world models; And (iii) a specialized Testing Team conducts adaptive unit testing and simulation-based validation. Agent2World demonstrates superior inference-time performance across three benchmarks spanning both Planning Domain Definition Language (PDDL) and executable code representations, achieving consistent state-of-the-art results. Beyond inference, Testing Team serves as an interactive environment for the Model Developer, providing behavior-aware adaptive feedback that yields multi-turn training trajectories. The model fine-tuned on these trajectories substantially improves world-model generation, yielding an average relative gain of 30.95% over the same model before training. Project page: https://agent2world.github.io.

</details>


### [12] [Subgoaling Relaxation-based Heuristics for Numeric Planning with Infinite Actions](https://arxiv.org/abs/2512.22367)
*Ángel Aso-Mollar,Diego Aineto,Enrico Scala,Eva Onaindia*

Main category: cs.AI

TL;DR: The paper introduces an approach for numeric planning with control parameters, transforming complex problems into manageable ones by allowing traditional numeric heuristics to address infinite possible actions efficiently.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of applying traditional numeric heuristics in numeric planning settings where infinite actions are possible due to the presence of control parameters.

Method: The authors propose an optimistic compilation technique that abstracts control-dependent expressions into bounded constant effects and relaxed preconditions, converting the planning problems into simple numeric tasks.

Result: The proposed approach allows the effective use of subgoaling heuristics, demonstrating computational feasibility and success in applying traditional numeric heuristics to problems with infinite action possibilities.

Conclusion: This approach significantly enhances the applicability of numeric heuristics, making it feasible to solve previously intractable numeric planning problems with control parameters.

Abstract: Numeric planning with control parameters extends the standard numeric planning model by introducing action parameters as free numeric variables that must be instantiated during planning. This results in a potentially infinite number of applicable actions in a state. In this setting, off-the-shelf numeric heuristics that leverage the action structure are not feasible. In this paper, we identify a tractable subset of these problems--namely, controllable, simple numeric problems--and propose an optimistic compilation approach that transforms them into simple numeric tasks. To do so, we abstract control-dependent expressions into bounded constant effects and relaxed preconditions. The proposed compilation makes it possible to effectively use subgoaling heuristics to estimate goal distance in numeric planning problems involving control parameters. Our results demonstrate that this approach is an effective and computationally feasible way of applying traditional numeric heuristics to settings with an infinite number of possible actions, pushing the boundaries of the current state of the art.

</details>


### [13] [HalluMat: Detecting Hallucinations in LLM-Generated Materials Science Content Through Multi-Stage Verification](https://arxiv.org/abs/2512.22396)
*Bhanu Prakash Vangala,Sajid Mahmud,Pawan Neupane,Joel Selvaraj,Jianlin Cheng*

Main category: cs.AI

TL;DR: This paper introduces HalluMatData and HalluMatDetector to address hallucination issues in AI-generated materials science content, presenting benchmarks, a detection framework, and measures for improving research integrity.


<details>
  <summary>Details</summary>
Motivation: The paper aims to tackle the challenge of hallucination in large language models that generate factual inconsistencies, threatening scientific reliability in AI-assisted discoveries.

Method: The authors developed HalluMatData as a benchmark dataset and HalluMatDetector, a multi-stage detection framework incorporating intrinsic verification, multi-source retrieval, contradiction graph analysis, and metric assessments.

Result: HalluMatDetector reduces hallucination rates in LLM outputs by 30% and reveals disparities in factual consistency across materials science subdomains.

Conclusion: The study provides tools and metrics to lessen hallucinations in AI research outputs, improving reliability and integrity in scientific applications.

Abstract: Artificial Intelligence (AI), particularly Large Language Models (LLMs), is transforming scientific discovery, enabling rapid knowledge generation and hypothesis formulation. However, a critical challenge is hallucination, where LLMs generate factually incorrect or misleading information, compromising research integrity. To address this, we introduce HalluMatData, a benchmark dataset for evaluating hallucination detection methods, factual consistency, and response robustness in AI-generated materials science content. Alongside this, we propose HalluMatDetector, a multi-stage hallucination detection framework that integrates intrinsic verification, multi-source retrieval, contradiction graph analysis, and metric-based assessment to detect and mitigate LLM hallucinations. Our findings reveal that hallucination levels vary significantly across materials science subdomains, with high-entropy queries exhibiting greater factual inconsistencies. By utilizing HalluMatDetector verification pipeline, we reduce hallucination rates by 30% compared to standard LLM outputs. Furthermore, we introduce the Paraphrased Hallucination Consistency Score (PHCS) to quantify inconsistencies in LLM responses across semantically equivalent queries, offering deeper insights into model reliability.

</details>


### [14] [Lightweight Inference-Time Personalization for Frozen Knowledge Graph Embeddings](https://arxiv.org/abs/2512.22398)
*Ozan Oguztuzun,Cerag Oguztuzun*

Main category: cs.AI

TL;DR: The paper introduces GatedBias, a lightweight framework for adapting foundation models for knowledge graphs to individual user contexts without retraining, improving personalization while preserving global accuracy.


<details>
  <summary>Details</summary>
Motivation: Foundation models for knowledge graphs perform well on general tasks but fail to account for individual user preferences, highlighting a gap between general reasoning and personalized needs.

Method: The GatedBias framework uses structure-gated adaptation with profile-specific features and graph-derived binary gates to create interpretable biases for users, requiring only around 300 trainable parameters. It operates during inference without fine-tuning the original model.

Result: GatedBias improves alignment metrics significantly on Amazon-Book and Last-FM datasets, maintains cohort-level performance, and provides causal responsiveness validated by counterfactual experiments.

Conclusion: Personalized adaptation of foundation models is feasible, efficient, and effective, enabling the integration of general knowledge with individual user requirements.

Abstract: Foundation models for knowledge graphs (KGs) achieve strong cohort-level performance in link prediction, yet fail to capture individual user preferences; a key disconnect between general relational reasoning and personalized ranking. We propose GatedBias, a lightweight inference-time personalization framework that adapts frozen KG embeddings to individual user contexts without retraining or compromising global accuracy. Our approach introduces structure-gated adaptation: profile-specific features combine with graph-derived binary gates to produce interpretable, per-entity biases, requiring only ${\sim}300$ trainable parameters. We evaluate GatedBias on two benchmark datasets (Amazon-Book and Last-FM), demonstrating statistically significant improvements in alignment metrics while preserving cohort performance. Counterfactual perturbation experiments validate causal responsiveness; entities benefiting from specific preference signals show 6--30$\times$ greater rank improvements when those signals are boosted. These results show that personalized adaptation of foundation models can be both parameter-efficient and causally verifiable, bridging general knowledge representations with individual user needs.

</details>


### [15] [Monadic Context Engineering](https://arxiv.org/abs/2512.22431)
*Yifan Zhang,Mengdi Wang*

Main category: cs.AI

TL;DR: The paper proposes Monadic Context Engineering (MCE), a formal architecture for building robust AI agents using algebraic structures like Functors, Applicatives, and Monads.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges in current agent architectures, such as state management, error handling, and concurrency, by providing a more structured and resilient framework.

Method: The method involves using algebraic structures (Functors, Applicative Functors, and Monads) and Monad Transformers to manage agent workflows and computational contexts.

Result: The proposed MCE framework enables the creation of robust AI agents with organized sequential and parallel execution while ensuring better state and error management.

Conclusion: MCE can be applied for building complex AI agents and even Meta-Agents, which utilize metaprogramming for dynamic orchestration of workflows, leading to more resilient systems.

Abstract: The proliferation of Large Language Models (LLMs) has catalyzed a shift towards autonomous agents capable of complex reasoning and tool use. However, current agent architectures are frequently constructed using imperative, ad hoc patterns. This results in brittle systems plagued by difficulties in state management, error handling, and concurrency. This paper introduces Monadic Context Engineering (MCE), a novel architectural paradigm leveraging the algebraic structures of Functors, Applicative Functors, and Monads to provide a formal foundation for agent design. MCE treats agent workflows as computational contexts where cross-cutting concerns, such as state propagation, short-circuiting error handling, and asynchronous execution, are managed intrinsically by the algebraic properties of the abstraction. We demonstrate how Monads enable robust sequential composition, how Applicatives provide a principled structure for parallel execution, and crucially, how Monad Transformers allow for the systematic composition of these capabilities. This layered approach enables developers to construct complex, resilient, and efficient AI agents from simple, independently verifiable components. We further extend this framework to describe Meta-Agents, which leverage MCE for generative orchestration, dynamically creating and managing sub-agent workflows through metaprogramming. Project Page: https://github.com/yifanzhang-pro/monadic-context-engineering.

</details>


### [16] [DarkPatterns-LLM: A Multi-Layer Benchmark for Detecting Manipulative and Harmful AI Behavior](https://arxiv.org/abs/2512.22470)
*Sadia Asif,Israel Antonio Rosales Laguan,Haris Khan,Shumaila Asif,Muneeb Asif*

Main category: cs.AI

TL;DR: This paper introduces DarkPatterns-LLM, a benchmark for assessing manipulative behaviors in LLMs across seven harm categories, with fine-grained evaluation methods.


<details>
  <summary>Details</summary>
Motivation: To address concerns about manipulative and deceptive behaviors in LLMs that may harm user autonomy, trust, and well-being.

Method: Developed a benchmark dataset with 401 examples and proposed a four-layer analytical pipeline: Multi-Granular Detection, Multi-Scale Intent Analysis, Threat Harmonization Protocol, and Deep Contextual Risk Alignment.

Result: Evaluated state-of-the-art LLMs, revealing performance disparities and weaknesses in detecting autonomy-related manipulation patterns.

Conclusion: DarkPatterns-LLM sets a new standard in multi-dimensional manipulation detection, aiding in creating more trustworthy AI systems.

Abstract: The proliferation of Large Language Models (LLMs) has intensified concerns about manipulative or deceptive behaviors that can undermine user autonomy, trust, and well-being. Existing safety benchmarks predominantly rely on coarse binary labels and fail to capture the nuanced psychological and social mechanisms constituting manipulation. We introduce \textbf{DarkPatterns-LLM}, a comprehensive benchmark dataset and diagnostic framework for fine-grained assessment of manipulative content in LLM outputs across seven harm categories: Legal/Power, Psychological, Emotional, Physical, Autonomy, Economic, and Societal Harm. Our framework implements a four-layer analytical pipeline comprising Multi-Granular Detection (MGD), Multi-Scale Intent Analysis (MSIAN), Threat Harmonization Protocol (THP), and Deep Contextual Risk Alignment (DCRA). The dataset contains 401 meticulously curated examples with instruction-response pairs and expert annotations. Through evaluation of state-of-the-art models including GPT-4, Claude 3.5, and LLaMA-3-70B, we observe significant performance disparities (65.2\%--89.7\%) and consistent weaknesses in detecting autonomy-undermining patterns. DarkPatterns-LLM establishes the first standardized, multi-dimensional benchmark for manipulation detection in LLMs, offering actionable diagnostics toward more trustworthy AI systems.

</details>


### [17] [Multi-AI Agent Framework Reveals the "Oxide Gatekeeper" in Aluminum Nanoparticle Oxidation](https://arxiv.org/abs/2512.22529)
*Yiming Lu,Tingyu Lu,Di Zhang,Lili Ye,Hao Li*

Main category: cs.AI

TL;DR: The study develops a collaborative AI-driven framework to simulate aluminum nanoparticle reactions, revealing key atomic-scale combustion mechanisms and resolving longstanding controversies.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of ab initio and empirical force field methods in simulating aluminum nanoparticles' transition to explosive reactants.

Method: A human-in-the-loop framework using self-auditing AI agents to evolve and validate a machine learning potential for large-scale, accurate simulations.

Result: The model achieved scalability to million-atom systems, uncovering dual-mode oxidation mechanisms and proving aluminum cation diffusion dominates mass transfer.

Conclusion: The atomic-level insights provide a unified framework for designing energetic nanomaterials with improved ignition sensitivity and energy release rates.

Abstract: Aluminum nanoparticles (ANPs) are among the most energy-dense solid fuels, yet the atomic mechanisms governing their transition from passivated particles to explosive reactants remain elusive. This stems from a fundamental computational bottleneck: ab initio methods offer quantum accuracy but are restricted to small spatiotemporal scales (< 500 atoms, picoseconds), while empirical force fields lack the reactive fidelity required for complex combustion environments. Herein, we bridge this gap by employing a "human-in-the-loop" closed-loop framework where self-auditing AI Agents validate the evolution of a machine learning potential (MLP). By acting as scientific sentinels that visualize hidden model artifacts for human decision-making, this collaborative cycle ensures quantum mechanical accuracy while exhibiting near-linear scalability to million-atom systems and accessing nanosecond timescales (energy RMSE: 1.2 meV/atom, force RMSE: 0.126 eV/Angstrom). Strikingly, our simulations reveal a temperature-regulated dual-mode oxidation mechanism: at moderate temperatures, the oxide shell acts as a dynamic "gatekeeper," regulating oxidation through a "breathing mode" of transient nanochannels; above a critical threshold, a "rupture mode" unleashes catastrophic shell failure and explosive combustion. Importantly, we resolve a decades-old controversy by demonstrating that aluminum cation outward diffusion, rather than oxygen transport, dominates mass transfer across all temperature regimes, with diffusion coefficients consistently exceeding those of oxygen by 2-3 orders of magnitude. These discoveries establish a unified atomic-scale framework for energetic nanomaterial design, enabling the precision engineering of ignition sensitivity and energy release rates through intelligent computational design.

</details>


### [18] [SANet: A Semantic-aware Agentic AI Networking Framework for Cross-layer Optimization in 6G](https://arxiv.org/abs/2512.22579)
*Yong Xiao,Xubo Li,Haoran Zhou,Yingyu Li,Yayu Gao,Guangming Shi,Ping Zhang,Marwan Krunz*

Main category: cs.AI

TL;DR: The paper introduces SANet, a semantic-aware AgentNet architecture for wireless networks, achieving better performance and efficiency using a decentralized, multi-agent approach.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of decentralized multi-agent frameworks in achieving autonomous and adaptive functions for complex wireless networks.

Method: The authors propose a decentralized SANet framework, multi-agent optimization, novel metrics, a model partition-sharing framework, and two optimization algorithms.

Result: The proposed SANet framework achieved performance gains of up to 14.61% while requiring only 44.37% of FLOPs compared to state-of-the-art methods.

Conclusion: SANet is an effective architecture for optimizing wireless networks using decentralized agent interactions with proven theoretical and hardware-based experimental validation.

Abstract: Agentic AI networking (AgentNet) is a novel AI-native networking paradigm in which a large number of specialized AI agents collaborate to perform autonomous decision-making, dynamic environmental adaptation, and complex missions. It has the potential to facilitate real-time network management and optimization functions, including self-configuration, self-optimization, and self-adaptation across diverse and complex environments. This paper proposes SANet, a novel semantic-aware AgentNet architecture for wireless networks that can infer the semantic goal of the user and automatically assign agents associated with different layers of the network to fulfill the inferred goal. Motivated by the fact that AgentNet is a decentralized framework in which collaborating agents may generally have different and even conflicting objectives, we formulate the decentralized optimization of SANet as a multi-agent multi-objective problem, and focus on finding the Pareto-optimal solution for agents with distinct and potentially conflicting objectives. We propose three novel metrics for evaluating SANet. Furthermore, we develop a model partition and sharing (MoPS) framework in which large models, e.g., deep learning models, of different agents can be partitioned into shared and agent-specific parts that are jointly constructed and deployed according to agents' local computational resources. Two decentralized optimization algorithms are proposed. We derive theoretical bounds and prove that there exists a three-way tradeoff among optimization, generalization, and conflicting errors. We develop an open-source RAN and core network-based hardware prototype that implements agents to interact with three different layers of the network. Experimental results show that the proposed framework achieved performance gains of up to 14.61% while requiring only 44.37% of FLOPs required by state-of-the-art algorithms.

</details>


### [19] [Tyee: A Unified, Modular, and Fully-Integrated Configurable Toolkit for Intelligent Physiological Health Care](https://arxiv.org/abs/2512.22601)
*Tao Zhou,Lingyu Shu,Zixing Zhang,Jing Han*

Main category: cs.AI

TL;DR: The paper presents Tyee, a toolkit addressing challenges in physiological signal analysis via unified interfaces, modular architecture, and end-to-end configurations, achieving strong performance across datasets.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address issues in physiological signal analysis—such as fragmented pipelines, inconsistent preprocessing, and lack of reproducibility—to improve intelligent healthcare applications.

Method: The authors developed Tyee, a toolkit featuring a unified data interface and preprocessing for 12 signal modalities, modular extensibility for prototyping, and end-to-end experiment scalability.

Result: Tyee consistently outperformed or matched baseline models, achieving state-of-the-art performance on 12 out of 13 datasets.

Conclusion: Tyee effectively improves reproducibility, scalability, and task performance in intelligent physiological healthcare, offering a robust toolkit for various applications.

Abstract: Deep learning has shown great promise in physiological signal analysis, yet its progress is hindered by heterogeneous data formats, inconsistent preprocessing strategies, fragmented model pipelines, and non-reproducible experimental setups. To address these limitations, we present Tyee, a unified, modular, and fully-integrated configurable toolkit designed for intelligent physiological healthcare. Tyee introduces three key innovations: (1) a unified data interface and configurable preprocessing pipeline for 12 kinds of signal modalities; (2) a modular and extensible architecture enabling flexible integration and rapid prototyping across tasks; and (3) end-to-end workflow configuration, promoting reproducible and scalable experimentation. Tyee demonstrates consistent practical effectiveness and generalizability, outperforming or matching baselines across all evaluated tasks (with state-of-the-art results on 12 of 13 datasets). The Tyee toolkit is released at https://github.com/SmileHnu/Tyee and actively maintained.

</details>


### [20] [Learning Multi-Modal Mobility Dynamics for Generalized Next Location Recommendation](https://arxiv.org/abs/2512.22605)
*Junshu Dai,Yu Wang,Tongya Zheng,Wei Ji,Qinghong Guo,Ji Cao,Jie Song,Canghong Jin,Mingli Song*

Main category: cs.AI

TL;DR: The paper proposes a novel approach called M$^3$ob for improving human mobility prediction by leveraging multi-modal spatial-temporal dynamics and large language model-enhanced spatial-temporal knowledge graphs.


<details>
  <summary>Details</summary>
Motivation: Existing methods for human mobility prediction face challenges like data sparsity and limited generalization caused by biases and inefficiencies in unimodal or static multi-modal approaches.

Method: The authors construct a unified spatial-temporal relational graph (STRG) using multi-modal representations and functional semantics from large language models (LLMs) and design a gating fusion mechanism with cross-modal alignment to enhance mobility dynamics.

Result: The proposed method demonstrates consistent improvements across six datasets in normal scenarios and shows robust generalization in abnormal scenarios.

Conclusion: Integrating multi-modal spatial-temporal knowledge significantly enhances the prediction of human mobility, providing a superior and generalizable framework over existing methods.

Abstract: The precise prediction of human mobility has produced significant socioeconomic impacts, such as location recommendations and evacuation suggestions. However, existing methods suffer from limited generalization capability: unimodal approaches are constrained by data sparsity and inherent biases, while multi-modal methods struggle to effectively capture mobility dynamics caused by the semantic gap between static multi-modal representation and spatial-temporal dynamics. Therefore, we leverage multi-modal spatial-temporal knowledge to characterize mobility dynamics for the location recommendation task, dubbed as \textbf{M}ulti-\textbf{M}odal \textbf{Mob}ility (\textbf{M}$^3$\textbf{ob}). First, we construct a unified spatial-temporal relational graph (STRG) for multi-modal representation, by leveraging the functional semantics and spatial-temporal knowledge captured by the large language models (LLMs)-enhanced spatial-temporal knowledge graph (STKG). Second, we design a gating mechanism to fuse spatial-temporal graph representations of different modalities, and propose an STKG-guided cross-modal alignment to inject spatial-temporal dynamic knowledge into the static image modality. Extensive experiments on six public datasets show that our proposed method not only achieves consistent improvements in normal scenarios but also exhibits significant generalization ability in abnormal scenarios.

</details>


### [21] [LLM Agents as VC investors: Predicting Startup Success via RolePlay-Based Collective Simulation](https://arxiv.org/abs/2512.22608)
*Zhongyang Liu,Haoyu Pei,Xiangyi Xiao,Xiaocong Du,Yihui Li,Suting Hong,Kunpeng Zhang,Haipeng Zhang*

Main category: cs.AI

TL;DR: This paper introduces SimVC-CAS, a model to improve startup success predictions by simulating VC decision-making using a multi-agent system.


<details>
  <summary>Details</summary>
Motivation: Existing methods for assessing startup success focus on individual decision-makers but ignore the collective dynamics of investor groups prevalent in real-world venture capital.

Method: SimVC-CAS uses a collective agent system with role-playing agents and a graph neural network (GNN)-based supervised interaction model. It evaluates startups by modeling group decision-making among diverse investor traits and co-investment networks.

Result: The model demonstrated approximately 25% relative improvement in predictive accuracy with average precision@10, using real-world PitchBook data while ensuring no data leakage.

Conclusion: SimVC-CAS provides a more accurate and interpretable way to predict startup success and opens pathways for tackling other group decision-making challenges.

Abstract: Due to the high value and high failure rate of startups, predicting their success has become a critical challenge across interdisciplinary research. Existing approaches typically model success prediction from the perspective of a single decision-maker, overlooking the collective dynamics of investor groups that dominate real-world venture capital (VC) decisions. In this paper, we propose SimVC-CAS, a novel collective agent system that simulates VC decision-making as a multi-agent interaction process. By designing role-playing agents and a GNN-based supervised interaction module, we reformulate startup financing prediction as a group decision-making task, capturing both enterprise fundamentals and the behavioral dynamics of potential investor networks. Each agent embodies an investor with unique traits and preferences, enabling heterogeneous evaluation and realistic information exchange through a graph-structured co-investment network. Using real-world data from PitchBook and under strict data leakage controls, we show that SimVC-CAS significantly improves predictive accuracy while providing interpretable, multiperspective reasoning, for example, approximately 25% relative improvement with respect to average precision@10. SimVC-CAS also sheds light on other complex group decision scenarios.

</details>


### [22] [The Wisdom of Deliberating AI Crowds: Does Deliberation Improve LLM-Based Forecasting?](https://arxiv.org/abs/2512.22625)
*Paul Schneider,Amalie Schramm*

Main category: cs.AI

TL;DR: The study explores if structured deliberation among Large Language Models (LLMs) could improve forecasting accuracy, finding improvements in diverse LLMs sharing information but not in homogeneous LLM settings.


<details>
  <summary>Details</summary>
Motivation: Investigating whether structured deliberation, proven effective in human forecasting, can enhance LLM forecasting accuracy.

Method: Using 202 binary questions from an AI forecasting tournament, the study evaluated accuracy across four scenarios: varying LLM diversity, information sharing, and distribution, to test the effects of structured deliberation.

Result: Structured deliberation significantly improved accuracy in diverse LLM groups with shared information, reducing Log Loss by 4%. No improvement was seen in homogeneous models or when additional context was provided.

Conclusion: The study concludes that structured deliberation among diverse LLMs can enhance forecasting performance, but information pooling remains uncertain, and homogeneous group accuracy is unaffected.

Abstract: Structured deliberation has been found to improve the performance of human forecasters. This study investigates whether a similar intervention, i.e. allowing LLMs to review each other's forecasts before updating, can improve accuracy in large language models (GPT-5, Claude Sonnet 4.5, Gemini Pro 2.5). Using 202 resolved binary questions from the Metaculus Q2 2025 AI Forecasting Tournament, accuracy was assessed across four scenarios: (1) diverse models with distributed information, (2) diverse models with shared information, (3) homogeneous models with distributed information, and (4) homogeneous models with shared information. Results show that the intervention significantly improves accuracy in scenario (2), reducing Log Loss by 0.020 or about 4 percent in relative terms (p = 0.017). However, when homogeneous groups (three instances of the same model) engaged in the same process, no benefit was observed. Unexpectedly, providing LLMs with additional contextual information did not improve forecast accuracy, limiting our ability to study information pooling as a mechanism. Our findings suggest that deliberation may be a viable strategy for improving LLM forecasting.

</details>


### [23] [DICE: Discrete Interpretable Comparative Evaluation with Probabilistic Scoring for Retrieval-Augmented Generation](https://arxiv.org/abs/2512.22629)
*Shiyan Liu,Jian Ma,Rui Qu*

Main category: cs.AI

TL;DR: DICE is introduced as an advanced evaluation framework for trustworthy RAG systems, offering robust, explainable, and efficient assessments.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing metrics for RAG systems in terms of interpretability, uncertainty quantification, and computational efficiency.

Method: DICE uses a two-stage framework combining probabilistic scoring with a Swiss-system tournament to reduce computational complexity while enhancing explainability and robustness.

Result: DICE achieved an 85.7% agreement with human experts and outperformed existing metrics like RAGAS on a Chinese financial QA dataset.

Conclusion: DICE establishes itself as a reliable, explainable, and computationally efficient approach for evaluating RAG systems.

Abstract: As Retrieval-Augmented Generation (RAG) systems evolve toward more sophisticated architectures, ensuring their trustworthiness through explainable and robust evaluation becomes critical. Existing scalar metrics suffer from limited interpretability, inadequate uncertainty quantification, and computational inefficiency in multi-system comparisons, hindering responsible deployment of RAG technologies. We introduce DICE (Discrete Interpretable Comparative Evaluation), a two-stage, evidence-coupled framework that advances explainability and robustness in RAG evaluation. DICE combines deep analytical reasoning with probabilistic $\{A, B, Tie\}$ scoring to produce transparent, confidence-aware judgments that support accountable system improvement through interpretable reasoning traces, enabling systematic error diagnosis and actionable insights. To address efficiency challenges at scale, DICE employs a Swiss-system tournament that reduces computational complexity from $O(N^2)$ to $O(N \log N)$, achieving a 42.9% reduction in our eight-system evaluation while preserving ranking fidelity. Validation on a curated Chinese financial QA dataset demonstrates that DICE achieves 85.7% agreement with human experts, substantially outperforming existing LLM-based metrics such as RAGAS. Our results establish DICE as a responsible, explainable, and efficient paradigm for trustworthy RAG system assessment.

</details>


### [24] [TravelBench: A Real-World Benchmark for Multi-Turn and Tool-Augmented Travel Planning](https://arxiv.org/abs/2512.22673)
*Xiang Cheng,Yulan Hu,Xiangwen Zhang,Lu Xu,Zheng Pan,Xin Li,Yong Liu*

Main category: cs.AI

TL;DR: The paper introduces TravelBench, a travel-planning benchmark for evaluating large language model (LLM) agents on multi-turn interactions, tool use, and reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing studies on LLMs in travel planning have limitations in domain coverage and dynamic, multi-turn interactions, leading to inadequate evaluation of these agents. The motivation is to create a comprehensive, practical benchmark for LLM capabilities in complex, real-world scenarios.

Method: The authors designed TravelBench, a controlled benchmarking environment featuring real-world user requests and 10 travel planning tools. The benchmark includes subsets such as multi-turn, single-turn, and unsolvable scenarios to assess various aspects of LLM's performance.

Result: The evaluation of multiple LLMs on TravelBench showed diverse behaviors and performance levels, highlighting strengths and weaknesses in reasoning and tool use under dynamic conditions.

Conclusion: TravelBench establishes a practical and reproducible benchmark for studying and advancing LLM capabilities in travel planning, addressing gaps in prior evaluations.

Abstract: Large language model (LLM) agents have demonstrated strong capabilities in planning and tool use. Travel planning provides a natural and high-impact testbed for these capabilities, as it requires multi-step reasoning, iterative preference elicitation through interaction, and calls to external tools under evolving constraints. Prior work has studied LLMs on travel-planning tasks, but existing settings are limited in domain coverage and multi-turn interaction. As a result, they cannot support dynamic user-agent interaction and therefore fail to comprehensively assess agent capabilities. In this paper, we introduce TravelBench, a real-world travel-planning benchmark featuring multi-turn interaction and tool use. We collect user requests from real-world scenarios and construct three subsets-multi-turn, single-turn, and unsolvable-to evaluate different aspects of agent performance. For stable and reproducible evaluation, we build a controlled sandbox environment with 10 travel-domain tools, providing deterministic tool outputs for reliable reasoning. We evaluate multiple LLMs on TravelBench and conduct an analysis of their behaviors and performance. TravelBench offers a practical and reproducible benchmark for advancing LLM agents in travel planning.

</details>


### [25] [Memento-II: Learning by Stateful Reflective Memory](https://arxiv.org/abs/2512.22716)
*Jun Wang*

Main category: cs.AI

TL;DR: The paper introduces a theoretical framework enabling large language model agents to learn continually via reflection, using episodic memory and reinforcement learning without parameter updates.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of continual learning and adaptation in large language models without requiring back propagation or fine-tuning during deployment.

Method: The framework, termed Stateful Reflective Decision Process, uses episodic memory to model learning through a two-stage read-write cycle: writing stores outcomes (policy evaluation), and reading retrieves cases (policy improvement). This induces an augmented Markov decision process for policy optimization.

Result: The framework demonstrates convergence to an optimal solution as the episodic memory sufficiently covers the state space, supported by entropy regularized policy iteration.

Conclusion: The study provides foundational principles for memory-augmented and retrieval-based language models, facilitating continual adaptation and learning without modifying model parameters.

Abstract: We propose a theoretical framework for continual and experiential learning in large language model agents that integrates episodic memory with reinforcement learning. The framework identifies reflection as the key mechanism that enables agents to adapt through interaction without back propagation or model fine tuning, thereby relaxing the conventional separation between training and deployment.To formalise this process, we introduce the Stateful Reflective Decision Process, which models reflective learning as a two stage read write interaction with episodic memory. Writing stores interaction outcomes and corresponds to policy evaluation, while reading retrieves relevant past cases and corresponds to policy improvement. We show that this process induces an equivalent Markov decision process over augmented state memory representations, allowing the use of classical tools from dynamic programming and reinforcement learning. We further instantiate the framework using entropy regularised policy iteration and establish convergence guarantees. As episodic memory grows and achieves sufficient coverage of the state space, the resulting policy converges to the optimal solution. This work provides a principled foundation for memory augmented and retrieval based language model agents capable of continual adaptation without parameter updates.

</details>


### [26] [SAMP-HDRL: Segmented Allocation with Momentum-Adjusted Utility for Multi-agent Portfolio Management via Hierarchical Deep Reinforcement Learning](https://arxiv.org/abs/2512.22895)
*Xiaotian Ren,Nuerxiati Abudurexiti,Zhengyong Jiang,Angelos Stefanidis,Hongbin Liu,Jionglong Su*

Main category: cs.AI

TL;DR: The paper introduces SAMP-HDRL, a hierarchical deep reinforcement learning approach for portfolio management in non-stationary markets, outperforming baselines and improving interpretability.


<details>
  <summary>Details</summary>
Motivation: The challenges of regime shifts, dynamic correlations, and limited interpretability in portfolio optimization within non-stationary financial markets motivate devising novel methods for improved performance and robustness.

Method: The authors propose SAMP-HDRL, which employs dynamic asset grouping, hierarchical reinforcement learning with upper- and lower-level agents, and utility-based capital allocation. These work together to ensure robust portfolio management in volatile markets.

Result: SAMP-HDRL outperforms nine traditional and nine DRL baselines with significant gains in Return, Sharpe ratio, Sortino ratio, and Omega ratio, particularly during volatile market conditions.

Conclusion: The presented framework addresses market constraints in DRL for portfolio management, offering improved adaptability, performance, robustness, and transparency in decision-making.

Abstract: Portfolio optimization in non-stationary markets is challenging due to regime shifts, dynamic correlations, and the limited interpretability of deep reinforcement learning (DRL) policies. We propose a Segmented Allocation with Momentum-Adjusted Utility for Multi-agent Portfolio Management via Hierarchical Deep Reinforcement Learning (SAMP-HDRL). The framework first applies dynamic asset grouping to partition the market into high-quality and ordinary subsets. An upper-level agent extracts global market signals, while lower-level agents perform intra-group allocation under mask constraints. A utility-based capital allocation mechanism integrates risky and risk-free assets, ensuring coherent coordination between global and local decisions. backtests across three market regimes (2019--2021) demonstrate that SAMP-HDRL consistently outperforms nine traditional baselines and nine DRL benchmarks under volatile and oscillating conditions. Compared with the strongest baseline, our method achieves at least 5\% higher Return, 5\% higher Sharpe ratio, 5\% higher Sortino ratio, and 2\% higher Omega ratio, with substantially larger gains observed in turbulent markets. Ablation studies confirm that upper--lower coordination, dynamic clustering, and capital allocation are indispensable to robustness. SHAP-based interpretability further reveals a complementary ``diversified + concentrated'' mechanism across agents, providing transparent insights into decision-making. Overall, SAMP-HDRL embeds structural market constraints directly into the DRL pipeline, offering improved adaptability, robustness, and interpretability in complex financial environments.

</details>


### [27] [HiSciBench: A Hierarchical Multi-disciplinary Benchmark for Scientific Intelligence from Reading to Discovery](https://arxiv.org/abs/2512.22899)
*Yaping Zhang,Qixuan Zhang,Xingquan Zhang,Zhiyuan Chen,Wenwen Zhuang,Yupu Liang,Lu Xiang,Yang Zhao,Jiajun Zhang,Yu Zhou,Chengqing Zong*

Main category: cs.AI

TL;DR: The paper introduces HiSciBench, a hierarchical benchmark for evaluating foundation models' scientific intelligence across five stages of scientific workflows and six disciplines.


<details>
  <summary>Details</summary>
Motivation: To address existing benchmark limitations by creating a comprehensive, hierarchical, and multidisciplinary evaluation for foundation models in scientific research.

Method: HiSciBench evaluates foundation models across five levels of scientific reasoning (from literacy to discovery) on curated tasks that support multimodal inputs and cross-lingual evaluations.

Result: Comprehensive tests on models like GPT-5 revealed performance gaps, with higher scores in basic literacy tasks (69%) and reduced accuracy on discovery-level challenges (25%).

Conclusion: HiSciBench sets a new standard for evaluating scientific intelligence, providing tools and insights to develop more capable and reliable models, with data made publicly available for further research.

Abstract: The rapid advancement of large language models (LLMs) and multimodal foundation models has sparked growing interest in their potential for scientific research. However, scientific intelligence encompasses a broad spectrum of abilities ranging from understanding fundamental knowledge to conducting creative discovery, and existing benchmarks remain fragmented. Most focus on narrow tasks and fail to reflect the hierarchical and multi-disciplinary nature of real scientific inquiry. We introduce \textbf{HiSciBench}, a hierarchical benchmark designed to evaluate foundation models across five levels that mirror the complete scientific workflow: \textit{Scientific Literacy} (L1), \textit{Literature Parsing} (L2), \textit{Literature-based Question Answering} (L3), \textit{Literature Review Generation} (L4), and \textit{Scientific Discovery} (L5). HiSciBench contains 8,735 carefully curated instances spanning six major scientific disciplines, including mathematics, physics, chemistry, biology, geography, and astronomy, and supports multimodal inputs including text, equations, figures, and tables, as well as cross-lingual evaluation. Unlike prior benchmarks that assess isolated abilities, HiSciBench provides an integrated, dependency-aware framework that enables detailed diagnosis of model capabilities across different stages of scientific reasoning. Comprehensive evaluations of leading models, including GPT-5, DeepSeek-R1, and several multimodal systems, reveal substantial performance gaps: while models achieve up to 69\% accuracy on basic literacy tasks, performance declines sharply to 25\% on discovery-level challenges. HiSciBench establishes a new standard for evaluating scientific Intelligence and offers actionable insights for developing models that are not only more capable but also more reliable. The benchmark will be publicly released to facilitate future research.

</details>


### [28] [Geometric Structural Knowledge Graph Foundation Model](https://arxiv.org/abs/2512.22931)
*Ling Xin,Mojtaba Nayyeri,Zahra Makki Nayeri,Steffen Staab*

Main category: cs.AI

TL;DR: The paper introduces Gamma, a knowledge graph reasoning model, which enhances expressiveness and adaptability through multi-head geometric attention mechanisms and outperforms existing models like Ultra.


<details>
  <summary>Details</summary>
Motivation: The key motivation is to overcome the limitations of single relational transformations in existing graph models like Ultra, which fail to capture diverse relational and structural patterns on different knowledge graphs.

Method: Gamma employs multiple parallel relational transformations (real, complex, split-complex, and dual number based) combined with relational conditioned attention fusion to adaptively emphasize suitable relational biases at the link level.

Result: Gamma demonstrates superior performance in zero-shot inductive link prediction, achieving 5.5% MRR improvement on inductive benchmarks and 4.4% improvement across all benchmarks compared to Ultra.

Conclusion: The introduction of diverse geometric transformations and an adaptive fusion mechanism make Gamma a more expressive and effective model for knowledge graph reasoning than Ultra.

Abstract: Structural knowledge graph foundation models aim to generalize reasoning to completely new graphs with unseen entities and relations. A key limitation of existing approaches like Ultra is their reliance on a single relational transformation (e.g., element-wise multiplication) in message passing, which can constrain expressiveness and fail to capture diverse relational and structural patterns exhibited on diverse graphs. In this paper, we propose Gamma, a novel foundation model that introduces multi-head geometric attention to knowledge graph reasoning. Gamma replaces the single relational transformation with multiple parallel ones, including real, complex, split-complex, and dual number based transformations, each designed to model different relational structures. A relational conditioned attention fusion mechanism then adaptively fuses them at link level via a lightweight gating with entropy regularization, allowing the model to robustly emphasize the most appropriate relational bias for each triple pattern. We present a full formalization of these algebraic message functions and discuss how their combination increases expressiveness beyond any single space. Comprehensive experiments on 56 diverse knowledge graphs demonstrate that Gamma consistently outperforms Ultra in zero-shot inductive link prediction, with a 5.5% improvement in mean reciprocal rank on the inductive benchmarks and a 4.4% improvement across all benchmarks, highlighting benefits from complementary geometric representations.

</details>


### [29] [Multimodal Fact-Checking: An Agent-based Approach](https://arxiv.org/abs/2512.22933)
*Danni Xu,Shaojing Fan,Xuanang Cheng,Mohan Kankanhalli*

Main category: cs.AI

TL;DR: This paper introduces RW-Post, a dataset for multimodal fact-checking, and AgentFact, a framework using specialized agents to improve accuracy and reasoning for detecting real-world misinformation.


<details>
  <summary>Details</summary>
Motivation: The rapid increase of multimodal misinformation and the limitations of current systems in reasoning and evidence usage prompted the creation of a dataset and framework to address these gaps.

Method: Introduced RW-Post, a dataset with real-world multimodal claims, annotated reasoning, and linked evidence, and developed AgentFact, a multi-agent framework with iterative workflows for evidence analysis and reasoning.

Result: The synergy of RW-Post and AgentFact improves the accuracy and interpretability of multimodal fact-checking based on experimental results.

Conclusion: Combining a high-quality dataset and a specialized agent framework significantly advances the capabilities of multimodal misinformation detection and reasoning.

Abstract: The rapid spread of multimodal misinformation poses a growing challenge for automated fact-checking systems. Existing approaches, including large vision language models (LVLMs) and deep multimodal fusion methods, often fall short due to limited reasoning and shallow evidence utilization. A key bottleneck is the lack of dedicated datasets that provide complete real-world multimodal misinformation instances accompanied by annotated reasoning processes and verifiable evidence. To address this limitation, we introduce RW-Post, a high-quality and explainable dataset for real-world multimodal fact-checking. RW-Post aligns real-world multimodal claims with their original social media posts, preserving the rich contextual information in which the claims are made. In addition, the dataset includes detailed reasoning and explicitly linked evidence, which are derived from human written fact-checking articles via a large language model assisted extraction pipeline, enabling comprehensive verification and explanation. Building upon RW-Post, we propose AgentFact, an agent-based multimodal fact-checking framework designed to emulate the human verification workflow. AgentFact consists of five specialized agents that collaboratively handle key fact-checking subtasks, including strategy planning, high-quality evidence retrieval, visual analysis, reasoning, and explanation generation. These agents are orchestrated through an iterative workflow that alternates between evidence searching and task-aware evidence filtering and reasoning, facilitating strategic decision-making and systematic evidence analysis. Extensive experimental results demonstrate that the synergy between RW-Post and AgentFact substantially improves both the accuracy and interpretability of multimodal fact-checking.

</details>


### [30] [Problems With Large Language Models for Learner Modelling: Why LLMs Alone Fall Short for Responsible Tutoring in K--12 Education](https://arxiv.org/abs/2512.23036)
*Danial Hooshyar,Yeongwook Yang,Gustav Šíř,Tommi Kärkkäinen,Raija Hämäläinen,Mutlu Cukurova,Roger Azevedo*

Main category: cs.AI

TL;DR: This study examines limitations of LLMs as tutors in adaptive education, specifically comparing Deep Knowledge Tracing (DKT) to an LLM on accuracy and reliability in learner modelling. DKT proves superior.


<details>
  <summary>Details</summary>
Motivation: Concerns about the EU AI Act classification of K-12 education as a high-risk domain prompt an investigation into limitations of LLM tutors, especially in reliably modelling learner knowledge.

Method: Using a dataset and comparing DKT with an LLM (evaluated zero-shot and fine-tuned), researchers assess performance on next-step correctness prediction and mastery estimation.

Result: DKT outperforms the LLM in discrimination performance (AUC = 0.83), stable knowledge updates, and temporal coherence, while LLM requires excessive computational resources and fails on early-sequence predictions.

Conclusion: LLMs alone cannot replace established learner modelling systems; hybrid frameworks blending LLMs with traditional models are essential for responsible adaptive tutoring.

Abstract: The rapid rise of large language model (LLM)-based tutors in K--12 education has fostered a misconception that generative models can replace traditional learner modelling for adaptive instruction. This is especially problematic in K--12 settings, which the EU AI Act classifies as high-risk domain requiring responsible design. Motivated by these concerns, this study synthesises evidence on limitations of LLM-based tutors and empirically investigates one critical issue: the accuracy, reliability, and temporal coherence of assessing learners' evolving knowledge over time. We compare a deep knowledge tracing (DKT) model with a widely used LLM, evaluated zero-shot and fine-tuned, using a large open-access dataset. Results show that DKT achieves the highest discrimination performance (AUC = 0.83) on next-step correctness prediction and consistently outperforms the LLM across settings. Although fine-tuning improves the LLM's AUC by approximately 8\% over the zero-shot baseline, it remains 6\% below DKT and produces higher early-sequence errors, where incorrect predictions are most harmful for adaptive support. Temporal analyses further reveal that DKT maintains stable, directionally correct mastery updates, whereas LLM variants exhibit substantial temporal weaknesses, including inconsistent and wrong-direction updates. These limitations persist despite the fine-tuned LLM requiring nearly 198 hours of high-compute training, far exceeding the computational demands of DKT. Our qualitative analysis of multi-skill mastery estimation further shows that, even after fine-tuning, the LLM produced inconsistent mastery trajectories, while DKT maintained smooth and coherent updates. Overall, the findings suggest that LLMs alone are unlikely to match the effectiveness of established intelligent tutoring systems, and that responsible tutoring requires hybrid frameworks that incorporate learner modelling.

</details>


### [31] [The Reward Model Selection Crisis in Personalized Alignment](https://arxiv.org/abs/2512.23067)
*Fady Rezk,Yuangang Pan,Chuan-Sheng Foo,Xun Xu,Nancy Chen,Henry Gouk,Timothy Hospedales*

Main category: cs.AI

TL;DR: This paper investigates the gap between reward model accuracy and its effectiveness in guiding token-level choices in personalized alignment. It introduces new benchmarks and metrics to assess policy-level performance and demonstrates the limitations of current approaches in deployment settings.


<details>
  <summary>Details</summary>
Motivation: The motivation is to highlight the inadequacy of current reward models in effectively guiding deployment-ready, token-level personalized response generation and to introduce better evaluation metrics.

Method: The study proposes policy accuracy as a new metric, uses systematic evaluation across datasets, and introduces Pref-LaMP, a personalized alignment benchmark with ground-truth user completions to evaluate models without circular reward-based metrics.

Result: Findings show a weak correlation between reward model accuracy and policy-level discrimination, a decoupling between discrimination and actual behavior generation, and that simple in-context learning outperforms reward-guided methods for larger models.

Conclusion: The paper concludes that optimizing for current metrics does not ensure effective behavioral alignment under deployment constraints, calling for new evaluation paradigms and metrics in personalized alignment research.

Abstract: Personalized alignment from preference data has focused primarily on improving reward model (RM) accuracy, with the implicit assumption that better preference ranking translates to better personalized behavior. However, in deployment, computational constraints necessitate inference-time adaptation via reward-guided decoding (RGD) rather than per-user policy fine-tuning. This creates a critical but overlooked requirement: reward models must not only rank preferences accurately but also effectively guide token-level generation decisions. We demonstrate that standard RM accuracy fails catastrophically as a selection criterion for deployment-ready personalized alignment. Through systematic evaluation across three datasets, we introduce policy accuracy, a metric quantifying whether RGD scoring functions correctly discriminate between preferred and dispreferred responses. We show that RM accuracy correlates only weakly with this policy-level discrimination ability (Kendall's tau = 0.08--0.31). More critically, we introduce Pref-LaMP, the first personalized alignment benchmark with ground-truth user completions, enabling direct behavioral evaluation without circular reward-based metrics. On Pref-LaMP, we expose a complete decoupling between discrimination and generation: methods with 20-point RM accuracy differences produce almost identical output quality, and even methods achieving high discrimination fail to generate behaviorally aligned responses. Finally, simple in-context learning (ICL) dominates all reward-guided methods for models > 3B parameters, achieving 3-5 point ROUGE-1 gains over the best reward method at 7B scale. These findings show that the field optimizes proxy metrics that fail to predict deployment performance and do not translate preferences into real behavioral adaptation under deployment constraints.

</details>


### [32] [Benchmark Success, Clinical Failure: When Reinforcement Learning Optimizes for Benchmarks, Not Patients](https://arxiv.org/abs/2512.23090)
*Armin Berger,Manuela Bergau,Helen Schneider,Saad Ahmad,Tom Anglim Lagones,Gianluca Brugnara,Martha Foltyn-Dumitru,Kai Schlamp,Philipp Vollmuth,Rafet Sifa*

Main category: cs.AI

TL;DR: The paper introduces ChexReason, a vision-language model for medical imaging utilizing a resource-efficient RL approach, and highlights challenges in performance trade-offs and generalization in clinical applications.


<details>
  <summary>Details</summary>
Motivation: To improve medical imaging reasoning tasks using reinforcement learning with limited computational and data resources, and address robustness across differing medical datasets.

Method: ChexReason employs stepwise supervised fine-tuning (SFT) followed by guidance-tuned reinforcement learning optimization (GRPO) with a focus on resource-efficient training.

Result: GRPO improves in-distribution performance by 23% on CheXpert but decreases cross-dataset generalization by 19% on NIH, revealing a paradox in the RL paradigm that also appears in high-resource models.

Conclusion: Supervised fine-tuning may be more effective than reinforcement learning for clinical deployment, particularly for achieving consistent performance across diverse datasets.

Abstract: Recent Reinforcement Learning (RL) advances for Large Language Models (LLMs) have improved reasoning tasks, yet their resource-constrained application to medical imaging remains underexplored. We introduce ChexReason, a vision-language model trained via R1-style methodology (SFT followed by GRPO) using only 2,000 SFT samples, 1,000 RL samples, and a single A100 GPU. Evaluations on CheXpert and NIH benchmarks reveal a fundamental tension: GRPO recovers in-distribution performance (23% improvement on CheXpert, macro-F1 = 0.346) but degrades cross-dataset transferability (19% drop on NIH). This mirrors high-resource models like NV-Reason-CXR-3B, suggesting the issue stems from the RL paradigm rather than scale. We identify a generalization paradox where the SFT checkpoint uniquely improves on NIH before optimization, indicating teacher-guided reasoning captures more institution-agnostic features. Furthermore, cross-model comparisons show structured reasoning scaffolds benefit general-purpose VLMs but offer minimal gain for medically pre-trained models. Consequently, curated supervised fine-tuning may outperform aggressive RL for clinical deployment requiring robustness across diverse populations.

</details>


### [33] [InSPO: Unlocking Intrinsic Self-Reflection for LLM Preference Optimization](https://arxiv.org/abs/2512.23126)
*Yu Li,Tian Lan,Zhengling Qi*

Main category: cs.AI

TL;DR: This paper introduces Intrinsic Self-reflective Preference Optimization (Q), an enhancement to Direct Preference Optimization (DPO) for aligning Large Language Models (LLMs) through robust self-reflection.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address two key limitations of DPO: reliance on arbitrary modeling choices that affect optimal policy, and failure to leverage pairwise comparative data for intrinsic self-reflection in response generation.

Method: The authors propose Intrinsic Self-reflective Preference Optimization (Q), which derives a globally optimal policy considering both context and alternative responses, ensuring invariance to modeling artifacts and utilizing pairwise data.

Result: Experiments demonstrate that Q improves win rates and length-controlled metrics consistently, enhancing the human-alignment and robustness of LLMs.

Conclusion: The paper concludes that Q unlocks intrinsic self-reflection capabilities in LLMs, significantly improving their robustness and alignment without requiring architectural changes or inference overhead.

Abstract: Direct Preference Optimization (DPO) and its variants have become standard for aligning Large Language Models due to their simplicity and offline stability. However, we identify two fundamental limitations. First, the optimal policy depends on arbitrary modeling choices (scalarization function, reference policy), yielding behavior reflecting parameterization artifacts rather than true preferences. Second, treating response generation in isolation fails to leverage comparative information in pairwise data, leaving the model's capacity for intrinsic self-reflection untapped. To address it, we propose Intrinsic Self-reflective Preference Optimization (\q), deriving a globally optimal policy conditioning on both context and alternative responses. We prove this formulation superior to DPO/RLHF while guaranteeing invariance to scalarization and reference choices. \q~serves as a plug-and-play enhancement without architectural changes or inference overhead. Experiments demonstrate consistent improvements in win rates and length-controlled metrics, validating that unlocking self-reflection yields more robust, human-aligned LLMs.

</details>


### [34] [Why We Need a New Framework for Emotional Intelligence in AI](https://arxiv.org/abs/2512.23163)
*Max Parks,Kheli Atluru,Meera Vinod,Mike Kuniavsky,Jud Brewer,Sean White,Sarah Adler,Wendy Ju*

Main category: cs.AI

TL;DR: The paper critiques and seeks to improve methods for evaluating emotional intelligence (EI) in artificial intelligence (AI) systems.


<details>
  <summary>Details</summary>
Motivation: Current frameworks for evaluating EI in AI are insufficient and not comprehensive. Human EI involves elements that may not apply to AI, but there are aspects of EI that AI systems can emulate to an extent.

Method: The paper reviews theories of EI and emotion, critiques existing benchmark frameworks for AI, and identifies their shortcomings. It also proposes improved strategies for evaluating EI in AI.

Result: Insights into the inadequacies of current EI benchmarks for AI and suggestions for refining evaluation strategies are provided.

Conclusion: The paper concludes that EI evaluation in AI requires more nuanced and robust frameworks grounded in a comprehensive understanding of emotion and emotional intelligence.

Abstract: In this paper, we develop the position that current frameworks for evaluating emotional intelligence (EI) in artificial intelligence (AI) systems need refinement because they do not adequately or comprehensively measure the various aspects of EI relevant in AI. Human EI often involves a phenomenological component and a sense of understanding that artificially intelligent systems lack; therefore, some aspects of EI are irrelevant in evaluating AI systems. However, EI also includes an ability to sense an emotional state, explain it, respond appropriately, and adapt to new contexts (e.g., multicultural), and artificially intelligent systems can do such things to greater or lesser degrees. Several benchmark frameworks specialize in evaluating the capacity of different AI models to perform some tasks related to EI, but these often lack a solid foundation regarding the nature of emotion and what it is to be emotionally intelligent. In this project, we begin by reviewing different theories about emotion and general EI, evaluating the extent to which each is applicable to artificial systems. We then critically evaluate the available benchmark frameworks, identifying where each falls short in light of the account of EI developed in the first section. Lastly, we outline some options for improving evaluation strategies to avoid these shortcomings in EI evaluation in AI systems.

</details>


### [35] [SPIRAL: Symbolic LLM Planning via Grounded and Reflective Search](https://arxiv.org/abs/2512.23167)
*Yifan Zhang,Giridhar Ganapavarapu,Srideepika Jayaraman,Bhavna Agrawal,Dhaval Patel,Achille Fokoue*

Main category: cs.AI

TL;DR: The paper introduces SPIRAL, a framework enhancing Large Language Models (LLMs) through a specialized cognitive architecture integrated into Monte Carlo Tree Search (MCTS) for improved planning and reasoning.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in complex planning tasks where LLMs fail due to linear reasoning processes and sparse reward guidance in traditional search algorithms.

Method: SPIRAL incorporates three LLM agents—Planner, Simulator, and Critic—within an MCTS loop to synergize creative planning, realistic outcome prediction, and dense reflective feedback.

Result: SPIRAL outperforms other state-of-the-art methods, achieving 83.6% accuracy on the DailyLifeAPIs dataset, over 16% above competing frameworks, with notable token efficiency.

Conclusion: Structuring LLM reasoning as a guided and reflective search process can significantly improve autonomous planning capabilities.

Abstract: Large Language Models (LLMs) often falter at complex planning tasks that require exploration and self-correction, as their linear reasoning process struggles to recover from early mistakes. While search algorithms like Monte Carlo Tree Search (MCTS) can explore alternatives, they are often ineffective when guided by sparse rewards and fail to leverage the rich semantic capabilities of LLMs. We introduce SPIRAL (Symbolic LLM Planning via Grounded and Reflective Search), a novel framework that embeds a cognitive architecture of three specialized LLM agents into an MCTS loop. SPIRAL's key contribution is its integrated planning pipeline where a Planner proposes creative next steps, a Simulator grounds the search by predicting realistic outcomes, and a Critic provides dense reward signals through reflection. This synergy transforms MCTS from a brute-force search into a guided, self-correcting reasoning process. On the DailyLifeAPIs and HuggingFace datasets, SPIRAL consistently outperforms the default Chain-of-Thought planning method and other state-of-the-art agents. More importantly, it substantially surpasses other state-of-the-art agents; for example, SPIRAL achieves 83.6% overall accuracy on DailyLifeAPIs, an improvement of over 16 percentage points against the next-best search framework, while also demonstrating superior token efficiency. Our work demonstrates that structuring LLM reasoning as a guided, reflective, and grounded search process yields more robust and efficient autonomous planners. The source code, full appendices, and all experimental data are available for reproducibility at the official project repository.

</details>


### [36] [From Model Choice to Model Belief: Establishing a New Measure for LLM-Based Research](https://arxiv.org/abs/2512.23184)
*Hongshen Sun,Juanjuan Zhang*

Main category: cs.AI

TL;DR: The paper introduces "model belief," derived from LLM token probabilities, as a statistically more efficient measure than traditional "model choice."


<details>
  <summary>Details</summary>
Motivation: To improve the statistical efficiency and information extraction from LLM-generated outputs, which are typically underutilized when treated as single data points.

Method: The authors formalize the concept of "model belief," prove its statistical properties, and validate it via a demand estimation study for simulating consumer behavior.

Result: Model belief offers reduced variance, faster convergence, and greater predictive power compared to model choice, cutting computation needs by 20x in practical applications.

Conclusion: Model belief should be the standard for extracting insights from LLM data, providing more efficient and accurate results compared to traditional model choice methods.

Abstract: Large language models (LLMs) are increasingly used to simulate human behavior, but common practices to use LLM-generated data are inefficient. Treating an LLM's output ("model choice") as a single data point underutilizes the information inherent to the probabilistic nature of LLMs. This paper introduces and formalizes "model belief," a measure derived from an LLM's token-level probabilities that captures the model's belief distribution over choice alternatives in a single generation run. The authors prove that model belief is asymptotically equivalent to the mean of model choices (a non-trivial property) but forms a more statistically efficient estimator, with lower variance and a faster convergence rate. Analogous properties are shown to hold for smooth functions of model belief and model choice often used in downstream applications. The authors demonstrate the performance of model belief through a demand estimation study, where an LLM simulates consumer responses to different prices. In practical settings with limited numbers of runs, model belief explains and predicts ground-truth model choice better than model choice itself, and reduces the computation needed to reach sufficiently accurate estimates by roughly a factor of 20. The findings support using model belief as the default measure to extract more information from LLM-generated data.

</details>


### [37] [TCEval: Using Thermal Comfort to Assess Cognitive and Perceptual Abilities of AI](https://arxiv.org/abs/2512.23217)
*Jingming Li*

Main category: cs.AI

TL;DR: TCEval is introduced as a new framework to evaluate AI cognitive abilities using thermal comfort scenarios, revealing that current LLMs perform well in foundational reasoning but lack precise causal understanding.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the critical gap in task-specific benchmarks for LLMs by creating a real-world evaluation framework that assesses cognitive capabilities like reasoning, causal association, and decision-making.

Method: The method involves using virtual LLM agents with personality attributes to generate thermal comfort responses, comparing outputs with human data in thermal comfort databases, and statistically analyzing the alignment.

Result: Current LLMs show foundational abilities in cross-modal reasoning but fail in precise causal understanding and discrete classification. Their generated data diverges notably from human responses.

Conclusion: TCEval provides an ecologically valid test for AI, establishing the importance of embodied, context-aware benchmarks over abstract task-focused ones, and offers insights for improving AI for human-centric applications.

Abstract: A critical gap exists in LLM task-specific benchmarks. Thermal comfort, a sophisticated interplay of environmental factors and personal perceptions involving sensory integration and adaptive decision-making, serves as an ideal paradigm for evaluating real-world cognitive capabilities of AI systems. To address this, we propose TCEval, the first evaluation framework that assesses three core cognitive capacities of AI, cross-modal reasoning, causal association, and adaptive decision-making, by leveraging thermal comfort scenarios and large language model (LLM) agents. The methodology involves initializing LLM agents with virtual personality attributes, guiding them to generate clothing insulation selections and thermal comfort feedback, and validating outputs against the ASHRAE Global Database and Chinese Thermal Comfort Database. Experiments on four LLMs show that while agent feedback has limited exact alignment with humans, directional consistency improves significantly with a 1 PMV tolerance. Statistical tests reveal that LLM-generated PMV distributions diverge markedly from human data, and agents perform near-randomly in discrete thermal comfort classification. These results confirm the feasibility of TCEval as an ecologically valid Cognitive Turing Test for AI, demonstrating that current LLMs possess foundational cross-modal reasoning ability but lack precise causal understanding of the nonlinear relationships between variables in thermal comfort. TCEval complements traditional benchmarks, shifting AI evaluation focus from abstract task proficiency to embodied, context-aware perception and decision-making, offering valuable insights for advancing AI in human-centric applications like smart buildings.

</details>


### [38] [Agentic Physical AI toward a Domain-Specific Foundation Model for Nuclear Reactor Control](https://arxiv.org/abs/2512.23292)
*Yoonpyo Lee,Kazuma Kobayashi,Sai Puppala,Sajedul Talukder,Seid Koric,Souvik Chakraborty,Syed Bahauddin Alam*

Main category: cs.AI

TL;DR: The paper introduces a new approach to AI for physical systems, focusing on domain-specific models validated by physical constraints, which exhibit significantly improved performance over general-purpose models.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of existing vision-language models in physics-based control tasks, which show low accuracy and violate physical principles, revealing structural deficiencies in such models.

Method: The authors developed a compact language model for reactor control tasks, trained it on a scaled synthetic dataset, and observed phase transitions in performance indicative of stabilized, efficient behavior.

Result: The model exhibits a 500x reduction in execution variance, autonomously rejects most of the training distribution, concentrating 95% of decisions on optimized strategies, and demonstrates transferable learned representations across different physics and input modalities.

Conclusion: General-purpose models are structurally unsuited for safety-critical physics tasks, and domain-specific AI models with physics-based validation are essential for stabilizing control performance and achieving reliable execution.

Abstract: The prevailing paradigm in AI for physical systems, scaling general-purpose foundation models toward universal multimodal reasoning, confronts a fundamental barrier at the control interface. Recent benchmarks show that even frontier vision-language models achieve only 50-53% accuracy on basic quantitative physics tasks, behaving as approximate guessers that preserve semantic plausibility while violating physical constraints. This input unfaithfulness is not a scaling deficiency but a structural limitation. Perception-centric architectures optimize parameter-space imitation, whereas safety-critical control demands outcome-space guarantees over executed actions. Here, we present a fundamentally different pathway toward domain-specific foundation models by introducing compact language models operating as Agentic Physical AI, in which policy optimization is driven by physics-based validation rather than perceptual inference. We train a 360-million-parameter model on synthetic reactor control scenarios, scaling the dataset from 10^3 to 10^5 examples. This induces a sharp phase transition absent in general-purpose models. Small-scale systems exhibit high-variance imitation with catastrophic tail risk, while large-scale models undergo variance collapse exceeding 500x reduction, stabilizing execution-level behavior. Despite balanced exposure to four actuation families, the model autonomously rejects approximately 70% of the training distribution and concentrates 95% of runtime execution on a single-bank strategy. Learned representations transfer across distinct physics and continuous input modalities without architectural modification.

</details>


### [39] [On Conformant Planning and Model-Checking of $\exists^*\forall^*$ Hyperproperties](https://arxiv.org/abs/2512.23324)
*Raven Beutner,Bernd Finkbeiner*

Main category: cs.AI

TL;DR: This paper explores the connection between conformant planning and model-checking of hyperproperties, showing they can be efficiently reduced to each other.


<details>
  <summary>Details</summary>
Motivation: To bridge the understanding of two domains: conformant planning and model-checking of hyperproperties, and to determine their equivalence.

Method: The paper proposes a reduction from hyperproperty model-checking to conformant planning, proving the encoding's soundness and completeness. Additionally, it establishes that conformant planning problems can be framed as hyperproperty model-checking tasks.

Result: The authors demonstrate an efficient reduction between the two problems and prove their mutual interrelation.

Conclusion: Conformant planning and model-checking of hyperproperties are closely related, with each problem reducible to the other, laying groundwork for cross-domain methodologies.

Abstract: We study the connection of two problems within the planning and verification community: Conformant planning and model-checking of hyperproperties. Conformant planning is the task of finding a sequential plan that achieves a given objective independent of non-deterministic action effects during the plan's execution. Hyperproperties are system properties that relate multiple execution traces of a system and, e.g., capture information-flow and fairness policies. In this paper, we show that model-checking of $\exists^*\forall^*$ hyperproperties is closely related to the problem of computing a conformant plan. Firstly, we show that we can efficiently reduce a hyperproperty model-checking instance to a conformant planning instance, and prove that our encoding is sound and complete. Secondly, we establish the converse direction: Every conformant planning problem is, itself, a hyperproperty model-checking task.

</details>


### [40] [CubeBench: Diagnosing Interactive, Long-Horizon Spatial Reasoning Under Partial Observations](https://arxiv.org/abs/2512.23328)
*Huan-ang Gao,Zikang Zhang,Tianwei Luo,Kaisen Yang,Xinzhe Juan,Jiahao Qiu,Tianxing Chen,Bingxiang He,Hao Zhao,Hao Zhou,Shilong Liu,Mengdi Wang*

Main category: cs.AI

TL;DR: LLM agents struggle with transitioning to physical-world tasks due to deficiencies in spatial reasoning, long-term state tracking, and partial observation-based exploration, addressed in a new benchmark, CubeBench.


<details>
  <summary>Details</summary>
Motivation: Bridge the gap between LLM agents' capabilities in digital environments and applications in physical-world tasks.

Method: Developed CubeBench, a generative benchmark testing LLMs through tasks related to Rubik's Cube, focusing on spatial reasoning and state tracking.

Result: Experiments showed LLMs uniformly failed long-horizon tasks, highlighting key cognitive limitations.

Conclusion: Insights from CubeBench experiments can guide the progression of LLMs towards enhanced physical-world intelligence.

Abstract: Large Language Model (LLM) agents, while proficient in the digital realm, face a significant gap in physical-world deployment due to the challenge of forming and maintaining a robust spatial mental model. We identify three core cognitive challenges hindering this transition: spatial reasoning, long-horizon state tracking via mental simulation, and active exploration under partial observation. To isolate and evaluate these faculties, we introduce CubeBench, a novel generative benchmark centered on the Rubik's Cube. CubeBench uses a three-tiered diagnostic framework that progressively assesses agent capabilities, from foundational state tracking with full symbolic information to active exploration with only partial visual data. Our experiments on leading LLMs reveal critical limitations, including a uniform 0.00% pass rate on all long-horizon tasks, exposing a fundamental failure in long-term planning. We also propose a diagnostic framework to isolate these cognitive bottlenecks by providing external solver tools. By analyzing the failure modes, we provide key insights to guide the development of more physically-grounded intelligent agents.

</details>


### [41] [MindWatcher: Toward Smarter Multimodal Tool-Integrated Reasoning](https://arxiv.org/abs/2512.23412)
*Jiawei Chen,Xintian Shen,Lihao Zheng,Zhenwei Shao,Hongyuan Zhang,Pengfei Yu,Xudong Rao,Ning Mao,Xiaobo Liu,Lian Wen,Chaoqun Du,Feng Gu,Wei He,Qizhen Li,Shanshan Li,Zide Liu,Jing Luo,Lifu Mu,Xuhao Pan,Chang Ren,Haoyi Sun,Qian Wang,Wei Wang,Hongfu Yang,Jiqing Zhan,Chunpeng Zhou,Zheng Zhou,Hao Ma,Tao Wei,Pan Zhou,Wei Chen*

Main category: cs.AI

TL;DR: MindWatcher is a tool-integrated reasoning agent with advanced decision-making capabilities, enhanced by interleaved thinking and multimodal chain-of-thought (CoT) reasoning.


<details>
  <summary>Details</summary>
Motivation: Traditional workflow-based agents fall short in solving complex real-world problems. There is a need for autonomous agents capable of tool invocation and reasoning for multi-step, multimodal tasks.

Method: MindWatcher integrates interleaved thinking and multimodal CoT reasoning, enabling the autonomous use of tools, manipulation of images, and reasoning without human prompts. It includes benchmarking and high-quality datasets for training and evaluation.

Result: Experiments show MindWatcher surpasses the performance of larger models through efficient tool invocation and offers valuable insights for training autonomous agents.

Conclusion: MindWatcher demonstrates the feasibility and advantages of integrating autonomous tool reasoning with efficient training, providing a robust solution for broad-domain multimodal problem-solving.

Abstract: Traditional workflow-based agents exhibit limited intelligence when addressing real-world problems requiring tool invocation. Tool-integrated reasoning (TIR) agents capable of autonomous reasoning and tool invocation are rapidly emerging as a powerful approach for complex decision-making tasks involving multi-step interactions with external environments. In this work, we introduce MindWatcher, a TIR agent integrating interleaved thinking and multimodal chain-of-thought (CoT) reasoning. MindWatcher can autonomously decide whether and how to invoke diverse tools and coordinate their use, without relying on human prompts or workflows. The interleaved thinking paradigm enables the model to switch between thinking and tool calling at any intermediate stage, while its multimodal CoT capability allows manipulation of images during reasoning to yield more precise search results. We implement automated data auditing and evaluation pipelines, complemented by manually curated high-quality datasets for training, and we construct a benchmark, called MindWatcher-Evaluate Bench (MWE-Bench), to evaluate its performance. MindWatcher is equipped with a comprehensive suite of auxiliary reasoning tools, enabling it to address broad-domain multimodal problems. A large-scale, high-quality local image retrieval database, covering eight categories including cars, animals, and plants, endows model with robust object recognition despite its small size. Finally, we design a more efficient training infrastructure for MindWatcher, enhancing training speed and hardware utilization. Experiments not only demonstrate that MindWatcher matches or exceeds the performance of larger or more recent models through superior tool invocation, but also uncover critical insights for agent training, such as the genetic inheritance phenomenon in agentic RL.

</details>


### [42] [The World Is Bigger! A Computationally-Embedded Perspective on the Big World Hypothesis](https://arxiv.org/abs/2512.23419)
*Alex Lewandowski,Adtiya A. Ramesh,Edan Meyer,Dale Schuurmans,Marlos C. Machado*

Main category: cs.AI

TL;DR: The paper introduces a computationally-embedded perspective to model constrained continual learning agents and proposes a new objective called interactivity alongside a model-based RL algorithm to evaluate this behavior.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing explicit agent constraints and offer a natural characterisation of continual learning within the context of being embedded in an environment.

Method: Characterize an agent as a computational automaton embedded in a formal universal computer. Established connections to specialized Markov decision processes and proposed 'interactivity' to assess adaptation capabilities. Developed a reinforcement learning algorithm to evaluate interactivity.

Result: The results highlight that deep linear networks sustain better continual adaptation (higher interactivity) compared to deep nonlinear networks when capacity scales up.

Conclusion: The computationally-embedded agent view and proposed interactivity provide strong foundations for evaluating continual learning and designing more adaptable systems.

Abstract: Continual learning is often motivated by the idea, known as the big world hypothesis, that "the world is bigger" than the agent. Recent problem formulations capture this idea by explicitly constraining an agent relative to the environment. These constraints lead to solutions in which the agent continually adapts to best use its limited capacity, rather than converging to a fixed solution. However, explicit constraints can be ad hoc, difficult to incorporate, and may limit the effectiveness of scaling up the agent's capacity. In this paper, we characterize a problem setting in which an agent, regardless of its capacity, is constrained by being embedded in the environment. In particular, we introduce a computationally-embedded perspective that represents an embedded agent as an automaton simulated within a universal (formal) computer. Such an automaton is always constrained; we prove that it is equivalent to an agent that interacts with a partially observable Markov decision process over a countably infinite state-space. We propose an objective for this setting, which we call interactivity, that measures an agent's ability to continually adapt its behaviour by learning new predictions. We then develop a model-based reinforcement learning algorithm for interactivity-seeking, and use it to construct a synthetic problem to evaluate continual learning capability. Our results show that deep nonlinear networks struggle to sustain interactivity, whereas deep linear networks sustain higher interactivity as capacity increases.

</details>


### [43] [AKG kernel Agent: A Multi-Agent Framework for Cross-Platform Kernel Synthesis](https://arxiv.org/abs/2512.23424)
*Jinye Du,Quan Yuan,Zuyao Zhang,Yanzhi Yi,Jiahui Hu,Wangyi Chen,Yiyang Zhu,Qishui Zheng,Wenxiang Zou,Xiangyu Chang,Zuohe Zheng,Zichun Ye,Chao Liu,Shanni Li,Renwei Zhang,Yiping Deng,Xinwei Hu,Xuefeng Jin,Jie Zhao*

Main category: cs.AI

TL;DR: The paper proposes AKG kernel agent, a multi-agent system for automating kernel generation and optimization for various AI workloads and hardware platforms.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity of AI models and diverse hardware architectures creates challenges for computational kernel development, making manual optimization impractical.

Method: AKG kernel agent utilizes multi-agent automation to generate, migrate, and optimize kernels for multiple domain-specific languages (DSLs) and hardware platforms.

Result: AKG kernel agent achieves an average speedup of 1.46× over PyTorch Eager baseline implementations, demonstrating improved performance on GPU and NPU hardware.

Conclusion: The system effectively accelerates kernel development, addresses the growing computational demands of AI, and supports adaptable and modular integration for various backend technologies.

Abstract: Modern AI models demand high-performance computation kernels. The growing complexity of LLMs, multimodal architectures, and recommendation systems, combined with techniques like sparsity and quantization, creates significant computational challenges. Moreover, frequent hardware updates and diverse chip architectures further complicate this landscape, requiring tailored kernel implementations for each platform. However, manual optimization cannot keep pace with these demands, creating a critical bottleneck in AI system development. Recent advances in LLM code generation capabilities have opened new possibilities for automating kernel development. In this work, we propose AKG kernel agent (AI-driven Kernel Generator), a multi-agent system that automates kernel generation, migration, and performance tuning. AKG kernel agent is designed to support multiple domain-specific languages (DSLs), including Triton, TileLang, CPP, and CUDA-C, enabling it to target different hardware backends while maintaining correctness and portability. The system's modular design allows rapid integration of new DSLs and hardware targets. When evaluated on KernelBench using Triton DSL across GPU and NPU backends, AKG kernel agent achieves an average speedup of 1.46$\times$ over PyTorch Eager baselines implementations, demonstrating its effectiveness in accelerating kernel development for modern AI workloads.

</details>


### [44] [Replay Failures as Successes: Sample-Efficient Reinforcement Learning for Instruction Following](https://arxiv.org/abs/2512.23457)
*Kongcheng Zhang,Qi Yao,Shunyu Liu,Wenjian Zhang,Min Cen,Yang Zhou,Wenkai Fang,Yiru Zhao,Baisheng Lai,Mingli Song*

Main category: cs.AI

TL;DR: The paper introduces Hindsight instruction Replay (HiR), a reinforcement learning framework that efficiently improves instruction-following capabilities of Large Language Models (LLMs) by replaying failed attempts as successes to tackle sparse rewards.


<details>
  <summary>Details</summary>
Motivation: Current reinforcement learning frameworks for aligning LLMs struggle with sparse or indistinguishable rewards, as the initial model often fails to generate high-quality, constraint-satisfying responses.

Method: The paper proposes HiR, which employs a select-then-rewrite strategy that modifies failed responses into successful ones based on fulfilled constraints, enabling dual-preference learning with only binary reward signals.

Result: HiR demonstrates notable improvements in instruction-following tasks with lesser computational requirements, showcasing its efficiency and capability for challenging learning tasks.

Conclusion: HiR offers a sample-efficient and computationally economical RL approach to enhance the ability of LLMs to follow complex instructions, supported by substantial experimental results.

Abstract: Reinforcement Learning (RL) has shown promise for aligning Large Language Models (LLMs) to follow instructions with various constraints. Despite the encouraging results, RL improvement inevitably relies on sampling successful, high-quality responses; however, the initial model often struggles to generate responses that satisfy all constraints due to its limited capabilities, yielding sparse or indistinguishable rewards that impede learning. In this work, we propose Hindsight instruction Replay (HiR), a novel sample-efficient RL framework for complex instruction following tasks, which employs a select-then-rewrite strategy to replay failed attempts as successes based on the constraints that have been satisfied in hindsight. We perform RL on these replayed samples as well as the original ones, theoretically framing the objective as dual-preference learning at both the instruction- and response-level to enable efficient optimization using only a binary reward signal. Extensive experiments demonstrate that the proposed HiR yields promising results across different instruction following tasks, while requiring less computational budget. Our code and dataset is available at https://github.com/sastpg/HIR.

</details>


### [45] [The Gaining Paths to Investment Success: Information-Driven LLM Graph Reasoning for Venture Capital Prediction](https://arxiv.org/abs/2512.23489)
*Haoyu Pei,Zhongyang Liu,Xiangyi Xiao,Xiaocong Du,Haipeng Zhang,Kunpeng Zhang,Suting Hong*

Main category: cs.AI

TL;DR: The paper introduces MIRAGE-VC, a multi-perspective framework designed to improve the prediction of startup success by addressing limitations in traditional machine learning and graph-LLM methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve venture capital investment success rates by enabling accurate predictions of startup outcomes using complex relational data, overcoming the limitations of current machine learning technologies.

Method: MIRAGE-VC combines an information-gain-driven path retriever to handle graph path explosion and a multi-agent architecture to synthesize heterogeneous evidence streams using a learnable gating mechanism tailored to startup attributes.

Result: Under strict controls, MIRAGE-VC improves prediction performance with a +5.0% boost in F1 score and +16.6% in PrecisionAt5, demonstrating its efficiency and potential application in other off-graph tasks.

Conclusion: MIRAGE-VC enriches venture capital decision-making by improving prediction accuracy and interpretability, offering a scalable and effective tool for off-graph prediction challenges.

Abstract: Most venture capital (VC) investments fail, while a few deliver outsized returns. Accurately predicting startup success requires synthesizing complex relational evidence, including company disclosures, investor track records, and investment network structures, through explicit reasoning to form coherent, interpretable investment theses. Traditional machine learning and graph neural networks both lack this reasoning capability. Large language models (LLMs) offer strong reasoning but face a modality mismatch with graphs. Recent graph-LLM methods target in-graph tasks where answers lie within the graph, whereas VC prediction is off-graph: the target exists outside the network. The core challenge is selecting graph paths that maximize predictor performance on an external objective while enabling step-by-step reasoning. We present MIRAGE-VC, a multi-perspective retrieval-augmented generation framework that addresses two obstacles: path explosion (thousands of candidate paths overwhelm LLM context) and heterogeneous evidence fusion (different startups need different analytical emphasis). Our information-gain-driven path retriever iteratively selects high-value neighbors, distilling investment networks into compact chains for explicit reasoning. A multi-agent architecture integrates three evidence streams via a learnable gating mechanism based on company attributes. Under strict anti-leakage controls, MIRAGE-VC achieves +5.0% F1 and +16.6% PrecisionAt5, and sheds light on other off-graph prediction tasks such as recommendation and risk assessment. Code: https://anonymous.4open.science/r/MIRAGE-VC-323F.

</details>


### [46] [Why AI Safety Requires Uncertainty, Incomplete Preferences, and Non-Archimedean Utilities](https://arxiv.org/abs/2512.23508)
*Alessio Benavoli,Alessandro Facchini,Marco Zaffalon*

Main category: cs.AI

TL;DR: This paper studies AI alignment through the AI assistance and shutdown problems, emphasizing an AI's need to reason under uncertainty and manage incomplete preferences.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address how we can design AI systems that stay aligned with human values and respond appropriately, particularly when humans have incomplete or uncertain utility functions.

Method: The paper uses the frameworks of AI assistance (learning a human's utility function) and AI shutdown (ensuring proper responses to shutdown commands) to frame the alignment problems.

Result: The study concludes that solving these issues requires developing AI agents capable of reasoning under uncertainty and managing preferences that are incomplete or non-Archimedean.

Conclusion: Alignment challenges can only be addressed by equipping AI with advanced reasoning abilities to handle human complexity and uncertainty.

Abstract: How can we ensure that AI systems are aligned with human values and remain safe? We can study this problem through the frameworks of the AI assistance and the AI shutdown games. The AI assistance problem concerns designing an AI agent that helps a human to maximise their utility function(s). However, only the human knows these function(s); the AI assistant must learn them. The shutdown problem instead concerns designing AI agents that: shut down when a shutdown button is pressed; neither try to prevent nor cause the pressing of the shutdown button; and otherwise accomplish their task competently. In this paper, we show that addressing these challenges requires AI agents that can reason under uncertainty and handle both incomplete and non-Archimedean preferences.

</details>


### [47] [Divergent-Convergent Thinking in Large Language Models for Creative Problem Generation](https://arxiv.org/abs/2512.23601)
*Manh Hung Nguyen,Adish Singla*

Main category: cs.AI

TL;DR: This paper introduces CreativeDC, a two-phase prompting method for solving the lack of diversity in educational problem generation by large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: LLMs generate homogeneous outputs due to their ``Artificial Hivemind'' effect, limiting creativity and diversity in educational problems provided to students.

Method: CreativeDC explicitly separates the problem generation process into creative exploration and constraint satisfaction phases, leveraging theories of creativity to promote diverse outputs.

Result: CreativeDC demonstrates higher diversity and novelty in generated problems compared to baseline methods, while maintaining their utility. The method scales effectively, creating more distinct problems as sampling increases.

Conclusion: CreativeDC improves the diversity and creativity of LLM-generated educational problems without compromising utility, addressing the challenges posed by repetitive outputs of current models.

Abstract: Large language models (LLMs) have significant potential for generating educational questions and problems, enabling educators to create large-scale learning materials. However, LLMs are fundamentally limited by the ``Artificial Hivemind'' effect, where they generate similar responses within the same model and produce homogeneous outputs across different models. As a consequence, students may be exposed to overly similar and repetitive LLM-generated problems, which harms diversity of thought. Drawing inspiration from Wallas's theory of creativity and Guilford's framework of divergent-convergent thinking, we propose CreativeDC, a two-phase prompting method that explicitly scaffolds the LLM's reasoning into distinct phases. By decoupling creative exploration from constraint satisfaction, our method enables LLMs to explore a broader space of ideas before committing to a final problem. We evaluate CreativeDC for creative problem generation using a comprehensive set of metrics that capture diversity, novelty, and utility. The results show that CreativeDC achieves significantly higher diversity and novelty compared to baselines while maintaining high utility. Moreover, scaling analysis shows that CreativeDC generates a larger effective number of distinct problems as more are sampled, increasing at a faster rate than baseline methods.

</details>


### [48] [Physics-Informed Neural Networks for Device and Circuit Modeling: A Case Study of NeuroSPICE](https://arxiv.org/abs/2512.23624)
*Chien-Ting Tung,Chenming Hu*

Main category: cs.AI

TL;DR: NeuroSPICE uses physics-informed neural networks (PINNs) for circuit simulation, offering unique advantages over conventional SPICE despite not being faster or more accurate during training.


<details>
  <summary>Details</summary>
Motivation: The paper aims to develop a novel framework using PINNs to address challenges in simulating emerging advanced devices and performing design optimization and inverse problems.

Method: The framework uses PINNs to replace time-discretized numerical solvers, solving circuit differential-algebraic equations (DAEs) by minimizing residuals through backpropagation.

Result: NeuroSPICE proves capable of modeling device and circuit waveforms in the time domain with exact temporal derivatives, enabling simulations of complex systems like ferroelectric memories.

Conclusion: While NeuroSPICE does not outperform traditional SPICE in terms of speed or accuracy, its flexibility and analytical modeling in the time domain provide advantages for applications in emerging technologies and optimization tasks.

Abstract: We present NeuroSPICE, a physics-informed neural network (PINN) framework for device and circuit simulation. Unlike conventional SPICE, which relies on time-discretized numerical solvers, NeuroSPICE leverages PINNs to solve circuit differential-algebraic equations (DAEs) by minimizing the residual of the equations through backpropagation. It models device and circuit waveforms using analytical equations in time domain with exact temporal derivatives. While PINNs do not outperform SPICE in speed or accuracy during training, they offer unique advantages such as surrogate models for design optimization and inverse problems. NeuroSPICE's flexibility enables the simulation of emerging devices, including highly nonlinear systems such as ferroelectric memories.

</details>


### [49] [Regret-Based Federated Causal Discovery with Unknown Interventions](https://arxiv.org/abs/2512.23626)
*Federico Baldo,Charles K. Assaad*

Main category: cs.AI

TL;DR: This paper presents I-PERI, an algorithm for federated causal discovery under unknown client-specific interventions, producing tighter equivalence classes called Φ-CPDAGs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to adapt causal discovery methods to federated settings where decentralized data and privacy constraints exist. The paper addresses the challenge of client-specific unknown interventions that make traditional assumptions of shared causal models unrealistic.

Method: The paper introduces I-PERI, a federated algorithm that recovers a CPDAG representing the union of client graphs and orients edges using structural differences from interventions. This results in Φ-CPDAGs, a tighter equivalence class.

Result: The algorithm provides theoretical guarantees on convergence and privacy, and empirical evaluations on synthetic data showcase its effectiveness.

Conclusion: I-PERI effectively conducts federated causal discovery accounting for unknown interventions, improving equivalence class representation while maintaining privacy.

Abstract: Most causal discovery methods recover a completed partially directed acyclic graph representing a Markov equivalence class from observational data. Recent work has extended these methods to federated settings to address data decentralization and privacy constraints, but often under idealized assumptions that all clients share the same causal model. Such assumptions are unrealistic in practice, as client-specific policies or protocols, for example, across hospitals, naturally induce heterogeneous and unknown interventions. In this work, we address federated causal discovery under unknown client-level interventions. We propose I-PERI, a novel federated algorithm that first recovers the CPDAG of the union of client graphs and then orients additional edges by exploiting structural differences induced by interventions across clients. This yields a tighter equivalence class, which we call the $\mathbfΦ$-Markov Equivalence Class, represented by the $\mathbfΦ$-CPDAG. We provide theoretical guarantees on the convergence of I-PERI, as well as on its privacy-preserving properties, and present empirical evaluations on synthetic data demonstrating the effectiveness of the proposed algorithm.

</details>


### [50] [Web World Models](https://arxiv.org/abs/2512.23676)
*Jichen Feng,Yifan Zhang,Chenggong Zhang,Yifu Lu,Shilong Liu,Mengdi Wang*

Main category: cs.AI

TL;DR: This paper introduces Web World Models (WWM), a balanced approach to creating environments for language agents using web-based systems instead of fully generative or strictly database-defined contexts.


<details>
  <summary>Details</summary>
Motivation: To address the need for persistent, logical, and scalable environments for language agents that blend control with open-ended exploration, overcoming the limitations of web frameworks and generative world models.

Method: WWM leverages web-based code to define structured latent states and deterministic rules, while using large language models to generate narratives and higher-level decision-making, achieving a middle ground between fixed and unlimited environments.

Result: A suite of WWMs was built deploying real-world and fictional scenarios, showing that web stacks can offer scalable and controllable world models. Practical principles for designing these models were identified.

Conclusion: The study demonstrates that web-based systems can effectively create structured yet open-ended environments for language agents, balancing logical consistency and creative flexibility.

Abstract: Language agents increasingly require persistent worlds in which they can act, remember, and learn. Existing approaches sit at two extremes: conventional web frameworks provide reliable but fixed contexts backed by databases, while fully generative world models aim for unlimited environments at the expense of controllability and practical engineering. In this work, we introduce the Web World Model (WWM), a middle ground where world state and ``physics'' are implemented in ordinary web code to ensure logical consistency, while large language models generate context, narratives, and high-level decisions on top of this structured latent state. We build a suite of WWMs on a realistic web stack, including an infinite travel atlas grounded in real geography, fictional galaxy explorers, web-scale encyclopedic and narrative worlds, and simulation- and game-like environments. Across these systems, we identify practical design principles for WWMs: separating code-defined rules from model-driven imagination, representing latent state as typed web interfaces, and utilizing deterministic generation to achieve unlimited but structured exploration. Our results suggest that web stacks themselves can serve as a scalable substrate for world models, enabling controllable yet open-ended environments. Project Page: https://github.com/Princeton-AI2-Lab/Web-World-Models.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [51] [An Energy-Efficient RFET-Based Stochastic Computing Neural Network Accelerator](https://arxiv.org/abs/2512.22131)
*Sheng Lu,Qianhou Qu,Sungyong Jung,Qilian Liang,Chenyun Pan*

Main category: cs.AR

TL;DR: This paper proposes an SCNN architecture using RFETs, which reduces hardware resource usage and improves efficiency in area, latency, and energy compared to FinFETs.


<details>
  <summary>Details</summary>
Motivation: Traditional stochastic computing CNNs face challenges in high resource usage mainly because of components like SNGs and APCs, which limit performance. The paper aims to address these limitations.

Method: The study introduces an SCNN architecture using RFETs to design more efficient core components. A dedicated SCNN accelerator is developed for simulations, leveraging RFET reconfigurability.

Result: Compared to a FinFET-based design, the RFET-based SCNN accelerator demonstrates significant reductions in area, latency, and energy consumption.

Conclusion: The proposed RFET-based SCNN architecture offers a promising solution for efficient and compact hardware designs in stochastic neural network applications.

Abstract: Stochastic computing (SC) offers significant reductions in hardware complexity for traditional convolutional neural networks (CNNs), but stochastic computing neural networks (SCNNs) still suffer from high resource usage due to components such as stochastic number generators (SNGs) and accumulative parallel counters (APCs), which limit performance. This paper introduces a novel SCNN architecture based on reconfigurable field-effect transistors (RFETs), whose device-level reconfigurability enables the design of highly efficient and compact SNGs, APCs, and other core modules. A dedicated SCNN accelerator architecture is also developed for system-level simulation. Using publicly available open-source standard cell libraries, experimental results show that the proposed RFET-based SCNN accelerator achieves substantial reductions in area, latency, and energy consumption compared to a FinFET-based design at the same technology node.

</details>


### [52] [AnalogSAGE: Self-evolving Analog Design Multi-Agents with Stratified Memory and Grounded Experience](https://arxiv.org/abs/2512.22435)
*Zining Wang,Jian Gao,Weimin Fu,Xiaolong Guo,Xuan Zhang*

Main category: cs.AR

TL;DR: The paper introduces AnalogSAGE, a multi-agent framework for analog circuit design that uses a self-evolving structure and grounded reasoning, significantly improving design accuracy and efficiency compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Current methods for analog circuit design rely heavily on human intuition and are limited by prompt-driven or template-based approaches, which are insufficient for meeting complex specifications.

Method: The proposed framework, AnalogSAGE, employs a self-evolving multi-agent approach across three stages and four stratified memory layers for iterative refinement, guided by simulation-grounded feedback.

Result: AnalogSAGE, when benchmarked, achieved a 10x overall pass rate, a 48x improvement in Pass@1, and a 4x reduction in the parameter search space compared to existing frameworks, under the analogous conditions of the SKY130 PDK with ngspice.

Conclusion: AnalogSAGE enhances the reliability and automation of analog circuit design, providing a reproducible, open-source solution that significantly outperforms current methodologies.

Abstract: Analog circuit design remains a knowledge- and experience-intensive process that relies heavily on human intuition for topology generation and device parameter tuning. Existing LLM-based approaches typically depend on prompt-driven netlist generation or predefined topology templates, limiting their ability to satisfy complex specification requirements. We propose AnalogSAGE, an open-source self-evolving multi-agent framework that coordinates three-stage agent explorations through four stratified memory layers, enabling iterative refinement with simulation-grounded feedback. To support reproducibility and generality, we release the source code. Our benchmark spans ten specification-driven operational amplifier design problems of varying difficulty, enabling quantitative and cross-task comparison under identical conditions. Evaluated under the open-source SKY130 PDK with ngspice, AnalogSAGE achieves a 10$\times$ overall pass rate, a 48$\times$ Pass@1, and a 4$\times$ reduction in parameter search space compared with existing frameworks, demonstrating that stratified memory and grounded reasoning substantially enhance the reliability and autonomy of analog design automation in practice.

</details>


### [53] [TYTAN: Taylor-series based Non-Linear Activation Engine for Deep Learning Accelerators](https://arxiv.org/abs/2512.23062)
*Soham Pramanik,Vimal William,Arnab Raha,Debayan Das,Amitava Mukherjee,Janet L. Paluh*

Main category: cs.AR

TL;DR: This paper introduces TYTAN, a domain-specific architecture that accelerates AI inference by minimizing power consumption and enhancing energy efficiency, particularly for edge computing.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the growing need for energy-efficient and resource-conscious AI architectures for edge computing, addressing issues like computational cost and energy consumption of intensive AI operations.

Method: A novel system called TYTAN is proposed, which combines a reconfigurable hardware design with a Taylor-series-based algorithm to optimize non-linear activation functions. It dynamically estimates approximations to maintain minimal accuracy deviation.

Result: TYTAN demonstrated significant improvements, achieving an operational clock frequency of over 950 MHz, approximately 2x performance improvement, 56% power reduction, and 35 times lower area compared to the baseline NVDLA implementation.

Conclusion: The results validate TYTAN as an effective solution for fast, energy-efficient AI computation at the edge, emphasizing the importance of domain-specific hardware and algorithmic co-optimization.

Abstract: The rapid advancement in AI architectures and the proliferation of AI-enabled systems have intensified the need for domain-specific architectures that enhance both the acceleration and energy efficiency of AI inference, particularly at the edge. This need arises from the significant resource constraints-such as computational cost and energy consumption-associated with deploying AI algorithms, which involve intensive mathematical operations across multiple layers. High-power-consuming operations, including General Matrix Multiplications (GEMMs) and activation functions, can be optimized to address these challenges. Optimization strategies for AI at the edge include algorithmic approaches like quantization and pruning, as well as hardware methodologies such as domain-specific accelerators. This paper proposes TYTAN: TaYlor-series based non-linear acTivAtion eNgine, which explores the development of a Generalized Non-linear Approximation Engine (G-NAE). TYTAN targets the acceleration of non-linear activation functions while minimizing power consumption. The TYTAN integrates a re-configurable hardware design with a specialized algorithm that dynamically estimates the necessary approximation for each activation function, aimed at achieving minimal deviation from baseline accuracy. The proposed system is validated through performance evaluations with state-of-the-art AI architectures, including Convolutional Neural Networks (CNNs) and Transformers. Results from system-level simulations using Silvaco's FreePDK45 process node demonstrate TYTAN's capability to operate at a clock frequency >950 MHz, showcasing its effectiveness in supporting accelerated, energy-efficient AI inference at the edge, which is ~2 times performance improvement, with ~56% power reduction and ~35 times lower area compared to the baseline open-source NVIDIA Deep Learning Accelerator (NVDLA) implementation.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [54] [Open-Source Multimodal Moxin Models with Moxin-VLM and Moxin-VLA](https://arxiv.org/abs/2512.22208)
*Pu Zhao,Xuan Shen,Zhenglun Kong,Yixin Shen,Sung-En Chang,Arash Akbari,Timothy Rupprecht,Lei Lu,Enfu Nan,Changdi Yang,Yumei He,Weiyan Shi,Xingchen Xu,Yu Huang,Wei Jiang,Wei Wang,Yue Chen,Yong He,Yanzhi Wang*

Main category: cs.CL

TL;DR: Moxin 7B, an open-source LLM adhering to the Model Openness Framework, is introduced alongside three variants for task-specific capabilities, achieving strong performance and fostering a collaborative research environment.


<details>
  <summary>Details</summary>
Motivation: The need for transparency, inclusivity, and collaboration within the LLM research community to create versatile models and sustain a healthy open-source ecosystem.

Method: Developing Moxin 7B based on the Model Openness Framework with fully open training processes, datasets, and implementation details; creating three variants (Moxin-VLM, Moxin-VLA, Moxin-Chinese) for specialized tasks.

Result: Moxin 7B and its variants demonstrate superior performance across multiple tasks, supported by experimental evaluations.

Conclusion: The release of Moxin 7B and its variants, along with their data and code, demonstrates commitment to fostering an open and collaborative LLM research ecosystem, advancing the capabilities of open-source models.

Abstract: Recently, Large Language Models (LLMs) have undergone a significant transformation, marked by a rapid rise in both their popularity and capabilities. Leading this evolution are proprietary LLMs like GPT-4 and GPT-o1, which have captured widespread attention in the AI community due to their remarkable performance and versatility. Simultaneously, open-source LLMs, such as LLaMA and Mistral, have made great contributions to the ever-increasing popularity of LLMs due to the ease to customize and deploy the models across diverse applications. Moxin 7B is introduced as a fully open-source LLM developed in accordance with the Model Openness Framework, which moves beyond the simple sharing of model weights to embrace complete transparency in training, datasets, and implementation detail, thus fostering a more inclusive and collaborative research environment that can sustain a healthy open-source ecosystem. To further equip Moxin with various capabilities in different tasks, we develop three variants based on Moxin, including Moxin-VLM, Moxin-VLA, and Moxin-Chinese, which target the vision-language, vision-language-action, and Chinese capabilities, respectively. Experiments show that our models achieve superior performance in various evaluations. We adopt open-source framework and open data for the training. We release our models, along with the available data and code to derive these models.

</details>


### [55] [Hierarchical Geometry of Cognitive States in Transformer Embedding Spaces](https://arxiv.org/abs/2512.22227)
*Sophie Zhao*

Main category: cs.CL

TL;DR: The study explores if transformer-based sentence embeddings exhibit hierarchical cognitive structures and shows they encode such structures, which are decodable through shallow probes.


<details>
  <summary>Details</summary>
Motivation: The study aims to investigate the underexplored area of higher-level cognitive organization and hierarchical structure present in transformer-based language model embeddings.

Method: A dataset of 480 sentences labeled with energy scores and cognitive category tiers was analyzed using fixed embeddings from transformer models. Probing techniques (linear and shallow nonlinear) were used to evaluate the decodability of these annotations.

Result: Transformer embeddings were shown to encode a hierarchical structure aligned with cognitive attributes, with shallow nonlinear probes outperforming linear ones. TF-IDF baselines performed poorly in comparison, and visualizations displayed smooth gradients in embedding spaces.

Conclusion: Transformer model embeddings have hierarchical geometric organization corresponding to cognitive structures, though no claims are made about internal awareness or phenomenology.

Abstract: Recent work has shown that transformer-based language models learn rich geometric structure in their embedding spaces, yet the presence of higher-level cognitive organization within these representations remains underexplored. In this work, we investigate whether sentence embeddings encode a graded, hierarchical structure aligned with human-interpretable cognitive or psychological attributes. We construct a dataset of 480 natural-language sentences annotated with continuous ordinal energy scores and discrete tier labels spanning seven ordered cognitive categories. Using fixed sentence embeddings from multiple transformer models, we evaluate the recoverability of these annotations via linear and shallow nonlinear probes. Across models, both continuous scores and tier labels are reliably decodable, with shallow nonlinear probes providing consistent performance gains over linear probes. Lexical TF-IDF baselines perform substantially worse, indicating that the observed structure is not attributable to surface word statistics alone. Nonparametric permutation tests further confirm that probe performance exceeds chance under label-randomization nulls. Qualitative analyses using UMAP visualizations and confusion matrices reveal smooth low-to-high gradients and predominantly adjacent-tier confusions in embedding space. Taken together, these results provide evidence that transformer embedding spaces exhibit a hierarchical geometric organization aligned with human-defined cognitive attributes, while remaining agnostic to claims of internal awareness or phenomenology.

</details>


### [56] [SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents](https://arxiv.org/abs/2512.22322)
*Shaofei Cai,Yulei Qin,Haojia Lin,Zihan Xu,Gang Li,Yuchen Shi,Zongyi Li,Yong Mao,Siqi Cai,Xiaoyu Tan,Yitao Liang,Ke Li,Xing Sun*

Main category: cs.CL

TL;DR: This paper introduces SmartSnap, a proactive self-verification method for reinforcement learning agents to address scalability issues by providing curated snapshot evidence instead of analyzing full interaction trajectories.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the scalability and reliability issues in agentic reinforcement learning caused by the passive and post-hoc nature of existing task verification methods, which rely on analyzing verbose and noisy historical data.

Method: The authors propose the SmartSnap framework, where agents perform in-situ self-verification using minimal and decisive snapshots. Guided by 3C principles (Completeness, Conciseness, Creativity), agents provide these snapshots to an LLM-based verifier for task assessment.

Result: SmartSnap achieves up to 26.08% and 16.66% performance gains respectively for 8B and 30B LLM-driven agents. It shows competitive performance against advanced benchmarks like DeepSeek V3.1 and Qwen3-235B-A22B.

Conclusion: The SmartSnap paradigm enables scalable reinforcement learning by integrating solution finding with evidence seeking, fostering efficient and self-verifying agents.

Abstract: Agentic reinforcement learning (RL) holds great promise for the development of autonomous agents under complex GUI tasks, but its scalability remains severely hampered by the verification of task completion. Existing task verification is treated as a passive, post-hoc process: a verifier (i.e., rule-based scoring script, reward or critic model, and LLM-as-a-Judge) analyzes the agent's entire interaction trajectory to determine if the agent succeeds. Such processing of verbose context that contains irrelevant, noisy history poses challenges to the verification protocols and therefore leads to prohibitive cost and low reliability. To overcome this bottleneck, we propose SmartSnap, a paradigm shift from this passive, post-hoc verification to proactive, in-situ self-verification by the agent itself. We introduce the Self-Verifying Agent, a new type of agent designed with dual missions: to not only complete a task but also to prove its accomplishment with curated snapshot evidences. Guided by our proposed 3C Principles (Completeness, Conciseness, and Creativity), the agent leverages its accessibility to the online environment to perform self-verification on a minimal, decisive set of snapshots. Such evidences are provided as the sole materials for a general LLM-as-a-Judge verifier to determine their validity and relevance. Experiments on mobile tasks across model families and scales demonstrate that our SmartSnap paradigm allows training LLM-driven agents in a scalable manner, bringing performance gains up to 26.08% and 16.66% respectively to 8B and 30B models. The synergizing between solution finding and evidence seeking facilitates the cultivation of efficient, self-verifying agents with competitive performance against DeepSeek V3.1 and Qwen3-235B-A22B.

</details>


### [57] [The Syntax of qulk-clauses in Yemeni Ibbi Arabic: A Minimalist Approach](https://arxiv.org/abs/2512.22376)
*Zubaida Mohammed Albadani,Mohammed Q. Shormani*

Main category: cs.CL

TL;DR: The paper examines the syntax of Yemeni Ibbi Arabic qulk-clauses through the Minimalist Program, proposing these as biclausal structures and analyzing their syntactic layering with dialect-specific features.


<details>
  <summary>Details</summary>
Motivation: To investigate the syntactic behavior and layered structure of qulk-clauses in Yemeni Ibbi Arabic and contribute to generative syntax, especially minimalism.

Method: Utilizes the Minimalist Program framework applying operations like Merge, Move, Agree, Spell-out, and Morphological Merger to analyze qulk-clauses.

Result: Reveals qulk-clauses as biclausal structures governed by standard computational and post-syntactic processes, including the dialect’s unique features, such as bipartite negation and cliticization.

Conclusion: The study raises theoretical questions about extending the analysis to kil-k 'you said' clauses and provides insights on minimalism's universality.

Abstract: This study investigates the syntax of qulk-clauses in Yemeni Ibbi Arabic (YIA) within the Minimalist Program. The construction qulk-clause, a morphologically fused form meaning 'I said,' introduces embedded declarative interrogative, and imperative clauses, often eithout complementizer. The central proposal of this paper is that qulk-clauses are biclausal structures in which qulk functions a clause-embedding predicate sec;ecting a dull CP complement. By applying core minimalist operations, viz., Merge, Move, Agree, and Spell-out, the study provides a layered syntactic analysis of qulk-clauses, for illustrating how their derivation proceeds through standard computational steps and post-syntactic processes such as Morphological Merger. The proposal also accounts for dialect-specific features like bipartite negation, cliticization, and CP embedding. The findings offer theoretical contributions to generative syntax, specifically minimalism. The study concludes raising theoretical questions concerning extending the analysis to the addressee-clause kil-k 'you said'. It also provides insights into the possibility of the universality of minimalism.

</details>


### [58] [Towards Efficient Post-Training via Fourier-Driven Adapter Architectures](https://arxiv.org/abs/2512.22378)
*Donggyun Bae,Jongil Park*

Main category: cs.CL

TL;DR: The paper introduces FAA, a new framework for efficient fine-tuning of language models using Fourier features, achieving strong performance with low computational cost.


<details>
  <summary>Details</summary>
Motivation: Large language models require efficient methods to adapt to diverse tasks without consuming extensive computational and memory resources. Current fine-tuning techniques often underutilize frequency representation.

Method: The FAA framework integrates random Fourier features into lightweight adapter modules. It decomposes representations into low- and high-frequency components to enable frequency-aware semantic modulation while keeping the language model backbone frozen.

Result: FAA demonstrates superior or competitive results on several benchmarks like GLUE, E2E NLG, and instruction-tuning while maintaining efficiency in terms of computation and memory.

Conclusion: FAA provides an effective and efficient method for tuning large language models by leveraging frequency-aware mechanisms, validating its robustness and scalability through experiments.

Abstract: We propose a novel framework, termed Fourier-Activated Adapter (FAA), for parameter-efficient fine-tuning of large pre-trained language models. By incorporating random Fourier features into lightweight adapter modules, FAA decomposes intermediate representations into complementary low- and high-frequency components, enabling frequency-aware modulation of semantic information. This design allows the model to selectively emphasize informative frequency bands during adaptation while preserving the representational capacity of the frozen backbone. Extensive experiments on GLUE, E2E NLG, and instruction-tuning benchmarks demonstrate that FAA consistently achieves competitive or superior performance compared to existing parameter-efficient fine-tuning methods, while maintaining low computational and memory overhead. Ablation studies further verify the effectiveness of frequency-aware activation and adaptive weighting mechanisms, highlighting FAA as a robust and efficient approach for post-training large language models.

</details>


### [59] [LLM-Guided Exemplar Selection for Few-Shot Wearable-Sensor Human Activity Recognition](https://arxiv.org/abs/2512.22385)
*Elsen Ronando,Sozo Inoue*

Main category: cs.CL

TL;DR: The paper introduces an LLM-Guided Exemplar Selection framework for Human Activity Recognition (HAR) to improve the selection of sensor activity exemplars using semantic reasoning combined with structural and geometric cues.


<details>
  <summary>Details</summary>
Motivation: Current HAR methods rely heavily on large labeled datasets and basic geometric selection methods, which fail to effectively discriminate between closely related wearable sensor activities, such as walking variations.

Method: The framework utilizes an LLM-generated knowledge prior for semantic reasoning, feature importance, and inter-class confusability. It combines this with margin-based validation cues, PageRank centrality, hubness penalization, and facility-location optimization for improved exemplar selection.

Result: The proposed method achieves a macro F1-score of 88.78% on the UCI-HAR dataset in strict few-shot conditions, surpassing traditional approaches like random sampling, herding, and $k$-center.

Conclusion: Integrating LLM-derived semantic priors with geometric and structural cues enhances exemplar selection accuracy, demonstrating their utility in few-shot wearable-sensor HAR.

Abstract: In this paper, we propose an LLM-Guided Exemplar Selection framework to address a key limitation in state-of-the-art Human Activity Recognition (HAR) methods: their reliance on large labeled datasets and purely geometric exemplar selection, which often fail to distinguish similar weara-ble sensor activities such as walking, walking upstairs, and walking downstairs. Our method incorporates semantic reasoning via an LLM-generated knowledge prior that captures feature importance, inter-class confusability, and exemplar budget multipliers, and uses it to guide exemplar scoring and selection. These priors are combined with margin-based validation cues, PageRank centrality, hubness penalization, and facility-location optimization to obtain a compact and informative set of exemplars. Evaluated on the UCI-HAR dataset under strict few-shot conditions, the framework achieves a macro F1-score of 88.78%, outperforming classical approaches such as random sampling, herding, and $k$-center. The results show that LLM-derived semantic priors, when integrated with structural and geometric cues, provide a stronger foundation for selecting representative sensor exemplars in few-shot wearable-sensor HAR.

</details>


### [60] [Hallucination Detection and Evaluation of Large Language Model](https://arxiv.org/abs/2512.22416)
*Chenggong Zhang,Haopeng Wang*

Main category: cs.CL

TL;DR: The paper introduces the Hughes Hallucination Evaluation Model (HHEM), a lightweight framework to detect hallucinations in LLMs efficiently, reducing evaluation time from 8 hours to 10 minutes.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in LLMs undermine reliability and trust in their generated content, necessitating efficient detection methods.

Method: The authors developed HHEM, a classification-based framework independent of LLM judgments, and enhanced it with segment-based retrieval for improved detection in localized hallucinations.

Result: HHEM dramatically reduces evaluation time and achieves an accuracy of 82.2% and True Positive Rate of 78.9%. Larger LLMs exhibit fewer hallucinations, while intermediate-sized models show more instability.

Conclusion: Efficient and structured evaluation frameworks like HHEM are crucial for improving factual reliability in LLM content, though challenges remain with localized hallucinations.

Abstract: Hallucinations in Large Language Models (LLMs) pose a significant challenge, generating misleading or unverifiable content that undermines trust and reliability. Existing evaluation methods, such as KnowHalu, employ multi-stage verification but suffer from high computational costs. To address this, we integrate the Hughes Hallucination Evaluation Model (HHEM), a lightweight classification-based framework that operates independently of LLM-based judgments, significantly improving efficiency while maintaining high detection accuracy. We conduct a comparative analysis of hallucination detection methods across various LLMs, evaluating True Positive Rate (TPR), True Negative Rate (TNR), and Accuracy on question-answering (QA) and summarization tasks. Our results show that HHEM reduces evaluation time from 8 hours to 10 minutes, while HHEM with non-fabrication checking achieves the highest accuracy \(82.2\%\) and TPR \(78.9\%\). However, HHEM struggles with localized hallucinations in summarization tasks. To address this, we introduce segment-based retrieval, improving detection by verifying smaller text components. Additionally, our cumulative distribution function (CDF) analysis indicates that larger models (7B-9B parameters) generally exhibit fewer hallucinations, while intermediate-sized models show higher instability. These findings highlight the need for structured evaluation frameworks that balance computational efficiency with robust factual validation, enhancing the reliability of LLM-generated content.

</details>


### [61] [HiFi-RAG: Hierarchical Content Filtering and Two-Pass Generation for Open-Domain RAG](https://arxiv.org/abs/2512.22442)
*Cattalyya Nuengsigkapian*

Main category: cs.CL

TL;DR: HiFi-RAG enhances RAG systems with hierarchical filtering and optimized processing, achieving superior metrics in the MMU-RAGent competition.


<details>
  <summary>Details</summary>
Motivation: Address challenges in handling irrelevant retrieved information and aligning generated answers with user intent in open-domain generation.

Method: Utilize a multi-stage pipeline combining Gemini 2.5 Flash for filtering and query efficiency, along with Gemini 2.5 Pro for reasoning and final generation.

Result: Achieved significantly better ROUGE-L (+19.6%) and DeBERTaScore (+6.2%) on validation and superior performance in Test2025 dataset with post-cutoff knowledge.

Conclusion: HiFi-RAG demonstrates improved cost-efficient retrieval and robust answer generation, showcasing potential in competitive open-domain Q&A tasks.

Abstract: Retrieval-Augmented Generation (RAG) in open-domain settings faces significant challenges regarding irrelevant information in retrieved documents and the alignment of generated answers with user intent. We present HiFi-RAG (Hierarchical Filtering RAG), the winning closed-source system in the Text-to-Text static evaluation of the MMU-RAGent NeurIPS 2025 Competition. Our approach moves beyond standard embedding-based retrieval via a multi-stage pipeline. We leverage the speed and cost-efficiency of Gemini 2.5 Flash (4-6x cheaper than Pro) for query formulation, hierarchical content filtering, and citation attribution, while reserving the reasoning capabilities of Gemini 2.5 Pro for final answer generation. On the MMU-RAGent validation set, our system outperformed the baseline, improving ROUGE-L to 0.274 (+19.6%) and DeBERTaScore to 0.677 (+6.2%). On Test2025, our custom dataset evaluating questions that require post-cutoff knowledge (post January 2025), HiFi-RAG outperforms the parametric baseline by 57.4% in ROUGE-L and 14.9% in DeBERTaScore.

</details>


### [62] [Exploring the Vertical-Domain Reasoning Capabilities of Large Language Models](https://arxiv.org/abs/2512.22443)
*Jie Zhou,Xin Chen,Jie Zhang,Zhe Li*

Main category: cs.CL

TL;DR: The paper evaluates the accounting reasoning capabilities of Large Language Models (LLMs) by introducing vertical-domain accounting reasoning and establishing related benchmarks. It finds GPT-4 to be the most capable but notes further work is needed for enterprise applications.


<details>
  <summary>Details</summary>
Motivation: To understand and improve Large Language Models' (LLMs) domain-specific reasoning capabilities for effective integration into the field of accounting, promoting the digital transformation of enterprises.

Method: Introduced vertical-domain accounting reasoning, developed evaluation criteria based on the GLM-series training data, and evaluated models like GLM-6B, GLM-130B, GLM-4, and OpenAI GPT-4 on accounting tasks using various prompt engineering strategies.

Result: GPT-4 demonstrated the strongest accounting reasoning capabilities, while performances varied across models with different prompt engineering strategies. However, the current models are insufficient for real-world enterprise applications.

Conclusion: While GPT-4 excels in accounting reasoning, further optimization and development are necessary for deploying LLMs in enterprise-level accounting scenarios to unleash their full potential.

Abstract: Large Language Models (LLMs) are reshaping learning paradigms, cognitive processes, and research methodologies across a wide range of domains. Integrating LLMs with professional fields and redefining the relationship between LLMs and domain-specific applications has become a critical challenge for promoting enterprise digital transformation and broader social development. To effectively integrate LLMs into the accounting domain, it is essential to understand their domain-specific reasoning capabilities. This study introduces the concept of vertical-domain accounting reasoning and establishes evaluation criteria by analyzing the training data characteristics of representative GLM-series models. These criteria provide a foundation for subsequent research on reasoning paradigms and offer benchmarks for improving accounting reasoning performance. Based on this framework, we evaluate several representative models, including GLM-6B, GLM-130B, GLM-4, and OpenAI GPT-4, on a set of accounting reasoning tasks. Experimental results show that different prompt engineering strategies lead to varying degrees of performance improvement across models, with GPT-4 achieving the strongest accounting reasoning capability. However, current LLMs still fall short of real-world application requirements. In particular, further optimization is needed for deployment in enterprise-level accounting scenarios to fully realize the potential value of LLMs in this domain.

</details>


### [63] [Constituency Structure over Eojeol in Korean Treebanks](https://arxiv.org/abs/2512.22487)
*Jungyeul Park,Chulwoo Park*

Main category: cs.CL

TL;DR: This paper proposes an eojeol-based constituency representation for Korean treebanks, separating morphological segmentation into a non-constituent layer, addressing mismatches in current treebank designs.


<details>
  <summary>Details</summary>
Motivation: To address the representational issues in Korean treebanks, particularly the problem of conflating word-internal morphology with phrase-level syntax and mismatches with existing dependency resources.

Method: The paper argues for eojeol-based constituency representation, conducts a comparative analysis of Sejong and Penn Korean treebanks, and introduces an annotation scheme supporting interpretable constituency and cross-treebank comparisons.

Result: It demonstrates that the Sejong and Penn Korean treebanks can be normalized and treated as equivalent at an eojeol-based constituency representation level, providing a consistent framework.

Conclusion: Eojeol-based treebank representation resolves fundamental design issues in Korean constituency treebanks, enabling better syntactic representation and resource compatibility.

Abstract: The design of Korean constituency treebanks raises a fundamental representational question concerning the choice of terminal units. Although Korean words are morphologically complex, treating morphemes as constituency terminals conflates word internal morphology with phrase level syntactic structure and creates mismatches with eojeol based dependency resources. This paper argues for an eojeol based constituency representation, with morphological segmentation and fine grained part of speech information encoded in a separate, non constituent layer. A comparative analysis shows that, under explicit normalization assumptions, the Sejong and Penn Korean treebanks can be treated as representationally equivalent at the eojeol based constituency level. Building on this result, we outline an eojeol based annotation scheme that preserves interpretable constituency and supports cross treebank comparison and constituency dependency conversion.

</details>


### [64] [Anka: A Domain-Specific Language for Reliable LLM Code Generation](https://arxiv.org/abs/2512.23214)
*Saif Khalfan Saif Al Mazrouei*

Main category: cs.CL

TL;DR: The paper introduces Anka, a domain-specific language (DSL) designed for data transformation pipelines, which outperforms general-purpose languages like Python for multi-step tasks in code generation by reducing errors.


<details>
  <summary>Details</summary>
Motivation: The study aims to address systematic errors in complex programming tasks by large language models (LLMs), hypothesizing these stem from the flexibility and ambiguity of general-purpose languages.

Method: The researchers developed Anka, a DSL with constrained and explicit syntax, to test the hypothesis and evaluated its impact on performance through benchmark problems involving models like Claude 3.5 Haiku and GPT-4o-mini.

Result: Anka achieved superior accuracy, with Claude 3.5 attaining 99.9% parse success and 95.8% overall task accuracy on benchmark tasks, outperforming Python significantly in multi-step pipeline accuracy (100% vs. 60%). Cross-model validation confirmed similar improvements.

Conclusion: Constrained DSLs like Anka not only reduce code generation errors in complex tasks but also demonstrate LLMs' ability to adopt novel languages from context. Purposefully-designed DSLs can outperform general-purpose languages tailored to LLM training.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in code generation, yet they exhibit systematic errors on complex, multi-step programming tasks. We hypothesize that these errors stem from the flexibility of general-purpose languages, which permits multiple valid approaches and requires implicit state management. To test this hypothesis, we introduce Anka, a domain-specific language (DSL) for data transformation pipelines designed with explicit, constrained syntax that reduces ambiguity in code generation. Despite having zero prior training exposure to Anka, Claude 3.5 Haiku achieves 99.9% parse success and 95.8% overall task accuracy across 100 benchmark problems. Critically, Anka demonstrates a 40 percentage point accuracy advantage over Python on multi-step pipeline tasks (100% vs. 60%), where Python's flexible syntax leads to frequent errors in operation sequencing and variable management. Cross-model validation with GPT-4o-mini confirms this advantage (+26.7 percentage points on multi-step tasks). Our results demonstrate that: (1) LLMs can learn novel DSLs entirely from in-context prompts, achieving near-native accuracy; (2) constrained syntax significantly reduces errors on complex tasks; and (3) domain-specific languages purposefully designed for LLM generation can outperform general-purpose languages on which the LLM has extensive training. We release the complete language implementation, benchmark suite, and evaluation framework to facilitate further research.

</details>


### [65] [ManchuTTS: Towards High-Quality Manchu Speech Synthesis via Flow Matching and Hierarchical Text Representation](https://arxiv.org/abs/2512.22491)
*Suhua Wang,Zifan Wang,Xiaoxin Sun,D. J. Wang,Zhanbo Liu,Xin Li*

Main category: cs.CL

TL;DR: The paper introduces ManchuTTS, a novel text-to-speech system adapted for the endangered Manchu language, overcoming challenges like agglutination and data scarcity through innovative techniques and achieving superior performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges of creating an effective text-to-speech system for the endangered Manchu language, particularly the issues of data scarcity and its highly agglutinative phonological system.

Method: The proposed method involves a three-tier text representation (phoneme, syllable, prosodic) and a cross-modal hierarchical attention mechanism for better alignment. It integrates deep convolutional networks with a flow-matching Transformer for efficient and non-autoregressive generation. The method also introduces a hierarchical contrastive loss for structured acoustic-linguistic correspondence and employs data augmentation.

Result: ManchuTTS achieved a Mean Opinion Score (MOS) of 4.52 using a 5.2-hour training subset, surpassing baseline models. Hierarchical guidance improved agglutinative word pronunciation accuracy by 31% and prosodic naturalness by 27%.

Conclusion: The paper demonstrates the effectiveness of ManchuTTS in overcoming the linguistic and data challenges of Manchu speech synthesis, setting a benchmark for endangered language technologies.

Abstract: As an endangered language, Manchu presents unique challenges for speech synthesis, including severe data scarcity and strong phonological agglutination. This paper proposes ManchuTTS(Manchu Text to Speech), a novel approach tailored to Manchu's linguistic characteristics. To handle agglutination, this method designs a three-tier text representation (phoneme, syllable, prosodic) and a cross-modal hierarchical attention mechanism for multi-granular alignment. The synthesis model integrates deep convolutional networks with a flow-matching Transformer, enabling efficient, non-autoregressive generation. This method further introduce a hierarchical contrastive loss to guide structured acoustic-linguistic correspondence. To address low-resource constraints, This method construct the first Manchu TTS dataset and employ a data augmentation strategy. Experiments demonstrate that ManchuTTS attains a MOS of 4.52 using a 5.2-hour training subset derived from our full 6.24-hour annotated corpus, outperforming all baseline models by a notable margin. Ablations confirm hierarchical guidance improves agglutinative word pronunciation accuracy (AWPA) by 31% and prosodic naturalness by 27%.

</details>


### [66] [Learning When Not to Attend Globally](https://arxiv.org/abs/2512.22562)
*Xuan Luo,Kailai Zhang,Xifeng Yan*

Main category: cs.CL

TL;DR: The paper introduces All-or-Here Attention (AHA), a dynamic mechanism for language models that toggles between full and local attention dynamically, reducing computation while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Improve the efficiency of Large Language Models by addressing the problem of redundant global attention operations, inspired by how humans only occasionally revisit prior context when reading.

Method: The authors propose using a binary router per attention head, enabling a dynamic switch between full attention and local sliding window attention of size 256 tokens, depending on the token's need for global information.

Result: The AHA mechanism reduces up to 93% of the full attention operations without performance degradation. Results showed a rapid decay in the necessity of full attention as local windows expand.

Conclusion: Full attention in language models is largely redundant and can be optimized through localized processing with on-demand global context access, enabling more efficient inference.

Abstract: When reading books, humans focus primarily on the current page, flipping back to recap prior context only when necessary. Similarly, we demonstrate that Large Language Models (LLMs) can learn to dynamically determine when to attend to global context. We propose All-or-Here Attention (AHA), which utilizes a binary router per attention head to dynamically toggle between full attention and local sliding window attention for each token. Our results indicate that with a window size of 256 tokens, up to 93\% of the original full attention operations can be replaced by sliding window attention without performance loss. Furthermore, by evaluating AHA across various window sizes, we identify a long-tail distribution in context dependency, where the necessity for full attention decays rapidly as the local window expands. By decoupling local processing from global access, AHA reveals that full attention is largely redundant, and that efficient inference requires only on-demand access to the global context.

</details>


### [67] [Structured Prompting and LLM Ensembling for Multimodal Conversational Aspect-based Sentiment Analysis](https://arxiv.org/abs/2512.22603)
*Zhiqiang Gao,Shihao Gao,Zixing Zhang,Yihao Guo,Hongyu Chen,Jing Han*

Main category: cs.CL

TL;DR: The paper addresses sentiment analysis in multimodal conversations by proposing methods to extract sentiment components and detect sentiment shifts using large language models (LLMs), achieving notable performance.


<details>
  <summary>Details</summary>
Motivation: To build emotionally intelligent AI systems capable of understanding complex sentiment dynamics in multi-speaker, multimodal conversations.

Method: Designed a structured prompting pipeline for large language models (LLMs) to extract sentiment components for Subtask-I and used an ensemble of three LLMs to detect sentiment shifts and triggers for Subtask-II.

Result: Achieved 47.38% average score on Subtask-I and 74.12% exact match F1 on Subtask-II, demonstrating the effectiveness of their proposed methods.

Conclusion: Step-wise refinement and ensemble strategies are effective approaches for addressing the complexities of multimodal sentiment analysis, especially in tasks requiring a nuanced understanding of dialogue context and sentiment transitions.

Abstract: Understanding sentiment in multimodal conversations is a complex yet crucial challenge toward building emotionally intelligent AI systems. The Multimodal Conversational Aspect-based Sentiment Analysis (MCABSA) Challenge invited participants to tackle two demanding subtasks: (1) extracting a comprehensive sentiment sextuple, including holder, target, aspect, opinion, sentiment, and rationale from multi-speaker dialogues, and (2) detecting sentiment flipping, which detects dynamic sentiment shifts and their underlying triggers. For Subtask-I, in the present paper, we designed a structured prompting pipeline that guided large language models (LLMs) to sequentially extract sentiment components with refined contextual understanding. For Subtask-II, we further leveraged the complementary strengths of three LLMs through ensembling to robustly identify sentiment transitions and their triggers. Our system achieved a 47.38% average score on Subtask-I and a 74.12% exact match F1 on Subtask-II, showing the effectiveness of step-wise refinement and ensemble strategies in rich, multimodal sentiment analysis tasks.

</details>


### [68] [Chain-of-thought Reviewing and Correction for Time Series Question Answering](https://arxiv.org/abs/2512.22627)
*Chen Su,Yuanhe Tian,Yan Song*

Main category: cs.CL

TL;DR: The paper introduces T3LLM, a novel framework for enhancing reasoning in time series question answering (TSQA) using large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based approaches often struggle with reasoning errors when processing time series data, despite the inherent verifiability of such data. The authors aim to improve multi-step reasoning and consistency in TSQA tasks.

Method: The framework involves three LLMs—a worker for initial reasoning, a reviewer for identifying and correcting errors, and a student for integrating corrections through fine-tuning on collaboratively generated corrected reasoning chains.

Result: The proposed T3LLM achieves state-of-the-art performance on multiple TSQA benchmarks compared to existing LLM-based baselines.

Conclusion: T3LLM effectively enhances the reasoning performance and reliability of time series analysis tasks by leveraging a structured multi-step reasoning and correction mechanism with LLMs.

Abstract: With the advancement of large language models (LLMs), diverse time series analysis tasks are reformulated as time series question answering (TSQA) through a unified natural language interface. However, existing LLM-based approaches largely adopt general natural language processing techniques and are prone to reasoning errors when handling complex numerical sequences. Different from purely textual tasks, time series data are inherently verifiable, enabling consistency checking between reasoning steps and the original input. Motivated by this property, we propose T3LLM, which performs multi-step reasoning with an explicit correction mechanism for time series question answering. The T3LLM framework consists of three LLMs, namely, a worker, a reviewer, and a student, that are responsible for generation, review, and reasoning learning, respectively. Within this framework, the worker generates step-wise chains of thought (CoT) under structured prompts, while the reviewer inspects the reasoning, identifies erroneous steps, and provides corrective comments. The collaboratively generated corrected CoT are used to fine-tune the student model, internalizing multi-step reasoning and self-correction into its parameters. Experiments on multiple real-world TSQA benchmarks demonstrate that T3LLM achieves state-of-the-art performance over strong LLM-based baselines.

</details>


### [69] [M2G-Eval: Enhancing and Evaluating Multi-granularity Multilingual Code Generation](https://arxiv.org/abs/2512.22628)
*Fanglin Xu,Wei Zhang,Jian Yang,Guo Chen,Aishan Liu,Zhoujun Li,Xianglong Liu,Bryan Dai*

Main category: cs.CL

TL;DR: The paper introduces M2G-Eval, a novel evaluation framework for testing large language models' (LLMs) code generation abilities across different granularity levels and programming languages. Results reveal varying difficulties across tasks and languages.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the lack of fine-grained and multilingual benchmarks for assessing the diverse code-generation abilities of large language models in various structural granularities and programming languages.

Method: The researchers created M2G-Eval, consisting of 17K+ training tasks and 1,286 human-annotated test cases. They trained M2G-Eval-Coder models using supervised fine-tuning and Group Relative Policy Optimization. The evaluation encompassed 30 models, including state-of-the-art and newly created variants.

Result: The findings include three main points: (1) Line-level tasks are easier, and Class-level tasks are most challenging; (2) performance gaps widen between languages when tasks become more complex; (3) cross-language correlations suggest the learning of transferable programming concepts.

Conclusion: M2G-Eval provides a comprehensive tool for diagnosing code generation capabilities in LLMs, emphasizing the challenges of synthesizing complex and long-form code, and bringing insights into multilingual and multi-granularity code generation evaluations.

Abstract: The rapid advancement of code large language models (LLMs) has sparked significant research interest in systematically evaluating their code generation capabilities, yet existing benchmarks predominantly assess models at a single structural granularity and focus on limited programming languages, obscuring fine-grained capability variations across different code scopes and multilingual scenarios. We introduce M2G-Eval, a multi-granularity, multilingual framework for evaluating code generation in large language models (LLMs) across four levels: Class, Function, Block, and Line. Spanning 18 programming languages, M2G-Eval includes 17K+ training tasks and 1,286 human-annotated, contamination-controlled test instances. We develop M2G-Eval-Coder models by training Qwen3-8B with supervised fine-tuning and Group Relative Policy Optimization. Evaluating 30 models (28 state-of-the-art LLMs plus our two M2G-Eval-Coder variants) reveals three main findings: (1) an apparent difficulty hierarchy, with Line-level tasks easiest and Class-level most challenging; (2) widening performance gaps between full- and partial-granularity languages as task complexity increases; and (3) strong cross-language correlations, suggesting that models learn transferable programming concepts. M2G-Eval enables fine-grained diagnosis of code generation capabilities and highlights persistent challenges in synthesizing complex, long-form code.

</details>


### [70] [On the Role of Discreteness in Diffusion LLMs](https://arxiv.org/abs/2512.22630)
*Ziqi Jin,Bin Wang,Xiang Lin,Lidong Bing,Aixin Sun*

Main category: cs.CL

TL;DR: The paper evaluates diffusion language modeling approaches, identifies their constraints, and proposes directions for improving coherence in language generation.


<details>
  <summary>Details</summary>
Motivation: To explore the challenges and limitations of applying diffusion model principles to language generation, a task complicated by text's discrete and structured nature.

Method: Two key methods are analyzed: continuous diffusion in embedding spaces and discrete diffusion over tokens. Additionally, structural issues in current models are identified through analytical comparisons and large model evaluations.

Result: Key challenges identified include mismatched corruption mechanisms and training schemes that fail to capture dependencies across tokens. These limit parallel decoding and refinement.

Conclusion: Adjustments to diffusion processes are needed to better accommodate text structure, advancing more coherent and effective language generation models.

Abstract: Diffusion models offer appealing properties for language generation, such as parallel decoding and iterative refinement, but the discrete and highly structured nature of text challenges the direct application of diffusion principles. In this paper, we revisit diffusion language modeling from the view of diffusion process and language modeling, and outline five properties that separate diffusion mechanics from language-specific requirements. We first categorize existing approaches into continuous diffusion in embedding space and discrete diffusion over tokens. We then show that each satisfies only part of the five essential properties and therefore reflects a structural trade-off. Through analyses of recent large diffusion language models, we identify two central issues: (i) uniform corruption does not respect how information is distributed across positions, and (ii) token-wise marginal training cannot capture multi-token dependencies during parallel decoding. These observations motivate diffusion processes that align more closely with the structure of text, and encourage future work toward more coherent diffusion language models.

</details>


### [71] [Evaluating GRPO and DPO for Faithful Chain-of-Thought Reasoning in LLMs](https://arxiv.org/abs/2512.22631)
*Hadi Mohammadi,Tamas Kozak,Anastasia Giachanou*

Main category: cs.CL

TL;DR: The paper evaluates optimization methods to improve the faithfulness of Chain-of-Thought reasoning in language models, proposing GRPO as a more effective but scale-dependent solution compared to DPO.


<details>
  <summary>Details</summary>
Motivation: Chain-of-Thought (CoT) reasoning has shown potential to enhance multi-step reasoning in language models, yet its reliability is undermined by misleading reasoning explanations and inconsistent answers.

Method: Two optimization methods, GRPO and DPO, are systematically tested to improve the faithfulness of CoT reasoning in large language models.

Result: The experiments demonstrate that GRPO outperforms DPO in terms of faithfulness, especially in larger models, though GRPO's stability decreases with smaller models.

Conclusion: GRPO presents as a superior optimization method for improving trustworthiness and faithfulness in Chain-of-Thought reasoning, offering valuable insights for safer and more transparent LLM reasoning.

Abstract: Chain-of-thought (CoT) reasoning has emerged as a powerful technique for improving the problem-solving capabilities of large language models (LLMs), particularly for tasks requiring multi-step reasoning. However, recent studies show that CoT explanations often fail to reflect the model's actual reasoning process, as models may produce coherent yet misleading justifications or modify answers without acknowledging external cues. Such discrepancies undermine the reliability of CoT-based methods for safety supervision and alignment monitoring, as models can generate plausible but deceptive rationales for incorrect answers. To better understand this limitation, we evaluate two optimization methods, Group Relative Policy Optimization (GRPO) and Direct Preference Optimization (DPO), in their ability to improve CoT faithfulness. Our experiments show that GRPO achieves higher performance than DPO in larger models, with the Qwen2.5-14B-Instruct model attaining the best results across all evaluation metrics. Both approaches exhibit positive correlations between model size and performance, but GRPO shows greater potential for improving faithfulness metrics, albeit with less stable behavior at smaller scales. These results suggest that GRPO offers a promising direction for developing more transparent and trustworthy reasoning in LLMs.

</details>


### [72] [Fragile Knowledge, Robust Instruction-Following: The Width Pruning Dichotomy in Llama-3.2](https://arxiv.org/abs/2512.22671)
*Pere Martra*

Main category: cs.CL

TL;DR: This study investigates the effect of width pruning in GLU-MLP layers, guided by the Maximum Absolute Weight (MAW), on model performance and capabilities, showing selective degradation and improvements in specific tasks.


<details>
  <summary>Details</summary>
Motivation: To understand how structured pruning in GLU-MLP layers influences various model capabilities, and to challenge the assumption that pruning uniformly degrades neural network performance.

Method: Applied a pruning methodology guided by the Maximum Absolute Weight (MAW) criterion, evaluating seven configurations of expansion ratios based on their impact on tasks such as factual knowledge, reasoning, instruction-following, and other abilities.

Result: Pruning does not lead to uniform degradation. Instruction-following capabilities improve significantly, multi-step reasoning remains robust, and factual knowledge capacity decreases while truthfulness improves. Pruned configurations save energy but may increase latency.

Conclusion: MAW-guided width pruning acts as a selective filter, modulating cognitive capabilities rather than uniformly compressing the model. This highlights the critical role of the expansion ratio in influencing model behavior and introduces a novel link between knowledge degradation and improved behavioral alignment.

Abstract: Structured width pruning of GLU-MLP layers, guided by the Maximum Absolute Weight (MAW) criterion, reveals a systematic dichotomy in how reducing the expansion ratio affects different model capabilities. While performance on tasks relying on parametric knowledge (e.g., MMLU, GSM8K) and perplexity metrics degrades predictably, instruction-following capabilities improve substantially (+46% to +75% in IFEval for Llama-3.2-1B and 3B models), and multi-step reasoning remains robust (MUSR). This pattern challenges the prevailing assumption that pruning induces uniform degradation. We evaluated seven expansion ratio configurations using comprehensive benchmarks assessing factual knowledge, mathematical reasoning, language comprehension, instruction-following, and truthfulness. Our analysis identifies the expansion ratio as a critical architectural parameter that selectively modulates cognitive capabilities, rather than merely serving as a compression metric. We provide the first systematic characterization of this selective preservation phenomenon. Notably, we document a robust inverse correlation (r = -0.864, p = 0.012 in Llama-3B) between factual knowledge capacity (MMLU) and truthfulness metrics (TruthfulQA-MC2): as knowledge degrades, the model's ability to discriminate misconceptions improves consistently. This connects two previously distinct research areas, demonstrating that MAW-guided width pruning acts as a selective filter, reducing parametric knowledge while preserving or enhancing behavioral alignment. Additionally, we quantify context-dependent efficiency trade-offs: pruned configurations achieve up to 23% reduction in energy consumption (J/token) but incur penalties in single-request latency, whereas batch processing workloads benefit uniformly.

</details>


### [73] [Conformal Prediction Sets for Next-Token Prediction in Large Language Models: Balancing Coverage Guarantees with Set Efficiency](https://arxiv.org/abs/2512.22682)
*Yoshith Roy Kotla,Varshith Roy Kotla*

Main category: cs.CL

TL;DR: This paper addresses the inefficiency of standard uncertainty quantification methods in language models and proposes a new approach called VACP for prediction improvement.


<details>
  <summary>Details</summary>
Motivation: Current uncertainty quantification in language models is often poorly calibrated, producing overly large and uninformative prediction sets when applied to large vocabularies in high-stakes domains.

Method: The authors present Vocabulary-Aware Conformal Prediction (VACP), a method that uses semantic masking and temperature-adjusted scoring to reduce prediction space while ensuring valid marginal coverage.

Result: Using VACP on the Gemma-2B model with SQUAD and WikiText benchmarks, the method achieves empirical coverage of 89.7% with a substantial reduction in prediction set size from 847 to 4.3 tokens.

Conclusion: VACP significantly improves the efficiency and practicality of prediction sets in large language models, offering better uncertainty quantification without sacrificing accuracy.

Abstract: Deploying large language models (LLMs) in high-stakes domains requires rigorous uncertainty quantification, yet standard softmax probabilities are often poorly calibrated. We present a systematic study of Adaptive Prediction Sets (APS) applied to next-token prediction in transformer-based models with large vocabularies (greater than 250,000 tokens). Our central contribution is the identification of a coverage-efficiency tradeoff: while naive conformal prediction achieves valid coverage, it produces prediction sets of hundreds of tokens, rendering them uninformative. We propose Vocabulary-Aware Conformal Prediction (VACP), a framework that leverages semantic masking and temperature-adjusted scoring to reduce the effective prediction space while provably maintaining marginal coverage. Experiments on Gemma-2B using SQUAD and WikiText benchmarks demonstrate that VACP achieves 89.7 percent empirical coverage (90 percent target) while reducing the mean prediction set size from 847 tokens to 4.3 tokens -- a 197x improvement in efficiency. We provide a theoretical analysis of vocabulary reduction and release our implementation for reproducibility.

</details>


### [74] [GHaLIB: A Multilingual Framework for Hope Speech Detection in Low-Resource Languages](https://arxiv.org/abs/2512.22705)
*Ahmed Abdullah,Sana Fatima,Haroon Mahmood*

Main category: cs.CL

TL;DR: This paper proposes a multilingual framework to detect hope speech, focusing on Urdu using transformer models and achieving strong performance across languages.


<details>
  <summary>Details</summary>
Motivation: Hope speech detection is underrepresented in NLP, especially for low-resource languages, limiting tools for positive online communication.

Method: The authors use pretrained transformer models like XLM-RoBERTa, mBERT, EuroBERT, and UrduBERT, applying simple preprocessing to train classifiers for hope speech detection.

Result: Results show F1-scores of 95.2% for Urdu binary classification and 65.2% for Urdu multi-class classification, with similar performance in Spanish, German, and English.

Conclusion: Existing multilingual transformer models can be effectively implemented in low-resource settings to improve hope speech detection and promote positive online interactions.

Abstract: Hope speech has been relatively underrepresented in Natural Language Processing (NLP). Current studies are largely focused on English, which has resulted in a lack of resources for low-resource languages such as Urdu. As a result, the creation of tools that facilitate positive online communication remains limited. Although transformer-based architectures have proven to be effective in detecting hate and offensive speech, little has been done to apply them to hope speech or, more generally, to test them across a variety of linguistic settings. This paper presents a multilingual framework for hope speech detection with a focus on Urdu. Using pretrained transformer models such as XLM-RoBERTa, mBERT, EuroBERT, and UrduBERT, we apply simple preprocessing and train classifiers for improved results. Evaluations on the PolyHope-M 2025 benchmark demonstrate strong performance, achieving F1-scores of 95.2% for Urdu binary classification and 65.2% for Urdu multi-class classification, with similarly competitive results in Spanish, German, and English. These results highlight the possibility of implementing existing multilingual models in low-resource environments, thus making it easier to identify hope speech and helping to build a more constructive digital discourse.

</details>


### [75] [Beg to Differ: Understanding Reasoning-Answer Misalignment Across Languages](https://arxiv.org/abs/2512.22712)
*Anaelia Ovalle,Candace Ross,Sebastian Ruder,Adina Williams,Karen Ullrich,Mark Ibrahim,Levent Sagun*

Main category: cs.CL

TL;DR: The paper investigates the transferability of reasoning capabilities across languages in large language models using chain-of-thought prompting, revealing significant reasoning failures particularly in non-Latin scripts.


<details>
  <summary>Details</summary>
Motivation: To explore if the reasoning abilities of large language models, particularly through chain-of-thought prompting, remain effective and robust across different languages, especially given the current lack of multilingual reasoning evaluations.

Method: The authors introduced a human-validated framework and analyzed 65k model-generated reasoning traces from GlobalMMLU questions in 6 languages using 6 frontier models, supported by a taxonomy of errors developed through human annotations.

Result: While task accuracy was high, reasoning often failed to support conclusions. Non-Latin scripts displayed at least twice the reasoning-conclusion misalignment seen in Latin scripts, with common errors being unsupported claims and illogical reasoning steps.

Conclusion: The study highlights critical shortcomings in multilingual reasoning evaluations and emphasizes the necessity for reasoning-aware evaluation frameworks for a more complete understanding of model reasoning capabilities.

Abstract: Large language models demonstrate strong reasoning capabilities through chain-of-thought prompting, but whether this reasoning quality transfers across languages remains underexplored. We introduce a human-validated framework to evaluate whether model-generated reasoning traces logically support their conclusions across languages. Analyzing 65k reasoning traces from GlobalMMLU questions across 6 languages and 6 frontier models, we uncover a critical blind spot: while models achieve high task accuracy, their reasoning can fail to support their conclusions. Reasoning traces in non-Latin scripts show at least twice as much misalignment between their reasoning and conclusions than those in Latin scripts. We develop an error taxonomy through human annotation to characterize these failures, finding they stem primarily from evidential errors (unsupported claims, ambiguous facts) followed by illogical reasoning steps. Our findings demonstrate that current multilingual evaluation practices provide an incomplete picture of model reasoning capabilities and highlight the need for reasoning-aware evaluation frameworks.

</details>


### [76] [Mitigating Social Desirability Bias in Random Silicon Sampling](https://arxiv.org/abs/2512.22725)
*Sashank Chapala,Maksym Mironov,Songgaojun Deng*

Main category: cs.CL

TL;DR: This paper explores methods to reduce social desirability bias in large language model (LLM) responses by testing minimal, psychologically grounded prompt adjustments.


<details>
  <summary>Details</summary>
Motivation: Social desirability bias (SDB) causes LLM responses to diverge from real human data, leading to less representative outputs, particularly on socially sensitive questions, necessitating solutions to address this issue.

Method: The study investigates SDB mitigation in LLMs by applying four prompt-based methods and evaluates alignment to real human data from the American National Election Study (ANES) using Jensen-Shannon Divergence.

Result: The reformulated (neutral, third-person phrasing) prompt proved most effective at reducing SDB and achieving closer alignment with human data, while other methods showed mixed or no significant benefits.

Conclusion: Prompt-based framing controls, such as reformulated prompts, can mitigate social desirability bias in LLMs, improving the representativeness of silicon-simulated population samples.

Abstract: Large Language Models (LLMs) are increasingly used to simulate population responses, a method known as ``Silicon Sampling''. However, responses to socially sensitive questions frequently exhibit Social Desirability Bias (SDB), diverging from real human data toward socially acceptable answers. Existing studies on social desirability bias in LLM-based sampling remain limited. In this work, we investigate whether minimal, psychologically grounded prompt wording can mitigate this bias and improve alignment between silicon and human samples. We conducted a study using data from the American National Election Study (ANES) on three LLMs from two model families: the open-source Llama-3.1 series and GPT-4.1-mini. We first replicate a baseline silicon sampling study, confirming the persistent Social Desirability Bias. We then test four prompt-based mitigation methods: \emph{reformulated} (neutral, third-person phrasing), \emph{reverse-coded} (semantic inversion), and two meta-instructions, \emph{priming} and \emph{preamble}, respectively encouraging analytics and sincerity. Alignment with ANES is evaluated using Jensen-Shannon Divergence with bootstrap confidence intervals. Our results demonstrate that reformulated prompts most effectively improve alignment by reducing distribution concentration on socially acceptable answers and achieving distributions closer to ANES. Reverse-coding produced mixed results across eligible items, while the Priming and Preamble encouraged response uniformity and showed no systematic benefit for bias mitigation. Our findings validate the efficacy of prompt-based framing controls in mitigating inherent Social Desirability Bias in LLMs, providing a practical path toward more representative silicon samples.

</details>


### [77] [Data Augmentation for Classification of Negative Pregnancy Outcomes in Imbalanced Data](https://arxiv.org/abs/2512.22732)
*Md Badsha Biswas*

Main category: cs.CL

TL;DR: This paper proposes utilizing social media data, particularly Twitter, and natural language processing (NLP) to enhance the study of negative pregnancy outcomes (e.g., birth defects, miscarriages).


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the ongoing public health concern of infant mortality, particularly driven by birth defects, and the need for more effective methods and comprehensive research strategies.

Method: The method involves developing a natural language processing (NLP) pipeline to analyze Twitter data to identify and categorize women who share pregnancy experiences based on outcomes (positive or negative cases).

Result: The study constructs a framework where NLP aids in identifying and categorizing cases, potentially applying the results to analyze interventions and exposures impacting maternal and fetal health.

Conclusion: Social media data, when combined with robust preprocessing techniques, can serve as a valuable supplemental resource for epidemiological research on pregnancy outcomes while paving the way for future studies with pregnant cohorts.

Abstract: Infant mortality remains a significant public health concern in the United States, with birth defects identified as a leading cause. Despite ongoing efforts to understand the causes of negative pregnancy outcomes like miscarriage, stillbirths, birth defects, and premature birth, there is still a need for more comprehensive research and strategies for intervention. This paper introduces a novel approach that uses publicly available social media data, especially from platforms like Twitter, to enhance current datasets for studying negative pregnancy outcomes through observational research. The inherent challenges in utilizing social media data, including imbalance, noise, and lack of structure, necessitate robust preprocessing techniques and data augmentation strategies. By constructing a natural language processing (NLP) pipeline, we aim to automatically identify women sharing their pregnancy experiences, categorizing them based on reported outcomes. Women reporting full gestation and normal birth weight will be classified as positive cases, while those reporting negative pregnancy outcomes will be identified as negative cases. Furthermore, this study offers potential applications in assessing the causal impact of specific interventions, treatments, or prenatal exposures on maternal and fetal health outcomes. Additionally, it provides a framework for future health studies involving pregnant cohorts and comparator groups. In a broader context, our research showcases the viability of social media data as an adjunctive resource in epidemiological investigations about pregnancy outcomes.

</details>


### [78] [WeDLM: Reconciling Diffusion Language Models with Standard Causal Attention for Fast Inference](https://arxiv.org/abs/2512.22737)
*Aiwei Liu,Minghua He,Shaoxun Zeng,Sijun Zhang,Linhao Zhang,Chuhan Wu,Wei Jia,Yuan Liu,Xiao Zhou,Jie Zhou*

Main category: cs.CL

TL;DR: WeDLM introduces a diffusion decoding framework using causal attention for faster and efficient parallel generation without sacrificing quality, achieving significant speedups over optimized autoregressive engines.


<details>
  <summary>Details</summary>
Motivation: Standard AR generation for LLMs remains slow due to its sequential nature, and while DLLMs offer parallel decoding, their efficiency is hindered by reliance on bidirectional attention. Addressing this inefficiency is crucial to improve deployment speed.

Method: WeDLM leverages Topological Reordering to enforce causal attention while maintaining logical token positions. It incorporates a streaming decoding process that progressively grows a left-to-right prefix, ensuring constant parallel workload and avoiding delays.

Result: WeDLM delivers substantial speedups in generation tasks, reaching up to 10x faster in low-entropy regimes and nearly 3x in reasoning tasks. It surpasses AR baselines optimized with vLLM under similar deployment conditions.

Conclusion: WeDLM demonstrates that causal attention-based diffusion decoding can achieve parallel generation efficiency, outperforming optimized AR systems while maintaining high quality in diverse generation scenarios.

Abstract: Autoregressive (AR) generation is the standard decoding paradigm for Large Language Models (LLMs), but its token-by-token nature limits parallelism at inference time. Diffusion Language Models (DLLMs) offer parallel decoding by recovering multiple masked tokens per step; however, in practice they often fail to translate this parallelism into deployment speed gains over optimized AR engines (e.g., vLLM). A key reason is that many DLLMs rely on bidirectional attention, which breaks standard prefix KV caching and forces repeated contextualization, undermining efficiency. We propose WeDLM, a diffusion decoding framework built entirely on standard causal attention to make parallel generation prefix-cache friendly. The core idea is to let each masked position condition on all currently observed tokens while keeping a strict causal mask, achieved by Topological Reordering that moves observed tokens to the physical prefix while preserving their logical positions. Building on this property, we introduce a streaming decoding procedure that continuously commits confident tokens into a growing left-to-right prefix and maintains a fixed parallel workload, avoiding the stop-and-wait behavior common in block diffusion methods. Experiments show that WeDLM preserves the quality of strong AR backbones while delivering substantial speedups, approaching 3x on challenging reasoning benchmarks and up to 10x in low-entropy generation regimes; critically, our comparisons are against AR baselines served by vLLM under matched deployment settings, demonstrating that diffusion-style decoding can outperform an optimized AR engine in practice.

</details>


### [79] [Harnessing Large Language Models for Biomedical Named Entity Recognition](https://arxiv.org/abs/2512.22738)
*Jian Chen,Leilei Su,Cong Sun*

Main category: cs.CL

TL;DR: BioSelectTune enhances biomedical named entity recognition (BioNER) by leveraging a data-centric fine-tuning approach.


<details>
  <summary>Details</summary>
Motivation: BioNER is essential in medical informatics and applications, but adapting LLMs faces challenges due to limited domain knowledge and poor-quality training data.

Method: Introduced BioSelectTune, which reformulates BioNER as a JSON generation task and uses a Hybrid Superfiltering strategy for high-quality data curation.

Result: BioSelectTune achieves state-of-the-art performance on BioNER benchmarks, even with reduced training data, outperforming domain-specific models like BioMedBERT.

Conclusion: BioSelectTune offers an efficient and effective approach to fine-tuning LLMs for BioNER, emphasizing data quality over quantity.

Abstract: Background and Objective: Biomedical Named Entity Recognition (BioNER) is a foundational task in medical informatics, crucial for downstream applications like drug discovery and clinical trial matching. However, adapting general-domain Large Language Models (LLMs) to this task is often hampered by their lack of domain-specific knowledge and the performance degradation caused by low-quality training data. To address these challenges, we introduce BioSelectTune, a highly efficient, data-centric framework for fine-tuning LLMs that prioritizes data quality over quantity. Methods and Results: BioSelectTune reformulates BioNER as a structured JSON generation task and leverages our novel Hybrid Superfiltering strategy, a weak-to-strong data curation method that uses a homologous weak model to distill a compact, high-impact training dataset. Conclusions: Through extensive experiments, we demonstrate that BioSelectTune achieves state-of-the-art (SOTA) performance across multiple BioNER benchmarks. Notably, our model, trained on only 50% of the curated positive data, not only surpasses the fully-trained baseline but also outperforms powerful domain-specialized models like BioMedBERT.

</details>


### [80] [Text-Routed Sparse Mixture-of-Experts Model with Explanation and Temporal Alignment for Multi-Modal Sentiment Analysis](https://arxiv.org/abs/2512.22741)
*Dongning Rao,Yunbiao Zeng,Zhihua Jiang,Jujian Lv*

Main category: cs.CL

TL;DR: The paper introduces TEXT, a model for Multi-modal Sentiment Analysis (MSA) that combines explanations and temporal alignments to outperform state-of-the-art approaches across multiple datasets.


<details>
  <summary>Details</summary>
Motivation: To address the limited exploration of explanations and temporal alignments in Multi-modal Sentiment Analysis, which are critical for understanding subtle emotions in human-interaction applications.

Method: Proposes TEXT, a Text-routed sparse mixture-of-Experts model using multi-modal large language models and a temporality-oriented neural network block for enhanced explanations and modality alignment.

Result: TEXT outperforms baseline models, achieving significant improvements across four datasets, reducing mean absolute error by up to 13.5% on the CH-SIMS dataset.

Conclusion: The inclusion of explanations and temporal alignment in TEXT enhances MSA performance, demonstrating its ability to handle subtle emotional cues across modalities.

Abstract: Human-interaction-involved applications underscore the need for Multi-modal Sentiment Analysis (MSA). Although many approaches have been proposed to address the subtle emotions in different modalities, the power of explanations and temporal alignments is still underexplored. Thus, this paper proposes the Text-routed sparse mixture-of-Experts model with eXplanation and Temporal alignment for MSA (TEXT). TEXT first augments explanations for MSA via Multi-modal Large Language Models (MLLM), and then novelly aligns the epresentations of audio and video through a temporality-oriented neural network block. TEXT aligns different modalities with explanations and facilitates a new text-routed sparse mixture-of-experts with gate fusion. Our temporal alignment block merges the benefits of Mamba and temporal cross-attention. As a result, TEXT achieves the best performance cross four datasets among all tested models, including three recently proposed approaches and three MLLMs. TEXT wins on at least four metrics out of all six metrics. For example, TEXT decreases the mean absolute error to 0.353 on the CH-SIMS dataset, which signifies a 13.5% decrement compared with recently proposed approaches.

</details>


### [81] [Fake News Classification in Urdu: A Domain Adaptation Approach for a Low-Resource Language](https://arxiv.org/abs/2512.22778)
*Muhammad Zain Ali,Bernhard Pfahringer,Tony Smith*

Main category: cs.CL

TL;DR: The study focuses on tackling misinformation in the low-resource Urdu language by employing domain-adaptive pretraining on multilingual models like XLM-RoBERTa and mBERT for fake news detection, achieving improved performance.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in addressing the challenge of misinformation detection in low-resource languages such as Urdu, which lacks sufficient attention compared to high-resource languages.

Method: The paper employs a staged training approach where domain-adaptive pretraining is conducted on multilingual language models (XLM-RoBERTa and mBERT) using an Urdu news corpus, followed by fine-tuning for fake news classification.

Result: Domain-adapted XLM-R consistently outperforms its base model on Urdu fake news datasets, while domain-adapted mBERT shows mixed performance outcomes.

Conclusion: The study highlights the effectiveness of domain-adaptive pretraining for improving misinformation detection in low-resource languages like Urdu, showcasing XLM-R's dominance in this context.

Abstract: Misinformation on social media is a widely acknowledged issue, and researchers worldwide are actively engaged in its detection. However, low-resource languages such as Urdu have received limited attention in this domain. An obvious approach is to utilize a multilingual pretrained language model and fine-tune it for a downstream classification task, such as misinformation detection. However, these models struggle with domain-specific terms, leading to suboptimal performance. To address this, we investigate the effectiveness of domain adaptation before fine-tuning for fake news classification in Urdu, employing a staged training approach to optimize model generalization. We evaluate two widely used multilingual models, XLM-RoBERTa and mBERT, and apply domain-adaptive pretraining using a publicly available Urdu news corpus. Experiments on four publicly available Urdu fake news datasets show that domain-adapted XLM-R consistently outperforms its vanilla counterpart, while domain-adapted mBERT exhibits mixed results.

</details>


### [82] [CNSight: Evaluation of Clinical Note Segmentation Tools](https://arxiv.org/abs/2512.22795)
*Risha Surana,Adrian Law,Sunwoo Kim,Rishab Sridhar,Angxiao Han,Peiyu Hong*

Main category: cs.CL

TL;DR: The paper evaluates clinical note segmentation approaches, concluding that large language models outperform others, and highlights implications for downstream tasks like information extraction.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the challenge of using unstructured or semi-structured clinical notes for secondary analysis and clinical applications. Structured segmentation is key for extracting distinct clinical contexts from notes.

Method: The authors evaluate rule-based baselines, domain-specific transformers, and large language models using a dataset of 1,000 MIMIC-IV clinical notes.

Result: GPT-5-mini achieved the best performance, with an average F1 score of 72.4, outperforming other models particularly on unstructured freetext segmentation.

Conclusion: The research provides practical guidance on method selection for clinical note segmentation and enables future applications such as information extraction and automated summarization.

Abstract: Clinical notes are often stored in unstructured or semi-structured formats after extraction from electronic medical record (EMR) systems, which complicates their use for secondary analysis and downstream clinical applications. Reliable identification of section boundaries is a key step toward structuring these notes, as sections such as history of present illness, medications, and discharge instructions each provide distinct clinical contexts. In this work, we evaluate rule-based baselines, domain-specific transformer models, and large language models for clinical note segmentation using a curated dataset of 1,000 notes from MIMIC-IV. Our experiments show that large API-based models achieve the best overall performance, with GPT-5-mini reaching a best average F1 of 72.4 across sentence-level and freetext segmentation. Lightweight baselines remain competitive on structured sentence-level tasks but falter on unstructured freetext. Our results provide guidance for method selection and lay the groundwork for downstream tasks such as information extraction, cohort identification, and automated summarization.

</details>


### [83] [NepEMO: A Multi-Label Emotion and Sentiment Analysis on Nepali Reddit with Linguistic Insights and Temporal Trends](https://arxiv.org/abs/2512.22823)
*Sameer Sitoula,Tej Bahadur Shahi,Laxmi Prasad Bhatt,Anisha Pokhrel,Arjun Neupane*

Main category: cs.CL

TL;DR: This paper introduces a manually-annotated dataset for classifying emotions and sentiments in Nepali subreddit posts with notable findings, including transformer models outperforming other techniques.


<details>
  <summary>Details</summary>
Motivation: To address the lack of emotion and sentiment analysis data specifically tailored to Nepali social media content, thereby creating opportunities for detailed linguistic and analytical studies.

Method: The authors built a dataset containing 4,462 posts from the Nepali subreddit annotated with five emotions and three sentiment classes. They performed linguistic analyses and tested traditional ML, deep learning, and transformer models for multi-label emotion and sentiment classification.

Result: Transformer models consistently outperformed traditional ML and deep learning approaches across both multi-label emotion and sentiment classification tasks.

Conclusion: The study establishes the value of transformer models for analyzing emotions and sentiments on the Nepali subreddit, while also providing researchers with a rich, annotated dataset for further investigation.

Abstract: Social media (SM) platforms (e.g. Facebook, Twitter, and Reddit) are increasingly leveraged to share opinions and emotions, specifically during challenging events, such as natural disasters, pandemics, and political elections, and joyful occasions like festivals and celebrations. Among the SM platforms, Reddit provides a unique space for its users to anonymously express their experiences and thoughts on sensitive issues such as health and daily life. In this work, we present a novel dataset, called NepEMO, for multi-label emotion (MLE) and sentiment classification (SC) on the Nepali subreddit post. We curate and build a manually annotated dataset of 4,462 posts (January 2019- June 2025) written in English, Romanised Nepali and Devanagari script for five emotions (fear, anger, sadness, joy, and depression) and three sentiment classes (positive, negative, and neutral). We perform a detailed analysis of posts to capture linguistic insights, including emotion trends, co-occurrence of emotions, sentiment-specific n-grams, and topic modelling using Latent Dirichlet Allocation and TF-IDF keyword extraction. Finally, we compare various traditional machine learning (ML), deep learning (DL), and transformer models for MLE and SC tasks. The result shows that transformer models consistently outperform the ML and DL models for both tasks.

</details>


### [84] [AutoForge: Automated Environment Synthesis for Agentic Reinforcement Learning](https://arxiv.org/abs/2512.22857)
*Shihao Cai,Runnan Fang,Jialong Wu,Baixuan Li,Xinyu Wang,Yong Jiang,Liangcai Su,Liwen Zhang,Wenbiao Yin,Zhen Zhang,Fuli Feng,Pengjun Xie,Xiaobin Wang*

Main category: cs.CL

TL;DR: The paper proposes an improved reinforcement learning framework with automated environment creation and a novel algorithm addressing user instability for training language-based agents.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning in simulated environments is limited by challenges such as semi-automated synthesis, lack of complex tasks, user instability, and heterogeneity.

Method: The authors present a pipeline for automated synthesis of challenging, verifiable tasks and a novel environment-level RL algorithm to tackle user instability and improve training efficiency.

Result: The proposed method demonstrates improved performance and generalization across agentic benchmarks including tau-bench, tau2-Bench, and VitaBench.

Conclusion: The approach enhances RL for language-based agents by addressing prior limitations, achieving better training stability, efficiency, and out-of-domain adaptability.

Abstract: Conducting reinforcement learning (RL) in simulated environments offers a cost-effective and highly scalable way to enhance language-based agents. However, previous work has been limited to semi-automated environment synthesis or tasks lacking sufficient difficulty, offering little breadth or depth. In addition, the instability of simulated users integrated into these environments, along with the heterogeneity across simulated environments, poses further challenges for agentic RL. In this work, we propose: (1) a unified pipeline for automated and scalable synthesis of simulated environments associated with high-difficulty but easily verifiable tasks; and (2) an environment level RL algorithm that not only effectively mitigates user instability but also performs advantage estimation at the environment level, thereby improving training efficiency and stability. Comprehensive evaluations on agentic benchmarks, including tau-bench, tau2-Bench, and VitaBench, validate the effectiveness of our proposed method. Further in-depth analyses underscore its out-of-domain generalization.

</details>


### [85] [Diversity or Precision? A Deep Dive into Next Token Prediction](https://arxiv.org/abs/2512.22955)
*Haoyuan Wu,Hai Wang,Jiajia Wu,Jinxiang Ou,Keyao Wang,Weile Chen,Zihao Zheng,Bei Yu*

Main category: cs.CL

TL;DR: The paper proposes a generalized pre-training objective to reshape a pre-trained language model’s distribution, improving its exploration space for subsequent reinforcement learning (RL) applications.


<details>
  <summary>Details</summary>
Motivation: To enhance the effectiveness of RL fine-tuning for large language models (LLMs) by better shaping the pre-trained token-output distribution and improving their reasoning abilities.

Method: The authors generalize pre-training objectives by embedding on-policy RL principles into supervised learning, introducing a reward-shaping strategy aimed at balancing diversity and precision.

Result: They show that precision-oriented exploration spaces outperform the traditional intuition which favors higher entropy for effective RL exploration.

Conclusion: The proposed approach improves exploration spaces for RL fine-tuning by encouraging a precision-focused pre-trained model, leading to better reasoning performance in LLMs.

Abstract: Recent advancements have shown that reinforcement learning (RL) can substantially improve the reasoning abilities of large language models (LLMs). The effectiveness of such RL training, however, depends critically on the exploration space defined by the pre-trained model's token-output distribution. In this paper, we revisit the standard cross-entropy loss, interpreting it as a specific instance of policy gradient optimization applied within a single-step episode. To systematically study how the pre-trained distribution shapes the exploration potential for subsequent RL, we propose a generalized pre-training objective that adapts on-policy RL principles to supervised learning. By framing next-token prediction as a stochastic decision process, we introduce a reward-shaping strategy that explicitly balances diversity and precision. Our method employs a positive reward scaling factor to control probability concentration on ground-truth tokens and a rank-aware mechanism that treats high-ranking and low-ranking negative tokens asymmetrically. This allows us to reshape the pre-trained token-output distribution and investigate how to provide a more favorable exploration space for RL, ultimately enhancing end-to-end reasoning performance. Contrary to the intuition that higher distribution entropy facilitates effective exploration, we find that imposing a precision-oriented prior yields a superior exploration space for RL.

</details>


### [86] [Prompt engineering does not universally improve Large Language Model performance across clinical decision-making tasks](https://arxiv.org/abs/2512.22966)
*Mengdi Chai,Ali R. Zomorrodi*

Main category: cs.CL

TL;DR: This paper evaluates three Large Language Models (LLMs) across clinical reasoning tasks, focusing on their utility in real-world healthcare settings and exploring the role of prompt engineering.


<details>
  <summary>Details</summary>
Motivation: While LLMs show potential in medical knowledge assessments, their application in practical clinical workflows is less understood. This study aims to explore their effectiveness in real-world clinical decision-making.

Method: Using 36 patient case studies, the performance of three LLMs was assessed across five clinical tasks under two temperature settings. Additionally, variations of the 'MedPrompt' framework were applied to examine the impact of prompt engineering.

Result: LLMs demonstrated variability in task accuracy, excelling in final diagnosis but struggling in diagnostic testing. Prompt engineering improved performance for specific tasks but also showed limitations depending on the model and context.

Conclusion: The findings highlight that prompt engineering is not universally effective and its benefits are model and task-dependent. Context-aware strategies are crucial for effectively integrating LLMs into healthcare applications.

Abstract: Large Language Models (LLMs) have demonstrated promise in medical knowledge assessments, yet their practical utility in real-world clinical decision-making remains underexplored. In this study, we evaluated the performance of three state-of-the-art LLMs-ChatGPT-4o, Gemini 1.5 Pro, and LIama 3.3 70B-in clinical decision support across the entire clinical reasoning workflow of a typical patient encounter. Using 36 case studies, we first assessed LLM's out-of-the-box performance across five key sequential clinical decision-making tasks under two temperature settings (default vs. zero): differential diagnosis, essential immediate steps, relevant diagnostic testing, final diagnosis, and treatment recommendation. All models showed high variability by task, achieving near-perfect accuracy in final diagnosis, poor performance in relevant diagnostic testing, and moderate performance in remaining tasks. Furthermore, ChatGPT performed better under the zero temperature, whereas LIama showed stronger performance under the default temperature. Next, we assessed whether prompt engineering could enhance LLM performance by applying variations of the MedPrompt framework, incorporating targeted and random dynamic few-shot learning. The results demonstrate that prompt engineering is not a one-size-fit-all solution. While it significantly improved the performance on the task with lowest baseline accuracy (relevant diagnostic testing), it was counterproductive for others. Another key finding was that the targeted dynamic few-shot prompting did not consistently outperform random selection, indicating that the presumed benefits of closely matched examples may be counterbalanced by loss of broader contextual diversity. These findings suggest that the impact of prompt engineering is highly model and task-dependent, highlighting the need for tailored, context-aware strategies for integrating LLMs into healthcare.

</details>


### [87] [Improving Generalization in LLM Structured Pruning via Function-Aware Neuron Grouping](https://arxiv.org/abs/2512.23014)
*Tao Yu,Yongqi An,Kuan Zhu,Guibo Zhu,Ming Tang,Jinqiao Wang*

Main category: cs.CL

TL;DR: The paper proposes FANG, a framework to improve post-training pruning of large language models (LLMs), achieving better accuracy and maintaining performance with optimized sparsity.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) are computationally and storage intensive due to their size. Current pruning methods struggle with downstream performance when calibration sets do not align with pretraining data. This motivates the need for more effective pruning strategies.

Method: FANG identifies and groups neurons based on their function and the semantic context they handle. It preserves critical neurons for specific and multi-context functions and adapts sparsity allocation based on functional complexity, addressing calibration bias in post-training pruning.

Result: FANG improves downstream accuracy and preserves language modeling performance. It surpasses state-of-the-art methods, FLAP and OBC, enhancing accuracy by 1.5%-8.5% at 30% and 40% sparsity levels.

Conclusion: The FANG framework offers a superior pruning method that combats calibration bias, maintains language model efficacy, and achieves state-of-the-art results by optimizing sparsity-performance trade-offs.

Abstract: Large Language Models (LLMs) demonstrate impressive performance across natural language tasks but incur substantial computational and storage costs due to their scale. Post-training structured pruning offers an efficient solution. However, when few-shot calibration sets fail to adequately reflect the pretraining data distribution, existing methods exhibit limited generalization to downstream tasks. To address this issue, we propose Function-Aware Neuron Grouping (FANG), a post-training pruning framework that alleviates calibration bias by identifying and preserving neurons critical to specific function. FANG groups neurons with similar function based on the type of semantic context they process and prunes each group independently. During importance estimation within each group, tokens that strongly correlate with the functional role of the neuron group are given higher weighting. Additionally, FANG also preserves neurons that contribute across multiple context types. To achieve a better trade-off between sparsity and performance, it allocates sparsity to each block adaptively based on its functional complexity. Experiments show that FANG improves downstream accuracy while preserving language modeling performance. It achieves the state-of-the-art (SOTA) results when combined with FLAP and OBC, two representative pruning methods. Specifically, FANG outperforms FLAP and OBC by 1.5%--8.5% in average accuracy under 30% and 40% sparsity.

</details>


### [88] [LENS: LLM-Enabled Narrative Synthesis for Mental Health by Aligning Multimodal Sensing with Language Models](https://arxiv.org/abs/2512.23025)
*Wenxuan Xu,Arvind Pillai,Subigya Nepal,Amanda C Collins,Daniel M Mackin,Michael V Heinz,Tess Z Griffin,Nicholas C Jacobson,Andrew Campbell*

Main category: cs.CL

TL;DR: The paper introduces LENS, a framework linking multimodal health sensing data with language models to create mental-health narratives.


<details>
  <summary>Details</summary>
Motivation: Current LLMs struggle with processing long-duration sensor streams, and there is a lack of sensor-text paired datasets for mental health assessment.

Method: The authors created over 100,000 sensor-text QA pairs by converting EMA responses into natural-language and trained a patch-level encoder to map sensor signals into LLM representation space.

Result: LENS surpassed benchmarks in NLP metrics and accurately assessed symptoms. Mental-health professionals found the narratives both comprehensive and clinically relevant.

Conclusion: LENS demonstrates the potential for scalable LLM implementations to interpret sensor data, assisting in clinical decision-making with enriched health sensing capabilities.

Abstract: Multimodal health sensing offers rich behavioral signals for assessing mental health, yet translating these numerical time-series measurements into natural language remains challenging. Current LLMs cannot natively ingest long-duration sensor streams, and paired sensor-text datasets are scarce. To address these challenges, we introduce LENS, a framework that aligns multimodal sensing data with language models to generate clinically grounded mental-health narratives. LENS first constructs a large-scale dataset by transforming Ecological Momentary Assessment (EMA) responses related to depression and anxiety symptoms into natural-language descriptions, yielding over 100,000 sensor-text QA pairs from 258 participants. To enable native time-series integration, we train a patch-level encoder that projects raw sensor signals directly into an LLM's representation space. Our results show that LENS outperforms strong baselines on standard NLP metrics and task-specific measures of symptom-severity accuracy. A user study with 13 mental-health professionals further indicates that LENS-produced narratives are comprehensive and clinically meaningful. Ultimately, our approach advances LLMs as interfaces for health sensing, providing a scalable path toward models that can reason over raw behavioral signals and support downstream clinical decision-making.

</details>


### [89] [Is Chain-of-Thought Really Not Explainability? Chain-of-Thought Can Be Faithful without Hint Verbalization](https://arxiv.org/abs/2512.23032)
*Kerem Zaman,Shashank Srivastava*

Main category: cs.CL

TL;DR: The paper critiques the Biasing Features metric for confusing unfaithfulness with incompleteness in chain-of-thought (CoT) reasoning. It proposes new metrics and methods to assess faithfulness, suggesting broader evaluation tools.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve the evaluation of faithfulness in chain-of-thought reasoning models, addressing issues with existing metrics like Biasing Features.

Method: It introduces a new metric called faithful@k, uses larger token budgets during inference, and applies techniques like Causal Mediation Analysis to evaluate reasoning models.

Result: The study finds that increasing inference token budgets improves hint verbalization significantly and demonstrates that non-verbalized hints can still affect predictions. Existing evaluations are overly limited.

Conclusion: The authors caution against exclusively using hint-based assessment methods, advocating for diversified evaluation approaches to better analyze model interpretability and faithfulness.

Abstract: Recent work, using the Biasing Features metric, labels a CoT as unfaithful if it omits a prompt-injected hint that affected the prediction. We argue this metric confuses unfaithfulness with incompleteness, the lossy compression needed to turn distributed transformer computation into a linear natural language narrative. On multi-hop reasoning tasks with Llama-3 and Gemma-3, many CoTs flagged as unfaithful by Biasing Features are judged faithful by other metrics, exceeding 50% in some models. With a new faithful@k metric, we show that larger inference-time token budgets greatly increase hint verbalization (up to 90% in some settings), suggesting much apparent unfaithfulness is due to tight token limits. Using Causal Mediation Analysis, we further show that even non-verbalized hints can causally mediate prediction changes through the CoT. We therefore caution against relying solely on hint-based evaluations and advocate a broader interpretability toolkit, including causal mediation and corruption-based metrics.

</details>


### [90] [Accelerating Language Model Workflows with Prompt Choreography](https://arxiv.org/abs/2512.23049)
*TJ Bai,Jason Eisner*

Main category: cs.CL

TL;DR: The paper introduces Prompt Choreography, a framework that optimizes multi-agent LLM workflows by using a dynamic KV cache, offering significant reductions in latency and computational redundancy.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies and high computational costs in multi-agent workflows using large language models, particularly due to redundant encoding of messages.

Method: The framework utilizes a global KV cache that allows LLMs to access and attend to previously encoded messages, while enabling parallel calls. LLMs are fine-tuned to ensure outputs are consistent with results from traditional methods.

Result: Prompt Choreography reduced latency by 2.0–6.2× for time-to-first-token and achieved over 2.2× end-to-end speedups in certain workflows with redundant computation.

Conclusion: The introduction of Prompt Choreography demonstrates a practical solution for improving efficiency in LLM deployments, making them more scalable and effective in multi-agent tasks.

Abstract: Large language models are increasingly deployed in multi-agent workflows. We introduce Prompt Choreography, a framework that efficiently executes LLM workflows by maintaining a dynamic, global KV cache. Each LLM call can attend to an arbitrary, reordered subset of previously encoded messages. Parallel calls are supported. Though caching messages' encodings sometimes gives different results from re-encoding them in a new context, we show in diverse settings that fine-tuning the LLM to work with the cache can help it mimic the original results. Prompt Choreography significantly reduces per-message latency (2.0--6.2$\times$ faster time-to-first-token) and achieves substantial end-to-end speedups ($>$2.2$\times$) in some workflows dominated by redundant computation.

</details>


### [91] [TabiBERT: A Large-Scale ModernBERT Foundation Model and Unified Benchmarking Framework for Turkish](https://arxiv.org/abs/2512.23065)
*Melikşah Türker,A. Ebrar Kızıloğlu,Onur Güngör,Susan Üsküdarlı*

Main category: cs.CL

TL;DR: This paper presents TabiBERT, a state-of-the-art monolingual Turkish encoder based on ModernBERT architecture, achieving advancements in Turkish NLP through improved training paradigms and extensive evaluation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the gap in Turkish NLP by creating a modern, monolingual encoder trained from scratch that incorporates advancements in Transformer architecture such as long-context support, computational efficiency, and training stability.

Method: TabiBERT was pre-trained on a large multi-domain Turkish corpus of one trillion tokens using Rotary Positional Embeddings, FlashAttention, and refined normalization. The model supports an extended context length of 8,192 tokens and enhances inference speed and memory efficiency. It was evaluated using TabiBench, consisting of 28 datasets across eight task categories.

Result: TabiBERT achieved a score of 77.58 on TabiBench, surpassing BERTurk by 1.62 points, and established state-of-the-art performance on five of the eight task categories. It also outperformed task-specific specialized models in cross-domain generalization by +1.47 points.

Conclusion: TabiBERT advances Turkish NLP by combining modern encoder architecture with extensive evaluation protocols. The release of model weights and configurations aims to support further reproducible research in Turkish encoder development.

Abstract: Since the inception of BERT, encoder-only Transformers have evolved significantly in computational efficiency, training stability, and long-context modeling. ModernBERT consolidates these advances by integrating Rotary Positional Embeddings (RoPE), FlashAttention, and refined normalization. Despite these developments, Turkish NLP lacks a monolingual encoder trained from scratch incorporating such modern architectural paradigms. This work introduces TabiBERT, a monolingual Turkish encoder based on ModernBERT architecture trained from scratch on a large, curated corpus. TabiBERT is pre-trained on one trillion tokens sampled from an 84.88B token multi-domain corpus: web text (73%), scientific publications (20%), source code (6%), and mathematical content (0.3%). The model supports 8,192-token context length (16x original BERT), achieves up to 2.65x inference speedup, and reduces GPU memory consumption, enabling larger batch sizes. We introduce TabiBench with 28 datasets across eight task categories with standardized splits and protocols, evaluated using GLUE-style macro-averaging. TabiBERT attains 77.58 on TabiBench, outperforming BERTurk by 1.62 points and establishing state-of-the-art on five of eight categories: question answering (+9.55), code retrieval (+2.41), and document retrieval (+0.60). Compared with task-specific prior best results, including specialized models like TurkishBERTweet, TabiBERT achieves +1.47 average improvement, indicating robust cross-domain generalization. We release model weights, training configurations, and evaluation code for transparent, reproducible Turkish encoder research.

</details>


### [92] [Reservoir Computing inspired Matrix Multiplication-free Language Model](https://arxiv.org/abs/2512.23145)
*Takumi Shiratsuchi,Yuichiro Tanaka,Hakaru Tamukoh*

Main category: cs.CL

TL;DR: The paper proposes a more computationally efficient language model by introducing a matrix multiplication free architecture aided by reservoir computing, achieving comparable performance with reduced costs.


<details>
  <summary>Details</summary>
Motivation: To address the computational inefficiency and high costs of training large language models while maintaining their performance.

Method: A matrix multiplication free language model is designed, embedding reservoir layers that share partially fixed weights while combining specific operations to minimize memory accesses.

Result: The architecture reduces the number of parameters by 19%, training time by 9.9%, and inference time by 8.0%, achieving similar performance level as the baseline model.

Conclusion: The study successfully achieves computational and memory efficiency for language models through innovative architectural modifications without compromising their performance.

Abstract: Large language models (LLMs) have achieved state-of-the-art performance in natural language processing; however, their high computational cost remains a major bottleneck. In this study, we target computational efficiency by focusing on a matrix multiplication free language model (MatMul-free LM) and further reducing the training cost through an architecture inspired by reservoir computing. Specifically, we partially fix and share the weights of selected layers in the MatMul-free LM and insert reservoir layers to obtain rich dynamic representations without additional training overhead. Additionally, several operations are combined to reduce memory accesses. Experimental results show that the proposed architecture reduces the number of parameters by up to 19%, training time by 9.9%, and inference time by 8.0%, while maintaining comparable performance to the baseline model.

</details>


### [93] [Not too long do read: Evaluating LLM-generated extreme scientific summaries](https://arxiv.org/abs/2512.23206)
*Zhuoqi Lyu,Qing Ke*

Main category: cs.CL

TL;DR: The paper introduces the BiomedTLDR dataset composed of researcher-authored summaries from scientific papers, addressing challenges in evaluating LLM summarization abilities.


<details>
  <summary>Details</summary>
Motivation: To understand and evaluate how large language models (LLMs) generate extreme summaries (TLDR) for scientific papers, and to address the lack of a high-quality TLDR dataset for facilitating this research.

Method: The authors create a novel dataset (BiomedTLDR) derived from researcher-authored summaries and test current open-weight LLMs' ability to generate TLDRs based on abstracts, analyzing their summarization approaches.

Result: The analysis shows that while some LLMs generate humanoid summaries, they generally lean towards extractive summarization, closely mirroring the original text's terms and structure, unlike humans who are more abstractive.

Conclusion: The study highlights limitations of existing LLMs in producing high-abstraction summaries and introduces the BiomedTLDR dataset as a resource to advance research in scientific summarization.

Abstract: High-quality scientific extreme summary (TLDR) facilitates effective science communication. How do large language models (LLMs) perform in generating them? How are LLM-generated summaries different from those written by human experts? However, the lack of a comprehensive, high-quality scientific TLDR dataset hinders both the development and evaluation of LLMs' summarization ability. To address these, we propose a novel dataset, BiomedTLDR, containing a large sample of researcher-authored summaries from scientific papers, which leverages the common practice of including authors' comments alongside bibliography items. We then test popular open-weight LLMs for generating TLDRs based on abstracts. Our analysis reveals that, although some of them successfully produce humanoid summaries, LLMs generally exhibit a greater affinity for the original text's lexical choices and rhetorical structures, hence tend to be more extractive rather than abstractive in general, compared to humans. Our code and datasets are available at https://github.com/netknowledge/LLM_summarization (Lyu and Ke, 2025).

</details>


### [94] [Scoring, Reasoning, and Selecting the Best! Ensembling Large Language Models via a Peer-Review Process](https://arxiv.org/abs/2512.23213)
*Zhijun Chen,Zeyu Ji,Qianren Mao,Junhang Cheng,Bangjie Qin,Hao Wu,Zhuoran Li,Jingzheng Li,Kai Sun,Zizhe Wang,Yikun Ban,Zhu Sun,Xiangyang Ji,Hailong Sun*

Main category: cs.CL

TL;DR: The paper introduces LLM-PeerReview, an unsupervised method for selecting the best response from multiple LLM outputs using a peer-review-inspired framework.


<details>
  <summary>Details</summary>
Motivation: Improve response selection from LLM-generated outputs effectively using unsupervised, adaptable methods.

Method: Three-stage approach: scoring responses using LLMs as judges, aggregating scores via graphical model or averaging, and selecting the highest-scoring response.

Result: The two proposed variants achieve strong results, outperforming Smoothie-Global by 6.9% and 7.3% across four datasets.

Conclusion: LLM-PeerReview demonstrates effectiveness and flexibility in ensemble response selection with interpretability, adaptability, and competitive performance.

Abstract: We propose LLM-PeerReview, an unsupervised LLM Ensemble method that selects the most ideal response from multiple LLM-generated candidates for each query, harnessing the collective wisdom of multiple models with diverse strengths. LLM-PeerReview is built on a novel, peer-review-inspired framework that offers a clear and interpretable mechanism, while remaining fully unsupervised for flexible adaptability and generalization. Specifically, it operates in three stages: For scoring, we use the emerging LLM-as-a-Judge technique to evaluate each response by reusing multiple LLMs at hand; For reasoning, we can apply a principled graphical model-based truth inference algorithm or a straightforward averaging strategy to aggregate multiple scores to produce a final score for each response; Finally, the highest-scoring response is selected as the best ensemble output. LLM-PeerReview is conceptually simple and empirically powerful. The two variants of the proposed approach obtain strong results across four datasets, including outperforming the recent advanced model Smoothie-Global by 6.9% and 7.3% points, respectively.

</details>


### [95] [Interpretable Safety Alignment via SAE-Constructed Low-Rank Subspace Adaptation](https://arxiv.org/abs/2512.23260)
*Dianyun Wang,Qingsen Ma,Yuhu Shang,Zhifeng Lu,Lechen Ning,Zhenbo Xu,Huijia Wu,Zhaofeng He*

Main category: cs.CL

TL;DR: This paper introduces a method to improve parameter-efficient fine-tuning by using Sparse Autoencoders (SAEs) to create an interpretable low-rank subspace for task adaptation.


<details>
  <summary>Details</summary>
Motivation: Current methods for fine-tuning large language models, such as LoRA, lack interpretability as they learn task-relevant subspaces implicitly, which may result in polysemantic dimensions entangling multiple concepts.

Method: The authors use pre-trained Sparse Autoencoders (SAEs) to identify task-relevant features in a disentangled space and construct an interpretable low-rank subspace for guiding adapter initialization. A theoretical analysis under monosemanticity assumptions supports their approach.

Result: The approach achieves up to a 99.6% safety rate--7.4% higher than full fine-tuning and near RLHF-based methods--while updating only 0.19-0.24% of parameters. It also provides semantic interpretability of the learned subspace.

Conclusion: Incorporating mechanistic interpretability into fine-tuning can enhance both performance and transparency in adapting language models to downstream tasks, as shown by the improved results and insights provided by this method.

Abstract: Parameter-efficient fine-tuning has become the dominant paradigm for adapting large language models to downstream tasks. Low-rank adaptation methods such as LoRA operate under the assumption that task-relevant weight updates reside in a low-rank subspace, yet this subspace is learned implicitly from data in a black-box manner, offering no interpretability or direct control. We hypothesize that this difficulty stems from polysemanticity--individual dimensions encoding multiple entangled concepts. To address this, we leverage pre-trained Sparse Autoencoders (SAEs) to identify task-relevant features in a disentangled feature space, then construct an explicit, interpretable low-rank subspace to guide adapter initialization. We provide theoretical analysis proving that under monosemanticity assumptions, SAE-based subspace identification achieves arbitrarily small recovery error, while direct identification in polysemantic space suffers an irreducible error floor. On safety alignment, our method achieves up to 99.6% safety rate--exceeding full fine-tuning by 7.4 percentage points and approaching RLHF-based methods--while updating only 0.19-0.24% of parameters. Crucially, our method provides interpretable insights into the learned alignment subspace through the semantic grounding of SAE features. Our work demonstrates that incorporating mechanistic interpretability into the fine-tuning process can simultaneously improve both performance and transparency.

</details>


### [96] [Chinese Morph Resolution in E-commerce Live Streaming Scenarios](https://arxiv.org/abs/2512.23280)
*Jiahao Zhu,Jipeng Qiang,Ran Bai,Chenyu Liu,Xiaoye Ouyang*

Main category: cs.CL

TL;DR: This paper introduces a task named LiveAMR to detect morph-based evasion in e-commerce live streaming, focusing on health and medical streams.


<details>
  <summary>Details</summary>
Motivation: Hosts in live-stream platforms evade regulations using morph techniques, creating a need for effective detection methods to ensure proper regulation and truth in advertising.

Method: The study formulated LiveAMR as a text-to-text generation task, leveraging large language models to generate training data on a newly constructed dataset of 86,790 samples.

Result: Leveraging LLMs improved task performance, and the findings indicate morph resolution significantly aids live streaming regulation.

Conclusion: Morph-based evasion detection enhances regulatory frameworks for live streaming, especially in health and e-commerce sectors.

Abstract: E-commerce live streaming in China, particularly on platforms like Douyin, has become a major sales channel, but hosts often use morphs to evade scrutiny and engage in false advertising. This study introduces the Live Auditory Morph Resolution (LiveAMR) task to detect such violations. Unlike previous morph research focused on text-based evasion in social media and underground industries, LiveAMR targets pronunciation-based evasion in health and medical live streams. We constructed the first LiveAMR dataset with 86,790 samples and developed a method to transform the task into a text-to-text generation problem. By leveraging large language models (LLMs) to generate additional training data, we improved performance and demonstrated that morph resolution significantly enhances live streaming regulation.

</details>


### [97] [AI4Reading: Chinese Audiobook Interpretation System Based on Multi-Agent Collaboration](https://arxiv.org/abs/2512.23300)
*Minjiang Huang,Jipeng Qiang,Yi Zhu,Chaowei Zhang,Xiangyu Zhao,Kui Yu*

Main category: cs.CL

TL;DR: AI4Reading is a system using large language models and speech synthesis to automate audiobook interpretation, aiming for accurate, comprehensible, and logically structured content.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the time-consuming and resource-intensive process of manually creating audiobook interpretations by automating it using AI technologies.

Method: The paper introduces AI4Reading, a collaborative multi-agent system that includes 11 specialized agents such as topic analysts, editors, and narrators, which explore themes, organize content, and synthesize spoken language.

Result: The generated AI interpretations were simpler and more accurate compared to expert-created scripts, though the speech generation quality still lags.

Conclusion: AI4Reading demonstrates promise in automating audiobook creation with simpler and accurate scripts, but further improvement is needed in the speech generation quality.

Abstract: Audiobook interpretations are attracting increasing attention, as they provide accessible and in-depth analyses of books that offer readers practical insights and intellectual inspiration. However, their manual creation process remains time-consuming and resource-intensive. To address this challenge, we propose AI4Reading, a multi-agent collaboration system leveraging large language models (LLMs) and speech synthesis technology to generate podcast, like audiobook interpretations. The system is designed to meet three key objectives: accurate content preservation, enhanced comprehensibility, and a logical narrative structure. To achieve these goals, we develop a framework composed of 11 specialized agents,including topic analysts, case analysts, editors, a narrator, and proofreaders that work in concert to explore themes, extract real world cases, refine content organization, and synthesize natural spoken language. By comparing expert interpretations with our system's output, the results show that although AI4Reading still has a gap in speech generation quality, the generated interpretative scripts are simpler and more accurate.

</details>


### [98] [AI Meets Brain: Memory Systems from Cognitive Neuroscience to Autonomous Agents](https://arxiv.org/abs/2512.23343)
*Jiafeng Liang,Hao Li,Chang Li,Jiaqi Zhou,Shixin Jiang,Zekun Wang,Changkai Ji,Zhihao Zhu,Runxuan Liu,Tao Ren,Jinlan Fu,See-Kiong Ng,Xia Liang,Ming Liu,Bing Qin*

Main category: cs.CL

TL;DR: This paper links human memory mechanisms and AI memory systems, offering insights from cognitive neuroscience to improve memory functionalities in LLM-driven agents.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between human memory mechanisms and the development of memory systems in autonomous AI agents, and address interdisciplinary barriers.

Method: The paper systematically synthesizes interdisciplinary knowledge from cognitive neuroscience and integrates it with LLM-driven agent memory systems via comparative analysis, reviews, and future recommendations.

Result: Provides insights into memory definitions, taxonomies, management lifecycles, benchmarks for memory evaluation, and security risks in AI systems.

Conclusion: The research promotes advanced memory designs for AI, addresses interdisciplinary limitations, and proposes future directions like multimodal memory and skill acquisition.

Abstract: Memory serves as the pivotal nexus bridging past and future, providing both humans and AI systems with invaluable concepts and experience to navigate complex tasks. Recent research on autonomous agents has increasingly focused on designing efficient memory workflows by drawing on cognitive neuroscience. However, constrained by interdisciplinary barriers, existing works struggle to assimilate the essence of human memory mechanisms. To bridge this gap, we systematically synthesizes interdisciplinary knowledge of memory, connecting insights from cognitive neuroscience with LLM-driven agents. Specifically, we first elucidate the definition and function of memory along a progressive trajectory from cognitive neuroscience through LLMs to agents. We then provide a comparative analysis of memory taxonomy, storage mechanisms, and the complete management lifecycle from both biological and artificial perspectives. Subsequently, we review the mainstream benchmarks for evaluating agent memory. Additionally, we explore memory security from dual perspectives of attack and defense. Finally, we envision future research directions, with a focus on multimodal memory systems and skill acquisition.

</details>


### [99] [A Stepwise-Enhanced Reasoning Framework for Large Language Models Based on External Subgraph Generation](https://arxiv.org/abs/2512.23356)
*Xin Zhang,Yang Cao,Baoxing Wu,Xinyi Chen,Kai Song,Siying Li*

Main category: cs.CL

TL;DR: The study introduces a framework called SGR that enhances reasoning in large language models (LLMs) by incorporating external subgraph generation and stepwise reasoning.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of LLMs facing difficulties in tasks requiring deep reasoning and logical inference, especially when noise or irrelevant data from training corpora interfere with their outputs.

Method: Proposes a Stepwise Reasoning Enhancement Framework (SGR), which dynamically constructs query-relevant subgraphs from external knowledge bases and uses these subgraphs for guided, multi-step reasoning.

Result: SGR demonstrated improved reasoning accuracy and consistently outperformed baseline models across multiple benchmark datasets.

Conclusion: The SGR framework enhances the reasoning capability of LLMs by effectively addressing their limitations in handling complex queries, making their predictions more accurate and consistent.

Abstract: Large Language Models (LLMs) have achieved strong performance across a wide range of natural language processing tasks in recent years, including machine translation, text generation, and question answering. As their applications extend to increasingly complex scenarios, however, LLMs continue to face challenges in tasks that require deep reasoning and logical inference. In particular, models trained on large scale textual corpora may incorporate noisy or irrelevant information during generation, which can lead to incorrect predictions or outputs that are inconsistent with factual knowledge. To address this limitation, we propose a stepwise reasoning enhancement framework for LLMs based on external subgraph generation, termed SGR. The proposed framework dynamically constructs query relevant subgraphs from external knowledge bases and leverages their semantic structure to guide the reasoning process. By performing reasoning in a step by step manner over structured subgraphs, SGR reduces the influence of noisy information and improves reasoning accuracy. Specifically, the framework first generates an external subgraph tailored to the input query, then guides the model to conduct multi step reasoning grounded in the subgraph, and finally integrates multiple reasoning paths to produce the final answer. Experimental results on multiple benchmark datasets demonstrate that SGR consistently outperforms strong baselines, indicating its effectiveness in enhancing the reasoning capabilities of LLMs.

</details>


### [100] [Entropy-Guided Token Dropout: Training Autoregressive Language Models with Limited Domain Data](https://arxiv.org/abs/2512.23422)
*Jiapeng Wang,Yiwen Hu,Yanzipeng Gao,Haoyu Wang,Shuo Wang,Hongyu Lu,Jiaxin Mao,Wayne Xin Zhao,Junyi Li,Xiao Zhang*

Main category: cs.CL

TL;DR: This paper introduces EntroDrop, an entropy-guided token dropout method to address performance degradation in large language models during multi-epoch training.


<details>
  <summary>Details</summary>
Motivation: Address the performance degradation in autoregressive large language models caused by repeated data exposure, especially in data-constrained settings.

Method: Introduce EntroDrop, which masks low-entropy tokens during training. A curriculum schedule adjusts the regularization strength as training progresses.

Result: EntroDrop outperforms standard baselines and maintains robust performance in extended multi-epoch training across different model scales.

Conclusion: Aligning regularization with token-level learning dynamics is crucial for effective LLM adaptation in limited data environments.

Abstract: As access to high-quality, domain-specific data grows increasingly scarce, multi-epoch training has become a practical strategy for adapting large language models (LLMs). However, autoregressive models often suffer from performance degradation under repeated data exposure, where overfitting leads to a marked decline in model capability. Through empirical analysis, we trace this degradation to an imbalance in learning dynamics: predictable, low-entropy tokens are learned quickly and come to dominate optimization, while the model's ability to generalize on high-entropy tokens deteriorates with continued training. To address this, we introduce EntroDrop, an entropy-guided token dropout method that functions as structured data regularization. EntroDrop selectively masks low-entropy tokens during training and employs a curriculum schedule to adjust regularization strength in alignment with training progress. Experiments across model scales from 0.6B to 8B parameters show that EntroDrop consistently outperforms standard regularization baselines and maintains robust performance throughout extended multi-epoch training. These findings underscore the importance of aligning regularization with token-level learning dynamics when training on limited data. Our approach offers a promising pathway toward more effective adaptation of LLMs in data-constrained domains.

</details>


### [101] [The Effect of Gender Diversity on Scientific Team Impact: A Team Roles Perspective](https://arxiv.org/abs/2512.23429)
*Yi Zhao,Yongjun Zhu,Donghun Kim,Yuzhuo Wang,Heng Zhang,Chao Lu,Chengzhi Zhang*

Main category: cs.CL

TL;DR: This study investigates the role of gender diversity in scientific teams' success, focusing on different team roles like leadership and support, using over 130,000 papers.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address gaps in understanding how gender diversity across team roles (leadership vs. support) influences the impact of scientific teams.

Method: The authors analyzed over 130,000 PLOS journal articles using multivariable regression and threshold regression models, incorporating gender roles and team sizes.

Result: Findings indicate an inverted U-shaped relationship between gender diversity and team impact. Teams with all-female leaders and all-male support groups performed best. Leadership diversity had a variable effect by team size, while support diversity consistently improved team performance.

Conclusion: The study highlights the complexities of gender diversity's effects on team success, emphasizing the distinct roles of leadership and support groups and their interplay with team size.

Abstract: The influence of gender diversity on the success of scientific teams is of great interest to academia. However, prior findings remain inconsistent, and most studies operationalize diversity in aggregate terms, overlooking internal role differentiation. This limitation obscures a more nuanced understanding of how gender diversity shapes team impact. In particular, the effect of gender diversity across different team roles remains poorly understood. To this end, we define a scientific team as all coauthors of a paper and measure team impact through five-year citation counts. Using author contribution statements, we classified members into leadership and support roles. Drawing on more than 130,000 papers from PLOS journals, most of which are in biomedical-related disciplines, we employed multivariable regression to examine the association between gender diversity in these roles and team impact. Furthermore, we apply a threshold regression model to investigate how team size moderates this relationship. The results show that (1) the relationship between gender diversity and team impact follows an inverted U-shape for both leadership and support groups; (2) teams with an all-female leadership group and an all-male support group achieve higher impact than other team types. Interestingly, (3) the effect of leadership-group gender diversity is significantly negative for small teams but becomes positive and statistically insignificant in large teams. In contrast, the estimates for support-group gender diversity remain significant and positive, regardless of team size.

</details>


### [102] [C2PO: Diagnosing and Disentangling Bias Shortcuts in LLMs](https://arxiv.org/abs/2512.23430)
*Xuan Feng,Bo An,Tianlong Gu,Liang Chang,Fengrui Hao,Peipeng Yu,Shuai Zhao*

Main category: cs.CL

TL;DR: The study introduces C2PO, a framework tackling biases in LLMs by identifying and addressing spurious feature correlations, effectively mitigating stereotypical and structural biases while preserving general reasoning abilities.


<details>
  <summary>Details</summary>
Motivation: Current approaches to mitigating biases in LLMs often address one type of bias at the expense of exacerbating others, necessitating a unified solution.

Method: The paper proposes C2PO, a framework using causal counterfactual signals to distinguish between bias-inducing and valid features, paired with fairness-sensitive updates to suppress undesired correlations during optimization.

Result: C2PO demonstrated success in reducing biases while maintaining reasoning capability, validated across several benchmarks covering stereotypical bias, structural bias, and overall fairness and utility.

Conclusion: C2PO shows promise as a unified approach to address various biases in LLMs without compromising general performance, offering substantial improvements in fairness and trustworthiness.

Abstract: Bias in Large Language Models (LLMs) poses significant risks to trustworthiness, manifesting primarily as stereotypical biases (e.g., gender or racial stereotypes) and structural biases (e.g., lexical overlap or position preferences). However, prior paradigms typically address these in isolation, often mitigating one at the expense of exacerbating the other. To address this, we conduct a systematic exploration of these reasoning failures and identify a primary inducement: the latent spurious feature correlations within the input that drive these erroneous reasoning shortcuts. Driven by these findings, we introduce Causal-Contrastive Preference Optimization (C2PO), a unified alignment framework designed to tackle these specific failures by simultaneously discovering and suppressing these correlations directly within the optimization process. Specifically, C2PO leverages causal counterfactual signals to isolate bias-inducing features from valid reasoning paths, and employs a fairness-sensitive preference update mechanism to dynamically evaluate logit-level contributions and suppress shortcut features. Extensive experiments across multiple benchmarks covering stereotypical bias (BBQ, Unqover), structural bias (MNLI, HANS, Chatbot, MT-Bench), out-of-domain fairness (StereoSet, WinoBias), and general utility (MMLU, GSM8K) demonstrate that C2PO effectively mitigates stereotypical and structural biases while preserving robust general reasoning capabilities.

</details>


### [103] [ClinDEF: A Dynamic Evaluation Framework for Large Language Models in Clinical Reasoning](https://arxiv.org/abs/2512.23440)
*Yuqi Tang,Jing Yu,Zichang Su,Kehua Feng,Zhihui Zhu,Libin Wang,Lei Liang,Qiang Zhang,Keyan Ding,Huajun Chen*

Main category: cs.CL

TL;DR: The paper introduces ClinDEF, a framework for evaluating LLMs' clinical reasoning via dynamic diagnostic dialogues, emphasizing evaluation beyond accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for LLMs in clinical applications fail to capture dynamic doctor-patient interactions, limiting understanding of clinical reasoning capabilities.

Method: ClinDEF creates simulated diagnostic dialogues using a disease knowledge graph, pairing an LLM-based doctor with an automated patient agent. Evaluation includes diagnostic accuracy, efficiency analysis, and rubric-based quality assessment.

Result: ClinDEF highlights critical gaps in clinical reasoning of state-of-the-art LLMs and provides nuanced evaluations of these models.

Conclusion: ClinDEF enhances understanding of LLMs’ capabilities in clinical reasoning, offering a more meaningful evaluation for medical applications.

Abstract: Clinical diagnosis begins with doctor-patient interaction, during which physicians iteratively gather information, determine examination and refine differential diagnosis through patients' response. This dynamic clinical-reasoning process is poorly represented by existing LLM benchmarks that focus on static question-answering. To mitigate these gaps, recent methods explore dynamic medical frameworks involving interactive clinical dialogues. Although effective, they often rely on limited, contamination-prone datasets and lack granular, multi-level evaluation. In this work, we propose ClinDEF, a dynamic framework for assessing clinical reasoning in LLMs through simulated diagnostic dialogues. Grounded in a disease knowledge graph, our method dynamically generates patient cases and facilitates multi-turn interactions between an LLM-based doctor and an automated patient agent. Our evaluation protocol goes beyond diagnostic accuracy by incorporating fine-grained efficiency analysis and rubric-based assessment of diagnostic quality. Experiments show that ClinDEF effectively exposes critical clinical reasoning gaps in state-of-the-art LLMs, offering a more nuanced and clinically meaningful evaluation paradigm.

</details>


### [104] [Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss](https://arxiv.org/abs/2512.23447)
*Ang Lv,Jin Ma,Yiyuan Ma,Siyuan Qiao*

Main category: cs.CL

TL;DR: The paper introduces Expert-Router Coupling (ERC) loss to improve router decisions in Mixture-of-Experts (MoE) models, ensuring alignment with expert capabilities to enhance performance.


<details>
  <summary>Details</summary>
Motivation: MoE models lack explicit constraints ensuring router decisions match expert capabilities, limiting overall model performance.

Method: ERC loss is proposed, enforcing constraints on activations using perturbed router embeddings as proxy tokens. It ensures alignment between experts and router embeddings efficiently, operating on n^2 activations (where n is the number of experts).

Result: ERC loss was validated by pre-training MoE-LLMs (3B to 15B parameters) on trillions of tokens, demonstrating improved routing accuracy and insights into expert specialization.

Conclusion: ERC loss improves expert-router alignment in MoE models, enhances computational efficiency, and provides better control and insights into expert specialization during training.

Abstract: Mixture-of-Experts (MoE) models lack explicit constraints to ensure the router's decisions align well with the experts' capabilities, which ultimately limits model performance. To address this, we propose expert-router coupling (ERC) loss, a lightweight auxiliary loss that tightly couples the router's decisions with expert capabilities. Our approach treats each expert's router embedding as a proxy token for the tokens assigned to that expert, and feeds perturbed router embeddings through the experts to obtain internal activations. The ERC loss enforces two constraints on these activations: (1) Each expert must exhibit higher activation for its own proxy token than for the proxy tokens of any other expert. (2) Each proxy token must elicit stronger activation from its corresponding expert than from any other expert. These constraints jointly ensure that each router embedding faithfully represents its corresponding expert's capability, while each expert specializes in processing the tokens actually routed to it. The ERC loss is computationally efficient, operating only on n^2 activations, where n is the number of experts. This represents a fixed cost independent of batch size, unlike prior coupling methods that scale with the number of tokens (often millions per batch). Through pre-training MoE-LLMs ranging from 3B to 15B parameters and extensive analysis on trillions of tokens, we demonstrate the effectiveness of the ERC loss. Moreover, the ERC loss offers flexible control and quantitative tracking of expert specialization levels during training, providing valuable insights into MoEs.

</details>


### [105] [Semantic Tree Inference on Text Corpa using a Nested Density Approach together with Large Language Model Embeddings](https://arxiv.org/abs/2512.23471)
*Thomas Haschka,Joseph Bakarji*

Main category: cs.CL

TL;DR: The paper proposes a nested density clustering method to derive hierarchical semantic relationships in text corpora using LLM embeddings. It demonstrates the method's effectiveness using case studies and benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: With the rise of Large Language Models (LLMs) and their embedding capabilities, understanding and visualizing global semantic structures in textual data remains a challenge, prompting the development of a new hierarchical clustering approach.

Method: The method identifies dense clusters of semantically similar texts in LLM embedding spaces and gradually relaxes the density criteria to merge clusters, forming hierarchical tree structures, which are further evaluated on scientific abstracts and benchmark datasets.

Result: The approach successfully captured hierarchical semantic relationships among texts, displayed robustness across benchmark datasets, and demonstrated its potential in identifying research areas and semantic topic evolution.

Conclusion: Nested density clustering offers a robust and versatile framework for analyzing hierarchical semantic structures and evolution in various textual datasets, enabling deeper insights into semantic relationships and data-driven classification.

Abstract: Semantic text classification has undergone significant advances in recent years due to the rise of large language models (LLMs) and their high dimensional embeddings. While LLM-embeddings are frequently used to store and retrieve text by semantic similarity in vector databases, the global structure semantic relationships in text corpora often remains opaque. Herein we propose a nested density clustering approach, to infer hierarchical trees of semantically related texts. The method starts by identifying texts of strong semantic similarity as it searches for dense clusters in LLM embedding space. As the density criterion is gradually relaxed, these dense clusters merge into more diffuse clusters, until the whole dataset is represented by a single cluster -- the root of the tree. By embedding dense clusters into increasingly diffuse ones, we construct a tree structure that captures hierarchical semantic relationships among texts. We outline how this approach can be used to classify textual data for abstracts of scientific abstracts as a case study. This enables the data-driven discovery research areas and their subfields without predefined categories. To evaluate the general applicability of the method, we further apply it to established benchmark datasets such as the 20 Newsgroups and IMDB 50k Movie Reviews, demonstrating its robustness across domains. Finally we discuss possible applications on scientometrics, topic evolution, highlighting how nested density trees can reveal semantic structure and evolution in textual datasets.

</details>


### [106] [Automatic Detection of Complex Quotation Patterns in Aggadic Literature](https://arxiv.org/abs/2512.23504)
*Hadar Miller,Tsvi Kuflik,Moshe Lavee*

Main category: cs.CL

TL;DR: The paper introduces ACT, a successful method for detecting biblical quotations in Rabbinic literature, achieving high precision and recall scores.


<details>
  <summary>Details</summary>
Motivation: Current frameworks struggle to identify short, paraphrased, or complex embedded biblical quotations in historical texts, necessitating a more effective approach.

Method: It proposes a three-stage algorithm combining morphology-aware alignment, context-sensitive enrichment, and stylistic classification for detection.

Result: The ACT-QE configuration outperformed baseline systems and achieved an F1 score of 0.91, Precision at 0.94, and Recall at 0.89.

Conclusion: ACT advances the understanding of textual relationships, aiding computational philology and opening paths in historical textual analysis and genre-specific studies.

Abstract: This paper presents ACT (Allocate Connections between Texts), a novel three-stage algorithm for the automatic detection of biblical quotations in Rabbinic literature. Unlike existing text reuse frameworks that struggle with short, paraphrased, or structurally embedded quotations, ACT combines a morphology-aware alignment algorithm with a context-sensitive enrichment stage that identifies complex citation patterns such as "Wave" and "Echo" quotations.
  Our approach was evaluated against leading systems, including Dicta, Passim, Text-Matcher, as well as human-annotated critical editions. We further assessed three ACT configurations to isolate the contribution of each component. Results demonstrate that the full ACT pipeline (ACT-QE) outperforms all baselines, achieving an F1 score of 0.91, with superior Recall (0.89) and Precision (0.94). Notably, ACT-2, which lacks stylistic enrichment, achieves higher Recall (0.90) but suffers in Precision, while ACT-3, using longer n-grams, offers a tradeoff between coverage and specificity.
  In addition to improving quotation detection, ACT's ability to classify stylistic patterns across corpora opens new avenues for genre classification and intertextual analysis. This work contributes to digital humanities and computational philology by addressing the methodological gap between exhaustive machine-based detection and human editorial judgment. ACT lays a foundation for broader applications in historical textual analysis, especially in morphologically rich and citation-dense traditions like Aggadic literature.

</details>


### [107] [UniHetero: Could Generation Enhance Understanding for Vision-Language-Model at Large Data Scale?](https://arxiv.org/abs/2512.23512)
*Fengjiao Chen,Minhao Jing,Weitao Lu,Yan Feng,Xiaoyu Li,Xuezhi Cao*

Main category: cs.CL

TL;DR: The study explores the unification of visual understanding and generation tasks, demonstrating insights about effective methods for enhancing understanding.


<details>
  <summary>Details</summary>
Motivation: Understanding whether vision generations can enhance understanding on a large dataset scale.

Method: Analysis of a unified model, UniHetero, pre-trained on over 200 million samples.

Result: Generation enhances understanding when semantics, not pixels, are generated. It also improves data utilization and scaling, with autoregression effectively capturing visual details.

Conclusion: Integrating semantic generation into vision-language models provides significant advantages for understanding and data efficiency.

Abstract: Vision-language large models are moving toward the unification of visual understanding and visual generation tasks. However, whether generation can enhance understanding is still under-explored on large data scale. In this work, we analysis the unified model with a concise structure, UniHetero, under large-scale pretraining (>200M samples). Our key observations are: (1) Generation can improve understanding, but Only if you generate Semantics, Not Pixels. (2) Generation reveals a superior Data Scaling trend and higher Data Utilization. (3) Autoregression on Input Embedding is effective to capture visual details.

</details>


### [108] [Single LLM Debate, MoLaCE: Mixture of Latent Concept Experts Against Confirmation Bias](https://arxiv.org/abs/2512.23518)
*Hazel Kim,Philip Torr*

Main category: cs.CL

TL;DR: Paper introduces a framework called Mixture of Latent Concept Experts (MoLaCE) to mitigate confirmation bias in large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: LLMs often amplify input confirmation bias, and this issue is critical in systems like multi-agent debate where echo chambers emerge, necessitating a solution for bias correction.

Method: MoLaCE uses expert mixing based on activation strengths over latent concepts influenced by prompt phrasing during inference time, allowing for scalable, computationally efficient reduction of bias.

Result: The approach reduces confirmation bias, enhances robustness, and competes or exceeds multi-agent debate while requiring significantly less computational resources.

Conclusion: MoLaCE is a promising solution for addressing confirmation bias in LLMs, scalable for multi-agent frameworks and efficient in computational cost.

Abstract: Large language models (LLMs) are highly vulnerable to input confirmation bias. When a prompt implies a preferred answer, models often reinforce that bias rather than explore alternatives. This phenomenon remains underexplored, yet it is already harmful in base models and poses an even greater risk in multi-agent debate, where echo chambers reinforce bias instead of correction. We introduce Mixture of Latent Concept Experts (MoLaCE), a lightweight inference-time framework that addresses confirmation bias by mixing experts instantiated as different activation strengths over latent concepts that shape model responses. Our key insight is that, due to the compositional nature of language, differently phrased prompts reweight latent concepts in prompt-specific ways that affect factual correctness, so no single fixed intervention can be applied universally across inputs. This design enables a single LLM to emulate the benefits of debate internally while remaining computationally efficient and scalable. It can also be integrated into multi-agent debate frameworks to diversify perspectives and reduce correlated errors. We empirically show that it consistently reduces confirmation bias, improves robustness, and matches or surpasses multi-agent debate while requiring only a fraction of the computation.

</details>


### [109] [Lie to Me: Knowledge Graphs for Robust Hallucination Self-Detection in LLMs](https://arxiv.org/abs/2512.23547)
*Sahil Kale,Antonio Luca Alfeo*

Main category: cs.CL

TL;DR: The paper presents a method using knowledge graphs to enhance hallucination self-detection in large language models (LLMs), resulting in up to 16% improvement in accuracy compared to previous approaches.


<details>
  <summary>Details</summary>
Motivation: Hallucinations generated by LLMs pose safety concerns, necessitating better methods for self-detection to ensure trustworthy language models.

Method: The approach converts LLM-generated responses into structured knowledge graphs (entities and relations) and uses them to assess the likelihood of hallucinations.

Result: The proposed method, tested on GPT-4o and Gemini-2.5-Flash, shows significant improvements (up to 16% accuracy and 20% F1-score) over existing methods.

Conclusion: Structuring LLM responses as knowledge graphs enhances hallucination detection effectively and offers a model-agnostic, cost-efficient path toward building safer LLMs.

Abstract: Hallucinations, the generation of apparently convincing yet false statements, remain a major barrier to the safe deployment of LLMs. Building on the strong performance of self-detection methods, we examine the use of structured knowledge representations, namely knowledge graphs, to improve hallucination self-detection. Specifically, we propose a simple yet powerful approach that enriches hallucination self-detection by (i) converting LLM responses into knowledge graphs of entities and relations, and (ii) using these graphs to estimate the likelihood that a response contains hallucinations. We evaluate the proposed approach using two widely used LLMs, GPT-4o and Gemini-2.5-Flash, across two hallucination detection datasets. To support more reliable future benchmarking, one of these datasets has been manually curated and enhanced and is released as a secondary outcome of this work. Compared to standard self-detection methods and SelfCheckGPT, a state-of-the-art approach, our method achieves up to 16% relative improvement in accuracy and 20% in F1-score. Our results show that LLMs can better analyse atomic facts when they are structured as knowledge graphs, even when initial outputs contain inaccuracies. This low-cost, model-agnostic approach paves the way toward safer and more trustworthy language models.

</details>


### [110] [Instruction-Following Evaluation of Large Vision-Language Models](https://arxiv.org/abs/2512.23572)
*Daiki Shiono,Shumpei Miyawaki,Ryota Tanaka,Jun Suzuki*

Main category: cs.CL

TL;DR: Large vision-language models (LVLMs) often face a decline in instruction-following capabilities post-fine-tuning. This research highlights how specifying output format during training can mitigate this issue.


<details>
  <summary>Details</summary>
Motivation: To address the issue of declining instruction-following abilities in LVLMs post-fine-tuning and understand its causes.

Method: The authors constructed new training datasets that emphasize output format specifications and evaluated the effects of this explicit instruction during fine-tuning.

Result: Quantitative analysis confirmed the decline in instruction-following capability post-fine-tuning with standard datasets. Including output format instructions during training improves accuracy.

Conclusion: Incorporating output format-specific instructions during fine-tuning mitigates the deterioration in LVLM instruction-following abilities and enhances task performance.

Abstract: Following the initial flourishing of large language models (LLMs), there has been a surge in proposed large vision-language models (LVLMs) that integrate LLMs with vision capabilities. However, it has been observed that LVLMs, after tuning to visual instruction using commonly used training datasets, often fail to exhibit the instruction-following ability that was present in the LLM before integration, leading to results in which they do not follow task instructions as expected. This study quantitatively demonstrates that LVLMs' instruction-following ability declines after fine-tuning and analyzes its underlying causes. In particular, we constructed new training datasets highlighting whether the output format is specified. Then, we investigated how explicitly indicating the output format during fine-tuning affects LVLMs' instruction-following ability. Our quantitative evaluation confirmed that LVLMs' instruction-following ability declines after fine-tuning with commonly used datasets. Furthermore, we found that LVLMs trained with datasets, including instructions on output format, tend to follow instructions more accurately than models that do not. These findings suggest that including samples with instructions on output format during (visual) instruction tuning may help mitigate the decline in instruction-following abilities.

</details>


### [111] [Style Amnesia: Investigating Speaking Style Degradation and Mitigation in Multi-Turn Spoken Language Models](https://arxiv.org/abs/2512.23578)
*Yu-Xiang Lin,Cheng-Han Chiang,Hung-yi Lee*

Main category: cs.CL

TL;DR: This paper identifies and examines the issue of style amnesia in spoken language models (SLMs), where they fail to maintain instructed speaking styles in multi-turn conversations.


<details>
  <summary>Details</summary>
Motivation: To investigate the capability of spoken language models to retain instructed paralinguistic speaking styles during multi-turn interactions, addressing a key limitation in their conversational consistency.

Method: The authors evaluated five spoken language models (three proprietary and two open-source) on their ability to maintain specific speaking styles, such as emotion, accent, volume, and speaking speed. They also studied whether prompting strategies, including asking the model to recall style instructions, could mitigate style amnesia.

Result: None of the evaluated SLMs consistently maintained instructed speaking styles throughout conversations. However, explicitly asking them to recall the style instruction helped partially mitigate this issue, while different prompting strategies, like using system messages, proved less effective.

Conclusion: SLMs exhibit style amnesia, failing to maintain speaking styles across multi-turn conversations. Explicit intervention can partially address this limitation, but challenges remain in ensuring fidelity to instructions, particularly with system prompts.

Abstract: In this paper, we show that when spoken language models (SLMs) are instructed to speak in a specific speaking style at the beginning of a multi-turn conversation, they cannot maintain the required speaking styles after several turns of interaction; we refer to this as the style amnesia of SLMs. We focus on paralinguistic speaking styles, including emotion, accent, volume, and speaking speed. We evaluate three proprietary and two open-source SLMs, demonstrating that none of these models can maintain a consistent speaking style when instructed to do so. We further show that when SLMs are asked to recall the style instruction in later turns, they can recall the style instruction, but they fail to express it throughout the conversation. We also show that explicitly asking the model to recall the style instruction can partially mitigate style amnesia. In addition, we examine various prompting strategies and find that SLMs struggle to follow the required style when the instruction is placed in system messages rather than user messages, which contradicts the intended function of system prompts.

</details>


### [112] [Close the Loop: Synthesizing Infinite Tool-Use Data via Multi-Agent Role-Playing](https://arxiv.org/abs/2512.23611)
*Yuwen Li,Wei Zhang,Zelong Huang,Mason Yang,Jiajun Wu,Shawn Guo,Huahao Hu,Lingyi Sun,Jian Yang,Mingjie Tang,Byran Dai*

Main category: cs.CL

TL;DR: The paper presents InfTool, an autonomous framework to enhance LLMs' capability to invoke external tools efficiently, using self-evolving multi-agent synthesis to overcome annotation costs, generalization issues, and biases.


<details>
  <summary>Details</summary>
Motivation: Current LLMs struggle to reliably use external tools due to reliance on expensive human annotations, poor generalization to untrained tools, and limitations in existing single-model systems that lead to biases and incomplete outcomes.

Method: InfTool uses three collaborative agents (User Simulator, Tool-Calling Assistant, and MCP Server) to autonomously generate and verify diverse tool interaction scenarios. These scenarios train the model through Group Relative Policy Optimization (GRPO), iteratively improving performance without human involvement.

Result: InfTool improves a base 32B model's accuracy from 19.8% to 70.9% on the Berkeley Function-Calling Leaderboard (BFCL), outperforming much larger models and achieving performance comparable to Claude-Opus using only synthetic data.

Conclusion: InfTool demonstrates the potential of self-evolving frameworks in training LLMs to interact with external tools efficiently and effectively, bypassing the limitations of human data annotation.

Abstract: Enabling Large Language Models (LLMs) to reliably invoke external tools remains a critical bottleneck for autonomous agents. Existing approaches suffer from three fundamental challenges: expensive human annotation for high-quality trajectories, poor generalization to unseen tools, and quality ceilings inherent in single-model synthesis that perpetuate biases and coverage gaps. We introduce InfTool, a fully autonomous framework that breaks these barriers through self-evolving multi-agent synthesis. Given only raw API specifications, InfTool orchestrates three collaborative agents (User Simulator, Tool-Calling Assistant, and MCP Server) to generate diverse, verified trajectories spanning single-turn calls to complex multi-step workflows. The framework establishes a closed loop: synthesized data trains the model via Group Relative Policy Optimization (GRPO) with gated rewards, the improved model generates higher-quality data targeting capability gaps, and this cycle iterates without human intervention. Experiments on the Berkeley Function-Calling Leaderboard (BFCL) demonstrate that InfTool transforms a base 32B model from 19.8% to 70.9% accuracy (+258%), surpassing models 10x larger and rivaling Claude-Opus, and entirely from synthetic data without human annotation.

</details>


### [113] [A Dataset and Benchmark for Consumer Healthcare Question Summarization](https://arxiv.org/abs/2512.23637)
*Abhishek Basu,Deepak Gupta,Dina Demner-Fushman,Shweta Yadav*

Main category: cs.CL

TL;DR: The paper introduces CHQ-Summ, a dataset of 1507 expert-annotated consumer health questions and summaries, to enhance health-related text summarization.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitation in summarizing consumer health-related questions due to their descriptive nature and the lack of expert-annotated datasets.

Method: The authors created CHQ-Summ using domain-expert annotations of community health question forums and benchmarked it on existing summarization models.

Result: CHQ-Summ demonstrates its utility as an effective dataset for evaluating state-of-the-art summarization systems.

Conclusion: This work provides a valuable resource for advancing summarization in consumer health questions by introducing CHQ-Summ and shows its viability through benchmarking.

Abstract: The quest for seeking health information has swamped the web with consumers health-related questions. Generally, consumers use overly descriptive and peripheral information to express their medical condition or other healthcare needs, contributing to the challenges of natural language understanding. One way to address this challenge is to summarize the questions and distill the key information of the original question. Recently, large-scale datasets have significantly propelled the development of several summarization tasks, such as multi-document summarization and dialogue summarization. However, a lack of a domain-expert annotated dataset for the consumer healthcare questions summarization task inhibits the development of an efficient summarization system. To address this issue, we introduce a new dataset, CHQ-Sum,m that contains 1507 domain-expert annotated consumer health questions and corresponding summaries. The dataset is derived from the community question answering forum and therefore provides a valuable resource for understanding consumer health-related posts on social media. We benchmark the dataset on multiple state-of-the-art summarization models to show the effectiveness of the dataset

</details>


### [114] [Nested Browser-Use Learning for Agentic Information Seeking](https://arxiv.org/abs/2512.23647)
*Baixuan Li,Jialong Wu,Wenbiao Yin,Kuan Li,Zhongwang Zhang,Huifeng Yin,Zhengwei Tao,Liwen Zhang,Pengjun Xie,Jingren Zhou,Yong Jiang*

Main category: cs.CL

TL;DR: The paper introduces "NestBrowse," a framework for enabling agents to utilize full browser interaction effectively.


<details>
  <summary>Details</summary>
Motivation: Current information-seeking agents are limited by their inability to interact deeply with web pages, which restricts their access to richer information available through browsing.

Method: NestBrowse provides a nested and minimal structure for browser interaction that decouples control and page exploration, optimizing agent reasoning and performance.

Result: Empirical tests show that NestBrowse outperforms on deep information-seeking benchmarks, enabling efficient and flexible web information acquisition.

Conclusion: NestBrowse bridges the gap between snippet-level tools and full browser interaction, unlocking new potential for deep web exploration and improving agent capability.

Abstract: Information-seeking (IS) agents have achieved strong performance across a range of wide and deep search tasks, yet their tool use remains largely restricted to API-level snippet retrieval and URL-based page fetching, limiting access to the richer information available through real browsing. While full browser interaction could unlock deeper capabilities, its fine-grained control and verbose page content returns introduce substantial complexity for ReAct-style function-calling agents. To bridge this gap, we propose Nested Browser-Use Learning (NestBrowse), which introduces a minimal and complete browser-action framework that decouples interaction control from page exploration through a nested structure. This design simplifies agentic reasoning while enabling effective deep-web information acquisition. Empirical results on challenging deep IS benchmarks demonstrate that NestBrowse offers clear benefits in practice. Further in-depth analyses underscore its efficiency and flexibility.

</details>


### [115] [Less is more: Probabilistic reduction is best explained by small-scale predictability measures](https://arxiv.org/abs/2512.23659)
*Cassandra L. Jacobs,Andrés Buxó-Lugo,Anna K. Taylor,Marie Leopold-Hooke*

Main category: cs.CL

TL;DR: The paper explores the context required to study the link between language model probabilities and cognitive phenomena, proposing n-grams instead of whole utterances for analysis.


<details>
  <summary>Details</summary>
Motivation: To determine how much linguistic context is necessary to analyze the relationship between language model probabilities and cognitive processes.

Method: The study examines the sufficiency of n-gram representations in capturing key cognitive planning phenomena, replacing the need for whole utterance analysis.

Result: Findings confirm that n-gram representations are adequate cognitive units of planning and probabilistic reduction does not require whole utterance contexts.

Conclusion: This paper concludes that n-grams effectively represent cognitive planning units, simplifying the approach for studying language models and cognitive phenomena.

Abstract: The primary research questions of this paper center on defining the amount of context that is necessary and/or appropriate when investigating the relationship between language model probabilities and cognitive phenomena. We investigate whether whole utterances are necessary to observe probabilistic reduction and demonstrate that n-gram representations suffice as cognitive units of planning.

</details>


### [116] [Multilingual Hidden Prompt Injection Attacks on LLM-Based Academic Reviewing](https://arxiv.org/abs/2512.23684)
*Panagiotis Theocharopoulos,Ajinkya Kulkarni,Mathew Magimai. -Doss*

Main category: cs.CL

TL;DR: The paper investigates vulnerabilities in LLM-based review systems to hidden prompt injection attacks in academic papers across multiple languages, revealing significant alterations in review outcomes for English, Japanese, and Chinese inputs.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the increasing use of LLMs in critical processes like academic peer review and the need to understand their susceptibility to document-level adversarial attacks.

Method: The authors created a dataset of approximately 500 accepted ICML academic papers and embedded hidden adversarial prompts in four languages within these documents. Reviews were conducted using an LLM to evaluate the impact of the injections.

Result: Prompt injections in three languages (English, Japanese, Chinese) significantly altered review scores and decisions, while Arabic injections exhibited minimal impact.

Conclusion: LLM-based review systems are vulnerable to adversarial prompt injections, with variations in susceptibility across different languages, underscoring the need for increased robustness in such systems.

Abstract: Large language models (LLMs) are increasingly considered for use in high-impact workflows, including academic peer review. However, LLMs are vulnerable to document-level hidden prompt injection attacks. In this work, we construct a dataset of approximately 500 real academic papers accepted to ICML and evaluate the effect of embedding hidden adversarial prompts within these documents. Each paper is injected with semantically equivalent instructions in four different languages and reviewed using an LLM. We find that prompt injection induces substantial changes in review scores and accept/reject decisions for English, Japanese, and Chinese injections, while Arabic injections produce little to no effect. These results highlight the susceptibility of LLM-based reviewing systems to document-level prompt injection and reveal notable differences in vulnerability across languages.

</details>


### [117] [PROFASR-BENCH: A Benchmark for Context-Conditioned ASR in High-Stakes Professional Speech](https://arxiv.org/abs/2512.23686)
*Deepak Babu Piskala*

Main category: cs.CL

TL;DR: The paper introduces ProfASR-Bench, an evaluation suite focusing on challenging professional speech recognition scenarios with dense domain vocabulary, formal variances, and minimal critical entity mistakes.


<details>
  <summary>Details</summary>
Motivation: Existing ASR benchmarks fail to adequately address challenges like dense terminology, formal register variation, and high criticality of errors in professional domains.

Method: ProfASR-Bench pairs natural-language prompts with entity-rich targets for measuring ASR performance, with metrics that include entity-aware scores and subset reporting based on attributes like accent and gender.

Result: The authors find that current ASR models exhibit a 'context-utilization gap,' where textual context has minimal effect on improving word error rates, even in ideal conditions.

Conclusion: ProfASR-Bench offers a framework to study and improve ASR systems, providing specific tools like entity-aware reporting and reproducible evaluations under varying context conditions.

Abstract: Automatic Speech Recognition (ASR) in professional settings faces challenges that existing benchmarks underplay: dense domain terminology, formal register variation, and near-zero tolerance for critical entity errors. We present ProfASR-Bench, a professional-talk evaluation suite for high-stakes applications across finance, medicine, legal, and technology. Each example pairs a natural-language prompt (domain cue and/or speaker profile) with an entity-rich target utterance, enabling controlled measurement of context-conditioned recognition. The corpus supports conventional ASR metrics alongside entity-aware scores and slice-wise reporting by accent and gender. Using representative families Whisper (encoder-decoder ASR) and Qwen-Omni (audio language models) under matched no-context, profile, domain+profile, oracle, and adversarial conditions, we find a consistent pattern: lightweight textual context produces little to no change in average word error rate (WER), even with oracle prompts, and adversarial prompts do not reliably degrade performance. We term this the context-utilization gap (CUG): current systems are nominally promptable yet underuse readily available side information. ProfASR-Bench provides a standardized context ladder, entity- and slice-aware reporting with confidence intervals, and a reproducible testbed for comparing fusion strategies across model families.
  Dataset: https://huggingface.co/datasets/prdeepakbabu/ProfASR-Bench
  Code: https://github.com/prdeepakbabu/ProfASR-Bench

</details>


### [118] [Fine-Tuning LLMs with Fine-Grained Human Feedback on Text Spans](https://arxiv.org/abs/2512.23693)
*Sky CH-Wang,Justin Svegliato,Helen Appel,Jason Eisner*

Main category: cs.CL

TL;DR: The paper introduces a method for fine-tuning language models using feedback-driven, incremental improvements based on user preferences, producing better model alignment results.


<details>
  <summary>Details</summary>
Motivation: To improve language model alignment through structured feedback-driven methods rather than using standard coarse-grained alignment approaches.

Method: The method involves users providing detailed feedback on "liked" and "disliked" portions of model responses, followed by the model making targeted, incremental rewrites to improve, based on the feedback.

Result: Their approach surpasses traditional A/B preference ranking or full contrastive rewrites in terms of efficiency and effectiveness for preference tuning.

Conclusion: Structured, revision-based preference supervision provides a significantly improved method for aligning language models with user preferences compared to standard methods.

Abstract: We present a method and dataset for fine-tuning language models with preference supervision using feedback-driven improvement chains. Given a model response, an annotator provides fine-grained feedback by marking ``liked'' and ``disliked'' spans and specifying what they liked or disliked about them. The base model then rewrites the disliked spans accordingly, proceeding from left to right, forming a sequence of incremental improvements. We construct preference pairs for direct alignment from each adjacent step in the chain, enabling the model to learn from localized, targeted edits. We find that our approach outperforms direct alignment methods based on standard A/B preference ranking or full contrastive rewrites, demonstrating that structured, revision-based supervision leads to more efficient and effective preference tuning.

</details>


### [119] [Eliciting Behaviors in Multi-Turn Conversations](https://arxiv.org/abs/2512.23701)
*Jing Huang,Shujian Zhang,Lun Wang,Andrew Hard,Rajiv Mathews,John Lambert*

Main category: cs.CL

TL;DR: The paper analyzes methods for identifying behaviors in conversational large language models (LLMs) across multi-turn settings, presenting a novel framework and evaluating their efficiency for behavior-eliciting inputs.


<details>
  <summary>Details</summary>
Motivation: There is a need to evaluate complex behaviors of LLMs in conversational, multi-turn settings, as existing benchmarks largely focus on single-turn interactions.

Method: The authors propose an analytical framework categorizing behavior elicitation methods and introduce a generalized multi-turn methodology that unifies single- and multi-turn scenarios. They evaluate methods' efficiency via a trade-off analysis between query budget and success rate.

Result: The study finds that online methods achieve significantly higher success rates (e.g., 77%) than static methods using fewer queries, even where benchmarks fail to detect failure cases.

Conclusion: The study emphasizes the potential of online behavior elicitation techniques in multi-turn dialogue settings and advocates for dynamic benchmarks over static ones for better LLM evaluation.

Abstract: Identifying specific and often complex behaviors from large language models (LLMs) in conversational settings is crucial for their evaluation. Recent work proposes novel techniques to find natural language prompts that induce specific behaviors from a target model, yet they are mainly studied in single-turn settings. In this work, we study behavior elicitation in the context of multi-turn conversations. We first offer an analytical framework that categorizes existing methods into three families based on their interactions with the target model: those that use only prior knowledge, those that use offline interactions, and those that learn from online interactions. We then introduce a generalized multi-turn formulation of the online method, unifying single-turn and multi-turn elicitation. We evaluate all three families of methods on automatically generating multi-turn test cases. We investigate the efficiency of these approaches by analyzing the trade-off between the query budget, i.e., the number of interactions with the target model, and the success rate, i.e., the discovery rate of behavior-eliciting inputs. We find that online methods can achieve an average success rate of 45/19/77% with just a few thousand queries over three tasks where static methods from existing multi-turn conversation benchmarks find few or even no failure cases. Our work highlights a novel application of behavior elicitation methods in multi-turn conversation evaluation and the need for the community to move towards dynamic benchmarks.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [120] [Characterizing Motion Encoding in Video Diffusion Timesteps](https://arxiv.org/abs/2512.22175)
*Vatsal Baherwani,Yixuan Ren,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: This paper analyzes how motion and appearance are encoded in text-to-video diffusion models and presents a method to better utilize motion-dominant timesteps for improved video synthesis.


<details>
  <summary>Details</summary>
Motivation: The study aims to systematically understand how motion and appearance are encoded across timesteps in text-to-video diffusion models.

Method: The researchers use a proxy to measure motion encoding by studying the trade-off between motion preservation and appearance editing across different timestep ranges through a quantitative study.

Result: The study identifies an early motion-dominant regime and a later appearance-dominant regime in the diffusion process, defining a motion-appearance boundary in timestep space. They further simplify motion customization to focus on the motion-dominant regime.

Conclusion: The paper transforms a heuristic about motion and appearance encoding into a principled method, enabling more effective spatiotemporal disentanglement and improved motion customization in video synthesis.

Abstract: Text-to-video diffusion models synthesize temporal motion and spatial appearance through iterative denoising, yet how motion is encoded across timesteps remains poorly understood. Practitioners often exploit the empirical heuristic that early timesteps mainly shape motion and layout while later ones refine appearance, but this behavior has not been systematically characterized. In this work, we proxy motion encoding in video diffusion timesteps by the trade-off between appearance editing and motion preservation induced when injecting new conditions over specified timestep ranges, and characterize this proxy through a large-scale quantitative study. This protocol allows us to factor motion from appearance by quantitatively mapping how they compete along the denoising trajectory. Across diverse architectures, we consistently identify an early, motion-dominant regime and a later, appearance-dominant regime, yielding an operational motion-appearance boundary in timestep space. Building on this characterization, we simplify current one-shot motion customization paradigm by restricting training and inference to the motion-dominant regime, achieving strong motion transfer without auxiliary debiasing modules or specialized objectives. Our analysis turns a widely used heuristic into a spatiotemporal disentanglement principle, and our timestep-constrained recipe can serve as ready integration into existing motion transfer and editing methods.

</details>


### [121] [Real-Time American Sign Language Recognition Using 3D Convolutional Neural Networks and LSTM: Architecture, Training, and Deployment](https://arxiv.org/abs/2512.22177)
*Dawnena Key*

Main category: cs.CV

TL;DR: This paper introduces a real-time ASL recognition system utilizing hybrid deep learning (3D CNN + LSTM) to process video streams and identify ASL signs effectively.


<details>
  <summary>Details</summary>
Motivation: To address communication challenges faced by over 70 million deaf and hard-of-hearing individuals by enabling automated recognition of ASL through real-time video processing.

Method: The system uses a hybrid model combining 3D CNNs to capture spatial-temporal features and LSTM networks to model sequential dependencies in ASL gestures. Training utilized datasets like WLASL, ASL-LEX, and an expert-annotated set of signs.

Result: The system achieved F1-scores between 0.71 and 0.99 across classes, and it was successfully deployed on AWS and OAK-D cameras for real-time inference.

Conclusion: This system demonstrates the potential of hybrid deep learning models for ASL recognition and provides practical solutions for enhancing accessibility for the deaf community.

Abstract: This paper presents a real-time American Sign Language (ASL) recognition system utilizing a hybrid deep learning architecture combining 3D Convolutional Neural Networks (3D CNN) with Long Short-Term Memory (LSTM) networks. The system processes webcam video streams to recognize word-level ASL signs, addressing communication barriers for over 70 million deaf and hard-of-hearing individuals worldwide. Our architecture leverages 3D convolutions to capture spatial-temporal features from video frames, followed by LSTM layers that model sequential dependencies inherent in sign language gestures. Trained on the WLASL dataset (2,000 common words), ASL-LEX lexical database (~2,700 signs), and a curated set of 100 expert-annotated ASL signs, the system achieves F1-scores ranging from 0.71 to 0.99 across sign classes. The model is deployed on AWS infrastructure with edge deployment capability on OAK-D cameras for real-time inference. We discuss the architecture design, training methodology, evaluation metrics, and deployment considerations for practical accessibility applications.

</details>


### [122] [Enhancing Medical Data Analysis through AI-Enhanced Locally Linear Embedding: Applications in Medical Point Location and Imagery](https://arxiv.org/abs/2512.22182)
*Hassan Khalid,Muhammad Mahad Khaliq,Muhammad Jawad Bashir*

Main category: cs.CV

TL;DR: The paper presents an AI-enhanced Locally Linear Embedding (LLE) model to optimize high-dimensional medical data processing, improving accuracy and efficiency in billing and transcription.


<details>
  <summary>Details</summary>
Motivation: To leverage AI enhancements for solving inefficiencies and inaccuracies in medical billing and transcription by effectively handling high-dimensional medical data.

Method: The study integrates Artificial Intelligence with Locally Linear Embedding (LLE) to create a mathematical model aimed at automating medical billing and transcription processes, validated through experiments.

Result: The model achieved significant improvements in data processing accuracy and operational efficiency within the healthcare domain, as observed in experimental settings.

Conclusion: AI-enhanced LLE demonstrates great potential for advancing medical data analysis, paving the way for further research and broader applications in healthcare.

Abstract: The rapid evolution of Artificial intelligence in healthcare has opened avenues for enhancing various processes, including medical billing and transcription. This paper introduces an innovative approach by integrating AI with Locally Linear Embedding (LLE) to revolutionize the handling of high-dimensional medical data. This AI-enhanced LLE model is specifically tailored to improve the accuracy and efficiency of medical billing systems and transcription services. By automating these processes, the model aims to reduce human error and streamline operations, thereby facilitating faster and more accurate patient care documentation and financial transactions. This paper provides a comprehensive mathematical model of AI-enhanced LLE, demonstrating its application in real-world healthcare scenarios through a series of experiments. The results indicate a significant improvement in data processing accuracy and operational efficiency. This study not only underscores the potential of AI-enhanced LLE in medical data analysis but also sets a foundation for future research into broader healthcare applications.

</details>


### [123] [Unbiased Visual Reasoning with Controlled Visual Inputs](https://arxiv.org/abs/2512.22183)
*Zhaonan Li,Shijie Lu,Fei Wang,Jacob Dineen,Xiao Ye,Zhikun Xu,Siyi Liu,Young Min Cho,Bangzheng Li,Daniel Chang,Kenny Nguyen,Qizheng Yang,Muhao Chen,Ben Zhou*

Main category: cs.CV

TL;DR: The paper proposes VISTA, a system that separates visual perception from reasoning to improve robustness and reduce bias in Vision-Language Models (VLMs).


<details>
  <summary>Details</summary>
Motivation: To address the problem of Vision-Language Models exploiting shortcuts and spurious correlations rather than grounding answers in causal visual evidence.

Method: The authors present VISTA, a framework that uses modularity by separating perception and reasoning tasks, leveraging a frozen visual sensor for perception and a text-only reasoner for reasoning and aggregation. Training is performed using reinforcement learning.

Result: VISTA significantly improves robustness against spurious correlations (+16.29% on SpuriVerse with Qwen-2.5-VL-7B and +6.77% with Llama-3.2-Vision-11B) and demonstrates competitive performance across other benchmarks while recognizing and recovering from VLM perception failures.

Conclusion: VISTA effectively mitigates reasoning biases and exploits causal visual evidence, demonstrating neutral and grounded reasoning traces compared to baseline Vision-Language Models.

Abstract: End-to-end Vision-language Models (VLMs) often answer visual questions by exploiting spurious correlations instead of causal visual evidence, and can become more shortcut-prone when fine-tuned. We introduce VISTA (Visual-Information Separation for Text-based Analysis), a modular framework that decouples perception from reasoning via an explicit information bottleneck. A frozen VLM sensor is restricted to short, objective perception queries, while a text-only LLM reasoner decomposes each question, plans queries, and aggregates visual facts in natural language. This controlled interface defines a reward-aligned environment for training unbiased visual reasoning with reinforcement learning. Instantiated with Qwen2.5-VL and Llama3.2-Vision sensors, and trained with GRPO from only 641 curated multi-step questions, VISTA significantly improves robustness to real-world spurious correlations on SpuriVerse (+16.29% with Qwen-2.5-VL-7B and +6.77% with Llama-3.2-Vision-11B), while remaining competitive on MMVP and a balanced SeedBench subset. VISTA transfers robustly across unseen VLM sensors and is able to recognize and recover from VLM perception failures. Human analysis further shows that VISTA's reasoning traces are more neutral, less reliant on spurious attributes, and more explicitly grounded in visual evidence than end-to-end VLM baselines.

</details>


### [124] [SAMM2D: Scale-Aware Multi-Modal 2D Dual-Encoder for High-Sensitivity Intracrania Aneurysm Screening](https://arxiv.org/abs/2512.22185)
*Antara Titikhsha,Divyanshu Tak*

Main category: cs.CV

TL;DR: SAMM2D is a dual-encoder framework achieving significant improvements in aneurysm detection, challenging the conventional reliance on augmentation in low-data medical cases.


<details>
  <summary>Details</summary>
Motivation: To address challenges in aneurysm detection such as subtle morphology, class imbalance, and limited annotated data, aiming to enhance diagnostic accuracy and clinical efficiency.

Method: Introduced SAMM2D, a dual-encoder architecture, validated through AUC analysis and multiple augmentation experiments on the RSNA dataset.

Result: SAMM2D achieved an AUC of 0.686, outperforming clinical baselines by 32%. Unaugmented models surpassed augmented counterparts, highlighting redundancy in additional augmentations for pretrained networks. Decision threshold adjustments improved sensitivity to 95%, exceeding radiologist performance, with significant economic benefits projected.

Conclusion: Medical imaging workflows may benefit more from robust pretraining strategies rather than complex augmentation pipelines, providing clinically impactful results and cost savings.

Abstract: Effective aneurysm detection is essential to avert life-threatening hemorrhages, but it remains challenging due to the subtle morphology of the aneurysm, pronounced class imbalance, and the scarcity of annotated data. We introduce SAMM2D, a dual-encoder framework that achieves an AUC of 0.686 on the RSNA intracranial aneurysm dataset; an improvement of 32% over the clinical baseline. In a comprehensive ablation across six augmentation regimes, we made a striking discovery: any form of data augmentation degraded performance when coupled with a strong pretrained backbone. Our unaugmented baseline model outperformed all augmented variants by 1.75--2.23 percentage points (p < 0.01), overturning the assumption that "more augmentation is always better" in low-data medical settings. We hypothesize that ImageNet-pretrained features already capture robust invariances, rendering additional augmentations both redundant and disruptive to the learned feature manifold. By calibrating the decision threshold, SAMM2D reaches 95% sensitivity, surpassing average radiologist performance, and translates to a projected \$13.9M in savings per 1,000 patients in screening applications. Grad-CAM visualizations confirm that 85% of true positives attend to relevant vascular regions (62% IoU with expert annotations), demonstrating the model's clinically meaningful focus. Our results suggest that future medical imaging workflows could benefit more from strong pretraining than from increasingly complex augmentation pipelines.

</details>


### [125] [HookMIL: Revisiting Context Modeling in Multiple Instance Learning for Computational Pathology](https://arxiv.org/abs/2512.22188)
*Xitong Ling,Minxi Ouyang,Xiaoxiao Li,Jiawen Li,Ying Chen,Yuxuan Sun,Xinrui Chen,Tian Guan,Xiaoping Liu,Yonghong He*

Main category: cs.CV

TL;DR: HookMIL is a new MIL framework designed for computational pathology, improving efficiency and context-awareness using compact learnable tokens.


<details>
  <summary>Details</summary>
Motivation: Traditional MIL loses contextual information, while transformer-based methods, though expressive, are computationally costly. HookMIL seeks to address these issues.

Method: HookMIL uses compact learnable hook tokens to aggregate context efficiently. These tokens are initialized with multimodal data such as key-patch features, text embeddings, or spatial features. Linear bidirectional attention is utilized along with a Hook Diversity Loss and a hook-to-hook communication mechanism.

Result: HookMIL achieves state-of-the-art performance in efficiency, interpretability, and accuracy on four public pathology datasets.

Conclusion: HookMIL represents a significant improvement in weakly supervised analysis for whole-slide images, balancing computational efficiency with enhanced representation and contextual understanding.

Abstract: Multiple Instance Learning (MIL) has enabled weakly supervised analysis of whole-slide images (WSIs) in computational pathology. However, traditional MIL approaches often lose crucial contextual information, while transformer-based variants, though more expressive, suffer from quadratic complexity and redundant computations. To address these limitations, we propose HookMIL, a context-aware and computationally efficient MIL framework that leverages compact, learnable hook tokens for structured contextual aggregation. These tokens can be initialized from (i) key-patch visual features, (ii) text embeddings from vision-language pathology models, and (iii) spatially grounded features from spatial transcriptomics-vision models. This multimodal initialization enables Hook Tokens to incorporate rich textual and spatial priors, accelerating convergence and enhancing representation quality. During training, Hook tokens interact with instances through bidirectional attention with linear complexity. To further promote specialization, we introduce a Hook Diversity Loss that encourages each token to focus on distinct histopathological patterns. Additionally, a hook-to-hook communication mechanism refines contextual interactions while minimizing redundancy. Extensive experiments on four public pathology datasets demonstrate that HookMIL achieves state-of-the-art performance, with improved computational efficiency and interpretability. Codes are available at https://github.com/lingxitong/HookMIL.

</details>


### [126] [Tiny-YOLOSAM: Fast Hybrid Image Segmentation](https://arxiv.org/abs/2512.22193)
*Kenneth Xu,Songhan Wu*

Main category: cs.CV

TL;DR: This paper introduces a lightweight segmentation model called TinySAM and proposes Tiny-YOLOSAM, a hybrid system combining YOLO with TinySAM for efficient scene segmentation.


<details>
  <summary>Details</summary>
Motivation: The Segment Anything Model (SAM) provides accurate segmentation but is computationally expensive in latency-critical scenarios.

Method: They replicate TinySAM benchmarks on COCO val2017, then propose a hybrid pipeline that uses YOLOv12 for box prompts and sparse point prompts to improve segmentation efficiency.

Result: The system enhances segmentation coverage (AR: 16.4% to 77.1%, mIoU: 19.2% to 67.8%) and reduces runtime significantly from 49.20s/image to 10.39s/image.

Conclusion: Detector-guided prompting with sparse sampling is shown as an effective method for practical full-scene segmentation, overcoming the computational limitations of dense segmentation models.

Abstract: The Segment Anything Model (SAM) enables promptable, high-quality segmentation but is often too computationally expensive for latency-critical settings. TinySAM is a lightweight, distilled SAM variant that preserves strong zero-shot mask quality, yet its "segment-everything" mode still requires hundreds of prompts and remains slow in practice. We first replicate TinySAM on COCO val2017 using official checkpoints, matching the reported AP within 0.03%, establishing a reliable experimental baseline. Building on this, we propose Tiny-YOLOSAM, a fast hybrid pipeline that uses a recent YOLO detector (YOLOv12) to generate box prompts for TinySAM on salient foreground objects, and supplements uncovered regions with sparse point prompts sampled only where YOLO-guided masks provide no coverage. On COCO val2017, the hybrid system substantially improves class-agnostic coverage (AR from 16.4% to 77.1%, mIoU from 19.2% to 67.8%) while reducing end-to-end runtime from 49.20s/image to 10.39s/image (4.7x) on an Apple M1 Pro CPU. These results suggest detector-guided prompting combined with targeted sparse sampling as an effective alternative to dense "segment-everything" prompting for practical full-scene segmentation.

</details>


### [127] [Quadrant Segmentation VLM with Few-Shot Adaptation and OCT Learning-based Explainability Methods for Diabetic Retinopathy](https://arxiv.org/abs/2512.22197)
*Shivum Telang*

Main category: cs.CV

TL;DR: This paper introduces a novel multimodal AI system for diabetic retinopathy (DR) detection that provides interpretable, quantitative results using fundus and OCT images, improving on prior one-dimensional models.


<details>
  <summary>Details</summary>
Motivation: Early detection of diabetic retinopathy is critical to prevent vision loss. However, limited access to physicians and impractical manual lesion annotation create challenges in using AI models for diagnosis. Current models lack comprehensive explainability and versatility.

Method: The paper presents a novel multimodal explainability model employing a vision-language model (VLM) with few-shot learning. It analyzes lesion distributions within retinal quadrants and uses Grad-CAM heatmaps to highlight regions linked to DR severity using both fundus and OCT images.

Result: The model processes a dataset of 3,000 fundus images and 1,000 OCT images, generating quantitative lesion analysis and improved interpretability for DR diagnostics.

Conclusion: This methodology enhances DR diagnostics by addressing explainability and data modality limitations, serving as a practical tool for screening, treatment, and research applications to improve patient outcomes.

Abstract: Diabetic Retinopathy (DR) is a leading cause of vision loss worldwide, requiring early detection to preserve sight. Limited access to physicians often leaves DR undiagnosed. To address this, AI models utilize lesion segmentation for interpretability; however, manually annotating lesions is impractical for clinicians. Physicians require a model that explains the reasoning for classifications rather than just highlighting lesion locations. Furthermore, current models are one-dimensional, relying on a single imaging modality for explainability and achieving limited effectiveness. In contrast, a quantitative-detection system that identifies individual DR lesions in natural language would overcome these limitations, enabling diverse applications in screening, treatment, and research settings. To address this issue, this paper presents a novel multimodal explainability model utilizing a VLM with few-shot learning, which mimics an ophthalmologist's reasoning by analyzing lesion distributions within retinal quadrants for fundus images. The model generates paired Grad-CAM heatmaps, showcasing individual neuron weights across both OCT and fundus images, which visually highlight the regions contributing to DR severity classification. Using a dataset of 3,000 fundus images and 1,000 OCT images, this innovative methodology addresses key limitations in current DR diagnostics, offering a practical and comprehensive tool for improving patient outcomes.

</details>


### [128] [TCFormer: A 5M-Parameter Transformer with Density-Guided Aggregation for Weakly-Supervised Crowd Counting](https://arxiv.org/abs/2512.22203)
*Qiang Guo,Rubo Zhang,Bingbing Zhang,Junjie Liu,Jianqing Liu*

Main category: cs.CV

TL;DR: The paper introduces TCFormer, a lightweight transformer-based framework for crowd counting under weak supervision, achieving competitive results using minimal computational resources.


<details>
  <summary>Details</summary>
Motivation: Current crowd counting methods rely on point-level annotations and resource-heavy frameworks, limiting scalability in resource-constrained settings.

Method: TCFormer uses an efficient vision transformer for feature extraction, combined with a Learnable Density-Weighted Averaging module and density-level classification loss to enable weakly-supervised training.

Result: TCFormer achieves competitive performance on benchmarks such as ShanghaiTech A/B, UCF-QNRF, and NWPU, balancing parameter efficiency with high accuracy.

Conclusion: TCFormer provides a scalable, efficient solution for crowd counting tasks in resource-constrained environments, utilizing weakly-supervised approaches effectively.

Abstract: Crowd counting typically relies on labor-intensive point-level annotations and computationally intensive backbones, restricting its scalability and deployment in resource-constrained environments. To address these challenges, this paper proposes the TCFormer, a tiny, ultra-lightweight, weakly-supervised transformer-based crowd counting framework with only 5 million parameters that achieves competitive performance. Firstly, a powerful yet efficient vision transformer is adopted as the feature extractor, the global context-aware capabilities of which provides semantic meaningful crowd features with a minimal memory footprint. Secondly, to compensate for the lack of spatial supervision, we design a feature aggregation mechanism termed the Learnable Density-Weighted Averaging module. This module dynamically re-weights local tokens according to predicted density scores, enabling the network to adaptively modulate regional features based on their specific density characteristics without the need for additional annotations. Furthermore, this paper introduces a density-level classification loss, which discretizes crowd density into distinct grades, thereby regularizing the training process and enhancing the model's classification power across varying levels of crowd density. Therefore, although TCformer is trained under a weakly-supervised paradigm utilizing only image-level global counts, the joint optimization of count and density-level losses enables the framework to achieve high estimation accuracy. Extensive experiments on four benchmarks including ShanghaiTech A/B, UCF-QNRF, and NWPU datasets demonstrate that our approach strikes a superior trade-off between parameter efficiency and counting accuracy and can be a good solution for crowd counting tasks in edge devices.

</details>


### [129] [A CNN-Based Malaria Diagnosis from Blood Cell Images with SHAP and LIME Explainability](https://arxiv.org/abs/2512.22205)
*Md. Ismiel Hossen Abir,Awolad Hossain*

Main category: cs.CV

TL;DR: This study proposes a custom CNN model for automatic malaria diagnosis, achieving high performance and utilizing Explainable AI techniques for better interpretability.


<details>
  <summary>Details</summary>
Motivation: Traditional malaria diagnostic methods lack sensitivity, require expert judgment, and need resources not accessible in remote areas.

Method: A custom Convolutional Neural Network (CNN) was developed for automatic classification of blood cell images, compared against existing architectures such as ResNet50 and VGG16. Explainable AI tools like SHAP, LIME, and Saliency Maps were used for interpretation.

Result: The model attained 96% accuracy, with precision and recall scores above 0.95 for both classes, showing superior performance compared to other deep learning architectures.

Conclusion: Deep learning approaches, particularly the proposed CNN, can revolutionize malaria diagnosis by providing rapid, accurate, and interpretable solutions, especially useful in resource-constrained regions.

Abstract: Malaria remains a prevalent health concern in regions with tropical and subtropical climates. The cause of malaria is the Plasmodium parasite, which is transmitted through the bites of infected female Anopheles mosquitoes. Traditional diagnostic methods, such as microscopic blood smear analysis, are low in sensitivity, depend on expert judgment, and require resources that may not be available in remote settings. To overcome these limitations, this study proposes a deep learning-based approach utilizing a custom Convolutional Neural Network (CNN) to automatically classify blood cell images as parasitized or uninfected. The model achieves an accuracy of 96%, with precision and recall scores exceeding 0.95 for both classes. This study also compares the custom CNN with established deep learning architectures, including ResNet50, VGG16, MobileNetV2, and DenseNet121. To enhance model interpretability, Explainable AI techniques such as SHAP, LIME, and Saliency Maps are applied. The proposed system shows how deep learning can provide quick, accurate and understandable malaria diagnosis, especially in areas with limited resources.

</details>


### [130] [Signal-SGN++: Topology-Enhanced Time-Frequency Spiking Graph Network for Skeleton-Based Action Recognition](https://arxiv.org/abs/2512.22214)
*Naichuan Zheng,Xiahai Lun,Weiyi Li,Yuchen Du*

Main category: cs.CV

TL;DR: The paper introduces Signal-SGN++, a topology-aware spiking graph network for action recognition that balances accuracy with energy efficiency.


<details>
  <summary>Details</summary>
Motivation: Graph Convolutional Networks (GCNs) are effective for action recognition but energy-intensive, while Spiking Neural Networks (SNNs) are energy-efficient but struggle with temporal-frequency and topological dependencies. The paper aims to integrate strengths of both.

Method: The authors propose Signal-SGN++, combining 1D Spiking Graph Convolution and Frequency Spiking Convolution with mechanisms like Topology-Shift Self-Attention and Multi-Scale Wavelet Transform Fusion to extract and fuse temporal, frequency, and topological features efficiently.

Result: Signal-SGN++ achieves competitive action recognition performance compared to state-of-the-art GCNs while significantly reducing energy consumption.

Conclusion: Signal-SGN++ bridges the efficiency gap between GCNs and SNNs, offering a promising solution for energy-efficient, topology-aware action recognition.

Abstract: Graph Convolutional Networks (GCNs) demonstrate strong capability in modeling skeletal topology for action recognition, yet their dense floating-point computations incur high energy costs. Spiking Neural Networks (SNNs), characterized by event-driven and sparse activation, offer energy efficiency but remain limited in capturing coupled temporal-frequency and topological dependencies of human motion. To bridge this gap, this article proposes Signal-SGN++, a topology-aware spiking graph framework that integrates structural adaptivity with time-frequency spiking dynamics. The network employs a backbone composed of 1D Spiking Graph Convolution (1D-SGC) and Frequency Spiking Convolution (FSC) for joint spatiotemporal and spectral feature extraction. Within this backbone, a Topology-Shift Self-Attention (TSSA) mechanism is embedded to adaptively route attention across learned skeletal topologies, enhancing graph-level sensitivity without increasing computational complexity. Moreover, an auxiliary Multi-Scale Wavelet Transform Fusion (MWTF) branch decomposes spiking features into multi-resolution temporal-frequency representations, wherein a Topology-Aware Time-Frequency Fusion (TATF) unit incorporates structural priors to preserve topology-consistent spectral fusion. Comprehensive experiments on large-scale benchmarks validate that Signal-SGN++ achieves superior accuracy-efficiency trade-offs, outperforming existing SNN-based methods and achieving competitive results against state-of-the-art GCNs under substantially reduced energy consumption.

</details>


### [131] [VLM-PAR: A Vision Language Model for Pedestrian Attribute Recognition](https://arxiv.org/abs/2512.22217)
*Abdellah Zakaria Sellam,Salah Eddine Bekhouche,Fadi Dornaika,Cosimo Distante,Abdenour Hadid*

Main category: cs.CV

TL;DR: VLM-PAR is a vision-language framework that improves pedestrian attribute recognition with a cross-attention fusion approach, achieving state-of-the-art performance on several benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address challenges in pedestrian attribute recognition, such as class imbalance, attribute co-dependencies, and domain shifts, by leveraging vision-language models.

Method: Developed a modular framework (VLM-PAR) based on frozen SigLIP 2 encoders and used cross-attention fusion to align image and prompt embeddings.

Result: Achieved state-of-the-art accuracy on the PA100K benchmark and significant gains in mean accuracy on PETA and Market-1501 benchmarks.

Conclusion: The framework demonstrates that integrating large-scale vision-language pretraining with tailored cross-modal refinement can effectively address challenges in PAR.

Abstract: Pedestrian Attribute Recognition (PAR) involves predicting fine-grained attributes such as clothing color, gender, and accessories from pedestrian imagery, yet is hindered by severe class imbalance, intricate attribute co-dependencies, and domain shifts. We introduce VLM-PAR, a modular vision-language framework built on frozen SigLIP 2 multilingual encoders. By first aligning image and prompt embeddings via refining visual features through a compact cross-attention fusion, VLM-PAR achieves significant accuracy improvement on the highly imbalanced PA100K benchmark, setting a new state-of-the-art performance, while also delivering significant gains in mean accuracy across PETA and Market-1501 benchmarks. These results underscore the efficacy of integrating large-scale vision-language pretraining with targeted cross-modal refinement to overcome imbalance and generalization challenges in PAR.

</details>


### [132] [Towards Signboard-Oriented Visual Question Answering: ViSignVQA Dataset, Method and Benchmark](https://arxiv.org/abs/2512.22218)
*Hieu Minh Nguyen,Tam Le-Thanh Dang,Kiet Van Nguyen*

Main category: cs.CV

TL;DR: The paper introduces ViSignVQA, a dataset for Vietnamese signboard Visual Question Answering (VQA), including 25,573 question-answer pairs from 10,762 images, and benchmarks models adapted for Vietnamese OCR and language features.


<details>
  <summary>Details</summary>
Motivation: The study addresses the underexplored area of understanding Vietnamese signboard text in VQA, a critical application particularly for low-resource languages.

Method: The researchers developed a dataset capturing linguistic and visual elements of Vietnamese signboards and adapted state-of-the-art VQA models with Vietnamese OCR and pretrained language models.

Result: OCR-enhanced context significantly improved model performance, with F1-score gains up to 209%. They also proposed a multi-agent VQA framework with GPT-4, achieving 75.98% accuracy.

Conclusion: This study introduces the first large-scale multimodal dataset for Vietnamese signboard understanding, emphasizing the value of domain-specific resources in advancing VQA for low-resource languages.

Abstract: Understanding signboard text in natural scenes is essential for real-world applications of Visual Question Answering (VQA), yet remains underexplored, particularly in low-resource languages. We introduce ViSignVQA, the first large-scale Vietnamese dataset designed for signboard-oriented VQA, which comprises 10,762 images and 25,573 question-answer pairs. The dataset captures the diverse linguistic, cultural, and visual characteristics of Vietnamese signboards, including bilingual text, informal phrasing, and visual elements such as color and layout. To benchmark this task, we adapted state-of-the-art VQA models (e.g., BLIP-2, LaTr, PreSTU, and SaL) by integrating a Vietnamese OCR model (SwinTextSpotter) and a Vietnamese pretrained language model (ViT5). The experimental results highlight the significant role of the OCR-enhanced context, with F1-score improvements of up to 209% when the OCR text is appended to questions. Additionally, we propose a multi-agent VQA framework combining perception and reasoning agents with GPT-4, achieving 75.98% accuracy via majority voting. Our study presents the first large-scale multimodal dataset for Vietnamese signboard understanding. This underscores the importance of domain-specific resources in enhancing text-based VQA for low-resource languages. ViSignVQA serves as a benchmark capturing real-world scene text characteristics and supporting the development and evaluation of OCR-integrated VQA models in Vietnamese.

</details>


### [133] [On Extending Semantic Abstraction for Efficient Search of Hidden Objects](https://arxiv.org/abs/2512.22220)
*Tasha Pais,Nikhilesh Belulkar*

Main category: cs.CV

TL;DR: The paper proposes a method for using 2D vision-language models (VLMs) to perform 3D localization of hidden objects by treating relevancy activations as abstract object representations to improve search efficiency.


<details>
  <summary>Details</summary>
Motivation: To enable household robots to localize and identify partially occluded or hidden objects efficiently using historical placement data, optimizing the process of unstructured search.

Method: Relevancy activations from 2D VLMs are used to represent abstract objects. The framework incorporates historical data to predict and locate hidden objects in a 3D space.

Result: The model consistently identifies the complete 3D location of hidden objects faster than random search methods, achieving first-try success.

Conclusion: The proposed extensions to semantic abstraction allow robots to save time and effort when searching for lost objects, making 3D localization more efficient.

Abstract: Semantic Abstraction's key observation is that 2D VLMs' relevancy activations roughly correspond to their confidence of whether and where an object is in the scene. Thus, relevancy maps are treated as "abstract object" representations. We use this framework for learning 3D localization and completion for the exclusive domain of hidden objects, defined as objects that cannot be directly identified by a VLM because they are at least partially occluded. This process of localizing hidden objects is a form of unstructured search that can be performed more efficiently using historical data of where an object is frequently placed. Our model can accurately identify the complete 3D location of a hidden object on the first try significantly faster than a naive random search. These extensions to semantic abstraction hope to provide household robots with the skills necessary to save time and effort when looking for lost objects.

</details>


### [134] [VideoScaffold: Elastic-Scale Visual Hierarchies for Streaming Video Understanding in MLLMs](https://arxiv.org/abs/2512.22226)
*Naishan Zheng,Jie Huang,Qingpei Guo,Feng Zhao*

Main category: cs.CV

TL;DR: The paper introduces VideoScaffold, a dynamic framework for improving streaming video understanding by adaptively refining event granularity and enabling smoother transitions from fine-grained to abstract representations.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of handling redundancy and achieving temporally coherent representations in long videos that existing static strategies cannot overcome, particularly for streaming contexts.

Method: The VideoScaffold framework introduces Elastic-Scale Event Segmentation (EES) for dynamic event segmentation and Hierarchical Event Consolidation (HEC) for progressive abstraction of video segments, allowing for adaptable video comprehension in streaming settings.

Result: VideoScaffold achieved state-of-the-art performance on both offline and streaming video understanding benchmarks and effectively extended image-based MLLMs for continuous video comprehension.

Conclusion: The framework demonstrates the ability to deliver enhanced video cognition by dynamically adjusting event granularity and abstracting video streams smoothly, proving the effectiveness and modularity of VideoScaffold.

Abstract: Understanding long videos with multimodal large language models (MLLMs) remains challenging due to the heavy redundancy across frames and the need for temporally coherent representations. Existing static strategies, such as sparse sampling, frame compression, and clustering, are optimized for offline settings and often produce fragmented or over-compressed outputs when applied to continuous video streams. We present VideoScaffold, a dynamic representation framework designed for streaming video understanding. It adaptively adjusts event granularity according to video duration while preserving fine-grained visual semantics. VideoScaffold introduces two key components: Elastic-Scale Event Segmentation (EES), which performs prediction-guided segmentation to dynamically refine event boundaries, and Hierarchical Event Consolidation (HEC), which progressively aggregates semantically related segments into multi-level abstractions. Working in concert, EES and HEC enable VideoScaffold to transition smoothly from fine-grained frame understanding to abstract event reasoning as the video stream unfolds. Extensive experiments across both offline and streaming video understanding benchmarks demonstrate that VideoScaffold achieves state-of-the-art performance. The framework is modular and plug-and-play, seamlessly extending existing image-based MLLMs to continuous video comprehension. The code is available at https://github.com/zheng980629/VideoScaffold.

</details>


### [135] [KAN-FPN-Stem:A KAN-Enhanced Feature Pyramid Stem for Boosting ViT-based Pose Estimation](https://arxiv.org/abs/2512.22228)
*HaoNan Tang*

Main category: cs.CV

TL;DR: This paper improves Vision Transformers for pose estimation by developing a KAN-enhanced FPN-Stem architecture that mitigates limitations in feature fusion, boosting performance significantly.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the performance bottleneck in Vision Transformers caused by simplistic front-end designs, particularly during multi-scale feature handling and fusion, which lead to information loss.

Method: The paper introduces a KAN-enhanced FPN-Stem that replaces standard FPN smoothing layers with a KAN-based convolutional layer, allowing adaptive learning to rectify fusion artifacts while retaining the classic upsample-and-add fusion stream.

Result: The KAN-FPN-Stem architecture provides a performance improvement of up to +2.0 AP over the ViTPose-S baseline according to experiments on the COCO dataset.

Conclusion: The study concludes that the bottleneck in ViT performance often resides in feature fusion rather than refinement, offering an effective solution with the KAN operator that improves dense prediction tasks.

Abstract: Vision Transformers (ViT) have demonstrated significant promise in dense prediction tasks such as pose estimation. However, their performance is frequently constrained by the overly simplistic front-end designs employed in models like ViTPose. This naive patchification mechanism struggles to effectively handle multi-scale variations and results in irreversible information loss during the initial feature extraction phase. To overcome this limitation, we introduce a novel KAN-enhanced FPN-Stem architecture. Through rigorous ablation studies, we first identified that the true bottleneck for performance improvement lies not in plug-and-play attention modules (e.g., CBAM), but in the post-fusion non-linear smoothing step within the FPN. Guided by this insight, our core innovation is to retain the classic "upsample-and-add" fusion stream of the FPN, but replace its terminal, standard linear 3x3 smoothing convolution with a powerful KAN-based convolutional layer. Leveraging its superior non-linear modeling capabilities, this KAN-based layer adaptively learns and rectifies the "artifacts" generated during the multi-scale fusion process. Extensive experiments on the COCO dataset demonstrate that our KAN-FPN-Stem achieves a significant performance boost of up to +2.0 AP over the lightweight ViTPose-S baseline. This work not only delivers a plug-and-play, high-performance module but, more importantly, reveals that: the performance bottleneck in ViT front-end often lies not in 'feature refinement' (Attention), but in the quality of 'feature fusion' (Fusion). Furthermore, it provides an effective path to address this bottleneck through the introduction of the KAN operator.

</details>


### [136] [Meta-information Guided Cross-domain Synergistic Diffusion Model for Low-dose PET Reconstruction](https://arxiv.org/abs/2512.22237)
*Mengxiao Geng,Ran Hong,Xiaoling Xu,Bingxuan Li,Qiegen Liu*

Main category: cs.CV

TL;DR: This paper presents a model called MiG-DM for improving low-dose PET imaging by integrating patient-specific meta-information and projection-domain physics knowledge to enhance image quality and preserve details.


<details>
  <summary>Details</summary>
Motivation: Low-dose PET imaging reduces radiation exposure but faces challenges related to noise, contrast loss, and difficulty in preserving physiological details, with current methods often neglecting crucial physics and patient-specific information.

Method: The paper introduces MiG-DM, leveraging a meta-information encoding module for semantic alignment and a cross-domain architecture combining projection-domain and image-domain processing.

Result: Experiments on public and clinical datasets with different dose levels show that MiG-DM improves PET image quality and preserves physiological details better than existing methods.

Conclusion: Integrating patient-specific meta-information and projection-domain knowledge enables MiG-DM to generate enhanced PET images, advancing low-dose imaging for clinical use.

Abstract: Low-dose PET imaging is crucial for reducing patient radiation exposure but faces challenges like noise interference, reduced contrast, and difficulty in preserving physiological details. Existing methods often neglect both projection-domain physics knowledge and patient-specific meta-information, which are critical for functional-semantic correlation mining. In this study, we introduce a meta-information guided cross-domain synergistic diffusion model (MiG-DM) that integrates comprehensive cross-modal priors to generate high-quality PET images. Specifically, a meta-information encoding module transforms clinical parameters into semantic prompts by considering patient characteristics, dose-related information, and semi-quantitative parameters, enabling cross-modal alignment between textual meta-information and image reconstruction. Additionally, the cross-domain architecture combines projection-domain and image-domain processing. In the projection domain, a specialized sinogram adapter captures global physical structures through convolution operations equivalent to global image-domain filtering. Experiments on the UDPET public dataset and clinical datasets with varying dose levels demonstrate that MiG-DM outperforms state-of-the-art methods in enhancing PET image quality and preserving physiological details.

</details>


### [137] [Multi-objective hybrid knowledge distillation for efficient deep learning in smart agriculture](https://arxiv.org/abs/2512.22239)
*Phi-Hung Hoang,Nam-Thuan Trinh,Van-Manh Tran,Thi-Thu-Hong Phan*

Main category: cs.CV

TL;DR: The paper introduces a hybrid knowledge distillation framework to create lightweight, high-performing models for resource-restricted smart agriculture systems.


<details>
  <summary>Details</summary>
Motivation: The study addresses the challenge of deploying deep learning models on constrained edge devices in smart agriculture by balancing computational efficiency and recognition accuracy.

Method: A customized student model is designed using inverted residual blocks and dense connectivity, trained under the guidance of ResNet18 teacher network with hard-label supervision, feature-level distillation, response-level distillation, and self-distillation strategies.

Result: The distilled student model achieves 98.56% accuracy in rice seed classification with significantly reduced computational cost and model size, and shows robust performance across other plant leaf disease datasets.

Conclusion: The proposed framework offers an efficient, lightweight, and robust solution for deploying deep learning models in hardware-limited smart agriculture systems while maintaining high accuracy.

Abstract: Deploying deep learning models on resource-constrained edge devices remains a major challenge in smart agriculture due to the trade-off between computational efficiency and recognition accuracy. To address this challenge, this study proposes a hybrid knowledge distillation framework for developing a lightweight yet high-performance convolutional neural network. The proposed approach designs a customized student model that combines inverted residual blocks with dense connectivity and trains it under the guidance of a ResNet18 teacher network using a multi-objective strategy that integrates hard-label supervision, feature-level distillation, response-level distillation, and self-distillation. Experiments are conducted on a rice seed variety identification dataset containing nine varieties and further extended to four plant leaf disease datasets, including rice, potato, coffee, and corn, to evaluate generalization capability. On the rice seed variety classification task, the distilled student model achieves an accuracy of 98.56%, which is only 0.09% lower than the teacher model (98.65%), while requiring only 0.68 GFLOPs and approximately 1.07 million parameters. This corresponds to a reduction of about 2.7 times in computational cost and more than 10 times in model size compared with the ResNet18 teacher model. In addition, compared with representative pretrained models, the proposed student reduces the number of parameters by more than 6 times relative to DenseNet121 and by over 80 times compared with the Vision Transformer (ViT) architecture, while maintaining comparable or superior classification accuracy. Consistent performance gains across multiple plant leaf disease datasets further demonstrate the robustness, efficiency, and strong deployment potential of the proposed framework for hardware-limited smart agriculture systems.

</details>


### [138] [Evaluating an Adaptive Multispectral Turret System for Autonomous Tracking Across Variable Illumination Conditions](https://arxiv.org/abs/2512.22263)
*Aahan Sachdeva,Dhanvinkumar Ganeshkumar,James E. Gallagher,Tyler Treat,Edward J. Oughton*

Main category: cs.CV

TL;DR: The study develops a framework fusing RGB and thermal infrared video to improve robotic detection in varying illumination, showing enhanced performance in low-light conditions.


<details>
  <summary>Details</summary>
Motivation: Robotic detection systems in emergency services struggle in low-light environments using RGB or thermal-only methods.

Method: Fusion of RGB and thermal infrared video streams is dynamically optimized across illumination conditions with three light levels using 33 YOLO models trained on 22,000 images.

Result: Fusion ratios optimized for specific light conditions achieved better detection confidence than YOLOv5n and YOLOv11n baselines.

Conclusion: The adaptive RGB-LWIR fusion enhances detection reliability for autonomous robotics in diverse lighting scenarios.

Abstract: Autonomous robotic platforms are playing a growing role across the emergency services sector, supporting missions such as search and rescue operations in disaster zones and reconnaissance. However, traditional red-green-blue (RGB) detection pipelines struggle in low-light environments, and thermal-based systems lack color and texture information. To overcome these limitations, we present an adaptive framework that fuses RGB and long-wave infrared (LWIR) video streams at multiple fusion ratios and dynamically selects the optimal detection model for each illumination condition. We trained 33 You Only Look Once (YOLO) models on over 22,000 annotated images spanning three light levels: no-light (<10 lux), dim-light (10-1000 lux), and full-light (>1000 lux). To integrate both modalities, fusion was performed by blending aligned RGB and LWIR frames at eleven ratios, from full RGB (100/0) to full LWIR (0/100) in 10% increments. Evaluation showed that the best full-light model (80/20 RGB-LWIR) and dim-light model (90/10 fusion) achieved 92.8% and 92.0% mean confidence; both significantly outperformed the YOLOv5 nano (YOLOv5n) and YOLOv11 nano (YOLOv11n) baselines. Under no-light conditions, the top 40/60 fusion reached 71.0%, exceeding baselines though not statistically significant. Adaptive RGB-LWIR fusion improved detection confidence and reliability across all illumination conditions, enhancing autonomous robotic vision performance.

</details>


### [139] [Human-Aligned Generative Perception: Bridging Psychophysics and Generative Models](https://arxiv.org/abs/2512.22272)
*Antara Titikhsha,Om Kulkarni,Dharun Muthaiah*

Main category: cs.CV

TL;DR: The paper introduces a Human Perception Embedding (HPE) teacher model to enhance geometric understanding in text-to-image diffusion models, allowing finer control over geometry and style.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image diffusion models achieve detailed textures but struggle to adhere to strict geometric constraints, revealing a gap between human perception and generative models.

Method: The proposed method uses an off-the-shelf Human Perception Embedding (HPE) teacher model trained on the THINGS dataset. By injecting gradients from this teacher into the latent diffusion process, the authors achieve geometric and stylistic separation.

Result: The approach improves semantic alignment by 80% compared to unguided baselines and demonstrates zero-shot transfer of 3D shapes (e.g., Eames chair) onto conflicting materials (e.g., pink metal). Flow models need continuous guidance to avoid drifting away from desired paths.

Conclusion: Small teacher models can reliably guide large generative systems, enabling stronger geometric control and enhancing creativity in text-to-image synthesis.

Abstract: Text-to-image diffusion models generate highly detailed textures, yet they often rely on surface appearance and fail to follow strict geometric constraints, particularly when those constraints conflict with the style implied by the text prompt. This reflects a broader semantic gap between human perception and current generative models. We investigate whether geometric understanding can be introduced without specialized training by using lightweight, off-the-shelf discriminators as external guidance signals. We propose a Human Perception Embedding (HPE) teacher trained on the THINGS triplet dataset, which captures human sensitivity to object shape. By injecting gradients from this teacher into the latent diffusion process, we show that geometry and style can be separated in a controllable manner. We evaluate this approach across three architectures: Stable Diffusion v1.5 with a U-Net backbone, the flow-matching model SiT-XL/2, and the diffusion transformer PixArt-Σ. Our experiments reveal that flow models tend to drift back toward their default trajectories without continuous guidance, and we demonstrate zero-shot transfer of complex three-dimensional shapes, such as an Eames chair, onto conflicting materials such as pink metal. This guided generation improves semantic alignment by about 80 percent compared to unguided baselines. Overall, our results show that small teacher models can reliably guide large generative systems, enabling stronger geometric control and broadening the creative range of text-to-image synthesis.

</details>


### [140] [GeCo: A Differentiable Geometric Consistency Metric for Video Generation](https://arxiv.org/abs/2512.22274)
*Leslie Gu,Junhwa Hur,Charles Herrmann,Fangneng Zhan,Todd Zickler,Deqing Sun,Hanspeter Pfister*

Main category: cs.CV

TL;DR: GeCo is a geometry-grounded metric designed to detect artifacts in static scenes through motion and depth fusion, with applications in benchmarking and improving video generation models.


<details>
  <summary>Details</summary>
Motivation: There is a need to detect and analyze geometric deformation and occlusion-inconsistency artifacts in video generation models, which are common but often overlooked failure modes.

Method: GeCo incorporates the fusion of residual motion and depth priors to create dense, interpretable consistency maps for artifact detection. It is also used as a training-free guidance loss to mitigate deformation artifacts during video generation.

Result: GeCo effectively benchmarks video generation models by revealing failure modes and also reduces deformation artifacts during video generation when used as guidance loss.

Conclusion: GeCo offers a comprehensive metric for detecting artifacts, serves as a benchmarking tool, and improves video generation performance, demonstrating its utility and versatility.

Abstract: We introduce GeCo, a geometry-grounded metric for jointly detecting geometric deformation and occlusion-inconsistency artifacts in static scenes. By fusing residual motion and depth priors, GeCo produces interpretable, dense consistency maps that reveal these artifacts. We use GeCo to systematically benchmark recent video generation models, uncovering common failure modes, and further employ it as a training-free guidance loss to reduce deformation artifacts during video generation.

</details>


### [141] [The Illusion of Clinical Reasoning: A Benchmark Reveals the Pervasive Gap in Vision-Language Models for Clinical Competency](https://arxiv.org/abs/2512.22275)
*Dingyu Wang,Zimu Yuan,Jiajun Liu,Shanggui Liu,Nan Zhou,Tianxing Xu,Di Huang,Dong Jiang*

Main category: cs.CV

TL;DR: The paper introduces the Bones and Joints (B&J) Benchmark to evaluate AI models' clinical reasoning capabilities in orthopedics and sports medicine, revealing a significant gap in multimodal reasoning tasks compared to structured question answering.


<details>
  <summary>Details</summary>
Motivation: The motivation is to assess the true clinical reasoning capabilities of AI models, which current benchmarks fail to do, by creating a comprehensive framework that captures the integrated, multimodal reasoning critical for real-world clinical applications.

Method: The researchers developed the B&J Benchmark with 1,245 real-world orthopedic and sports medicine cases, evaluating AI models (11 vision-language models and six large language models) across 7 tasks aligned with the clinical reasoning process.

Result: The study found a notable disparity in performance, with AI models achieving over 90% accuracy on multiple-choice tasks but only around 60% on open-ended, multimodal tasks requiring visual and text integration. Vision-language models showed deficiencies in interpreting medical images and generated inconsistent outputs.

Conclusion: The study concludes that current AI models are inadequate for complex multimodal clinical reasoning and should be limited to supportive, text-based roles until significant advancements in multimodal integration and visual understanding are achieved.

Abstract: Background: The rapid integration of foundation models into clinical practice and public health necessitates a rigorous evaluation of their true clinical reasoning capabilities beyond narrow examination success. Current benchmarks, typically based on medical licensing exams or curated vignettes, fail to capture the integrated, multimodal reasoning essential for real-world patient care. Methods: We developed the Bones and Joints (B&J) Benchmark, a comprehensive evaluation framework comprising 1,245 questions derived from real-world patient cases in orthopedics and sports medicine. This benchmark assesses models across 7 tasks that mirror the clinical reasoning pathway, including knowledge recall, text and image interpretation, diagnosis generation, treatment planning, and rationale provision. We evaluated eleven vision-language models (VLMs) and six large language models (LLMs), comparing their performance against expert-derived ground truth. Results: Our results demonstrate a pronounced performance gap between task types. While state-of-the-art models achieved high accuracy, exceeding 90%, on structured multiple-choice questions, their performance markedly declined on open-ended tasks requiring multimodal integration, with accuracy scarcely reaching 60%. VLMs demonstrated substantial limitations in interpreting medical images and frequently exhibited severe text-driven hallucinations, often ignoring contradictory visual evidence. Notably, models specifically fine-tuned for medical applications showed no consistent advantage over general-purpose counterparts. Conclusions: Current artificial intelligence models are not yet clinically competent for complex, multimodal reasoning. Their safe deployment should currently be limited to supportive, text-based roles. Future advancement in core clinical tasks awaits fundamental breakthroughs in multimodal integration and visual understanding.

</details>


### [142] [FETAL-GAUGE: A Benchmark for Assessing Vision-Language Models in Fetal Ultrasound](https://arxiv.org/abs/2512.22278)
*Hussain Alasmawi,Numan Saeed,Mohammad Yaqub*

Main category: cs.CV

TL;DR: The paper introduces Fetal-Gauge, a benchmark to evaluate Vision-Language Models (VLMs) in fetal ultrasound tasks. It highlights the current limitations of VLMs for prenatal imaging and aims to foster advancements in multimodal deep learning.


<details>
  <summary>Details</summary>
Motivation: The global shortage of trained sonographers limits access to critical fetal health monitoring. Deep learning, particularly Vision-Language Models (VLMs), could improve efficiency and training but lacks a benchmark for fetal ultrasound task evaluation.

Method: The authors created Fetal-Gauge, a benchmark comprising over 42,000 images and 93,000 question-answer pairs. This evaluates VLM performance across five fetal ultrasound tasks using multiple state-of-the-art models.

Result: The best-performing model among those tested achieved 55% accuracy, which is far below clinical standards. This reveals significant limitations in adapting VLMs to fetal ultrasound interpretations.

Conclusion: Fetal-Gauge establishes a foundational benchmark for VLM advancements in prenatal care, showing current gaps and proposing pathways for improvement. It will support future research to tackle global healthcare accessibility issues.

Abstract: The growing demand for prenatal ultrasound imaging has intensified a global shortage of trained sonographers, creating barriers to essential fetal health monitoring. Deep learning has the potential to enhance sonographers' efficiency and support the training of new practitioners. Vision-Language Models (VLMs) are particularly promising for ultrasound interpretation, as they can jointly process images and text to perform multiple clinical tasks within a single framework. However, despite the expansion of VLMs, no standardized benchmark exists to evaluate their performance in fetal ultrasound imaging. This gap is primarily due to the modality's challenging nature, operator dependency, and the limited public availability of datasets. To address this gap, we present Fetal-Gauge, the first and largest visual question answering benchmark specifically designed to evaluate VLMs across various fetal ultrasound tasks. Our benchmark comprises over 42,000 images and 93,000 question-answer pairs, spanning anatomical plane identification, visual grounding of anatomical structures, fetal orientation assessment, clinical view conformity, and clinical diagnosis. We systematically evaluate several state-of-the-art VLMs, including general-purpose and medical-specific models, and reveal a substantial performance gap: the best-performing model achieves only 55\% accuracy, far below clinical requirements. Our analysis identifies critical limitations of current VLMs in fetal ultrasound interpretation, highlighting the urgent need for domain-adapted architectures and specialized training approaches. Fetal-Gauge establishes a rigorous foundation for advancing multimodal deep learning in prenatal care and provides a pathway toward addressing global healthcare accessibility challenges. Our benchmark will be publicly available once the paper gets accepted.

</details>


### [143] [A Three-Level Alignment Framework for Large-Scale 3D Retrieval and Controlled 4D Generation](https://arxiv.org/abs/2512.22294)
*Philip Xu,David Elizondo,Raouf Hamzaoui*

Main category: cs.CV

TL;DR: Uni4D is a framework enabling large-scale open vocabulary 3D retrieval and controlled 4D generation using structured alignment across texts, 3D models, and images.


<details>
  <summary>Details</summary>
Motivation: The paper aims to enhance 3D retrieval and 4D generation capabilities by integrating multimodal elements to address dynamic multimodal understanding and real-world applications.

Method: The framework employs structured three-level alignment across text, 3D models, and images. It utilizes a text-to-3D retrieval model and additional components for cross-modal alignment.

Result: Experimental results showcase Uni4D achieving high-quality 3D retrieval and controllable 4D generation.

Conclusion: The proposed framework advances the field of dynamic multimodal understanding and expands practical applications in 3D and 4D domains.

Abstract: We introduce Uni4D, a unified framework for large scale open vocabulary 3D retrieval and controlled 4D generation based on structured three level alignment across text, 3D models, and image modalities. Built upon the Align3D 130 dataset, Uni4D employs a 3D text multi head attention and search model to optimize text to 3D retrieval through improved semantic alignment. The framework further strengthens cross modal alignment through three components: precise text to 3D retrieval, multi view 3D to image alignment, and image to text alignment for generating temporally consistent 4D assets. Experimental results demonstrate that Uni4D achieves high quality 3D retrieval and controllable 4D generation, advancing dynamic multimodal understanding and practical applications.

</details>


### [144] [Learning Dynamic Scene Reconstruction with Sinusoidal Geometric Priors](https://arxiv.org/abs/2512.22295)
*Tian Guo,Hui Yuan,Philip Xu,David Elizondo*

Main category: cs.CV

TL;DR: The paper presents SirenPose, a novel loss function aimed at improving 3D scene reconstruction for dynamic, fast-moving, and multi-target environments by leveraging sinusoidal properties and geometric priors.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with motion modeling accuracy and spatiotemporal consistency, especially in rapid motion and complex multi-target scenarios.

Method: SirenPose combines sinusoidal representation networks' periodic activations and geometric priors, introducing physics-inspired constraints to ensure coherent keypoint predictions across spatial and temporal dimensions.

Result: With an expanded dataset of 600,000 annotated instances, SirenPose showed significant improvements in spatiotemporal consistency metrics and outperformed prior methods in handling rapid motion and scene complexities.

Conclusion: SirenPose effectively enhances 3D dynamic scene reconstruction by addressing key challenges in motion modeling and consistency, offering superior accuracy and robustness.

Abstract: We propose SirenPose, a novel loss function that combines the periodic activation properties of sinusoidal representation networks with geometric priors derived from keypoint structures to improve the accuracy of dynamic 3D scene reconstruction. Existing approaches often struggle to maintain motion modeling accuracy and spatiotemporal consistency in fast moving and multi target scenes. By introducing physics inspired constraint mechanisms, SirenPose enforces coherent keypoint predictions across both spatial and temporal dimensions. We further expand the training dataset to 600,000 annotated instances to support robust learning. Experimental results demonstrate that models trained with SirenPose achieve significant improvements in spatiotemporal consistency metrics compared to prior methods, showing superior performance in handling rapid motion and complex scene changes.

</details>


### [145] [An Architecture-Led Hybrid Report on Body Language Detection Project](https://arxiv.org/abs/2512.23028)
*Thomson Tong,Diba Darooneh*

Main category: cs.CV

TL;DR: This paper analyzes two modern vision-language models (Qwen2.5-VL-7B-Instruct and Llama-4-Scout-17B-16E-Instruct) and their application in a video-to-artifact pipeline for detecting and analyzing body language in videos.


<details>
  <summary>Details</summary>
Motivation: To understand and evaluate the architectural and operational properties of modern vision-language models in real-world applications like video analysis and body language detection.

Method: The authors examined the architecture of the VLMs, described how these models integrate into a video-to-artifact pipeline, and analyzed schema validation, output structure, and interactive single-frame analysis capabilities.

Result: The paper highlighted the interplay between model behavior and system constraints, emphasizing distinctions like schema validation being structural, frame-local person identifiers, and the limitations of freeform text outputs in interactive analysis.

Conclusion: Understanding these architectural insights aids in making defensible claims, designing reliable interfaces, and improving evaluation metrics for vision-language models in complex real-world systems.

Abstract: This report provides an architecture-led analysis of two modern vision-language models (VLMs), Qwen2.5-VL-7B-Instruct and Llama-4-Scout-17B-16E-Instruct, and explains how their architectural properties map to a practical video-to-artifact pipeline implemented in the BodyLanguageDetection repository [1]. The system samples video frames, prompts a VLM to detect visible people and generate pixel-space bounding boxes with prompt-conditioned attributes (emotion by default), validates output structure using a predefined schema, and optionally renders an annotated video. We first summarize the shared multimodal foundation (visual tokenization, Transformer attention, and instruction following), then describe each architecture at a level sufficient to justify engineering choices without speculative internals. Finally, we connect model behavior to system constraints: structured outputs can be syntactically valid while semantically incorrect, schema validation is structural (not geometric correctness), person identifiers are frame-local in the current prompting contract, and interactive single-frame analysis returns free-form text rather than schema-enforced JSON. These distinctions are critical for writing defensible claims, designing robust interfaces, and planning evaluation.

</details>


### [146] [Real-Time In-Cabin Driver Behavior Recognition on Low-Cost Edge Hardware](https://arxiv.org/abs/2512.22298)
*Vesal Ahsani,Babak Hossein Khalaj*

Main category: cs.CV

TL;DR: This paper presents a low-cost driver monitoring system designed for deployment on edge platforms that recognizes 17 behavior classes such as distraction or drowsiness, operating in real-time.


<details>
  <summary>Details</summary>
Motivation: The paper aims to develop driver monitoring systems that ensure low latency while operating under stringent hardware constraints in terms of compute, power, and cost.

Method: The system integrates a compact per-frame vision model, confounder-aware label design to minimize false positives, and a temporal decision mechanism for sustained predictions, tested on licensed datasets on edge platforms.

Result: The system operates at 16 FPS on Raspberry Pi 5 with INT8 inference and 25 FPS on Coral Edge TPU, meeting real-time performance requirements under low-cost hardware.

Conclusion: The proposed driver monitoring system facilitates reliable and real-time in-cabin human-state perception, potentially serving as a foundation for human-centered vehicle intelligence.

Abstract: In-cabin Driver Monitoring Systems (DMS) must recognize distraction- and drowsiness-related behaviors with low latency under strict constraints on compute, power, and cost. We present a single-camera in-cabin driver behavior recognition system designed for deployment on two low-cost edge platforms: Raspberry Pi 5 (CPU-only) and Google Coral Edge TPU. The proposed pipeline combines (i) a compact per-frame vision model, (ii) a confounder-aware label design to reduce visually similar false positives, and (iii) a temporal decision head that triggers alerts only when predictions are both confident and sustained. The system covers 17 behavior classes, including multiple phone-use modes, eating/drinking, smoking, reaching behind, gaze/attention shifts, passenger interaction, grooming, control-panel interaction, yawning, and eyes-closed sleep. Training and evaluation use licensed datasets spanning diverse drivers, vehicles, and lighting conditions (details in Section 6), and we further validate runtime behavior in real in-vehicle tests. The optimized deployments achieve about 16 FPS on Raspberry Pi 5 with INT8 inference (per-frame latency under 60 ms) and about 25 FPS on Coral Edge TPU, enabling real-time monitoring and stable alert generation on inexpensive hardware. Finally, we discuss how reliable in-cabin human-state perception can serve as an upstream input for human-centered vehicle intelligence, including emerging agentic vehicle concepts.

</details>


### [147] [Attack-Aware Deepfake Detection under Counter-Forensic Manipulations](https://arxiv.org/abs/2512.22303)
*Noor Fatima,Hasan Faraz Khan,Muzammil Behzad*

Main category: cs.CV

TL;DR: Develops an advanced detector using two-stream architecture and red-team training for robust deepfake and image forensics detection, achieving accuracy with realistic deployment considerations.


<details>
  <summary>Details</summary>
Motivation: To create a robust and deployable solution for deepfake detection and digital image forensics that accounts for realistic attack scenarios.

Method: The method integrates a two-stream architecture combining semantic content encoding and forensic residual extraction alongside red-team training using attack simulations and test-time defenses with low-cost perturbations.

Result: Evaluation on various benchmarks demonstrated high performance (near-perfect ranking, low calibration error, and minimal abstention risk) and effective handling of clean and attacked data.

Conclusion: This approach establishes a modular and practical baseline for deepfake detection with robust, calibrated, and interpretable results.

Abstract: This work presents an attack-aware deepfake and image-forensics detector designed for robustness, well-calibrated probabilities, and transparent evidence under realistic deployment conditions. The method combines red-team training with randomized test-time defense in a two-stream architecture, where one stream encodes semantic content using a pretrained backbone and the other extracts forensic residuals, fused via a lightweight residual adapter for classification, while a shallow Feature Pyramid Network style head produces tamper heatmaps under weak supervision. Red-team training applies worst-of-K counter-forensics per batch, including JPEG realign and recompress, resampling warps, denoise-to-regrain operations, seam smoothing, small color and gamma shifts, and social-app transcodes, while test-time defense injects low-cost jitters such as resize and crop phase changes, mild gamma variation, and JPEG phase shifts with aggregated predictions. Heatmaps are guided to concentrate within face regions using face-box masks without strict pixel-level annotations. Evaluation on existing benchmarks, including standard deepfake datasets and a surveillance-style split with low light and heavy compression, reports clean and attacked performance, AUC, worst-case accuracy, reliability, abstention quality, and weak-localization scores. Results demonstrate near-perfect ranking across attacks, low calibration error, minimal abstention risk, and controlled degradation under regrain, establishing a modular, data-efficient, and practically deployable baseline for attack-aware detection with calibrated probabilities and actionable heatmaps.

</details>


### [148] [PortionNet: Distilling 3D Geometric Knowledge for Food Nutrition Estimation](https://arxiv.org/abs/2512.22304)
*Darrin Bright,Rakshith Raj,Kanchan Keisham*

Main category: cs.CV

TL;DR: PortionNet is a framework for accurate food nutrition estimation using only RGB image input, learning pseudo-3D reasoning without requiring specialized hardware.


<details>
  <summary>Details</summary>
Motivation: Accurate food nutrition estimation is difficult due to the lack of 3D information in single images, and the existing depth-based methods are impractical for most smartphones due to hardware requirements.

Method: A cross-modal knowledge distillation framework is proposed where geometric features are learned from point clouds during training, and only RGB images are needed for inference using a lightweight adapter network.

Result: PortionNet achieves state-of-the-art results in food volume and energy estimation on the MetaFood3D dataset and generalizes well in energy estimation on another dataset, SimpleFood45.

Conclusion: PortionNet overcomes the limitations of requiring specialized hardware by achieving accurate food nutrition estimation using RGB images, which makes the method practical and effective.

Abstract: Accurate food nutrition estimation from single images is challenging due to the loss of 3D information. While depth-based methods provide reliable geometry, they remain inaccessible on most smartphones because of depth-sensor requirements. To overcome this challenge, we propose PortionNet, a novel cross-modal knowledge distillation framework that learns geometric features from point clouds during training while requiring only RGB images at inference. Our approach employs a dual-mode training strategy where a lightweight adapter network mimics point cloud representations, enabling pseudo-3D reasoning without any specialized hardware requirements. PortionNet achieves state-of-the-art performance on MetaFood3D, outperforming all previous methods in both volume and energy estimation. Cross-dataset evaluation on SimpleFood45 further demonstrates strong generalization in energy estimation.

</details>


### [149] [MoFu: Scale-Aware Modulation and Fourier Fusion for Multi-Subject Video Generation](https://arxiv.org/abs/2512.22310)
*Run Ling,Ke Cao,Jian Lu,Ao Ma,Haowei Liu,Runze He,Changwei Wang,Rongtao Xu,Yihua Shao,Zhanjie Zhang,Peng Wu,Guibing Guo,Wei Feng,Zheng Zhang,Jingjing Lv,Junjie Shen,Ching Law,Xingwei Wang*

Main category: cs.CV

TL;DR: The paper introduces MoFu, a novel framework addressing scale inconsistency and permutation sensitivity in multi-subject video synthesis.


<details>
  <summary>Details</summary>
Motivation: Current methods for multi-subject video generation suffer from scale inconsistency and permutation sensitivity, affecting the visual fidelity and natural representation of subjects.

Method: MoFu integrates Scale-Aware Modulation (SMO) for size consistency, Fourier Fusion to address permutation sensitivity, and Scale-Permutation Stability Loss for improved generation. A dedicated benchmark is also set up.

Result: MoFu demonstrates superior performance in maintaining scale consistency, subject fidelity, and visual quality compared to existing approaches.

Conclusion: The proposed MoFu framework effectively resolves key challenges in multi-subject video generation, contributing to enhanced video synthesis outcomes.

Abstract: Multi-subject video generation aims to synthesize videos from textual prompts and multiple reference images, ensuring that each subject preserves natural scale and visual fidelity. However, current methods face two challenges: scale inconsistency, where variations in subject size lead to unnatural generation, and permutation sensitivity, where the order of reference inputs causes subject distortion. In this paper, we propose MoFu, a unified framework that tackles both challenges. For scale inconsistency, we introduce Scale-Aware Modulation (SMO), an LLM-guided module that extracts implicit scale cues from the prompt and modulates features to ensure consistent subject sizes. To address permutation sensitivity, we present a simple yet effective Fourier Fusion strategy that processes the frequency information of reference features via the Fast Fourier Transform to produce a unified representation. Besides, we design a Scale-Permutation Stability Loss to jointly encourage scale-consistent and permutation-invariant generation. To further evaluate these challenges, we establish a dedicated benchmark with controlled variations in subject scale and reference permutation. Extensive experiments demonstrate that MoFu significantly outperforms existing methods in preserving natural scale, subject fidelity, and overall visual quality.

</details>


### [150] [VideoZoomer: Reinforcement-Learned Temporal Focusing for Long Video Reasoning](https://arxiv.org/abs/2512.22315)
*Yang Ding,Yizhen Zhang,Xin Lai,Ruihang Chu,Yujiu Yang*

Main category: cs.CV

TL;DR: VideoZoomer is a framework that enhances Multimodal Large Language Models (MLLMs) for long video understanding by dynamically controlling visual focus and correcting initial selection errors.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs struggle with long video understanding due to limited context windows, as current methods rely on uniform frame sampling or static pre-selection, often missing critical information.

Method: VideoZoomer utilizes a temporal zoom tool to dynamically adjust the focus on video frames during reasoning. The model is trained using a two-stage strategy: supervised fine-tuning followed by reinforcement learning.

Result: Experiments show VideoZoomer achieves complex reasoning patterns, strong performance in long video tasks, surpassing open-source models, rivaling proprietary systems, and maintaining efficiency under reduced frame budgets.

Conclusion: VideoZoomer proves effective for long video understanding, showcasing flexibility, and performance enhancements in reasoning tasks, providing a competitive edge over existing models.

Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable progress in vision-language tasks yet remain limited in long video understanding due to the limited context window. Consequently, prevailing approaches tend to rely on uniform frame sampling or static pre-selection, which might overlook critical evidence and unable to correct its initial selection error during its reasoning process. To overcome these limitations, we propose VideoZoomer, a novel agentic framework that enables MLLMs to dynamically control their visual focus during reasoning. Starting from a coarse low-frame-rate overview, VideoZoomer invokes a temporal zoom tool to obtain high-frame-rate clips at autonomously chosen moments, thereby progressively gathering fine-grained evidence in a multi-turn interactive manner. Accordingly, we adopt a two-stage training strategy: a cold-start supervised fine-tuning phase on a curated dataset of distilled exemplar and reflection trajectories, followed by reinforcement learning to further refine the agentic policy. Extensive experiments demonstrate that our 7B model delivers diverse and complex reasoning patterns, yielding strong performance across a broad set of long video understanding and reasoning benchmarks. These emergent capabilities allow it to consistently surpass existing open-source models and even rival proprietary systems on challenging tasks, while achieving superior efficiency under reduced frame budgets.

</details>


### [151] [SpotEdit: Selective Region Editing in Diffusion Transformers](https://arxiv.org/abs/2512.22323)
*Zhibin Qin,Zhenxiong Tan,Zeqing Wang,Songhua Liu,Xinchao Wang*

Main category: cs.CV

TL;DR: SpotEdit improves image editing efficiency by selectively updating modified areas instead of processing entire images during diffusion Transformer tasks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address inefficiencies in current image editing methods where unchanged regions are redundantly processed, leading to computational waste and potential degradation in quality.

Method: SpotEdit introduces two components: SpotSelector for identifying stable regions to skip processing, and SpotFusion for adaptive blending, achieving selective computation and preservation of unmodified regions.

Result: SpotEdit provides efficient and precise image editing, reducing computational effort while maintaining fidelity in unchanged areas.

Conclusion: Selective region editing via SpotEdit enhances efficiency and image quality during diffusion-based image modification tasks.

Abstract: Diffusion Transformer models have significantly advanced image editing by encoding conditional images and integrating them into transformer layers. However, most edits involve modifying only small regions, while current methods uniformly process and denoise all tokens at every timestep, causing redundant computation and potentially degrading unchanged areas. This raises a fundamental question: Is it truly necessary to regenerate every region during editing? To address this, we propose SpotEdit, a training-free diffusion editing framework that selectively updates only the modified regions. SpotEdit comprises two key components: SpotSelector identifies stable regions via perceptual similarity and skips their computation by reusing conditional image features; SpotFusion adaptively blends these features with edited tokens through a dynamic fusion mechanism, preserving contextual coherence and editing quality. By reducing unnecessary computation and maintaining high fidelity in unmodified areas, SpotEdit achieves efficient and precise image editing.

</details>


### [152] [DeMoGen: Towards Decompositional Human Motion Generation with Energy-Based Diffusion Models](https://arxiv.org/abs/2512.22324)
*Jianrong Zhang,Hehe Fan,Yi Yang*

Main category: cs.CV

TL;DR: The paper introduces DeMoGen, a framework for decompositional learning of complex human motions into meaningful sub-components using an energy-based diffusion model.


<details>
  <summary>Details</summary>
Motivation: To enable decompositional understanding of human motions, addressing the lack of approaches that focus on breaking down complex motions into reusable primitives instead of just forward modeling.

Method: The authors propose three variations of DeMoGen (DeMoGen-Exp, DeMoGen-OSS, DeMoGen-SC) using energy-based diffusion modeling to learn decomposed motion primitives without ground-truth for individual concepts.

Result: The decomposed motion concepts learned by the model can be recombined to generate diverse and novel motions, extending beyond the training distribution.

Conclusion: DeMoGen successfully disentangles motion primitives, enabling flexible recomposition of motions and supports compositional training with an introduced text-decomposed dataset for broader applications.

Abstract: Human motions are compositional: complex behaviors can be described as combinations of simpler primitives. However, existing approaches primarily focus on forward modeling, e.g., learning holistic mappings from text to motion or composing a complex motion from a set of motion concepts. In this paper, we consider the inverse perspective: decomposing a holistic motion into semantically meaningful sub-components. We propose DeMoGen, a compositional training paradigm for decompositional learning that employs an energy-based diffusion model. This energy formulation directly captures the composed distribution of multiple motion concepts, enabling the model to discover them without relying on ground-truth motions for individual concepts. Within this paradigm, we introduce three training variants to encourage a decompositional understanding of motion: 1. DeMoGen-Exp explicitly trains on decomposed text prompts; 2. DeMoGen-OSS performs orthogonal self-supervised decomposition; 3. DeMoGen-SC enforces semantic consistency between original and decomposed text embeddings. These variants enable our approach to disentangle reusable motion primitives from complex motion sequences. We also demonstrate that the decomposed motion concepts can be flexibly recombined to generate diverse and novel motions, generalizing beyond the training distribution. Additionally, we construct a text-decomposed dataset to support compositional training, serving as an extended resource to facilitate text-to-motion generation and motion composition.

</details>


### [153] [The Multi-View Paradigm Shift in MRI Radiomics: Predicting MGMT Methylation in Glioblastoma](https://arxiv.org/abs/2512.22331)
*Mariya Miteva,Maria Nisheva-Pavlova*

Main category: cs.CV

TL;DR: This paper proposes a new framework using variational autoencoders to integrate radiomic MRI features for better MGMT promoter methylation classification in glioblastoma.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenges of high feature redundancy and incomplete modeling in current radiomics-based machine learning for inferring tumor molecular characteristics in glioblastoma.

Method: The proposed method uses a multi-view latent representation learning framework that employs variational autoencoders to integrate radiomic features from two MRI modalities, preserving modality-specific structure and achieving effective multimodal fusion.

Result: The latent embeddings generated by the framework were applied for MGMT promoter methylation classification, offering a solution to key radiogenomic integration challenges.

Conclusion: This framework demonstrates potential for improving prognostic and therapeutic assessments in glioblastoma by leveraging advances in multimodal radiogenomics.

Abstract: Non-invasive inference of molecular tumor characteristics from medical imaging is a central goal of radiogenomics, particularly in glioblastoma (GBM), where O6-methylguanine-DNA methyltransferase (MGMT) promoter methylation carries important prognostic and therapeutic significance. Although radiomics-based machine learning methods have shown promise for this task, conventional unimodal and early-fusion approaches are often limited by high feature redundancy and an incomplete modeling of modality-specific information. In this work, we introduce a multi-view latent representation learning framework based on variational autoencoders (VAE) to integrate complementary radiomic features derived from post-contrast T1-weighted (T1Gd) and Fluid-Attenuated Inversion Recovery (FLAIR) magnetic resonance imaging (MRI). By encoding each modality through an independent probabilistic encoder and performing fusion in a compact latent space, the proposed approach preserves modality-specific structure while enabling effective multimodal integration. The resulting latent embeddings are subsequently used for MGMT promoter methylation classification.

</details>


### [154] [Feature Learning with Multi-Stage Vision Transformers on Inter-Modality HER2 Status Scoring and Tumor Classification on Whole Slides](https://arxiv.org/abs/2512.22335)
*Olaide N. Oyelade,Oliver Hoxey,Yulia Humrye*

Main category: cs.CV

TL;DR: This paper proposes a novel method using vision transformers (ViTs) for HER2 protein expression scoring on histopathology images, achieving accurate results in cancer detection and scoring.


<details>
  <summary>Details</summary>
Motivation: Accurate HER2 protein expression quantification is critical for cancer treatment, but current methods face challenges in analyzing various stain types like H&E and IHC, and providing pixel-level localization of HER2 status.

Method: The authors developed an end-to-end pipeline based on vision transformers (ViTs). Tumor localization is achieved through patch-wise processing of H&E WSIs, and a novel mapping function aligns IHC regions with malignant H&E sites. The pipeline includes an automatic HER2 scoring mechanism for 4-way HER2 scoring (0, 1+, 2+, 3+).

Result: The proposed method demonstrated high classification accuracy (0.94) and specificity (0.933) for HER2 status prediction using a dataset from 13 cases. Tumor localization performance was also good.

Conclusion: The study establishes the feasibility of combining H&E and IHC images with vision transformer-based end-to-end models for HER2 scoring, performing comparably to human pathologists and enabling accurate pixel-level predictions.

Abstract: The popular use of histopathology images, such as hematoxylin and eosin (H&E), has proven to be useful in detecting tumors. However, moving such cancer cases forward for treatment requires accurate on the amount of the human epidermal growth factor receptor 2 (HER2) protein expression. Predicting both the lower and higher levels of HER2 can be challenging. Moreover, jointly analyzing H&E and immunohistochemistry (IHC) stained images for HER2 scoring is difficult. Although several deep learning methods have been investigated to address the challenge of HER2 scoring, they suffer from providing a pixel-level localization of HER2 status. In this study, we propose a single end-to-end pipeline using a system of vision transformers with HER2 status scoring on whole slide images of WSIs. The method includes patch-wise processing of H&E WSIs for tumor localization. A novel mapping function is proposed to correspondingly identify correlated IHC WSIs regions with malignant regions on H&E. A clinically inspired HER2 scoring mechanism is embedded in the pipeline and allows for automatic pixel-level annotation of 4-way HER2 scoring (0, 1+, 2+, and 3+). Also, the proposed method accurately returns HER2-negative and HER2-positive. Privately curated datasets were collaboratively extracted from 13 different cases of WSIs of H&E and IHC. A thorough experiment is conducted on the proposed method. Results obtained showed a good classification accuracy during tumor localization. Also, a classification accuracy of 0.94 and a specificity of 0.933 were returned for the prediction of HER2 status, scoring in the 4-way methods. The applicability of the proposed pipeline was investigated using WSIs patches as comparable to human pathologists. Findings from the study showed the usability of jointly evaluated H&E and IHC images on end-to-end ViTs-based models for HER2 scoring

</details>


### [155] [Human-like visual computing advances explainability and few-shot learning in deep neural networks for complex physiological data](https://arxiv.org/abs/2512.22349)
*Alaa Alahmadi,Mohamed Hasan*

Main category: cs.CV

TL;DR: The study proposes a pseudo-colouring technique to enhance deep neural networks' explainability and few-shot learning for ECG interpretation, specifically tackling the challenge of drug-induced long QT syndrome (LQTS).


<details>
  <summary>Details</summary>
Motivation: Deep neural networks for ECG interpretation require large datasets and lack interpretability, limiting their clinical reliability. The motivation is to address these limitations by introducing methods that align with human reasoning and are data-efficient.

Method: The authors used a perception-informed pseudo-colouring technique to encode clinically significant features into structured colour representations and employed prototypical networks and a ResNet-18 architecture for one-shot and few-shot learning. ECG data from single cardiac cycles and full rhythms were utilized for evaluation.

Result: The pseudo-colouring technique improved the models' ability to learn from minimal data, correctly focusing on clinically relevant features while ignoring irrelevant ones. Performance was also enhanced by aggregating data from multiple cardiac cycles.

Conclusion: Human-like perceptual encoding, through techniques like pseudo-colouring, can improve data efficiency, explainability, and causal reasoning in medical machine intelligence, aiding clinical reliability even under extreme data scarcity.

Abstract: Machine vision models, particularly deep neural networks, are increasingly applied to physiological signal interpretation, including electrocardiography (ECG), yet they typically require large training datasets and offer limited insight into the causal features underlying their predictions. This lack of data efficiency and interpretability constrains their clinical reliability and alignment with human reasoning. Here, we show that a perception-informed pseudo-colouring technique, previously demonstrated to enhance human ECG interpretation, can improve both explainability and few-shot learning in deep neural networks analysing complex physiological data.
  We focus on acquired, drug-induced long QT syndrome (LQTS) as a challenging case study characterised by heterogeneous signal morphology, variable heart rate, and scarce positive cases associated with life-threatening arrhythmias such as torsades de pointes. This setting provides a stringent test of model generalisation under extreme data scarcity. By encoding clinically salient temporal features, such as QT-interval duration, into structured colour representations, models learn discriminative and interpretable features from as few as one or five training examples. Using prototypical networks and a ResNet-18 architecture, we evaluate one-shot and few-shot learning on ECG images derived from single cardiac cycles and full 10-second rhythms. Explainability analyses show that pseudo-colouring guides attention toward clinically meaningful ECG features while suppressing irrelevant signal components. Aggregating multiple cardiac cycles further improves performance, mirroring human perceptual averaging across heartbeats. Together, these findings demonstrate that human-like perceptual encoding can bridge data efficiency, explainability, and causal reasoning in medical machine intelligence.

</details>


### [156] [VULCAN: Tool-Augmented Multi Agents for Iterative 3D Object Arrangement](https://arxiv.org/abs/2512.22351)
*Zhengfei Kuang,Rui Lin,Long Zhao,Gordon Wetzstein,Saining Xie,Sanghyun Woo*

Main category: cs.CV

TL;DR: This paper extends Multimodal Large Language Models (MLLMs) from 2D vision-language tasks to complex 3D scene manipulations, leveraging a new API, specialized visual tools, and a collaborative multi-agent framework.


<details>
  <summary>Details</summary>
Motivation: Current Multimodal Large Language Models (MLLMs) are insufficient for 3D scene manipulation tasks, despite their significant advances in 2D tasks. The motivation is to address this gap by enhancing MLLMs' 3D functionalities.

Method: The paper introduces a robust MCP-based API for precise 3D manipulation, implements specialized visual tools for improved 3D scene analysis, and designs a collaborative multi-agent framework for task planning, execution, and error recovery.

Result: The proposed approach was tested on 25 complex object arrangement tasks, significantly outperforming existing baselines in terms of effectiveness and robustness.

Conclusion: This work successfully extends the applicability of MLLMs to challenging 3D object arrangement tasks by improving their perception, reasoning, and execution capabilities, paving the way for broader applications in 3D environments.

Abstract: Despite the remarkable progress of Multimodal Large Language Models (MLLMs) in 2D vision-language tasks, their application to complex 3D scene manipulation remains underexplored. In this paper, we bridge this critical gap by tackling three key challenges in 3D object arrangement task using MLLMs. First, to address the weak visual grounding of MLLMs, which struggle to link programmatic edits with precise 3D outcomes, we introduce an MCP-based API. This shifts the interaction from brittle raw code manipulation to more robust, function-level updates. Second, we augment the MLLM's 3D scene understanding with a suite of specialized visual tools to analyze scene state, gather spatial information, and validate action outcomes. This perceptual feedback loop is critical for closing the gap between language-based updates and precise 3D-aware manipulation. Third, to manage the iterative, error-prone updates, we propose a collaborative multi-agent framework with designated roles for planning, execution, and verification. This decomposition allows the system to robustly handle multi-step instructions and recover from intermediate errors. We demonstrate the effectiveness of our approach on a diverse set of 25 complex object arrangement tasks, where it significantly outperforms existing baselines. Website: vulcan-3d.github.io

</details>


### [157] [Self-Evaluation Unlocks Any-Step Text-to-Image Generation](https://arxiv.org/abs/2512.22374)
*Xin Yu,Xiaojuan Qi,Zhengqi Li,Kai Zhang,Richard Zhang,Zhe Lin,Eli Shechtman,Tianyu Wang,Yotam Nitzan*

Main category: cs.CV

TL;DR: Self-E offers a novel, from-scratch training technique for text-to-image generation, ensuring high-quality results across varying inference steps.


<details>
  <summary>Details</summary>
Motivation: The study aims to create an efficient and versatile text-to-image generation model that merges the strengths of various paradigms: bypassing dependencies on pretrained teachers, avoiding many-step local supervision, and achieving high-quality generation with fewer inference steps.

Method: Self-E combines Flow Matching model learning with a new self-evaluation mechanism. The model dynamically evaluates its generated samples using its own estimates, allowing it to act as a self-teacher. This approach enables any-step inference without relying on pretrained models or many inference steps.

Result: Experiments demonstrate that Self-E achieves both ultra-fast few-step generation and high-quality long-trajectory figures, competing with state-of-the-art models at both extremes of step counts.

Conclusion: Self-E introduces a unified solution to text-to-image generation, offering scalability, efficiency, and versatility in training and inference processes, making it a significant advancement in the field.

Abstract: We introduce the Self-Evaluating Model (Self-E), a novel, from-scratch training approach for text-to-image generation that supports any-step inference. Self-E learns from data similarly to a Flow Matching model, while simultaneously employing a novel self-evaluation mechanism: it evaluates its own generated samples using its current score estimates, effectively serving as a dynamic self-teacher. Unlike traditional diffusion or flow models, it does not rely solely on local supervision, which typically necessitates many inference steps. Unlike distillation-based approaches, it does not require a pretrained teacher. This combination of instantaneous local learning and self-driven global matching bridges the gap between the two paradigms, enabling the training of a high-quality text-to-image model from scratch that excels even at very low step counts. Extensive experiments on large-scale text-to-image benchmarks show that Self-E not only excels in few-step generation, but is also competitive with state-of-the-art Flow Matching models at 50 steps. We further find that its performance improves monotonically as inference steps increase, enabling both ultra-fast few-step generation and high-quality long-trajectory sampling within a single unified model. To our knowledge, Self-E is the first from-scratch, any-step text-to-image model, offering a unified framework for efficient and scalable generation.

</details>


### [158] [iOSPointMapper: RealTime Pedestrian and Accessibility Mapping with Mobile AI](https://arxiv.org/abs/2512.22392)
*Himanshu Naidu,Yuxiang Zhang,Sachin Mehta,Anat Caspi*

Main category: cs.CV

TL;DR: iOSPointMapper is a mobile application for scalable and privacy-conscious mapping of sidewalk features, using iPhone/iPad technology for segmentation, LiDAR depth estimation, and GPS/IMU data integration.


<details>
  <summary>Details</summary>
Motivation: Current sidewalk data collection methods are inadequate due to high costs, fragmentation, and scalability issues. This paper aims to address these limitations.

Method: Using on-device semantic segmentation, LiDAR depth estimation, and GPS/IMU data in iOS devices, the system maps sidewalk features like traffic signs and poles. It also allows user validation and anonymizes collected data.

Result: Evaluations demonstrate the system's effectiveness in feature detection and spatial mapping, showing promise for improving pedestrian data collection.

Conclusion: iOSPointMapper provides a scalable, transparent, and user-centered solution for pedestrian infrastructure data collection, addressing critical gaps with innovative technology.

Abstract: Accurate, up-to-date sidewalk data is essential for building accessible and inclusive pedestrian infrastructure, yet current approaches to data collection are often costly, fragmented, and difficult to scale. We introduce iOSPointMapper, a mobile application that enables real-time, privacy-conscious sidewalk mapping on the ground, using recent-generation iPhones and iPads. The system leverages on-device semantic segmentation, LiDAR-based depth estimation, and fused GPS/IMU data to detect and localize sidewalk-relevant features such as traffic signs, traffic lights and poles. To ensure transparency and improve data quality, iOSPointMapper incorporates a user-guided annotation interface for validating system outputs before submission. Collected data is anonymized and transmitted to the Transportation Data Exchange Initiative (TDEI), where it integrates seamlessly with broader multimodal transportation datasets. Detailed evaluations of the system's feature detection and spatial mapping performance reveal the application's potential for enhanced pedestrian mapping. Together, these capabilities offer a scalable and user-centered approach to closing critical data gaps in pedestrian

</details>


### [159] [DeFloMat: Detection with Flow Matching for Stable and Efficient Generative Object Localization](https://arxiv.org/abs/2512.22406)
*Hansang Lee,Chaelin Lee,Nieun Seo,Joon Seok Lim,Helen Hong*

Main category: cs.CV

TL;DR: DeFloMat is a generative object detection framework that addresses the latency issues of diffusion-based detectors by integrating Conditional Flow Matching for faster and more accurate inference.


<details>
  <summary>Details</summary>
Motivation: To resolve the latency bottlenecks in existing diffusion-based detectors such as DiffusionDet, particularly for time-sensitive clinical applications like Crohn's Disease detection in Magnetic Resonance Enterography (MRE).

Method: DeFloMat employs Conditional Flow Matching based on Conditional Optimal Transport theory to approximate Rectified Flow for deterministic and faster inference using an Ordinary Differential Equation solver instead of the stochastic multi-step denoising process.

Result: DeFloMat achieves state-of-the-art accuracy (43.32% AP10:50) with only 3 inference steps, outperforming DiffusionDet's performance of 31.03% AP10:50 at 4 steps and achieving superior Recall and stability in fewer steps.

Conclusion: DeFloMat resolves the trade-off between high accuracy and latency, setting a new benchmark for rapid and stable object localization, particularly in clinical applications requiring efficiency and precision.

Abstract: We propose DeFloMat (Detection with Flow Matching), a novel generative object detection framework that addresses the critical latency bottleneck of diffusion-based detectors, such as DiffusionDet, by integrating Conditional Flow Matching (CFM). Diffusion models achieve high accuracy by formulating detection as a multi-step stochastic denoising process, but their reliance on numerous sampling steps ($T \gg 60$) makes them impractical for time-sensitive clinical applications like Crohn's Disease detection in Magnetic Resonance Enterography (MRE). DeFloMat replaces this slow stochastic path with a highly direct, deterministic flow field derived from Conditional Optimal Transport (OT) theory, specifically approximating the Rectified Flow. This shift enables fast inference via a simple Ordinary Differential Equation (ODE) solver. We demonstrate the superiority of DeFloMat on a challenging MRE clinical dataset. Crucially, DeFloMat achieves state-of-the-art accuracy ($43.32\% \text{ } AP_{10:50}$) in only $3$ inference steps, which represents a $1.4\times$ performance improvement over DiffusionDet's maximum converged performance ($31.03\% \text{ } AP_{10:50}$ at $4$ steps). Furthermore, our deterministic flow significantly enhances localization characteristics, yielding superior Recall and stability in the few-step regime. DeFloMat resolves the trade-off between generative accuracy and clinical efficiency, setting a new standard for stable and rapid object localization.

</details>


### [160] [Bright 4B: Scaling Hyperspherical Learning for Segmentation in 3D Brightfield Microscopy](https://arxiv.org/abs/2512.22423)
*Amil Khan,Matheus Palhares Viana,Suraj Mishra,B. S. Manjunath*

Main category: cs.CV

TL;DR: This study introduces Bright-4B, a 4 billion parameter model for segmenting subcellular structures directly from 3D brightfield microscopy, without relying on fluorescence or heavy post-processing.


<details>
  <summary>Details</summary>
Motivation: Current 3D brightfield microscopy struggles with robust segmentation of subcellular structures without fluorescence or extensive post-processing.

Method: Bright-4B integrates Native Sparse Attention, depth-width residual HyperConnections, a soft Mixture-of-Experts mechanism, and anisotropic patch embedding to achieve accurate, geometry-faithful segmentation.

Result: Bright-4B demonstrated superior segmentation performance on nuclei, mitochondria, and other organelles across multiple cell datasets compared to CNN and Transformer baselines.

Conclusion: Bright-4B allows for label-free and accurate segmentation in 3D brightfield microscopy, enabling large-scale cell mapping and advancements in the field.

Abstract: Label-free 3D brightfield microscopy offers a fast and noninvasive way to visualize cellular morphology, yet robust volumetric segmentation still typically depends on fluorescence or heavy post-processing. We address this gap by introducing Bright-4B, a 4 billion parameter foundation model that learns on the unit hypersphere to segment subcellular structures directly from 3D brightfield volumes. Bright-4B combines a hardware-aligned Native Sparse Attention mechanism (capturing local, coarse, and selected global context), depth-width residual HyperConnections that stabilize representation flow, and a soft Mixture-of-Experts for adaptive capacity. A plug-and-play anisotropic patch embed further respects confocal point-spread and axial thinning, enabling geometry-faithful 3D tokenization. The resulting model produces morphology-accurate segmentations of nuclei, mitochondria, and other organelles from brightfield stacks alone--without fluorescence, auxiliary channels, or handcrafted post-processing. Across multiple confocal datasets, Bright-4B preserves fine structural detail across depth and cell types, outperforming contemporary CNN and Transformer baselines. All code, pretrained weights, and models for downstream finetuning will be released to advance large-scale, label-free 3D cell mapping.

</details>


### [161] [FluenceFormer: Transformer-Driven Multi-Beam Fluence Map Regression for Radiotherapy Planning](https://arxiv.org/abs/2512.22425)
*Ujunwa Mgboh,Rafi Ibn Sultan,Joshua Kim,Kundan Thind,Dongxiao Zhu*

Main category: cs.CV

TL;DR: FluenceFormer, a transformer-based framework, addresses the limitations of convolutional methods in predicting fluence maps for automated radiotherapy by incorporating geometry-aware fluence regression with a two-stage design.


<details>
  <summary>Details</summary>
Motivation: Convolutional methods struggle to address long-range dependencies and structural inconsistencies in fluence map predictions, necessitating more advanced techniques for radiotherapy planning.

Method: FluenceFormer operates with a two-stage design: Stage 1 predicts global dose prior based on anatomy, while Stage 2 adjusts it based on beam geometry. A physics-informed loss function ensures accuracy and consistency.

Result: FluenceFormer with the Swin UNETR backbone outperformed other transformer and CNN-based models, reducing Energy Error to 4.5% and improving structural fidelity with statistical significance (p < 0.05).

Conclusion: FluenceFormer demonstrates better performance than existing methods for fluence map prediction, highlighting the potential of transformer-based architectures and geometry-aware approaches for automated radiotherapy planning.

Abstract: Fluence map prediction is central to automated radiotherapy planning but remains an ill-posed inverse problem due to the complex relationship between volumetric anatomy and beam-intensity modulation. Convolutional methods in prior work often struggle to capture long-range dependencies, which can lead to structurally inconsistent or physically unrealizable plans. We introduce \textbf{FluenceFormer}, a backbone-agnostic transformer framework for direct, geometry-aware fluence regression. The model uses a unified two-stage design: Stage~1 predicts a global dose prior from anatomical inputs, and Stage~2 conditions this prior on explicit beam geometry to regress physically calibrated fluence maps. Central to the approach is the \textbf{Fluence-Aware Regression (FAR)} loss, a physics-informed objective that integrates voxel-level fidelity, gradient smoothness, structural consistency, and beam-wise energy conservation. We evaluate the generality of the framework across multiple transformer backbones, including Swin UNETR, UNETR, nnFormer, and MedFormer, using a prostate IMRT dataset. FluenceFormer with Swin UNETR achieves the strongest performance among the evaluated models and improves over existing benchmark CNN and single-stage methods, reducing Energy Error to $\mathbf{4.5\%}$ and yielding statistically significant gains in structural fidelity ($p < 0.05$).

</details>


### [162] [EmoCtrl: Controllable Emotional Image Content Generation](https://arxiv.org/abs/2512.22437)
*Jingyuan Yang,Weibin Luo,Hui Huang*

Main category: cs.CV

TL;DR: The paper introduces a model to generate images that align with both given descriptions and target emotions, outperforming existing methods in maintaining content faithfulness and emotional expression.


<details>
  <summary>Details</summary>
Motivation: The study addresses the limitation of existing image generation models that either maintain content accuracy at the expense of emotional expression or prioritize emotional tone but distort content.

Method: The proposed method, EmoCtrl, utilizes a dataset with content, emotion, and affective prompts annotations, alongside modules for textual and visual emotion enhancement to balance semantic and perceptual aspects of emotional expression.

Result: EmoCtrl effectively generates images faithful to content descriptions while enriching emotional tone. Both qualitative and quantitative analyses, including user studies, show its superiority over existing methods.

Conclusion: EmoCtrl provides robust and adaptable generation capabilities, aligning with human preferences and offering potential utility in creative fields.

Abstract: An image conveys meaning through both its visual content and emotional tone, jointly shaping human perception. We introduce Controllable Emotional Image Content Generation (C-EICG), which aims to generate images that remain faithful to a given content description while expressing a target emotion. Existing text-to-image models ensure content consistency but lack emotional awareness, whereas emotion-driven models generate affective results at the cost of content distortion. To address this gap, we propose EmoCtrl, supported by a dataset annotated with content, emotion, and affective prompts, bridging abstract emotions to visual cues. EmoCtrl incorporates textual and visual emotion enhancement modules that enrich affective expression via descriptive semantics and perceptual cues. The learned emotion tokens exhibit complementary effects, as demonstrated through ablations and visualizations. Quantatitive and qualatitive experiments demonstrate that EmoCtrl achieves faithful content and expressive emotion control, outperforming existing methods across multiple aspects. User studies confirm EmoCtrl's strong alignment with human preference. Moreover, EmoCtrl generalizes well to creative applications, further demonstrating the robustness and adaptability of the learned emotion tokens.

</details>


### [163] [SuperiorGAT: Graph Attention Networks for Sparse LiDAR Point Cloud Reconstruction in Autonomous Systems](https://arxiv.org/abs/2512.22439)
*Khalfalla Awedat,Mohamed Abidalrekab,Gurcan Comert,Mustafa Ayad*

Main category: cs.CV

TL;DR: The paper introduces SuperiorGAT, a graph attention-based framework to reconstruct missing data in sparse LiDAR point clouds and evaluates it on KITTI datasets.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of fixed vertical beam resolution and environmental occlusions leading to beam dropout in LiDAR-based perception.

Method: Developed SuperiorGAT, a graph attention framework that models LiDAR scans as beam-aware graphs, incorporates gated fusion, and achieves reconstruction without deeper networks.

Result: Extensive experiments show lower reconstruction error and better geometric consistency compared to PointNet and baseline GAT models. Outputs maintain structural integrity and minimal distortions.

Conclusion: SuperiorGAT provides a computationally efficient solution to improve LiDAR resolution without the need for additional hardware by refining its architecture.

Abstract: LiDAR-based perception in autonomous systems is constrained by fixed vertical beam resolution and further compromised by beam dropout resulting from environmental occlusions. This paper introduces SuperiorGAT, a graph attention-based framework designed to reconstruct missing elevation information in sparse LiDAR point clouds. By modeling LiDAR scans as beam-aware graphs and incorporating gated residual fusion with feed-forward refinement, SuperiorGAT enables accurate reconstruction without increasing network depth. To evaluate performance, structured beam dropout is simulated by removing every fourth vertical scanning beam. Extensive experiments across diverse KITTI environments, including Person, Road, Campus, and City sequences, demonstrate that SuperiorGAT consistently achieves lower reconstruction error and improved geometric consistency compared to PointNet-based models and deeper GAT baselines. Qualitative X-Z projections further confirm the model's ability to preserve structural integrity with minimal vertical distortion. These results suggest that architectural refinement offers a computationally efficient method for improving LiDAR resolution without requiring additional sensor hardware.

</details>


### [164] [LECalib: Line-Based Event Camera Calibration](https://arxiv.org/abs/2512.22441)
*Zibin Liu,Banglei Guana,Yang Shanga,Zhenbao Yu,Yifei Bian,Qifeng Yu*

Main category: cs.CV

TL;DR: A new line-based framework is proposed for event camera calibration, using geometric lines from objects in man-made environments to improve efficiency and reduce manual intervention.


<details>
  <summary>Details</summary>
Motivation: Existing event camera calibration methods are inefficient, time-consuming, and require manually placed calibration objects, making them unsuitable for rapidly changing scenarios.

Method: The method detects geometric lines directly from event streams and uses an event-line calibration model for an initial parameter estimate, followed by non-linear optimization to refine the parameters.

Result: Simulation and real-world experiments validate the proposed method's accuracy and feasibility, applicable to both monocular and stereo event cameras.

Conclusion: The proposed approach offers an efficient, accurate, and automated alternative for event camera calibration, particularly useful in dynamic environments.

Abstract: Camera calibration is an essential prerequisite for event-based vision applications. Current event camera calibration methods typically involve using flashing patterns, reconstructing intensity images, and utilizing the features extracted from events. Existing methods are generally time-consuming and require manually placed calibration objects, which cannot meet the needs of rapidly changing scenarios. In this paper, we propose a line-based event camera calibration framework exploiting the geometric lines of commonly-encountered objects in man-made environments, e.g., doors, windows, boxes, etc. Different from previous methods, our method detects lines directly from event streams and leverages an event-line calibration model to generate the initial guess of camera parameters, which is suitable for both planar and non-planar lines. Then, a non-linear optimization is adopted to refine camera parameters. Both simulation and real-world experiments have demonstrated the feasibility and accuracy of our method, with validation performed on monocular and stereo event cameras. The source code is released at https://github.com/Zibin6/line_based_event_camera_calib.

</details>


### [165] [Towards Robust Optical-SAR Object Detection under Missing Modalities: A Dynamic Quality-Aware Fusion Framework](https://arxiv.org/abs/2512.22447)
*Zhicheng Zhao,Yuancheng Xu,Andong Lu,Chenglong Li,Jin Tang*

Main category: cs.CV

TL;DR: This paper introduces QDFNet, a fusion-based object detection method for optical and SAR images, designed to handle missing or corrupted data effectively.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with optical-SAR image fusion due to challenges like missing, degraded, or misaligned data.

Method: QDFNet uses Dynamic Modality Quality Assessment (DMQA) and Orthogonal Constraint Normalization Fusion (OCNF) modules to assess and adaptively fuse features, ensuring robustness against missing modalities.

Result: QDFNet outperforms state-of-the-art methods in object detection on SpaceNet6-OTD and OGSOD-2.0 datasets under conditions of partial modality degradation.

Conclusion: QDFNet ensures reliable performance in optical-SAR fusion-based object detection, addressing major limitations in the field and proving effective in real-world data scenarios.

Abstract: Optical and Synthetic Aperture Radar (SAR) fusion-based object detection has attracted significant research interest in remote sensing, as these modalities provide complementary information for all-weather monitoring. However, practical deployment is severely limited by inherent challenges. Due to distinct imaging mechanisms, temporal asynchrony, and registration difficulties, obtaining well-aligned optical-SAR image pairs remains extremely difficult, frequently resulting in missing or degraded modality data. Although recent approaches have attempted to address this issue, they still suffer from limited robustness to random missing modalities and lack effective mechanisms to ensure consistent performance improvement in fusion-based detection. To address these limitations, we propose a novel Quality-Aware Dynamic Fusion Network (QDFNet) for robust optical-SAR object detection. Our proposed method leverages learnable reference tokens to dynamically assess feature reliability and guide adaptive fusion in the presence of missing modalities. In particular, we design a Dynamic Modality Quality Assessment (DMQA) module that employs learnable reference tokens to iteratively refine feature reliability assessment, enabling precise identification of degraded regions and providing quality guidance for subsequent fusion. Moreover, we develop an Orthogonal Constraint Normalization Fusion (OCNF) module that employs orthogonal constraints to preserve modality independence while dynamically adjusting fusion weights based on reliability scores, effectively suppressing unreliable feature propagation. Extensive experiments on the SpaceNet6-OTD and OGSOD-2.0 datasets demonstrate the superiority and effectiveness of QDFNet compared to state-of-the-art methods, particularly under partial modality corruption or missing data scenarios.

</details>


### [166] [SonoVision: A Computer Vision Approach for Helping Visually Challenged Individuals Locate Objects with the Help of Sound Cues](https://arxiv.org/abs/2512.22449)
*Md Abu Obaida Zishan,Annajiat Alim Rasel*

Main category: cs.CV

TL;DR: SonoVision is a smartphone app aiding visually impaired individuals in locating objects using auditory cues. It employs sinusoidal sounds to inform users of an object's location (left, right, or front) via ear/headphones.


<details>
  <summary>Details</summary>
Motivation: To address the difficulty visually impaired individuals face in locating objects, thus enhancing their independence and reducing reliance on others.

Method: Developed a Flutter-based smartphone application that uses the Efficientdet-D2 model for object detection. It functions completely offline and communicates object locations via tailored auditory cues.

Result: SonoVision enables visually impaired individuals to locate everyday objects easily using auditory feedback, improving their autonomy, and can be freely accessed via GitHub.

Conclusion: SonoVision provides a safe, offline, and user-friendly solution for object localization, significantly assisting the visually impaired in daily life.

Abstract: Locating objects for the visually impaired is a significant challenge and is something no one can get used to over time. However, this hinders their independence and could push them towards risky and dangerous scenarios. Hence, in the spirit of making the visually challenged more self-sufficient, we present SonoVision, a smart-phone application that helps them find everyday objects using sound cues through earphones/headphones. This simply means, if an object is on the right or left side of a user, the app makes a sinusoidal sound in a user's respective ear through ear/headphones. However, to indicate objects located directly in front, both the left and right earphones are rung simultaneously. These sound cues could easily help a visually impaired individual locate objects with the help of their smartphones and reduce the reliance on people in their surroundings, consequently making them more independent. This application is made with the flutter development platform and uses the Efficientdet-D2 model for object detection in the backend. We believe the app will significantly assist the visually impaired in a safe and user-friendly manner with its capacity to work completely offline. Our application can be accessed here https://github.com/MohammedZ666/SonoVision.git.

</details>


### [167] [Pose-Guided Residual Refinement for Interpretable Text-to-Motion Generation and Editing](https://arxiv.org/abs/2512.22464)
*Sukhyun Jeong,Yong-Hoon Choi*

Main category: cs.CV

TL;DR: This paper introduces PGR$^2$M, a hybrid approach for generating and editing text-driven 3D motions, combining pose codes and residual codes to improve reconstruction fidelity and editability.


<details>
  <summary>Details</summary>
Motivation: Existing methods for text-based 3D motion generation and editing fail to capture fine-grained motion details and temporal dynamics, degrading reconstruction quality and control.

Method: The method uses pose-guided residual refinement, combining discrete pose codes with residual codes learned through residual vector quantization, alongside a dual-Transformer architecture for text-driven motion generation and editing.

Result: PGR$^2$M demonstrates improved metrics (e.g., Fréchet inception distance) for generation and editing compared to state-of-the-art methods, achieving better reconstruction fidelity and user-intuitive edits.

Conclusion: PGR$^2$M offers a robust solution to generate and edit 3D motion from text while preserving structure and fine details, validated by quantitative metrics and user studies.

Abstract: Text-based 3D motion generation aims to automatically synthesize diverse motions from natural-language descriptions to extend user creativity, whereas motion editing modifies an existing motion sequence in response to text while preserving its overall structure. Pose-code-based frameworks such as CoMo map quantifiable pose attributes into discrete pose codes that support interpretable motion control, but their frame-wise representation struggles to capture subtle temporal dynamics and high-frequency details, often degrading reconstruction fidelity and local controllability. To address this limitation, we introduce pose-guided residual refinement for motion (PGR$^2$M), a hybrid representation that augments interpretable pose codes with residual codes learned via residual vector quantization (RVQ). A pose-guided RVQ tokenizer decomposes motion into pose latents that encode coarse global structure and residual latents that model fine-grained temporal variations. Residual dropout further discourages over-reliance on residuals, preserving the semantic alignment and editability of the pose codes. On top of this tokenizer, a base Transformer autoregressively predicts pose codes from text, and a refine Transformer predicts residual codes conditioned on text, pose codes, and quantization stage. Experiments on HumanML3D and KIT-ML show that PGR$^2$M improves Fréchet inception distance and reconstruction metrics for both generation and editing compared with CoMo and recent diffusion- and tokenization-based baselines, while user studies confirm that it enables intuitive, structure-preserving motion edits.

</details>


### [168] [SAM 3D for 3D Object Reconstruction from Remote Sensing Images](https://arxiv.org/abs/2512.22452)
*Junsheng Yao,Lichao Mou,Qingyu Li*

Main category: cs.CV

TL;DR: This paper evaluates SAM 3D, a general-purpose foundation model, for monocular 3D reconstruction of remote sensing imagery, showing better results than TRELLIS and expanding its use to urban scene modeling.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitations of current monocular 3D building reconstruction methods that rely on task-specific structures and require intensive supervision, proposing a general-purpose solution.

Method: The authors systematically evaluate SAM 3D, benchmark it against TRELLIS using datasets and metrics like FID and CMMD, and extend its application to urban scene modeling using a segment-reconstruct-compose pipeline.

Result: SAM 3D demonstrates superior results, producing sharper boundaries and coherent geometry when compared to TRELLIS and shows feasibility for urban scene reconstruction.

Conclusion: The study confirms SAM 3D as an effective general-purpose model for 3D building and urban scene reconstruction, identifies limitations, and highlights the necessity for scene-level structural priors in future research.

Abstract: Monocular 3D building reconstruction from remote sensing imagery is essential for scalable urban modeling, yet existing methods often require task-specific architectures and intensive supervision. This paper presents the first systematic evaluation of SAM 3D, a general-purpose image-to-3D foundation model, for monocular remote sensing building reconstruction. We benchmark SAM 3D against TRELLIS on samples from the NYC Urban Dataset, employing Frechet Inception Distance (FID) and CLIP-based Maximum Mean Discrepancy (CMMD) as evaluation metrics. Experimental results demonstrate that SAM 3D produces more coherent roof geometry and sharper boundaries compared to TRELLIS. We further extend SAM 3D to urban scene reconstruction through a segment-reconstruct-compose pipeline, demonstrating its potential for urban scene modeling. We also analyze practical limitations and discuss future research directions. These findings provide practical guidance for deploying foundation models in urban 3D reconstruction and motivate future integration of scene-level structural priors.

</details>


### [169] [MUSON: A Reasoning-oriented Multimodal Dataset for Socially Compliant Navigation in Urban Environments](https://arxiv.org/abs/2512.22867)
*Zhuonan Liu,Xinyu Zhang,Zishuo Wang,Tomohito Kawabata,Xuesu Xiao,Ling Xiao*

Main category: cs.CV

TL;DR: The paper introduces MUSON, a multimodal dataset for socially compliant navigation that emphasizes structured reasoning, providing a robust benchmark for evaluating navigation models.


<details>
  <summary>Details</summary>
Motivation: Existing datasets for social navigation lack explicit reasoning supervision and have highly imbalanced action distributions, which limits the ability of models to learn critical safety behaviors.

Method: The authors created the MUSON dataset with a structured five-step Chain-of-Thought annotation approach, including components like perception, prediction, reasoning, action, and explanation, while addressing the modeling of static physical constraints and balanced action space.

Result: The dataset showed improved reasoning and explanation compared to existing datasets, and state-of-the-art models benchmarked on it, such as Qwen2.5-VL-3B, achieved high decision accuracy (0.8625).

Conclusion: MUSON is a valuable resource for advancing socially compliant navigation by offering a structured, reusable benchmark dataset with explicit reasoning and balanced action space.

Abstract: Socially compliant navigation requires structured reasoning over dynamic pedestrians and physical constraints to ensure safe and interpretable decisions. However, existing social navigation datasets often lack explicit reasoning supervision and exhibit highly long-tailed action distributions, limiting models' ability to learn safety-critical behaviors. To address these issues, we introduce MUSON, a multimodal dataset for short-horizon social navigation collected across diverse indoor and outdoor campus scenes. MUSON adopts a structured five-step Chain-of-Thought annotation consisting of perception, prediction, reasoning, action, and explanation, with explicit modeling of static physical constraints and a rationally balanced discrete action space. Compared to SNEI, MUSON provides consistent reasoning, action, and explanation. Benchmarking multiple state-of-the-art Small Vision Language Models on MUSON shows that Qwen2.5-VL-3B achieves the highest decision accuracy of 0.8625, demonstrating that MUSON serves as an effective and reusable benchmark for socially compliant navigation. The dataset is publicly available at https://huggingface.co/datasets/MARSLab/MUSON

</details>


### [170] [Comparing Object Detection Models for Electrical Substation Component Mapping](https://arxiv.org/abs/2512.22454)
*Haley Mody,Namish Bansal,Dennies Kiprono Bor,Edward J. Oughton*

Main category: cs.CV

TL;DR: This paper explores an autonomous solution for mapping components of electrical substations using computer vision models to assess and address vulnerabilities efficiently.


<details>
  <summary>Details</summary>
Motivation: The motivation is to prevent and mitigate failures in electrical substations due to hazards by automating the identification of critical substation components, improving upon the slow and labor-intensive traditional methods.

Method: The authors train and compare three computer vision models (YOLOv8, YOLOv11, RF-DETR) on a labeled dataset of US substation images, evaluating their accuracy, precision, and efficiency.

Result: The trained models successfully mapped various substation components, showcasing each model's strengths and limitations and demonstrating machine learning's capability for large-scale substation mapping.

Conclusion: The paper concludes that computer vision models provide effective and scalable solutions for mapping substation components, aiding infrastructure vulnerability assessments and hazard mitigation.

Abstract: Electrical substations are a significant component of an electrical grid. Indeed, the assets at these substations (e.g., transformers) are prone to disruption from many hazards, including hurricanes, flooding, earthquakes, and geomagnetically induced currents (GICs). As electrical grids are considered critical national infrastructure, any failure can have significant economic and public safety implications. To help prevent and mitigate these failures, it is thus essential that we identify key substation components to quantify vulnerability. Unfortunately, traditional manual mapping of substation infrastructure is time-consuming and labor-intensive. Therefore, an autonomous solution utilizing computer vision models is preferable, as it allows for greater convenience and efficiency. In this research paper, we train and compare the outputs of 3 models (YOLOv8, YOLOv11, RF-DETR) on a manually labeled dataset of US substation images. Each model is evaluated for detection accuracy, precision, and efficiency. We present the key strengths and limitations of each model, identifying which provides reliable and large-scale substation component mapping. Additionally, we utilize these models to effectively map the various substation components in the United States, showcasing a use case for machine learning in substation mapping.

</details>


### [171] [Event-based high temporal resolution measurement of shock wave motion field](https://arxiv.org/abs/2512.22474)
*Taihang Lei,Banglei Guan,Minzu Liang,Pengju Sun,Jing Tao,Yang Shang,Qifeng Yu*

Main category: cs.CV

TL;DR: This paper proposes a method using multiple event cameras for high-resolution shock wave motion measurement, achieving multi-angle measurements and high precision with minimal errors.


<details>
  <summary>Details</summary>
Motivation: Accurately measuring shock wave motion parameters is critical for applications in power field testing and damage assessment, but current methods face challenges due to fast and uneven propagation and unstable testing conditions.

Method: The framework uses multiple event cameras with high-speed and high-dynamic range capabilities. It involves encoding events in a polar coordinate system, adaptive ROI extraction, iterative slope analysis for velocity continuity, and deriving geometric models for 3D reconstruction of the shock wave motion field.

Result: The proposed method achieved a maximum error of 5.20% and a minimum error of 0.06% compared to pressure sensors and empirical formulas, demonstrating its high precision in measuring shock wave motion parameters.

Conclusion: The method provides accurate and high-resolution measurement of shock wave motion fields, representing significant progress in addressing challenges in this area.

Abstract: Accurate measurement of shock wave motion parameters with high spatiotemporal resolution is essential for applications such as power field testing and damage assessment. However, significant challenges are posed by the fast, uneven propagation of shock waves and unstable testing conditions. To address these challenges, a novel framework is proposed that utilizes multiple event cameras to estimate the asymmetry of shock waves, leveraging its high-speed and high-dynamic range capabilities. Initially, a polar coordinate system is established, which encodes events to reveal shock wave propagation patterns, with adaptive region-of-interest (ROI) extraction through event offset calculations. Subsequently, shock wave front events are extracted using iterative slope analysis, exploiting the continuity of velocity changes. Finally, the geometric model of events and shock wave motion parameters is derived according to event-based optical imaging model, along with the 3D reconstruction model. Through the above process, multi-angle shock wave measurement, motion field reconstruction, and explosive equivalence inversion are achieved. The results of the speed measurement are compared with those of the pressure sensors and the empirical formula, revealing a maximum error of 5.20% and a minimum error of 0.06%. The experimental results demonstrate that our method achieves high-precision measurement of the shock wave motion field with both high spatial and temporal resolution, representing significant progress.

</details>


### [172] [Scalpel-SAM: A Semi-Supervised Paradigm for Adapting SAM to Infrared Small Object Detection](https://arxiv.org/abs/2512.22483)
*Zihan Liu,Xiangning Ren,Dezhang Kong,Yipeng Zhang,Meng Han*

Main category: cs.CV

TL;DR: This paper introduces a semi-supervised paradigm for infrared small object detection with minimal annotated data, addressing challenges faced by existing methods like SAM.


<details>
  <summary>Details</summary>
Motivation: Existing methods face challenges in domain gaps, encoding physical priors, and architectural complexity, necessitating a new approach for detecting infrared small objects.

Method: Designed a Hierarchical MoE Adapter and a two-stage paradigm: Prior-Guided Knowledge Distillation using partial labeled data, and Deployment-Oriented Knowledge Transfer to train efficient models through pseudo labels.

Result: The paradigm achieves performance comparable to or better than fully supervised models with minimal annotations.

Conclusion: This is the first semi-supervised framework to systematically address data scarcity in infrared small object detection using SAM, offering a promising solution.

Abstract: Infrared small object detection urgently requires semi-supervised paradigms due to the high cost of annotation. However, existing methods like SAM face significant challenges of domain gaps, inability of encoding physical priors, and inherent architectural complexity. To address this, we designed a Hierarchical MoE Adapter consisting of four white-box neural operators. Building upon this core component, we propose a two-stage paradigm for knowledge distillation and transfer: (1) Prior-Guided Knowledge Distillation, where we use our MoE adapter and 10% of available fully supervised data to distill SAM into an expert teacher (Scalpel-SAM); and (2) Deployment-Oriented Knowledge Transfer, where we use Scalpel-SAM to generate pseudo labels for training lightweight and efficient downstream models. Experiments demonstrate that with minimal annotations, our paradigm enables downstream models to achieve performance comparable to, or even surpassing, their fully supervised counterparts. To our knowledge, this is the first semi-supervised paradigm that systematically addresses the data scarcity issue in IR-SOT using SAM as the teacher model.

</details>


### [173] [Tracking by Predicting 3-D Gaussians Over Time](https://arxiv.org/abs/2512.22489)
*Tanish Baranwal,Himanshu Gaurav Singh,Jathushan Rajasegaran,Jitendra Malik*

Main category: cs.CV

TL;DR: Proposed Video-GMAE, a self-supervised method encoding videos as Gaussian splats, achieving strong results for zero-shot tracking and outperforming benchmarks with fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To improve self-supervised video representation learning by introducing a model architecture with a realistic video representation bias for dynamic 3D scenes.

Method: Video-GMAE encodes video sequences into Gaussian representations to capture motion dynamics through pretraining, enabling zero-shot tracking and fine-tuning for video analysis tasks.

Result: The approach achieved state-of-the-art zero-shot tracking and significantly improved results (34.6% on Kinetics, 13.1% on Kubric datasets) after fine-tuning.

Conclusion: Video-GMAE inherently learns tracking capabilities and offers substantial performance improvements for video-related tasks, demonstrating its effectiveness as a self-supervised learning approach.

Abstract: We propose Video Gaussian Masked Autoencoders (Video-GMAE), a self-supervised approach for representation learning that encodes a sequence of images into a set of Gaussian splats moving over time. Representing a video as a set of Gaussians enforces a reasonable inductive bias: that 2-D videos are often consistent projections of a dynamic 3-D scene. We find that tracking emerges when pretraining a network with this architecture. Mapping the trajectory of the learnt Gaussians onto the image plane gives zero-shot tracking performance comparable to state-of-the-art. With small-scale finetuning, our models achieve 34.6% improvement on Kinetics, and 13.1% on Kubric datasets, surpassing existing self-supervised video approaches. The project page and code are publicly available at https://videogmae.org/ and https://github.com/tekotan/video-gmae.

</details>


### [174] [SCAFusion: A Multimodal 3D Detection Framework for Small Object Detection in Lunar Surface Exploration](https://arxiv.org/abs/2512.22503)
*Xin Chen,Kang Luo,Yangyi Xiao,Hesheng Wang*

Main category: cs.CV

TL;DR: SCAFusion is a new 3D object detection model designed for lunar missions, focusing on better detection of small and irregular objects.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for reliable detection of small and irregular objects, such as meteor fragments and rocks, which are critical for lunar exploration but challenging for existing 3D perception methods.

Method: The SCAFusion model introduces a Cognitive Adapter for camera tuning, Contrastive Alignment Module for improving feature consistency, Camera Auxiliary Training Branch for stronger visual representation, and Section-aware Coordinate Attention for better detection of small targets.

Result: The model achieves 69.7% mAP and 72.1% NDS on the nuScenes validation set, with improvements of 5.0% and 2.7% over the baseline. In lunar environments, it achieves 90.93% mAP, outperforming the baseline by 11.5%.

Conclusion: SCAFusion is highly effective for small object detection in lunar missions, demonstrating significant performance gains in both terrestrial and simulated lunar settings.

Abstract: Reliable and precise detection of small and irregular objects, such as meteor fragments and rocks, is critical for autonomous navigation and operation in lunar surface exploration. Existing multimodal 3D perception methods designed for terrestrial autonomous driving often underperform in off world environments due to poor feature alignment, limited multimodal synergy, and weak small object detection. This paper presents SCAFusion, a multimodal 3D object detection model tailored for lunar robotic missions. Built upon the BEVFusion framework, SCAFusion integrates a Cognitive Adapter for efficient camera backbone tuning, a Contrastive Alignment Module to enhance camera LiDAR feature consistency, a Camera Auxiliary Training Branch to strengthen visual representation, and most importantly, a Section aware Coordinate Attention mechanism explicitly designed to boost the detection performance of small, irregular targets. With negligible increase in parameters and computation, our model achieves 69.7% mAP and 72.1% NDS on the nuScenes validation set, improving the baseline by 5.0% and 2.7%, respectively. In simulated lunar environments built on Isaac Sim, SCAFusion achieves 90.93% mAP, outperforming the baseline by 11.5%, with notable gains in detecting small meteor like obstacles.

</details>


### [175] [DreamOmni3: Scribble-based Editing and Generation](https://arxiv.org/abs/2512.22525)
*Bin Xia,Bohao Peng,Jiyang Liu,Sitong Wu,Jingyao Li,Junjia Huang,Xu Zhao,Yitong Wang,Ruihang Chu,Bei Yu,Jiaya Jia*

Main category: cs.CV

TL;DR: DreamOmni3 introduces scribble-based editing and generation tasks for graphical user interfaces using textual, image, and freehand inputs for flexible creation.


<details>
  <summary>Details</summary>
Motivation: Existing models relying on text prompts often fail to capture detailed edits and user intentions, necessitating more flexible interaction methods.

Method: DreamOmni3 employs a novel data synthesis pipeline and joint input scheme distinguishing editable regions for precise model processing and editing.

Result: Comprehensive benchmarks showcase DreamOmni3's strong performance in scribble-based editing and generation tasks.

Conclusion: DreamOmni3 establishes new benchmarks and innovative methods, enabling improved and more flexible graphical creation across diverse tasks.

Abstract: Recently unified generation and editing models have achieved remarkable success with their impressive performance. These models rely mainly on text prompts for instruction-based editing and generation, but language often fails to capture users intended edit locations and fine-grained visual details. To this end, we propose two tasks: scribble-based editing and generation, that enables more flexible creation on graphical user interface (GUI) combining user textual, images, and freehand sketches. We introduce DreamOmni3, tackling two challenges: data creation and framework design. Our data synthesis pipeline includes two parts: scribble-based editing and generation. For scribble-based editing, we define four tasks: scribble and instruction-based editing, scribble and multimodal instruction-based editing, image fusion, and doodle editing. Based on DreamOmni2 dataset, we extract editable regions and overlay hand-drawn boxes, circles, doodles or cropped image to construct training data. For scribble-based generation, we define three tasks: scribble and instruction-based generation, scribble and multimodal instruction-based generation, and doodle generation, following similar data creation pipelines. For the framework, instead of using binary masks, which struggle with complex edits involving multiple scribbles, images, and instructions, we propose a joint input scheme that feeds both the original and scribbled source images into the model, using different colors to distinguish regions and simplify processing. By applying the same index and position encodings to both images, the model can precisely localize scribbled regions while maintaining accurate editing. Finally, we establish comprehensive benchmarks for these tasks to promote further research. Experimental results demonstrate that DreamOmni3 achieves outstanding performance, and models and code will be publicly released.

</details>


### [176] [CoAgent: Collaborative Planning and Consistency Agent for Coherent Video Generation](https://arxiv.org/abs/2512.22536)
*Qinglin Zeng,Kaitong Cai,Ruiqi Chen,Qinhan Lv,Keze Wang*

Main category: cs.CV

TL;DR: CoAgent enhances video generation with coherent narratives, visual consistency, and controlled pacing using a structured and closed-loop pipeline.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-video models fail to ensure narrative coherence, resulting in issues like identity drift and scene inconsistency.

Method: CoAgent uses a plan-synthesize-verify pipeline, including Storyboard Planner, Global Context Manager, Visual Consistency Controller, Verifier Agent, and pacing-aware editor.

Result: Experiments show that CoAgent outperforms previous models, improving coherence, visual consistency, and narrative quality in long-form videos.

Conclusion: CoAgent effectively addresses narrative coherence and visual consistency challenges in open-domain video generation, providing innovative and reliable results.

Abstract: Maintaining narrative coherence and visual consistency remains a central challenge in open-domain video generation. Existing text-to-video models often treat each shot independently, resulting in identity drift, scene inconsistency, and unstable temporal structure. We propose CoAgent, a collaborative and closed-loop framework for coherent video generation that formulates the process as a plan-synthesize-verify pipeline. Given a user prompt, style reference, and pacing constraints, a Storyboard Planner decomposes the input into structured shot-level plans with explicit entities, spatial relations, and temporal cues. A Global Context Manager maintains entity-level memory to preserve appearance and identity consistency across shots. Each shot is then generated by a Synthesis Module under the guidance of a Visual Consistency Controller, while a Verifier Agent evaluates intermediate results using vision-language reasoning and triggers selective regeneration when inconsistencies are detected. Finally, a pacing-aware editor refines temporal rhythm and transitions to match the desired narrative flow. Extensive experiments demonstrate that CoAgent significantly improves coherence, visual consistency, and narrative quality in long-form video generation.

</details>


### [177] [Self-Rewarded Multimodal Coherent Reasoning Across Diverse Visual Domains](https://arxiv.org/abs/2512.22545)
*Jesen Zhang,Ningyuan Liu,Kaitong Cai,Sidi Liu,Jing Yang,Ziliang Chen,Xiaofei Sun,Keze Wang*

Main category: cs.CV

TL;DR: The paper introduces SR-MCR, a framework to improve reasoning reliability in multimodal LLMs by using intrinsic process signals without labels. It achieves superior accuracy and reasoning coherence against visual benchmarks.


<details>
  <summary>Details</summary>
Motivation: Multimodal language models often generate reasoning results that lack coherence and visual grounding due to reliance on final answers rather than intermediate reasoning processes.

Method: The framework integrates five self-referential cues into a reliability-weighted reward that guides reasoning and stabilizes training using a critic-free GRPO objective and confidence-aware cooling mechanism.

Result: SR-MCR enhances accuracy and reasoning coherence, achieving state-of-the-art performance (81.4% accuracy) among similar open-source 7 billion parameter models.

Conclusion: The approach demonstrates the importance of process-level alignment and self-referential signals for improving reasoning reliability in multimodal LLMs.

Abstract: Multimodal LLMs often produce fluent yet unreliable reasoning, exhibiting weak step-to-step coherence and insufficient visual grounding, largely because existing alignment approaches supervise only the final answer while ignoring the reliability of the intermediate reasoning process. We introduce SR-MCR, a lightweight and label-free framework that aligns reasoning by exploiting intrinsic process signals derived directly from model outputs. Five self-referential cues -- semantic alignment, lexical fidelity, non-redundancy, visual grounding, and step consistency -- are integrated into a normalized, reliability-weighted reward that provides fine-grained process-level guidance. A critic-free GRPO objective, enhanced with a confidence-aware cooling mechanism, further stabilizes training and suppresses trivial or overly confident generations. Built on Qwen2.5-VL, SR-MCR improves both answer accuracy and reasoning coherence across a broad set of visual benchmarks; among open-source models of comparable size, SR-MCR-7B achieves state-of-the-art performance with an average accuracy of 81.4%. Ablation studies confirm the independent contributions of each reward term and the cooling module.

</details>


### [178] [ReFRM3D: A Radiomics-enhanced Fused Residual Multiparametric 3D Network with Multi-Scale Feature Fusion for Glioma Characterization](https://arxiv.org/abs/2512.22570)
*Md. Abdur Rahman,Mohaimenul Azam Khan Raiaan,Arefin Ittesafun Abian,Yan Zhang,Mirjam Jonkman,Sami Azam*

Main category: cs.CV

TL;DR: The paper introduces ReFRM3D, a novel radiomics-enhanced 3D network for glioma segmentation and classification that shows improved performance on BraTS datasets.


<details>
  <summary>Details</summary>
Motivation: To address challenges in glioma diagnosis including variability in imaging, suboptimal computational resource use, and ineffective segmentation and classification methods.

Method: This study proposed ReFRM3D, a 3D U-Net-based architecture incorporating multi-scale feature fusion, hybrid upsampling, and extended residual skip mechanisms. Further, a tumor marker-based classifier was developed using radiomic features.

Result: The model achieved high Dice Similarity Coefficients: 94.04% (WT), 92.68% (ET), 93.64% (TC) on BraTS2019; 94.09% (WT), 92.91% (ET), 93.84% (TC) on BraTS2020; 93.70% (WT), 90.36% (ET), 92.13% (TC) on BraTS2021.

Conclusion: The proposed techniques improve efficiency and accuracy in glioma tumor segmentation and classification, paving the way for enhanced diagnosis and tracking of this aggressive brain cancer.

Abstract: Gliomas are among the most aggressive cancers, characterized by high mortality rates and complex diagnostic processes. Existing studies on glioma diagnosis and classification often describe issues such as high variability in imaging data, inadequate optimization of computational resources, and inefficient segmentation and classification of gliomas. To address these challenges, we propose novel techniques utilizing multi-parametric MRI data to enhance tumor segmentation and classification efficiency. Our work introduces the first-ever radiomics-enhanced fused residual multiparametric 3D network (ReFRM3D) for brain tumor characterization, which is based on a 3D U-Net architecture and features multi-scale feature fusion, hybrid upsampling, and an extended residual skip mechanism. Additionally, we propose a multi-feature tumor marker-based classifier that leverages radiomic features extracted from the segmented regions. Experimental results demonstrate significant improvements in segmentation performance across the BraTS2019, BraTS2020, and BraTS2021 datasets, achieving high Dice Similarity Coefficients (DSC) of 94.04%, 92.68%, and 93.64% for whole tumor (WT), enhancing tumor (ET), and tumor core (TC) respectively in BraTS2019; 94.09%, 92.91%, and 93.84% in BraTS2020; and 93.70%, 90.36%, and 92.13% in BraTS2021.

</details>


### [179] [KV-Tracker: Real-Time Pose Tracking with Transformers](https://arxiv.org/abs/2512.22581)
*Marwan Taher,Ignacio Alzugaray,Kirill Mazur,Xin Kong,Andrew J. Davison*

Main category: cs.CV

TL;DR: The paper proposes a novel real-time method for 6-DoF pose tracking and online 3D reconstruction using monocular RGB videos, achieving significant speed-ups without retraining.


<details>
  <summary>Details</summary>
Motivation: Real-time 6-DoF pose tracking and 3D reconstruction is crucial for applications like AR/VR and robotics; however, existing multi-view 3D geometry networks are too slow for practical uses.

Method: The method introduces a keyframe selection and management strategy combined with caching global self-attention block's key-value pairs, making the process significantly faster and model-agnostic.

Result: The approach achieves up to 15x speedup and operates at up to 27 FPS, with strong performance demonstrated on multiple benchmark datasets for scene and object tracking without depth or prior information.

Conclusion: This efficient, generalizable approach enables fast and accurate online 3D tracking and reconstruction, making it viable for practical real-time applications.

Abstract: Multi-view 3D geometry networks offer a powerful prior but are prohibitively slow for real-time applications. We propose a novel way to adapt them for online use, enabling real-time 6-DoF pose tracking and online reconstruction of objects and scenes from monocular RGB videos. Our method rapidly selects and manages a set of images as keyframes to map a scene or object via $π^3$ with full bidirectional attention. We then cache the global self-attention block's key-value (KV) pairs and use them as the sole scene representation for online tracking. This allows for up to $15\times$ speedup during inference without the fear of drift or catastrophic forgetting. Our caching strategy is model-agnostic and can be applied to other off-the-shelf multi-view networks without retraining. We demonstrate KV-Tracker on both scene-level tracking and the more challenging task of on-the-fly object tracking and reconstruction without depth measurements or object priors. Experiments on the TUM RGB-D, 7-Scenes, Arctic and OnePose datasets show the strong performance of our system while maintaining high frame-rates up to ${\sim}27$ FPS.

</details>


### [180] [PTalker: Personalized Speech-Driven 3D Talking Head Animation via Style Disentanglement and Modality Alignment](https://arxiv.org/abs/2512.22602)
*Bin Wang,Yang Xu,Huan Zhao,Hao Zhang,Zixing Zhang*

Main category: cs.CV

TL;DR: The study introduces PTalker, a framework for creating personalized 3D talking heads that accurately reflect speaking styles while improving lip synchronization.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 3D talking head generation lack personalization and do not capture individual speaking styles, limiting realism.

Method: PTalker employs style disentanglement to separate speaking style from content, and utilizes a three-level alignment mechanism (spatial, temporal, feature-level) to synchronize speech and mesh modalities.

Result: PTalker demonstrates superior performance in generating realistic, stylized 3D heads with accurate identity-specific speaking styles compared to state-of-the-art methods.

Conclusion: PTalker advances 3D talking head animation by improving both realism and synchronization, promising enhanced personalization in lifelike facial animations.

Abstract: Speech-driven 3D talking head generation aims to produce lifelike facial animations precisely synchronized with speech. While considerable progress has been made in achieving high lip-synchronization accuracy, existing methods largely overlook the intricate nuances of individual speaking styles, which limits personalization and realism. In this work, we present a novel framework for personalized 3D talking head animation, namely "PTalker". This framework preserves speaking style through style disentanglement from audio and facial motion sequences and enhances lip-synchronization accuracy through a three-level alignment mechanism between audio and mesh modalities. Specifically, to effectively disentangle style and content, we design disentanglement constraints that encode driven audio and motion sequences into distinct style and content spaces to enhance speaking style representation. To improve lip-synchronization accuracy, we adopt a modality alignment mechanism incorporating three aspects: spatial alignment using Graph Attention Networks to capture vertex connectivity in the 3D mesh structure, temporal alignment using cross-attention to capture and synchronize temporal dependencies, and feature alignment by top-k bidirectional contrastive losses and KL divergence constraints to ensure consistency between speech and mesh modalities. Extensive qualitative and quantitative experiments on public datasets demonstrate that PTalker effectively generates realistic, stylized 3D talking heads that accurately match identity-specific speaking styles, outperforming state-of-the-art methods. The source code and supplementary videos are available at: PTalker.

</details>


### [181] [Enhancing Noise Resilience in Face Clustering via Sparse Differential Transformer](https://arxiv.org/abs/2512.22612)
*Dafeng Zhang,Yongqi Song,Shizhuo Liu*

Main category: cs.CV

TL;DR: This paper improves face clustering performance by introducing a Sparse Differential Transformer (SDT) for better face embedding similarity measurements.


<details>
  <summary>Details</summary>
Motivation: Existing methods to measure relationships between face embeddings lack sufficient discriminative power and introduce irrelevant nodes, negatively impacting clustering performance.

Method: The authors propose a prediction-driven Top-K Jaccard similarity combined with a Sparse Differential Transformer (SDT) to refine similarity estimates and eliminate noise in face embeddings.

Result: Experiments on face clustering datasets (e.g., MS-Celeb-1M) demonstrate superior performance over state-of-the-art (SOTA) approaches.

Conclusion: The proposed method significantly improves reliability and clustering accuracy, addressing key limitations in existing similarity measurement techniques.

Abstract: The method used to measure relationships between face embeddings plays a crucial role in determining the performance of face clustering. Existing methods employ the Jaccard similarity coefficient instead of the cosine distance to enhance the measurement accuracy. However, these methods introduce too many irrelevant nodes, producing Jaccard coefficients with limited discriminative power and adversely affecting clustering performance. To address this issue, we propose a prediction-driven Top-K Jaccard similarity coefficient that enhances the purity of neighboring nodes, thereby improving the reliability of similarity measurements. Nevertheless, accurately predicting the optimal number of neighbors (Top-K) remains challenging, leading to suboptimal clustering results. To overcome this limitation, we develop a Transformer-based prediction model that examines the relationships between the central node and its neighboring nodes near the Top-K to further enhance the reliability of similarity estimation. However, vanilla Transformer, when applied to predict relationships between nodes, often introduces noise due to their overemphasis on irrelevant feature relationships. To address these challenges, we propose a Sparse Differential Transformer (SDT), instead of the vanilla Transformer, to eliminate noise and enhance the model's anti-noise capabilities. Extensive experiments on multiple datasets, such as MS-Celeb-1M, demonstrate that our approach achieves state-of-the-art (SOTA) performance, outperforming existing methods and providing a more robust solution for face clustering.

</details>


### [182] [Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone](https://arxiv.org/abs/2512.22615)
*Jiacheng Ye,Shansan Gong,Jiahui Gao,Junming Fan,Shuang Wu,Wei Bi,Haoli Bai,Lifeng Shang,Lingpeng Kong*

Main category: cs.CV

TL;DR: The paper introduces Dream-VL, a diffusion-based vision-language model, and Dream-VLA, a vision-language-action model, exhibiting superior performance in visual planning and robotic control tasks compared to autoregressive models.


<details>
  <summary>Details</summary>
Motivation: To address limitations of autoregressive large vision-language models in complex visual planning and dynamic robotic control tasks.

Method: The researchers utilized diffusion-based language models to create Dream-VL and Dream-VLA. Dream-VL focuses on state-of-the-art performance in visual planning tasks and Dream-VLA employs continuous robotic dataset pre-training for better action modeling in VLA tasks.

Result: Dream-VLA achieves 97.2% success rate on LIBERO, 71.4% average on SimplerEnv-Bridge, and 60.5% average on SimplerEnv-Fractal, surpassing other leading models.

Conclusion: Diffusion-based vision-language models demonstrate superior capabilities in visual-language and action tasks, paving the way for advancements in robotic control and visual planning fields.

Abstract: While autoregressive Large Vision-Language Models (VLMs) have achieved remarkable success, their sequential generation often limits their efficacy in complex visual planning and dynamic robotic control. In this work, we investigate the potential of constructing Vision-Language Models upon diffusion-based large language models (dLLMs) to overcome these limitations. We introduce Dream-VL, an open diffusion-based VLM (dVLM) that achieves state-of-the-art performance among previous dVLMs. Dream-VL is comparable to top-tier AR-based VLMs trained on open data on various benchmarks but exhibits superior potential when applied to visual planning tasks. Building upon Dream-VL, we introduce Dream-VLA, a dLLM-based Vision-Language-Action model (dVLA) developed through continuous pre-training on open robotic datasets. We demonstrate that the natively bidirectional nature of this diffusion backbone serves as a superior foundation for VLA tasks, inherently suited for action chunking and parallel generation, leading to significantly faster convergence in downstream fine-tuning. Dream-VLA achieves top-tier performance of 97.2% average success rate on LIBERO, 71.4% overall average on SimplerEnv-Bridge, and 60.5% overall average on SimplerEnv-Fractal, surpassing leading models such as $π_0$ and GR00T-N1. We also validate that dVLMs surpass AR baselines on downstream tasks across different training objectives. We release both Dream-VL and Dream-VLA to facilitate further research in the community.

</details>


### [183] [Rethinking Memory Design in SAM-Based Visual Object Tracking](https://arxiv.org/abs/2512.22624)
*Mohamad Alansari,Muzammal Naseer,Hasan Al Marzouqi,Naoufel Werghi,Sajid Javed*

Main category: cs.CV

TL;DR: The paper investigates memory design principles in segmentation-based visual object tracking using SAM2 and SAM3 models, proposing a hybrid memory framework for improved performance.


<details>
  <summary>Details</summary>
Motivation: To address limited understanding and method-specific approaches in memory mechanisms for SAM-based visual tracking, and to analyze the transferability of these mechanisms to next-generation models (SAM3).

Method: A systematic study of SAM2 and SAM3-based trackers is conducted, followed by reimplementation and evaluation of memory mechanisms. A hybrid memory framework is proposed, decomposing memory into short-term appearance and long-term distractor-resolving components.

Result: Experiments show that the proposed hybrid memory framework improves robustness under challenging scenarios like occlusion, complex motion, and distractors on both SAM2 and SAM3 backbones.

Conclusion: The study reveals generalizable memory design insights and introduces a unified memory framework, which enhances performance in SAM-based tracking models.

Abstract: \noindent Memory has become the central mechanism enabling robust visual object tracking in modern segmentation-based frameworks. Recent methods built upon Segment Anything Model 2 (SAM2) have demonstrated strong performance by refining how past observations are stored and reused. However, existing approaches address memory limitations in a method-specific manner, leaving the broader design principles of memory in SAM-based tracking poorly understood. Moreover, it remains unclear how these memory mechanisms transfer to stronger, next-generation foundation models such as Segment Anything Model 3 (SAM3). In this work, we present a systematic memory-centric study of SAM-based visual object tracking. We first analyze representative SAM2-based trackers and show that most methods primarily differ in how short-term memory frames are selected, while sharing a common object-centric representation. Building on this insight, we faithfully reimplement these memory mechanisms within the SAM3 framework and conduct large-scale evaluations across ten diverse benchmarks, enabling a controlled analysis of memory design independent of backbone strength. Guided by our empirical findings, we propose a unified hybrid memory framework that explicitly decomposes memory into short-term appearance memory and long-term distractor-resolving memory. This decomposition enables the integration of existing memory policies in a modular and principled manner. Extensive experiments demonstrate that the proposed framework consistently improves robustness under long-term occlusion, complex motion, and distractor-heavy scenarios on both SAM2 and SAM3 backbones. Code is available at: https://github.com/HamadYA/SAM3_Tracking_Zoo. \textbf{This is a preprint. Some results are being finalized and may be updated in a future revision.}

</details>


### [184] [Envision: Embodied Visual Planning via Goal-Imagery Video Diffusion](https://arxiv.org/abs/2512.22626)
*Yuming Gu,Yizhi Wang,Yining Hong,Yipeng Gao,Hao Jiang,Angtian Wang,Bo Liu,Nathaniel S. Dennler,Zhengfei Kuang,Hao Li,Gordon Wetzstein,Chongyang Ma*

Main category: cs.CV

TL;DR: Envision is a diffusion-based framework designed for embodied visual planning, enabling agents to generate video trajectories that maintain goal alignment and spatial consistency.


<details>
  <summary>Details</summary>
Motivation: Traditional approaches to visual planning often result in spatial drift and goal misalignment due to the lack of explicit goal modeling in trajectory generation.

Method: Envision introduces a two-stage framework: first, a Goal Imagery Model is used to synthesize a goal image; second, an Env-Goal Video Model interpolates between the start and goal using a video diffusion approach to ensure smooth and plausible trajectories.

Result: Envision outperforms baselines in goal alignment, spatial consistency, and object preservation in experiments on object manipulation and image editing tasks.

Conclusion: Envision effectively bridges the gap between initial states and desired goals in visual planning, enabling its use in downstream robotic planning and control tasks.

Abstract: Embodied visual planning aims to enable manipulation tasks by imagining how a scene evolves toward a desired goal and using the imagined trajectories to guide actions. Video diffusion models, through their image-to-video generation capability, provide a promising foundation for such visual imagination. However, existing approaches are largely forward predictive, generating trajectories conditioned on the initial observation without explicit goal modeling, thus often leading to spatial drift and goal misalignment. To address these challenges, we propose Envision, a diffusion-based framework that performs visual planning for embodied agents. By explicitly constraining the generation with a goal image, our method enforces physical plausibility and goal consistency throughout the generated trajectory. Specifically, Envision operates in two stages. First, a Goal Imagery Model identifies task-relevant regions, performs region-aware cross attention between the scene and the instruction, and synthesizes a coherent goal image that captures the desired outcome. Then, an Env-Goal Video Model, built upon a first-and-last-frame-conditioned video diffusion model (FL2V), interpolates between the initial observation and the goal image, producing smooth and physically plausible video trajectories that connect the start and goal states. Experiments on object manipulation and image editing benchmarks demonstrate that Envision achieves superior goal alignment, spatial consistency, and object preservation compared to baselines. The resulting visual plans can directly support downstream robotic planning and control, providing reliable guidance for embodied agents.

</details>


### [185] [FinPercep-RM: A Fine-grained Reward Model and Co-evolutionary Curriculum for RL-based Real-world Super-Resolution](https://arxiv.org/abs/2512.22647)
*Yidi Liu,Zihao Fan,Jie Huang,Jie Xiao,Dong Li,Wenlong Zhang,Lei Bai,Xueyang Fu,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: The paper introduces a system for improving image super-resolution tasks with reinforcement learning methods. It addresses flaws in traditional IQA models by introducing a fine-grained reward model and a co-evolutionary training strategy to enhance perceptual quality.


<details>
  <summary>Details</summary>
Motivation: To improve the perceptual quality of image super-resolution tasks by addressing insensitivity of traditional IQA models to local distortions and overcoming issues like reward hacking.

Method: A Fine-grained Perceptual Reward Model (FinPercep-RM) was proposed, which includes a global quality score and a Perceptual Degradation Map. A co-evolutionary curriculum learning (CCL) technique was introduced for synchronized reward model and generator training.

Result: Experimental results demonstrated the effectiveness of the proposed method in improving both global and local perceptual qualities in image super-resolution tasks with enhanced training stability.

Conclusion: The paper concludes that using a fine-grained reward model combined with co-evolutionary curriculum learning significantly enhances perceptual quality and stability in reinforcement learning for image super-resolution.

Abstract: Reinforcement Learning with Human Feedback (RLHF) has proven effective in image generation field guided by reward models to align human preferences. Motivated by this, adapting RLHF for Image Super-Resolution (ISR) tasks has shown promise in optimizing perceptual quality with Image Quality Assessment (IQA) model as reward models. However, the traditional IQA model usually output a single global score, which are exceptionally insensitive to local and fine-grained distortions. This insensitivity allows ISR models to produce perceptually undesirable artifacts that yield spurious high scores, misaligning optimization objectives with perceptual quality and results in reward hacking. To address this, we propose a Fine-grained Perceptual Reward Model (FinPercep-RM) based on an Encoder-Decoder architecture. While providing a global quality score, it also generates a Perceptual Degradation Map that spatially localizes and quantifies local defects. We specifically introduce the FGR-30k dataset to train this model, consisting of diverse and subtle distortions from real-world super-resolution models. Despite the success of the FinPercep-RM model, its complexity introduces significant challenges in generator policy learning, leading to training instability. To address this, we propose a Co-evolutionary Curriculum Learning (CCL) mechanism, where both the reward model and the ISR model undergo synchronized curricula. The reward model progressively increases in complexity, while the ISR model starts with a simpler global reward for rapid convergence, gradually transitioning to the more complex model outputs. This easy-to-hard strategy enables stable training while suppressing reward hacking. Experiments validates the effectiveness of our method across ISR models in both global quality and local realism on RLHF methods.

</details>


### [186] [Visual Autoregressive Modelling for Monocular Depth Estimation](https://arxiv.org/abs/2512.22653)
*Amir El-Ghoussani,André Kaup,Nassir Navab,Gustavo Carneiro,Vasileios Belagiannis*

Main category: cs.CV

TL;DR: The paper presents a monocular depth estimation technique using visual autoregressive (VAR) priors, outperforming other methods in certain benchmarks while being highly data-efficient.


<details>
  <summary>Details</summary>
Motivation: The study aims to address challenges in monocular depth estimation by introducing a novel VAR-based method that competes with diffusion models and overcomes limitations in data scalability and adaptability.

Method: The paper adapts a large-scale text-to-image VAR model, introduces a scale-wise conditional upsampling mechanism, and utilizes classifier-free guidance for depth estimation. It uses only 74K synthetic samples for refinement and follows a fixed 10-stage autoregressive inference process.

Result: The method achieves state-of-the-art performance on indoor benchmarks under restricted training conditions and performs well on outdoor datasets, showcasing versatility and effectiveness.

Conclusion: Autoregressive priors are shown as a viable and efficient alternative in depth estimation, excelling in data scalability and adaptability to 3D vision tasks while requiring less data for fine-tuning.

Abstract: We propose a monocular depth estimation method based on visual autoregressive (VAR) priors, offering an alternative to diffusion-based approaches. Our method adapts a large-scale text-to-image VAR model and introduces a scale-wise conditional upsampling mechanism with classifier-free guidance. Our approach performs inference in ten fixed autoregressive stages, requiring only 74K synthetic samples for fine-tuning, and achieves competitive results. We report state-of-the-art performance in indoor benchmarks under constrained training conditions, and strong performance when applied to outdoor datasets. This work establishes autoregressive priors as a complementary family of geometry-aware generative models for depth estimation, highlighting advantages in data scalability, and adaptability to 3D vision tasks. Code available at "https://github.com/AmirMaEl/VAR-Depth".

</details>


### [187] [Investigating Deep Learning Models for Ejection Fraction Estimation from Echocardiography Videos](https://arxiv.org/abs/2512.22657)
*Shravan Saranyan,Pramit Saha*

Main category: cs.CV

TL;DR: This paper explores deep learning models for automated left ventricular ejection fraction (LVEF) estimation from echocardiography videos, achieving promising result using modified 3D Inception architectures.


<details>
  <summary>Details</summary>
Motivation: Manual assessment of LVEF from echocardiograms is time-consuming and prone to inter-observer variability, highlighting the need for fast and reliable alternatives.

Method: The study evaluates deep learning architectures like 3D Inception, two-stream, and CNN-RNN. Modifications and fusion strategies were explored, and models were trained on the EchoNet-Dynamic dataset of 10,030 videos.

Result: The modified 3D Inception networks showed the best performance with a root mean squared error (RMSE) of 6.79%, and smaller/simpler models demonstrated better generalization.

Conclusion: Deep learning, specifically modified 3D Inception architectures, is a promising approach for LVEF estimation, with potential applicability to other video analysis tasks in both medical and non-medical domains.

Abstract: Left ventricular ejection fraction (LVEF) is a key indicator of cardiac function and plays a central role in the diagnosis and management of cardiovascular disease. Echocardiography, as a readily accessible and non-invasive imaging modality, is widely used in clinical practice to estimate LVEF. However, manual assessment of cardiac function from echocardiograms is time-consuming and subject to considerable inter-observer variability. Deep learning approaches offer a promising alternative, with the potential to achieve performance comparable to that of experienced human experts. In this study, we investigate the effectiveness of several deep learning architectures for LVEF estimation from echocardiography videos, including 3D Inception, two-stream, and CNN-RNN models. We systematically evaluate architectural modifications and fusion strategies to identify configurations that maximize prediction accuracy. Models were trained and evaluated on the EchoNet-Dynamic dataset, comprising 10,030 echocardiogram videos. Our results demonstrate that modified 3D Inception architectures achieve the best overall performance, with a root mean squared error (RMSE) of 6.79%. Across architectures, we observe a tendency toward overfitting, with smaller and simpler models generally exhibiting improved generalization. Model performance was also found to be highly sensitive to hyperparameter choices, particularly convolutional kernel sizes and normalization strategies. While this study focuses on echocardiography-based LVEF estimation, the insights gained regarding architectural design and training strategies may be applicable to a broader range of medical and non-medical video analysis tasks.

</details>


### [188] [Unleashing Foundation Vision Models: Adaptive Transfer for Diverse Data-Limited Scientific Domains](https://arxiv.org/abs/2512.22664)
*Qiankun Li,Feng He,Huabao Chen,Xin Ning,Kun Wang,Zengfu Wang*

Main category: cs.CV

TL;DR: The paper introduces CLAdapter, a novel mechanism designed to better adapt models pre-trained on large datasets to data-limited downstream scientific tasks by using cluster attention and a unified interface.


<details>
  <summary>Details</summary>
Motivation: To address the difficulty of adapting large-scale pre-trained models to specialized scientific tasks with limited data effectively.

Method: The paper proposes the Cluster Attention Adapter (CLAdapter), which enhances feature transformation using attention mechanisms and cluster centers for personalized feature adaptation. This approach supports both CNNs and Transformers in 2D and 3D tasks.

Result: CLAdapter achieves state-of-the-art results across 10 datasets from a variety of scientific domains, proving its effectiveness in data-limited downstream tasks.

Conclusion: CLAdapter successfully unleashes the potential of pre-trained models, demonstrating effective adaptive transfer in specialized and data-limited scientific domains.

Abstract: In the big data era, the computer vision field benefits from large-scale datasets such as LAION-2B, LAION-400M, and ImageNet-21K, Kinetics, on which popular models like the ViT and ConvNeXt series have been pre-trained, acquiring substantial knowledge. However, numerous downstream tasks in specialized and data-limited scientific domains continue to pose significant challenges. In this paper, we propose a novel Cluster Attention Adapter (CLAdapter), which refines and adapts the rich representations learned from large-scale data to various data-limited downstream tasks. Specifically, CLAdapter introduces attention mechanisms and cluster centers to personalize the enhancement of transformed features through distribution correlation and transformation matrices. This enables models fine-tuned with CLAdapter to learn distinct representations tailored to different feature sets, facilitating the models' adaptation from rich pre-trained features to various downstream scenarios effectively. In addition, CLAdapter's unified interface design allows for seamless integration with multiple model architectures, including CNNs and Transformers, in both 2D and 3D contexts. Through extensive experiments on 10 datasets spanning domains such as generic, multimedia, biological, medical, industrial, agricultural, environmental, geographical, materials science, out-of-distribution (OOD), and 3D analysis, CLAdapter achieves state-of-the-art performance across diverse data-limited scientific domains, demonstrating its effectiveness in unleashing the potential of foundation vision models via adaptive transfer. Code is available at https://github.com/qklee-lz/CLAdapter.

</details>


### [189] [INTERACT-CMIL: Multi-Task Shared Learning and Inter-Task Consistency for Conjunctival Melanocytic Intraepithelial Lesion Grading](https://arxiv.org/abs/2512.22666)
*Mert Ikinci,Luna Toma,Karin U. Loeffler,Leticia Ussem,Daniela Süsskind,Julia M. Weller,Yousef Yeganeh,Martina C. Herwig-Carl,Shadi Albarqouni*

Main category: cs.CV

TL;DR: The paper introduces INTERACT-CMIL, a deep learning framework predicting histopathological axes for CMIL grading, achieving significant improvements over existing models.


<details>
  <summary>Details</summary>
Motivation: Accurate CMIL grading is crucial for effective treatment and prediction of melanoma, yet remains challenging due to subtle and interdependent criteria.

Method: INTERACT-CMIL utilizes Shared Feature Learning, Combinatorial Partial Supervision, and Inter-Dependence Loss to predict five histopathological axes based on multi-center dataset analysis.

Result: INTERACT-CMIL demonstrated consistent superiority over CNN and foundation models, with substantial gains in macro F1 scores, enhancing grading accuracy and interpretability.

Conclusion: The framework establishes a reproducible benchmark, enabling standardized digital ocular pathology for CMIL diagnosis aligned with expert evaluations.

Abstract: Accurate grading of Conjunctival Melanocytic Intraepithelial Lesions (CMIL) is essential for treatment and melanoma prediction but remains difficult due to subtle morphological cues and interrelated diagnostic criteria. We introduce INTERACT-CMIL, a multi-head deep learning framework that jointly predicts five histopathological axes; WHO4, WHO5, horizontal spread, vertical spread, and cytologic atypia, through Shared Feature Learning with Combinatorial Partial Supervision and an Inter-Dependence Loss enforcing cross-task consistency. Trained and evaluated on a newly curated, multi-center dataset of 486 expert-annotated conjunctival biopsy patches from three university hospitals, INTERACT-CMIL achieves consistent improvements over CNN and foundation-model (FM) baselines, with relative macro F1 gains up to 55.1% (WHO4) and 25.0% (vertical spread). The framework provides coherent, interpretable multi-criteria predictions aligned with expert grading, offering a reproducible computational benchmark for CMIL diagnosis and a step toward standardized digital ocular pathology.

</details>


### [190] [CritiFusion: Semantic Critique and Spectral Alignment for Faithful Text-to-Image Generation](https://arxiv.org/abs/2512.22681)
*ZhenQi Chen,TsaiChing Ni,YuanFu Yang*

Main category: cs.CV

TL;DR: CritiFusion integrates semantic critique and frequency-domain refinement into text-to-image diffusion models, improving alignment with prompts without extra training.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of achieving semantic alignment and high visual fidelity in text-to-image diffusion models, which often struggle with complex prompts.

Method: CritiFusion introduces a CritiCore module for semantic feedback using vision-language and large language models, and a SpecFusion module for spectral-domain refinement, without additional model training.

Result: CritiFusion improves text-to-image alignment and visual quality metrics, consistently achieving human-aligned evaluation scores comparable to state-of-the-art methods.

Conclusion: The framework is effective as a plug-in refinement stage for existing models, boosting detail, realism, and prompt fidelity of generated images.

Abstract: Recent text-to-image diffusion models have achieved remarkable visual fidelity but often struggle with semantic alignment to complex prompts. We introduce CritiFusion, a novel inference-time framework that integrates a multimodal semantic critique mechanism with frequency-domain refinement to improve text-to-image consistency and detail. The proposed CritiCore module leverages a vision-language model and multiple large language models to enrich the prompt context and produce high-level semantic feedback, guiding the diffusion process to better align generated content with the prompt's intent. Additionally, SpecFusion merges intermediate generation states in the spectral domain, injecting coarse structural information while preserving high-frequency details. No additional model training is required. CritiFusion serves as a plug-in refinement stage compatible with existing diffusion backbones. Experiments on standard benchmarks show that our method notably improves human-aligned metrics of text-to-image correspondence and visual quality. CritiFusion consistently boosts performance on human preference scores and aesthetic evaluations, achieving results on par with state-of-the-art reward optimization approaches. Qualitative results further demonstrate superior detail, realism, and prompt fidelity, indicating the effectiveness of our semantic critique and spectral alignment strategy.

</details>


### [191] [Autoregressive Flow Matching for Motion Prediction](https://arxiv.org/abs/2512.22688)
*Johnathan Xie,Stefan Stojanov,Cristobal Eyzaguirre,Daniel L. K. Yamins,Jiajun Wu*

Main category: cs.CV

TL;DR: The paper introduces ARFM, a model for predicting sequential motion applied to human and robot tasks, showing improved results in downstream performance.


<details>
  <summary>Details</summary>
Motivation: The study aims to overcome challenges in current motion prediction models, which either have narrow applications or lack accuracy in modeling complex motions at scale.

Method: The authors propose ARFM, an autoregressive flow matching method, trained on diverse datasets for probabilistic modeling of motion sequences over long-term horizons.

Result: ARFM achieves accurate predictions of complex motions and demonstrates improved performance in tasks like robot action and human motion prediction.

Conclusion: ARFM effectively enhances downstream task performance in motion prediction while addressing limitations in existing methods. Code and models are openly shared.

Abstract: Motion prediction has been studied in different contexts with models trained on narrow distributions and applied to downstream tasks in human motion prediction and robotics. Simultaneously, recent efforts in scaling video prediction have demonstrated impressive visual realism, yet they struggle to accurately model complex motions despite massive scale. Inspired by the scaling of video generation, we develop autoregressive flow matching (ARFM), a new method for probabilistic modeling of sequential continuous data and train it on diverse video datasets to generate future point track locations over long horizons. To evaluate our model, we develop benchmarks for evaluating the ability of motion prediction models to predict human and robot motion. Our model is able to predict complex motions, and we demonstrate that conditioning robot action prediction and human motion prediction on predicted future tracks can significantly improve downstream task performance. Code and models publicly available at: https://github.com/Johnathan-Xie/arfm-motion-prediction.

</details>


### [192] [Multimodal Diffeomorphic Registration with Neural ODEs and Structural Descriptors](https://arxiv.org/abs/2512.22689)
*Salvador Rodriguez-Sanz,Monica Hernandez*

Main category: cs.CV

TL;DR: The paper introduces a multimodal diffeomorphic registration method using Neural ODEs, overcoming the limitations of intensity correlation assumptions and extensive training data. It performs well in scenarios with large deformations and varying scales.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of nonrigid registration algorithms that struggle with monomodal settings, are computationally complex, and suffer with limited applicability to unseen modalities when using learning-based models.

Method: Proposes an instance-specific registration framework utilizing Neural ODEs, structural and nonstructural similarity metrics, and three variants combining structural descriptors and local mutual information.

Result: The proposed method outperforms state-of-the-art baselines in accuracy, robustness, and efficiency for multimodal registration, especially in scenarios involving large or small deformations.

Conclusion: The framework demonstrates that instance-specific registration with Neural ODEs is effective, robust to regularization, and suitable for varying scales without requiring extensive training data.

Abstract: This work proposes a multimodal diffeomorphic registration method using Neural Ordinary Differential Equations (Neural ODEs). Nonrigid registration algorithms exhibit tradeoffs between their accuracy, the computational complexity of their deformation model, and its proper regularization. In addition, they also assume intensity correlation in anatomically homologous regions of interest among image pairs, limiting their applicability to the monomodal setting. Unlike learning-based models, we propose an instance-specific framework that is not subject to high scan requirements for training and does not suffer performance degradation at inference time on modalities unseen during training. Our method exploits the potential of continuous-depth networks in the Neural ODE paradigm with structural descriptors, widely adopted as modality-agnostic metric models which exploit self-similarities on parameterized neighborhood geometries. We propose three different variants that integrate image-based or feature-based structural descriptors and nonstructural image similarities computed by local mutual information. We conduct extensive evaluations on different experiments formed by scan dataset combinations and show surpassing qualitative and quantitative results compared to state-of-the-art baselines adequate for large or small deformations, and specific of multimodal registration. Lastly, we also demonstrate the underlying robustness of the proposed framework to varying levels of explicit regularization while maintaining low error, its suitability for registration at varying scales, and its efficiency with respect to other methods targeted to large-deformation registration.

</details>


### [193] [SCPainter: A Unified Framework for Realistic 3D Asset Insertion and Novel View Synthesis](https://arxiv.org/abs/2512.22706)
*Paul Dobre,Jackson Cooper,Xin Wang,Hongzhou Yang*

Main category: cs.CV

TL;DR: The paper introduces SCPainter, a unified framework combining realistic 3D asset insertion and novel view synthesis (NVS) for autonomous driving simulation, using 3D Gaussian Splat representations, point clouds, and diffusion models to create high-quality data.


<details>
  <summary>Details</summary>
Motivation: To improve the robustness and safety of autonomous driving models by enhancing training data diversity, including rare and challenging driving scenarios, through more realistic integration of 3D assets and NVS.

Method: SCPainter integrates 3D Gaussian Splat car representations, 3D scene point clouds, and a diffusion model to generate high-quality images that combine asset insertion and NVS. It conditions diffusion-based synthesis on the joint projections of 3D assets and scenes.

Result: SCPainter demonstrated its capability on the Waymo Open Dataset by enabling realistic 3D asset insertion and high-quality NVS, creating diverse and realistic training datasets for autonomous driving.

Conclusion: The integration of innovative representations and generative models provides a unified solution that advances both asset insertion and NVS, boosting the diversity and realism needed for autonomous driving training data.

Abstract: 3D Asset insertion and novel view synthesis (NVS) are key components for autonomous driving simulation, enhancing the diversity of training data. With better training data that is diverse and covers a wide range of situations, including long-tailed driving scenarios, autonomous driving models can become more robust and safer. This motivates a unified simulation framework that can jointly handle realistic integration of inserted 3D assets and NVS. Recent 3D asset reconstruction methods enable reconstruction of dynamic actors from video, supporting their re-insertion into simulated driving scenes. While the overall structure and appearance can be accurate, it still struggles to capture the realism of 3D assets through lighting or shadows, particularly when inserted into scenes. In parallel, recent advances in NVS methods have demonstrated promising results in synthesizing viewpoints beyond the originally recorded trajectories. However, existing approaches largely treat asset insertion and NVS capabilities in isolation. To allow for interaction with the rest of the scene and to enable more diverse creation of new scenarios for training, realistic 3D asset insertion should be combined with NVS. To address this, we present SCPainter (Street Car Painter), a unified framework which integrates 3D Gaussian Splat (GS) car asset representations and 3D scene point clouds with diffusion-based generation to jointly enable realistic 3D asset insertion and NVS. The 3D GS assets and 3D scene point clouds are projected together into novel views, and these projections are used to condition a diffusion model to generate high quality images. Evaluation on the Waymo Open Dataset demonstrate the capability of our framework to enable 3D asset insertion and NVS, facilitating the creation of diverse and realistic driving data.

</details>


### [194] [Improved cystic hygroma detection from prenatal imaging using ultrasound-specific self-supervised representation learning](https://arxiv.org/abs/2512.22730)
*Youssef Megahed,Robin Ducharme,Inok Lee,Inbal Willner,Olivier X. Miguel,Kevin Dick,Adrian D. C. Chan,Mark Walker,Steven Hawken*

Main category: cs.CV

TL;DR: The study showcases a novel deep learning model relying on self-supervised ultrasound pretraining for identifying cystic hygroma, surpassing existing approaches in accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: To develop a robust and scalable deep learning-based detection tool for cystic hygroma that overcomes the limitation of small labeled datasets through self-supervised pretraining.

Method: The study employed the USF-MAE model, pretrained on 370,000 unlabeled ultrasound images, fine-tuned for binary classification of cystic hygroma cases and healthy controls, and compared it with the DenseNet-169 baseline using cross-validation and interpretability analysis.

Result: USF-MAE demonstrated higher performance metrics (accuracy: 0.96, sensitivity: 0.94, specificity: 0.98, ROC-AUC: 0.98) over the DenseNet-169 baseline. Statistical validation confirmed significant improvements.

Conclusion: USF-MAE is a promising tool for automated cystic hygroma detection, providing accurate and clinically interpretable results, with potential to enhance early prenatal screening programs.

Abstract: Cystic hygroma is a high-risk prenatal ultrasound finding that portends high rates of chromosomal abnormalities, structural malformations, and adverse pregnancy outcomes. Automated detection can increase reproducibility and support scalable early screening programs, but supervised deep learning methods are limited by small labelled datasets. This study assesses whether ultrasound-specific self-supervised pretraining can facilitate accurate, robust deep learning detection of cystic hygroma in first-trimester ultrasound images. We fine-tuned the Ultrasound Self-Supervised Foundation Model with Masked Autoencoding (USF-MAE), pretrained on over 370,000 unlabelled ultrasound images, for binary classification of normal controls and cystic hygroma cases used in this study. Performance was evaluated on the same curated ultrasound dataset, preprocessing pipeline, and 4-fold cross-validation protocol as for the DenseNet-169 baseline, using accuracy, sensitivity, specificity, and the area under the receiver operating characteristic curve (ROC-AUC). Model interpretability was analyzed qualitatively using Score-CAM visualizations. USF-MAE outperformed the DenseNet-169 baseline on all evaluation metrics. The proposed model yielded a mean accuracy of 0.96, sensitivity of 0.94, specificity of 0.98, and ROC-AUC of 0.98 compared to 0.93, 0.92, 0.94, and 0.94 for the DenseNet-169 baseline, respectively. Qualitative Score-CAM visualizations of model predictions demonstrated clinical relevance by highlighting expected regions in the fetal neck for both positive and negative cases. Paired statistical analysis using a Wilcoxon signed-rank test confirmed that performance improvements achieved by USF-MAE were statistically significant (p = 0.0057).

</details>


### [195] [Split4D: Decomposed 4D Scene Reconstruction Without Video Segmentation](https://arxiv.org/abs/2512.22745)
*Yongzhen Hu,Yihui Yang,Haotong Lin,Yifan Wang,Junting Dong,Yifu Deng,Xinyu Zhu,Fan Jia,Hujun Bao,Xiaowei Zhou,Sida Peng*

Main category: cs.CV

TL;DR: This paper proposes a novel method, Freetime FeatureGS, for decomposed 4D scene reconstruction from multi-view videos, bypassing unstable video segmentation maps for improved results.


<details>
  <summary>Details</summary>
Motivation: Overcoming challenges in 4D scene reconstruction caused by reliance on unstable video segmentation maps.

Method: Represents 4D scenes with Gaussian primitives (Freetime FeatureGS) that learn features and motion dynamics, combined with contrastive loss and temporal streaming to improve reconstruction.

Result: The proposed method outperforms recent methods in reconstruction quality, as shown through experimental results.

Conclusion: By leveraging Gaussian primitives and a streaming feature learning strategy, the method provides a more robust approach to 4D scene reconstruction without requiring stable video segmentation maps.

Abstract: This paper addresses the problem of decomposed 4D scene reconstruction from multi-view videos. Recent methods achieve this by lifting video segmentation results to a 4D representation through differentiable rendering techniques. Therefore, they heavily rely on the quality of video segmentation maps, which are often unstable, leading to unreliable reconstruction results. To overcome this challenge, our key idea is to represent the decomposed 4D scene with the Freetime FeatureGS and design a streaming feature learning strategy to accurately recover it from per-image segmentation maps, eliminating the need for video segmentation. Freetime FeatureGS models the dynamic scene as a set of Gaussian primitives with learnable features and linear motion ability, allowing them to move to neighboring regions over time. We apply a contrastive loss to Freetime FeatureGS, forcing primitive features to be close or far apart based on whether their projections belong to the same instance in the 2D segmentation map. As our Gaussian primitives can move across time, it naturally extends the feature learning to the temporal dimension, achieving 4D segmentation. Furthermore, we sample observations for training in a temporally ordered manner, enabling the streaming propagation of features over time and effectively avoiding local minima during the optimization process. Experimental results on several datasets show that the reconstruction quality of our method outperforms recent methods by a large margin.

</details>


### [196] [TrimTokenator-LC: Towards Adaptive Visual Token Pruning for Large Multimodal Models with Long Contexts](https://arxiv.org/abs/2512.22748)
*Hao Zhang,Mengsi Lyu,Bo Huang,Yulong Ao,Yonghua Lin*

Main category: cs.CV

TL;DR: This paper focuses on improving inference efficiency for tasks involving multi-image and long context settings in Large Multimodal Models (LMMs) through an adaptive visual token pruning method.


<details>
  <summary>Details</summary>
Motivation: Increasing inference efficiency in LMMs by addressing the issues caused by redundancy in visual inputs, specifically in cases of long context and multiple images.

Method: An adaptive token pruning approach using intra-image and inter-image redundancy analysis, dynamic token budget allocation, and a Pareto selection procedure to balance diversity and text alignment.

Result: The proposed method maintains performance while significantly reducing the number of visual tokens used in inference tasks.

Conclusion: Adaptive pruning techniques can optimize multi-image settings with long contexts, reducing computational costs without compromising task accuracy.

Abstract: Large Multimodal Models (LMMs) have proven effective on various tasks. They typically encode visual inputs into Original Model sequences of tokens, which are then concatenated with textual tokens and jointly processed by the language model. However, the growing number of visual tokens greatly increases inference cost. Visual token pruning has emerged as a promising solution. However, existing methods often overlook scenarios involving long context inputs with multiple images. In this paper, we analyze the challenges of visual token pruning in long context, multi-image settings and introduce an adaptive pruning method tailored for such scenarios. We decompose redundancy into intra-image and inter-image components and quantify them through intra-image diversity and inter-image variation, which jointly guide dynamic budget allocation. Our approach consists of two stages. The intra-image stage allocates each image a content-aware token budget and greedily selects its most representative tokens. The inter-image stage performs global diversity filtering to form a candidate pool and then applies a Pareto selection procedure that balances diversity with text alignment. Extensive experiments show that our approach maintains strong performance in long context settings while significantly cutting down the number of visual tokens.

</details>


### [197] [Neighbor-Aware Token Reduction via Hilbert Curve for Vision Transformers](https://arxiv.org/abs/2512.22760)
*Yunge Li,Lanyu Xu*

Main category: cs.CV

TL;DR: The paper introduces neighbor-aware token reduction methods using Hilbert curve reordering to improve efficiency in Vision Transformers (ViTs).


<details>
  <summary>Details</summary>
Motivation: ViTs face limitations in computational efficiency due to redundant token representations, and existing reduction strategies fail to maintain spatial and neighbor relationships, leading to a loss of local context.

Method: The paper proposes two methods: Neighbor-Aware Pruning (NAP) for selective token retention and Merging by Adjacent Token similarity (MAT) for local token aggregation, leveraging Hilbert curve reordering for preserving neighbor structure.

Result: The proposed methods achieve state-of-the-art accuracy-efficiency trade-offs, outperforming existing token merging and pruning strategies.

Conclusion: Spatial continuity and neighbor structure are crucial for optimizing ViTs, and the novel methods enhance their performance and efficiency, offering valuable insights for architectural improvements.

Abstract: Vision Transformers (ViTs) have achieved remarkable success in visual recognition tasks, but redundant token representations limit their computational efficiency. Existing token merging and pruning strategies often overlook spatial continuity and neighbor relationships, resulting in the loss of local context. This paper proposes novel neighbor-aware token reduction methods based on Hilbert curve reordering, which explicitly preserves the neighbor structure in a 2D space using 1D sequential representations. Our method introduces two key strategies: Neighbor-Aware Pruning (NAP) for selective token retention and Merging by Adjacent Token similarity (MAT) for local token aggregation. Experiments demonstrate that our approach achieves state-of-the-art accuracy-efficiency trade-offs compared to existing methods. This work highlights the importance of spatial continuity and neighbor structure, offering new insights for the architectural optimization of ViTs.

</details>


### [198] [Next Best View Selections for Semantic and Dynamic 3D Gaussian Splatting](https://arxiv.org/abs/2512.22771)
*Yiqian Li,Wen Jiang,Kostas Daniilidis*

Main category: cs.CV

TL;DR: The paper proposes an active learning algorithm using Fisher Information to prioritize informative frames for embodied agent tasks, improving rendering quality and semantic segmentation.


<details>
  <summary>Details</summary>
Motivation: Embodied agents need efficient mechanisms to handle the data redundancy in semantic reasoning and dynamic scene modeling tasks.

Method: They introduce an active learning algorithm based on Fisher Information, quantifying candidate views' informativeness with Gaussian parameters and deformation networks.

Result: The algorithm enhances rendering quality and semantic segmentation performance across large-scale datasets, surpassing random and heuristic baseline approaches.

Conclusion: This method offers a structured and effective alternative for frame selection, addressing both semantic and dynamic scene challenges for embodied agents.

Abstract: Understanding semantics and dynamics has been crucial for embodied agents in various tasks. Both tasks have much more data redundancy than the static scene understanding task. We formulate the view selection problem as an active learning problem, where the goal is to prioritize frames that provide the greatest information gain for model training. To this end, we propose an active learning algorithm with Fisher Information that quantifies the informativeness of candidate views with respect to both semantic Gaussian parameters and deformation networks. This formulation allows our method to jointly handle semantic reasoning and dynamic scene modeling, providing a principled alternative to heuristic or random strategies. We evaluate our method on large-scale static images and dynamic video datasets by selecting informative frames from multi-camera setups. Experimental results demonstrate that our approach consistently improves rendering quality and semantic segmentation performance, outperforming baseline methods based on random selection and uncertainty-based heuristics.

</details>


### [199] [Plug In, Grade Right: Psychology-Inspired AGIQA](https://arxiv.org/abs/2512.22780)
*Zhicheng Liao,Baoliang Chen,Hanwei Zhu,Lingyu Zhu,Shiqi Wang,Weisi Lin*

Main category: cs.CV

TL;DR: The paper addresses the problem of inconsistent text-image embedding similarities in AGIQA models, proposing a Graded Response Model (GRM) to alleviate semantic drift and improve quality estimation.


<details>
  <summary>Details</summary>
Motivation: Existing AGIQA models suffer from multimodal similarity distributions between image embeddings and text-based grade descriptions, termed as "semantic drift," which reduces the reliability of quality assessment.

Method: The authors propose an Arithmetic Graded Response Model (GRM), utilizing a two-branch module: one branch estimates image quality while the other creates difficulty levels for grades, ensuring monotonicity in quality distribution by modeling difficulty in an arithmetic manner.

Result: The proposed AGQG module consistently improves the performance of multiple AGIQA frameworks and generalizes effectively across different types of image quality assessment.

Conclusion: This work offers a robust and interpretable solution for image quality assessment, demonstrating the potential of the AGQG module as a key feature in future advancements in IQA models.

Abstract: Existing AGIQA models typically estimate image quality by measuring and aggregating the similarities between image embeddings and text embeddings derived from multi-grade quality descriptions. Although effective, we observe that such similarity distributions across grades usually exhibit multimodal patterns. For instance, an image embedding may show high similarity to both "excellent" and "poor" grade descriptions while deviating from the "good" one. We refer to this phenomenon as "semantic drift", where semantic inconsistencies between text embeddings and their intended descriptions undermine the reliability of text-image shared-space learning. To mitigate this issue, we draw inspiration from psychometrics and propose an improved Graded Response Model (GRM) for AGIQA. The GRM is a classical assessment model that categorizes a subject's ability across grades using test items with various difficulty levels. This paradigm aligns remarkably well with human quality rating, where image quality can be interpreted as an image's ability to meet various quality grades. Building on this philosophy, we design a two-branch quality grading module: one branch estimates image ability while the other constructs multiple difficulty levels. To ensure monotonicity in difficulty levels, we further model difficulty generation in an arithmetic manner, which inherently enforces a unimodal and interpretable quality distribution. Our Arithmetic GRM based Quality Grading (AGQG) module enjoys a plug-and-play advantage, consistently improving performance when integrated into various state-of-the-art AGIQA frameworks. Moreover, it also generalizes effectively to both natural and screen content image quality assessment, revealing its potential as a key component in future IQA models.

</details>


### [200] [Parallel Diffusion Solver via Residual Dirichlet Policy Optimization](https://arxiv.org/abs/2512.22796)
*Ruoyu Wang,Ziyu Li,Beier Zhu,Liangyu Yuan,Hanwang Zhang,Xun Yang,Xiaojun Chang,Chi Zhang*

Main category: cs.CV

TL;DR: The paper introduces EPD-Solver, an ODE solver that accelerates diffusion model sampling while preserving image quality.


<details>
  <summary>Details</summary>
Motivation: Diffusion models suffer from high sampling latency and quality degradation in low-latency settings due to truncation errors in solvers.

Method: Presents a novel EPD-Solver based on gradient evaluation parallelization, leveraging geometric insights and optimization techniques including RL fine-tuning.

Result: EPD-Solver mitigates truncation errors efficiently, supports parallelism, and enhances performance in text-to-image generation tasks.

Conclusion: EPD-Solver improves the trade-off between latency and image quality, offers flexibility, and can plug into existing ODE samplers.

Abstract: Diffusion models (DMs) have achieved state-of-the-art generative performance but suffer from high sampling latency due to their sequential denoising nature. Existing solver-based acceleration methods often face significant image quality degradation under a low-latency budget, primarily due to accumulated truncation errors arising from the inability to capture high-curvature trajectory segments. In this paper, we propose the Ensemble Parallel Direction solver (dubbed as EPD-Solver), a novel ODE solver that mitigates these errors by incorporating multiple parallel gradient evaluations in each step. Motivated by the geometric insight that sampling trajectories are largely confined to a low-dimensional manifold, EPD-Solver leverages the Mean Value Theorem for vector-valued functions to approximate the integral solution more accurately. Importantly, since the additional gradient computations are independent, they can be fully parallelized, preserving low-latency sampling nature. We introduce a two-stage optimization framework. Initially, EPD-Solver optimizes a small set of learnable parameters via a distillation-based approach. We further propose a parameter-efficient Reinforcement Learning (RL) fine-tuning scheme that reformulates the solver as a stochastic Dirichlet policy. Unlike traditional methods that fine-tune the massive backbone, our RL approach operates strictly within the low-dimensional solver space, effectively mitigating reward hacking while enhancing performance in complex text-to-image (T2I) generation tasks. In addition, our method is flexible and can serve as a plugin (EPD-Plugin) to improve existing ODE samplers.

</details>


### [201] [VPTracker: Global Vision-Language Tracking via Visual Prompt and MLLM](https://arxiv.org/abs/2512.22799)
*Jingchao Wang,Kaiwen Zhou,Zhijian Wu,Kunhua Ji,Dingjiang Huang,Yefeng Zheng*

Main category: cs.CV

TL;DR: This paper presents VPTracker, a global vision-language tracking approach utilizing Multimodal Large Language Models (MLLMs), which improves robustness and addresses visual distractions.


<details>
  <summary>Details</summary>
Motivation: Traditional vision-language tracking methods fail under challenging conditions like viewpoint changes and occlusions due to their reliance on local search.

Method: The authors leverage MLLMs for global search and propose a location-aware visual prompting mechanism that combines spatial priors with region-level recognition.

Result: Extensive experiments demonstrate that the VPTracker significantly improves tracking stability and reduces interference in challenging scenarios.

Conclusion: VPTracker effectively integrates MLLMs into vision-language tracking, enhancing robustness and target disambiguation in complex tasks.

Abstract: Vision-Language Tracking aims to continuously localize objects described by a visual template and a language description. Existing methods, however, are typically limited to local search, making them prone to failures under viewpoint changes, occlusions, and rapid target movements. In this work, we introduce the first global tracking framework based on Multimodal Large Language Models (VPTracker), exploiting their powerful semantic reasoning to locate targets across the entire image space. While global search improves robustness and reduces drift, it also introduces distractions from visually or semantically similar objects. To address this, we propose a location-aware visual prompting mechanism that incorporates spatial priors into the MLLM. Specifically, we construct a region-level prompt based on the target's previous location, enabling the model to prioritize region-level recognition and resort to global inference only when necessary. This design retains the advantages of global tracking while effectively suppressing interference from distracting visual content. Extensive experiments show that our approach significantly enhances tracking stability and target disambiguation under challenging scenarios, opening a new avenue for integrating MLLMs into visual tracking. Code is available at https://github.com/jcwang0602/VPTracker.

</details>


### [202] [Medical Scene Reconstruction and Segmentation based on 3D Gaussian Representation](https://arxiv.org/abs/2512.22800)
*Bin Liu,Wenyan Tian,Huangxin Fu,Zizheng Li,Zhifen He,Bo Li*

Main category: cs.CV

TL;DR: This paper proposes an efficient 3D reconstruction method using 3D Gaussian and tri-plane representations, resulting in improved accuracy and efficiency in medical imaging, especially under sparse data conditions.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of traditional 3D reconstruction methods, which are computationally expensive and struggle with discontinuities and detail loss in sparse slices, making them inadequate for clinical accuracy.

Method: The paper introduces a 3D reconstruction approach leveraging 3D Gaussian and tri-plane representations to optimize rendering efficiency, structural continuity, and semantic consistency under sparse slicing conditions.

Result: Experimental findings on multimodal datasets like US and MRI demonstrate the method generates anatomically accurate and semantically coherent images while significantly enhancing reconstruction efficiency.

Conclusion: The proposed method offers a reliable and efficient solution for 3D visualization and clinical analysis of medical images, addressing challenges in traditional methods under sparse data conditions.

Abstract: 3D reconstruction of medical images is a key technology in medical image analysis and clinical diagnosis, providing structural visualization support for disease assessment and surgical planning. Traditional methods are computationally expensive and prone to structural discontinuities and loss of detail in sparse slices, making it difficult to meet clinical accuracy requirements.To address these challenges, we propose an efficient 3D reconstruction method based on 3D Gaussian and tri-plane representations. This method not only maintains the advantages of Gaussian representation in efficient rendering and geometric representation but also significantly enhances structural continuity and semantic consistency under sparse slicing conditions. Experimental results on multimodal medical datasets such as US and MRI show that our proposed method can generate high-quality, anatomically coherent, and semantically stable medical images under sparse data conditions, while significantly improving reconstruction efficiency. This provides an efficient and reliable new approach for 3D visualization and clinical analysis of medical images.

</details>


### [203] [Evaluating the Performance of Open-Vocabulary Object Detection in Low-quality Image](https://arxiv.org/abs/2512.22801)
*Po-Chih Wu*

Main category: cs.CV

TL;DR: This paper evaluates open-vocabulary object detection models under low-quality image conditions, introduces a new dataset, and finds severe performance degradation under high-level image degradation.


<details>
  <summary>Details</summary>
Motivation: To assess the robustness of open-vocabulary object detection models when tasked with detecting objects in low-quality images, which are common in real-world scenarios.

Method: A new dataset simulating low-quality images was introduced, and models were tested for performance under varying levels of image quality degradation.

Result: Open-vocabulary object detection models showed stable performance with low-level image degradation but significant drops under high-level degradation. OWLv2 consistently outperformed other models like OWL-ViT, GroundingDINO, and Detic.

Conclusion: Though some models like OWLv2 display better resilience to image degradation, high-level degradation severely impacts the performance of all tested models, highlighting the need for robust solutions.

Abstract: Open-vocabulary object detection enables models to localize and recognize objects beyond a predefined set of categories and is expected to achieve recognition capabilities comparable to human performance. In this study, we aim to evaluate the performance of existing models on open-vocabulary object detection tasks under low-quality image conditions. For this purpose, we introduce a new dataset that simulates low-quality images in the real world. In our evaluation experiment, we find that although open-vocabulary object detection models exhibited no significant decrease in mAP scores under low-level image degradation, the performance of all models dropped sharply under high-level image degradation. OWLv2 models consistently performed better across different types of degradation, while OWL-ViT, GroundingDINO, and Detic showed significant performance declines. We will release our dataset and codes to facilitate future studies.

</details>


### [204] [EgoReAct: Egocentric Video-Driven 3D Human Reaction Generation](https://arxiv.org/abs/2512.22808)
*Libo Zhang,Zekun Li,Tianyu Li,Zeyu Cao,Rui Xu,Xiaoxiao Long,Wenjia Wang,Jingbo Wang,Yuan Liu,Wenping Wang,Daquan Zhou,Taku Komura,Zhiyang Dou*

Main category: cs.CV

TL;DR: The paper introduces a dataset called Human Reaction Dataset (HRD) and a framework named EgoReAct for generating 3D-aligned human reactions to egocentric videos in real-time, ensuring spatial consistency and causality.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges in modeling human reactions from egocentric video due to data scarcity and spatial misalignment, as well as to enable precise, real-time 3D-aligned reaction generation.

Method: The authors create the HRD dataset to ensure spatial alignment and use it to train EgoReAct, an autoregressive model combining a Vector Quantised-Variational AutoEncoder and a Generative Pre-trained Transformer that incorporates 3D dynamic elements for reaction generation.

Result: EgoReAct demonstrates significantly higher realism, spatial consistency, and generation efficiency compared to existing methods, while maintaining strict causality.

Conclusion: The proposed EgoReAct and HRD dataset successfully address the challenges of egocentric visual input modeling, setting a new standard in realism, spatial alignment, and causality for reaction generation. Code, models, and data will be shared with the community.

Abstract: Humans exhibit adaptive, context-sensitive responses to egocentric visual input. However, faithfully modeling such reactions from egocentric video remains challenging due to the dual requirements of strictly causal generation and precise 3D spatial alignment. To tackle this problem, we first construct the Human Reaction Dataset (HRD) to address data scarcity and misalignment by building a spatially aligned egocentric video-reaction dataset, as existing datasets (e.g., ViMo) suffer from significant spatial inconsistency between the egocentric video and reaction motion, e.g., dynamically moving motions are always paired with fixed-camera videos. Leveraging HRD, we present EgoReAct, the first autoregressive framework that generates 3D-aligned human reaction motions from egocentric video streams in real-time. We first compress the reaction motion into a compact yet expressive latent space via a Vector Quantised-Variational AutoEncoder and then train a Generative Pre-trained Transformer for reaction generation from the visual input. EgoReAct incorporates 3D dynamic features, i.e., metric depth, and head dynamics during the generation, which effectively enhance spatial grounding. Extensive experiments demonstrate that EgoReAct achieves remarkably higher realism, spatial consistency, and generation efficiency compared with prior methods, while maintaining strict causality during generation. We will release code, models, and data upon acceptance.

</details>


### [205] [Depth Anything in $360^\circ$: Towards Scale Invariance in the Wild](https://arxiv.org/abs/2512.22819)
*Hualie Jiang,Ziyang Song,Zhiqiang Lou,Rui Xu,Minglang Tan*

Main category: cs.CV

TL;DR: The paper introduces DA360, an advanced model for zero-shot panoramic depth estimation, which significantly reduces depth estimation errors in indoor and outdoor environments.


<details>
  <summary>Details</summary>
Motivation: To address the limited zero-shot generalization of panoramic depth estimation in open-world settings, by leveraging knowledge transfer from perspective domain depth estimation.

Method: The authors developed DA360 by learning a shift parameter from the ViT backbone for producing accurate point clouds, and integrating circular padding into the DPT decoder for spatially coherent depth maps.

Result: DA360 achieved substantial error reductions (over 50% for indoor benchmarks, 10% for outdoor benchmarks) and outperformed existing methods like PanDA by 30% on three test datasets.

Conclusion: DA360 sets a new state-of-the-art for zero-shot panoramic depth estimation, confirming its effectiveness and potential in robotics and AR/VR applications.

Abstract: Panoramic depth estimation provides a comprehensive solution for capturing complete $360^\circ$ environmental structural information, offering significant benefits for robotics and AR/VR applications. However, while extensively studied in indoor settings, its zero-shot generalization to open-world domains lags far behind perspective images, which benefit from abundant training data. This disparity makes transferring capabilities from the perspective domain an attractive solution. To bridge this gap, we present Depth Anything in $360^\circ$ (DA360), a panoramic-adapted version of Depth Anything V2. Our key innovation involves learning a shift parameter from the ViT backbone, transforming the model's scale- and shift-invariant output into a scale-invariant estimate that directly yields well-formed 3D point clouds. This is complemented by integrating circular padding into the DPT decoder to eliminate seam artifacts, ensuring spatially coherent depth maps that respect spherical continuity. Evaluated on standard indoor benchmarks and our newly curated outdoor dataset, Metropolis, DA360 shows substantial gains over its base model, achieving over 50\% and 10\% relative depth error reduction on indoor and outdoor benchmarks, respectively. Furthermore, DA360 significantly outperforms robust panoramic depth estimation methods, achieving about 30\% relative error improvement compared to PanDA across all three test datasets and establishing new state-of-the-art performance for zero-shot panoramic depth estimation.

</details>


### [206] [KANO: Kolmogorov-Arnold Neural Operator for Image Super-Resolution](https://arxiv.org/abs/2512.22822)
*Chenyu Li,Danfeng Hong,Bing Zhang,Zhaojie Pan,Jocelyn Chanussot*

Main category: cs.CV

TL;DR: This paper introduces an interpretable approach called Kolmogorov-Arnold Neural Operator (KANO) for single-image super-resolution by leveraging structured latent degradation representation.


<details>
  <summary>Details</summary>
Motivation: Existing super-resolution methods lack interpretability and rely on black-box deep networks, making degradation processes uncontrollable and opaque.

Method: KANO employs structured representation using B-spline functions to model spectral curves and optimizes shape parameters for capturing spectral characteristics.

Result: KANO provides accurate results in characterizing degradation mechanisms and spectral behavior, validated through experiments on diverse data types (e.g., natural, aerial, satellite images).

Conclusion: Introducing KANO advances interpretable super-resolution techniques, offering insights into complex degradation processes and addressing limitations of prior methods.

Abstract: The highly nonlinear degradation process, complex physical interactions, and various sources of uncertainty render single-image Super-resolution (SR) a particularly challenging task. Existing interpretable SR approaches, whether based on prior learning or deep unfolding optimization frameworks, typically rely on black-box deep networks to model latent variables, which leaves the degradation process largely unknown and uncontrollable. Inspired by the Kolmogorov-Arnold theorem (KAT), we for the first time propose a novel interpretable operator, termed Kolmogorov-Arnold Neural Operator (KANO), with the application to image SR. KANO provides a transparent and structured representation of the latent degradation fitting process. Specifically, we employ an additive structure composed of a finite number of B-spline functions to approximate continuous spectral curves in a piecewise fashion. By learning and optimizing the shape parameters of these spline functions within defined intervals, our KANO accurately captures key spectral characteristics, such as local linear trends and the peak-valley structures at nonlinear inflection points, thereby endowing SR results with physical interpretability. Furthermore, through theoretical modeling and experimental evaluations across natural images, aerial photographs, and satellite remote sensing data, we systematically compare multilayer perceptrons (MLPs) and Kolmogorov-Arnold networks (KANs) in handling complex sequence fitting tasks. This comparative study elucidates the respective advantages and limitations of these models in characterizing intricate degradation mechanisms, offering valuable insights for the development of interpretable SR techniques.

</details>


### [207] [3D Scene Change Modeling With Consistent Multi-View Aggregation](https://arxiv.org/abs/2512.22830)
*Zirui Zhou,Junfeng Ni,Shujie Zhang,Yixin Chen,Siyuan Huang*

Main category: cs.CV

TL;DR: This paper introduces SCaR-3D, a new 3D change detection framework that addresses spatial inconsistencies and lack of separation between pre- and post-change states in existing methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve 3D change detection accuracy and resolve spatial inconsistencies and inability to differentiate pre- and post-change states in current methods.

Method: SCaR-3D uses a signed-distance-based 2D differencing module, multi-view aggregation with voting/pruning, and a continual scene reconstruction strategy for more accurate distinctions.

Result: SCaR-3D demonstrates high accuracy and efficiency, surpassing existing methods, as evidenced by experiments.

Conclusion: SCaR-3D is effective in identifying object-level changes, maintaining scene consistency, and performing selective reconstruction while introducing a new dataset CCS3D for evaluation.

Abstract: Change detection plays a vital role in scene monitoring, exploration, and continual reconstruction. Existing 3D change detection methods often exhibit spatial inconsistency in the detected changes and fail to explicitly separate pre- and post-change states. To address these limitations, we propose SCaR-3D, a novel 3D scene change detection framework that identifies object-level changes from a dense-view pre-change image sequence and sparse-view post-change images. Our approach consists of a signed-distance-based 2D differencing module followed by multi-view aggregation with voting and pruning, leveraging the consistent nature of 3DGS to robustly separate pre- and post-change states. We further develop a continual scene reconstruction strategy that selectively updates dynamic regions while preserving the unchanged areas. We also contribute CCS3D, a challenging synthetic dataset that allows flexible combinations of 3D change types to support controlled evaluations. Extensive experiments demonstrate that our method achieves both high accuracy and efficiency, outperforming existing methods.

</details>


### [208] [A Minimal Solver for Relative Pose Estimation with Unknown Focal Length from Two Affine Correspondences](https://arxiv.org/abs/2512.22833)
*Zhenbao Yu,Shirong Ye,Ronghe Jin,Shunkun Liang,Zibin Liu,Huiyun Zhang,Banglei Guan*

Main category: cs.CV

TL;DR: This paper introduces a method to estimate relative pose and focal length between two views using two affine correspondences and known vertical directions from IMU measurements.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the accuracy of determining the relative pose and focal length between two camera views, which is critical for applications in self-driving cars, smartphones, and UAVs.

Method: The authors propose a solver that derives four constraint equations from affine correspondences and known vertical directions using IMU data. These equations are solved using a polynomial eigenvalue method to estimate the focal length and relative rotation angle.

Result: The proposed solver, tested on both synthetic and real-world datasets, achieved superior performance compared to state-of-the-art methods.

Conclusion: This work demonstrates an effective method for estimating 3DOF relative pose and focal length, showing improvements over existing solvers in practical and simulated scenarios.

Abstract: In this paper, we aim to estimate the relative pose and focal length between two views with known intrinsic parameters except for an unknown focal length from two affine correspondences (ACs). Cameras are commonly used in combination with inertial measurement units (IMUs) in applications such as self-driving cars, smartphones, and unmanned aerial vehicles. The vertical direction of camera views can be obtained by IMU measurements. The relative pose between two cameras is reduced from 5DOF to 3DOF. We propose a new solver to estimate the 3DOF relative pose and focal length. First, we establish constraint equations from two affine correspondences when the vertical direction is known. Then, based on the properties of the equation system with nontrivial solutions, four equations can be derived. These four equations only involve two parameters: the focal length and the relative rotation angle. Finally, the polynomial eigenvalue method is utilized to solve the problem of focal length and relative rotation angle. The proposed solver is evaluated using synthetic and real-world datasets. The results show that our solver performs better than the existing state-of-the-art solvers.

</details>


### [209] [ByteLoom: Weaving Geometry-Consistent Human-Object Interactions through Progressive Curriculum Learning](https://arxiv.org/abs/2512.22854)
*Bangya Liu,Xinyu Gong,Zelin Zhao,Ziyang Song,Yulei Lu,Suhui Wu,Jun Zhang,Suman Banerjee,Hao Zhang*

Main category: cs.CV

TL;DR: The paper introduces ByteLoom, a Diffusion Transformer-based framework for generating realistic human-object interaction (HOI) videos, addressing limitations in object geometry consistency and reliance on fine-grained hand annotations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome challenges in cross-view consistency of objects and dependency on hand mesh annotations, which hinder existing methods for HOI video generation.

Method: The approach uses a Diffusion Transformer framework, introduces an RCM-cache mechanism with Relative Coordinate Maps (RCM) for geometric consistency, and incorporates a progressively enhancing training curriculum.

Result: The method preserves human identity, ensures multi-view geometry consistency of objects, and enables smooth motion and object manipulation.

Conclusion: ByteLoom effectively generates realistic HOI videos, addressing key limitations of current methods and reducing dependency on detailed hand modeling.

Abstract: Human-object interaction (HOI) video generation has garnered increasing attention due to its promising applications in digital humans, e-commerce, advertising, and robotics imitation learning. However, existing methods face two critical limitations: (1) a lack of effective mechanisms to inject multi-view information of the object into the model, leading to poor cross-view consistency, and (2) heavy reliance on fine-grained hand mesh annotations for modeling interaction occlusions. To address these challenges, we introduce ByteLoom, a Diffusion Transformer (DiT)-based framework that generates realistic HOI videos with geometrically consistent object illustration, using simplified human conditioning and 3D object inputs. We first propose an RCM-cache mechanism that leverages Relative Coordinate Maps (RCM) as a universal representation to maintain object's geometry consistency and precisely control 6-DoF object transformations in the meantime. To compensate HOI dataset scarcity and leverage existing datasets, we further design a training curriculum that enhances model capabilities in a progressive style and relaxes the demand of hand mesh. Extensive experiments demonstrate that our method faithfully preserves human identity and the object's multi-view geometry, while maintaining smooth motion and object manipulation.

</details>


### [210] [Learning Anatomy from Multiple Perspectives via Self-supervision in Chest Radiographs](https://arxiv.org/abs/2512.22872)
*Ziyu Zhou,Haozhe Luo,Mohammad Reza Hosseinzadeh Taher,Jiaxuan Pang,Xiaowei Ding,Michael B. Gotway,Jianming Liang*

Main category: cs.CV

TL;DR: The paper introduces Lamps, a self-supervised learning framework designed to capture anatomical features using large-scale chest radiographs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitation of existing SSL methods in medical imaging that overlook anatomical structures, which are crucial for effective learning and clinical applications.

Method: Lamps employs self-supervised learning pre-trained on chest radiographs, leveraging the consistency, coherence, and hierarchy of human anatomy as supervision signals.

Result: Lamps demonstrated superior robustness, transferability, and clinical potential across evaluations on 10 datasets compared to 10 baseline models.

Conclusion: By learning anatomy from multiple perspectives, Lamps establishes a pathway for foundation models to develop clinically meaningful and robust representations aligned with human anatomy.

Abstract: Foundation models have been successful in natural language processing and computer vision because they are capable of capturing the underlying structures (foundation) of natural languages. However, in medical imaging, the key foundation lies in human anatomy, as these images directly represent the internal structures of the body, reflecting the consistency, coherence, and hierarchy of human anatomy. Yet, existing self-supervised learning (SSL) methods often overlook these perspectives, limiting their ability to effectively learn anatomical features. To overcome the limitation, we built Lamps (learning anatomy from multiple perspectives via self-supervision) pre-trained on large-scale chest radiographs by harmoniously utilizing the consistency, coherence, and hierarchy of human anatomy as the supervision signal. Extensive experiments across 10 datasets evaluated through fine-tuning and emergent property analysis demonstrate Lamps' superior robustness, transferability, and clinical potential when compared to 10 baseline models. By learning from multiple perspectives, Lamps presents a unique opportunity for foundation models to develop meaningful, robust representations that are aligned with the structure of human anatomy.

</details>


### [211] [Let Samples Speak: Mitigating Spurious Correlation by Exploiting the Clusterness of Samples](https://arxiv.org/abs/2512.22874)
*Weiwei Li,Junzhuo Liu,Yuanyuan Ren,Yuchen Zheng,Yahao Liu,Wen Li*

Main category: cs.CV

TL;DR: This paper introduces a data-oriented pipeline to address spurious correlations in deep learning models without requiring prior annotations, achieving significant improvements on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to effectively address spurious correlations due to their reliance on annotations or assumptions and the complexity of real-world data.

Method: The pipeline consists of four steps: identifying spurious features by their dispersed distribution, neutralizing them via grouping, eliminating them through feature transformation aligned with a bias-invariant representation, and updating the classifier.

Result: The method shows over a 20% improvement in worst group accuracy on image and NLP debiasing benchmarks compared to standard empirical risk minimization (ERM).

Conclusion: This proposed approach effectively mitigates spurious correlations and enhances deep learning model robustness, demonstrating significant performance gains in diverse debiasing tasks.

Abstract: Deep learning models are known to often learn features that spuriously correlate with the class label during training but are irrelevant to the prediction task. Existing methods typically address this issue by annotating potential spurious attributes, or filtering spurious features based on some empirical assumptions (e.g., simplicity of bias). However, these methods may yield unsatisfactory performance due to the intricate and elusive nature of spurious correlations in real-world data. In this paper, we propose a data-oriented approach to mitigate the spurious correlation in deep learning models. We observe that samples that are influenced by spurious features tend to exhibit a dispersed distribution in the learned feature space. This allows us to identify the presence of spurious features. Subsequently, we obtain a bias-invariant representation by neutralizing the spurious features based on a simple grouping strategy. Then, we learn a feature transformation to eliminate the spurious features by aligning with this bias-invariant representation. Finally, we update the classifier by incorporating the learned feature transformation and obtain an unbiased model. By integrating the aforementioned identifying, neutralizing, eliminating and updating procedures, we build an effective pipeline for mitigating spurious correlation. Experiments on image and NLP debiasing benchmarks show an improvement in worst group accuracy of more than 20% compared to standard empirical risk minimization (ERM). Codes and checkpoints are available at https://github.com/davelee-uestc/nsf_debiasing .

</details>


### [212] [M-ErasureBench: A Comprehensive Multimodal Evaluation Benchmark for Concept Erasure in Diffusion Models](https://arxiv.org/abs/2512.22877)
*Ju-Hsuan Weng,Jia-Wei Liao,Cheng-Fu Chou,Jun-Cheng Chen*

Main category: cs.CV

TL;DR: The paper addresses the challenge of concept erasure in text-to-image models across various input modalities, proposing a benchmark (M-ErasureBench) and a plug-and-play module (IRECE) to improve robustness.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle limitations in existing concept erasure methods, which primarily focus on text prompts and fail to consider other input modalities like learned embeddings and inverted latents, critical for secure generative model applications.

Method: The authors introduce M-ErasureBench, a multimodal evaluation framework for benchmarking concept erasure and propose IRECE, a module that enhances robustness by localizing concepts via cross-attention and perturbing latents during denoising.

Result: M-ErasureBench revealed existing erasure methods' failures in non-text inputs, with CRR exceeding 90% in white-box settings. The proposed IRECE reduces CRR by up to 40% while maintaining visual quality.

Conclusion: M-ErasureBench establishes a comprehensive framework to assess concept erasure, and IRECE addresses vulnerabilities across input modalities, offering a practical step toward safer generative models.

Abstract: Text-to-image diffusion models may generate harmful or copyrighted content, motivating research on concept erasure. However, existing approaches primarily focus on erasing concepts from text prompts, overlooking other input modalities that are increasingly critical in real-world applications such as image editing and personalized generation. These modalities can become attack surfaces, where erased concepts re-emerge despite defenses. To bridge this gap, we introduce M-ErasureBench, a novel multimodal evaluation framework that systematically benchmarks concept erasure methods across three input modalities: text prompts, learned embeddings, and inverted latents. For the latter two, we evaluate both white-box and black-box access, yielding five evaluation scenarios. Our analysis shows that existing methods achieve strong erasure performance against text prompts but largely fail under learned embeddings and inverted latents, with Concept Reproduction Rate (CRR) exceeding 90% in the white-box setting. To address these vulnerabilities, we propose IRECE (Inference-time Robustness Enhancement for Concept Erasure), a plug-and-play module that localizes target concepts via cross-attention and perturbs the associated latents during denoising. Experiments demonstrate that IRECE consistently restores robustness, reducing CRR by up to 40% under the most challenging white-box latent inversion scenario, while preserving visual quality. To the best of our knowledge, M-ErasureBench provides the first comprehensive benchmark of concept erasure beyond text prompts. Together with IRECE, our benchmark offers practical safeguards for building more reliable protective generative models.

</details>


### [213] [SwinTF3D: A Lightweight Multimodal Fusion Approach for Text-Guided 3D Medical Image Segmentation](https://arxiv.org/abs/2512.22878)
*Hasan Faraz Khan,Noor Fatima,Muzammil Behzad*

Main category: cs.CV

TL;DR: SwinTF3D proposes a multimodal approach combining visual and linguistic features for text-guided 3D medical image segmentation, excelling in adaptability, efficiency, and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing 3D segmentation frameworks lack semantic understanding and adaptability to new domains and user-defined segmentation tasks, limiting their practical application in clinical scenarios.

Method: SwinTF3D uses a transformer-based visual encoder to extract features and integrates text prompts through an efficient multimodal fusion mechanism, enabling context-aware segmentation.

Result: The model achieves competitive Dice and IoU scores on the BTCV dataset, demonstrates strong generalization to unseen data, and offers notable efficiency gains over traditional methods.

Conclusion: SwinTF3D establishes a practical method for text-driven 3D medical image segmentation, combining interpretability and efficiency to enhance clinical imaging capabilities.

Abstract: The recent integration of artificial intelligence into medical imaging has driven remarkable advances in automated organ segmentation. However, most existing 3D segmentation frameworks rely exclusively on visual learning from large annotated datasets restricting their adaptability to new domains and clinical tasks. The lack of semantic understanding in these models makes them ineffective in addressing flexible, user-defined segmentation objectives. To overcome these limitations, we propose SwinTF3D, a lightweight multimodal fusion approach that unifies visual and linguistic representations for text-guided 3D medical image segmentation. The model employs a transformer-based visual encoder to extract volumetric features and integrates them with a compact text encoder via an efficient fusion mechanism. This design allows the system to understand natural-language prompts and correctly align semantic cues with their corresponding spatial structures in medical volumes, while producing accurate, context-aware segmentation results with low computational overhead. Extensive experiments on the BTCV dataset demonstrate that SwinTF3D achieves competitive Dice and IoU scores across multiple organs, despite its compact architecture. The model generalizes well to unseen data and offers significant efficiency gains compared to conventional transformer-based segmentation networks. Bridging visual perception with linguistic understanding, SwinTF3D establishes a practical and interpretable paradigm for interactive, text-driven 3D medical image segmentation, opening perspectives for more adaptive and resource-efficient solutions in clinical imaging.

</details>


### [214] [Guided Path Sampling: Steering Diffusion Models Back on Track with Principled Path Guidance](https://arxiv.org/abs/2512.22881)
*Haosen Li,Wenshuo Chen,Shaofeng Liang,Lei Wang,Haozhe Jia,Yutao Yue*

Main category: cs.CV

TL;DR: The paper proposes Guided Path Sampling (GPS), a new iterative refinement method that maintains sampling paths on the data manifold for stable diffusion model enhancement, addressing limitations in traditional Classifier-Free Guidance (CFG).


<details>
  <summary>Details</summary>
Motivation: To improve the stability and effectiveness of iterative refinement methods in diffusion models, which are hindered by the limitations of the standard Classifier-Free Guidance (CFG).

Method: The authors propose Guided Path Sampling (GPS), which replaces CFG's extrapolative approach with a manifold-constrained interpolation to ensure sampling paths remain stable and on the data manifold. They also develop an optimal scheduling strategy that dynamically aligns guidance strength with the model's generation process.

Result: GPS achieves stability in iterative refinement, improving both perceptual quality and semantic alignment. It outperforms existing methods, achieving ImageReward of 0.79 and HPS v2 of 0.2995 on SDXL, and 57.45% alignment accuracy on GenEval.

Conclusion: The research concludes that maintaining path stability is critical for effective iterative refinement in diffusion models and shows that GPS is a robust framework for achieving this stability.

Abstract: Iterative refinement methods based on a denoising-inversion cycle are powerful tools for enhancing the quality and control of diffusion models. However, their effectiveness is critically limited when combined with standard Classifier-Free Guidance (CFG). We identify a fundamental limitation: CFG's extrapolative nature systematically pushes the sampling path off the data manifold, causing the approximation error to diverge and undermining the refinement process. To address this, we propose Guided Path Sampling (GPS), a new paradigm for iterative refinement. GPS replaces unstable extrapolation with a principled, manifold-constrained interpolation, ensuring the sampling path remains on the data manifold. We theoretically prove that this correction transforms the error series from unbounded amplification to strictly bounded, guaranteeing stability. Furthermore, we devise an optimal scheduling strategy that dynamically adjusts guidance strength, aligning semantic injection with the model's natural coarse-to-fine generation process. Extensive experiments on modern backbones like SDXL and Hunyuan-DiT show that GPS outperforms existing methods in both perceptual quality and complex prompt adherence. For instance, GPS achieves a superior ImageReward of 0.79 and HPS v2 of 0.2995 on SDXL, while improving overall semantic alignment accuracy on GenEval to 57.45%. Our work establishes that path stability is a prerequisite for effective iterative refinement, and GPS provides a robust framework to achieve it.

</details>


### [215] [Hash Grid Feature Pruning](https://arxiv.org/abs/2512.22882)
*Yangzhi Ma,Bojun Liu,Jie Li,Li Li,Dong Liu*

Main category: cs.CV

TL;DR: The paper proposes a pruning method for hash grid features in neural fields for Gaussian splatting, optimizing storage and reducing bitrate by 8% without degrading performance.


<details>
  <summary>Details</summary>
Motivation: Current hash grid approaches in Gaussian splatting suffer from inefficiencies due to sparse regions caused by non-uniform distribution in 3D space, leading to redundant storage and overhead.

Method: The authors introduced a pruning mechanism that removes invalid features in a hash grid based on Gaussian splat coordinates, ensuring only valid features remain encoded.

Result: The method achieves an 8% average bitrate reduction compared to the baseline approach while maintaining model performance.

Conclusion: Pruning invalid features in hash grids optimizes storage and transmission for Gaussian splatting, offering better rate-distortion performance with no compromise on quality.

Abstract: Hash grids are widely used to learn an implicit neural field for Gaussian splatting, serving either as part of the entropy model or for inter-frame prediction. However, due to the irregular and non-uniform distribution of Gaussian splats in 3D space, numerous sparse regions exist, rendering many features in the hash grid invalid. This leads to redundant storage and transmission overhead. In this work, we propose a hash grid feature pruning method that identifies and prunes invalid features based on the coordinates of the input Gaussian splats, so that only the valid features are encoded. This approach reduces the storage size of the hash grid without compromising model performance, leading to improved rate-distortion performance. Following the Common Test Conditions (CTC) defined by the standardization committee, our method achieves an average bitrate reduction of 8% compared to the baseline approach.

</details>


### [216] [JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation](https://arxiv.org/abs/2512.22905)
*Kai Liu,Jungang Li,Yuchong Sun,Shengqiong Wu,Jianzhang Gao,Daoan Zhang,Wei Zhang,Sheng Jin,Sicheng Yu,Geng Zhan,Jiayi Ji,Fan Zhou,Liang Zheng,Shuicheng Yan,Hao Fei,Tat-Seng Chua*

Main category: cs.CV

TL;DR: JavisGPT introduces a unified multimodal model for audio-video comprehension and generation with innovative architecture and training approaches.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the need for joint audio-video comprehension and generation within multimodal models, which are increasingly relevant due to their applications in synchronized audiovisual tasks.

Method: The model employs a concise encoder-LLM-decoder framework, featuring a SyncFusion module for spatio-temporal fusion and synchrony-aware queries. It also follows a three-stage training pipeline consisting of multimodal pretraining, audio-video fine-tuning, and large-scale instruction-tuning. Additionally, a curated instruction dataset, JavisInst-Omni, was developed with 200K dialogues.

Result: JavisGPT achieves significant improvements over existing multimodal large language models on benchmarks pertaining to audio-video comprehension and generation, especially in complex synchronized settings.

Conclusion: JavisGPT demonstrates the potential of unified multimodal models for handling sophisticated audio-video tasks, paving the way for advancements in temporally coherent multimedia generation and understanding through innovative architecture and training strategies.

Abstract: This paper presents JavisGPT, the first unified multimodal large language model (MLLM) for Joint Audio-Video (JAV) comprehension and generation. JavisGPT adopts a concise encoder-LLM-decoder architecture, featuring a SyncFusion module for spatio-temporal audio-video fusion and synchrony-aware learnable queries to bridge a pretrained JAV-DiT generator. This design enables temporally coherent video-audio understanding and generation from multimodal instructions. We design an effective three-stage training pipeline consisting of multimodal pretraining, audio-video fine-tuning, and large-scale instruction-tuning, to progressively build multimodal comprehension and generation from existing vision-language models. To support this, we further construct JavisInst-Omni, a high-quality instruction dataset with over 200K GPT-4o-curated audio-video-text dialogues that span diverse and multi-level comprehension and generation scenarios. Extensive experiments on JAV comprehension and generation benchmarks show that JavisGPT outperforms existing MLLMs, particularly in complex and temporally synchronized settings.

</details>


### [217] [ColaVLA: Leveraging Cognitive Latent Reasoning for Hierarchical Parallel Trajectory Planning in Autonomous Driving](https://arxiv.org/abs/2512.22939)
*Qihang Peng,Xuesong Chen,Chenye Yang,Shaoshuai Shi,Hongsheng Li*

Main category: cs.CV

TL;DR: ColaVLA introduces a vision-language-action framework addressing challenges in autonomous driving with efficient trajectory planning and reasoning, achieving state-of-the-art performance on nuScenes.


<details>
  <summary>Details</summary>
Motivation: To improve autonomous driving systems by addressing challenges in vision-language-based trajectory planning, such as mismatched reasoning, latency, and inefficiency.

Method: ColaVLA uses a Cognitive Latent Reasoner for scene understanding and a Hierarchical Parallel Planner for causality-accurate, efficient trajectory generation, both leveraging vision-language priors.

Result: ColaVLA delivered state-of-the-art results on the nuScenes benchmark in both open-loop and closed-loop settings, proving its efficiency and robustness.

Conclusion: The paper demonstrates ColaVLA's ability to combine vision-language interpretability with fast and safe trajectory generation for autonomous driving applications.

Abstract: Autonomous driving requires generating safe and reliable trajectories from complex multimodal inputs. Traditional modular pipelines separate perception, prediction, and planning, while recent end-to-end (E2E) systems learn them jointly. Vision-language models (VLMs) further enrich this paradigm by introducing cross-modal priors and commonsense reasoning, yet current VLM-based planners face three key challenges: (i) a mismatch between discrete text reasoning and continuous control, (ii) high latency from autoregressive chain-of-thought decoding, and (iii) inefficient or non-causal planners that limit real-time deployment. We propose ColaVLA, a unified vision-language-action framework that transfers reasoning from text to a unified latent space and couples it with a hierarchical, parallel trajectory decoder. The Cognitive Latent Reasoner compresses scene understanding into compact, decision-oriented meta-action embeddings through ego-adaptive selection and only two VLM forward passes. The Hierarchical Parallel Planner then generates multi-scale, causality-consistent trajectories in a single forward pass. Together, these components preserve the generalization and interpretability of VLMs while enabling efficient, accurate and safe trajectory generation. Experiments on the nuScenes benchmark show that ColaVLA achieves state-of-the-art performance in both open-loop and closed-loop settings with favorable efficiency and robustness.

</details>


### [218] [Learning Where to Focus: Density-Driven Guidance for Detecting Dense Tiny Objects](https://arxiv.org/abs/2512.22949)
*Zhicheng Zhao,Xuanang Fan,Lingma Sun,Chenglong Li,Jin Tang*

Main category: cs.CV

TL;DR: High-resolution remote sensing imagery poses challenges in detecting dense clusters of tiny objects due to occlusion and small pixel footprints. DRMNet utilizes density maps as spatial priors for adaptive feature learning and improves detection performance in high-density, occluded settings.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with detecting densely clustered tiny objects in high-resolution imagery due to occlusion and uniform allocation of computational resources, limiting feature learning in critical areas.

Method: DRMNet integrates three key components: Density Generation Branch (DGB) for modeling object density, Dense Area Focusing Module (DAFM) for efficient local-global attention, and Dual Filter Fusion Module (DFFM) for mitigating feature degradation using density-guided cross-attention.

Result: DRMNet outperforms state-of-the-art detection methods on AI-TOD and DTOD datasets, especially in scenarios with high object density and severe occlusions.

Conclusion: Using density maps as spatial priors in DRMNet improves the detection of tiny, densely clustered objects, addressing both feature degradation and computational inefficiencies effectively.

Abstract: High-resolution remote sensing imagery increasingly contains dense clusters of tiny objects, the detection of which is extremely challenging due to severe mutual occlusion and limited pixel footprints. Existing detection methods typically allocate computational resources uniformly, failing to adaptively focus on these density-concentrated regions, which hinders feature learning effectiveness. To address these limitations, we propose the Dense Region Mining Network (DRMNet), which leverages density maps as explicit spatial priors to guide adaptive feature learning. First, we design a Density Generation Branch (DGB) to model object distribution patterns, providing quantifiable priors that guide the network toward dense regions. Second, to address the computational bottleneck of global attention, our Dense Area Focusing Module (DAFM) uses these density maps to identify and focus on dense areas, enabling efficient local-global feature interaction. Finally, to mitigate feature degradation during hierarchical extraction, we introduce a Dual Filter Fusion Module (DFFM). It disentangles multi-scale features into high- and low-frequency components using a discrete cosine transform and then performs density-guided cross-attention to enhance complementarity while suppressing background interference. Extensive experiments on the AI-TOD and DTOD datasets demonstrate that DRMNet surpasses state-of-the-art methods, particularly in complex scenarios with high object density and severe occlusion.

</details>


### [219] [CLIP-Joint-Detect: End-to-End Joint Training of Object Detectors with Contrastive Vision-Language Supervision](https://arxiv.org/abs/2512.22969)
*Behnam Raoufi,Hossein Sharify,Mohamad Mahdee Ramezanee,Khosrow Hajsadeghi,Saeed Bagheri Shouraki*

Main category: cs.CV

TL;DR: The paper introduces CLIP-Joint-Detect, a framework that integrates vision-language supervision into object detectors to tackle challenges like class imbalance and label noise, showing enhanced performance on popular benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the vulnerabilities of conventional object detectors, such as issues with class imbalance and label noise, and to improve detection performance using vision-language supervision.

Method: The proposed framework, CLIP-Joint-Detect, integrates CLIP-style contrastive vision-language supervision with detectors by employing a lightweight parallel head that aligns region or grid features with text embeddings. It applies joint training using InfoNCE contrastive loss, an auxiliary cross-entropy term, and standard detection losses. The method is adaptable to both two-stage and one-stage detection architectures.

Result: The framework achieves consistent and significant performance improvements in closed-set detection on Pascal VOC 2007+2012 (with Faster R-CNN) and MS COCO 2017 (with YOLOv11), without compromising real-time inference usability.

Conclusion: CLIP-Joint-Detect effectively enhances object detection performance by leveraging vision-language joint training, making it an adaptable and robust approach across diverse architectures and datasets.

Abstract: Conventional object detectors rely on cross-entropy classification, which can be vulnerable to class imbalance and label noise. We propose CLIP-Joint-Detect, a simple and detector-agnostic framework that integrates CLIP-style contrastive vision-language supervision through end-to-end joint training. A lightweight parallel head projects region or grid features into the CLIP embedding space and aligns them with learnable class-specific text embeddings via InfoNCE contrastive loss and an auxiliary cross-entropy term, while all standard detection losses are optimized simultaneously. The approach applies seamlessly to both two-stage and one-stage architectures. We validate it on Pascal VOC 2007+2012 using Faster R-CNN and on the large-scale MS COCO 2017 benchmark using modern YOLO detectors (YOLOv11), achieving consistent and substantial improvements while preserving real-time inference speed. Extensive experiments and ablations demonstrate that joint optimization with learnable text embeddings markedly enhances closed-set detection performance across diverse architectures and datasets.

</details>


### [220] [Wavelet-based Multi-View Fusion of 4D Radar Tensor and Camera for Robust 3D Object Detection](https://arxiv.org/abs/2512.22972)
*Runwei Guan,Jianan Liu,Shaofeng Liang,Fangqiang Ding,Shanliang Yao,Xiaokai Bai,Daizong Liu,Tao Huang,Guoqiang Mao,Hui Xiong*

Main category: cs.CV

TL;DR: This paper introduces WRCFormer, a framework for improving 3D object detection by fusing raw radar and camera data, achieving state-of-the-art results under diverse conditions.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of 4D mmWave radar's sparsity and limited semantics, as well as the computational challenges of processing raw radar data when fused with camera inputs for enhanced perception.

Method: WRCFormer fuses raw radar cubes and camera data through multi-view representations and incorporates a Wavelet Attention Module within a wavelet-based FPN. The framework also employs Geometry-guided Progressive Fusion for efficient multi-view feature integration.

Result: WRCFormer achieves state-of-the-art performance on the K-Radar benchmark, with a 2.4% improvement overall and 1.6% in sleet conditions, demonstrating superior robustness in adverse weather.

Conclusion: WRCFormer effectively addresses radar-and-camera fusion challenges, providing enhanced and robust 3D object detection capabilities in varied scenarios, especially poor weather conditions.

Abstract: 4D millimeter-wave (mmWave) radar has been widely adopted in autonomous driving and robot perception due to its low cost and all-weather robustness. However, its inherent sparsity and limited semantic richness significantly constrain perception capability. Recently, fusing camera data with 4D radar has emerged as a promising cost effective solution, by exploiting the complementary strengths of the two modalities. Nevertheless, point-cloud-based radar often suffer from information loss introduced by multi-stage signal processing, while directly utilizing raw 4D radar data incurs prohibitive computational costs. To address these challenges, we propose WRCFormer, a novel 3D object detection framework that fuses raw radar cubes with camera inputs via multi-view representations of the decoupled radar cube. Specifically, we design a Wavelet Attention Module as the basic module of wavelet-based Feature Pyramid Network (FPN) to enhance the representation of sparse radar signals and image data. We further introduce a two-stage query-based, modality-agnostic fusion mechanism termed Geometry-guided Progressive Fusion to efficiently integrate multi-view features from both modalities. Extensive experiments demonstrate that WRCFormer achieves state-of-the-art performance on the K-Radar benchmarks, surpassing the best model by approximately 2.4% in all scenarios and 1.6% in the sleet scenario, highlighting its robustness under adverse weather conditions.

</details>


### [221] [YOLO-IOD: Towards Real Time Incremental Object Detection](https://arxiv.org/abs/2512.22973)
*Shizhou Zhang,Xueqiang Lv,Yinghui Xing,Qirui Wu,Di Xu,Chen Zhao,Yanning Zhang*

Main category: cs.CV

TL;DR: This study introduces YOLO-IOD, a framework for real-time incremental object detection, addressing catastrophic forgetting in YOLO-based detectors using innovative techniques like CPR, IKS, and CAKD.


<details>
  <summary>Details</summary>
Motivation: Traditional IOD methods mainly rely on Faster R-CNN or DETR-like detectors, which are unsuitable for real-time YOLO models. The research aims to solve knowledge conflicts and catastrophic forgetting in YOLO-based incremental learning.

Method: The framework employs CPR to refine pseudo labels, IKS to target important kernels, and CAKD to resolve knowledge distillation issues, allowing effective stage-wise learning in YOLO-based detectors.

Result: Experiments on public benchmarks and the new LoCo COCO dataset show that YOLO-IOD outperforms existing methods with reduced forgetting and better incremental object detection.

Conclusion: YOLO-IOD is a feasible and superior solution for real-time incremental object detection, incorporating innovative techniques to manage complex challenges in YOLO-based frameworks.

Abstract: Current methods for incremental object detection (IOD) primarily rely on Faster R-CNN or DETR series detectors; however, these approaches do not accommodate the real-time YOLO detection frameworks. In this paper, we first identify three primary types of knowledge conflicts that contribute to catastrophic forgetting in YOLO-based incremental detectors: foreground-background confusion, parameter interference, and misaligned knowledge distillation. Subsequently, we introduce YOLO-IOD, a real-time Incremental Object Detection (IOD) framework that is constructed upon the pretrained YOLO-World model, facilitating incremental learning via a stage-wise parameter-efficient fine-tuning process. Specifically, YOLO-IOD encompasses three principal components: 1) Conflict-Aware Pseudo-Label Refinement (CPR), which mitigates the foreground-background confusion by leveraging the confidence levels of pseudo labels and identifying potential objects relevant to future tasks. 2) Importancebased Kernel Selection (IKS), which identifies and updates the pivotal convolution kernels pertinent to the current task during the current learning stage. 3) Cross-Stage Asymmetric Knowledge Distillation (CAKD), which addresses the misaligned knowledge distillation conflict by transmitting the features of the student target detector through the detection heads of both the previous and current teacher detectors, thereby facilitating asymmetric distillation between existing and newly introduced categories. We further introduce LoCo COCO, a more realistic benchmark that eliminates data leakage across stages. Experiments on both conventional and LoCo COCO benchmarks show that YOLO-IOD achieves superior performance with minimal forgetting.

</details>


### [222] [RealCamo: Boosting Real Camouflage Synthesis with Layout Controls and Textual-Visual Guidance](https://arxiv.org/abs/2512.22974)
*Chunyuan Chen,Yunuo Cai,Shujuan Li,Weiyun Liang,Bin Wang,Jing Xu*

Main category: cs.CV

TL;DR: ReamCamo enhances camouflaged image generation by improving semantic coherence and visual fidelity using layout controls and multi-modal conditions.


<details>
  <summary>Details</summary>
Motivation: Existing camouflaged image generation methods either lack camouflage quality or have inconsistent backgrounds, leading to inefficiencies in training camouflaged object detection models.

Method: ReamCamo employs an out-painting framework with layout controls to ensure semantic coherence between objects and backgrounds, and uses multi-modal textual-visual conditions for enhanced realism.

Result: ReamCamo achieves high-quality camouflaged images with semantic coherence and visual fidelity, proven through extensive experiments and a proposed quantitative metric.

Conclusion: ReamCamo significantly improves camouflaged image generation quality, offering an effective alternative for producing training data for camouflaged object detection.

Abstract: Camouflaged image generation (CIG) has recently emerged as an efficient alternative for acquiring high-quality training data for camouflaged object detection (COD). However, existing CIG methods still suffer from a substantial gap to real camouflaged imagery: generated images either lack sufficient camouflage due to weak visual similarity, or exhibit cluttered backgrounds that are semantically inconsistent with foreground targets. To address these limitations, we propose ReamCamo, a unified out-painting based framework for realistic camouflaged image generation. ReamCamo explicitly introduces additional layout controls to regulate global image structure, thereby improving semantic coherence between foreground objects and generated backgrounds. Moreover, we construct a multi-modal textual-visual condition by combining a unified fine-grained textual task description with texture-oriented background retrieval, which jointly guides the generation process to enhance visual fidelity and realism. To quantitatively assess camouflage quality, we further introduce a background-foreground distribution divergence metric that measures the effectiveness of camouflage in generated images. Extensive experiments and visualizations demonstrate the effectiveness of our proposed framework.

</details>


### [223] [PoseStreamer: A Multi-modal Framework for 6DoF Pose Estimation of Unseen Moving Objects](https://arxiv.org/abs/2512.22979)
*Huiming Yang,Linglin Liao,Fei Ding,Sibo Wang,Zijian Zeng*

Main category: cs.CV

TL;DR: PoseStreamer is a robust multi-modal framework for 6DoF pose estimation under high-speed motion, addressing limitations of traditional RGB and event cameras.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of accurate 6DoF pose estimation of objects in high-speed and low-light situations, where traditional approaches fail due to motion blur and suboptimal performance.

Method: PoseStreamer incorporates three core techniques: Adaptive Pose Memory Queue for temporal consistency, Object-centric 2D Tracker for boosting 3D center recall, and Ray Pose Filter for geometric refinement. It also introduces a new dataset, MoCapCube6D.

Result: PoseStreamer demonstrates superior accuracy in high-speed scenarios and generalizes well to unseen objects, validated through extensive experiments.

Conclusion: This framework makes significant strides in 6DoF pose estimation, especially for novel objects in challenging environments, providing a robust, template-free solution.

Abstract: Six degree of freedom (6DoF) pose estimation for novel objects is a critical task in computer vision, yet it faces significant challenges in high-speed and low-light scenarios where standard RGB cameras suffer from motion blur. While event cameras offer a promising solution due to their high temporal resolution, current 6DoF pose estimation methods typically yield suboptimal performance in high-speed object moving scenarios. To address this gap, we propose PoseStreamer, a robust multi-modal 6DoF pose estimation framework designed specifically on high-speed moving scenarios. Our approach integrates three core components: an Adaptive Pose Memory Queue that utilizes historical orientation cues for temporal consistency, an Object-centric 2D Tracker that provides strong 2D priors to boost 3D center recall, and a Ray Pose Filter for geometric refinement along camera rays. Furthermore, we introduce MoCapCube6D, a novel multi-modal dataset constructed to benchmark performance under rapid motion. Extensive experiments demonstrate that PoseStreamer not only achieves superior accuracy in high-speed moving scenarios, but also exhibits strong generalizability as a template-free framework for unseen moving objects.

</details>


### [224] [Spatial-aware Symmetric Alignment for Text-guided Medical Image Segmentation](https://arxiv.org/abs/2512.22981)
*Linglin Liao,Qichuan Geng,Yu Liu*

Main category: cs.CV

TL;DR: The paper proposes the SSA framework to enhance medical image segmentation using spatial-aware and symmetric alignment for hybrid medical texts, achieving SOTA results.


<details>
  <summary>Details</summary>
Motivation: To address limitations in text-guided medical image segmentation, particularly in processing hybrid descriptive, positional, and diagnostic information and tackling critical deviations in spatial constraints.

Method: The authors propose the Spatial-aware Symmetric Alignment (SSA) framework, which uses symmetric optimal transport alignment for bi-directional multimodal correspondence and composite directional guidance for spatial constraints.

Result: SSA demonstrated state-of-the-art performance on public benchmarks, showing improved segmentation accuracy, especially for lesions defined by spatial constraints.

Conclusion: The SSA framework effectively enhances segmentation tasks by addressing current limitations, providing precise lesion identification and spatial alignment using hybrid medical text guidance.

Abstract: Text-guided Medical Image Segmentation has shown considerable promise for medical image segmentation, with rich clinical text serving as an effective supplement for scarce data. However, current methods have two key bottlenecks. On one hand, they struggle to process diagnostic and descriptive texts simultaneously, making it difficult to identify lesions and establish associations with image regions. On the other hand, existing approaches focus on lesions description and fail to capture positional constraints, leading to critical deviations. Specifically, with the text "in the left lower lung", the segmentation results may incorrectly cover both sides of the lung. To address the limitations, we propose the Spatial-aware Symmetric Alignment (SSA) framework to enhance the capacity of referring hybrid medical texts consisting of locational, descriptive, and diagnostic information. Specifically, we propose symmetric optimal transport alignment mechanism to strengthen the associations between image regions and multiple relevant expressions, which establishes bi-directional fine-grained multimodal correspondences. In addition, we devise a composite directional guidance strategy that explicitly introduces spatial constraints in the text by constructing region-level guidance masks. Extensive experiments on public benchmarks demonstrate that SSA achieves state-of-the-art (SOTA) performance, particularly in accurately segmenting lesions characterized by spatial relational constraints.

</details>


### [225] [Reverse Personalization](https://arxiv.org/abs/2512.22984)
*Han-Wei Kung,Tuomas Varanka,Nicu Sebe*

Main category: cs.CV

TL;DR: The paper proposes a novel framework for anonymizing facial images by directly manipulating them using conditional diffusion inversion, surpassing limitations of prompt-based methods and enabling attribute-controllable anonymization.


<details>
  <summary>Details</summary>
Motivation: Current methods for removing or modifying identity-specific facial features are either not generalizable or require extensive model fine-tuning, which limits their applicability and flexibility.

Method: The framework uses conditional diffusion inversion for direct image manipulation and extends generalization capabilities through an identity-guided conditioning branch, enabling control over facial attributes.

Result: The proposed method achieves a superior balance of identity removal, attribute preservation, and overall image quality compared to existing approaches.

Conclusion: The paper introduces an adaptable and attribute-controllable face anonymization framework, offering state-of-the-art performance and overcoming existing challenges in facial identity manipulation.

Abstract: Recent text-to-image diffusion models have demonstrated remarkable generation of realistic facial images conditioned on textual prompts and human identities, enabling creating personalized facial imagery. However, existing prompt-based methods for removing or modifying identity-specific features rely either on the subject being well-represented in the pre-trained model or require model fine-tuning for specific identities. In this work, we analyze the identity generation process and introduce a reverse personalization framework for face anonymization. Our approach leverages conditional diffusion inversion, allowing direct manipulation of images without using text prompts. To generalize beyond subjects in the model's training data, we incorporate an identity-guided conditioning branch. Unlike prior anonymization methods, which lack control over facial attributes, our framework supports attribute-controllable anonymization. We demonstrate that our method achieves a state-of-the-art balance between identity removal, attribute preservation, and image quality. Source code and data are available at https://github.com/hanweikung/reverse-personalization .

</details>


### [226] [A Low-Cost UAV Deep Learning Pipeline for Integrated Apple Disease Diagnosis,Freshness Assessment, and Fruit Detection](https://arxiv.org/abs/2512.22990)
*Soham Dutta,Soham Banerjee,Sneha Mahata,Anindya Sen,Sayantani Datta*

Main category: cs.CV

TL;DR: This paper presents a low-cost, unified RGB-only UAV-based system using advanced neural networks for apple orchard monitoring, achieving high accuracy in disease detection, freshness assessment, and apple localization.


<details>
  <summary>Details</summary>
Motivation: The motivation is to create an affordable, integrated solution for apple orchard monitoring that eliminates reliance on expensive multispectral sensors while enabling major tasks like disease detection, quality assessment, and yield estimation in a unified system.

Method: The paper integrates ResNet50 for leaf disease detection, VGG16 for apple freshness classification, and YOLOv8 for detecting and locating apples, all running on low-cost hardware (ESP32-CAM and Raspberry Pi) for on-site offline inference.

Result: The system achieves 98.9% accuracy for leaf disease classification, 97.4% for apple freshness classification, and a 0.857 F1 score for apple detection.

Conclusion: The proposed system is a scalable, affordable alternative to multispectral UAV solutions, supporting real-world precision agriculture tasks efficiently with low-cost hardware.

Abstract: Apple orchards require timely disease detection, fruit quality assessment, and yield estimation, yet existing UAV-based systems address such tasks in isolation and often rely on costly multispectral sensors. This paper presents a unified, low-cost RGB-only UAV-based orchard intelligent pipeline integrating ResNet50 for leaf disease detection, VGG 16 for apple freshness determination, and YOLOv8 for real-time apple detection and localization. The system runs on an ESP32-CAM and Raspberry Pi, providing fully offline on-site inference without cloud support. Experiments demonstrate 98.9% accuracy for leaf disease classification, 97.4% accuracy for freshness classification, and 0.857 F1 score for apple detection. The framework provides an accessible and scalable alternative to multispectral UAV solutions, supporting practical precision agriculture on affordable hardware.

</details>


### [227] [OpenGround: Active Cognition-based Reasoning for Open-World 3D Visual Grounding](https://arxiv.org/abs/2512.23020)
*Wenyuan Huang,Zhao Wang,Zhou Wei,Ting Huang,Fang Zhao,Jian Yang,Zhenyu Zhang*

Main category: cs.CV

TL;DR: OpenGround introduces a framework for 3D visual grounding in open-world scenarios without relying on a pre-defined Object Lookup Table (OLT).


<details>
  <summary>Details</summary>
Motivation: Existing methods for 3D visual grounding are constrained by reliance on pre-defined OLTs, which restrict adaptability to undefined or unforeseen targets.

Method: The Active Cognition-based Reasoning (ACR) module dynamically augments VLMs' cognitive scope through human-like perception and reasoning, using a continuously updated OLT.

Result: OpenGround achieves competitive performance on Nr3D, state-of-the-art results on ScanRefer, and a 17.6% improvement on the newly proposed OpenTarget dataset.

Conclusion: OpenGround effectively addresses the limitations of pre-defined categories in 3D visual grounding, making it adaptable to both pre-defined and open-world scenarios.

Abstract: 3D visual grounding aims to locate objects based on natural language descriptions in 3D scenes. Existing methods rely on a pre-defined Object Lookup Table (OLT) to query Visual Language Models (VLMs) for reasoning about object locations, which limits the applications in scenarios with undefined or unforeseen targets. To address this problem, we present OpenGround, a novel zero-shot framework for open-world 3D visual grounding. Central to OpenGround is the Active Cognition-based Reasoning (ACR) module, which is designed to overcome the fundamental limitation of pre-defined OLTs by progressively augmenting the cognitive scope of VLMs. The ACR module performs human-like perception of the target via a cognitive task chain and actively reasons about contextually relevant objects, thereby extending VLM cognition through a dynamically updated OLT. This allows OpenGround to function with both pre-defined and open-world categories. We also propose a new dataset named OpenTarget, which contains over 7000 object-description pairs to evaluate our method in open-world scenarios. Extensive experiments demonstrate that OpenGround achieves competitive performance on Nr3D, state-of-the-art on ScanRefer, and delivers a substantial 17.6% improvement on OpenTarget. Project Page at [this https URL](https://why-102.github.io/openground.io/).

</details>


### [228] [With Great Context Comes Great Prediction Power: Classifying Objects via Geo-Semantic Scene Graphs](https://arxiv.org/abs/2512.23024)
*Ciprian Constantinescu,Marius Leordeanu*

Main category: cs.CV

TL;DR: The paper introduces a novel framework for object recognition using contextual understanding via Geo-Semantic Contextual Graphs (GSCG), significantly outperforming baseline approaches.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional object recognition systems that lack contextual understanding and rely on isolated image regions.

Method: Construct a Geo-Semantic Contextual Graph (GSCG) by combining metric depth estimation, panoptic, and material segmentation to encode objects and spatial relationships. Using a graph-based classifier, the model integrates target, neighbor, and global scene context for predictions.

Result: The context-aware model achieved a classification accuracy of 73.4%, outperforming context-agnostic models (38.4%), fine-tuned ResNet (53.5%), and multimodal LLMs (42.3%) on COCO 2017 datasets.

Conclusion: Explicitly structured and interpretable contextual representations significantly enhance object recognition performance compared to conventional methods.

Abstract: Humans effortlessly identify objects by leveraging a rich understanding of the surrounding scene, including spatial relationships, material properties, and the co-occurrence of other objects. In contrast, most computational object recognition systems operate on isolated image regions, devoid of meaning in isolation, thus ignoring this vital contextual information. This paper argues for the critical role of context and introduces a novel framework for contextual object classification. We first construct a Geo-Semantic Contextual Graph (GSCG) from a single monocular image. This rich, structured representation is built by integrating a metric depth estimator with a unified panoptic and material segmentation model. The GSCG encodes objects as nodes with detailed geometric, chromatic, and material attributes, and their spatial relationships as edges. This explicit graph structure makes the model's reasoning process inherently interpretable. We then propose a specialized graph-based classifier that aggregates features from a target object, its immediate neighbors, and the global scene context to predict its class. Through extensive ablation studies, we demonstrate that our context-aware model achieves a classification accuracy of 73.4%, dramatically outperforming context-agnostic versions (as low as 38.4%). Furthermore, our GSCG-based approach significantly surpasses strong baselines, including fine-tuned ResNet models (max 53.5%) and a state-of-the-art multimodal Large Language Model (LLM), Llama 4 Scout, which, even when given the full image alongside a detailed description of objects, maxes out at 42.3%. These results on COCO 2017 train/val splits highlight the superiority of explicitly structured and interpretable context for object recognition tasks.

</details>


### [229] [Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion](https://arxiv.org/abs/2512.23035)
*Yi Zhou,Xuechao Zou,Shun Zhang,Kai Li,Shiying Wang,Jingming Chen,Congyan Lang,Tengfei Cao,Pin Tao,Yuanchun Shi*

Main category: cs.CV

TL;DR: This paper introduces Co2S, a semi-supervised framework for remote sensing image segmentation, addressing pseudo-label drift using a dual-student architecture combining vision-language and self-supervised priors.


<details>
  <summary>Details</summary>
Motivation: The motivation is to reduce the dependency on exhaustive annotations in remote sensing image segmentation while addressing the issue of pseudo-label drift, which leads to errors during training.

Method: Co2S uses two distinct ViT-based models (CLIP and DINOv3) integrated through an explicit-implicit semantic co-guidance mechanism for semantic consistency, along with a global-local feature fusion strategy for enhanced segmentation precision.

Result: The method achieves state-of-the-art results on six datasets, performing consistently across various protocols and scenarios in remote sensing image segmentation.

Conclusion: The framework effectively alleviates pseudo-label drift, showing superior performance and presenting a scalable solution for semi-supervised segmentation challenges in diverse contexts.

Abstract: Semi-supervised remote sensing (RS) image semantic segmentation offers a promising solution to alleviate the burden of exhaustive annotation, yet it fundamentally struggles with pseudo-label drift, a phenomenon where confirmation bias leads to the accumulation of errors during training. In this work, we propose Co2S, a stable semi-supervised RS segmentation framework that synergistically fuses priors from vision-language models and self-supervised models. Specifically, we construct a heterogeneous dual-student architecture comprising two distinct ViT-based vision foundation models initialized with pretrained CLIP and DINOv3 to mitigate error accumulation and pseudo-label drift. To effectively incorporate these distinct priors, an explicit-implicit semantic co-guidance mechanism is introduced that utilizes text embeddings and learnable queries to provide explicit and implicit class-level guidance, respectively, thereby jointly enhancing semantic consistency. Furthermore, a global-local feature collaborative fusion strategy is developed to effectively fuse the global contextual information captured by CLIP with the local details produced by DINOv3, enabling the model to generate highly precise segmentation results. Extensive experiments on six popular datasets demonstrate the superiority of the proposed method, which consistently achieves leading performance across various partition protocols and diverse scenarios. Project page is available at https://xavierjiezou.github.io/Co2S/.

</details>


### [230] [3D sans 3D Scans: Scalable Pre-training from Video-Generated Point Clouds](https://arxiv.org/abs/2512.23042)
*Ryousuke Yamada,Kohsuke Ide,Yoshihiro Fukuhara,Hirokatsu Kataoka,Gilles Puy,Andrei Bursuc,Yuki M. Asano*

Main category: cs.CV

TL;DR: The study proposes LAM3C, a framework using point clouds generated from unlabeled videos to learn 3D representations without real 3D sensors. It outperforms previous methods in indoor segmentation tasks.


<details>
  <summary>Details</summary>
Motivation: The need for reducing reliance on costly and labor-intensive large-scale 3D scene scans for learning 3D representations.

Method: Introduction of the RoomTours dataset and LAM3C framework, including a noise-regularized loss for stabilizing representation learning using video-generated point clouds.

Result: LAM3C surpasses prior self-supervised methods in indoor semantic and instance segmentation tasks despite no use of real 3D scans.

Conclusion: Unlabeled videos are a viable and abundant data source for effective 3D self-supervised learning.

Abstract: Despite recent progress in 3D self-supervised learning, collecting large-scale 3D scene scans remains expensive and labor-intensive. In this work, we investigate whether 3D representations can be learned from unlabeled videos recorded without any real 3D sensors. We present Laplacian-Aware Multi-level 3D Clustering with Sinkhorn-Knopp (LAM3C), a self-supervised framework that learns from video-generated point clouds from unlabeled videos. We first introduce RoomTours, a video-generated point cloud dataset constructed by collecting room-walkthrough videos from the web (e.g., real-estate tours) and generating 49,219 scenes using an off-the-shelf feed-forward reconstruction model. We also propose a noise-regularized loss that stabilizes representation learning by enforcing local geometric smoothness and ensuring feature stability under noisy point clouds. Remarkably, without using any real 3D scans, LAM3C achieves higher performance than the previous self-supervised methods on indoor semantic and instance segmentation. These results suggest that unlabeled videos represent an abundant source of data for 3D self-supervised learning.

</details>


### [231] [Video-BrowseComp: Benchmarking Agentic Video Research on Open Web](https://arxiv.org/abs/2512.23044)
*Zhengyang Liang,Yan Shu,Xiangrui Liu,Minghao Qin,Kaixin Liang,Paolo Rota,Nicu Sebe,Zheng Liu,Lizi Liao*

Main category: cs.CV

TL;DR: The paper introduces "Video-BrowseComp," a benchmark aimed at addressing the limitations of current models in video-based open-web reasoning.


<details>
  <summary>Details</summary>
Motivation: To bridge the modality gap in video reasoning, as current benchmarks fail to test agentic video research involving active timeline navigation and external claim verification.

Method: Developed a benchmark, Video-BrowseComp, with 210 open-web reasoning questions that enforce retrieving and interpreting video evidence over textual data.

Result: State-of-the-art models like GPT-5.1 (w/ Search) achieved only 15.24% accuracy, highlighting their reliance on textual proxies and poor performance in metadata-sparse tasks.

Conclusion: Video-BrowseComp represents a shift from passive video perception to proactive, grounded video reasoning, uncovering significant weaknesses in current models.

Abstract: The evolution of autonomous agents is redefining information seeking, transitioning from passive retrieval to proactive, open-ended web research. However, while textual and static multimodal agents have seen rapid progress, a significant modality gap remains in processing the web's most dynamic modality: video. Existing video benchmarks predominantly focus on passive perception, feeding curated clips to models without requiring external retrieval. They fail to evaluate agentic video research, which necessitates actively interrogating video timelines, cross-referencing dispersed evidence, and verifying claims against the open web. To bridge this gap, we present \textbf{Video-BrowseComp}, a challenging benchmark comprising 210 questions tailored for open-web agentic video reasoning. Unlike prior benchmarks, Video-BrowseComp enforces a mandatory dependency on temporal visual evidence, ensuring that answers cannot be derived solely through text search but require navigating video timelines to verify external claims. Our evaluation of state-of-the-art models reveals a critical bottleneck: even advanced search-augmented models like GPT-5.1 (w/ Search) achieve only 15.24\% accuracy. Our analysis reveals that these models largely rely on textual proxies, excelling in metadata-rich domains (e.g., TV shows with plot summaries) but collapsing in metadata-sparse, dynamic environments (e.g., sports, gameplay) where visual grounding is essential. As the first open-web video research benchmark, Video-BrowseComp advances the field beyond passive perception toward proactive video reasoning.

</details>


### [232] [MedSAM-based lung masking for multi-label chest X-ray classification](https://arxiv.org/abs/2512.23089)
*Brayden Miao,Zain Rehman,Xin Miao,Siming Liu,Jianjie Wang*

Main category: cs.CV

TL;DR: This paper presents a segmentation-guided CXR classification approach using MedSAM for lung region extraction to improve the classification of pulmonary abnormalities.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in automated CXR interpretation, such as weak disease signals, dataset bias, and limited spatial supervision, by using anatomically grounded priors.

Method: The proposed method integrates the MedSAM model for lung segmentation, which is fine-tuned with public datasets, into a multi-label abnormality classification pipeline using deep convolutional neural networks.

Result: The study finds that MedSAM generates anatomically plausible lung masks and that lung masking's effects vary based on tasks and architectures, balancing accuracy for detecting abnormalities versus screening normal cases.

Conclusion: Lung masking should be tailored to match the neural network backbone and clinical objectives as it can significantly influence classification performance and task-specific goals.

Abstract: Chest X-ray (CXR) imaging is widely used for screening and diagnosing pulmonary abnormalities, yet automated interpretation remains challenging due to weak disease signals, dataset bias, and limited spatial supervision. Foundation models for medical image segmentation (MedSAM) provide an opportunity to introduce anatomically grounded priors that may improve robustness and interpretability in CXR analysis. We propose a segmentation-guided CXR classification pipeline that integrates MedSAM as a lung region extraction module prior to multi-label abnormality classification. MedSAM is fine-tuned using a public image-mask dataset from Airlangga University Hospital. We then apply it to a curated subset of the public NIH CXR dataset to train and evaluate deep convolutional neural networks for multi-label prediction of five abnormalities (Mass, Nodule, Pneumonia, Edema, and Fibrosis), with the normal case (No Finding) evaluated via a derived score. Experiments show that MedSAM produces anatomically plausible lung masks across diverse imaging conditions. We find that masking effects are both task-dependent and architecture-dependent. ResNet50 trained on original images achieves the strongest overall abnormality discrimination, while loose lung masking yields comparable macro AUROC but significantly improves No Finding discrimination, indicating a trade-off between abnormality-specific classification and normal case screening. Tight masking consistently reduces abnormality level performance but improves training efficiency. Loose masking partially mitigates this degradation by preserving perihilar and peripheral context. These results suggest that lung masking should be treated as a controllable spatial prior selected to match the backbone and clinical objective, rather than applied uniformly.

</details>


### [233] [PathoSyn: Imaging-Pathology MRI Synthesis via Disentangled Deviation Diffusion](https://arxiv.org/abs/2512.23130)
*Jian Wang,Sixing Rong,Jiarui Xing,Yuling Xu,Weide Liu*

Main category: cs.CV

TL;DR: PathoSyn, a generative framework for MRI image synthesis, combines anatomical reconstruction and deviation modeling to create realistic, interpretable pathological images while preserving structural integrity.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address limitations of current generative models for MRI synthesis, such as feature entanglement and structural artifact issues, by designing a more precise and interpretable method.

Method: PathoSyn decomposes the synthesis task into two parts: deterministic anatomical reconstruction and stochastic pathological deviation modeling. It introduces a Deviation-Space Diffusion Model with seam-aware fusion and stabilization modules to achieve high-quality outputs.

Result: The framework shows superior performance over baseline models in perceptual realism and anatomical fidelity, as proven by both qualitative and quantitative results on tumor imaging benchmarks.

Conclusion: PathoSyn provides a robust and interpretable approach for generating synthetic MRI datasets, aiding diagnostic algorithm development, disease progression modeling, and clinical decision-support benchmarking.

Abstract: We present PathoSyn, a unified generative framework for Magnetic Resonance Imaging (MRI) image synthesis that reformulates imaging-pathology as a disentangled additive deviation on a stable anatomical manifold. Current generative models typically operate in the global pixel domain or rely on binary masks, these paradigms often suffer from feature entanglement, leading to corrupted anatomical substrates or structural discontinuities. PathoSyn addresses these limitations by decomposing the synthesis task into deterministic anatomical reconstruction and stochastic deviation modeling. Central to our framework is a Deviation-Space Diffusion Model designed to learn the conditional distribution of pathological residuals, thereby capturing localized intensity variations while preserving global structural integrity by construction. To ensure spatial coherence, the diffusion process is coupled with a seam-aware fusion strategy and an inference-time stabilization module, which collectively suppress boundary artifacts and produce high-fidelity internal lesion heterogeneity. PathoSyn provides a mathematically principled pipeline for generating high-fidelity patient-specific synthetic datasets, facilitating the development of robust diagnostic algorithms in low-data regimes. By allowing interpretable counterfactual disease progression modeling, the framework supports precision intervention planning and provides a controlled environment for benchmarking clinical decision-support systems. Quantitative and qualitative evaluations on tumor imaging benchmarks demonstrate that PathoSyn significantly outperforms holistic diffusion and mask-conditioned baselines in both perceptual realism and anatomical fidelity. The source code of this work will be made publicly available.

</details>


### [234] [Domain-Shift Immunity in Deep Deformable Registration via Local Feature Representations](https://arxiv.org/abs/2512.23142)
*Mingzhen Shao,Sarang Joshi*

Main category: cs.CV

TL;DR: The paper highlights the inherent domain-shift immunity of deep deformable registration models due to their use of local feature representations, proposing the UniReg framework for improved robustness and multi-modal applicability.


<details>
  <summary>Details</summary>
Motivation: Traditional deformable image registration processes aren't robust to domain shifts, and while deep learning addresses accuracy and efficiency, it’s unclear why these models are seen as sensitive to domain changes.

Method: The paper introduces UniReg, a framework utilizing pre-trained feature extractors combined with a UNet-based network to decouple feature extraction from deformation estimation.

Result: UniReg achieves robust cross-domain and multi-modal performance, showing results comparable to traditional optimization-based methods and identifying local feature consistency as critical to robustness.

Conclusion: Local features play a critical role in enabling cross-domain robustness in learning-based deformable registration. UniReg's design motivates future architectures focusing on domain-invariant features.

Abstract: Deep learning has advanced deformable image registration, surpassing traditional optimization-based methods in both accuracy and efficiency. However, learning-based models are widely believed to be sensitive to domain shift, with robustness typically pursued through large and diverse training datasets, without explaining the underlying mechanisms. In this work, we show that domain-shift immunity is an inherent property of deep deformable registration models, arising from their reliance on local feature representations rather than global appearance for deformation estimation. To isolate and validate this mechanism, we introduce UniReg, a universal registration framework that decouples feature extraction from deformation estimation using fixed, pre-trained feature extractors and a UNet-based deformation network. Despite training on a single dataset, UniReg exhibits robust cross-domain and multi-modal performance comparable to optimization-based methods. Our analysis further reveals that failures of conventional CNN-based models under modality shift originate from dataset-induced biases in early convolutional layers. These findings identify local feature consistency as the key driver of robustness in learning-based deformable registration and motivate backbone designs that preserve domain-invariant local features.

</details>


### [235] [GeoTeacher: Geometry-Guided Semi-Supervised 3D Object Detection](https://arxiv.org/abs/2512.23147)
*Jingyu Li,Xiaolong Zhao,Zhe Liu,Wenxiao Wu,Li Zhang*

Main category: cs.CV

TL;DR: The paper introduces GeoTeacher, a method for semi-supervised 3D object detection that focuses on enhancing geometric understanding through keypoint-based supervision and voxel-wise data augmentation. It achieves state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Current semi-supervised 3D object detection methods struggle with capturing geometric information due to limited labeled data, thus hindering object perception and localization.

Method: The paper proposes GeoTeacher, which uses a keypoint-based geometric relation supervision module to teach geometry awareness from teacher to student. Additionally, it uses a voxel-wise data augmentation strategy with a distance-decay mechanism to enrich geometric understanding.

Result: The proposed method was shown to improve performance on 3D object detection benchmarks ONCE and Waymo, achieving state-of-the-art results.

Conclusion: GeoTeacher effectively enhances the capability of semi-supervised 3D detectors to understand geometric relations by transferring geometry-based knowledge and employing advanced data augmentation techniques.

Abstract: Semi-supervised 3D object detection, aiming to explore unlabeled data for boosting 3D object detectors, has emerged as an active research area in recent years. Some previous methods have shown substantial improvements by either employing heterogeneous teacher models to provide high-quality pseudo labels or enforcing feature-perspective consistency between the teacher and student networks. However, these methods overlook the fact that the model usually tends to exhibit low sensitivity to object geometries with limited labeled data, making it difficult to capture geometric information, which is crucial for enhancing the student model's ability in object perception and localization. In this paper, we propose GeoTeacher to enhance the student model's ability to capture geometric relations of objects with limited training data, especially unlabeled data. We design a keypoint-based geometric relation supervision module that transfers the teacher model's knowledge of object geometry to the student, thereby improving the student's capability in understanding geometric relations. Furthermore, we introduce a voxel-wise data augmentation strategy that increases the diversity of object geometries, thereby further improving the student model's ability to comprehend geometric structures. To preserve the integrity of distant objects during augmentation, we incorporate a distance-decay mechanism into this strategy. Moreover, GeoTeacher can be combined with different SS3D methods to further improve their performance. Extensive experiments on the ONCE and Waymo datasets indicate the effectiveness and generalization of our method and we achieve the new state-of-the-art results. Code will be available at https://github.com/SII-Whaleice/GeoTeacher

</details>


### [236] [REVEALER: Reinforcement-Guided Visual Reasoning for Element-Level Text-Image Alignment Evaluation](https://arxiv.org/abs/2512.23169)
*Fulin Shi,Wenyi Xiao,Bin Chen,Liang Din,Leilei Gan*

Main category: cs.CV

TL;DR: The paper introduces REVEALER, a framework for improving alignment evaluation between textual prompts and generated images, using reinforcement-guided visual reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing T2I model evaluation methods rely on static metrics and QA pipelines that lack fine-grained interpretability and fail to capture human preference.

Method: The proposed REVEALER adopts a "grounding-reasoning-conclusion" framework for semantic element localization and alignment optimization using Group Relative Policy Optimization (GRPO) with a composite reward function.

Result: REVEALER achieves state-of-the-art performance across four benchmarks, outperforming proprietary and supervised models while offering greater inference efficiency.

Conclusion: The framework provides a fine-grained, interpretable, and efficient approach for evaluating the alignment in T2I models and outperforms existing approaches in reliability and usability.

Abstract: Evaluating the alignment between textual prompts and generated images is critical for ensuring the reliability and usability of text-to-image (T2I) models. However, most existing evaluation methods rely on coarse-grained metrics or static QA pipelines, which lack fine-grained interpretability and struggle to reflect human preferences. To address this, we propose REVEALER, a unified framework for element-level alignment evaluation based on reinforcement-guided visual reasoning. Adopting a structured "grounding-reasoning-conclusion" paradigm, our method enables Multimodal Large Language Models (MLLMs) to explicitly localize semantic elements and derive interpretable alignment judgments. We optimize the model via Group Relative Policy Optimization(GRPO) using a composite reward function that incorporates structural format, grounding accuracy, and alignment fidelity. Extensive experiments across four benchmarks-EvalMuse-40K, RichHF, MHaluBench, and GenAI-Bench-demonstrate that REVEALER achieves state-of-the-art performance. Our approach consistently outperforms both strong proprietary models and supervised baselines while demonstrating superior inference efficiency compared to existing iterative visual reasoning methods.

</details>


### [237] [GVSynergy-Det: Synergistic Gaussian-Voxel Representations for Multi-View 3D Object Detection](https://arxiv.org/abs/2512.23176)
*Yi Zhang,Yi Wang,Lei Yao,Lap-Pui Chau*

Main category: cs.CV

TL;DR: The paper introduces GVSynergy-Det, a novel framework that improves image-based 3D object detection using synergistic Gaussian-Voxel representation learning, achieving state-of-the-art results without requiring dense 3D supervision.


<details>
  <summary>Details</summary>
Motivation: Image-based 3D object detection avoids expensive depth sensors but struggles with accuracy due to lack of dense supervision or reliance on extraneous geometry extraction.

Method: The proposed GVSynergy-Det employs continuous Gaussian and discrete voxel representations, utilizing a dual-representation architecture for synergistic feature learning and integration.

Result: The framework achieves state-of-the-art performance on ScanNetV2 and ARKitScenes datasets, surpassing existing methods and requiring no dense 3D geometry supervision.

Conclusion: Through synergistic representation learning, GVSynergy-Det enhances 3D detection accuracy, demonstrating the potential of combining Gaussian and voxel representations effectively.

Abstract: Image-based 3D object detection aims to identify and localize objects in 3D space using only RGB images, eliminating the need for expensive depth sensors required by point cloud-based methods. Existing image-based approaches face two critical challenges: methods achieving high accuracy typically require dense 3D supervision, while those operating without such supervision struggle to extract accurate geometry from images alone. In this paper, we present GVSynergy-Det, a novel framework that enhances 3D detection through synergistic Gaussian-Voxel representation learning. Our key insight is that continuous Gaussian and discrete voxel representations capture complementary geometric information: Gaussians excel at modeling fine-grained surface details while voxels provide structured spatial context. We introduce a dual-representation architecture that: 1) adapts generalizable Gaussian Splatting to extract complementary geometric features for detection tasks, and 2) develops a cross-representation enhancement mechanism that enriches voxel features with geometric details from Gaussian fields. Unlike previous methods that either rely on time-consuming per-scene optimization or utilize Gaussian representations solely for depth regularization, our synergistic strategy directly leverages features from both representations through learnable integration, enabling more accurate object localization. Extensive experiments demonstrate that GVSynergy-Det achieves state-of-the-art results on challenging indoor benchmarks, significantly outperforming existing methods on both ScanNetV2 and ARKitScenes datasets, all without requiring any depth or dense 3D geometry supervision (e.g., point clouds or TSDF).

</details>


### [238] [GaussianDWM: 3D Gaussian Driving World Model for Unified Scene Understanding and Multi-Modal Generation](https://arxiv.org/abs/2512.23180)
*Tianchen Deng,Xuefeng Chen,Yi Chen,Qu Chen,Yuyao Xu,Lijin Yang,Le Xu,Yu Zhang,Bo Zhang,Wuxiong Huang,Hesheng Wang*

Main category: cs.CV

TL;DR: The paper introduces a new Driving World Model framework using 3D Gaussian scene representation and achieves state-of-the-art performance for multi-modal tasks.


<details>
  <summary>Details</summary>
Motivation: Existing Driving World Models lack capabilities to interpret, reason about the 3D environment, or align textual information accurately with spatial data.

Method: The framework uses 3D Gaussian primitives for scene understanding and generation, along with a language-guided sampling strategy and a dual-condition multi-modal generation model.

Result: The proposed approach outperforms existing methods on the nuScenes and NuInteract datasets.

Conclusion: The novel framework addresses critical limitations in driving models by advancing 3D scene understanding and multi-modal generation. The code will be publicly available.

Abstract: Driving World Models (DWMs) have been developing rapidly with the advances of generative models. However, existing DWMs lack 3D scene understanding capabilities and can only generate content conditioned on input data, without the ability to interpret or reason about the driving environment. Moreover, current approaches represent 3D spatial information with point cloud or BEV features do not accurately align textual information with the underlying 3D scene. To address these limitations, we propose a novel unified DWM framework based on 3D Gaussian scene representation, which enables both 3D scene understanding and multi-modal scene generation, while also enabling contextual enrichment for understanding and generation tasks. Our approach directly aligns textual information with the 3D scene by embedding rich linguistic features into each Gaussian primitive, thereby achieving early modality alignment. In addition, we design a novel task-aware language-guided sampling strategy that removes redundant 3D Gaussians and injects accurate and compact 3D tokens into LLM. Furthermore, we design a dual-condition multi-modal generation model, where the information captured by our vision-language model is leveraged as a high-level language condition in combination with a low-level image condition, jointly guiding the multi-modal generation process. We conduct comprehensive studies on the nuScenes, and NuInteract datasets to validate the effectiveness of our framework. Our method achieves state-of-the-art performance. We will release the code publicly on GitHub https://github.com/dtc111111/GaussianDWM.

</details>


### [239] [ForCM: Forest Cover Mapping from Multispectral Sentinel-2 Image by Integrating Deep Learning with Object-Based Image Analysis](https://arxiv.org/abs/2512.23196)
*Maisha Haque,Israt Jahan Ayshi,Sadaf M. Anis,Nahian Tasnim,Mithila Moontaha,Md. Sabbir Ahmed,Muhammad Iqbal Hossain,Mohammad Zavid Parvez,Subrata Chakraborty,Biswajeet Pradhan,Biswajit Banik*

Main category: cs.CV

TL;DR: The paper introduces "ForCM," an improved forest cover mapping method by integrating Deep Learning models with OBIA using Sentinel-2 image data, achieving better accuracy compared to traditional techniques.


<details>
  <summary>Details</summary>
Motivation: To enhance the accuracy and efficiency of forest cover mapping by leveraging advanced deep learning models and object-based techniques on accessible, freely available satellite imagery.

Method: ForCM combines OBIA techniques with several Deep Learning models (e.g., UNet, AttentionUNet) applied on multispectral Sentinel-2 imagery to improve mapping precision, and integrates them for evaluation against traditional methods.

Result: The proposed method achieves higher overall accuracies (94.54% with ResUNet-OBIA and 95.64% with AttentionUNet-OBIA) compared to traditional OBIA (92.91%).

Conclusion: ForCM successfully demonstrates the integration of DL models with OBIA techniques can significantly improve forest cover mapping, supporting better environmental monitoring and conservation efforts.

Abstract: This research proposes "ForCM", a novel approach to forest cover mapping that combines Object-Based Image Analysis (OBIA) with Deep Learning (DL) using multispectral Sentinel-2 imagery. The study explores several DL models, including UNet, UNet++, ResUNet, AttentionUNet, and ResNet50-Segnet, applied to high-resolution Sentinel-2 Level 2A satellite images of the Amazon Rainforest. The datasets comprise three collections: two sets of three-band imagery and one set of four-band imagery. After evaluation, the most effective DL models are individually integrated with the OBIA technique to enhance mapping accuracy. The originality of this work lies in evaluating different deep learning models combined with OBIA and comparing them with traditional OBIA methods. The results show that the proposed ForCM method improves forest cover mapping, achieving overall accuracies of 94.54 percent with ResUNet-OBIA and 95.64 percent with AttentionUNet-OBIA, compared to 92.91 percent using traditional OBIA. This research also demonstrates the potential of free and user-friendly tools such as QGIS for accurate mapping within their limitations, supporting global environmental monitoring and conservation efforts.

</details>


### [240] [Exploring Syn-to-Real Domain Adaptation for Military Target Detection](https://arxiv.org/abs/2512.23208)
*Jongoh Jeong,Youngjin Oh,Gyeongrae Nam,Jeongeun Lee,Kuk-Jin Yoon*

Main category: cs.CV

TL;DR: This paper addresses the challenge of detecting military targets using RGB-based synthetic data for cross-domain application, leveraging Unreal Engine to create a realistic dataset.


<details>
  <summary>Details</summary>
Motivation: There's a need for effective object detection methods in varied military environments, as conventional tools like SAR data are costly and RGB cameras offer a low-cost alternative. Existing datasets for military target detection are scarce.

Method: Synthetic RGB datasets were generated using Unreal Engine, followed by synthetic-to-real transfer experiments. State-of-the-art domain adaptive methods were benchmarked using varying degrees of supervision.

Result: Methods with minimal supervision (e.g., object class hints) performed better than unsupervised or semi-supervised approaches, showcasing a substantial improvement.

Conclusion: Despite the improvements with minimal supervision, current domain adaptation methods still face considerable challenges for effective military target detection across variable domains.

Abstract: Object detection is one of the key target tasks of interest in the context of civil and military applications. In particular, the real-world deployment of target detection methods is pivotal in the decision-making process during military command and reconnaissance. However, current domain adaptive object detection algorithms consider adapting one domain to another similar one only within the scope of natural or autonomous driving scenes. Since military domains often deal with a mixed variety of environments, detecting objects from multiple varying target domains poses a greater challenge. Several studies for armored military target detection have made use of synthetic aperture radar (SAR) data due to its robustness to all weather, long range, and high-resolution characteristics. Nevertheless, the costs of SAR data acquisition and processing are still much higher than those of the conventional RGB camera, which is a more affordable alternative with significantly lower data processing time. Furthermore, the lack of military target detection datasets limits the use of such a low-cost approach. To mitigate these issues, we propose to generate RGB-based synthetic data using a photorealistic visual tool, Unreal Engine, for military target detection in a cross-domain setting. To this end, we conducted synthetic-to-real transfer experiments by training our synthetic dataset and validating on our web-collected real military target datasets. We benchmark the state-of-the-art domain adaptation methods distinguished by the degree of supervision on our proposed train-val dataset pair, and find that current methods using minimal hints on the image (e.g., object class) achieve a substantial improvement over unsupervised or semi-supervised DA methods. From these observations, we recognize the current challenges that remain to be overcome.

</details>


### [241] [Task-oriented Learnable Diffusion Timesteps for Universal Few-shot Learning of Dense Tasks](https://arxiv.org/abs/2512.23210)
*Changgyoon Oh,Jongoh Jeong,Jegyeong Cho,Kuk-Jin Yoon*

Main category: cs.CV

TL;DR: This paper improves diffusion models for few-shot dense prediction tasks by introducing adaptive timestep selection and feature consolidation techniques.


<details>
  <summary>Details</summary>
Motivation: To address limitations in heuristic feature selection by diffusion models, aiming for more effective and versatile dense prediction in universal few-shot learning scenarios.

Method: The authors propose two modules: Task-aware Timestep Selection (TTS) selects ideal diffusion timesteps based on losses and similarity scores, and Timestep Feature Consolidation (TFC) consolidates selected features. They also use a parameter-efficient fine-tuning adapter.

Result: Performance improvements are empirically validated on the Taskonomy dataset, showcasing the framework's enhanced ability for dense predictions with few support queries.

Conclusion: The proposed framework demonstrates superiority in dense prediction tasks with adaptive timestep feature utilization, particularly in challenging few-shot learning scenarios.

Abstract: Denoising diffusion probabilistic models have brought tremendous advances in generative tasks, achieving state-of-the-art performance thus far. Current diffusion model-based applications exploit the power of learned visual representations from multistep forward-backward Markovian processes for single-task prediction tasks by attaching a task-specific decoder. However, the heuristic selection of diffusion timestep features still heavily relies on empirical intuition, often leading to sub-optimal performance biased towards certain tasks. To alleviate this constraint, we investigate the significance of versatile diffusion timestep features by adaptively selecting timesteps best suited for the few-shot dense prediction task, evaluated on an arbitrary unseen task. To this end, we propose two modules: Task-aware Timestep Selection (TTS) to select ideal diffusion timesteps based on timestep-wise losses and similarity scores, and Timestep Feature Consolidation (TFC) to consolidate the selected timestep features to improve the dense predictive performance in a few-shot setting. Accompanied by our parameter-efficient fine-tuning adapter, our framework effectively achieves superiority in dense prediction performance given only a few support queries. We empirically validate our learnable timestep consolidation method on the large-scale challenging Taskonomy dataset for dense prediction, particularly for practical universal and few-shot learning scenarios.

</details>


### [242] [AVOID: The Adverse Visual Conditions Dataset with Obstacles for Driving Scene Understanding](https://arxiv.org/abs/2512.23215)
*Jongoh Jeong,Taek-Jin Song,Jong-Hwan Kim,Kuk-Jin Yoon*

Main category: cs.CV

TL;DR: The paper introduces AVOID, a simulated dataset for real-time obstacle detection under adverse road conditions, providing images, depth, semantic maps, and more for diverse vision tasks.


<details>
  <summary>Details</summary>
Motivation: To address the lack of datasets containing unexpected road obstacles under varying adverse conditions in a unified visual domain to improve real-time obstacle detection for self-driving cars.

Method: Develop a dataset (AVOID) using simulation, featuring road obstacles captured in adverse weather and time conditions. The dataset includes multiple data formats (e.g., images, depth maps), enabling benchmarking and multi-task learning.

Result: Benchmark tests and ablation studies were conducted to evaluate real-time obstacle detection and multi-task networks for tasks such as semantic segmentation and waypoint prediction.

Conclusion: AVOID provides a comprehensive resource for studying visual perception tasks in challenging driving scenarios, emphasizing its potential to improve road obstacle detection and related tasks.

Abstract: Understanding road scenes for visual perception remains crucial for intelligent self-driving cars. In particular, it is desirable to detect unexpected small road hazards reliably in real-time, especially under varying adverse conditions (e.g., weather and daylight). However, existing road driving datasets provide large-scale images acquired in either normal or adverse scenarios only, and often do not contain the road obstacles captured in the same visual domain as for the other classes. To address this, we introduce a new dataset called AVOID, the Adverse Visual Conditions Dataset, for real-time obstacle detection collected in a simulated environment. AVOID consists of a large set of unexpected road obstacles located along each path captured under various weather and time conditions. Each image is coupled with the corresponding semantic and depth maps, raw and semantic LiDAR data, and waypoints, thereby supporting most visual perception tasks. We benchmark the results on high-performing real-time networks for the obstacle detection task, and also propose and conduct ablation studies using a comprehensive multi-task network for semantic segmentation, depth and waypoint prediction tasks.

</details>


### [243] [MM-UAVBench: How Well Do Multimodal Large Language Models See, Think, and Plan in Low-Altitude UAV Scenarios?](https://arxiv.org/abs/2512.23219)
*Shiqi Dai,Zizhi Ma,Zhicong Luo,Xuesong Yang,Yibin Huang,Wanyue Zhang,Chi Chen,Zonghao Guo,Wang Xu,Yufei Sun,Maosong Sun*

Main category: cs.CV

TL;DR: The paper introduces MM-UAVBench, a benchmark designed to evaluate Multimodal Large Language Models (MLLMs) in low-altitude Unmanned Aerial Vehicle (UAV) scenarios, exploring their abilities in perception, cognition, and planning.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of comprehensive benchmarks and evaluations for assessing MLLMs' general intelligence specifically in low-altitude UAV scenarios, which are not adequately covered by existing MLLM benchmarks or UAV-related evaluations.

Method: The authors created MM-UAVBench, featuring 19 sub-tasks and over 5.7K annotated questions derived from real-world UAV data. The framework systematically evaluates MLLMs on three core capabilities: perception, cognition, and planning.

Result: Experiments on 16 MLLMs revealed that current models struggle in adapting to low-altitude UAV scenarios, facing challenges like spatial bias and multi-view understanding.

Conclusion: The study concludes that significant improvements are needed for MLLMs to meet the demands of real-world low-altitude UAV applications, with MM-UAVBench serving as a resource to guide future research.

Abstract: While Multimodal Large Language Models (MLLMs) have exhibited remarkable general intelligence across diverse domains, their potential in low-altitude applications dominated by Unmanned Aerial Vehicles (UAVs) remains largely underexplored. Existing MLLM benchmarks rarely cover the unique challenges of low-altitude scenarios, while UAV-related evaluations mainly focus on specific tasks such as localization or navigation, without a unified evaluation of MLLMs'general intelligence. To bridge this gap, we present MM-UAVBench, a comprehensive benchmark that systematically evaluates MLLMs across three core capability dimensions-perception, cognition, and planning-in low-altitude UAV scenarios. MM-UAVBench comprises 19 sub-tasks with over 5.7K manually annotated questions, all derived from real-world UAV data collected from public datasets. Extensive experiments on 16 open-source and proprietary MLLMs reveal that current models struggle to adapt to the complex visual and cognitive demands of low-altitude scenarios. Our analyses further uncover critical bottlenecks such as spatial bias and multi-view understanding that hinder the effective deployment of MLLMs in UAV scenarios. We hope MM-UAVBench will foster future research on robust and reliable MLLMs for real-world UAV intelligence.

</details>


### [244] [Holi-DETR: Holistic Fashion Item Detection Leveraging Contextual Information](https://arxiv.org/abs/2512.23221)
*Youngchae Kwon,Jinyoung Choi,Injung Kim*

Main category: cs.CV

TL;DR: This paper proposes Holi-DETR, a model that enhances fashion item detection by leveraging three types of contextual information, improving detection performance significantly.


<details>
  <summary>Details</summary>
Motivation: The detection of fashion items in images is challenging because of the diverse appearances of items and similar item subcategories, which creates ambiguities.

Method: The researchers developed Holi-DETR, which incorporates three types of contextual information: item co-occurrence probability, inter-item spatial arrangements, and spatial relationships with human body key-points, into the Detection Transformer (DETR).

Result: Holi-DETR improved detection accuracy, outperforming vanilla DETR by 3.6pp and Co-DETR by 1.1pp in average precision (AP).

Conclusion: Integrating contextual information into fashion item detection models can significantly enhance detection accuracy and reduce ambiguities on challenging datasets.

Abstract: Fashion item detection is challenging due to the ambiguities introduced by the highly diverse appearances of fashion items and the similarities among item subcategories. To address this challenge, we propose a novel Holistic Detection Transformer (Holi-DETR) that detects fashion items in outfit images holistically, by leveraging contextual information. Fashion items often have meaningful relationships as they are combined to create specific styles. Unlike conventional detectors that detect each item independently, Holi-DETR detects multiple items while reducing ambiguities by leveraging three distinct types of contextual information: (1) the co-occurrence relationship between fashion items, (2) the relative position and size based on inter-item spatial arrangements, and (3) the spatial relationships between items and human body key-points. %Holi-DETR explicitly incorporates three types of contextual information: (1) the co-occurrence probability between fashion items, (2) the relative position and size based on inter-item spatial arrangements, and (3) the spatial relationships between items and human body key-points. To this end, we propose a novel architecture that integrates these three types of heterogeneous contextual information into the Detection Transformer (DETR) and its subsequent models. In experiments, the proposed methods improved the performance of the vanilla DETR and the more recently developed Co-DETR by 3.6 percent points (pp) and 1.1 pp, respectively, in terms of average precision (AP).

</details>


### [245] [Bridging Your Imagination with Audio-Video Generation via a Unified Director](https://arxiv.org/abs/2512.23222)
*Jiaxu Zhang,Tianshu Hu,Yuan Zhang,Zenan Li,Linjie Luo,Guosheng Lin,Xin Chen*

Main category: cs.CV

TL;DR: Unified AI model integrates script drafting and key-shot design for non-experts to create multi-shot films using audio-video generation.


<details>
  <summary>Details</summary>
Motivation: Current AI systems separate script drafting and image generation, lacking cohesion similar to a film director's qualities.

Method: Introduced UniMAGE, using Mixture-of-Transformers and 'interleaving-disentangling' training to unify concept learning and decouple tasks.

Result: UniMAGE improves narrative logic and keyframe consistency while achieving state-of-the-art performance among open-source models.

Conclusion: UniMAGE effectively enables non-experts to produce coherent multi-shot films via unified script and keyframe generation.

Abstract: Existing AI-driven video creation systems typically treat script drafting and key-shot design as two disjoint tasks: the former relies on large language models, while the latter depends on image generation models. We argue that these two tasks should be unified within a single framework, as logical reasoning and imaginative thinking are both fundamental qualities of a film director. In this work, we propose UniMAGE, a unified director model that bridges user prompts with well-structured scripts, thereby empowering non-experts to produce long-context, multi-shot films by leveraging existing audio-video generation models. To achieve this, we employ the Mixture-of-Transformers architecture that unifies text and image generation. To further enhance narrative logic and keyframe consistency, we introduce a ``first interleaving, then disentangling'' training paradigm. Specifically, we first perform Interleaved Concept Learning, which utilizes interleaved text-image data to foster the model's deeper understanding and imaginative interpretation of scripts. We then conduct Disentangled Expert Learning, which decouples script writing from keyframe generation, enabling greater flexibility and creativity in storytelling. Extensive experiments demonstrate that UniMAGE achieves state-of-the-art performance among open-source models, generating logically coherent video scripts and visually consistent keyframe images.

</details>


### [246] [Anomaly Detection by Effectively Leveraging Synthetic Images](https://arxiv.org/abs/2512.23227)
*Sungho Kang,Hyunkyu Park,Yeonho Lee,Hanbyul Lee,Mijoo Jeong,YeongHyeon Park,Injae Lee,Juneho Yi*

Main category: cs.CV

TL;DR: The paper proposes an efficient framework for anomaly detection in industrial manufacturing by leveraging synthetic defect images and introducing a two-stage training strategy.


<details>
  <summary>Details</summary>
Motivation: Address the limitation of real defect image scarcity and the trade-off between cost-effective rule-based synthesis and high-quality, but costly, generative model-based synthesis strategies for anomaly detection.

Method: The authors use a pre-trained text-guided image-to-image translation model combined with an image retrieval model to generate high-quality synthetic defect images. A two-stage training strategy is introduced, pre-training the model on a large rule-based image set, followed by fine-tuning on fewer high-quality generated images.

Result: The proposed framework achieves improved anomaly detection performance with reduced data collection cost when evaluated on the MVTec AD dataset.

Conclusion: The framework effectively balances cost and image quality, offering a practical solution for synthetic data generation and enhancing anomaly detection in industrial settings.

Abstract: Anomaly detection plays a vital role in industrial manufacturing. Due to the scarcity of real defect images, unsupervised approaches that rely solely on normal images have been extensively studied. Recently, diffusion-based generative models brought attention to training data synthesis as an alternative solution. In this work, we focus on a strategy to effectively leverage synthetic images to maximize the anomaly detection performance. Previous synthesis strategies are broadly categorized into two groups, presenting a clear trade-off. Rule-based synthesis, such as injecting noise or pasting patches, is cost-effective but often fails to produce realistic defect images. On the other hand, generative model-based synthesis can create high-quality defect images but requires substantial cost. To address this problem, we propose a novel framework that leverages a pre-trained text-guided image-to-image translation model and image retrieval model to efficiently generate synthetic defect images. Specifically, the image retrieval model assesses the similarity of the generated images to real normal images and filters out irrelevant outputs, thereby enhancing the quality and relevance of the generated defect images. To effectively leverage synthetic images, we also introduce a two stage training strategy. In this strategy, the model is first pre-trained on a large volume of images from rule-based synthesis and then fine-tuned on a smaller set of high-quality images. This method significantly reduces the cost for data collection while improving the anomaly detection performance. Experiments on the MVTec AD dataset demonstrate the effectiveness of our approach.

</details>


### [247] [SURE Guided Posterior Sampling: Trajectory Correction for Diffusion-Based Inverse Problems](https://arxiv.org/abs/2512.23232)
*Minwoo Kim,Hongki Lim*

Main category: cs.CV

TL;DR: Diffusion models are typically slow and error-prone for inverse problems; SGPS improves sampling quality using SURE gradient updates and PCA noise estimation, achieving high reconstruction with fewer steps.


<details>
  <summary>Details</summary>
Motivation: Current iterative approaches for solving inverse problems with diffusion models require many steps due to accumulated errors, making them inefficient.

Method: SGPS (SURE Guided Posterior Sampling) uses Stein's Unbiased Risk Estimate for gradient updates and PCA based noise estimation to correct deviations in sampling trajectory.

Result: SGPS enables more accurate reconstructions in fewer than 100 Neural Function Evaluations (NFEs), demonstrating superior performance compared to existing methods.

Conclusion: SGPS delivers efficient and high-quality inverse problem reconstruction, addressing noise-induced errors to outperform current techniques with lower computing demands.

Abstract: Diffusion models have emerged as powerful learned priors for solving inverse problems. However, current iterative solving approaches which alternate between diffusion sampling and data consistency steps typically require hundreds or thousands of steps to achieve high quality reconstruction due to accumulated errors. We address this challenge with SURE Guided Posterior Sampling (SGPS), a method that corrects sampling trajectory deviations using Stein's Unbiased Risk Estimate (SURE) gradient updates and PCA based noise estimation. By mitigating noise induced errors during the critical early and middle sampling stages, SGPS enables more accurate posterior sampling and reduces error accumulation. This allows our method to maintain high reconstruction quality with fewer than 100 Neural Function Evaluations (NFEs). Our extensive evaluation across diverse inverse problems demonstrates that SGPS consistently outperforms existing methods at low NFE counts.

</details>


### [248] [Physics-Inspired Modeling and Content Adaptive Routing in an Infrared Gas Leak Detection Network](https://arxiv.org/abs/2512.23234)
*Dongsheng Li,Chaobo Chen,Siling Wang,Song Gao*

Main category: cs.CV

TL;DR: The paper introduces PEG-DRNet, a novel architecture for detecting faint infrared gas leaks with enhanced boundary detection, achieving superior accuracy and efficiency compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of detecting faint, semi-transparent, and weakly bounded infrared gas plumes, which is crucial for environmental and industrial safety.

Method: The proposed PEG-DRNet includes three key innovations: the Gas Block for modeling gas transport, the adaptive gradient and phase edge operator (AGPEO) for edge extraction, and the content-adaptive sparse routing path aggregation network (CASR-PAN) for feature propagation and fusion. These components improve boundary detection and cross-scale feature discrimination.

Result: PEG-DRNet demonstrated superior performance with an AP of 29.8\%, AP$_{50}$ of 84.3\%, and a small-object AP of 25.3\%, outperforming the baseline RT-DETR-R18 on the IIG dataset while maintaining computational efficiency (43.7 Gflops, 14.9 M parameters).

Conclusion: PEG-DRNet is highly effective in detecting gas leaks, offering an optimal trade-off between accuracy and computational efficiency, making it a significant advancement over existing CNN and Transformer-based methods.

Abstract: Detecting infrared gas leaks is critical for environmental monitoring and industrial safety, yet remains difficult because plumes are faint, small, semitransparent, and have weak, diffuse boundaries. We present physics-edge hybrid gas dynamic routing network (PEG-DRNet). First, we introduce the Gas Block, a diffusion-convection unit modeling gas transport: a local branch captures short-range variations, while a large-kernel branch captures long-range propagation. An edge-gated learnable fusion module balances local detail and global context, strengthening weak-contrast plume and contour cues. Second, we propose the adaptive gradient and phase edge operator (AGPEO), computing reliable edge priors from multi-directional gradients and phase-consistent responses. These are transformed by a multi-scale edge perception module (MSEPM) into hierarchical edge features that reinforce boundaries. Finally, the content-adaptive sparse routing path aggregation network (CASR-PAN), with adaptive information modulation modules for fusion and self, selectively propagates informative features across scales based on edge and content cues, improving cross-scale discriminability while reducing redundancy. Experiments on the IIG dataset show that PEG-DRNet achieves an overall AP of 29.8\%, an AP$_{50}$ of 84.3\%, and a small-object AP of 25.3\%, surpassing the RT-DETR-R18 baseline by 3.0\%, 6.5\%, and 5.3\%, respectively, while requiring only 43.7 Gflops and 14.9 M parameters. The proposed PEG-DRNet achieves superior overall performance with the best balance of accuracy and computational efficiency, outperforming existing CNN and Transformer detectors in AP and AP$_{50}$ on the IIG and LangGas dataset.

</details>


### [249] [RS-Prune: Training-Free Data Pruning at High Ratios for Efficient Remote Sensing Diffusion Foundation Models](https://arxiv.org/abs/2512.23239)
*Fan Wei,Runmin Dong,Yushan Lai,Yixiang Yang,Zhaoyang Luo,Jinxiao Zhang,Miao Yang,Shuai Yuan,Jiyao Zhao,Bin Luo,Haohuan Fu*

Main category: cs.CV

TL;DR: The study addresses inefficiencies in training diffusion-based remote sensing generative foundation models by proposing a novel, training-free, two-stage data pruning approach that enhances data quality while maintaining overall diversity.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges like redundancy, noise, and class imbalance in training data for remote sensing generative foundation models, which reduce training efficiency and convergence.

Method: A two-stage training-free data pruning method: 1) Entropy-based criterion removes low-information samples, and 2) Scene-aware clustering with stratified sampling ensures global diversity and uniformity while improving computational efficiency.

Result: The proposed pruning method allows for reducing up to 85% of training data while improving training convergence, generation quality, and downstream task performance.

Conclusion: This paradigm establishes practical guidance for efficiently pruning data, leading to superior generative foundation models for remote sensing tasks.

Abstract: Diffusion-based remote sensing (RS) generative foundation models are cruial for downstream tasks. However, these models rely on large amounts of globally representative data, which often contain redundancy, noise, and class imbalance, reducing training efficiency and preventing convergence. Existing RS diffusion foundation models typically aggregate multiple classification datasets or apply simplistic deduplication, overlooking the distributional requirements of generation modeling and the heterogeneity of RS imagery. To address these limitations, we propose a training-free, two-stage data pruning approach that quickly select a high-quality subset under high pruning ratios, enabling a preliminary foundation model to converge rapidly and serve as a versatile backbone for generation, downstream fine-tuning, and other applications. Our method jointly considers local information content with global scene-level diversity and representativeness. First, an entropy-based criterion efficiently removes low-information samples. Next, leveraging RS scene classification datasets as reference benchmarks, we perform scene-aware clustering with stratified sampling to improve clustering effectiveness while reducing computational costs on large-scale unlabeled data. Finally, by balancing cluster-level uniformity and sample representativeness, the method enables fine-grained selection under high pruning ratios while preserving overall diversity and representativeness. Experiments show that, even after pruning 85\% of the training data, our method significantly improves convergence and generation quality. Furthermore, diffusion foundation models trained with our method consistently achieve state-of-the-art performance across downstream tasks, including super-resolution and semantic image synthesis. This data pruning paradigm offers practical guidance for developing RS generative foundation models.

</details>


### [250] [Multimodal Interpretation of Remote Sensing Images: Dynamic Resolution Input Strategy and Multi-scale Vision-Language Alignment Mechanism](https://arxiv.org/abs/2512.23243)
*Siyu Zhang,Ying Chen,Lianlei Shan,Runhe Qiu*

Main category: cs.CV

TL;DR: This study introduces a Vision-language Model framework with innovations to improve accuracy and efficiency in remote sensing multimodal fusion tasks.


<details>
  <summary>Details</summary>
Motivation: To tackle deficiencies in current methods like fixed resolutions and lack of semantic hierarchy in single-scale alignment.

Method: Proposed a Vision-language Model integrated with Dynamic Resolution Input Strategy and Multi-scale Vision-language Alignment Mechanism.

Result: Improved semantic understanding accuracy and computational efficiency in multimodal tasks, evidenced by superior BLEU-4, CIDEr, and R@10 performance metrics.

Conclusion: The framework advances multimodal remote sensing systems, supporting intelligent interpretation applications.

Abstract: Multimodal fusion of remote sensing images serves as a core technology for overcoming the limitations of single-source data and improving the accuracy of surface information extraction, which exhibits significant application value in fields such as environmental monitoring and urban planning. To address the deficiencies of existing methods, including the failure of fixed resolutions to balance efficiency and detail, as well as the lack of semantic hierarchy in single-scale alignment, this study proposes a Vision-language Model (VLM) framework integrated with two key innovations: the Dynamic Resolution Input Strategy (DRIS) and the Multi-scale Vision-language Alignment Mechanism (MS-VLAM).Specifically, the DRIS adopts a coarse-to-fine approach to adaptively allocate computational resources according to the complexity of image content, thereby preserving key fine-grained features while reducing redundant computational overhead. The MS-VLAM constructs a three-tier alignment mechanism covering object, local-region and global levels, which systematically captures cross-modal semantic consistency and alleviates issues of semantic misalignment and granularity imbalance.Experimental results on the RS-GPT4V dataset demonstrate that the proposed framework significantly improves the accuracy of semantic understanding and computational efficiency in tasks including image captioning and cross-modal retrieval. Compared with conventional methods, it achieves superior performance in evaluation metrics such as BLEU-4 and CIDEr for image captioning, as well as R@10 for cross-modal retrieval. This technical framework provides a novel approach for constructing efficient and robust multimodal remote sensing systems, laying a theoretical foundation and offering technical guidance for the engineering application of intelligent remote sensing interpretation.

</details>


### [251] [ViLaCD-R1: A Vision-Language Framework for Semantic Change Detection in Remote Sensing](https://arxiv.org/abs/2512.23244)
*Xingwei Ma,Shiyang Feng,Bo Zhang,Bin Wang*

Main category: cs.CV

TL;DR: This paper proposes ViLaCD-R1, a framework for remote sensing change detection (RSCD), which addresses limitations of previous methods by enhancing semantic understanding and spatial localization.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome challenges in traditional RSCD methods and vision-language model-based approaches, such as poor semantic capture, inaccurate spatial localization, and difficulty in boundary delineation.

Method: The proposed ViLaCD-R1 framework introduces a Multi-Image Reasoner (MIR) trained using supervised fine-tuning and reinforcement learning to generate coarse change masks and a Mask-Guided Decoder (MGD) for precise binary change maps.

Result: Comprehensive evaluations show that ViLaCD-R1 achieves state-of-the-art accuracy in semantic change recognition, improves localization, and handles non-semantic variations in RSCD tasks.

Conclusion: ViLaCD-R1 is an effective tool for RSCD, improving semantic interpretation and spatial precision while outperforming previous methods in real-world scenarios.

Abstract: Remote sensing change detection (RSCD), a complex multi-image inference task, traditionally uses pixel-based operators or encoder-decoder networks that inadequately capture high-level semantics and are vulnerable to non-semantic perturbations. Although recent multimodal and vision-language model (VLM)-based approaches enhance semantic understanding of change regions by incorporating textual descriptions, they still suffer from challenges such as inaccurate spatial localization, imprecise pixel-level boundary delineation, and limited interpretability. To address these issues, we propose ViLaCD-R1, a two-stage framework comprising a Multi-Image Reasoner (MIR) and a Mask-Guided Decoder (MGD). Specifically, the VLM is trained through supervised fine-tuning (SFT) and reinforcement learning (RL) on block-level dual-temporal inference tasks, taking dual-temporal image patches as input and outputting a coarse change mask. Then, the decoder integrates dual-temporal image features with this coarse mask to predict a precise binary change map. Comprehensive evaluations on multiple RSCD benchmarks demonstrate that ViLaCD-R1 substantially improves true semantic change recognition and localization, robustly suppresses non-semantic variations, and achieves state-of-the-art accuracy in complex real-world scenarios.

</details>


### [252] [ASemConsist: Adaptive Semantic Feature Control for Training-Free Identity-Consistent Generation](https://arxiv.org/abs/2512.23245)
*Shin seong Kim,Minjung Shin,Hyunin Cho,Youngjung Uh*

Main category: cs.CV

TL;DR: ASemconsist framework addresses identity consistency for text-to-image diffusion models by introducing selective text embedding modification and novel evaluation metric CQS.


<details>
  <summary>Details</summary>
Motivation: To solve the challenge of maintaining consistent character identity across a sequence of images while ensuring alignment with text prompts.

Method: Introduced selective text embedding modification, semantic control via repurposed padding embeddings, and an adaptive feature-sharing strategy. Proposed a novel evaluation protocol, Consistency Quality Score (CQS).

Result: ASemconsist achieves state-of-the-art performance in balancing identity consistency and per-image text alignment.

Conclusion: ASemconsist provides an effective solution to the trade-off problem between identity preservation and text alignment for text-to-image generation models.

Abstract: Recent text-to-image diffusion models have significantly improved visual quality and text alignment. However, generating a sequence of images while preserving consistent character identity across diverse scene descriptions remains a challenging task. Existing methods often struggle with a trade-off between maintaining identity consistency and ensuring per-image prompt alignment. In this paper, we introduce a novel framework, ASemconsist, that addresses this challenge through selective text embedding modification, enabling explicit semantic control over character identity without sacrificing prompt alignment. Furthermore, based on our analysis of padding embeddings in FLUX, we propose a semantic control strategy that repurposes padding embeddings as semantic containers. Additionally, we introduce an adaptive feature-sharing strategy that automatically evaluates textual ambiguity and applies constraints only to the ambiguous identity prompt. Finally, we propose a unified evaluation protocol, the Consistency Quality Score (CQS), which integrates identity preservation and per-image text alignment into a single comprehensive metric, explicitly capturing performance imbalances between the two metrics. Our framework achieves state-of-the-art performance, effectively overcoming prior trade-offs. Project page: https://minjung-s.github.io/asemconsist

</details>


### [253] [Contour Information Aware 2D Gaussian Splatting for Image Representation](https://arxiv.org/abs/2512.23255)
*Masaya Takabe,Hiroshi Watanabe,Sujun Hong,Tomohiro Ikai,Zheming Fan,Ryo Ishimoto,Kakeru Sugimoto,Ruri Imichi*

Main category: cs.CV

TL;DR: This paper proposes a Contour Information-Aware 2D Gaussian Splatting framework to enhance image representation by incorporating object segmentation priors, improving edge quality in compressed representations.


<details>
  <summary>Details</summary>
Motivation: The need for efficient and expressive image representation frameworks while addressing the limitations of blurry boundaries in recent 2D Gaussian Splatting methods.

Method: Integrating contour awareness into the Gaussian representation by constraining rasterization within segmentation regions and introducing a warm-up scheme for optimized training.

Result: Improved reconstruction quality, particularly around edges, in high-compression scenarios with very few Gaussians, alongside fast rendering and low memory usage.

Conclusion: The framework enhances the expressiveness and edge preservation in image modeling, especially when Gaussians are limited, without compromising computational efficiency.

Abstract: Image representation is a fundamental task in computer vision. Recently, Gaussian Splatting has emerged as an efficient representation framework, and its extension to 2D image representation enables lightweight, yet expressive modeling of visual content. While recent 2D Gaussian Splatting (2DGS) approaches provide compact storage and real-time decoding, they often produce blurry or indistinct boundaries when the number of Gaussians is small due to the lack of contour awareness. In this work, we propose a Contour Information-Aware 2D Gaussian Splatting framework that incorporates object segmentation priors into Gaussian-based image representation. By constraining each Gaussian to a specific segmentation region during rasterization, our method prevents cross-boundary blending and preserves edge structures under high compression. We also introduce a warm-up scheme to stabilize training and improve convergence. Experiments on synthetic color charts and the DAVIS dataset demonstrate that our approach achieves higher reconstruction quality around object edges compared to existing 2DGS methods. The improvement is particularly evident in scenarios with very few Gaussians, while our method still maintains fast rendering and low memory usage.

</details>


### [254] [Plug-and-Play Fidelity Optimization for Diffusion Transformer Acceleration via Cumulative Error Minimization](https://arxiv.org/abs/2512.23258)
*Tong Shao,Yusen Fu,Guoying Sun,Jingde Kong,Zhuotao Tian,Jingyong Su*

Main category: cs.CV

TL;DR: The paper addresses the slow inference issue in Diffusion Transformers by introducing CEM, a fidelity-optimization plugin that minimizes caching error for improved generation fidelity.


<details>
  <summary>Details</summary>
Motivation: The motivation arises from the slow inference process in Diffusion Transformer architectures due to iterative denoising, which limits their applicability. Existing caching-based methods provide acceleration but introduce computational errors, and fixed strategies fail to adapt to complex error variations.

Method: The authors developed CEM (Cumulative Error Minimization), which uses predefined errors to guide a dynamic programming algorithm. This strategy adapts to error variations by minimizing cumulative caching error for enhanced generation fidelity, without adding computational overhead.

Result: CEM demonstrates significant improvements in generation fidelity across nine generative models and three tasks, outperforming state-of-the-art performance on FLUX.1-dev, PixArt-α, StableDiffusion1.5, and Hunyuan.

Conclusion: CEM is a general, model-agnostic plugin that improves error correction and generation fidelity for accelerated Diffusion Transformers, scalable to various models and acceleration requirements, and can integrate seamlessly into existing methods.

Abstract: Although Diffusion Transformer (DiT) has emerged as a predominant architecture for image and video generation, its iterative denoising process results in slow inference, which hinders broader applicability and development. Caching-based methods achieve training-free acceleration, while suffering from considerable computational error. Existing methods typically incorporate error correction strategies such as pruning or prediction to mitigate it. However, their fixed caching strategy fails to adapt to the complex error variations during denoising, which limits the full potential of error correction. To tackle this challenge, we propose a novel fidelity-optimization plugin for existing error correction methods via cumulative error minimization, named CEM. CEM predefines the error to characterize the sensitivity of model to acceleration jointly influenced by timesteps and cache intervals. Guided by this prior, we formulate a dynamic programming algorithm with cumulative error approximation for strategy optimization, which achieves the caching error minimization, resulting in a substantial improvement in generation fidelity. CEM is model-agnostic and exhibits strong generalization, which is adaptable to arbitrary acceleration budgets. It can be seamlessly integrated into existing error correction frameworks and quantized models without introducing any additional computational overhead. Extensive experiments conducted on nine generation models and quantized methods across three tasks demonstrate that CEM significantly improves generation fidelity of existing acceleration models, and outperforms the original generation performance on FLUX.1-dev, PixArt-$α$, StableDiffusion1.5 and Hunyuan. The code will be made publicly available.

</details>


### [255] [YOLO-Master: MOE-Accelerated with Specialized Transformers for Enhanced Real-time Detection](https://arxiv.org/abs/2512.23273)
*Xu Lin,Jinlong Peng,Zhenye Gan,Jiawen Zhu,Jun Liu*

Main category: cs.CV

TL;DR: YOLO-Master introduces an adaptive YOLO-like framework for real-time object detection, offering dynamic computation based on scene complexity, which enhances efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency and suboptimal detection performance in existing YOLO-like models caused by static, dense computation that applies uniform processing to all inputs regardless of scene complexity.

Method: The paper proposes YOLO-Master with an Efficient Sparse Mixture-of-Experts (ES-MoE) block for dynamic allocation of computational resources based on scene complexity. This includes a lightweight dynamic routing network guiding expert specialization and adaptive expert activation to minimize computational overhead.

Result: YOLO-Master achieves a significant performance improvement, with 42.4% AP on MS COCO, surpassing YOLOv13-N by +0.8% mAP, and operates 17.8% faster, especially excelling in complex scenes while maintaining efficiency on simpler ones.

Conclusion: YOLO-Master effectively optimizes computational resources via instance-conditional dynamic computation, leading to superior performance and real-time capabilities, making it a competitive solution for real-time object detection.

Abstract: Existing Real-Time Object Detection (RTOD) methods commonly adopt YOLO-like architectures for their favorable trade-off between accuracy and speed. However, these models rely on static dense computation that applies uniform processing to all inputs, misallocating representational capacity and computational resources such as over-allocating on trivial scenes while under-serving complex ones. This mismatch results in both computational redundancy and suboptimal detection performance. To overcome this limitation, we propose YOLO-Master, a novel YOLO-like framework that introduces instance-conditional adaptive computation for RTOD. This is achieved through a Efficient Sparse Mixture-of-Experts (ES-MoE) block that dynamically allocates computational resources to each input according to its scene complexity. At its core, a lightweight dynamic routing network guides expert specialization during training through a diversity enhancing objective, encouraging complementary expertise among experts. Additionally, the routing network adaptively learns to activate only the most relevant experts, thereby improving detection performance while minimizing computational overhead during inference. Comprehensive experiments on five large-scale benchmarks demonstrate the superiority of YOLO-Master. On MS COCO, our model achieves 42.4% AP with 1.62ms latency, outperforming YOLOv13-N by +0.8% mAP and 17.8% faster inference. Notably, the gains are most pronounced on challenging dense scenes, while the model preserves efficiency on typical inputs and maintains real-time inference speed. Code will be available.

</details>


### [256] [Multi-Track Multimodal Learning on iMiGUE: Micro-Gesture and Emotion Recognition](https://arxiv.org/abs/2512.23291)
*Arman Martirosyan,Shahane Tigranyan,Maria Razzhivina,Artak Aslanyan,Nazgul Salikhova,Ilya Makarov,Andrey Savchenko,Aram Avetisyan*

Main category: cs.CV

TL;DR: This paper introduces two multimodal frameworks for micro-gesture classification and behavior-based emotion prediction using video and skeletal pose data, demonstrating high accuracy on the iMiGUE dataset.


<details>
  <summary>Details</summary>
Motivation: To address the challenging tasks of understanding fine-grained human behaviors for micro-gesture recognition and emotion prediction.

Method: Utilizing video and skeletal embeddings extracted by advanced models (MViTv2-S, 2s-AGCN, SwinFace), combined with fusion modules (Cross-Modal Token Fusion and InterFusion) for spatial, pose, and emotional analysis.

Result: The proposed frameworks achieved robust performance and secured 2nd place on emotion prediction tasks in the MiGA 2025 Challenge using the iMiGUE dataset.

Conclusion: This work showcases effective multimodal learning strategies to advance gesture recognition and emotion prediction, leveraging complementary data modalities and fusion techniques.

Abstract: Micro-gesture recognition and behavior-based emotion prediction are both highly challenging tasks that require modeling subtle, fine-grained human behaviors, primarily leveraging video and skeletal pose data. In this work, we present two multimodal frameworks designed to tackle both problems on the iMiGUE dataset. For micro-gesture classification, we explore the complementary strengths of RGB and 3D pose-based representations to capture nuanced spatio-temporal patterns. To comprehensively represent gestures, video, and skeletal embeddings are extracted using MViTv2-S and 2s-AGCN, respectively. Then, they are integrated through a Cross-Modal Token Fusion module to combine spatial and pose information. For emotion recognition, our framework extends to behavior-based emotion prediction, a binary classification task identifying emotional states based on visual cues. We leverage facial and contextual embeddings extracted using SwinFace and MViTv2-S models and fuse them through an InterFusion module designed to capture emotional expressions and body gestures. Experiments conducted on the iMiGUE dataset, within the scope of the MiGA 2025 Challenge, demonstrate the robust performance and accuracy of our method in the behavior-based emotion prediction task, where our approach secured 2nd place.

</details>


### [257] [MedGemma vs GPT-4: Open-Source and Proprietary Zero-shot Medical Disease Classification from Images](https://arxiv.org/abs/2512.23304)
*Md. Sazzadul Islam Prottasha,Nabil Walid Rafi*

Main category: cs.CV

TL;DR: MedGemma, a fine-tuned model, outperforms GPT-4 in medical imaging diagnostics, achieving higher accuracy and sensitivity for disease detection.


<details>
  <summary>Details</summary>
Motivation: To assess the performance of specialized versus generalized multimodal AI models in accurately interpreting medical scans for disease classification.

Method: Comparison of MedGemma and GPT-4 using diagnostic accuracy tests, fine-tuning techniques (LoRA for MedGemma), and quantitative data analysis with confusion matrices and classification reports.

Result: MedGemma achieved a mean test accuracy of 80.37%, demonstrating higher performance and sensitivity, especially in critical tasks like cancer and pneumonia detection, compared to GPT-4's accuracy of 69.58%.

Conclusion: Domain-specific fine-tuning significantly enhances diagnostic reliability, reducing hallucinations, and making MedGemma an effective tool for evidence-based medical reasoning.

Abstract: Multimodal Large Language Models (LLMs) introduce an emerging paradigm for medical imaging by interpreting scans through the lens of extensive clinical knowledge, offering a transformative approach to disease classification. This study presents a critical comparison between two fundamentally different AI architectures: the specialized open-source agent MedGemma and the proprietary large multimodal model GPT-4 for diagnosing six different diseases. The MedGemma-4b-it model, fine-tuned using Low-Rank Adaptation (LoRA), demonstrated superior diagnostic capability by achieving a mean test accuracy of 80.37% compared to 69.58% for the untuned GPT-4. Furthermore, MedGemma exhibited notably higher sensitivity in high-stakes clinical tasks, such as cancer and pneumonia detection. Quantitative analysis via confusion matrices and classification reports provides comprehensive insights into model performance across all categories. These results emphasize that domain-specific fine-tuning is essential for minimizing hallucinations in clinical implementation, positioning MedGemma as a sophisticated tool for complex, evidence-based medical reasoning.

</details>


### [258] [CME-CAD: Heterogeneous Collaborative Multi-Expert Reinforcement Learning for CAD Code Generation](https://arxiv.org/abs/2512.23333)
*Ke Niu,Haiyang Yu,Zhuofan Chen,Zhengtao Yao,Weitao Jia,Xiaodong Ge,Jingqun Tang,Benlei Cui,Bin Li,Xiangyang Xue*

Main category: cs.CV

TL;DR: This paper proposes CME-CAD, a new training paradigm for generating accurate and editable CAD models, solving existing issues in 3D reconstruction from sketches.


<details>
  <summary>Details</summary>
Motivation: Traditional CAD workflows struggle with precision and editability and require significant manual effort. Automating precision CAD modeling is crucial in industry.

Method: The paper introduces CME-CAD, a collaborative multi-expert reinforcement learning paradigm, with two training stages: Multi-Expert Fine-Tuning and Multi-Expert Reinforcement Learning.

Result: Alongside the methodology, the paper includes the creation of CADExpert, a benchmark with 17,299 annotated instances, enabling constraint-compatible modeling.

Conclusion: The proposed framework facilitates the generation of high-precision, editable CAD models, overcoming workflow limitations and ensuring scalability in industrial design.

Abstract: Computer-Aided Design (CAD) is essential in industrial design, but the complexity of traditional CAD modeling and workflows presents significant challenges for automating the generation of high-precision, editable CAD models. Existing methods that reconstruct 3D models from sketches often produce non-editable and approximate models that fall short of meeting the stringent requirements for precision and editability in industrial design. Moreover, the reliance on text or image-based inputs often requires significant manual annotation, limiting their scalability and applicability in industrial settings. To overcome these challenges, we propose the Heterogeneous Collaborative Multi-Expert Reinforcement Learning (CME-CAD) paradigm, a novel training paradigm for CAD code generation. Our approach integrates the complementary strengths of these models, facilitating collaborative learning and improving the model's ability to generate accurate, constraint-compatible, and fully editable CAD models. We introduce a two-stage training process: Multi-Expert Fine-Tuning (MEFT), and Multi-Expert Reinforcement Learning (MERL). Additionally, we present CADExpert, an open-source benchmark consisting of 17,299 instances, including orthographic projections with precise dimension annotations, expert-generated Chain-of-Thought (CoT) processes, executable CADQuery code, and rendered 3D models.

</details>


### [259] [Visual Language Hypothesis](https://arxiv.org/abs/2512.23335)
*Xiu Li*

Main category: cs.CV

TL;DR: The paper explores visual representation learning from a topological perspective, proposing that semantic structure requires a fiber bundle organization and discussing the implications for model architectures.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the foundational principles of visual representation learning through the lens of semantics and topology, focusing on how discrete semantic states arise from visual observations.

Method: The authors theorize that visual observations align with a fiber bundle structure, with nuisance variations forming fibers and semantics forming a quotient base space. They provide theoretical reasoning to explore these ideas, focusing on structural and topological implications.

Result: Two key results were derived: (1) semantic invariance requires non-homeomorphic targets (e.g., supervision through labels, multimodal alignment), and (2) model architectures must support topology changes for semantic abstraction, involving an "expand-and-snap" mechanism.

Conclusion: The framework reveals fundamental topological constraints in visual representation learning, aligning with observed empirical patterns and principles in statistical learning theory.

Abstract: We study visual representation learning from a structural and topological perspective. We begin from a single hypothesis: that visual understanding presupposes a semantic language for vision, in which many perceptual observations correspond to a small number of discrete semantic states. Together with widely assumed premises on transferability and abstraction in representation learning, this hypothesis implies that the visual observation space must be organized in a fiber bundle like structure, where nuisance variation populates fibers and semantics correspond to a quotient base space. From this structure we derive two theoretical consequences. First, the semantic quotient $X/G$ is not a submanifold of $X$ and cannot be obtained through smooth deformation alone, semantic invariance requires a non-homeomorphic, discriminative target, for example, supervision via labels, cross instance identification, or multimodal alignment that supplies explicit semantic equivalence. Second, we show that approximating the quotient also places structural demands on the model architecture. Semantic abstraction requires not only an external semantic target, but a representation mechanism capable of supporting topology change: an expand-and-snap process in which the manifold is first geometrically expanded to separate structure and then collapsed to form discrete semantic regions. We emphasize that these results are interpretive rather than prescriptive: the framework provides a topological lens that aligns with empirical regularities observed in large-scale discriminative and multimodal models, and with classical principles in statistical learning theory.

</details>


### [260] [CountGD++: Generalized Prompting for Open-World Counting](https://arxiv.org/abs/2512.23351)
*Niki Amini-Naieni,Andrew Zisserman*

Main category: cs.CV

TL;DR: This paper introduces refined methods for object counting in images and videos, adding the ability to exclude objects from counting, automating visual example creation, and improving flexibility with external images.


<details>
  <summary>Details</summary>
Motivation: Existing counting methods lack the flexibility to exclude certain objects or automate the process of annotating visual examples, restricting performance and usability.

Method: The authors propose enhancements to counting prompts (text and visual) by introducing pseudo-exemplars, extending models to accept diverse external images, and improving counting algorithms for better integration with large language models.

Result: Their model, CountGD++, delivers notable improvement in counting tasks across multiple datasets, proving greater accuracy, efficiency, and generalization.

Conclusion: These developments broaden the capabilities of open-world multi-modal counting, making counting systems more adaptable and versatile while refining performance.

Abstract: The flexibility and accuracy of methods for automatically counting objects in images and videos are limited by the way the object can be specified. While existing methods allow users to describe the target object with text and visual examples, the visual examples must be manually annotated inside the image, and there is no way to specify what not to count. To address these gaps, we introduce novel capabilities that expand how the target object can be specified. Specifically, we extend the prompt to enable what not to count to be described with text and/or visual examples, introduce the concept of `pseudo-exemplars' that automate the annotation of visual examples at inference, and extend counting models to accept visual examples from both natural and synthetic external images. We also use our new counting model, CountGD++, as a vision expert agent for an LLM. Together, these contributions expand the prompt flexibility of multi-modal open-world counting and lead to significant improvements in accuracy, efficiency, and generalization across multiple datasets. Code is available at https://github.com/niki-amini-naieni/CountGDPlusPlus.

</details>


### [261] [SpatialMosaic: A Multiview VLM Dataset for Partial Visibility](https://arxiv.org/abs/2512.23365)
*Kanghee Lee,Injae Lee,Minseok Kwak,Kwonyoung Ryu,Jungi Hong,Jaesik Park*

Main category: cs.CV

TL;DR: This paper introduces a new approach called SpatialMosaic, which improves 3D scene understanding and spatial reasoning for Vision-Language Models by addressing key challenges in fragmented visual conditions without explicit 3D reconstruction.


<details>
  <summary>Details</summary>
Motivation: Existing models for spatial reasoning rely heavily on pre-constructed 3D representations or pre-built pipelines, limiting scalability and applicability, especially in real-world environments with occlusion and limited visibility.

Method: The authors propose a scalable multi-view data generation pipeline that creates realistic spatial reasoning QA datasets (SpatialMosaic with 2M QA pairs) and introduce SpatialMosaicVLM, a hybrid framework integrating 3D models as geometry encoders within VLMs.

Result: Results show that the new dataset and framework significantly improve spatial reasoning in challenging scenarios like partial visibility and occlusion, demonstrating the effectiveness of the proposed methods.

Conclusion: The study introduces a realistic and diverse spatial reasoning dataset and a robust framework for spatial understanding, showing promise for improved performance in multi-view spatial reasoning tasks. The resources will be publicly available soon.

Abstract: The rapid progress of Multimodal Large Language Models (MLLMs) has unlocked the potential for enhanced 3D scene understanding and spatial reasoning. However, existing approaches often rely on pre-constructed 3D representations or off-the-shelf reconstruction pipelines, which constrain scalability and real-world applicability. A recent line of work explores learning spatial reasoning directly from multi-view images, enabling Vision-Language Models (VLMs) to understand 3D scenes without explicit 3D reconstructions. Nevertheless, key challenges that frequently arise in real-world environments, such as partial visibility, occlusion, and low-overlap conditions that require spatial reasoning from fragmented visual cues, remain under-explored. To address these limitations, we propose a scalable multi-view data generation and annotation pipeline that constructs realistic spatial reasoning QAs, resulting in SpatialMosaic, a comprehensive instruction-tuning dataset featuring 2M QA pairs. We further introduce SpatialMosaic-Bench, a challenging benchmark for evaluating multi-view spatial reasoning under realistic and challenging scenarios, consisting of 1M QA pairs across 6 tasks. In addition, we present SpatialMosaicVLM, a hybrid framework that integrates 3D reconstruction models as geometry encoders within VLMs for robust spatial reasoning. Extensive experiments demonstrate that our proposed dataset and VQA tasks effectively enhance spatial reasoning under challenging multi-view conditions, validating the effectiveness of our data generation pipeline in constructing realistic and diverse QA pairs. Code and dataset will be available soon.

</details>


### [262] [MGCA-Net: Multi-Graph Contextual Attention Network for Two-View Correspondence Learning](https://arxiv.org/abs/2512.23369)
*Shuyuan Lin,Mengtin Lo,Haosheng Chen,Yanjie Liang,Qiangqiang Wu*

Main category: cs.CV

TL;DR: The paper introduces MGCA-Net, a model addressing challenges in geometric modeling and cross-stage optimization for two-view correspondence learning.


<details>
  <summary>Details</summary>
Motivation: Existing methods in two-view correspondence learning struggle to accurately model local geometric constraints and optimize cross-stage information, impacting robustness.

Method: The proposed MGCA-Net combines Contextual Geometric Attention (to capture geometric relations) and Cross-Stage Multi-Graph Consensus (to ensure geometric consistency).

Result: MGCA-Net outperforms state-of-the-art methods in tasks like outlier rejection and camera pose estimation, validated on YFCC100M and SUN3D datasets.

Conclusion: MGCA-Net enhances reliability in establishing matching relationships for applications such as 3D reconstruction and shows superior performance in geometric modeling.

Abstract: Two-view correspondence learning is a key task in computer vision, which aims to establish reliable matching relationships for applications such as camera pose estimation and 3D reconstruction. However, existing methods have limitations in local geometric modeling and cross-stage information optimization, which make it difficult to accurately capture the geometric constraints of matched pairs and thus reduce the robustness of the model. To address these challenges, we propose a Multi-Graph Contextual Attention Network (MGCA-Net), which consists of a Contextual Geometric Attention (CGA) module and a Cross-Stage Multi-Graph Consensus (CSMGC) module. Specifically, CGA dynamically integrates spatial position and feature information via an adaptive attention mechanism and enhances the capability to capture both local and global geometric relationships. Meanwhile, CSMGC establishes geometric consensus via a cross-stage sparse graph network, ensuring the consistency of geometric information across different stages. Experimental results on two representative YFCC100M and SUN3D datasets show that MGCA-Net significantly outperforms existing SOTA methods in the outlier rejection and camera pose estimation tasks. Source code is available at http://www.linshuyuan.com.

</details>


### [263] [NeXT-IMDL: Build Benchmark for NeXT-Generation Image Manipulation Detection & Localization](https://arxiv.org/abs/2512.23374)
*Yifei Li,Haoyuan He,Yu Zheng,Bingyao Yu,Wenzhao Zheng,Lei Chen,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: This paper addresses the need for advanced methods in Image Manipulation Detection and Localization (IMDL) through the proposed diagnostic benchmark, NeXT-IMDL.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of existing IMDL methods, which fail in generalized scenarios due to the simplistic cross-dataset evaluations and lack robustness against diverse AI-generated manipulations.

Method: The authors introduce NeXT-IMDL, a benchmark that organizes manipulations into four key dimensions and implements five cross-dimension evaluation protocols to test the generalization capacity of IMDL models.

Result: Extensive experiments on 11 models reveal that current IMDL methods significantly degrade when evaluated under NeXT-IMDL protocols, highlighting their lack of generalization capabilities in real-world scenarios.

Conclusion: The paper emphasizes the need for robust, generalizable IMDL models and offers NeXT-IMDL as a toolkit for diagnosing and addressing the current gaps in IMDL progress.

Abstract: The accessibility surge and abuse risks of user-friendly image editing models have created an urgent need for generalizable, up-to-date methods for Image Manipulation Detection and Localization (IMDL). Current IMDL research typically uses cross-dataset evaluation, where models trained on one benchmark are tested on others. However, this simplified evaluation approach conceals the fragility of existing methods when handling diverse AI-generated content, leading to misleading impressions of progress. This paper challenges this illusion by proposing NeXT-IMDL, a large-scale diagnostic benchmark designed not just to collect data, but to probe the generalization boundaries of current detectors systematically. Specifically, NeXT-IMDL categorizes AIGC-based manipulations along four fundamental axes: editing models, manipulation types, content semantics, and forgery granularity. Built upon this, NeXT-IMDL implements five rigorous cross-dimension evaluation protocols. Our extensive experiments on 11 representative models reveal a critical insight: while these models perform well in their original settings, they exhibit systemic failures and significant performance degradation when evaluated under our designed protocols that simulate real-world, various generalization scenarios. By providing this diagnostic toolkit and the new findings, we aim to advance the development towards building truly robust, next-generation IMDL models.

</details>


### [264] [SoulX-LiveTalk Technical Report](https://arxiv.org/abs/2512.23379)
*Le Shen,Qiao Qian,Tan Yu,Ke Zhou,Tianhang Yu,Yu Zhan,Zhenjie Wang,Ming Tao,Shunshun Yin,Siyuan Liu*

Main category: cs.CV

TL;DR: The paper introduces SoulX-LiveTalk, a 14B-parameter model designed for high-quality real-time audio-driven avatar generation, addressing latency and fidelity via innovative strategies.


<details>
  <summary>Details</summary>
Motivation: To resolve the conflict between computational demands and strict latency requirements in real-time, infinite-duration, audio-driven avatar generation, which often sacrifices visual fidelity.

Method: The authors propose a Self-correcting Bidirectional Distillation strategy, a Multi-step Retrospective Self-Correction Mechanism, and a full-stack inference acceleration suite.

Result: SoulX-LiveTalk achieves sub-second start-up latency (0.87s) and real-time throughput of 32 FPS while preserving motion coherence and visual detail.

Conclusion: SoulX-LiveTalk sets a new benchmark for high-fidelity real-time digital human synthesis, overcoming previous performance limitations while maintaining latency and fidelity standards.

Abstract: Deploying massive diffusion models for real-time, infinite-duration, audio-driven avatar generation presents a significant engineering challenge, primarily due to the conflict between computational load and strict latency constraints. Existing approaches often compromise visual fidelity by enforcing strictly unidirectional attention mechanisms or reducing model capacity. To address this problem, we introduce \textbf{SoulX-LiveTalk}, a 14B-parameter framework optimized for high-fidelity real-time streaming. Diverging from conventional unidirectional paradigms, we use a \textbf{Self-correcting Bidirectional Distillation} strategy that retains bidirectional attention within video chunks. This design preserves critical spatiotemporal correlations, significantly enhancing motion coherence and visual detail. To ensure stability during infinite generation, we incorporate a \textbf{Multi-step Retrospective Self-Correction Mechanism}, enabling the model to autonomously recover from accumulated errors and preventing collapse. Furthermore, we engineered a full-stack inference acceleration suite incorporating hybrid sequence parallelism, Parallel VAE, and kernel-level optimizations. Extensive evaluations confirm that SoulX-LiveTalk is the first 14B-scale system to achieve a \textbf{sub-second start-up latency (0.87s)} while reaching a real-time throughput of \textbf{32 FPS}, setting a new standard for high-fidelity interactive digital human synthesis.

</details>


### [265] [SOFTooth: Semantics-Enhanced Order-Aware Fusion for Tooth Instance Segmentation](https://arxiv.org/abs/2512.23411)
*Xiaolan Li,Wanquan Liu,Pengcheng Li,Pengyu Jie,Chenqiang Gao*

Main category: cs.CV

TL;DR: SOFTooth is a novel 2D-3D fusion framework that improves 3D tooth segmentation by integrating 2D semantics from foundation models like SAM to overcome challenges like boundary leakage and missing teeth.


<details>
  <summary>Details</summary>
Motivation: To improve 3D tooth instance segmentation which faces challenges such as crowded arches, boundary issues, missing teeth, and difficulties in identifying rare structures like third molars.

Method: SOFTooth uses a semantics-enhanced and order-aware 2D-3D fusion approach with three core innovations: a point-wise residual gating module, center-guided mask refinement, and an order-aware Hungarian matching strategy.

Result: SOFTooth achieves state-of-the-art performance in accuracy and mean IoU on the 3DTeethSeg'22 dataset, particularly excelling in complex cases involving third molars.

Conclusion: The approach demonstrates that rich 2D semantic information can be effectively leveraged to address challenges in 3D tooth segmentation, eliminating the need for explicit 2D mask supervision.

Abstract: Three-dimensional (3D) tooth instance segmentation remains challenging due to crowded arches, ambiguous tooth-gingiva boundaries, missing teeth, and rare yet clinically important third molars. Native 3D methods relying on geometric cues often suffer from boundary leakage, center drift, and inconsistent tooth identities, especially for minority classes and complex anatomies. Meanwhile, 2D foundation models such as the Segment Anything Model (SAM) provide strong boundary-aware semantics, but directly applying them in 3D is impractical in clinical workflows. To address these issues, we propose SOFTooth, a semantics-enhanced, order-aware 2D-3D fusion framework that leverages frozen 2D semantics without explicit 2D mask supervision. First, a point-wise residual gating module injects occlusal-view SAM embeddings into 3D point features to refine tooth-gingiva and inter-tooth boundaries. Second, a center-guided mask refinement regularizes consistency between instance masks and geometric centroids, reducing center drift. Furthermore, an order-aware Hungarian matching strategy integrates anatomical tooth order and center distance into similarity-based assignment, ensuring coherent labeling even under missing or crowded dentitions. On 3DTeethSeg'22, SOFTooth achieves state-of-the-art overall accuracy and mean IoU, with clear gains on cases involving third molars, demonstrating that rich 2D semantics can be effectively transferred to 3D tooth instance segmentation without 2D fine-tuning.

</details>


### [266] [Bridging Cognitive Gap: Hierarchical Description Learning for Artistic Image Aesthetics Assessment](https://arxiv.org/abs/2512.23413)
*Henglin Liu,Nisha Huang,Chang Liu,Jiangpeng Yan,Huijuan Huang,Jixuan Ying,Tong-Yee Lee,Pengfei Wan,Xiangyang Ji*

Main category: cs.CV

TL;DR: This paper addresses challenges in aesthetic quality assessment for Artistic Image Generation and proposes solutions through a robust dataset (RAD) and framework (ArtQuant).


<details>
  <summary>Details</summary>
Motivation: The study addresses complexity in aesthetic quality assessment for artistic images, which involves visual perception, cognition, and emotion.

Method: It introduces the RAD dataset to tackle data scarcity and imbalance, paired with ArtQuant, a joint aesthetic framework utilizing advanced long-text modeling with LLM decoders.

Result: The proposed approach outperforms existing methods, achieves reduced training epochs, and enhances alignment between artistic images and aesthetic judgment.

Conclusion: RAD and ArtQuant collectively improve aesthetic evaluations, bridging gaps in artistic image understanding. The framework and dataset will be released for further exploration.

Abstract: The aesthetic quality assessment task is crucial for developing a human-aligned quantitative evaluation system for AIGC. However, its inherently complex nature, spanning visual perception, cognition, and emotion, poses fundamental challenges. Although aesthetic descriptions offer a viable representation of this complexity, two critical challenges persist: (1) data scarcity and imbalance: existing dataset overly focuses on visual perception and neglects deeper dimensions due to the expensive manual annotation; and (2) model fragmentation: current visual networks isolate aesthetic attributes with multi-branch encoder, while multimodal methods represented by contrastive learning struggle to effectively process long-form textual descriptions. To resolve challenge (1), we first present the Refined Aesthetic Description (RAD) dataset, a large-scale (70k), multi-dimensional structured dataset, generated via an iterative pipeline without heavy annotation costs and easy to scale. To address challenge (2), we propose ArtQuant, an aesthetics assessment framework for artistic images which not only couples isolated aesthetic dimensions through joint description generation, but also better models long-text semantics with the help of LLM decoders. Besides, theoretical analysis confirms this symbiosis: RAD's semantic adequacy (data) and generation paradigm (model) collectively minimize prediction entropy, providing mathematical grounding for the framework. Our approach achieves state-of-the-art performance on several datasets while requiring only 33% of conventional training epochs, narrowing the cognitive gap between artistic images and aesthetic judgment. We will release both code and dataset to support future research.

</details>


### [267] [DriveLaW:Unifying Planning and Video Generation in a Latent Driving World](https://arxiv.org/abs/2512.23421)
*Tianze Xia,Yongkang Li,Lijun Zhou,Jingfeng Yao,Kaixin Xiong,Haiyang Sun,Bing Wang,Kun Ma,Hangjun Ye,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: DriveLaW unifies video generation and motion planning to improve autonomous driving's predictive and planning abilities, achieving notable advancements in both tasks.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of decoupled processes in current approaches to autonomous driving where world prediction and motion planning are separated.

Method: DriveLaW introduces a unified framework combining video generation and motion planning. It features ‘DriveLaW-Video’ for high-fidelity forecasting and ‘DriveLaW-Act’ for trajectory planning, linked via latent representation, optimized through a three-stage training strategy.

Result: Achieved new state-of-the-art results: 33.3% improvement in FID, 1.8% improvement in FVD for video prediction, and best performance on the NAVSIM planning benchmark.

Conclusion: DriveLaW represents a significant step forward by integrating predictive video generation and planning, providing more consistent and reliable outputs for autonomous driving systems.

Abstract: World models have become crucial for autonomous driving, as they learn how scenarios evolve over time to address the long-tail challenges of the real world. However, current approaches relegate world models to limited roles: they operate within ostensibly unified architectures that still keep world prediction and motion planning as decoupled processes. To bridge this gap, we propose DriveLaW, a novel paradigm that unifies video generation and motion planning. By directly injecting the latent representation from its video generator into the planner, DriveLaW ensures inherent consistency between high-fidelity future generation and reliable trajectory planning. Specifically, DriveLaW consists of two core components: DriveLaW-Video, our powerful world model that generates high-fidelity forecasting with expressive latent representations, and DriveLaW-Act, a diffusion planner that generates consistent and reliable trajectories from the latent of DriveLaW-Video, with both components optimized by a three-stage progressive training strategy. The power of our unified paradigm is demonstrated by new state-of-the-art results across both tasks. DriveLaW not only advances video prediction significantly, surpassing best-performing work by 33.3% in FID and 1.8% in FVD, but also achieves a new record on the NAVSIM planning benchmark.

</details>


### [268] [Direct Diffusion Score Preference Optimization via Stepwise Contrastive Policy-Pair Supervision](https://arxiv.org/abs/2512.23426)
*Dohyun Kim,Seungwoo Lyu,Seung Wook Kim,Paul Hongsuck Seo*

Main category: cs.CV

TL;DR: The paper introduces DDSPO, a preference-based training method for diffusion models, avoiding costly human-labeled datasets and improving text-to-image synthesis.


<details>
  <summary>Details</summary>
Motivation: To address limitations in diffusion models regarding alignment with user intent and aesthetic quality, especially avoiding noisy and expensive human-labeled dataset reliance.

Method: DDSPO generates automatic preference signals by evaluating differences in outputs from original and semantically degraded prompts using a pretrained reference model, providing dense supervision across denoising steps.

Result: Empirical studies show improved text-image alignment and visual quality using DDSPO, outperforming or matching other preference-based methods with reduced supervision.

Conclusion: DDSPO is an efficient alternative for preference-based training in diffusion models, enhancing user intent alignment and aesthetics while reducing supervision costs.

Abstract: Diffusion models have achieved impressive results in generative tasks such as text-to-image synthesis, yet they often struggle to fully align outputs with nuanced user intent and maintain consistent aesthetic quality. Existing preference-based training methods like Diffusion Direct Preference Optimization help address these issues but rely on costly and potentially noisy human-labeled datasets. In this work, we introduce Direct Diffusion Score Preference Optimization (DDSPO), which directly derives per-timestep supervision from winning and losing policies when such policies are available. Unlike prior methods that operate solely on final samples, DDSPO provides dense, transition-level signals across the denoising trajectory. In practice, we avoid reliance on labeled data by automatically generating preference signals using a pretrained reference model: we contrast its outputs when conditioned on original prompts versus semantically degraded variants. This practical strategy enables effective score-space preference supervision without explicit reward modeling or manual annotations. Empirical results demonstrate that DDSPO improves text-image alignment and visual quality, outperforming or matching existing preference-based methods while requiring significantly less supervision. Our implementation is available at: https://dohyun-as.github.io/DDSPO

</details>


### [269] [Towards Integrating Uncertainty for Domain-Agnostic Segmentation](https://arxiv.org/abs/2512.23427)
*Jesse Brouwers,Xiaoyan Xing,Alexander Timans*

Main category: cs.CV

TL;DR: The paper examines the use of uncertainty quantification to improve the generalizability and robustness of the Segment Anything Model (SAM) in domain-agnostic segmentation tasks, introducing a benchmark called UncertSAM, various uncertainty estimation methods, and a preliminary refinement step.


<details>
  <summary>Details</summary>
Motivation: Foundation models for segmentation like SAM perform well in zero-shot settings but struggle in challenging domains with shifted or limited knowledge. The authors aim to explore whether uncertainty quantification can mitigate these challenges.

Method: The authors introduce UncertSAM, a benchmark of eight datasets targeting challenging segmentation tasks, evaluate lightweight uncertainty estimation techniques, and test an uncertainty-guided prediction refinement step.

Result: A last-layer Laplace approximation provides uncertainty estimates that align well with segmentation errors. Initial results hint at the potential of refinement methods, though further validation is required.

Conclusion: Incorporating uncertainty into segmentation models holds promise for improving domain-agnostic robustness. The authors release their benchmark and code to enable further research.

Abstract: Foundation models for segmentation such as the Segment Anything Model (SAM) family exhibit strong zero-shot performance, but remain vulnerable in shifted or limited-knowledge domains. This work investigates whether uncertainty quantification can mitigate such challenges and enhance model generalisability in a domain-agnostic manner. To this end, we (1) curate UncertSAM, a benchmark comprising eight datasets designed to stress-test SAM under challenging segmentation conditions including shadows, transparency, and camouflage; (2) evaluate a suite of lightweight, post-hoc uncertainty estimation methods; and (3) assess a preliminary uncertainty-guided prediction refinement step. Among evaluated approaches, a last-layer Laplace approximation yields uncertainty estimates that correlate well with segmentation errors, indicating a meaningful signal. While refinement benefits are preliminary, our findings underscore the potential of incorporating uncertainty into segmentation models to support robust, domain-agnostic performance. Our benchmark and code are made publicly available.

</details>


### [270] [Fuzzy-Logic and Deep Learning for Environmental Condition-Aware Road Surface Classification](https://arxiv.org/abs/2512.23436)
*Mustafa Demetgul,Sanja Lazarova Molnar*

Main category: cs.CV

TL;DR: The paper introduces a real-time system for road surface monitoring using deep learning and data from mobile phone cameras and acceleration sensors.


<details>
  <summary>Details</summary>
Motivation: Classical road surface monitoring methods are costly and inefficient, requiring a systematic and faster solution for real-time vehicle control and planning.

Method: A mobile phone camera and acceleration data were used to collect and classify road surfaces through image-based deep learning algorithms. Performance of various deep learning models like AlexNet, LeNet, VGG, and ResNet were evaluated. A fuzzy logic system was also suggested for enhanced classification based on conditions like weather and time of day.

Result: The proposed system achieved over 95% accuracy in classifying 5 road condition classes: asphalt, damaged asphalt, gravel road, damaged gravel road, and pavement road.

Conclusion: Deep learning and sensor data integration offer an effective solution for real-time and accurate road surface monitoring, showcasing strong classification performance and the potential for advanced vehicle systems integration.

Abstract: Monitoring states of road surfaces provides valuable information for the planning and controlling vehicles and active vehicle control systems. Classical road monitoring methods are expensive and unsystematic because they require time for measurements. This article proposes an real time system based on weather conditional data and road surface condition data. For this purpose, we collected data with a mobile phone camera on the roads around the campus of the Karlsruhe Institute of Technology. We tested a large number of different image-based deep learning algorithms for road classification. In addition, we used road acceleration data along with road image data for training by using them as images. We compared the performances of acceleration-based and camera image-based approaches. The performances of the simple Alexnet, LeNet, VGG, and Resnet algorithms were compared as deep learning algorithms. For road condition classification, 5 classes were considered: asphalt, damaged asphalt, gravel road, damaged gravel road, pavement road and over 95% accuracy performance was achieved. It is also proposed to use the acceleration or the camera image to classify the road surface according to the weather and the time of day using fuzzy logic.

</details>


### [271] [RealX3D: A Physically-Degraded 3D Benchmark for Multi-view Visual Restoration and Reconstruction](https://arxiv.org/abs/2512.23437)
*Shuhong Liu,Chenyu Bao,Ziteng Cui,Yun Liu,Xuangeng Chu,Lin Gu,Marcos V. Conde,Ryo Umagami,Tomohiro Hashimoto,Zijian Hu,Tianhan Xu,Yuan Gan,Yusuke Kurose,Tatsuya Harada*

Main category: cs.CV

TL;DR: The paper introduces RealX3D, a real-capture benchmark for improving multi-view visual restoration and 3D reconstruction under various real-world challenges.


<details>
  <summary>Details</summary>
Motivation: To address the lack of robust benchmarks that evaluate the performance of multi-view visual systems under real-world physical degradations.

Method: The authors grouped physical corruptions (illumination, scattering, occlusion, and blurring) into families and captured data at multiple severity levels using a unified protocol, producing aligned low-quality (LQ) and ground truth (GT) views with high-resolution captures, RAW images, and laser-generated world-scale metrics.

Result: Extensive testing of various optimization-based and feed-forward methods revealed significant reconstruction quality issues when faced with captured physical degradations.

Conclusion: Existing multi-view reconstruction pipelines are fragile in real-world environments, highlighting the necessity for RealX3D and motivating the development of more robust techniques.

Abstract: We introduce RealX3D, a real-capture benchmark for multi-view visual restoration and 3D reconstruction under diverse physical degradations. RealX3D groups corruptions into four families, including illumination, scattering, occlusion, and blurring, and captures each at multiple severity levels using a unified acquisition protocol that yields pixel-aligned LQ/GT views. Each scene includes high-resolution capture, RAW images, and dense laser scans, from which we derive world-scale meshes and metric depth. Benchmarking a broad range of optimization-based and feed-forward methods shows substantial degradation in reconstruction quality under physical corruptions, underscoring the fragility of current multi-view pipelines in real-world challenging environments.

</details>


### [272] [CoFi-Dec: Hallucination-Resistant Decoding via Coarse-to-Fine Generative Feedback in Large Vision-Language Models](https://arxiv.org/abs/2512.23453)
*Zongsheng Cao,Yangfan He,Anran Liu,Jun Xie,Feng Chen,Zepeng Wang*

Main category: cs.CV

TL;DR: CoFi-Dec is a novel training-free decoding framework aiming to reduce hallucinations in Large Vision-Language Models (LVLMs) by combining self-feedback and coarse-to-fine visual conditioning.


<details>
  <summary>Details</summary>
Motivation: LVLMs, while effective for multimodal tasks, often produce hallucinated outputs inconsistent with visual inputs, which hinders their applicability in real-world scenarios.

Method: The proposed framework generates two intermediate textual responses based on coarse and fine visual cues, transforms them into synthetic images via a text-to-image model, and employs a Wasserstein-based fusion to align the outputs for consistent decoding.

Result: CoFi-Dec significantly reduces both semantic- and entity-level hallucinations across six benchmarks, surpassing existing methods in performance.

Conclusion: CoFi-Dec offers a model-agnostic, training-free solution for enhancing the reliability of LVLMs, making it adaptable to a variety of scenarios while maintaining semantic and visual accuracy.

Abstract: Large Vision-Language Models (LVLMs) have achieved impressive progress in multi-modal understanding and generation. However, they still tend to produce hallucinated content that is inconsistent with the visual input, which limits their reliability in real-world applications. We propose \textbf{CoFi-Dec}, a training-free decoding framework that mitigates hallucinations by integrating generative self-feedback with coarse-to-fine visual conditioning. Inspired by the human visual process from global scene perception to detailed inspection, CoFi-Dec first generates two intermediate textual responses conditioned on coarse- and fine-grained views of the original image. These responses are then transformed into synthetic images using a text-to-image model, forming multi-level visual hypotheses that enrich grounding cues. To unify the predictions from these multiple visual conditions, we introduce a Wasserstein-based fusion mechanism that aligns their predictive distributions into a geometrically consistent decoding trajectory. This principled fusion reconciles high-level semantic consistency with fine-grained visual grounding, leading to more robust and faithful outputs. Extensive experiments on six hallucination-focused benchmarks show that CoFi-Dec substantially reduces both entity-level and semantic-level hallucinations, outperforming existing decoding strategies. The framework is model-agnostic, requires no additional training, and can be seamlessly applied to a wide range of LVLMs. The implementation is available at https://github.com/AI-Researcher-Team/CoFi-Dec.

</details>


### [273] [Automated river gauge plate reading using a hybrid object detection and generative AI framework in the Limpopo River Basin](https://arxiv.org/abs/2512.23454)
*Kayathri Vigneswaran,Hugo Retief,Jai Clifford Holmes,Mariangel Garcia Andarcia,Hansaka Tennakoon*

Main category: cs.CV

TL;DR: This study introduces a hybrid framework combining vision-based detection, YOLOv8, and LLMs for automated river water level readings, showcasing high precision and accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need for accurate and continuous river water level monitoring for flood forecasting, water resource management, and ecological protection. Traditional methods face manual errors and environmental constraints, driving the need for automated, efficient solutions.

Method: The methodology integrates image preprocessing, annotation, waterline detection, YOLOv8 for scale extraction, and large language models (LLMs) like GPT-4o and Gemini 2.0 to automate numeric reading extraction from river gauge plates.

Result: The framework achieved 94.24% precision and an F1 score of 83.64% for waterline detection. Gemini Stage 2 demonstrated the best accuracy with a mean absolute error of 5.43 cm, RMSE of 8.58 cm, and R² of 0.84 under optimal input.

Conclusion: The proposed framework proves to be scalable and reliable for automated hydrological monitoring, with potential for real-time applications. Challenges include sensitivity to image quality, which emphasizes combining geometry with multimodal AI for robust outputs.

Abstract: Accurate and continuous monitoring of river water levels is essential for flood forecasting, water resource management, and ecological protection. Traditional hydrological observation methods are often limited by manual measurement errors and environmental constraints. This study presents a hybrid framework integrating vision based waterline detection, YOLOv8 pose scale extraction, and large multimodal language models (GPT 4o and Gemini 2.0 Flash) for automated river gauge plate reading. The methodology involves sequential stages of image preprocessing, annotation, waterline detection, scale gap estimation, and numeric reading extraction. Experiments demonstrate that waterline detection achieved high precision of 94.24 percent and an F1 score of 83.64 percent, while scale gap detection provided accurate geometric calibration for subsequent reading extraction. Incorporating scale gap metadata substantially improved the predictive performance of LLMs, with Gemini Stage 2 achieving the highest accuracy, with a mean absolute error of 5.43 cm, root mean square error of 8.58 cm, and R squared of 0.84 under optimal image conditions. Results highlight the sensitivity of LLMs to image quality, with degraded images producing higher errors, and underscore the importance of combining geometric metadata with multimodal artificial intelligence for robust water level estimation. Overall, the proposed approach offers a scalable, efficient, and reliable solution for automated hydrological monitoring, demonstrating potential for real time river gauge digitization and improved water resource management.

</details>


### [274] [Deterministic Image-to-Image Translation via Denoising Brownian Bridge Models with Dual Approximators](https://arxiv.org/abs/2512.23463)
*Bohan Xiao,Peiyong Wang,Qisheng He,Ming Dong*

Main category: cs.CV

TL;DR: The paper introduces the 'Dual-approx Bridge' model for deterministic image-to-image translations achieving high fidelity outputs with minimal variance, focusing on super-resolution and benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: The aim is to enhance deterministic I2I translation, ensuring consistent, predictable outputs with high fidelity for tasks like image super-resolution.

Method: A denoising Brownian bridge approach is used, integrating two neural network approximators for forward and reverse processes to ensure high-quality and faithful outputs in I2I translations.

Result: Experiments on benchmark datasets prove the model’s superior performance in terms of fidelity and quality, outperforming stochastic and deterministic baseline methods.

Conclusion: Dual-approx Bridge significantly improves deterministic I2I translation, ensuring faithful and high-quality outputs while outperforming existing approaches.

Abstract: Image-to-Image (I2I) translation involves converting an image from one domain to another. Deterministic I2I translation, such as in image super-resolution, extends this concept by guaranteeing that each input generates a consistent and predictable output, closely matching the ground truth (GT) with high fidelity. In this paper, we propose a denoising Brownian bridge model with dual approximators (Dual-approx Bridge), a novel generative model that exploits the Brownian bridge dynamics and two neural network-based approximators (one for forward and one for reverse process) to produce faithful output with negligible variance and high image quality in I2I translations. Our extensive experiments on benchmark datasets including image generation and super-resolution demonstrate the consistent and superior performance of Dual-approx Bridge in terms of image quality and faithfulness to GT when compared to both stochastic and deterministic baselines. Project page and code: https://github.com/bohan95/dual-app-bridge

</details>


### [275] [HY-Motion 1.0: Scaling Flow Matching Models for Text-To-Motion Generation](https://arxiv.org/abs/2512.23464)
*Yuxin Wen,Qing Shuai,Di Kang,Jing Li,Cheng Wen,Yue Qian,Ningxin Jiao,Changhai Chen,Weijie Chen,Yiran Wang,Jinkun Guo,Dongyue An,Han Liu,Yanyu Tong,Chao Zhang,Qing Guo,Juan Chen,Qiao Zhang,Youyi Zhang,Zihao Yao,Cheng Zhang,Hong Duan,Xiaoping Wu,Qi Chen,Fei Cheng,Liang Dong,Peng He,Hao Zhang,Jiaxin Lin,Chao Zhang,Zhongyi Fan,Yifan Li,Zhichao Hu,Yuhong Liu,Linus,Jie Jiang,Xiaolong Li,Linchao Bao*

Main category: cs.CV

TL;DR: HY-Motion 1.0 generates 3D human motions from text descriptions, scaling up diffusion transformer models to billions of parameters for unprecedented performance.


<details>
  <summary>Details</summary>
Motivation: Existing motion generation models lack scalability and instruction-following precision.

Method: HY-Motion 1.0 uses large-scale pretraining, fine-tuning, reinforcement learning, and meticulous data processing.

Result: Achieves alignment with text instructions, high motion quality, and coverage across 200 motion categories.

Conclusion: HY-Motion 1.0 sets a benchmark for open-source motion generation research, accelerating commercial adoption.

Abstract: We present HY-Motion 1.0, a series of state-of-the-art, large-scale, motion generation models capable of generating 3D human motions from textual descriptions. HY-Motion 1.0 represents the first successful attempt to scale up Diffusion Transformer (DiT)-based flow matching models to the billion-parameter scale within the motion generation domain, delivering instruction-following capabilities that significantly outperform current open-source benchmarks. Uniquely, we introduce a comprehensive, full-stage training paradigm -- including large-scale pretraining on over 3,000 hours of motion data, high-quality fine-tuning on 400 hours of curated data, and reinforcement learning from both human feedback and reward models -- to ensure precise alignment with the text instruction and high motion quality. This framework is supported by our meticulous data processing pipeline, which performs rigorous motion cleaning and captioning. Consequently, our model achieves the most extensive coverage, spanning over 200 motion categories across 6 major classes. We release HY-Motion 1.0 to the open-source community to foster future research and accelerate the transition of 3D human motion generation models towards commercial maturity.

</details>


### [276] [MCI-Net: A Robust Multi-Domain Context Integration Network for Point Cloud Registration](https://arxiv.org/abs/2512.23472)
*Shuyuan Lin,Wenwu Peng,Junjie Huang,Qiang Qi,Miaohui Wang,Jian Weng*

Main category: cs.CV

TL;DR: The paper introduces MCI-Net, a method to enhance point cloud registration through improved feature learning by leveraging contextual cues from multiple domains.


<details>
  <summary>Details</summary>
Motivation: Existing methods for point cloud registration fail to effectively capture implicit semantics and structural consistency, relying mostly on Euclidean neighborhood strategies.

Method: MCI-Net incorporates a graph neighborhood aggregation module for global structural relationships, a progressive context interaction module for feature enhancement, and a dynamic inlier selection method optimizing inlier weights during pose estimation.

Result: Experiments show MCI-Net achieves superior performance, notably a registration recall of 96.4% on 3DMatch dataset, outperforming other methods.

Conclusion: By integrating multi-domain contextual cues, MCI-Net enhances feature representation and robustness in point cloud registration, proving to be a significant advancement over current approaches.

Abstract: Robust and discriminative feature learning is critical for high-quality point cloud registration. However, existing deep learning-based methods typically rely on Euclidean neighborhood-based strategies for feature extraction, which struggle to effectively capture the implicit semantics and structural consistency in point clouds. To address these issues, we propose a multi-domain context integration network (MCI-Net) that improves feature representation and registration performance by aggregating contextual cues from diverse domains. Specifically, we propose a graph neighborhood aggregation module, which constructs a global graph to capture the overall structural relationships within point clouds. We then propose a progressive context interaction module to enhance feature discriminability by performing intra-domain feature decoupling and inter-domain context interaction. Finally, we design a dynamic inlier selection method that optimizes inlier weights using residual information from multiple iterations of pose estimation, thereby improving the accuracy and robustness of registration. Extensive experiments on indoor RGB-D and outdoor LiDAR datasets show that the proposed MCI-Net significantly outperforms existing state-of-the-art methods, achieving the highest registration recall of 96.4\% on 3DMatch. Source code is available at http://www.linshuyuan.com.

</details>


### [277] [SC-Net: Robust Correspondence Learning via Spatial and Cross-Channel Context](https://arxiv.org/abs/2512.23473)
*Shuyuan Lin,Hailiang Liao,Qiang Qi,Junjie Huang,Taotao Lai,Jian Weng*

Main category: cs.CV

TL;DR: The study introduces SC-Net, enhancing two-view correspondence learning by addressing limitations in CNN backbones. SC-Net excels in pose estimation and outlier removal tasks.


<details>
  <summary>Details</summary>
Motivation: CNN backbones exhibit issues like inadequate global context aggregation and oversmoothing in scenes with large disparity, prompting the need for a tailored solution.

Method: The paper introduces SC-Net with three main modules: the AFR module for robust position-awareness, BFA module for refined motion field modeling, and PAR module for precise recovery of motion vectors.

Result: SC-Net demonstrates superior performance over existing methods for pose estimation and outlier removal on YFCC100M and SUN3D datasets.

Conclusion: SC-Net offers a novel approach to overcome existing limitations in two-view correspondence learning, showcasing improved accuracy and robustness.

Abstract: Recent research has focused on using convolutional neural networks (CNNs) as the backbones in two-view correspondence learning, demonstrating significant superiority over methods based on multilayer perceptrons. However, CNN backbones that are not tailored to specific tasks may fail to effectively aggregate global context and oversmooth dense motion fields in scenes with large disparity. To address these problems, we propose a novel network named SC-Net, which effectively integrates bilateral context from both spatial and channel perspectives. Specifically, we design an adaptive focused regularization module (AFR) to enhance the model's position-awareness and robustness against spurious motion samples, thereby facilitating the generation of a more accurate motion field. We then propose a bilateral field adjustment module (BFA) to refine the motion field by simultaneously modeling long-range relationships and facilitating interaction across spatial and channel dimensions. Finally, we recover the motion vectors from the refined field using a position-aware recovery module (PAR) that ensures consistency and precision. Extensive experiments demonstrate that SC-Net outperforms state-of-the-art methods in relative pose estimation and outlier removal tasks on YFCC100M and SUN3D datasets. Source code is available at http://www.linshuyuan.com.

</details>


### [278] [TV-RAG: A Temporal-aware and Semantic Entropy-Weighted Framework for Long Video Retrieval and Understanding](https://arxiv.org/abs/2512.23483)
*Zongsheng Cao,Yangfan He,Anran Liu,Feng Chen,Zepeng Wang,Jun Xie*

Main category: cs.CV

TL;DR: TV-RAG is a training-free framework designed to improve long-video reasoning in Large Video Language Models (LVLMs) by addressing temporal and semantic challenges.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of LVLMs in handling lengthy videos, particularly their difficulty in tracking fine-grained semantic shifts and temporal interdependencies across multimedia channels.

Method: TV-RAG introduces two mechanisms: (i) a time-decay retrieval module for better contextual ranking through temporal offsets and (ii) an entropy-weighted key-frame sampler for selecting evenly spaced, information-dense frames.

Result: TV-RAG surpasses leading baselines on benchmarks such as Video-MME, MLVU, and LongVideoBench for long-video reasoning tasks.

Conclusion: TV-RAG offers a lightweight upgrade path for LVLMs, enhancing their long-video reasoning capabilities without requiring re-training or fine-tuning, demonstrating consistent effectiveness across benchmarks.

Abstract: Large Video Language Models (LVLMs) have rapidly emerged as the focus of multimedia AI research. Nonetheless, when confronted with lengthy videos, these models struggle: their temporal windows are narrow, and they fail to notice fine-grained semantic shifts that unfold over extended durations. Moreover, mainstream text-based retrieval pipelines, which rely chiefly on surface-level lexical overlap, ignore the rich temporal interdependence among visual, audio, and subtitle channels. To mitigate these limitations, we propose TV-RAG, a training-free architecture that couples temporal alignment with entropy-guided semantics to improve long-video reasoning. The framework contributes two main mechanisms: \emph{(i)} a time-decay retrieval module that injects explicit temporal offsets into the similarity computation, thereby ranking text queries according to their true multimedia context; and \emph{(ii)} an entropy-weighted key-frame sampler that selects evenly spaced, information-dense frames, reducing redundancy while preserving representativeness. By weaving these temporal and semantic signals together, TV-RAG realises a dual-level reasoning routine that can be grafted onto any LVLM without re-training or fine-tuning. The resulting system offers a lightweight, budget-friendly upgrade path and consistently surpasses most leading baselines across established long-video benchmarks such as Video-MME, MLVU, and LongVideoBench, confirming the effectiveness of our model. The code can be found at https://github.com/AI-Researcher-Team/TV-RAG.

</details>


### [279] [Multi-label Classification with Panoptic Context Aggregation Networks](https://arxiv.org/abs/2512.23486)
*Mingyuan Jiu,Hailong Zhu,Wenchuan Wei,Hichem Sahbi,Rongrong Ji,Mingliang Xu*

Main category: cs.CV

TL;DR: The paper proposes Deep Panoptic Context Aggregation Network (PanCAN) to improve image recognition by effectively integrating contextual information across scales and features.


<details>
  <summary>Details</summary>
Motivation: Current visual recognition approaches inadequately address cross-scale contextual interactions between objects, limiting the understanding of complex scenes.

Method: The PanCAN model hierarchically integrates multi-order geometric contexts through cross-scale feature aggregation using random walks and attention mechanisms in multi-label classification tasks.

Result: PanCAN outperforms state-of-the-art techniques on NUS-WIDE, PASCAL VOC2007, and MS-COCO datasets in multi-label classification with both quantitative and qualitative improvements.

Conclusion: PanCAN enhances multi-label classification performance by capturing multi-order and cross-scale contextual information, demonstrating superior understanding of complex visual scenes.

Abstract: Context modeling is crucial for visual recognition, enabling highly discriminative image representations by integrating both intrinsic and extrinsic relationships between objects and labels in images. A limitation in current approaches is their focus on basic geometric relationships or localized features, often neglecting cross-scale contextual interactions between objects. This paper introduces the Deep Panoptic Context Aggregation Network (PanCAN), a novel approach that hierarchically integrates multi-order geometric contexts through cross-scale feature aggregation in a high-dimensional Hilbert space. Specifically, PanCAN learns multi-order neighborhood relationships at each scale by combining random walks with an attention mechanism. Modules from different scales are cascaded, where salient anchors at a finer scale are selected and their neighborhood features are dynamically fused via attention. This enables effective cross-scale modeling that significantly enhances complex scene understanding by combining multi-order and cross-scale context-aware features. Extensive multi-label classification experiments on NUS-WIDE, PASCAL VOC2007, and MS-COCO benchmarks demonstrate that PanCAN consistently achieves competitive results, outperforming state-of-the-art techniques in both quantitative and qualitative evaluations, thereby substantially improving multi-label classification performance.

</details>


### [280] [IdentityStory: Taming Your Identity-Preserving Generator for Human-Centric Story Generation](https://arxiv.org/abs/2512.23519)
*Donghao Zhou,Jingyu Lin,Guibao Shen,Quande Liu,Jialin Gao,Lihao Liu,Lan Du,Cunjian Chen,Chi-Wing Fu,Xiaowei Hu,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: The paper introduces IdentityStory, a model for generating human-centric stories with consistent character identities across sequential images.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of maintaining detailed face consistency and coordinating multiple characters in human-centric story generation.

Method: Proposed a framework called IdentityStory with two components: Iterative Identity Discovery and Re-denoising Identity Injection, ensuring character identity consistency across images.

Result: Experiments on the ConsiStory-Human benchmark show improved face consistency and support for multi-character scenarios compared to existing methods.

Conclusion: IdentityStory effectively addresses human face consistency challenges in story generation and has promising applications, such as infinite-length stories and dynamic character compositions.

Abstract: Recent visual generative models enable story generation with consistent characters from text, but human-centric story generation faces additional challenges, such as maintaining detailed and diverse human face consistency and coordinating multiple characters across different images. This paper presents IdentityStory, a framework for human-centric story generation that ensures consistent character identity across multiple sequential images. By taming identity-preserving generators, the framework features two key components: Iterative Identity Discovery, which extracts cohesive character identities, and Re-denoising Identity Injection, which re-denoises images to inject identities while preserving desired context. Experiments on the ConsiStory-Human benchmark demonstrate that IdentityStory outperforms existing methods, particularly in face consistency, and supports multi-character combinations. The framework also shows strong potential for applications such as infinite-length story generation and dynamic character composition.

</details>


### [281] [Iterative Inference-time Scaling with Adaptive Frequency Steering for Image Super-Resolution](https://arxiv.org/abs/2512.23532)
*Hexin Zhang,Dong Li,Jie Huang,Bingzhou Wang,Xueyang Fu,Zhengjun Zha*

Main category: cs.CV

TL;DR: This paper introduces IAFS, a novel framework that effectively balances perceptual quality and structural fidelity in image super-resolution, outperforming current methods.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models for image super-resolution often fail to balance perceptual quality and structural fidelity effectively. Current inference-time scaling methods either lead to over-smoothing or loss of structure.

Method: The authors propose Iterative Diffusion Inference-Time Scaling with Adaptive Frequency Steering (IAFS), a training-free method that uses iterative refinement and frequency-aware particle fusion to balance perceptual and structural quality.

Result: IAFS consistently improves the perceptual and structural quality of generated images, outperforming other inference-time scaling methods through extensive experiments across multiple SR models.

Conclusion: IAFS successfully resolves the perception-fidelity conflict in diffusion-based image super-resolution, achieving superior results without requiring additional training.

Abstract: Diffusion models have become a leading paradigm for image super-resolution (SR), but existing methods struggle to guarantee both the high-frequency perceptual quality and the low-frequency structural fidelity of generated images. Although inference-time scaling can theoretically improve this trade-off by allocating more computation, existing strategies remain suboptimal: reward-driven particle optimization often causes perceptual over-smoothing, while optimal-path search tends to lose structural consistency. To overcome these difficulties, we propose Iterative Diffusion Inference-Time Scaling with Adaptive Frequency Steering (IAFS), a training-free framework that jointly leverages iterative refinement and frequency-aware particle fusion. IAFS addresses the challenge of balancing perceptual quality and structural fidelity by progressively refining the generated image through iterative correction of structural deviations. Simultaneously, it ensures effective frequency fusion by adaptively integrating high-frequency perceptual cues with low-frequency structural information, allowing for a more accurate and balanced reconstruction across different image details. Extensive experiments across multiple diffusion-based SR models show that IAFS effectively resolves the perception-fidelity conflict, yielding consistently improved perceptual detail and structural accuracy, and outperforming existing inference-time scaling methods.

</details>


### [282] [AnyMS: Bottom-up Attention Decoupling for Layout-guided and Training-free Multi-subject Customization](https://arxiv.org/abs/2512.23537)
*Binhe Yu,Zhen Wang,Kexin Li,Yuqian Yuan,Wenqiao Zhang,Long Chen,Juncheng Li,Jun Xiao,Yueting Zhuang*

Main category: cs.CV

TL;DR: AnyMS introduces a training-free framework for layout-guided multi-subject customization, resolving issues of text alignment, subject identity, and layout control.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle to balance text alignment, subject identity preservation, and layout control while their dependency on training hinders scalability.

Method: AnyMS integrates text, image, and layout inputs using a dual-level attention decoupling mechanism and employs pre-trained image adapters to eliminate additional training.

Result: The framework achieves superior performance in complex compositions and can scale efficiently to handle numerous subjects.

Conclusion: AnyMS offers an efficient, scalable solution for multi-subject customization with state-of-the-art results and bypasses the need for additional training.

Abstract: Multi-subject customization aims to synthesize multiple user-specified subjects into a coherent image. To address issues such as subjects missing or conflicts, recent works incorporate layout guidance to provide explicit spatial constraints. However, existing methods still struggle to balance three critical objectives: text alignment, subject identity preservation, and layout control, while the reliance on additional training further limits their scalability and efficiency. In this paper, we present AnyMS, a novel training-free framework for layout-guided multi-subject customization. AnyMS leverages three input conditions: text prompt, subject images, and layout constraints, and introduces a bottom-up dual-level attention decoupling mechanism to harmonize their integration during generation. Specifically, global decoupling separates cross-attention between textual and visual conditions to ensure text alignment. Local decoupling confines each subject's attention to its designated area, which prevents subject conflicts and thus guarantees identity preservation and layout control. Moreover, AnyMS employs pre-trained image adapters to extract subject-specific features aligned with the diffusion model, removing the need for subject learning or adapter tuning. Extensive experiments demonstrate that AnyMS achieves state-of-the-art performance, supporting complex compositions and scaling to a larger number of subjects.

</details>


### [283] [PathFound: An Agentic Multimodal Model Activating Evidence-seeking Pathological Diagnosis](https://arxiv.org/abs/2512.23545)
*Shengyi Hua,Jianfeng Wu,Tianle Shen,Kangzhe Hu,Zhongzhen Huang,Shujuan Ni,Zhihong Zhang,Yuan Li,Zhe Wang,Xiaofan Zhang*

Main category: cs.CV

TL;DR: PathFound is a novel multimodal model designed for evidence-seeking inference in pathology, enhancing diagnostic accuracy with a focus on incremental hypothesis refinement.


<details>
  <summary>Details</summary>
Motivation: Pathological diagnosis models mostly rely on static inference, limiting their ability to mimic clinical workflows that refine diagnoses iteratively through evidence-seeking.

Method: PathFound integrates visual foundation models, vision-language models, and reasoning models trained with reinforcement learning, enabling it to actively acquire information and refine diagnoses by following evidence-seeking workflows.

Result: PathFound improves diagnostic accuracy across multiple multimodal models and achieves state-of-the-art performance in various clinical scenarios, including identifying subtle details such as nuclear features.

Conclusion: Evidence-seeking inference workflows, as implemented in PathFound, greatly enhance computational pathology and provide significant potential for refined and accurate diagnosis.

Abstract: Recent pathological foundation models have substantially advanced visual representation learning and multimodal interaction. However, most models still rely on a static inference paradigm in which whole-slide images are processed once to produce predictions, without reassessment or targeted evidence acquisition under ambiguous diagnoses. This contrasts with clinical diagnostic workflows that refine hypotheses through repeated slide observations and further examination requests. We propose PathFound, an agentic multimodal model designed to support evidence-seeking inference in pathological diagnosis. PathFound integrates the power of pathological visual foundation models, vision-language models, and reasoning models trained with reinforcement learning to perform proactive information acquisition and diagnosis refinement by progressing through the initial diagnosis, evidence-seeking, and final decision stages. Across several large multimodal models, adopting this strategy consistently improves diagnostic accuracy, indicating the effectiveness of evidence-seeking workflows in computational pathology. Among these models, PathFound achieves state-of-the-art diagnostic performance across diverse clinical scenarios and demonstrates strong potential to discover subtle details, such as nuclear features and local invasions.

</details>


### [284] [PurifyGen: A Risk-Discrimination and Semantic-Purification Model for Safe Text-to-Image Generation](https://arxiv.org/abs/2512.23546)
*Zongsheng Cao,Yangfan He,Anran Liu,Jun Xie,Feng Chen,Zepeng Wang*

Main category: cs.CV

TL;DR: PurifyGen is a novel, training-free, and plug-and-play method to improve the safety of text-to-image (T2I) generation by purifying risky prompts while retaining the model's original capabilities.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the growing concern of generating unsafe content through T2I models. Existing safety methods, such as blacklisting and harmful content classification, face significant challenges like circumvention and high dataset demands.

Method: PurifyGen uses a dual-stage approach: first, it evaluates token safety through complementary semantic distance and identifies risky tokens. Second, it applies a dual-space transformation to purify risky embeddings, removing toxic semantics and reinforcing safe ones. This is done without retraining and ensures minimal disruption to intended outputs.

Result: PurifyGen was validated across five datasets, showing superior performance in reducing unsafe content compared to existing methods. It also demonstrated strong generalization to unseen prompts and models.

Conclusion: PurifyGen is an effective and theoretically grounded approach for enhancing T2I safety. It addresses limitations of traditional methods and offers a generalizable, lightweight, and efficient solution.

Abstract: Recent advances in diffusion models have notably enhanced text-to-image (T2I) generation quality, but they also raise the risk of generating unsafe content. Traditional safety methods like text blacklisting or harmful content classification have significant drawbacks: they can be easily circumvented or require extensive datasets and extra training. To overcome these challenges, we introduce PurifyGen, a novel, training-free approach for safe T2I generation that retains the model's original weights. PurifyGen introduces a dual-stage strategy for prompt purification. First, we evaluate the safety of each token in a prompt by computing its complementary semantic distance, which measures the semantic proximity between the prompt tokens and concept embeddings from predefined toxic and clean lists. This enables fine-grained prompt classification without explicit keyword matching or retraining. Tokens closer to toxic concepts are flagged as risky. Second, for risky prompts, we apply a dual-space transformation: we project toxic-aligned embeddings into the null space of the toxic concept matrix, effectively removing harmful semantic components, and simultaneously align them into the range space of clean concepts. This dual alignment purifies risky prompts by both subtracting unsafe semantics and reinforcing safe ones, while retaining the original intent and coherence. We further define a token-wise strategy to selectively replace only risky token embeddings, ensuring minimal disruption to safe content. PurifyGen offers a plug-and-play solution with theoretical grounding and strong generalization to unseen prompts and models. Extensive testing shows that PurifyGen surpasses current methods in reducing unsafe content across five datasets and competes well with training-dependent approaches. The code can refer to https://github.com/AI-Researcher-Team/PurifyGen.

</details>


### [285] [RxnBench: A Multimodal Benchmark for Evaluating Large Language Models on Chemical Reaction Understanding from Scientific Literature](https://arxiv.org/abs/2512.23565)
*Hanzheng Li,Xi Fang,Yixuan Li,Chaozheng Huang,Junjie Wang,Xi Wang,Hongzhe Bai,Bojun Hao,Shenyu Lin,Huiqi Liang,Linfeng Zhang,Guolin Ke*

Main category: cs.CV

TL;DR: RxnBench is proposed to benchmark Multimodal Large Language Models (MLLMs) in their ability to understand chemical reactions in scientific PDFs, revealing a gap in deep logical and structural comprehension.


<details>
  <summary>Details</summary>
Motivation: To address the unexplored capability of MLLMs in interpreting complex chemical reactions within authentic scientific literature.

Method: Introduced a benchmark called RxnBench, consisting of two tasks: (1) Single-Figure QA (SF-QA): evaluating visual perception and reasoning with curated reaction schemes and (2) Full-Document QA (FD-QA): testing cross-modal integration of chemical data from scientific articles.

Result: MLLMs perform well in extracting explicit text but struggle with deep logical reasoning and structural recognition, with inference-time reasoning models performing slightly better but not achieving 50% accuracy in FD-QA.

Conclusion: The study highlights the necessity for domain-specific visual encoders and enhanced reasoning capabilities to improve MLLMs' performance in understanding chemical literature.

Abstract: The integration of Multimodal Large Language Models (MLLMs) into chemistry promises to revolutionize scientific discovery, yet their ability to comprehend the dense, graphical language of reactions within authentic literature remains underexplored. Here, we introduce RxnBench, a multi-tiered benchmark designed to rigorously evaluate MLLMs on chemical reaction understanding from scientific PDFs. RxnBench comprises two tasks: Single-Figure QA (SF-QA), which tests fine-grained visual perception and mechanistic reasoning using 1,525 questions derived from 305 curated reaction schemes, and Full-Document QA (FD-QA), which challenges models to synthesize information from 108 articles, requiring cross-modal integration of text, schemes, and tables. Our evaluation of MLLMs reveals a critical capability gap: while models excel at extracting explicit text, they struggle with deep chemical logic and precise structural recognition. Notably, models with inference-time reasoning significantly outperform standard architectures, yet none achieve 50\% accuracy on FD-QA. These findings underscore the urgent need for domain-specific visual encoders and stronger reasoning engines to advance autonomous AI chemists.

</details>


### [286] [ThinkGen: Generalized Thinking for Visual Generation](https://arxiv.org/abs/2512.23568)
*Siyu Jiao,Yiheng Lin,Yujie Zhong,Qi She,Wei Zhou,Xiaohan Lan,Zilong Huang,Fei Yu,Yingchen Yu,Yunqing Zhao,Yao Zhao,Yunchao Wei*

Main category: cs.CV

TL;DR: The paper introduces ThinkGen, a novel framework leveraging Chain-of-Thought reasoning in visual generation tasks, combining a Multimodal Large Language Model (MLLM) with a Diffusion Transformer (DiT) to achieve state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: The authors aim to extend the Chain-of-Thought reasoning approach, which has shown promise in complex understanding tasks, into the realm of visual generation. Current generations lack generalization and adaptation due to scenario-specific limitations.

Method: They propose ThinkGen, a framework combining a pretrained MLLM with a Diffusion Transformer (DiT). It uses MLLM for instruction generation and DiT for image production. Moreover, a separable GRPO-based training paradigm (SepGRPO) alternates reinforcement learning between the MLLM and DiT for joint training across diverse datasets.

Result: ThinkGen demonstrates robust and state-of-the-art performance across multiple generative benchmarks, showcasing its effectiveness in diverse visual generation scenarios.

Conclusion: ThinkGen successfully expands the application of Chain-of-Thought reasoning into the domain of visual generation, offering a generalized and adaptable framework for high-quality image creation. The code is accessible for further research and use.

Abstract: Recent progress in Multimodal Large Language Models (MLLMs) demonstrates that Chain-of-Thought (CoT) reasoning enables systematic solutions to complex understanding tasks. However, its extension to generation tasks remains nascent and limited by scenario-specific mechanisms that hinder generalization and adaptation. In this work, we present ThinkGen, the first think-driven visual generation framework that explicitly leverages MLLM's CoT reasoning in various generation scenarios. ThinkGen employs a decoupled architecture comprising a pretrained MLLM and a Diffusion Transformer (DiT), wherein the MLLM generates tailored instructions based on user intent, and DiT produces high-quality images guided by these instructions. We further propose a separable GRPO-based training paradigm (SepGRPO), alternating reinforcement learning between the MLLM and DiT modules. This flexible design enables joint training across diverse datasets, facilitating effective CoT reasoning for a wide range of generative scenarios. Extensive experiments demonstrate that ThinkGen achieves robust, state-of-the-art performance across multiple generation benchmarks. Code is available: https://github.com/jiaosiyuu/ThinkGen

</details>


### [287] [Image Denoising Using Global and Local Circulant Representation](https://arxiv.org/abs/2512.23569)
*Zhaoming Kong,Xiaowei Yang,Jiahuan Zhang*

Main category: cs.CV

TL;DR: The paper introduces Haar-tSVD, a computationally efficient image denoising method leveraging tensor singular value decomposition and Haar transforms. It achieves effective noise removal and strikes a balance between speed and performance.


<details>
  <summary>Details</summary>
Motivation: Due to the exponential growth of imaging devices and image data, there is a strong need for rapid and precise image denoising solutions.

Method: This work integrates the Haar transform with tensor singular value decomposition (t-SVD), and introduces an adaptive noise estimation scheme. Additionally, deep neural networks are utilized under severe noise conditions.

Result: Experimental evaluations across multiple datasets confirm Haar-tSVD's efficiency and effectiveness in denoising tasks.

Conclusion: The paper presents Haar-tSVD as an innovative and practical denoising solution, balancing computational speed, robustness, and accuracy, supported by strong theoretical connections and experimental validations.

Abstract: The proliferation of imaging devices and countless image data generated every day impose an increasingly high demand on efficient and effective image denoising. In this paper, we establish a theoretical connection between principal component analysis (PCA) and the Haar transform under circulant representation, and present a computationally simple denoising algorithm. The proposed method, termed Haar-tSVD, exploits a unified tensor singular value decomposition (t-SVD) projection combined with Haar transform to efficiently capture global and local patch correlations. Haar-tSVD operates as a one-step, parallelizable plug-and-play denoiser that eliminates the need for learning local bases, thereby striking a balance between denoising speed and performance. Besides, an adaptive noise estimation scheme is introduced to improve robustness according to eigenvalue analysis of the circulant structure. To further enhance the performance under severe noise conditions, we integrate deep neural networks with Haar-tSVD based on the established Haar-PCA relationship. Experimental results on various denoising datasets demonstrate the efficiency and effectiveness of proposed method for noise removal. Our code is publicly available at https://github.com/ZhaomingKong/Haar-tSVD.

</details>


### [288] [ProGuard: Towards Proactive Multimodal Safeguard](https://arxiv.org/abs/2512.23573)
*Shaohan Yu,Lijun Li,Chenyang Si,Lu Sheng,Jing Shao*

Main category: cs.CV

TL;DR: ProGuard is a vision-language guard system that proactively identifies multimodal safety risks without needing model adjustments, trained using reinforcement learning to ensure concise reasoning.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing reactive methods in moderating emerging multimodal safety risks and improve risk detection and description.

Method: ProGuard uses a vision-language model trained on a balanced dataset with binary safety labels and risk categories. Reinforcement learning is applied with a synonym-bank-based similarity reward to improve reasoning and proactive risk descriptions.

Result: ProGuard outperformed existing open-source models in safety categorization and improved OOD risk detection by 52.6% and description by 64.8%.

Conclusion: ProGuard demonstrates strong proactive moderation capabilities, achieving competitive or superior performance against closed and open-source guard models for detecting multimodal safety risks.

Abstract: The rapid evolution of generative models has led to a continuous emergence of multimodal safety risks, exposing the limitations of existing defense methods. To address these challenges, we propose ProGuard, a vision-language proactive guard that identifies and describes out-of-distribution (OOD) safety risks without the need for model adjustments required by traditional reactive approaches. We first construct a modality-balanced dataset of 87K samples, each annotated with both binary safety labels and risk categories under a hierarchical multimodal safety taxonomy, effectively mitigating modality bias and ensuring consistent moderation across text, image, and text-image inputs. Based on this dataset, we train our vision-language base model purely through reinforcement learning (RL) to achieve efficient and concise reasoning. To approximate proactive safety scenarios in a controlled setting, we further introduce an OOD safety category inference task and augment the RL objective with a synonym-bank-based similarity reward that encourages the model to generate concise descriptions for unseen unsafe categories. Experimental results show that ProGuard achieves performance comparable to closed-source large models on binary safety classification, substantially outperforms existing open-source guard models on unsafe content categorization. Most notably, ProGuard delivers a strong proactive moderation ability, improving OOD risk detection by 52.6% and OOD risk description by 64.8%.

</details>


### [289] [LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation](https://arxiv.org/abs/2512.23576)
*Ethan Chern,Zhulin Hu,Bohao Tang,Jiadi Su,Steffi Chern,Zhijie Deng,Pengfei Liu*

Main category: cs.CV

TL;DR: This paper presents an improved real-time video generation model conditioned on multimodal inputs (text, image, audio) with reduced latency and higher efficiency.


<details>
  <summary>Details</summary>
Motivation: Current diffusion-based video generation models face challenges in real-time interaction due to high computational demands and inefficiency in multimodal scenarios.

Method: The authors enhance the distillation process for on-policy optimization, optimizing multimodal conditioning quality and scheduling to address visual artifacts and inefficiencies in prior methods.

Result: The proposed model achieves comparable visual quality to bidirectional baselines with significantly reduced inference cost (20x less) and latency. It is integrated into an interactive avatar system, outperforming existing solutions in coherence and quality, while supporting real-time generation.

Conclusion: This work enables efficient and high-quality real-time multimodal interactive video generation, significantly improving human-AI interaction frameworks.

Abstract: Real-time video generation via diffusion is essential for building general-purpose multimodal interactive AI systems. However, the simultaneous denoising of all video frames with bidirectional attention via an iterative process in diffusion models prevents real-time interaction. While existing distillation methods can make the model autoregressive and reduce sampling steps to mitigate this, they focus primarily on text-to-video generation, leaving the human-AI interaction unnatural and less efficient. This paper targets real-time interactive video diffusion conditioned on a multimodal context, including text, image, and audio, to bridge the gap. Given the observation that the leading on-policy distillation approach Self Forcing encounters challenges (visual artifacts like flickering, black frames, and quality degradation) with multimodal conditioning, we investigate an improved distillation recipe with emphasis on the quality of condition inputs as well as the initialization and schedule for the on-policy optimization. On benchmarks for multimodal-conditioned (audio, image, and text) avatar video generation including HDTF, AVSpeech, and CelebV-HQ, our distilled model matches the visual quality of the full-step, bidirectional baselines of similar or larger size with 20x less inference cost and latency. Further, we integrate our model with audio language models and long-form video inference technique Anchor-Heavy Identity Sinks to build LiveTalk, a real-time multimodal interactive avatar system. System-level evaluation on our curated multi-turn interaction benchmark shows LiveTalk outperforms state-of-the-art models (Sora2, Veo3) in multi-turn video coherence and content quality, while reducing response latency from 1 to 2 minutes to real-time generation, enabling seamless human-AI multimodal interaction.

</details>


### [290] [Same or Not? Enhancing Visual Perception in Vision-Language Models](https://arxiv.org/abs/2512.23592)
*Damiano Marsili,Aditya Mehta,Ryan Y. Lin,Georgia Gkioxari*

Main category: cs.CV

TL;DR: The paper focuses on improving the fine-grained visual understanding of Vision-Language Models (VLMs) by introducing a new dataset, TWIN, and benchmark FGVQA.


<details>
  <summary>Details</summary>
Motivation: Current Vision-Language Models struggle with fine-grained visual perception as existing training datasets are overly focused on generalized categorization rather than nuanced visual details.

Method: The authors present TWIN, a dataset of 561,000 image-pair queries encouraging nuanced attention to detail, and FGVQA, a benchmark of 12,000 fine-grained recognition queries. They fine-tune VLMs on TWIN and evaluate their performance using FGVQA.

Result: Fine-tuning on TWIN improves fine-grained recognition by up to 19.3% without performance trade-offs on general VQA benchmarks. The dataset’s scale significantly impacts these performance gains.

Conclusion: TWIN enhances the perceptual precision of VLMs, making it a valuable addition to open-source training corpora for visual and multi-domain applications.

Abstract: Vision-language models (VLMs) excel at broad visual understanding but remain coarse-grained, exhibit visual biases, and miss subtle visual details. Existing training corpora reinforce this limitation by emphasizing general recognition ("Is it a cat or a dog?") over fine-grained perception. To address this, we introduce a new training corpus and task designed to enhance the perceptual abilities of VLMs. TWIN is a large-scale dataset of 561,000 image-pair queries that task models to determine whether two visually similar images depict the same object, encouraging attention to nuanced visual cues. The dataset spans a diverse range of everyday objects across contexts, viewpoints, and appearances. Fine-tuning VLMs on TWIN yields notable gains in fine-grained recognition, even on unseen domains such as art, animals, plants, and landmarks. To quantify these gains, we introduce FGVQA, a benchmark suite of 12,000 queries that repurposes fine-grained recognition and retrieval datasets from multiple domains. While existing VLMs struggle on FGVQA, when fine-tuned on TWIN they improve by up to 19.3%, without compromising performance on general VQA benchmarks. Finally, our TWIN dataset scales favorably with object annotations, and our analysis shows that scale is key to performance. We envision TWIN as a drop-in addition to open-source VLM training corpora, advancing perceptual precision of future models. Project webpage: https://glab-caltech.github.io/twin/

</details>


### [291] [Detection Fire in Camera RGB-NIR](https://arxiv.org/abs/2512.23594)
*Nguyen Truong Khai,Luong Duc Vinh*

Main category: cs.CV

TL;DR: This paper enhances fire detection accuracy using infrared night vision cameras by introducing an NIR dataset, a two-stage detection model, and Patched-YOLO.


<details>
  <summary>Details</summary>
Motivation: Challenges such as misclassifications of bright artificial lights as fire and dataset limitations hinder the accuracy of fire detection in infrared night vision cameras.

Method: The authors address data scarcity with augmented datasets, utilize a two-stage detection model combining YOLOv11 and EfficientNetV2-B0 to reduce false positives, and develop Patched-YOLO for better detection of small and distant fire objects.

Result: The proposed methods achieve improved detection accuracy, especially in night-time environments, by reducing false positives and enhancing small object detection.

Conclusion: These advancements, particularly through a novel two-stage pipeline and patch-based processing with Patched-YOLO, demonstrate notable improvements in fire detection, tackling previous challenges effectively.

Abstract: Improving the accuracy of fire detection using infrared night vision cameras remains a challenging task. Previous studies have reported strong performance with popular detection models. For example, YOLOv7 achieved an mAP50-95 of 0.51 using an input image size of 640 x 1280, RT-DETR reached an mAP50-95 of 0.65 with an image size of 640 x 640, and YOLOv9 obtained an mAP50-95 of 0.598 at the same resolution. Despite these results, limitations in dataset construction continue to cause issues, particularly the frequent misclassification of bright artificial lights as fire.
  This report presents three main contributions: an additional NIR dataset, a two-stage detection model, and Patched-YOLO. First, to address data scarcity, we explore and apply various data augmentation strategies for both the NIR dataset and the classification dataset. Second, to improve night-time fire detection accuracy while reducing false positives caused by artificial lights, we propose a two-stage pipeline combining YOLOv11 and EfficientNetV2-B0. The proposed approach achieves higher detection accuracy compared to previous methods, particularly for night-time fire detection. Third, to improve fire detection in RGB images, especially for small and distant objects, we introduce Patched-YOLO, which enhances the model's detection capability through patch-based processing. Further details of these contributions are discussed in the following sections.

</details>


### [292] [Scalable Residual Feature Aggregation Framework with Hybrid Metaheuristic Optimization for Robust Early Pancreatic Neoplasm Detection in Multimodal CT Imaging](https://arxiv.org/abs/2512.23597)
*Janani Annur Thiruvengadam,Kiran Mayee Nabigaru,Anusha Kovi*

Main category: cs.CV

TL;DR: This paper proposes a system for early pancreatic neoplasm detection using the Scalable Residual Feature Aggregation (SRFA) framework for enhanced imaging and classification, achieving superior performance metrics.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of detecting pancreatic neoplasms early, which is difficult due to subtle visual cues in CT scans, variable anatomy across patients, and a lack of effective detection systems.

Method: The study introduces SRFA, combining segmentation (MAGRes-UNet), feature extraction (DenseNet-121 with residual feature storage), hybrid feature selection (HHO-BA), a hybrid classifier (ViT with EfficientNet-B3), and dual hyperparameter optimization (SSA and GWO).

Result: The proposed model achieves 96.23% accuracy, 95.58% F1-score, and 94.83% specificity, significantly outperforming CNNs and transformer models.

Conclusion: The SRFA framework demonstrates promise as a robust tool for the early detection of pancreatic tumors, with enhanced imaging and diagnostic precision.

Abstract: The early detection of pancreatic neoplasm is a major clinical dilemma, and it is predominantly so because tumors are likely to occur with minimal contrast margins and a large spread anatomy-wide variation amongst patients on a CT scan. These complexities require to be addressed with an effective and scalable system that can assist in enhancing the salience of the subtle visual cues and provide a high level of the generalization on the multimodal imaging data. A Scalable Residual Feature Aggregation (SRFA) framework is proposed to be used to meet these conditions in this study. The framework integrates a pipeline of preprocessing followed by the segmentation using the MAGRes-UNet that is effective in making the pancreatic structures and isolating regions of interest more visible. DenseNet-121 performed with residual feature storage is used to extract features to allow deep hierarchical features to be aggregated without properties loss. To go further, hybrid HHO-BA metaheuristic feature selection strategy is used, which guarantees the best feature subset refinement. To be classified, the system is trained based on a new hybrid model that integrates the ability to pay attention on the world, which is the Vision Transformer (ViT) with the high representational efficiency of EfficientNet-B3. A dual optimization mechanism incorporating SSA and GWO is used to fine-tune hyperparameters to enhance greater robustness and less overfitting. Experimental results support the significant improvement in performance, with the suggested model reaching 96.23% accuracy, 95.58% F1-score and 94.83% specificity, the model is significantly better than the traditional CNNs and contemporary transformer-based models. Such results highlight the possibility of the SRFA framework as a useful instrument in the early detection of pancreatic tumors.

</details>


### [293] [Memorization in 3D Shape Generation: An Empirical Study](https://arxiv.org/abs/2512.23628)
*Shu Pu,Boya Zeng,Kaichen Zhou,Mengyu Wang,Zhuang Liu*

Main category: cs.CV

TL;DR: This paper investigates memorization in 3D generative models and proposes an evaluation framework to measure memorization. It analyzes factors affecting memorization and suggests strategies to minimize it.


<details>
  <summary>Details</summary>
Motivation: The paper aims to understand how 3D generative models rely on memorized training data, potentially causing data leakage and limiting generation diversity. This understanding can help improve model performance and address potential risks.

Method: The authors introduce a framework to quantify memorization in 3D generative models. They use controlled experiments with a Vecset diffusion model to analyze memorization influencers such as data modality, diversity, and model designs.

Result: Quantifying memorization in existing models reveals dependence on data modality and diversity. Memorization rises with finer conditioning and peaks at moderate guidance scales. Rotation augmentation and longer Vecsets mitigate memorization without affecting quality.

Conclusion: The framework and analysis provide insights into reducing memorization in 3D generative models while preserving generation quality. The study suggests simple strategies for improvement and contributes to understanding memorization across design choices.

Abstract: Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with a latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at a moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at https://github.com/zlab-princeton/3d_mem.

</details>


### [294] [Rethinking the Spatio-Temporal Alignment of End-to-End 3D Perception](https://arxiv.org/abs/2512.23635)
*Xiaoyu Li,Peidong Li,Xian Wu,Long Shi,Dedong Liu,Yitao Wu,Jiajia Fu,Dixiao Cui,Lijun Zhao,Lining Sun*

Main category: cs.CV

TL;DR: The paper introduces HAT, a spatio-temporal alignment module for end-to-end autonomous driving, improving perception and tracking through adaptive hypothesis decoding.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of current alignment methods relying on semantic features, which are suboptimal due to motion state variations.

Method: HAT generates spatial anchors and motion-aware proposals using multiple explicit motion models, then decodes optimal alignment proposals through multi-hypothesis decoding with embedded semantic and motion cues.

Result: HAT improves 3D tracking and detection performance (46.0% AMOTA on nuScenes test set, +1.3% mAP and +3.1% AMOTA in E2E AD), reduces collision rates by 32%, and enhances robustness in corrupted semantic conditions.

Conclusion: HAT significantly refines spatio-temporal alignment, optimizing perception and tracking for end-to-end autonomous driving applications.

Abstract: Spatio-temporal alignment is crucial for temporal modeling of end-to-end (E2E) perception in autonomous driving (AD), providing valuable structural and textural prior information. Existing methods typically rely on the attention mechanism to align objects across frames, simplifying the motion model with a unified explicit physical model (constant velocity, etc.). These approaches prefer semantic features for implicit alignment, challenging the importance of explicit motion modeling in the traditional perception paradigm. However, variations in motion states and object features across categories and frames render this alignment suboptimal. To address this, we propose HAT, a spatio-temporal alignment module that allows each object to adaptively decode the optimal alignment proposal from multiple hypotheses without direct supervision. Specifically, HAT first utilizes multiple explicit motion models to generate spatial anchors and motion-aware feature proposals for historical instances. It then performs multi-hypothesis decoding by incorporating semantic and motion cues embedded in cached object queries, ultimately providing the optimal alignment proposal for the target frame. On nuScenes, HAT consistently improves 3D temporal detectors and trackers across diverse baselines. It achieves state-of-the-art tracking results with 46.0% AMOTA on the test set when paired with the DETR3D detector. In an object-centric E2E AD method, HAT enhances perception accuracy (+1.3% mAP, +3.1% AMOTA) and reduces the collision rate by 32%. When semantics are corrupted (nuScenes-C), the enhancement of motion modeling by HAT enables more robust perception and planning in the E2E AD.

</details>


### [295] [OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding](https://arxiv.org/abs/2512.23646)
*Keda Tao,Wenjie Du,Bohan Yu,Weiqiang Wang,Jian Liu,Huan Wang*

Main category: cs.CV

TL;DR: OmniAgent introduces audio-guided active perception for improved audio-visual reasoning, surpassing existing models significantly.


<details>
  <summary>Details</summary>
Motivation: Current multimodal models lack fine-grained cross-modal understanding and effective audio-visual alignment.

Method: OmniAgent employs dynamic planning, tool orchestration, and a coarse-to-fine audio-guided perception paradigm for reasoning.

Result: OmniAgent achieves state-of-the-art accuracy, outperforming competitors by 10%-20% across three benchmarks.

Conclusion: The approach shifts multimodal interaction from passive response generation to active audio-guided inquiry for advanced reasoning.

Abstract: Omnimodal large language models have made significant strides in unifying audio and visual modalities; however, they often lack the fine-grained cross-modal understanding and have difficulty with multimodal alignment. To address these limitations, we introduce OmniAgent, a fully audio-guided active perception agent that dynamically orchestrates specialized tools to achieve more fine-grained audio-visual reasoning. Unlike previous works that rely on rigid, static workflows and dense frame-captioning, this paper demonstrates a paradigm shift from passive response generation to active multimodal inquiry. OmniAgent employs dynamic planning to autonomously orchestrate tool invocation on demand, strategically concentrating perceptual attention on task-relevant cues. Central to our approach is a novel coarse-to-fine audio-guided perception paradigm, which leverages audio cues to localize temporal events and guide subsequent reasoning. Extensive empirical evaluations on three audio-video understanding benchmarks demonstrate that OmniAgent achieves state-of-the-art performance, surpassing leading open-source and proprietary models by substantial margins of 10% - 20% accuracy.

</details>


### [296] [IDT: A Physically Grounded Transformer for Feed-Forward Multi-View Intrinsic Decomposition](https://arxiv.org/abs/2512.23667)
*Kang Du,Yirui Guan,Zeyu Wang*

Main category: cs.CV

TL;DR: Proposes the Intrinsic Decomposition Transformer (IDT), a new transformer-based method for multi-view intrinsic image decomposition, addressing view inconsistency and enabling interpretable material illumination analysis.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of view inconsistency in multi-view intrinsic image decomposition, leveraging structured decomposition for improved visual understanding.

Method: Introduced the Intrinsic Decomposition Transformer (IDT), a feed-forward transformer framework using attention mechanisms to achieve view-consistent intrinsic decomposition without iterative sampling.

Result: IDT demonstrated superior performance on synthetic and real-world datasets, achieving improvements in diffuse reflectance, coherent shading, and specular component isolation with enhanced multi-view consistency.

Conclusion: IDT offers an interpretable and effective approach for multi-view intrinsic decomposition of images, outperforming existing methods in consistency and factor separation.

Abstract: Intrinsic image decomposition is fundamental for visual understanding, as RGB images entangle material properties, illumination, and view-dependent effects. Recent diffusion-based methods have achieved strong results for single-view intrinsic decomposition; however, extending these approaches to multi-view settings remains challenging, often leading to severe view inconsistency. We propose \textbf{Intrinsic Decomposition Transformer (IDT)}, a feed-forward framework for multi-view intrinsic image decomposition. By leveraging transformer-based attention to jointly reason over multiple input images, IDT produces view-consistent intrinsic factors in a single forward pass, without iterative generative sampling. IDT adopts a physically grounded image formation model that explicitly decomposes images into diffuse reflectance, diffuse shading, and specular shading. This structured factorization separates Lambertian and non-Lambertian light transport, enabling interpretable and controllable decomposition of material and illumination effects across views. Experiments on both synthetic and real-world datasets demonstrate that IDT achieves cleaner diffuse reflectance, more coherent diffuse shading, and better-isolated specular components, while substantially improving multi-view consistency compared to prior intrinsic decomposition methods.

</details>


### [297] [Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation](https://arxiv.org/abs/2512.23705)
*Shaocong Xu,Songlin Wei,Qizhe Wei,Zheng Geng,Hong Li,Licheng Shen,Qianpu Sun,Shu Han,Bin Ma,Bohan Li,Chongjie Ye,Yuhang Zheng,Nan Wang,Saining Zhang,Hao Zhao*

Main category: cs.CV

TL;DR: This paper introduces DKT, a diffusion model-based approach to estimate depth and normals for transparent objects in videos, achieving state-of-the-art zero-shot performance on several benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the challenges traditional perception systems face with transparent objects due to optical complexities like refraction and reflection, leveraging modern video diffusion models' implicit understanding of these phenomena.

Method: The authors developed TransPhy3D, a synthetic video dataset with transparent/reflective scenes, and trained a video diffusion model using this dataset to predict depth and normals. Lightweight LoRA adapters were used to refine the model's video-to-video translating capability.

Result: DKT achieved state-of-the-art performance on both real-world and synthetic benchmarks involving transparent objects, improving accuracy and temporal consistency. It also demonstrated success in integration into robotic grasping tasks.

Conclusion: Diffusion models capture implicit understanding of transparency and can be repurposed for robust, label-free perception, enabling accurate depth prediction for challenging real-world scenarios involving transparent and reflective objects.

Abstract: Transparent objects remain notoriously hard for perception systems: refraction, reflection and transmission break the assumptions behind stereo, ToF and purely discriminative monocular depth, causing holes and temporally unstable estimates. Our key observation is that modern video diffusion models already synthesize convincing transparent phenomena, suggesting they have internalized the optical rules. We build TransPhy3D, a synthetic video corpus of transparent/reflective scenes: 11k sequences rendered with Blender/Cycles. Scenes are assembled from a curated bank of category-rich static assets and shape-rich procedural assets paired with glass/plastic/metal materials. We render RGB + depth + normals with physically based ray tracing and OptiX denoising. Starting from a large video diffusion model, we learn a video-to-video translator for depth (and normals) via lightweight LoRA adapters. During training we concatenate RGB and (noisy) depth latents in the DiT backbone and co-train on TransPhy3D and existing frame-wise synthetic datasets, yielding temporally consistent predictions for arbitrary-length input videos. The resulting model, DKT, achieves zero-shot SOTA on real and synthetic video benchmarks involving transparency: ClearPose, DREDS (CatKnown/CatNovel), and TransPhy3D-Test. It improves accuracy and temporal consistency over strong image/video baselines, and a normal variant sets the best video normal estimation results on ClearPose. A compact 1.3B version runs at ~0.17 s/frame. Integrated into a grasping stack, DKT's depth boosts success rates across translucent, reflective and diffuse surfaces, outperforming prior estimators. Together, these results support a broader claim: "Diffusion knows transparency." Generative video priors can be repurposed, efficiently and label-free, into robust, temporally coherent perception for challenging real-world manipulation.

</details>


### [298] [Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion](https://arxiv.org/abs/2512.23709)
*Hau-Shiang Shiu,Chin-Yang Lin,Zhixiang Wang,Chi-Wei Hsiao,Po-Fan Yu,Yu-Chih Chen,Yu-Lun Liu*

Main category: cs.CV

TL;DR: Stream-DiffVSR is a causal diffusion-based video super-resolution method designed for efficient online use, achieving the lowest latency among similar methods while enhancing perceptual quality.


<details>
  <summary>Details</summary>
Motivation: To address the impracticality of existing diffusion-based video super-resolution methods in latency-sensitive online settings by reducing dependence on future frames and multi-step denoising processes.

Method: It introduces a four-step distilled denoiser for fast inference, an Auto-regressive Temporal Guidance (ARTG) module to align motion cues during denoising, and a lightweight temporal-aware decoder with a Temporal Processor Module (TPM) for improved detail and coherence.

Result: Stream-DiffVSR processes 720p frames in 0.328 seconds on an RTX4090 GPU, outperforms prior diffusion-based methods, boosts perceptual quality compared to the SOTA TMP, and reduces latency significantly.

Conclusion: Stream-DiffVSR's innovations make diffusion-based video super-resolution feasible for low-latency online deployment, achieving strong quality and efficiency improvements.

Abstract: Diffusion-based video super-resolution (VSR) methods achieve strong perceptual quality but remain impractical for latency-sensitive settings due to reliance on future frames and expensive multi-step denoising. We propose Stream-DiffVSR, a causally conditioned diffusion framework for efficient online VSR. Operating strictly on past frames, it combines a four-step distilled denoiser for fast inference, an Auto-regressive Temporal Guidance (ARTG) module that injects motion-aligned cues during latent denoising, and a lightweight temporal-aware decoder with a Temporal Processor Module (TPM) that enhances detail and temporal coherence. Stream-DiffVSR processes 720p frames in 0.328 seconds on an RTX4090 GPU and significantly outperforms prior diffusion-based methods. Compared with the online SOTA TMP, it boosts perceptual quality (LPIPS +0.095) while reducing latency by over 130x. Stream-DiffVSR achieves the lowest latency reported for diffusion-based VSR, reducing initial delay from over 4600 seconds to 0.328 seconds, thereby making it the first diffusion VSR method suitable for low-latency online deployment. Project page: https://jamichss.github.io/stream-diffvsr-project-page/

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [299] [GPU Kernel Optimization Beyond Full Builds: An LLM Framework with Minimal Executable Programs](https://arxiv.org/abs/2512.22147)
*Ruifan Chu,Anbang Wang,Xiuxiu Bai,Shuai Liu,Xiaoshe Dong*

Main category: cs.DC

TL;DR: This paper introduces an end-to-end large language model (LLM) framework that optimizes GPU kernels without requiring full builds or execution, achieving significant performance improvements across various platforms.


<details>
  <summary>Details</summary>
Motivation: High-performance computing faces bottlenecks due to hotspot GPU kernels, and manually optimizing these is costly and not easily portable. Existing LLM methods are inefficient for large applications where compiling and executing kernels fully is expensive.

Method: The authors propose extracting hotspot GPU kernels and transforming them into Minimal Executable Programs (MEPs) for offline optimization. The process includes iterative performance feedback, Automatic Error Repair, and Performance Pattern Inheritance to ensure correctness and enhance strategies like tiling and synchronization.

Result: The proposed method demonstrates substantial speedups: 5.05x (PolyBench on NVIDIA GPUs), 7.77x (PolyBench on Haiguang DCU), 1.77x (AMD APP SDK), and 1.25x on three large-scale application kernels, outperforming direct LLM optimization approaches.

Conclusion: This framework enables practical and adaptable GPU kernel optimization by eliminating the need for full-source dependencies, reducing costs, and improving cross-platform performance optimizations.

Abstract: In high-performance computing, hotspot GPU kernels are primary bottlenecks, and expert manual tuning is costly and hard to port. Large language model methods often assume kernels can be compiled and executed cheaply, which fails in large applications where full builds and runs are expensive. We present an end-to-end LLM framework with performance feedback that optimizes kernels without building the full application. From independently extracted hotspot kernels, it automatically completes code into a Minimal Executable Program (MEP), then performs multi-round iterative optimization and evaluation outside the full application. The framework integrates Automatic Error Repair and Performance Pattern Inheritance to fix faults, preserve correctness, reuse effective tiling/memory/synchronization strategies, and reduce search cost. Optimized variants are reintegrated into the original application for validation. We evaluate on NVIDIA GPUs and the Haiguang Deep Computing Unit (DCU) platform (AMD-licensed architecture) using PolyBench, the AMD APP SDK, and hotspot kernels from large-scale supercomputing applications. The method achieves average speedups of 5.05x (PolyBench on NVIDIA), 7.77x (PolyBench on DCU), 1.77x (AMD APP SDK), and 1.25x on three hotspot kernels, surpassing direct LLM optimization. The approach requires no full-source dependencies, offers cross-platform portability, and enables practical, low-cost GPU kernel optimization.

</details>


### [300] [GPU-Virt-Bench: A Comprehensive Benchmarking Framework for Software-Based GPU Virtualization Systems](https://arxiv.org/abs/2512.22125)
*Jithin VG,Ditto PS*

Main category: cs.DC

TL;DR: The paper introduces GPU-Virt-Bench, a benchmarking framework for evaluating GPU virtualization systems.


<details>
  <summary>Details</summary>
Motivation: The rising demand for efficient GPU resource sharing, especially for AI and LLM workloads, necessitates robust virtualization methods due to hardware-level limitations and lack of standardized evaluations.

Method: The authors developed GPU-Virt-Bench, a framework measuring 56 metrics in 10 categories to assess GPU virtualization systems and compare them with ideal MIG behavior.

Result: GPU-Virt-Bench effectively evaluates systems like HAMi-core, BUD-FCSP, and simulated MIG setups, providing insights into performance characteristics across various dimensions.

Conclusion: The framework enables systematic evaluation, helping optimize GPU resource allocation and selection for multi-tenant environments.

Abstract: The proliferation of GPU-accelerated workloads, particularly in artificial intelligence and large language model (LLM) inference, has created unprecedented demand for efficient GPU resource sharing in cloud and container environments. While NVIDIA's Multi-Instance GPU (MIG) technology provides hardware-level isolation, its availability is limited to high-end datacenter GPUs. Software-based virtualization solutions such as HAMi-core and BUD-FCSP offer alternatives for broader GPU families but lack standardized evaluation methodologies. We present GPU-Virt-Bench, a comprehensive benchmarking framework that evaluates GPU virtualization systems across 56 performance metrics organized into 10 categories. Our framework measures overhead, isolation quality, LLM-specific performance, memory bandwidth, cache behavior, PCIe throughput, multi-GPU communication, scheduling efficiency, memory fragmentation, and error recovery. GPU-Virt-Bench enables systematic comparison between software virtualization approaches and ideal MIG behavior, providing actionable insights for practitioners deploying GPU resources in multi-tenant environments. We demonstrate the framework's utility through evaluation of HAMi-core, BUD-FCSP, and simulated MIG baselines, revealing performance characteristics critical for production deployment decisions.

</details>


### [301] [SoDA: An Efficient Interaction Paradigm for the Agentic Web](https://arxiv.org/abs/2512.22135)
*Zicai Cui,Zhouyuan Jian,Weiwen Liu,Weinan Zhang*

Main category: cs.DC

TL;DR: The paper introduces a new user sovereignty interaction paradigm for the Agentic Web, emphasizing efficiency by shifting from explicit user instructions to intent-aligned actions, and overcoming challenges of data lock-in and cognitive overload through the Sovereign Digital Avatar (SoDA).


<details>
  <summary>Details</summary>
Motivation: The motivation is to resolve challenges posed by the evolving internet, such as data lock-in and cognitive overload, and to enable a user-centric system where user memory and interaction are independent of platform monopolies.

Method: The authors developed a Sovereign Digital Avatar (SoDA) that utilizes a decoupled design of storage, computation, and interaction. They supplemented it with an Intent-Permission Handshake Mechanism to address risk governance.

Result: The SoDA paradigm reduced cross-platform token consumption by 27-35%, decreased user cognitive load by 72% (versus RAG architectures) and by 88% (versus manual processes), and increased the Information Signal-to-Noise Ratio significantly.

Conclusion: The paper concludes that SoDA can act as a core interaction infrastructure for the Agentic Web, addressing key challenges while promoting decentralization and operational efficiency.

Abstract: As the internet evolves from the mobile App-dominated Attention Economy to the Intent-Interconnection of the Agentic Web era, existing interaction modes fail to address the escalating challenges of data lock-in and cognitive overload. Addressing this, we defines a future-oriented user sovereignty interaction paradigm, aiming to realize a fundamental shift from killing time to saving time. Specifically, we argue that decoupling memory from application logic eliminates the structural basis of data lock-in, while shifting from explicit manual instruction to implicit intent alignment resolves cognitive overload by offloading execution complexity. This paradigm is implemented via the Sovereign Digital Avatar (SoDA), which employs an orthogonal decoupling design of storage, computation, and interaction. This establishes the architectural principle of data as a persistent asset, model as a transient tool, fundamentally breaking the platform monopoly on user memory. To support the operation of this new paradigm in zero-trust environments, we design an Intent-Permission Handshake Mechanism based on A2A protocols, utilizing dual-factor (Sensitivity Coefficient and Strictness Parameter) adaptive routing to achieve active risk governance. Empirical evaluation with a high-fidelity simulation environment indicates that this paradigm reduces token consumption by approximately 27-35\% during cross-platform service migration and complex task execution. Furthermore, in the orchestration of multi-modal complex tasks, it reduces user cognitive load by 72\% compared to standard Retrieval-Augmented Generation (RAG) architectures, by 88\% relative to manual workflows, while significantly boosting the Information Signal-to-Noise Ratio (SNR). These results demonstrate that the SoDA is the essential interaction infrastructure for building an efficient, low-friction, and decentralized Agentic Web.

</details>


### [302] [Local Rendezvous Hashing: Bounded Loads and Minimal Churn via Cache-Local Candidates](https://arxiv.org/abs/2512.23434)
*Yongjie Guan*

Main category: cs.DC

TL;DR: Local Rendezvous Hashing (LRH) improves load balance and speeds up consistent hashing in distributed systems by restricting hash selection to a cache-local window without excessive memory traffic.


<details>
  <summary>Details</summary>
Motivation: Existing consistent hashing schemes suffer from high load imbalance or high memory access costs, necessitating improvement in efficiency and balance.

Method: LRH uses a ring-based scheme with a cache-local window for hash selection, binary search for key lookup, and fixed-candidate filtering to minimize churn during topology changes.

Result: LRH achieves better load balance (Max/Avg load near 1.0947) and significantly faster performance (60.05 Mkeys/s, 6.8x faster than traditional methods).

Conclusion: LRH offers an efficient, scalable, and balanced hashing approach, reducing computational and access costs while improving system performance.

Abstract: Consistent hashing is fundamental to distributed systems, but ring-based schemes can exhibit high peak-to-average load ratios unless they use many virtual nodes, while multi-probe methods improve balance at the cost of scattered memory accesses. This paper introduces Local Rendezvous Hashing (LRH), which preserves a token ring but restricts Highest Random Weight (HRW) selection to a cache-local window of C distinct neighboring physical nodes. LRH locates a key by one binary search, enumerates exactly C distinct candidates using precomputed next-distinct offsets, and chooses the HRW winner (optionally weighted). Lookup cost is O(log|R| + C). Under fixed-topology liveness changes, fixed-candidate filtering remaps only keys whose original winner is down, yielding zero excess churn. In a benchmark with N=5000, V=256 (|R|=1.28M), K=50M and C=8, LRH reduces Max/Avg load from 1.2785 to 1.0947 and achieves 60.05 Mkeys/s, about 6.8x faster than multi-probe consistent hashing with 8 probes (8.80 Mkeys/s) while approaching its balance (Max/Avg 1.0697). A microbenchmark indicates multi-probe assignment is dominated by repeated ring searches and memory traffic rather than probe-generation arithmetic.

</details>


### [303] [SlimEdge: Lightweight Distributed DNN Deployment on Constrained Hardware](https://arxiv.org/abs/2512.22136)
*Mahadev Sunil Kumar,Arnab Raha,Debayan Das,Gopakumar G,Amitava Mukherjee*

Main category: cs.DC

TL;DR: Efficient deployment of deep distributed networks (DNNs) for edge devices is achieved via structured model pruning and multi-objective optimization, balancing accuracy and hardware limitations.


<details>
  <summary>Details</summary>
Motivation: High computational demands of DNNs hinder their deployment in resource-limited edge devices despite their central role in computer vision.

Method: Integrates structured model pruning with multi-objective optimization to adapt network capacity for heterogeneous constraints. Used MVCNN as an example for 3D object recognition.

Result: Achieved reduction in inference latency (1.2x to 5.0x) while satisfying accuracy and memory limitations across varying hardware platforms.

Conclusion: Performance-aware, view-adaptive compression enables deployment of complex vision models effectively in distributed edge environments.

Abstract: Deep distributed networks (DNNs) have become central to modern computer vision, yet their deployment on resource-constrained edge devices remains hindered by substantial parameter counts and computational demands. Here, we present an approach to the efficient deployment of distributed DNNs that jointly respects hardware limitations and preserves task performance. Our method integrates a structured model pruning with a multi-objective optimization to tailor network capacity to heterogeneous device constraints. We demonstrate this framework using Multi-View Convolutional Neural Network (MVCNN), a state-of-the-art architecture for 3D object recognition, by quantifying the contribution of individual views to classification accuracy and allocating pruning budgets, respectively. Experimental results show that the resulting models satisfy user-specified bounds on accuracy and memory footprint while reducing inference latency by factors ranging from 1.2x to 5.0x across diverse hardware platforms. These findings suggest that performance-aware, view-adaptive compression provides a viable pathway for deploying complex vision models in distributed edge environments.

</details>


### [304] [Optimal Configuration of API Resources in Cloud Native Computing](https://arxiv.org/abs/2512.23494)
*Eddy Truyen,Wouter Joosen*

Main category: cs.DC

TL;DR: This paper explores applying offline performance optimization to microservices during the DevOps Release phase, focusing on CPU and memory resource configurations.


<details>
  <summary>Details</summary>
Motivation: Optimization during the Release phase is underexplored, despite its importance to prevent inefficiencies in resource allocation before deployment, as opposed to relying solely on Ops phase mechanisms like auto-scaling.

Method: A performance optimization framework is applied to TeaStore application to explore trade-offs between optimization algorithms, using statistical comparisons and upfront factor screening.

Result: Factor screening reduces search space for optimal resource configurations, proving useful when managing limited sampling budgets. Bayesian optimization performs better without screening for near-optimal outcomes.

Conclusion: Applying upfront fine-tuning in the Release phase can improve resource configurations; using the right optimization approach depends on specific goals like cost efficiency or accuracy in achieving optimal configurations.

Abstract: This paper presents how an existing framework for offline performance optimization can be applied to microservice applications during the Release phase of the DevOps life cycle. Optimization of resource allocation configuration parameters for CPU and memory during the Release phase remains a largely unexplored problem as most research has focused on intelligent scheduling and autoscaling of microservices during the Ops stage of the DevOps cycle. Yet horizontal auto-scaling of containers, based on CPU usage for instance, may still leave these containers with an inappropriately allocated amount of memory, if no upfront fine-tuning of both resources is applied before the Deployment phase. We evaluate the performance optimization framework using the TeaStore microservice application and statistically compare different optimization algorithms, supporting informed decisions about their trade-offs between sampling cost and distance to the optimal resource configuration. This shows that upfront factor screening, for reducing the search space, is helpful when the goal is to find the optimal resource configuration with an affordable sampling budget. When the goal is to statistically compare different algorithms, screening must also be applied to make data collection of all data points in the search space feasible.  If the goal is to find a near-optimal configuration, however, it is better to run bayesian optimization without screening.

</details>


### [305] [HLS4PC: A Parametrizable Framework For Accelerating Point-Based 3D Point Cloud Models on FPGA](https://arxiv.org/abs/2512.22139)
*Amur Saqib Pal,Muhammad Mohsin Ghaffar,Faisal Shafait,Christian Weis,Norbert Wehn*

Main category: cs.DC

TL;DR: The paper introduces HLS4PC, an FPGA acceleration framework for efficient processing of 3D point cloud models, achieving greater computational efficiency and throughput.


<details>
  <summary>Details</summary>
Motivation: High computational and memory demands of 3D point cloud classification/segmentation models hinder their real-time processing, especially in safety-critical applications.

Method: The authors develop HLS4PC, an FPGA-based framework leveraging hardware-aware optimizations like parameter quantization, layer fusion, and input-point pruning to optimize PointMLP models.

Result: PointMLP-Lite (a simplified PointMLP model) achieves a 4x reduction in complexity with only 2% accuracy drop. FPGA implementation delivers 3.56x, 2.3x, and 22x higher throughput than prior works, GPU, and CPU executions, respectively.

Conclusion: HLS4PC demonstrates the feasibility and advantages of leveraging FPGA for 3D point cloud data processing in terms of throughput and real-time performance.

Abstract: Point-based 3D point cloud models employ computation and memory intensive mapping functions alongside NN layers for classification/segmentation, and are executed on server-grade GPUs. The sparse, and unstructured nature of 3D point cloud data leads to high memory and computational demand, hindering real-time performance in safety critical applications due to GPU under-utilization. To address this challenge, we present HLS4PC, a parameterizable HLS framework for FPGA acceleration. Our approach leverages FPGA parallelization and algorithmic optimizations to enable efficient fixed-point implementations of both mapping and NN functions. We explore several hardware-aware compression techniques on a state-of-the-art PointMLP-Elite model, including replacing FPS with URS, parameter quantization, layer fusion, and input-points pruning, yielding PointMLP-Lite, a 4x less complex variant with only 2% accuracy drop on ModelNet40. Secondly, we demonstrate that the FPGA acceleration of the PointMLP-Lite results in 3.56x higher throughput than previous works. Furthermore, our implementation achieves 2.3x and 22x higher throughput compared to the GPU and CPU implementations, respectively.

</details>


### [306] [HybridFlow: Adaptive Task Scheduling for Fast and Token-Efficient LLM Inference in Edge-Cloud Collaboration](https://arxiv.org/abs/2512.22137)
*Jiangwen Dong,Jiayu Li,Wanyu Lin*

Main category: cs.DC

TL;DR: HybridFlow is a framework that splits complex queries into subtasks for edge-cloud collaborative inference, reducing latency and token usage while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Large language models have high inference latency and token consumption, making real-time deployment challenging for resource-limited edge devices.

Method: HybridFlow uses (1) dynamic task decomposition and (2) resource-aware subtask routing to optimize edge-cloud collaboration.

Result: HybridFlow decreases inference time and token usage while sustaining competitive accuracy, as confirmed through evaluations on multiple benchmarks.

Conclusion: HybridFlow offers an efficient edge-cloud collaborative solution for LLM inference by dynamically decomposing and routing tasks based on resource constraints.

Abstract: Large language models (LLMs) exhibit impressive reasoning and problem-solving abilities, yet their substantial inference latency and token consumption pose major challenges for real-time deployment on resource-limited edge devices. Recent efforts toward edge-cloud collaboration have attempted to mitigate this issue, but most existing methods adopt coarse-grained task allocation strategies-assigning entire queries either to the edge or the cloud. Such rigid partitioning fails to exploit fine-grained reasoning parallelism and often leads to redundant computation and inefficient resource utilization. To this end, we propose HybridFlow, a resource-adaptive inference framework that enables fast and token-efficient collaborative reasoning between edge and cloud LLMs. HybridFlow operates in two stages: (1) task decomposition and parallel execution, which dynamically splits a complex query into interdependent subtasks that can execute as soon as their dependencies are resolved; and (2) resource-aware subtask routing, where a learned router adaptively assigns each subtask to the edge or cloud model according to predicted utility gains and real-time budget states. Comprehensive evaluations on GPQA, MMLU-Pro, AIME, and LiveBench-Reasoning demonstrate that HybridFlow effectively reduces end-to-end inference time and overall token usage while maintaining competitive accuracy.

</details>


### [307] [On Harnessing Idle Compute at the Edge for Foundation Model Training](https://arxiv.org/abs/2512.22142)
*Leyang Xue,Meghana Madhyastha,Myungjin Lee,Amos Storkey,Randal Burns,Mahesh K. Marina*

Main category: cs.DC

TL;DR: Cleave introduces a decentralized alternative for training large foundation models on edge devices. It employs novel hybrid tensor parallelism and optimization techniques to match cloud training performance while addressing device limitations and heterogeneity.


<details>
  <summary>Details</summary>
Motivation: To provide a democratized alternative to the centralized and resource-intensive foundation model training on cloud servers by leveraging decentralized edge devices.

Method: The Cleave paradigm utilizes selective hybrid tensor parallelism, parameter server centric training, and a cost optimization model to efficiently distribute training workloads across heterogeneous edge devices while minimizing communication overhead.

Result: Cleave achieves cloud-level performance, supports up to 8x more devices compared to edge-training baselines, achieves 10x faster per-batch training time, and recovers from device failures 100x faster than prior approaches.

Conclusion: Cleave demonstrates decentralized and scalable edge-device-based training as a feasible alternative to centralized cloud training, making it efficient, resilient, and capable of handling large-scaled models.

Abstract: The ecosystem behind foundation model development today is highly centralized and limited to large-scale cloud data center operators: training foundation models is costly, needing immense compute resources. Decentralized foundation model training across edge devices, leveraging their spare compute, promises a democratized alternative. However, existing edge-training approaches fall short: they struggle to match cloud-based training performance, exhibit limited scalability with model size, exceed device memory capacity, and have prohibitive communication overhead. They also fail to satisfactorily handle device heterogeneity and dynamism.
  We introduce a new paradigm, Cleave, which finely partitions training operations through a novel selective hybrid tensor parallelism method. Together with a parameter server centric training framework, Cleave copes with device memory limits and avoids communication bottlenecks, thereby enabling efficient training of large models on par with the cloud. Further, with a cost optimization model to guide device selection and training workload distribution, Cleave effectively accounts for device heterogeneity and churn.
  Our evaluations show that Cleave matches cloud-based GPU training by scaling efficiently to larger models and thousands of devices, supporting up to 8x more devices than baseline edge-training approaches. It outperforms state-of-the-art edge training methods by up to a factor of 10 in per-batch training time and efficiently handles device failures, achieving at least 100x faster recovery than prior methods.

</details>


### [308] [Adaptive GPU Resource Allocation for Multi-Agent Collaborative Reasoning in Serverless Environments](https://arxiv.org/abs/2512.22149)
*Guilin Zhang,Wulan Guo,Ziqi Tan*

Main category: cs.DC

TL;DR: The paper proposes an adaptive GPU resource allocation framework for serverless multi-agent systems, achieving significant improvements in latency and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of deploying multi-agent systems with large language models on serverless GPU platforms, especially in handling heterogeneous workloads, efficient scaling, and cost-effectiveness.

Method: Uses an O(N) complexity algorithm to dynamically allocate GPU resources based on workload characteristics, agent priorities, and minimum requirements, tackling dynamic fluctuations and heterogeneous agent demands.

Result: The adaptive framework reduces latency by 85% over round-robin scheduling and achieves comparable throughput to static allocation, as demonstrated in realistic simulations with heterogeneous agents.

Conclusion: Adaptive GPU allocation enhances cost-efficiency and performance in serverless environments, making it a practical solution for multi-agent AI deployments.

Abstract: Multi-agent systems powered by large language models have emerged as a promising paradigm for solving complex reasoning tasks through collaborative intelligence. However, efficiently deploying these systems on serverless GPU platforms presents significant resource allocation challenges due to heterogeneous agent workloads, varying computational demands, and the need for cost-effective scaling. This paper presents an adaptive GPU resource allocation framework that achieves 85\% latency reduction compared to round-robin scheduling while maintaining comparable throughput to static allocation, using an $O(N)$ complexity algorithm for real-time adaptation. Our approach dynamically allocates GPU resources based on workload characteristics, agent priorities, and minimum resource requirements, enabling efficient utilization while maintaining quality of service. The framework addresses three key challenges: (1) heterogeneous computational demands across lightweight coordinators and heavyweight specialists, (2) dynamic workload fluctuations requiring millisecond-scale reallocation, and (3) capacity constraints in serverless environments. Through comprehensive simulations modeling realistic multi-agent workflows with four heterogeneous agents, we demonstrate that adaptive allocation outperforms static equal and round-robin strategies across latency, cost, and GPU utilization metrics. The framework provides a practical solution for deploying cost-efficient multi-agent AI systems on serverless GPU infrastructure.

</details>


### [309] [TL: Automatic End-to-End Compiler of Tile-Based Languages for Spatial Dataflow Architectures](https://arxiv.org/abs/2512.22168)
*Wei Li,Zhenyu Bai,Heru Wang,Pranav Dangi,Zhiqiang Zhang,Cheng Tan,Huiying Lan,Weng-Fai Wong,Tulika Mitra*

Main category: cs.DC

TL;DR: This paper introduces TL, a framework that optimizes the compilation of tile-based programs for spatial dataflow accelerators, overcoming key programmability challenges.


<details>
  <summary>Details</summary>
Motivation: Spatial dataflow accelerators have significant potential for improving performance, cost-efficiency, and energy efficiency in computing. However, their programmability challenges hinder wider adoption, particularly when mapping workloads efficiently across the hardware.

Method: The paper presents TL, an end-to-end framework that focuses on optimizing the distribution of tile instances across spatially distributed cores. It integrates hardware descriptions into an MLIR-based ecosystem for enhanced programmability and optimization.

Result: TL allows for specialized architecture-specific optimizations, facilitates effective use of on-chip networks and distributed memories, and supports a wide array of spatial dataflow architectures.

Conclusion: The TL framework addresses critical programmability issues, enhancing wider usability of spatial dataflow accelerators and unlocking their potential for high-performance and efficient computing.

Abstract: Spatial dataflow accelerators are a promising direction for next-generation computer systems because they can reduce the memory bottlenecks of traditional von Neumann machines such as CPUs and GPUs. They do so by organizing computation around explicit, compiler-managed data movement over the on-chip network, allowing operands to be directly forwarded between processing elements and reducing reliance on high-latency, bandwidth-limited global shared memory. Such localized communications can provide higher throughput and efficiency compared to repeated off-chip memory accesses. However, their end-to-end performance depends strongly on how workloads are mapped to the hardware. Naive mappings can perform very poorly, and most users rely on hand-tuned vendor libraries. In practice, although existing spatial-dataflow accelerators have strong potential for high performance, energy- and cost-efficiency, their limited programmability remains a major barrier to their wider adoption. This paper presents TL, an end-to-end framework that compiles tile-based programs (such as Triton kernels) onto spatial dataflow architectures. Unlike most existing compiler frameworks that focus on optimizing code generation within a single tile, TL addresses the central challenge of distributing tile instances across spatially distributed cores and exploiting the on-chip network and distributed memories to increase data reuse and reduce communications. TL proposes a hardware representation that captures interconnect topology, memory hierarchy, and compute capabilities, enabling both specialized architecture-specific optimizations and support for diverse spatial dataflow targets. TL is built on the MLIR ecosystem and defines a generic entry point for different front-ends and an end point for different back-ends.

</details>


### [310] [AiiDAlab: on the route to accelerate science](https://arxiv.org/abs/2512.22173)
*Aliaksandr V. Yakutovich,Jusong Yu,Daniel Hollas,Edan Bainglass,Corsin Battaglia,Miki Bonacci,Lucas Fernandez Vilanova,Stephan Henne,Anders Kaestner,Michel Kenzelmann,Graham Kimbell,Jakob Lass,Fabio Lopes,Daniel G. Mazzone,Andres Ortega-Guerrero,Xing Wang,Nicola Marzari,Carlo A. Pignedoli,Giovanni Pizzi*

Main category: cs.DC

TL;DR: AiiDAlab is a platform that facilitates complex computational workflows with a user-friendly interface accessible through web browsers. It has evolved to support various disciplines and ensures reproducibility through simulation provenance tracking.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges of executing complex computational workflows that require expertise in simulation setups, remote code execution, and outputs interpretation.

Method: The AiiDAlab platform was designed with an intuitive web interface that leverages the AiiDA engine for tracking simulation provenance and simplifying computational tasks.

Result: The platform has expanded from computational materials science to support diverse disciplines like quantum chemistry and battery research, while optimizing user onboarding, dataset handling, and integration with tools like ELNs.

Conclusion: AiiDAlab has matured into a versatile research platform that facilitates reproducible and accessible science across multiple fields, adhering to FAIR principles and fostering Open Research Data practices.

Abstract: With the availability of ever-increasing computational capabilities, robust and automated research workflows are essential to enable and facilitate the execution and orchestration of large numbers of interdependent simulations in supercomputer facilities. However, the execution of these workflows still typically requires technical expertise in setting up calculation inputs, interpreting outputs, and handling the complexity of parallel code execution on remote machines. To address these challenges, the AiiDAlab platform was developed, making complex computational workflows accessible through an intuitive user interface that runs in a web browser. Here, we discuss how AiiDAlab has matured over the past few years, shifting its focus from computational materials science to become a powerful platform that accelerates scientific discovery across multiple disciplines. Thanks to its design, AiiDAlab allows scientists to focus on their research rather than on computational details and challenges, while keeping automatically track of the full simulation provenance via the underlying AiiDA engine and thus ensuring reproducibility. In particular, we discuss its adoption into quantum chemistry, atmospheric modeling, battery research, and even experimental data analysis at large-scale facilities, while also being actively used in educational settings. Driven by user feedback, significant effort has been made to simplify user onboarding, streamline access to computational resources, and provide robust mechanisms to work with large datasets. Furthermore, AiiDAlab is being integrated with electronic laboratory notebooks (ELNs), reinforcing adherence to the FAIR principles and supporting researchers in data-centric scientific disciplines in easily generating reproducible Open Research Data (ORD).

</details>


### [311] [Mirage Persistent Kernel: A Compiler and Runtime for Mega-Kernelizing Tensor Programs](https://arxiv.org/abs/2512.22219)
*Xinhao Cheng,Zhihao Zhang,Yu Zhou,Jianan Ji,Jinchen Jiang,Zepeng Zhao,Ziruo Xiao,Zihao Ye,Yingyi Huang,Ruihang Lai,Hongyi Jin,Bohan Hou,Mengdi Wu,Yixin Dong,Anthony Yip,Zihao Ye,Songting Wang,Wenqin Yang,Xupeng Miao,Tianqi Chen,Zhihao Jia*

Main category: cs.DC

TL;DR: MPK is a compiler and runtime system designed for efficient multi-GPU model inference by transforming multiple kernels into a single high-performance kernel with decentralized scheduling.


<details>
  <summary>Details</summary>
Motivation: To address performance inefficiencies in multi-GPU model inference by enabling high-performance kernel fusion with minimal developer effort, achieving near hardware-limit performance.

Method: Developed MPK compiler and runtime system that transforms tensor programs into SM-level task graphs and implements decentralized scheduling within a single mega-kernel.

Result: MPK reduces inference latency by up to 1.7x compared to existing systems, significantly improving performance close to hardware limits.

Conclusion: MPK simplifies high-performance GPU optimizations, achieving enhanced inference performance while maintaining developer flexibility, and is publicly available for use.

Abstract: We introduce Mirage Persistent Kernel (MPK), the first compiler and runtime system that automatically transforms multi-GPU model inference into a single high-performance megakernel. MPK introduces an SM-level graph representation that captures data dependencies at the granularity of individual streaming multiprocessors (SMs), enabling cross-operator software pipelining, fine-grained kernel overlap, and other previously infeasible GPU optimizations. The MPK compiler lowers tensor programs into highly optimized SM-level task graphs and generates optimized CUDA implementations for all tasks, while the MPK in-kernel parallel runtime executes these tasks within a single mega-kernel using decentralized scheduling across SMs. Together, these components provide end-to-end kernel fusion with minimal developer effort, while preserving the flexibility of existing programming models. Our evaluation shows that MPK significantly outperforms existing kernel-per-operator LLM serving systems by reducing end-to-end inference latency by up to 1.7x, pushing LLM inference performance close to hardware limits. MPK is publicly available at https://github.com/mirage-project/mirage.

</details>


### [312] [BitFlipScope: Scalable Fault Localization and Recovery for Bit-Flip Corruptions in LLMs](https://arxiv.org/abs/2512.22174)
*Muhammad Zeeshan Karamat,Sadman Saif,Christiana Chamon Garcia*

Main category: cs.DC

TL;DR: BitFlipScope is a framework for identifying and localizing faults in Large Language Models, ensuring fault-resilience and enabling recovery without retraining.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) in safety-critical applications are vulnerable to hardware faults or attacks, which may compromise their functionality and behavior. Identifying and localizing such faults is crucial for diagnosing and recovering affected models efficiently.

Method: BitFlipScope utilizes differential analysis (with a clean reference model) and residual-path perturbation combined with loss-sensitivity profiling (without a reference model) to pinpoint corrupted regions in transformer-based LLMs.

Result: The framework can localize faults effectively in both deployment scenarios and enables lightweight performance recovery without requiring costly fine-tuning or retraining.

Conclusion: BitFlipScope advances fault diagnosis and recovery capabilities, promoting fault-resilient deployment of LLMs in environments prone to hardware faults or adversarial attacks.

Abstract: Large Language Models (LLMs) deployed in practical and safety-critical settings are increasingly susceptible to bit-flip faults caused by hardware degradation, cosmic radiation, or deliberate fault-injection attacks such as Rowhammer. These faults silently corrupt internal parameters and can lead to unpredictable or dangerous model behavior. Localizing these corruptions is essential: without identifying the affected region, it is impossible to diagnose the source of degradation, apply targeted corrective measures, or restore model functionality without resorting to costly fine-tuning or full retraining. This work introduces BitFlipScope, a scalable, software-based framework for identifying fault-affected regions within transformer architectures under two deployment scenarios. When a clean reference model is available, BitFlipScope performs differential analysis of outputs, hidden states, and internal activations for detecting anomalous behavior indicative of corruption to pinpoint or localize faults. When no reference model exists, it uses residual-path perturbation and loss-sensitivity profiling to infer the fault-impacted region directly from the corrupted model. In both settings, the framework not only enables effective fault diagnosis but also supports lightweight performance recovery without fine-tuning, offering a practical path to restoring corrupted models. Together, these capabilities make BitFlipScope an important step toward trustworthy, fault-resilient LLM deployment in hardware-prone and adversarial environments.

</details>


### [313] [iOS as Acceleration](https://arxiv.org/abs/2512.22180)
*Alexander K. Chen*

Main category: cs.DC

TL;DR: This paper explores using modern iOS devices for machine learning tasks in restricted computing environments, proposing pipeline parallelism for distributed workloads.


<details>
  <summary>Details</summary>
Motivation: Address the barrier of limited local computing setups for machine learning tasks where cloud computing isn't feasible due to cost, privacy, or environment constraints.

Method: Employ pipeline parallelism to utilize iOS devices for distributed machine learning tasks, overcoming challenges like memory and thermal restrictions.

Result: Achieved notable improvements in model training, batch inference, and agentic LRM tool-usage using iOS devices in constrained environments.

Conclusion: Mobile devices like iOS phones can significantly contribute to machine learning tasks in weaker local compute environments, opening new avenues for utilization.

Abstract: Practical utilization of large-scale machine learning requires a powerful compute setup, a necessity which poses a significant barrier to engagement with such artificial intelligence in more restricted system environments. While cloud computing offers a solution to weaker local environments, certain situations like training involving private or sensitive data, physical environments not available through the cloud, or higher anticipated usage costs, necessitate computing locally. We explore the potential to improve weaker local compute systems at zero additional cost by taking advantage of ubiquitous yet underutilized resources: mobile phones. Specifically, recent iOS phones are equipped with surprisingly powerful processors, but they also face limitations like memory constraints, thermal throttling, and OS sandboxing. We present a proof-of-concept system demonstrating a novel approach to harness an iOS device via distributed pipeline parallelism, achieving significant benefits in a lesser compute environment by accelerating modest model training, batch inference, and agentic LRM tool-usage. We discuss practical use-cases, limitations, and directions for future work. The findings of this paper highlight the potential for the improving commonplace mobile devices to provide greater contributions to machine learning.

</details>


### [314] [MatKV: Trading Compute for Flash Storage in LLM Inference](https://arxiv.org/abs/2512.22195)
*Kun-Woo Shin,Jay H. Park,Moonwook Oh,Yohan Jo,Jaeyoung Do,Sang-Won Lee*

Main category: cs.DC

TL;DR: MatKV enhances inference efficiency in RAG systems by precomputing and storing key-value vectors, leading to halved power and time consumption.


<details>
  <summary>Details</summary>
Motivation: Inference cost and energy consumption in generative AI now outweigh training costs, especially in RAG setups where processing long inputs is inefficient.

Method: MatKV precomputes key-value vectors of RAG documents, stores them in flash memory, and retrieves them during inference, avoiding expensive GPU recomputation.

Result: Experimental results show MatKV reduces inference time and power consumption by 50% without significantly altering accuracy in tasks like question-answering.

Conclusion: MatKV improves cost-effectiveness, power efficiency, and accessibility for generative AI systems, even on lower-end hardware.

Abstract: We observe two major trends in LLM-based generative AI: (1) inference is becoming the dominant factor in terms of cost and power consumption, surpassing training, and (2) retrieval augmented generation (RAG) is becoming prevalent. When processing long inputs in RAG, the prefill phase of computing the key-value vectors of input text is energy-intensive and time-consuming even with high-end GPUs. Thus, it is crucial to make the prefill phase in RAG inference efficient. To address this issue, we propose MatKV, a scheme that precomputes the key-value vectors (KVs) of RAG objects (e.g., documents), materializes them in inexpensive but fast and power-efficient flash storage, and reuses them at inference time instead of recomputing the KVs using costly and power-inefficient GPU. Experimental results using Hugging Face's Transformers library across state-of-the-art GPUs and flash memory SSDs confirm that, compared to full KV computation on GPUs, MatKV reduces both inference time and power consumption by half for RAG workloads, without severely impacting accuracy in the question-answering task. Furthermore, we demonstrate that MatKV enables additional optimizations in two ways. First, a GPU can decode text while simultaneously loading the materialized KVs for the next instance, reducing load latency. Second, since decoding speed is less sensitive to GPU performance than KV computation, low-end GPUs can be leveraged for decoding without significantly compromising speed once the materialized KVs are loaded into GPU memory. These findings underscore MatKV's potential to make large-scale generative AI applications more cost-effective, power-efficient, and accessible across a wider range of tasks and hardware environments.

</details>


### [315] [SPUMA: a minimally invasive approach to the GPU porting of OPENFOAM](https://arxiv.org/abs/2512.22215)
*Simone Bnà,Giuseppe Giaquinto,Ettore Fadiga,Tommaso Zanelli,Francesco Bottau*

Main category: cs.DC

TL;DR: This paper introduces SPUMA, a full GPU porting of OPENFOAM for NVIDIA and AMD GPUs, showcasing significant computational efficiency and energy reduction in HPC tasks targeting CFD applications.


<details>
  <summary>Details</summary>
Motivation: To address programmability challenges in open-source GPU computing for CFD applications and leverage modern accelerator capabilities in HPC environments.

Method: A portable programming model with a memory pool manager utilizing GPUs' unified memory, tested on LUMI and Leonardo clusters with AMD MI250X and NVIDIA A100 GPUs.

Result: Achieved up to 90% efficiency using GPU solvers and demonstrated GPU scalability, outperforming CPUs in computational power and reducing energy consumption by up to 82%.

Conclusion: SPUMA significantly improves OPENFOAM's GPU effectiveness, providing substantial scalability and energy efficiency advantages, paving the way for enhanced CFD applications in hybrid HPC clusters.

Abstract: High Performance Computing (HPC) on hybrid clusters represents a significant opportunity for Computational Fluid Dynamics (CFD), especially when modern accelerators are utilized effectively. However, despite the widespread adoption of GPUs, programmability remains a challenge, particularly in open-source contexts. In this paper, we present SPUMA, a full GPU porting of OPENFOAM targeting NVIDIA and AMD GPUs. The implementation strategy is based on a portable programming model and the adoption of a memory pool manager that leverages the unified memory feature of modern GPUs. This approach is discussed alongside several numerical tests conducted on two pre-exascale clusters in Europe, LUMI and Leonardo, which host AMD MI250X and NVIDIA A100 GPUs, respectively. In the performance analysis section, we present results related to memory usage profiling and kernel wall-time, the impact of the memory pool, and energy consumption obtained by simulating the well-known DrivAer industrial test case. GPU utilization strongly affects strong scalability results, reaching 65% efficiency on both LUMI and Leonardo when approaching a load of 8 million cells per GPU. Weak scalability results, obtained on 20 GPUs with the OpenFOAM native multigrid solver, range from 75% on Leonardo to 85% on LUMI. Notably, efficiency is no lower than 90% when switching to the NVIDIA AmgX linear algebra solver. Our tests also reveal that one A100 GPU on Leonardo is equivalent 200-300 Intel Sapphire Rapids cores, provided the GPUs are sufficiently oversubscribed (more than 10 million of cells per GPU). Finally, energy consumption is reduced by up to 82% compared to analogous simulations executed on CPUs.

</details>


### [316] [Scalable Cloud-Native Architectures for Intelligent PMU Data Processing](https://arxiv.org/abs/2512.22231)
*Nachiappan Chockalingam,Akshay Deshpande,Lokesh Butra,Ram Sekhar Bodala,Nitin Saksena,Adithya Parthasarathy,Balakrishna Pothineni,Akash Kumar Agarwal*

Main category: cs.DC

TL;DR: The paper proposes a scalable, cloud-native PMU data processing architecture using AI, edge, and cloud computing to ensure low-latency, reliable, and scalable grid analytics.


<details>
  <summary>Details</summary>
Motivation: To address challenges in latency, scalability, and reliability that arise from the increasing data volume and velocity generated by widespread PMU deployments in modern power grids.

Method: The paper introduces a scalable framework that integrates AI, edge, and cloud computing with distributed stream processing, containerized microservices, elastic resource orchestration, and machine learning for time-series analysis.

Result: The proposed system achieves sub-second response times and scalability for large PMU deployments, with demonstrated reliability, throughput, and latency improvements.

Conclusion: The architecture offers a robust, flexible foundation for smart grid analytics, embedding security and privacy mechanisms for critical infrastructure.

Abstract: Phasor Measurement Units (PMUs) generate high-frequency, time-synchronized data essential for real-time power grid monitoring, yet the growing scale of PMU deployments creates significant challenges in latency, scalability, and reliability. Conventional centralized processing architectures are increasingly unable to handle the volume and velocity of PMU data, particularly in modern grids with dynamic operating conditions. This paper presents a scalable cloud-native architecture for intelligent PMU data processing that integrates artificial intelligence with edge and cloud computing. The proposed framework employs distributed stream processing, containerized microservices, and elastic resource orchestration to enable low-latency ingestion, real-time anomaly detection, and advanced analytics. Machine learning models for time-series analysis are incorporated to enhance grid observability and predictive capabilities. Analytical models are developed to evaluate system latency, throughput, and reliability, showing that the architecture can achieve sub-second response times while scaling to large PMU deployments. Security and privacy mechanisms are embedded to support deployment in critical infrastructure environments. The proposed approach provides a robust and flexible foundation for next-generation smart grid analytics.

</details>


### [317] [Efficient Multi-Model Orchestration for Self-Hosted Large Language Models](https://arxiv.org/abs/2512.22402)
*Bhanu Prakash Vangala,Tanu Malik*

Main category: cs.DC

TL;DR: The paper introduces 'Pick and Spin,' a Kubernetes-based framework to enable efficient and cost-effective self-hosting of large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: Organizations seek privacy, cost control, and model customization by self-hosting LLMs, but this comes with significant challenges in GPU utilization, workload management, and reliability.

Method: The framework utilizes Kubernetes with a Helm-based deployment system, adaptive scale-to-zero automation, and hybrid routing employing keyword heuristics and a lightweight DistilBERT classifier.

Result: 'Pick and Spin' demonstrated up to 21.6% higher success rates, 30% lower latency, and 33% lower GPU cost per query compared with conventional static deployments.

Conclusion: 'Pick and Spin' makes self-hosting LLMs more practical, scalable, and economical, addressing key challenges faced by organizations.

Abstract: Self-hosting large language models (LLMs) is increasingly appealing for organizations seeking privacy, cost control, and customization. Yet deploying and maintaining in-house models poses challenges in GPU utilization, workload routing, and reliability. We introduce Pick and Spin, a practical framework that makes self-hosted LLM orchestration scalable and economical. Built on Kubernetes, it integrates a unified Helm-based deployment system, adaptive scale-to-zero automation, and a hybrid routing module that balances cost, latency, and accuracy using both keyword heuristics and a lightweight DistilBERT classifier. We evaluate four models, Llama-3 (90B), Gemma-3 (27B), Qwen-3 (235B), and DeepSeek-R1 (685B) across eight public benchmark datasets, with five inference strategies, and two routing variants encompassing 31,019 prompts and 163,720 inference runs. Pick and Spin achieves up to 21.6% higher success rates, 30% lower latency, and 33% lower GPU cost per query compared with static deployments of the same models.

</details>


### [318] [Nightjar: Dynamic Adaptive Speculative Decoding for Large Language Models Serving](https://arxiv.org/abs/2512.22420)
*Rui Li,Zhaoning Zhang,Libo Zhang,Huaimin Wang,Xiang Fu,Zhiquan Lai*

Main category: cs.DC

TL;DR: Nightjar introduces a learning-based adaptive algorithm to optimize speculative decoding in LLM inference, improving throughput and latency.


<details>
  <summary>Details</summary>
Motivation: Speculative decoding improves performance but struggles to adapt to dynamic system loads, resulting in inefficiencies under varying scenarios.

Method: Nightjar employs a learning-based approach to dynamically adjust speculative length based on request load and batch sizes, optimizing performance.

Result: Nightjar improves throughput by up to 14.8% and reduces latency by up to 20.2% compared to existing speculative decoding methods.

Conclusion: Nightjar can dynamically optimize speculative decoding, offering robust efficiency for real-time LLM serving under a wide range of loads.

Abstract: Speculative decoding (SD) accelerates LLM inference by verifying draft tokens in parallel. However, this method presents a critical trade-off: it improves throughput in low-load, memory-bound systems but degrades performance in high-load, compute-bound environments due to verification overhead. Current SD implementations use a fixed speculative length, failing to adapt to dynamic request rates and creating a significant performance bottleneck in real-world serving scenarios. To overcome this, we propose Nightjar, a novel learning-based algorithm for adaptive speculative inference that adjusts to request load by dynamically selecting the optimal speculative length for different batch sizes and even disabling speculative decoding when it provides no benefit. Experiments show that Nightjar achieves up to 14.8% higher throughput and 20.2% lower latency compared to standard speculative decoding, demonstrating robust efficiency for real-time serving.

</details>


### [319] [Role-Based Fault Tolerance System for LLM RL Post-Training](https://arxiv.org/abs/2512.22492)
*Zhenqian Chen,Baoquan Zhong,Xiang Li,Qing Dai,Xinkui Zhao,Miao Ye,Ren Cheng,Lufei Zhang,Jianwei Yin*

Main category: cs.DC

TL;DR: The paper presents RobustRL, a system designed to improve fault tolerance and efficiency in RL post-training for LLMs by recovering only failed roles instead of restarting entire tasks, enhancing performance and reducing training time.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of existing fault tolerance frameworks for LLMs, which do not optimize asynchronous execution in RL environments and treat failures either during training or inference without isolating faults effectively.

Method: The method includes role-based fault isolation and robust recovery mechanisms: (1) Implement role-aware monitoring, (2) enable non-disruptive recovery methods for trainers and rollouts, and (3) replace static communication with dynamic, UCX-based point-to-point communication for efficient role reconnection.

Result: RobustRL demonstrated significant improvements, achieving over 80% Effective Training Time Ratio under high failure rates and enhancing end-to-end training time by 8.4-17.4% compared to ByteRobust.

Conclusion: RobustRL effectively handles GPU machine errors during RL post-training, reducing the impact of failures and optimizing training efficiency by isolating and recovering failed roles without full-task restarts.

Abstract: RL post-training for LLMs has been widely scaled to enhance reasoning and tool-using capabilities. However, RL post-training interleaves training and inference workloads, exposing the system to faults from both sides. Existing fault tolerance frameworks for LLMs target either training or inference, leaving the optimization potential in the asynchronous execution unexplored for RL. Our key insight is role-based fault isolation so the failure in one machine does not affect the others. We treat trainer, rollout, and other management roles in RL training as distinct distributed sub-tasks. Instead of restarting the entire RL task in ByteRobust, we recover only the failed role and reconnect it to living ones, thereby eliminating the full-restart overhead including rollout replay and initialization delay.
  We present RobustRL, the first comprehensive robust system to handle GPU machine errors for RL post-training Effective Training Time Ratio improvement. (1) \textit{Detect}. We implement role-aware monitoring to distinguish actual failures from role-specific behaviors to avoid the false positive and delayed detection. (2) \textit{Restart}. For trainers, we implement a non-disruptive recovery where rollouts persist state and continue trajectory generation, while the trainer is rapidly restored via rollout warm standbys. For rollout, we perform isolated machine replacement without interrupting the RL task. (3) \textit{Reconnect}. We replace static collective communication with dynamic, UCX-based (Unified Communication X) point-to-point communication, enabling immediate weight synchronization between recovered roles. In an RL training task on a 256-GPU cluster with Qwen3-8B-Math workload under 10\% failure injection frequency, RobustRL can achieve an ETTR of over 80\% compared with the 60\% in ByteRobust and achieves 8.4\%-17.4\% faster in end-to-end training time.

</details>


### [320] [Object Abstraction To Streamline Edge-Cloud-Native Application Development](https://arxiv.org/abs/2512.22534)
*Pawissanutt Lertpongrujikorn*

Main category: cs.DC

TL;DR: The paper proposes the Object-as-a-Service (OaaS) paradigm to unify cloud-native application development and infrastructure management.


<details>
  <summary>Details</summary>
Motivation: The gap between the simplicity promised by serverless computing and the practical challenges of fragmented runtime, state management, and orchestration systems.

Method: Grounded in empirical evidence including practitioner interviews, human studies on developer experience, and customer discovery, combined with technical contributions such as prototypes and methodologies.

Result: OaaS achieves negligible overhead, state-of-the-art scalability, faster task completion, reduced code complexity, and commercialization paths validated for technology SMEs and startups.

Conclusion: OaaS simplifies cloud-native development, hides infrastructure complexity, focuses on practitioner needs, and empowers developers to innovate.

Abstract: Cloud computing has fundamentally transformed application development, yet a gap remains between the serverless promise of simplified deployment and its practical realization due to fragmentation across function runtimes, state management, and orchestration. This dissertation addresses this gap through empirical validation and technical innovation, establishing the Object-as-a-Service (OaaS) paradigm as a unified approach to cloud-native development. Grounded in evidence from three studies - practitioner interviews (21 participants), a human study on developer experience (39 participants), and NSF I-Corps customer discovery (101 interviews across 86 organizations) - this work demonstrates that infrastructure complexity taxes productivity, with practitioners prioritizing automation and maintainability over cost optimization. The dissertation makes five major contributions: (1) the OaaS paradigm unifies resource, state, and workflow management via the Oparaca prototype, demonstrating negligible overhead and state-of-the-art scalability; (2) SLA-driven OaaS enables declarative management of non-functional requirements like availability, consistency, and latency; (3) OaaS-IoT with EdgeWeaver extends the paradigm to the edge-cloud continuum, achieving 31% faster task completion and a 44.5% reduction in lines of code compared to traditional FaaS; (4) commercialization validation establishes a pathway targeting technology SMEs and startups; and (5) an empirical methodology for grounding technical research in validated practitioner needs. By consolidating fragmented abstractions and automating performance optimization, OaaS establishes a foundation for cloud-native platforms that hide infrastructure complexity and empower developers to focus on innovation.

</details>


### [321] [RollArt: Scaling Agentic RL Training via Disaggregated Infrastructure](https://arxiv.org/abs/2512.22560)
*Wei Gao,Yuheng Zhao,Tianyuan Wu,Shaopan Xiong,Weixun Wang,Dakai An,Lunxi Cao,Dilxat Muhtar,Zichen Liu,Haizhou Zhao,Ju Huang,Siran Yang,Yongbin Li,Wenbo Su,Jiamang Wang,Lin Qu,Bo Zheng,Wei Wang*

Main category: cs.DC

TL;DR: The paper introduces RollArc, a distributed system addressing efficient agentic reinforcement learning by optimizing infrastructure for LLMs through advanced workload mapping, asynchrony, and stateful computation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle inefficiencies in agentic RL training that arise from heterogeneous workloads and complex stage dependencies, aiming for better hardware utilization and reduced synchronization overhead.

Method: RollArc employs workload mapping to match tasks to suitable GPUs, integrates fine-grained asynchrony to manage execution efficiently, and utilizes statefulness-aware computation to scale components elastically via serverless infrastructure.

Result: RollArc achieves a 1.35-2.05x reduction in training time compared to traditional baselines and demonstrates its scalability by training a massive model on a cluster with 3000+ GPUs.

Conclusion: RollArc significantly enhances the efficiency and scalability of agentic RL training systems while mitigating hardware resource underutilization and synchronization issues.

Abstract: Agentic Reinforcement Learning (RL) enables Large Language Models (LLMs) to perform autonomous decision-making and long-term planning. Unlike standard LLM post-training, agentic RL workloads are highly heterogeneous, combining compute-intensive prefill phases, bandwidth-bound decoding, and stateful, CPU-heavy environment simulations. We argue that efficient agentic RL training requires disaggregated infrastructure to leverage specialized, best-fit hardware. However, naive disaggregation introduces substantial synchronization overhead and resource underutilization due to the complex dependencies between stages.
  We present RollArc, a distributed system designed to maximize throughput for multi-task agentic RL on disaggregated infrastructure. RollArc is built on three core principles: (1) hardware-affinity workload mapping, which routes compute-bound and bandwidth-bound tasks to bestfit GPU devices, (2) fine-grained asynchrony, which manages execution at the trajectory level to mitigate resource bubbles, and (3) statefulness-aware computation, which offloads stateless components (e.g., reward models) to serverless infrastructure for elastic scaling. Our results demonstrate that RollArc effectively improves training throughput and achieves 1.35-2.05\(\times\) end-to-end training time reduction compared to monolithic and synchronous baselines. We also evaluate RollArc by training a hundreds-of-billions-parameter MoE model for Qoder product on an Alibaba cluster with more than 3,000 GPUs, further demonstrating RollArc scalability and robustness. The code is available at https://github.com/alibaba/ROLL.

</details>


### [322] [Modality Inflation: Energy Characterization and Optimization Opportunities for MLLM Inference](https://arxiv.org/abs/2512.22695)
*Mona Moghadampanah,Adib Rezaei Shahmirzadi,Farhana Amin,Dimitrios S. Nikolopoulos*

Main category: cs.DC

TL;DR: This paper investigates energy inefficiencies in multimodal large language models (MLLMs) due to modality inflation, exploring how multimodal inputs increase inference workloads and energy consumption.


<details>
  <summary>Details</summary>
Motivation: To analyze the energy trade-offs introduced by additional modalities in MLLMs, an area poorly understood compared to text-only LLMs, and to provide insights for designing energy-efficient systems.

Method: The paper performs a stage-level analysis of MLLM inference by breaking it into vision encoding, prefill, and decoding stages, examining energy consumption using four representative models on an NVIDIA A100 GPU. It also utilizes dynamic voltage and frequency scaling (DVFS) for optimization.

Result: Energy overheads for multimodal inference range from 17% to 94% across models for identical inputs. It identifies energy bottlenecks in vision encoders or large visual token sequences and notes substantial GPU underutilization, leading to varying energy scaling across models.

Conclusion: Stage-wise DVFS optimization can significantly improve energy efficiency in MLLMs with minimal impact on performance, providing guidance for more energy-efficient multimodal serving systems.

Abstract: Multimodal large language models (MLLMs) are built on text-only LLMs by incorporating additional modalities, enabling multimodal understanding and a broader range of applications. However, these additions introduce a previously unexplored energy trade-off across modalities that remains poorly understood, as most prior work focuses on text-only models. In this paper, we examine modality inflation, a key source of inefficiency in which multimodal inputs increase inference workloads through extra encoding stages and expanded token sequences. We provide the first detailed, stage-level analysis of energy consumption in MLLM inference by breaking the pipeline into vision encoding, prefill, and decoding stages. Using four representative MLLMs evaluated on NVIDIA A100 GPU, we quantify the additional energy required for multimodal inference compared to text-only baselines, observing overheads ranging from 17% to 94% across models for identical inputs. Our results show that energy bottlenecks differ widely across model architectures, stemming either from compute-heavy vision encoders or from the downstream impact of large visual token sequences during prefill. By examining GPU power traces, we further uncover substantial GPU underutilization during multimodal execution and show that input complexity leads to markedly different energy scaling behaviors across models. Finally, we demonstrate that stage-wise dynamic voltage and frequency scaling (DVFS) is an effective optimization, allowing energy savings with only modest performance impact. Together, these findings offer practical insights and concrete guidance for designing more energy-efficient multimodal LLM serving systems.

</details>


### [323] [OptiNIC: A Resilient and Tail-Optimal RDMA NIC for Distributed ML Workloads](https://arxiv.org/abs/2512.22743)
*Ertza Warraich,Ali Imran,Annus Zulfiqar,Shay Vargaftik,Sonia Fahmy,Muhammad Shahbaz*

Main category: cs.DC

TL;DR: OptiNIC introduces a tailored RDMA transport to address latency issues in distributed ML workloads by avoiding strict reliability guarantees such as retransmissions.


<details>
  <summary>Details</summary>
Motivation: Traditional RDMA transport introduces inefficiencies in distributed machine learning workloads due to strict reliability and in-order delivery requirements, causing latency bottlenecks.

Method: OptiNIC revisits RDMA reliability guarantees, removing retransmissions and in-order delivery while leveraging adaptive timeouts and shifting loss recovery to the ML pipeline.

Result: OptiNIC achieves 2x improvement in time-to-accuracy, 1.6x increase in throughput, 3.5x reduction in tail latency, and enhanced hardware efficiency and resilience.

Conclusion: OptiNIC provides a domain-specific solution that optimizes RDMA transport for distributed ML, improving performance and scalability by utilizing ML's tolerance for partial or lost data.

Abstract: As distributed machine learning (ML) workloads scale to thousands of GPUs connected by high-speed interconnects, tail latency in collective communication has become a major bottleneck. Existing RDMA transports, such as RoCE, IRN, SRNIC, and Falcon, enforce strict reliability and in-order delivery, relying on retransmissions and packet sequencing to ensure correctness. While these approaches work well for general-purpose workloads, they introduce complexity and latency that scale poorly in ML, where even rare packet delays can stall entire model pipelines.
  We present OptiNIC, a domain-specific RDMA transport that revisits traditional reliability guarantees based on ML's tolerance for partial or missing data. OptiNIC eliminates retransmissions and in-order delivery from the NIC, enabling a best-effort, out-of-order transport model for RDMA. Unlike traditional RDMA, which signals completion only after complete data delivery, OptiNIC introduces adaptive timeouts to trigger forward progress when data may be lost or delayed. OptiNIC retains standard congestion control mechanisms (e.g., DCQCN, EQDS, or Swift) while shifting loss recovery to the ML pipeline itself (e.g., via the Hadamard Transform and Erasure Coding).
  Our evaluation shows that OptiNIC improves time-to-accuracy (TTA) by 2x and increases throughput by 1.6x for training and inference, respectively, across two public clouds (i.e., Hyperstack and CloudLab). OptiNIC also lowers 99th-percentile latency by 3.5x, cuts BRAM usage by 2.7x, and nearly doubles NIC resilience to faults-delivering a resilient, tail-optimized RDMA transport purpose-built for distributed ML workloads.

</details>


### [324] [Argus: Token Aware Distributed LLM Inference Optimization](https://arxiv.org/abs/2512.22925)
*Panlong Wu,Yifei Zhong,Danyang Chen,Ting Wang,Fangxin Wang*

Main category: cs.DC

TL;DR: The paper introduces Argus, a framework optimizing Large Language Models' (LLMs') inference across edge-cloud systems, addressing token length variability and dynamic environments.


<details>
  <summary>Details</summary>
Motivation: LLMs face significant efficiency issues due to inference time variability caused by their architecture, particularly in dynamic and heterogeneous systems. Current solutions ignore variability in token lengths and device diversity.

Method: The paper proposes the Argus framework with a Length-Aware Semantics (LAS) module for token length prediction and a Lyapunov-guided Offloading Optimization (LOO) module utilizing Iterative Offloading Algorithm with Damping and Congestion Control (IODCC) to optimize task allocation.

Result: Theoretical and experimental evaluations show that Argus delivers robust performance and improved efficiency in diverse, rapidly changing conditions.

Conclusion: Argus demonstrates the viability of token-aware and optimized offloading in LLM inference, enhancing performance and efficiency in heterogeneous edge-cloud environments.

Abstract: Large Language Models (LLMs) are rapidly being integrated into real-world applications, yet their autoregressive architectures introduce significant inference time variability, especially when deployed across heterogeneous edge-cloud systems. Existing solutions largely neglect the dynamic, stochastic, and heterogeneous nature of such environments, often ignoring the impact of variable output token lengths and device diversity. In this work, we present Argus, the first token-aware distributed edge-cloud LLM inference framework that conducts efficient task offloading. Argus features a Length-Aware Semantics (LAS) module, which predicts output token lengths for incoming prompts using a fine-tuned language model with token-length-sensitive feature modulation, enabling precise estimation. Building on this, our Lyapunov-guided Offloading Optimization (LOO) module formulates long-term Quality-of-Experience optimization that explicitly considers both LLM prefilling and decoding costs. We introduce a novel Iterative Offloading Algorithm with Damping and Congestion Control (IODCC) to effectively solve the resulting integer nonlinear programming problem under time-varying constraints. Extensive theoretical and empirical evaluations demonstrate that Argus achieves robust performance and superior efficiency in highly dynamic, heterogeneous settings.

</details>


### [325] [Viability and Performance of a Private LLM Server for SMBs: A Benchmark Analysis of Qwen3-30B on Consumer-Grade Hardware](https://arxiv.org/abs/2512.23029)
*Alex Khalil,Guillaume Heilles,Maria Parraga,Simon Heilles*

Main category: cs.DC

TL;DR: This paper evaluates the feasibility of deploying a high-performance, private LLM solution for SMBs using a locally hosted quantized 30-billion parameter model on consumer-grade hardware.


<details>
  <summary>Details</summary>
Motivation: To address concerns of data privacy, operational sovereignty, and high costs associated with cloud-based LLMs, providing an accessible and private alternative for SMBs.

Method: The paper benchmarks a quantized 30-billion parameter MoE model (based on Qwen3) on a consumer-grade server equipped with a next-gen NVIDIA GPU, evaluating model performance and server efficiency under load.

Result: Findings show that an on-premises setup using emerging consumer hardware and a quantized model delivers performance comparable to cloud-based solutions, ensuring affordability and privacy.

Conclusion: SMBs can deploy high-performance LLMs cost-effectively and securely using locally hosted setups, reducing reliance on expensive cloud services.

Abstract: The proliferation of Large Language Models (LLMs) has been accompanied by a reliance on cloud-based, proprietary systems, raising significant concerns regarding data privacy, operational sovereignty, and escalating costs. This paper investigates the feasibility of deploying a high-performance, private LLM inference server at a cost accessible to Small and Medium Businesses (SMBs). We present a comprehensive benchmarking analysis of a locally hosted, quantized 30-billion parameter Mixture-of-Experts (MoE) model based on Qwen3, running on a consumer-grade server equipped with a next-generation NVIDIA GPU. Unlike cloud-based offerings, which are expensive and complex to integrate, our approach provides an affordable and private solution for SMBs. We evaluate two dimensions: the model's intrinsic capabilities and the server's performance under load. Model performance is benchmarked against academic and industry standards to quantify reasoning and knowledge relative to cloud services. Concurrently, we measure server efficiency through latency, tokens per second, and time to first token, analyzing scalability under increasing concurrent users. Our findings demonstrate that a carefully configured on-premises setup with emerging consumer hardware and a quantized open-source model can achieve performance comparable to cloud-based services, offering SMBs a viable pathway to deploy powerful LLMs without prohibitive costs or privacy compromises.

</details>


### [326] [Bitcoin-IPC: Scaling Bitcoin with a Network of Proof-of-Stake Subnets](https://arxiv.org/abs/2512.23439)
*Marko Vukolić,Orestis Alpos,Jakov Mitrovski,Themis Papameletiou,Nikola Ristić,Dionysis Zindros*

Main category: cs.DC

TL;DR: The paper introduces Bitcoin-IPC, enabling layered scalability of Bitcoin via Layer-2 Proof-of-Stake subnets.


<details>
  <summary>Details</summary>
Motivation: To scale Bitcoin and enhance its functionality as a universal Medium of Exchange (MoE).

Method: Developing Bitcoin-IPC protocol embedded in Bitcoin's SegWit for communication, settlement, and security while linking L2 subnets to Bitcoin L1.

Result: Bitcoin-IPC achieves up to 23x reduced transaction cost and boosts transaction throughput from 7 tps to over 160 tps.

Conclusion: Bitcoin-IPC enhances Bitcoin scalability and transaction efficiency without modifying Bitcoin L1.

Abstract: We introduce Bitcoin-IPC, a software stack and protocol that scales Bitcoin towards helping it become the universal Medium of Exchange (MoE) by enabling the permissionless creation of fully programmable Proof-of-Stake (PoS) Layer-2 chains, called subnets, whose stake is denominated in L1 BTC. Bitcoin-IPC subnets rely on Bitcoin L1 for the communication of critical information, settlement, and security.
  Our design, inspired by SWIFT messaging and embedded within Bitcoin's SegWit mechanism, enables seamless value transfer across L2 subnets, routed through Bitcoin L1. Uniquely, this mechanism reduces the virtual-byte cost per transaction (vB per tx) by up to 23x, compared to transacting natively on Bitcoin L1, effectively increasing monetary transaction throughput from 7 tps to over 160 tps, without requiring any modifications to Bitcoin L1.

</details>


### [327] [Decoupling Adaptive Control in TeaStore](https://arxiv.org/abs/2512.23495)
*Eddy Truyen*

Main category: cs.DC

TL;DR: The paper discusses implementing self-adaptation in the TeaStore microservices case study by analyzing architectural methods, cloud-native Operators, and programming techniques.


<details>
  <summary>Details</summary>
Motivation: To address challenges in achieving system-wide consistency, planning, and modularity in self-adaptive microservices in the TeaStore case study.

Method: Analyzed the decoupling of self-adaptive control logic using architectural methods, the cloud-native Operator pattern, and programming techniques, comparing trade-offs and integration.

Result: Identified trade-offs between detailed adaptation and system-wide control, showing that these approaches can be effectively combined in a multi-tiered architecture.

Conclusion: Different self-adaptive approaches are complementary and can be integrated into a robust architecture for flexible and reusable microservices adaptation.

Abstract: The Adaptable TeaStore specification provides a microservice-based case study for implementing self-adaptation through a control loop.  We argue that implementations of this specification should be informed by key properties of self-adaptation: system-wide consistency (coordinated adaptations across replicas), planning (executing an adaptation until appropriate conditions are met),  and modularity (clean integration of adaptation logic).  In this implementation discussion paper, we examine how software architectural methods, the cloud-native Operator pattern, and legacy programming language techniques can decouple self-adaptive control logic from the TeaStore application. We analyze the trade-offs that these different approaches make between fine-grained expressive adaptation and system-wide control, and highlight when reuse of adaptation strategies is most effective. Our analysis suggests that these approaches are not mutually exclusive but can be combined into a multi-tiered architecture for self-adaptive microservices.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [328] [Pruning Graphs by Adversarial Robustness Evaluation to Strengthen GNN Defenses](https://arxiv.org/abs/2512.22128)
*Yongyu Wang*

Main category: cs.LG

TL;DR: The paper introduces a pruning framework to enhance Graph Neural Networks (GNNs) by removing noise or vulnerable edges using adversarial robustness scores, improving their defense against perturbations.


<details>
  <summary>Details</summary>
Motivation: GNNs are highly impactful for graph-structured data learning but are vulnerable to adversarial attacks and noisy graph structures, necessitating methods to improve their reliability.

Method: The paper uses a pruning framework that identifies fragile graph components using adversarial robustness evaluation and removes the problematic edges to create cleaner graph representations.

Result: Experiments on several benchmark datasets demonstrate that the proposed framework significantly improves GNN performance and robustness under high perturbation scenarios.

Conclusion: Pruning based on robustness scores is an effective strategy to make GNNs more resilient to noise and adversarial conditions while preserving their learning capabilities.

Abstract: Graph Neural Networks (GNNs) have emerged as a dominant paradigm for learning on graph-structured data, thanks to their ability to jointly exploit node features and relational information encoded in the graph topology. This joint modeling, however, also introduces a critical weakness: perturbations or noise in either the structure or the features can be amplified through message passing, making GNNs highly vulnerable to adversarial attacks and spurious connections. In this work, we introduce a pruning framework that leverages adversarial robustness evaluation to explicitly identify and remove fragile or detrimental components of the graph. By using robustness scores as guidance, our method selectively prunes edges that are most likely to degrade model reliability, thereby yielding cleaner and more resilient graph representations. We instantiate this framework on three representative GNN architectures and conduct extensive experiments on benchmarks. The experimental results show that our approach can significantly enhance the defense capability of GNNs in the high-perturbation regime.

</details>


### [329] [KernelEvolve: Scaling Agentic Kernel Coding for Heterogeneous AI Accelerators at Meta](https://arxiv.org/abs/2512.23236)
*Gang Liao,Hongsen Qin,Ying Wang,Alicia Golden,Michael Kuchnik,Yavuz Yetim,Jia Jiunn Ang,Chunli Fu,Yihan He,Samuel Hsia,Zewei Jiang,Dianshi Li,Uladzimir Pashkevich,Varna Puvvada,Feng Shi,Matt Steiner,Ruichao Xiao,Nathan Yan,Xiayu Yu,Zhou Fang,Abdul Zainul-Abedin,Ketan Singh,Hongtao Yu,Wenyuan Chi,Barney Huang,Sean Zhang,Noah Weller,Zach Marine,Wyatt Cook,Carole-Jean Wu,Gaoxiang Liu*

Main category: cs.LG

TL;DR: KernelEvolve is a framework to optimize training and inference in deep learning recommendation models (DLRM) by automating kernel generation across diverse hardware.


<details>
  <summary>Details</summary>
Motivation: The increasing diversity in DLRM architectures and hardware platforms presents challenges in efficiently developing and implementing such models.

Method: KernelEvolve automates kernel optimization using a graph-based search process and operates across programming abstractions, enabling integration with various hardware architectures.

Result: KernelEvolve achieves 100% correctness on tests, accelerates development from weeks to hours, and enhances performance across heterogeneous hardware systems.

Conclusion: KernelEvolve simplifies kernel generation, improves performance, and reduces programmability barriers for AI systems on diverse hardware platforms, including custom AI hardware.

Abstract: Making deep learning recommendation model (DLRM) training and inference fast and efficient is important. However, this presents three key system challenges - model architecture diversity, kernel primitive diversity, and hardware generation and architecture heterogeneity. This paper presents KernelEvolve-an agentic kernel coding framework-to tackle heterogeneity at-scale for DLRM. KernelEvolve is designed to take kernel specifications as input and automate the process of kernel generation and optimization for recommendation model across heterogeneous hardware architectures. KernelEvolve does so by operating at multiple programming abstractions, from Triton and CuTe DSL to low-level hardware agnostic languages, spanning the full hardware-software optimization stack. The kernel optimization process is described as graph-based search with selection policy, universal operator, fitness function, and termination rule, dynamically adapts to runtime execution context through retrieval-augmented prompt synthesis. We designed, implemented, and deployed KernelEvolve to optimize a wide variety of production recommendation models across generations of NVIDIA and AMD GPUs, as well as Meta's AI accelerators. We validate KernelEvolve on the publicly-available KernelBench suite, achieving 100% pass rate on all 250 problems across three difficulty levels, and 160 PyTorch ATen operators across three heterogeneous hardware platforms, demonstrating 100% correctness. KernelEvolve reduces development time from weeks to hours and achieves substantial performance improvements over PyTorch baselines across diverse production use cases and for heterogeneous AI systems at-scale. Beyond performance efficiency improvements, KernelEvolve significantly mitigates the programmability barrier for new AI hardware by enabling automated kernel generation for in-house developed AI hardware.

</details>


### [330] [Towards Unsupervised Causal Representation Learning via Latent Additive Noise Model Causal Autoencoders](https://arxiv.org/abs/2512.22150)
*Hans Jarett J. Ong,Brian Godwin S. Lim,Dominic Dayta,Renzo Roel P. Tan,Kazushi Ikeda*

Main category: cs.LG

TL;DR: The paper introduces LANCA, a model for unsupervised representation learning that uses causality as an inductive bias and achieves better performance on benchmarks compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Overcoming limitations in unsupervised representation learning methods that cannot disentangle causal variables due to lack of supervision or strong inductive biases.

Method: The paper proposes LANCA, a deterministic Wasserstein Auto-Encoder combined with a differentiable ANM Layer, enforcing residual independence as an explicit optimization objective.

Result: LANCA shows superior robustness and performs better than state-of-the-art baselines on synthetic and real-world benchmarks.

Conclusion: LANCA operationalizes causal discovery in unsupervised learning using an Additive Noise Model and demonstrates its effectiveness through improved performance on multiple benchmarks.

Abstract: Unsupervised representation learning seeks to recover latent generative factors, yet standard methods relying on statistical independence often fail to capture causal dependencies. A central challenge is identifiability: as established in disentangled representation learning and nonlinear ICA literature, disentangling causal variables from observational data is impossible without supervision, auxiliary signals, or strong inductive biases. In this work, we propose the Latent Additive Noise Model Causal Autoencoder (LANCA) to operationalize the Additive Noise Model (ANM) as a strong inductive bias for unsupervised discovery. Theoretically, we prove that while the ANM constraint does not guarantee unique identifiability in the general mixing case, it resolves component-wise indeterminacy by restricting the admissible transformations from arbitrary diffeomorphisms to the affine class. Methodologically, arguing that the stochastic encoding inherent to VAEs obscures the structural residuals required for latent causal discovery, LANCA employs a deterministic Wasserstein Auto-Encoder (WAE) coupled with a differentiable ANM Layer. This architecture transforms residual independence from a passive assumption into an explicit optimization objective. Empirically, LANCA outperforms state-of-the-art baselines on synthetic physics benchmarks (Pendulum, Flow), and on photorealistic environments (CANDLE), where it demonstrates superior robustness to spurious correlations arising from complex background scenes.

</details>


### [331] [SoliReward: Mitigating Susceptibility to Reward Hacking and Annotation Noise in Video Generation Reward Models](https://arxiv.org/abs/2512.22170)
*Jiesong Lian,Ruizhe Zhong,Zixiang Zhou,Xiaoyue Mi,Yixue Hao,Yuan Zhou,Qinglin Lu,Long Hu,Junchi Yan*

Main category: cs.LG

TL;DR: SoliReward enhances video generation models' alignment with human preferences using a novel systematic RM training framework.


<details>
  <summary>Details</summary>
Motivation: To address challenges in post-training alignment of video generation models, such as data labeling noise, lack of exploration in VLM-based RM design, and reward hacking.

Method: Proposed a framework with single-item binary annotations, cross-prompt pairing, Hierarchical Progressive Query Attention, and modified BT loss regularization.

Result: Empirical validation shows improvement in RM metrics and post-training efficacy on video generation benchmarks like physical plausibility and semantic alignment.

Conclusion: SoliReward offers robust advancements in RM training methodology, promising better alignment of video generation models with human preferences.

Abstract: Post-training alignment of video generation models with human preferences is a critical goal. Developing effective Reward Models (RMs) for this process faces significant methodological hurdles. Current data collection paradigms, reliant on in-prompt pairwise annotations, suffer from labeling noise. Concurrently, the architectural design of VLM-based RMs, particularly their output mechanisms, remains underexplored. Furthermore, RM is susceptible to reward hacking in post-training. To mitigate these limitations, we propose SoliReward, a systematic framework for video RM training. Our framework first sources high-quality, cost-efficient data via single-item binary annotations, then constructs preference pairs using a cross-prompt pairing strategy. Architecturally, we employ a Hierarchical Progressive Query Attention mechanism to enhance feature aggregation. Finally, we introduce a modified BT loss that explicitly accommodates win-tie scenarios. This approach regularizes the RM's score distribution for positive samples, providing more nuanced preference signals to alleviate over-focus on a small number of top-scoring samples. Our approach is validated on benchmarks evaluating physical plausibility, subject deformity, and semantic alignment, demonstrating improvements in direct RM evaluation metrics and in the efficacy of post-training on video generation models. Code and benchmark will be publicly available.

</details>


### [332] [Wireless Traffic Prediction with Large Language Model](https://arxiv.org/abs/2512.22178)
*Chuanting Zhang,Haixia Zhang,Jingping Qiao,Zongzhang Li,Mohamed-Slim Alouini*

Main category: cs.LG

TL;DR: TIDES is a framework integrating large language models (LLMs) with spatial-temporal insights for wireless traffic prediction, demonstrating superior accuracy and scalability.


<details>
  <summary>Details</summary>
Motivation: The rising need for intelligent and adaptive management of next-gen wireless networks stresses the importance of accurate and scalable wireless traffic prediction, which current models fail to fully address due to neglecting spatial dependencies.

Method: TIDES uses a clustering mechanism to manage heterogeneous traffic patterns, personalized models for regions, a prompt engineering scheme for embedding structured traffic inputs, and a DeepSeek module for spatial alignment via cross-domain attention. It fine-tunes lightweight components to efficiently adapt LLMs to specific domains.

Result: TIDES achieves impressive performance and robustness, surpassing state-of-the-art models, validated through extensive experiments on cellular traffic data.

Conclusion: Incorporating spatial awareness into LLM-based predictors is crucial for scalable and intelligent wireless network management, especially for forthcoming 6G systems.

Abstract: The growing demand for intelligent, adaptive resource management in next-generation wireless networks has underscored the importance of accurate and scalable wireless traffic prediction. While recent advancements in deep learning and foundation models such as large language models (LLMs) have demonstrated promising forecasting capabilities, they largely overlook the spatial dependencies inherent in city-scale traffic dynamics. In this paper, we propose TIDES (Traffic Intelligence with DeepSeek-Enhanced Spatial-temporal prediction), a novel LLM-based framework that captures spatial-temporal correlations for urban wireless traffic prediction. TIDES first identifies heterogeneous traffic patterns across regions through a clustering mechanism and trains personalized models for each region to balance generalization and specialization. To bridge the domain gap between numerical traffic data and language-based models, we introduce a prompt engineering scheme that embeds statistical traffic features as structured inputs. Furthermore, we design a DeepSeek module that enables spatial alignment via cross-domain attention, allowing the LLM to leverage information from spatially related regions. By fine-tuning only lightweight components while freezing core LLM layers, TIDES achieves efficient adaptation to domain-specific patterns without incurring excessive training overhead. Extensive experiments on real-world cellular traffic datasets demonstrate that TIDES significantly outperforms state-of-the-art baselines in both prediction accuracy and robustness. Our results indicate that integrating spatial awareness into LLM-based predictors is the key to unlocking scalable and intelligent network management in future 6G systems.

</details>


### [333] [Latent Sculpting for Zero-Shot Generalization: A Manifold Learning Approach to Out-of-Distribution Anomaly Detection](https://arxiv.org/abs/2512.22179)
*Rajeeb Thapa Chhetri,Zhixiong Chen,Saurab Thapa*

Main category: cs.LG

TL;DR: The paper introduces Latent Sculpting, a representation learning framework, to address the issue of catastrophic failure in supervised models dealing with unseen (Out-of-Distribution) data.


<details>
  <summary>Details</summary>
Motivation: Supervised models fail on Out-of-Distribution (OOD) tabular data due to the lack of topological constraints in latent space. This failure leads to 'Generalization Collapse,' where models cannot differentiate anomalies from benign data.

Method: The proposed Latent Sculpting framework comprises two stages: (1) A 1D-CNN and Transformer Encoder are trained with Dual-Centroid Compactness Loss (DCCL) to shape benign data into a dense hyperspherical cluster, and (2) A Masked Autoregressive Flow (MAF) is conditioned on this structured manifold to estimate density precisely.

Result: Empirical evaluation on the CIC-IDS-2017 benchmark shows superior zero-shot generalization. The method outperformed supervised baselines (F1=0.87 compared to 0.30) and other strong unsupervised baselines (best alternative F1=0.76). It achieved an 88.89% detection rate in challenging 'Infiltration' scenarios, where supervised models failed entirely (0% accuracy).

Conclusion: Sculpting latent manifolds explicitly is essential for robust, generalized anomaly detection. Separating structure learning from density estimation offers a scalable solution for addressing distributional shifts.

Abstract: A fundamental limitation of supervised deep learning in high-dimensional tabular domains is "Generalization Collapse": models learn precise decision boundaries for known distributions but fail catastrophically when facing Out-of-Distribution (OOD) data. We hypothesize that this failure stems from the lack of topological constraints in the latent space, resulting in diffuse manifolds where novel anomalies remain statistically indistinguishable from benign data. To address this, we propose Latent Sculpting, a hierarchical two-stage representation learning framework. Stage 1 utilizes a hybrid 1D-CNN and Transformer Encoder trained with a novel Dual-Centroid Compactness Loss (DCCL) to actively "sculpt" benign traffic into a low-entropy, hyperspherical cluster. Unlike standard contrastive losses that rely on triplet mining, DCCL optimizes global cluster centroids to enforce absolute manifold density. Stage 2 conditions a Masked Autoregressive Flow (MAF) on this pre-structured manifold to learn an exact density estimate. We evaluate this methodology on the rigorous CIC-IDS-2017 benchmark, treating it as a proxy for complex, non-stationary data streams. Empirical results demonstrate that explicit manifold sculpting is a prerequisite for robust zero-shot generalization. While supervised baselines suffered catastrophic performance collapse on unseen distribution shifts (F1 approx 0.30) and the strongest unsupervised baseline achieved only 0.76, our framework achieved an F1-Score of 0.87 on strictly zero-shot anomalies. Notably, we report an 88.89% detection rate on "Infiltration" scenarios--a complex distributional shift where state-of-the-art supervised models achieved 0.00% accuracy. These findings suggest that decoupling structure learning from density estimation provides a scalable path toward generalized anomaly detection.

</details>


### [334] [Learning Tennis Strategy Through Curriculum-Based Dueling Double Deep Q-Networks](https://arxiv.org/abs/2512.22186)
*Vishnu Mohan*

Main category: cs.LG

TL;DR: The paper addresses tennis strategy optimization by integrating a reinforcement learning framework using DDQN and curriculum learning. The developed agent shows high win rates against opponents but showcases a defensive bias.


<details>
  <summary>Details</summary>
Motivation: The motivation is to optimize tennis strategies as a sequential decision-making problem, addressing challenges like hierarchical scoring, stochastic outcomes, fatigue, and adaptation to opponents.

Method: The method employs a custom tennis simulation with a Dueling Double Deep Q-Network (DDQN), using curriculum learning to train the agent against incrementally harder opponents.

Result: The trained agent achieves win rates of 98-100% against balanced opponents and strong performance against tougher opponents, with high serve and return efficiency metrics.

Conclusion: The study highlights the necessity of dueling architectures and curriculum learning for stable convergence, while emphasizing the challenges posed by win-rate driven optimization in realistic sports simulations.

Abstract: Tennis strategy optimization is a challenging sequential decision-making problem involving hierarchical scoring, stochastic outcomes, long-horizon credit assignment, physical fatigue, and adaptation to opponent skill. I present a reinforcement learning framework that integrates a custom tennis simulation environment with a Dueling Double Deep Q-Network(DDQN) trained using curriculum learning. The environment models complete tennis scoring at the level of points, games, and sets, rally-level tactical decisions across ten discrete action categories, symmetric fatigue dynamics, and a continuous opponent skill parameter. The dueling architecture decomposes action-value estimation into state-value and advantage components, while double Q-learning reduces overestimation bias and improves training stability in this long-horizon stochastic domain. Curriculum learning progressively increases opponent difficulty from 0.40 to 0.50, enabling robust skill acquisition without the training collapse observed under fixed opponents. Across extensive evaluations, the trained agent achieves win rates between 98 and 100 percent against balanced opponents and maintains strong performance against more challenging opponents. Serve efficiency ranges from 63.0 to 67.5 percent, and return efficiency ranges from 52.8 to 57.1 percent. Ablation studies demonstrate that both the dueling architecture and curriculum learning are necessary for stable convergence, while a standard DQN baseline fails to learn effective policies. Despite strong performance, tactical analysis reveals a pronounced defensive bias, with the learned policy prioritizing error avoidance and prolonged rallies over aggressive point construction. These results highlight a limitation of win-rate driven optimization in simplified sports simulations and emphasize the importance of reward design for realistic sports reinforcement learning.

</details>


### [335] [Physics-Informed Machine Learning for Transformer Condition Monitoring -- Part II: Physics-Informed Neural Networks and Uncertainty Quantification](https://arxiv.org/abs/2512.22189)
*Jose I. Aizpurua*

Main category: cs.LG

TL;DR: This paper explores the integration of physics-informed machine learning models, specifically Physics-Informed Neural Networks (PINNs) and Bayesian PINNs, for monitoring electrical transformers.


<details>
  <summary>Details</summary>
Motivation: To enhance the monitoring and diagnostic capabilities of electrical transformers by integrating physics-based knowledge and uncertainty quantification with machine learning models.

Method: Researchers utilized Physics-Informed Neural Networks (PINNs) for thermal modeling and aging analysis of solid insulation, and incorporated Bayesian techniques to handle epistemic uncertainty and improve prediction robustness.

Result: The enhancement of predictive capabilities in electrical transformers, even under sparse data, through the use of Bayesian PINNs integrated with physics-informed approaches.

Conclusion: A promising direction for applying physics-aware and uncertainty-focused machine learning models to critical power systems is highlighted.

Abstract: The integration of physics-based knowledge with machine learning models is increasingly shaping the monitoring, diagnostics, and prognostics of electrical transformers. In this two-part series, the first paper introduced the foundations of Neural Networks (NNs) and their variants for health assessment tasks. This second paper focuses on integrating physics and uncertainty into the learning process. We begin with the fundamentals of Physics-Informed Neural Networks (PINNs), applied to spatiotemporal thermal modeling and solid insulation ageing. Building on this, we present Bayesian PINNs as a principled framework to quantify epistemic uncertainty and deliver robust predictions under sparse data. Finally, we outline emerging research directions that highlight the potential of physics-aware and trustworthy machine learning for critical power assets.

</details>


### [336] [Physics-Informed Machine Learning for Transformer Condition Monitoring -- Part I: Basic Concepts, Neural Networks, and Variants](https://arxiv.org/abs/2512.22190)
*Jose I. Aizpurua*

Main category: cs.LG

TL;DR: The paper examines the role of Neural Networks, including CNNs and their integration with RL, in monitoring and managing transformer conditions effectively.


<details>
  <summary>Details</summary>
Motivation: To address reliability challenges in power transformers using modern machine learning techniques.

Method: Introduces and applies Convolutional Neural Networks (CNNs) for analyzing transformer conditions and discusses their integration with Reinforcement Learning for decision-making.

Result: Provides a framework to enhance transformer diagnostics and health management with Neural Network models.

Conclusion: Neural Networks, integrated with emerging AI methods, show promising potential in improving transformer operation and condition management.

Abstract: Power transformers are critical assets in power networks, whose reliability directly impacts grid resilience and stability. Traditional condition monitoring approaches, often rule-based or purely physics-based, struggle with uncertainty, limited data availability, and the complexity of modern operating conditions. Recent advances in machine learning (ML) provide powerful tools to complement and extend these methods, enabling more accurate diagnostics, prognostics, and control. In this two-part series, we examine the role of Neural Networks (NNs) and their extensions in transformer condition monitoring and health management tasks. This first paper introduces the basic concepts of NNs, explores Convolutional Neural Networks (CNNs) for condition monitoring using diverse data modalities, and discusses the integration of NN concepts within the Reinforcement Learning (RL) paradigm for decision-making and control. Finally, perspectives on emerging research directions are also provided.

</details>


### [337] [Graph Neural Networks with Transformer Fusion of Brain Connectivity Dynamics and Tabular Data for Forecasting Future Tobacco Use](https://arxiv.org/abs/2512.23137)
*Runzhi Zhou,Xi Luo*

Main category: cs.LG

TL;DR: The study introduces a GNN-TF model designed to predict future outcomes by integrating longitudinal neuroimaging and tabular clinical data, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: The research addresses the difficulty of integrating non-Euclidean brain imaging data with Euclidean tabular data for forecasting outcomes in longitudinal imaging studies.

Method: The proposed GNN-TF model leverages temporal order by using a graph neural network with transformer fusion to integrate non-Euclidean functional imaging data and Euclidean clinical data.

Result: GNN-TF showed superior predictive accuracy in forecasting tobacco usage in comparison to other machine learning and deep learning models.

Conclusion: The GNN-TF model effectively integrates multiple data modalities and temporal dynamics, proving to be a promising tool for longitudinal functional brain imaging and outcome prediction.

Abstract: Integrating non-Euclidean brain imaging data with Euclidean tabular data, such as clinical and demographic information, poses a substantial challenge for medical imaging analysis, particularly in forecasting future outcomes. While machine learning and deep learning techniques have been applied successfully to cross-sectional classification and prediction tasks, effectively forecasting outcomes in longitudinal imaging studies remains challenging. To address this challenge, we introduce a time-aware graph neural network model with transformer fusion (GNN-TF). This model flexibly integrates both tabular data and dynamic brain connectivity data, leveraging the temporal order of these variables within a coherent framework. By incorporating non-Euclidean and Euclidean sources of information from a longitudinal resting-state fMRI dataset from the National Consortium on Alcohol and Neurodevelopment in Adolescence (NCANDA), the GNN-TF enables a comprehensive analysis that captures critical aspects of longitudinal imaging data. Comparative analyses against a variety of established machine learning and deep learning models demonstrate that GNN-TF outperforms these state-of-the-art methods, delivering superior predictive accuracy for predicting future tobacco usage. The end-to-end, time-aware transformer fusion structure of the proposed GNN-TF model successfully integrates multiple data modalities and leverages temporal dynamics, making it a valuable analytic tool for functional brain imaging studies focused on clinical outcome prediction.

</details>


### [338] [Frequency Regularization: Unveiling the Spectral Inductive Bias of Deep Neural Networks](https://arxiv.org/abs/2512.22192)
*Jiahao Lu*

Main category: cs.LG

TL;DR: This study examines how regularization techniques in deep networks influence feature frequency selection, introducing tools to measure and track this effect.


<details>
  <summary>Details</summary>
Motivation: To understand the underlying physical mechanisms of regularization techniques like L2 regularization and dropout in deep neural networks, focusing on their impact on feature frequency selection.

Method: The study introduces a Visual Diagnostic Framework to analyze weight frequency dynamics and proposes the Spectral Suppression Ratio (SSR) metric to quantify regularizers' low-pass filtering effects. Empirical tests were conducted using ResNet-18 and the CIFAR-10 dataset.

Result: L2 regularization effectively suppresses high-frequency accumulation in weights (3x reduction compared to unregularized models) and demonstrates advantages in robustness against high-frequency information loss, achieving 6% better performance in blurred scenarios.

Conclusion: Regularization techniques provide a spectral inductive bias favoring low-frequency structures, influencing model generalization, and presenting an accuracy-robustness trade-off.

Abstract: Regularization techniques such as L2 regularization (Weight Decay) and Dropout are fundamental to training deep neural networks, yet their underlying physical mechanisms regarding feature frequency selection remain poorly understood. In this work, we investigate the Spectral Bias of modern Convolutional Neural Networks (CNNs). We introduce a Visual Diagnostic Framework to track the dynamic evolution of weight frequencies during training and propose a novel metric, the Spectral Suppression Ratio (SSR), to quantify the "low-pass filtering" intensity of different regularizers. By addressing the aliasing issue in small kernels (e.g., 3x3) through discrete radial profiling, our empirical results on ResNet-18 and CIFAR-10 demonstrate that L2 regularization suppresses high-frequency energy accumulation by over 3x compared to unregularized baselines. Furthermore, we reveal a critical Accuracy-Robustness Trade-off: while L2 models are sensitive to broadband Gaussian noise due to over-specialization in low frequencies, they exhibit superior robustness against high-frequency information loss (e.g., low resolution), outperforming baselines by >6% in blurred scenarios. This work provides a signal-processing perspective on generalization, confirming that regularization enforces a strong spectral inductive bias towards low-frequency structures.

</details>


### [339] [Emotion-Inspired Learning Signals (EILS): A Homeostatic Framework for Adaptive Autonomous Agents](https://arxiv.org/abs/2512.22200)
*Dhruv Tiwari*

Main category: cs.LG

TL;DR: The paper proposes Emotion-Inspired Learning Signals (EILS) as a framework for improving AI agents' robustness and adaptability. EILS uses continuous, bio-inspired internal feedback to enhance exploration, adaptation, and stability in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: Modern AI methods rely heavily on static, external reward functions, which limit agent autonomy in dynamic real-world environments, causing issues like poor exploration, lack of adaptation, and extensive manual tuning.

Method: The paper introduces Emotion-Inspired Learning Signals (EILS) that derive vector-valued internal states from interaction history, acting as homeostatic appraisal tools (e.g., curiosity, stress, confidence), to dynamically adjust the agent's optimization and learning behavior.

Result: EILS dynamically modulates agent parameters (e.g., exploration entropy, learning plasticity, and trust regions) in real-time. This could enable AI agents to achieve improved sample efficiency and adaptability in non-stationary environments.

Conclusion: EILS framework could overcome the limitations of static rewards in AI by integrating bio-inspired emotional signals, boosting agent autonomy in complex, real-world scenarios and outperforming standard approaches.

Abstract: The ruling method in modern Artificial Intelligence spanning from Deep Reinforcement Learning (DRL) to Large Language Models (LLMs) relies on a surge of static, externally defined reward functions. While this "extrinsic maximization" approach has rendered superhuman performance in closed, stationary fields, it produces agents that are fragile in open-ended, real-world environments. Standard agents lack internal autonomy: they struggle to explore without dense feedback, fail to adapt to distribution shifts (non-stationarity), and require extensive manual tuning of static hyperparameters. This paper proposes that the unaddressed factor in robust autonomy is a functional analog to biological emotion, serving as a high-level homeostatic control mechanism. We introduce Emotion-Inspired Learning Signals (EILS), a unified framework that replaces scattered optimization heuristics with a coherent, bio-inspired internal feedback engine. Unlike traditional methods that treat emotions as semantic labels, EILS models them as continuous, homeostatic appraisal signals such as Curiosity, Stress, and Confidence. We formalize these signals as vector-valued internal states derived from interaction history. These states dynamically modulate the agent's optimization landscape in real time: curiosity regulates entropy to prevent mode collapse, stress modulates plasticity to overcome inactivity, and confidence adapts trust regions to stabilize convergence. We hypothesize that this closed-loop homeostatic regulation can enable EILS agents to outperform standard baselines in terms of sample efficiency and non-stationary adaptation.

</details>


### [340] [Transformer Reconstructed with Dynamic Value Attention](https://arxiv.org/abs/2512.22212)
*Xiaowei Wang*

Main category: cs.LG

TL;DR: The paper proposes an optimization for transformers by introducing a single-head Dynamic Value Attention (DVA), reducing training time significantly while improving learning capability.


<details>
  <summary>Details</summary>
Motivation: Transformer models have remained structurally unchanged since 2017, disregarding the intrinsic limitation where the same static value is used for all queries in a head, despite attempts to mitigate it with multi-head attention.

Method: The method revolves around dynamically assigning values for each query, eliminating redundant heads and the feed-forward network, thus simplifying the transformer architecture to a single-head Dynamic Value Attention (DVA).

Result: Experiments show that DVA reduces training time by 37.6% compared to traditional transformers and enhances learning capability.

Conclusion: Dynamic Value Attention (DVA) optimizes transformer efficiency, simplifies its architecture, and improves performance, presenting it as a transformative development.

Abstract: Since transformer was firstly published in 2017, several works have been proposed to optimize it. However, the major structure of transformer remains unchanged, ignoring one of its main intrinsic limitations, which is the same static value is used for every query in a head. Transformer itself tries to solve this problem by implementing multi-head attentions, yet the number of heads is limited by complexity. I propose a method to decide a value for each query dynamically, which could cut down all the redundant heads, keeping only one. Consequently, the following feed forward network could be cut down entirely, as each revised embedding has already fetched enough useful values far beyond the context. As a result, a single-head Dynamic Value Attention (DVA) is all you need in a transformer. According to the experiment, DVA may save 37.6% training time than the original transformer meanwhile increasing the learning capability.

</details>


### [341] [On the Existence and Behaviour of Secondary Attention Sinks](https://arxiv.org/abs/2512.22213)
*Jeffrey T. H. Wong,Cheng Zhang,Louis Mahon,Wayne Luk,Anton Isopoussu,Yiren Zhao*

Main category: cs.LG

TL;DR: This research explores secondary attention sinks in transformer models, which differ from primary sinks such as the beginning-of-sequence token. Secondary sinks arise in middle layers, are influenced by MLP modules, and impact attention dynamics.


<details>
  <summary>Details</summary>
Motivation: To better understand the dynamics of attention allocation in transformer models and explore novel sink patterns that may influence performance.

Method: Extensive analysis across 11 model families to characterize secondary sinks, their formation via middle-layer MLP modules, and their effects on attention mechanisms.

Result: Secondary sinks emerge primarily in middle layers, influenced by $ll_2$-norm mapped vectors aligning with primary sinks. These sinks show varying durations and affect attention mechanisms differently. Specific patterns were observed in larger models with predictable sink levels.

Conclusion: The existence of secondary sinks adds complexity to attention mechanisms, highlighting their formation, persistence, and impact as pivotal areas for optimizing model behavior.

Abstract: Attention sinks are tokens, often the beginning-of-sequence (BOS) token, that receive disproportionately high attention despite limited semantic relevance. In this work, we identify a class of attention sinks, which we term secondary sinks, that differ fundamentally from the sinks studied in prior works, which we term primary sinks. While prior works have identified that tokens other than BOS can sometimes become sinks, they were found to exhibit properties analogous to the BOS token. Specifically, they emerge at the same layer, persist throughout the network and draw a large amount of attention mass. Whereas, we find the existence of secondary sinks that arise primarily in middle layers and can persist for a variable number of layers, and draw a smaller, but still significant, amount of attention mass. Through extensive experiments across 11 model families, we analyze where these secondary sinks appear, their properties, how they are formed, and their impact on the attention mechanism. Specifically, we show that: (1) these sinks are formed by specific middle-layer MLP modules; these MLPs map token representations to vectors that align with the direction of the primary sink of that layer. (2) The $\ell_2$-norm of these vectors determines the sink score of the secondary sink, and also the number of layers it lasts for, thereby leading to different impacts on the attention mechanisms accordingly. (3) The primary sink weakens in middle layers, coinciding with the emergence of secondary sinks. We observe that in larger-scale models, the location and lifetime of the sinks, together referred to as sink levels, appear in a more deterministic and frequent manner. Specifically, we identify three sink levels in QwQ-32B and six levels in Qwen3-14B.

</details>


### [342] [Interpretable and Adaptive Node Classification on Heterophilic Graphs via Combinatorial Scoring and Hybrid Learning](https://arxiv.org/abs/2512.22221)
*Soroush Vahidi*

Main category: cs.LG

TL;DR: The paper introduces a novel framework for semi-supervised node classification that adapts effectively to both homophilic and heterophilic graphs using combinatorial inference and a hybrid strategy.


<details>
  <summary>Details</summary>
Motivation: Conventional GNNs perform well on homophilic graphs but struggle with heterophilic graphs, where neighboring nodes have differing labels. This paper addresses the challenge of improving interpretability and performance on heterophilic graphs.

Method: The framework uses a confidence-ordered greedy procedure, combining factors like class priors, neighborhood statistics, feature similarity, and label-label compatibility. It also incorporates a hybrid strategy where neural models adjust only when necessary.

Result: The proposed method achieves competitive results with modern GNNs on both heterophilic and transitional benchmarks, while offering better interpretability, tunability, and computational efficiency.

Conclusion: This approach demonstrates that combinatorial inference combined with selective neural refinement can achieve strong node classification performance across graph regimes, with additional benefits of transparency and adaptability.

Abstract: Graph neural networks (GNNs) achieve strong performance on homophilic graphs but often struggle under heterophily, where adjacent nodes frequently belong to different classes. We propose an interpretable and adaptive framework for semi-supervised node classification based on explicit combinatorial inference rather than deep message passing. Our method assigns labels using a confidence-ordered greedy procedure driven by an additive scoring function that integrates class priors, neighborhood statistics, feature similarity, and training-derived label-label compatibility. A small set of transparent hyperparameters controls the relative influence of these components, enabling smooth adaptation between homophilic and heterophilic regimes.
  We further introduce a validation-gated hybrid strategy in which combinatorial predictions are optionally injected as priors into a lightweight neural model. Hybrid refinement is applied only when it improves validation performance, preserving interpretability when neuralization is unnecessary. All adaptation signals are computed strictly from training data, ensuring a leakage-free evaluation protocol. Experiments on heterophilic and transitional benchmarks demonstrate competitive performance with modern GNNs while offering advantages in interpretability, tunability, and computational efficiency.

</details>


### [343] [Predicting Mycotoxin Contamination in Irish Oats Using Deep and Transfer Learning](https://arxiv.org/abs/2512.22243)
*Alan Inglis,Fiona Doohan,Subramani Natarajan,Breige McNulty,Chris Elliott,Anne Nugent,Julie Meneely,Brett Greer,Stephen Kildea,Diana Bucur,Martin Danaher,Melissa Di Rocco,Lisa Black,Adam Gauley,Naoise McKenna,Andrew Parnell*

Main category: cs.LG

TL;DR: The study explores using neural networks and transfer learning models to predict mycotoxin levels in Irish oats based on environmental and agronomic data, with the TabPFN model achieving the best performance.


<details>
  <summary>Details</summary>
Motivation: To address the significant risk posed by mycotoxin contamination to cereal crops, which impacts food safety, crop quality, and productivity, and to enable early intervention strategies for reducing economic losses.

Method: The paper evaluates five modeling approaches (MLP, pre-trained MLP, TabPFN, TabNet, FT-Transformer) using Irish oat crop data. These include environmental, agronomic, and geographical predictors. The models' performances were assessed using RMSE, R^2, AUC, and F1 metrics. A permutation-based variable importance analysis was also performed.

Result: The TabPFN transfer learning model outperformed other techniques, with weather history and seed moisture content identified as the most influential predictors.

Conclusion: Using transfer learning techniques, specifically the TabPFN model, can provide accurate predictions of mycotoxin contamination, highlighting the significance of weather patterns and seed moisture in predictive modeling efforts.

Abstract: Mycotoxin contamination poses a significant risk to cereal crop quality, food safety, and agricultural productivity. Accurate prediction of mycotoxin levels can support early intervention strategies and reduce economic losses. This study investigates the use of neural networks and transfer learning models to predict mycotoxin contamination in Irish oat crops as a multi-response prediction task. Our dataset comprises oat samples collected in Ireland, containing a mix of environmental, agronomic, and geographical predictors. Five modelling approaches were evaluated: a baseline multilayer perceptron (MLP), an MLP with pre-training, and three transfer learning models; TabPFN, TabNet, and FT-Transformer. Model performance was evaluated using regression (RMSE, $R^2$) and classification (AUC, F1) metrics, with results reported per toxin and on average. Additionally, permutation-based variable importance analysis was conducted to identify the most influential predictors across both prediction tasks. The transfer learning approach TabPFN provided the overall best performance, followed by the baseline MLP. Our variable importance analysis revealed that weather history patterns in the 90-day pre-harvest period were the most important predictors, alongside seed moisture content.

</details>


### [344] [Müntz-Szász Networks: Neural Architectures with Learnable Power-Law Bases](https://arxiv.org/abs/2512.22222)
*Gnankan Landry Regis N'guessan*

Main category: cs.LG

TL;DR: This paper introduces Müntz-Szász Networks (MSN), a new neural network architecture using learnable fractional power bases to approximate functions with singular or fractional power behavior, achieving superior results in both accuracy and efficiency compared to standard networks.


<details>
  <summary>Details</summary>
Motivation: Standard neural networks use fixed activation functions, which struggle to approximate functions with singular or fractional power behaviors commonly found in physical processes.

Method: MSN uses learnable fractional power basis functions inspired by the Müntz-Szász theorem, allowing exponents and coefficients to be optimized instead of fixed activations.

Result: MSN demonstrates universal approximation capabilities, achieves superior approximation rates, and outperforms traditional MLPs and PINNs in regression and physics-informed benchmarks for singular functions.

Conclusion: Theory-guided architectural design, as applied in MSN, can significantly improve performance for scientifically motivated tasks, offering efficient and interpretable solutions.

Abstract: Standard neural network architectures employ fixed activation functions (ReLU, tanh, sigmoid) that are poorly suited for approximating functions with singular or fractional power behavior, a structure that arises ubiquitously in physics, including boundary layers, fracture mechanics, and corner singularities. We introduce Müntz-Szász Networks (MSN), a novel architecture that replaces fixed smooth activations with learnable fractional power bases grounded in classical approximation theory. Each MSN edge computes $φ(x) = \sum_k a_k |x|^{μ_k} + \sum_k b_k \mathrm{sign}(x)|x|^{λ_k}$, where the exponents $\{μ_k, λ_k\}$ are learned alongside the coefficients. We prove that MSN inherits universal approximation from the Müntz-Szász theorem and establish novel approximation rates: for functions of the form $|x|^α$, MSN achieves error $\mathcal{O}(|μ- α|^2)$ with a single learned exponent, whereas standard MLPs require $\mathcal{O}(ε^{-1/α})$ neurons for comparable accuracy. On supervised regression with singular target functions, MSN achieves 5-8x lower error than MLPs with 10x fewer parameters. Physics-informed neural networks (PINNs) represent a particularly demanding application for singular function approximation; on PINN benchmarks including a singular ODE and stiff boundary-layer problems, MSN achieves 3-6x improvement while learning interpretable exponents that match the known solution structure. Our results demonstrate that theory-guided architectural design can yield dramatic improvements for scientifically-motivated function classes.

</details>


### [345] [Completed Hyperparameter Transfer across Modules, Width, Depth, Batch and Duration](https://arxiv.org/abs/2512.22382)
*Bruno Mlodozeniec,Pierre Ablin,Louis Béthune,Dan Busbridge,Michal Klein,Jason Ramapuram,Marco Cuturi*

Main category: cs.LG

TL;DR: This paper addresses improvements in hyperparameter transfer across model sizes using enhanced neural network parameterizations and provides practical guidelines for optimizing and transferring per-module hyperparameters.


<details>
  <summary>Details</summary>
Motivation: Hyperparameter tuning is crucial for training stability and performance of large models, but transferring optimal settings across scales remains challenging.

Method: The authors propose Complete$^{(d)}$ Parameterisation extending $μ$P to unify scaling in multiple axes, and explore per-module hyperparameter optimization for effective transfer.

Result: They demonstrate effective hyperparameter transfer, even at the per-module level, leading to significant training speed improvements in Large Language Models.

Conclusion: With the proposed parameterization and optimization techniques, hyperparameters can be transferred effectively, improving training efficiency of large-scale models.

Abstract: Hyperparameter tuning can dramatically impact training stability and final performance of large-scale models. Recent works on neural network parameterisations, such as $μ$P, have enabled transfer of optimal global hyperparameters across model sizes. These works propose an empirical practice of search for optimal global base hyperparameters at a small model size, and transfer to a large size. We extend these works in two key ways. To handle scaling along most important scaling axes, we propose the Complete$^{(d)}$ Parameterisation that unifies scaling in width and depth -- using an adaptation of CompleteP -- as well as in batch-size and training duration. Secondly, with our parameterisation, we investigate per-module hyperparameter optimisation and transfer. We characterise the empirical challenges of navigating the high-dimensional hyperparameter landscape, and propose practical guidelines for tackling this optimisation problem. We demonstrate that, with the right parameterisation, hyperparameter transfer holds even in the per-module hyperparameter regime. Our study covers an extensive range of optimisation hyperparameters of modern models: learning rates, AdamW parameters, weight decay, initialisation scales, and residual block multipliers. Our experiments demonstrate significant training speed improvements in Large Language Models with the transferred per-module hyperparameters.

</details>


### [346] [ReGAIN: Retrieval-Grounded AI Framework for Network Traffic Analysis](https://arxiv.org/abs/2512.22223)
*Shaghayegh Shajarian,Kennedy Marsh,James Benson,Sajad Khorsandroo,Mahmoud Abdelsalam*

Main category: cs.LG

TL;DR: ReGAIN proposes a transparent network traffic analysis method combining LLM reasoning with evidence-backed responses, achieving up to 98.82% accuracy.


<details>
  <summary>Details</summary>
Motivation: To address high false positives and lack of interpretability in traditional and ML-based network traffic analysis systems.

Method: ReGAIN integrates traffic summarization, retrieval-augmented generation with LLM reasoning, embedding traffic summaries into a vector database with evidence citations for responses.

Result: Achieved 95.95%-98.82% accuracy across different attack types, outperforming existing baselines, validated by datasets and experts.

Conclusion: ReGAIN provides a more interpretable, accurate, and trustworthy solution for network traffic analysis.

Abstract: Modern networks generate vast, heterogeneous traffic that must be continuously analyzed for security and performance. Traditional network traffic analysis systems, whether rule-based or machine learning-driven, often suffer from high false positives and lack interpretability, limiting analyst trust. In this paper, we present ReGAIN, a multi-stage framework that combines traffic summarization, retrieval-augmented generation (RAG), and Large Language Model (LLM) reasoning for transparent and accurate network traffic analysis. ReGAIN creates natural-language summaries from network traffic, embeds them into a multi-collection vector database, and utilizes a hierarchical retrieval pipeline to ground LLM responses with evidence citations. The pipeline features metadata-based filtering, MMR sampling, a two-stage cross-encoder reranking mechanism, and an abstention mechanism to reduce hallucinations and ensure grounded reasoning. Evaluated on ICMP ping flood and TCP SYN flood traces from the real-world traffic dataset, it demonstrates robust performance, achieving accuracy between 95.95% and 98.82% across different attack types and evaluation benchmarks. These results are validated against two complementary sources: dataset ground truth and human expert assessments. ReGAIN also outperforms rule-based, classical ML, and deep learning baselines while providing unique explainability through trustworthy, verifiable responses.

</details>


### [347] [BLISS: Bandit Layer Importance Sampling Strategy for Efficient Training of Graph Neural Networks](https://arxiv.org/abs/2512.22388)
*Omar Alsaqa,Linh Thi Hoang,Muhammed Fatih Balin*

Main category: cs.LG

TL;DR: BLISS is a novel sampling strategy for GNNs to improve scalability by using dynamic multi-armed bandit-based sampling, reducing computational costs while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Large-scale graph processing with GNNs is computationally expensive due to the need to process numerous neighbors for each node, creating bottlenecks.

Method: BLISS, a dynamic sampling method using multi-armed bandits, adaptively selects important nodes for processing at each layer, balancing exploration and exploitation.

Result: BLISS integrates well with GCNs and GATs, outperforming or matching accuracy of full-batch training while offering better scalability.

Conclusion: BLISS addresses scalability issues in GNNs through adaptive sampling, maintaining model performance and reducing computational overhead.

Abstract: Graph Neural Networks (GNNs) are powerful tools for learning from graph-structured data, but their application to large graphs is hindered by computational costs. The need to process every neighbor for each node creates memory and computational bottlenecks. To address this, we introduce BLISS, a Bandit Layer Importance Sampling Strategy. It uses multi-armed bandits to dynamically select the most informative nodes at each layer, balancing exploration and exploitation to ensure comprehensive graph coverage. Unlike existing static sampling methods, BLISS adapts to evolving node importance, leading to more informed node selection and improved performance. It demonstrates versatility by integrating with both Graph Convolutional Networks (GCNs) and Graph Attention Networks (GATs), adapting its selection policy to their specific aggregation mechanisms. Experiments show that BLISS maintains or exceeds the accuracy of full-batch training.

</details>


### [348] [DiRL: An Efficient Post-Training Framework for Diffusion Language Models](https://arxiv.org/abs/2512.22234)
*Ying Zhu,Jiaxin Wan,Xiaoran Liu,Siyanag He,Qiqi Wang,Xu Guo,Tianyi Liang,Zengfeng Huang,Ziwei He,Xipeng Qiu*

Main category: cs.LG

TL;DR: The paper introduces DiRL, a post-training framework for Diffusion Language Models (dLLMs), and proposes DiPO, a Group Relative Policy Optimization tailored for dLLMs to enhance efficiency and performance, particularly in complex reasoning tasks like mathematics.


<details>
  <summary>Details</summary>
Motivation: Existing Diffusion Language Models face significant limitations in their performance on reasoning tasks due to inefficient post-training methods and mismatches between training and inference objectives.

Method: The authors developed DiRL, a novel framework integrating FlexAttention-accelerated blockwise training with LMDeploy-optimized inference for efficient post-training. They also proposed DiPO, an unbiased Group Relative Policy Optimization method tailored for dLLMs.

Result: Their model, DiRL-8B-Instruct, outperformed state-of-the-art Diffusion Language Models on mathematical tasks and surpassed the Qwen2.5 series on several benchmarks.

Conclusion: The proposed DiRL and DiPO frameworks significantly advance the post-training landscape for dLLMs, achieving enhanced performance in complex reasoning tasks through efficient and optimized training.

Abstract: Diffusion Language Models (dLLMs) have emerged as promising alternatives to Auto-Regressive (AR) models. While recent efforts have validated their pre-training potential and accelerated inference speeds, the post-training landscape for dLLMs remains underdeveloped. Existing methods suffer from computational inefficiency and objective mismatches between training and inference, severely limiting performance on complex reasoning tasks such as mathematics. To address this, we introduce DiRL, an efficient post-training framework that tightly integrates FlexAttention-accelerated blockwise training with LMDeploy-optimized inference. This architecture enables a streamlined online model update loop, facilitating efficient two-stage post-training (Supervised Fine-Tuning followed by Reinforcement Learning). Building on this framework, we propose DiPO, the first unbiased Group Relative Policy Optimization (GRPO) implementation tailored for dLLMs. We validate our approach by training DiRL-8B-Instruct on high-quality math data. Our model achieves state-of-the-art math performance among dLLMs and surpasses comparable models in the Qwen2.5 series on several benchmarks.

</details>


### [349] [Causality-Inspired Safe Residual Correction for Multivariate Time Series](https://arxiv.org/abs/2512.22428)
*Jianxiang Xie,Yuncheng Hua*

Main category: cs.LG

TL;DR: The paper introduces CRC, a framework for safe and effective residual correction in multivariate forecasting, ensuring non-degradation and improved accuracy.


<details>
  <summary>Details</summary>
Motivation: To address systematic errors and lack of deployment safety in modern multivariate forecasting methods while avoiding performance degradation.

Method: The method employs a causality-inspired encoder for direction-aware structure and a hybrid corrector for residual errors, supplemented by a strict safety mechanism for non-degradation.

Result: Experiments show improved accuracy and high non-degradation rates across various forecasting datasets and models.

Conclusion: CRC is a reliable correction framework for enhancing accuracy with safety guarantees, making it suitable for practical deployment.

Abstract: While modern multivariate forecasters such as Transformers and GNNs achieve strong benchmark performance, they often suffer from systematic errors at specific variables or horizons and, critically, lack guarantees against performance degradation in deployment. Existing post-hoc residual correction methods attempt to fix these errors, but are inherently greedy: although they may improve average accuracy, they can also "help in the wrong way" by overcorrecting reliable predictions and causing local failures in unseen scenarios.
  To address this critical "safety gap," we propose CRC (Causality-inspired Safe Residual Correction), a plug-and-play framework explicitly designed to ensure non-degradation. CRC follows a divide-and-conquer philosophy: it employs a causality-inspired encoder to expose direction-aware structure by decoupling self- and cross-variable dynamics, and a hybrid corrector to model residual errors. Crucially, the correction process is governed by a strict four-fold safety mechanism that prevents harmful updates.
  Experiments across multiple datasets and forecasting backbones show that CRC consistently improves accuracy, while an in-depth ablation study confirms that its core safety mechanisms ensure exceptionally high non-degradation rates (NDR), making CRC a correction framework suited for safe and reliable deployment.

</details>


### [350] [Masking Teacher and Reinforcing Student for Distilling Vision-Language Models](https://arxiv.org/abs/2512.22238)
*Byung-Kwan Lee,Yu-Chiang Frank Wang,Ryo Hachiuma*

Main category: cs.LG

TL;DR: This paper proposes the 'Masters' framework for efficiently distilling knowledge from large-scale vision-language models (VLMs) to compact models using progressive masking and reinforcement learning strategies.


<details>
  <summary>Details</summary>
Motivation: The massive size of VLMs renders them impractical for mobile or edge device deployment, necessitating the development of smaller, capable versions that can efficiently learn from larger teacher models.

Method: The 'Masters' framework distills knowledge by progressively masking less important weights in the teacher model and restoring its capacity during training. It incorporates offline reinforcement learning with two rewards: accuracy for responses' correctness and distillation for the transfer efficiency.

Result: The framework achieves smooth and stable knowledge transfer, allowing compact models to retain performance and rich representations without computationally expensive processes.

Conclusion: The proposed approach demonstrates the ability to train effective compact VLMs that are deployable on limited-resource devices while maintaining learning efficacy.

Abstract: Large-scale vision-language models (VLMs) have recently achieved remarkable multimodal understanding, but their massive size makes them impractical for deployment on mobile or edge devices. This raises the need for compact yet capable VLMs that can efficiently learn from powerful large teachers. However, distilling knowledge from a large teacher to a small student remains challenging due to their large size gap: the student often fails to reproduce the teacher's complex, high-dimensional representations, leading to unstable learning and degraded performance. To address this, we propose Masters (Masking Teacher and Reinforcing Student), a mask-progressive reinforcement learning (RL) distillation framework. Masters first masks non-dominant weights of the teacher to reduce unnecessary complexity, then progressively restores the teacher by gradually increasing its capacity during training. This strategy allows the student to learn richer representations from the teacher in a smooth and stable manner. To further refine knowledge transfer, Masters integrates an offline RL stage with two complementary rewards: an accuracy reward that measures the correctness of the generated responses, and a distillation reward that quantifies the ease of transferring responses from teacher to student. Unlike online think-answer RL paradigms that are computationally expensive and generate lengthy responses, our offline RL leverages pre-generated responses from masked teachers. These provide rich yet efficient guidance, enabling students to achieve strong performance without requiring the think-answer process.

</details>


### [351] [The Bayesian Geometry of Transformer Attention](https://arxiv.org/abs/2512.22471)
*Naman Aggarwal,Siddhartha R. Dalal,Vishal Misra*

Main category: cs.LG

TL;DR: The study constructs controlled environments, called 'Bayesian wind tunnels,' to rigorously test Bayesian reasoning in small transformers. It shows that transformers outperform MLPs and implement Bayesian inference through a consistent geometric mechanism.


<details>
  <summary>Details</summary>
Motivation: The motivation is to rigorously verify claims that transformers perform Bayesian reasoning in context, despite challenges of natural data ambiguity and confounding memorization in large models.

Method: They design 'Bayesian wind tunnels,' controlled experiments where the posterior is known, and memorization is impossible, and test small transformers performing inference tasks like bijection elimination and Hidden Markov Model tracking.

Result: Small transformers achieve near-perfect Bayesian posterior accuracy, outperforming MLPs. They reveal how attention, residual streams, and feed-forward networks geometrically implement Bayesian inference.

Conclusion: The hierarchical structure and attention mechanism in transformers enable Bayesian inference, providing insights into their design necessity and their advantages over flat architectures. These findings connect small, testable systems with large language model behaviors.

Abstract: Transformers often appear to perform Bayesian reasoning in context, but verifying this rigorously has been impossible: natural data lack analytic posteriors, and large models conflate reasoning with memorization. We address this by constructing \emph{Bayesian wind tunnels} -- controlled environments where the true posterior is known in closed form and memorization is provably impossible. In these settings, small transformers reproduce Bayesian posteriors with $10^{-3}$-$10^{-4}$ bit accuracy, while capacity-matched MLPs fail by orders of magnitude, establishing a clear architectural separation.
  Across two tasks -- bijection elimination and Hidden Markov Model (HMM) state tracking -- we find that transformers implement Bayesian inference through a consistent geometric mechanism: residual streams serve as the belief substrate, feed-forward networks perform the posterior update, and attention provides content-addressable routing. Geometric diagnostics reveal orthogonal key bases, progressive query-key alignment, and a low-dimensional value manifold parameterized by posterior entropy. During training this manifold unfurls while attention patterns remain stable, a \emph{frame-precision dissociation} predicted by recent gradient analyses.
  Taken together, these results demonstrate that hierarchical attention realizes Bayesian inference by geometric design, explaining both the necessity of attention and the failure of flat architectures. Bayesian wind tunnels provide a foundation for mechanistically connecting small, verifiable systems to reasoning phenomena observed in large language models.

</details>


### [352] [EvoXplain: When Machine Learning Models Agree on Predictions but Disagree on Why -- Measuring Mechanistic Multiplicity Across Training Runs](https://arxiv.org/abs/2512.22240)
*Chama Bensmail*

Main category: cs.LG

TL;DR: EvoXplain is a new method to assess the stability of model explanations by revealing whether explanations vary under repeated training, even when models achieve high accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the overlooked issue of whether models achieving high accuracy rely on the same internal logic or if their mechanisms differ significantly, which questions the trustworthiness of explanations.

Method: The authors propose EvoXplain, a framework that treats explanations as samples from the stochastic optimization process and examines their stability by analyzing explanatory multimodality across repeated training.

Result: EvoXplain is applied to Breast Cancer and COMPAS datasets with Logistic Regression and Random Forest models, showing that explanations often exhibit multimodality despite high accuracy, revealing instability even in supposedly stable models.

Conclusion: The paper introduces EvoXplain as a diagnostic tool to highlight explanatory instability in models, emphasizing that explanations should be reframed as properties of model classes rather than individual models.

Abstract: Machine learning models are primarily judged by predictive performance, especially in applied settings. Once a model reaches high accuracy, its explanation is often assumed to be correct and trustworthy. However, this assumption raises an overlooked question: when two models achieve high accuracy, do they rely on the same internal logic, or do they reach the same outcome via different -- and potentially competing -- mechanisms? We introduce EvoXplain, a diagnostic framework that measures the stability of model explanations across repeated training. Rather than analysing a single trained model, EvoXplain treats explanations as samples drawn from the stochastic optimisation process itself -- without aggregating predictions or constructing ensembles -- and examines whether these samples form a single coherent explanation or separate into multiple, distinct explanatory modes. We evaluate EvoXplain on the Breast Cancer and COMPAS datasets using two widely deployed model classes: Logistic Regression and Random Forests. Although all models achieve high predictive accuracy, their explanations frequently exhibit clear multimodality. Even models commonly assumed to be stable, such as Logistic Regression, can produce multiple well-separated explanatory basins under repeated training on the same data split. These differences are not explained by hyperparameter variation or simple performance trade-offs. EvoXplain does not attempt to select a 'correct' explanation. Instead, it makes explanatory instability visible and quantifiable, revealing when single-instance or averaged explanations obscure the existence of multiple underlying mechanisms. More broadly, EvoXplain reframes interpretability as a property of a model class under repeated instantiation, rather than of any single trained model.

</details>


### [353] [On Admissible Rank-based Input Normalization Operators](https://arxiv.org/abs/2512.22587)
*Taeyun Kim*

Main category: cs.LG

TL;DR: Rank-based normalization, while robust to transformations and variations, faces instability due to the structural design of differentiable ranking operators. The paper introduces axioms for stable normalization and proposes a new operator design.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the instability issues in widely used rank-based normalization operators under monotone transformations, batch shifts, and input perturbations.

Method: The authors propose three formal axioms to ensure invariance and stability in rank-based normalization. They analytically derive a structure for operators that satisfy these axioms and construct a minimal operator meeting these criteria.

Result: Their designed operator fulfills the proposed stability and invariance criteria, and empirical tests highlight the value of these constraints in practical scenarios.

Conclusion: The paper redefines the design framework for rank-based normalization operators, offering a stable and invariant approach that surpasses existing methods.

Abstract: Rank-based input normalization is a workhorse of modern machine learning, prized for its robustness to scale, monotone transformations, and batch-to-batch variation. In many real systems, the ordering of feature values matters far more than their raw magnitudes - yet the structural conditions that a rank-based normalization operator must satisfy to remain stable under these invariances have never been formally pinned down.
  We show that widely used differentiable sorting and ranking operators fundamentally fail these criteria. Because they rely on value gaps and batch-level pairwise interactions, they are intrinsically unstable under strictly monotone transformations, shifts in mini-batch composition, and even tiny input perturbations. Crucially, these failures stem from the operators' structural design, not from incidental implementation choices.
  To address this, we propose three axioms that formalize the minimal invariance and stability properties required of rank-based input normalization. We prove that any operator satisfying these axioms must factor into (i) a feature-wise rank representation and (ii) a scalarization map that is both monotone and Lipschitz-continuous. We then construct a minimal operator that meets these criteria and empirically show that the resulting constraints are non-trivial in realistic setups. Together, our results sharply delineate the design space of valid rank-based normalization operators and formally separate them from existing continuous-relaxation-based sorting methods.

</details>


### [354] [Enhanced geometry prediction in laser directed energy deposition using meta-learning](https://arxiv.org/abs/2512.22241)
*Abdul Malik Al Mardhouf Al Saadi,Amrita Basak*

Main category: cs.LG

TL;DR: A meta-learning approach using MAML and Reptile achieves accurate predictions of bead geometry in laser-directed energy deposition, adapting effectively to new conditions with limited data.


<details>
  <summary>Details</summary>
Motivation: The research aims to improve bead geometry predictions in L-DED, overcoming the challenge of data scarcity and variability due to different materials, configurations, and parameters.

Method: The study uses meta-learning techniques, specifically Model-Agnostic Meta-Learning (MAML) and Reptile, to train models that adapt rapidly to varying experimental conditions using minimal training data.

Result: MAML and Reptile successfully outperform conventional models in bead height prediction by achieving high accuracy (R-squared ~0.9, errors 0.03-0.08 mm) even with limited training examples (3-9).

Conclusion: The proposed meta-learning framework effectively enables cross-dataset knowledge transfer in heterogeneous L-DED environments, showing strong potential for improving industrial manufacturing processes.

Abstract: Accurate bead geometry prediction in laser-directed energy deposition (L-DED) is often hindered by the scarcity and heterogeneity of experimental datasets collected under different materials, machine configurations, and process parameters. To address this challenge, a cross-dataset knowledge transfer model based on meta-learning for predicting deposited track geometry in L-DED is proposed. Specifically, two gradient-based meta-learning algorithms, i.e., Model-Agnostic Meta-Learning (MAML) and Reptile, are investigated to enable rapid adaptation to new deposition conditions with limited data. The proposed framework is performed using multiple experimental datasets compiled from peer-reviewed literature and in-house experiments and evaluated across powder-fed, wire-fed, and hybrid wire-powder L-DED processes. Results show that both MAML and Reptile achieve accurate bead height predictions on unseen target tasks using as few as three to nine training examples, consistently outperforming conventional feedforward neural networks trained under comparable data constraints. Across multiple target tasks representing different printing conditions, the meta-learning models achieve strong generalization performance, with R-squared values reaching up to approximately 0.9 and mean absolute errors between 0.03-0.08 mm, demonstrating effective knowledge transfer across heterogeneous L-DED settings.

</details>


### [355] [Fairness Evaluation of Risk Estimation Models for Lung Cancer Screening](https://arxiv.org/abs/2512.22242)
*Shaurya Gaur,Michel Vitale,Alessa Hering,Johan Kwisthout,Colin Jacobs,Lena Philipp,Fennie van der Graaf*

Main category: cs.LG

TL;DR: The study evaluates the fairness and performance of two AI models and a guideline-based model for lung cancer risk estimation, revealing notable disparities among demographic groups.


<details>
  <summary>Details</summary>
Motivation: To explore the fairness and performance disparities in AI models used for lung cancer risk estimation, and assess potential biases across demographic groups.

Method: The study evaluated two deep learning models (Sybil and Venkadesh21) and one logistic regression model (PanCan2b) using metrics like AUROC, sensitivity, and specificity on the NLST validation set, considering clinical confounders and bias with the JustEFAB framework.

Result: Performance disparities were observed: Sybil performed worse for men than women, and Venkadesh21 had lower sensitivity for Black versus White participants. These biases were not solely explained by clinical confounders.

Conclusion: AI models for lung cancer risk have significant demographic biases, emphasizing a need for better monitoring, fairness evaluation, and improved performance across underrepresented groups.

Abstract: Lung cancer is the leading cause of cancer-related mortality in adults worldwide. Screening high-risk individuals with annual low-dose CT (LDCT) can support earlier detection and reduce deaths, but widespread implementation may strain the already limited radiology workforce. AI models have shown potential in estimating lung cancer risk from LDCT scans. However, high-risk populations for lung cancer are diverse, and these models' performance across demographic groups remains an open question. In this study, we drew on the considerations on confounding factors and ethically significant biases outlined in the JustEFAB framework to evaluate potential performance disparities and fairness in two deep learning risk estimation models for lung cancer screening: the Sybil lung cancer risk model and the Venkadesh21 nodule risk estimator. We also examined disparities in the PanCan2b logistic regression model recommended in the British Thoracic Society nodule management guideline. Both deep learning models were trained on data from the US-based National Lung Screening Trial (NLST), and assessed on a held-out NLST validation set. We evaluated AUROC, sensitivity, and specificity across demographic subgroups, and explored potential confounding from clinical risk factors. We observed a statistically significant AUROC difference in Sybil's performance between women (0.88, 95% CI: 0.86, 0.90) and men (0.81, 95% CI: 0.78, 0.84, p < .001). At 90% specificity, Venkadesh21 showed lower sensitivity for Black (0.39, 95% CI: 0.23, 0.59) than White participants (0.69, 95% CI: 0.65, 0.73). These differences were not explained by available clinical confounders and thus may be classified as unfair biases according to JustEFAB. Our findings highlight the importance of improving and monitoring model performance across underrepresented subgroups, and further research on algorithmic fairness, in lung cancer screening.

</details>


### [356] [Understanding the Mechanisms of Fast Hyperparameter Transfer](https://arxiv.org/abs/2512.22768)
*Nikhil Ghosh,Denny Wu,Alberto Bietti*

Main category: cs.LG

TL;DR: The paper addresses the challenge of hyperparameter optimization in large-scale deep learning by exploring scale-aware hyperparameters that can transfer from small to large models efficiently. It develops a conceptual framework to analyze and improve hyperparameter transfer.


<details>
  <summary>Details</summary>
Motivation: The motivation arises from the increasing computational cost of hyperparameter optimization as deep learning models grow in scale. The study aims to find efficient methods for transferring optimal hyperparameters across model scales.

Method: The authors develop a framework to characterize hyperparameter transfer across scales, analyze transfer efficiency under Maximal Update Parameterization (μP), present synthetic examples for understanding transfer behaviors, and propose a decomposition of optimization trajectories.

Result: The analysis shows that fast transfer is equivalent to an asymptotically compute-efficient transfer strategy, identifies cases where transfer fails or succeeds under μP, and provides empirical evidence supporting the proposed hypothesis.

Conclusion: Hyperparameter transfer across scale is possible and can lead to significant computational efficiency, but its success depends on problem structure and inherent optimization trajectory components. This insight can improve methods for scaling up models efficiently.

Abstract: The growing scale of deep learning models has rendered standard hyperparameter (HP) optimization prohibitively expensive. A promising solution is the use of scale-aware hyperparameters, which can enable direct transfer of optimal HPs from small-scale grid searches to large models with minimal performance loss. To understand the principles governing such transfer strategy, we develop a general conceptual framework for reasoning about HP transfer across scale, characterizing transfer as fast when the suboptimality it induces vanishes asymptotically faster than the finite-scale performance gap. We show formally that fast transfer is equivalent to useful transfer for compute-optimal grid search, meaning that transfer is asymptotically more compute-efficient than direct tuning. While empirical work has found that the Maximal Update Parameterization ($μ$P) exhibits fast transfer when scaling model width, the mechanisms remain poorly understood. We show that this property depends critically on problem structure by presenting synthetic settings where transfer either offers provable computational advantage or fails to outperform direct tuning even under $μ$P. To explain the fast transfer observed in practice, we conjecture that decomposing the optimization trajectory reveals two contributions to loss reduction: (1) a width-stable component that determines the optimal HPs, and (2) a width-sensitive component that improves with width but weakly perturbs the HP optimum. We present empirical evidence for this hypothesis across various settings, including large language model pretraining.

</details>


### [357] [Calibrating LLM Judges: Linear Probes for Fast and Reliable Uncertainty Estimation](https://arxiv.org/abs/2512.22245)
*Bhaktipriya Radharapu,Eshika Saxena,Kenneth Li,Chenxi Whitehouse,Adina Williams,Nicola Cancedda*

Main category: cs.LG

TL;DR: The paper introduces an efficient method using linear probes with a Brier score-based loss to estimate uncertainty in LLM-based judges, achieving high calibration and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: With the increasing integration of LLM-based judges in industrial applications, the need for well-calibrated and efficient uncertainty estimation has become essential due to limitations in existing methods.

Method: The authors propose using linear probes trained with a Brier score-based loss on reasoning judges' hidden states to provide calibrated uncertainty estimates without requiring additional model training.

Result: Linear probes outperform existing methods in calibration and computational efficiency, generalize to new evaluation domains, and improve accuracy for high-confidence predictions. However, they underperform on simpler datasets but benefit safety-critical applications by minimizing false positives.

Conclusion: The proposed interpretability-based uncertainty estimation method is practical, scalable, and suitable for production deployment of LLM judges, especially in contexts focusing on safety-critical applications.

Abstract: As LLM-based judges become integral to industry applications, obtaining well-calibrated uncertainty estimates efficiently has become critical for production deployment. However, existing techniques, such as verbalized confidence and multi-generation methods, are often either poorly calibrated or computationally expensive. We introduce linear probes trained with a Brier score-based loss to provide calibrated uncertainty estimates from reasoning judges' hidden states, requiring no additional model training. We evaluate our approach on both objective tasks (reasoning, mathematics, factuality, coding) and subjective human preference judgments. Our results demonstrate that probes achieve superior calibration compared to existing methods with $\approx10$x computational savings, generalize robustly to unseen evaluation domains, and deliver higher accuracy on high-confidence predictions. However, probes produce conservative estimates that underperform on easier datasets but may benefit safety-critical deployments prioritizing low false-positive rates. Overall, our work demonstrates that interpretability-based uncertainty estimation provides a practical and scalable plug-and-play solution for LLM judges in production.

</details>


### [358] [Fundamental Novel Consistency Theory: $H$-Consistency Bounds](https://arxiv.org/abs/2512.22880)
*Yutao Zhong*

Main category: cs.LG

TL;DR: The paper investigates the connection between surrogate loss functions and target loss functions in machine learning, proposing new $H$-consistency bounds that are more comprehensive and informative than existing guarantees.


<details>
  <summary>Details</summary>
Motivation: The gap between computationally tractable surrogate losses used during training and the target losses that define task performance has been a long-standing challenge in machine learning, demanding better analytical frameworks to understand and minimize this gap.

Method: The paper introduces $H$-consistency bounds to quantify the relationship between surrogate and target loss functions, providing theoretical results for both binary and multi-class classification through distribution-dependent and -independent bounds. It considers various surrogate loss functions and develops a universal framework.

Result: The authors establish explicit $H$-consistency bounds for convex and adversarial loss surrogates in binary and multi-class settings. They show when such bounds are unattainable, introduce smooth adversarial variants of certain loss functions, and derive the growth rates of these bounds.

Conclusion: The study provides a rigorous framework and insights that can guide the design and selection of surrogate loss functions, advancing our understanding of loss consistency and offering practical implications for robust learning algorithm development.

Abstract: In machine learning, the loss functions optimized during training often differ from the target loss that defines task performance due to computational intractability or lack of differentiability. We present an in-depth study of the target loss estimation error relative to the surrogate loss estimation error. Our analysis leads to $H$-consistency bounds, which are guarantees accounting for the hypothesis set $H$. These bounds offer stronger guarantees than Bayes-consistency or $H$-calibration and are more informative than excess error bounds.
  We begin with binary classification, establishing tight distribution-dependent and -independent bounds. We provide explicit bounds for convex surrogates (including linear models and neural networks) and analyze the adversarial setting for surrogates like $ρ$-margin and sigmoid loss. Extending to multi-class classification, we present the first $H$-consistency bounds for max, sum, and constrained losses, covering both non-adversarial and adversarial scenarios. We demonstrate that in some cases, non-trivial $H$-consistency bounds are unattainable. We also investigate comp-sum losses (e.g., cross-entropy, MAE), deriving their first $H$-consistency bounds and introducing smooth adversarial variants that yield robust learning algorithms.
  We develop a comprehensive framework for deriving these bounds across various surrogates, introducing new characterizations for constrained and comp-sum losses. Finally, we examine the growth rates of $H$-consistency bounds, establishing a universal square-root growth rate for smooth surrogates in binary and multi-class tasks, and analyze minimizability gaps to guide surrogate selection.

</details>


### [359] [The Affine Divergence: Aligning Activation Updates Beyond Normalisation](https://arxiv.org/abs/2512.22247)
*George Bird*

Main category: cs.LG

TL;DR: This paper highlights a mismatch between mathematically ideal and actual activation updates during gradient descent and proposes alternative approaches to activation normalization and functional forms, such as "PatchNorm."


<details>
  <summary>Details</summary>
Motivation: The motivation stems from addressing the mismatch between ideal steepest-descent activation updates and their effective behavior. Improving this could enhance optimization by prioritizing activations closer to the loss function.

Method: The authors analyzed the sample-wise scaling behavior of different network layers and derived solutions to improve activation updates, including introducing new normalization frameworks like "PatchNorm."

Result: The proposed solutions outperform conventional normalizers across multiple tests, offering both empirical success and theoretical insights into optimization and activation normalization.

Conclusion: The study proposes a theoretical approach yielding new functions and alternatives to traditional affine-normalization methods, questioning the standard model design and advocating for a refined decomposition of normalization functions.

Abstract: A systematic mismatch exists between mathematically ideal and effective activation updates during gradient descent. As intended, parameters update in their direction of steepest descent. However, activations are argued to constitute a more directly impactful quantity to prioritise in optimisation, as they are closer to the loss in the computational graph and carry sample-dependent information through the network. Yet their propagated updates do not take the optimal steepest-descent step. These quantities exhibit non-ideal sample-wise scaling across affine, convolutional, and attention layers. Solutions to correct for this are trivial and, entirely incidentally, derive normalisation from first principles despite motivational independence. Consequently, such considerations offer a fresh and conceptual reframe of normalisation's action, with auxiliary experiments bolstering this mechanistically. Moreover, this analysis makes clear a second possibility: a solution that is functionally distinct from modern normalisations, without scale-invariance, yet remains empirically successful, outperforming conventional normalisers across several tests. This is presented as an alternative to the affine map. This generalises to convolution via a new functional form, "PatchNorm", a compositionally inseparable normaliser. Together, these provide an alternative mechanistic framework that adds to, and counters some of, the discussion of normalisation. Further, it is argued that normalisers are better decomposed into activation-function-like maps with parameterised scaling, thereby aiding the prioritisation of representations during optimisation. Overall, this constitutes a theoretical-principled approach that yields several new functions that are empirically validated and raises questions about the affine + nonlinear approach to model creation.

</details>


### [360] [Theory and Algorithms for Learning with Multi-Class Abstention and Multi-Expert Deferral](https://arxiv.org/abs/2512.22886)
*Anqi Mao*

Main category: cs.LG

TL;DR: The paper addresses challenges in large language models (LLMs) and explores methods of learning with multiple-expert deferral and abstention in classification and regression, providing theoretical guarantees and effective algorithms.


<details>
  <summary>Details</summary>
Motivation: LLMs face challenges like hallucinations and high inference costs. Using multiple experts can improve reliability and efficiency by properly routing difficult and simple queries to appropriate models.

Method: The study designs and analyzes surrogate losses and algorithms for scenarios involving abstention, multi-expert deferral in classification, and regression with deferral, with empirical validation and strong theoretical consistency guarantees.

Result: Strong $H$-consistency bounds for surrogate losses were achieved, and the designs demonstrated superior performance in experiments on datasets like CIFAR-10, CIFAR-100, and SVHN.

Conclusion: The proposed frameworks and algorithms effectively handle multi-expert deferral and learning with abstention or regression scenarios, with practical and theoretical advancements resolving key open problems.

Abstract: Large language models (LLMs) have achieved remarkable performance but face critical challenges: hallucinations and high inference costs. Leveraging multiple experts offers a solution: deferring uncertain inputs to more capable experts improves reliability, while routing simpler queries to smaller, distilled models enhances efficiency. This motivates the problem of learning with multiple-expert deferral. This thesis presents a comprehensive study of this problem and the related problem of learning with abstention, supported by strong consistency guarantees.
  First, for learning with abstention (a special case of deferral), we analyze score-based and predictor-rejector formulations in multi-class classification. We introduce new families of surrogate losses and prove strong non-asymptotic, hypothesis set-specific consistency guarantees, resolving two existing open questions. We analyze both single-stage and practical two-stage settings, with experiments on CIFAR-10, CIFAR-100, and SVHN demonstrating the superior performance of our algorithms.
  Second, we address general multi-expert deferral in classification. We design new surrogate losses for both single-stage and two-stage scenarios and prove they benefit from strong $H$-consistency bounds. For the two-stage scenario, we show that our surrogate losses are realizable $H$-consistent for constant cost functions, leading to effective new algorithms.
  Finally, we introduce a novel framework for regression with deferral to address continuous label spaces. Our versatile framework accommodates multiple experts and various cost structures, supporting both single-stage and two-stage methods. It subsumes recent work on regression with abstention. We propose new surrogate losses with proven $H$-consistency and demonstrate the empirical effectiveness of the resulting algorithms.

</details>


### [361] [Amortized Inference for Model Rocket Aerodynamics: Learning to Estimate Physical Parameters from Simulation](https://arxiv.org/abs/2512.22248)
*Rohit Pandey,Rohan Pandey*

Main category: cs.LG

TL;DR: The paper introduces a neural network-based approach for predicting model rocket flight performance using synthetic data, achieving strong results in real-world applications without fine-tuning or real flight training data.


<details>
  <summary>Details</summary>
Motivation: Model rocketry requires accurate flight performance predictions, but traditional aerodynamic parameter estimation methods are either computationally expensive or require extensive real data. This paper aims to overcome these challenges through a simulation-based, data-driven approach.

Method: A neural network is trained on synthetic flight data generated from physics simulations to predict drag coefficient and thrust correction factors. The model uses apogee measurements, along with motor and configuration features, and transitions directly from simulation to real-world application without further fine-tuning.

Result: The method achieved a mean absolute error of 12.3 m on apogee prediction when tested on 8 real flights. It outperformed the OpenRocket baseline, demonstrating effective sim-to-real transfer with zero real-world training data.

Conclusion: This proof-of-concept shows promise in using simulated training data to improve predictive performance for model rockets, providing new tools for the amateur rocketry community. Public implementation fosters reproducibility and further adoption.

Abstract: Accurate prediction of model rocket flight performance requires estimating aerodynamic parameters that are difficult to measure directly. Traditional approaches rely on computational fluid dynamics or empirical correlations, while data-driven methods require extensive real flight data that is expensive and time-consuming to collect. We present a simulation-based amortized inference approach that trains a neural network on synthetic flight data generated from a physics simulator, then applies the learned model to real flights without any fine-tuning. Our method learns to invert the forward physics model, directly predicting drag coefficient and thrust correction factor from a single apogee measurement combined with motor and configuration features. In this proof-of-concept study, we train on 10,000 synthetic flights and evaluate on 8 real flights, achieving a mean absolute error of 12.3 m in apogee prediction - demonstrating promising sim-to-real transfer with zero real training examples. Analysis reveals a systematic positive bias in predictions, providing quantitative insight into the gap between idealized physics and real-world flight conditions. We additionally compare against OpenRocket baseline predictions, showing that our learned approach reduces apogee prediction error. Our implementation is publicly available to support reproducibility and adoption in the amateur rocketry community.

</details>


### [362] [Temporal Visual Semantics-Induced Human Motion Understanding with Large Language Models](https://arxiv.org/abs/2512.22249)
*Zheng Xing,Weibing Zhao*

Main category: cs.LG

TL;DR: This paper proposes enhancing unsupervised human motion segmentation by integrating temporal vision semantics (TVS) using a large language model into subspace clustering.


<details>
  <summary>Details</summary>
Motivation: Existing methods for human motion segmentation inadequately utilize temporal semantic information present in motion sequences, leaving a gap in performance potential.

Method: The paper employs large language models to derive temporal vision semantics (TVS) from motion sequences, integrates them via subspace embedding with temporal constraints, and further optimizes performance using a feedback-enabled framework.

Result: The proposed approach outperforms state-of-the-art methods on four benchmark human motion datasets, demonstrating its efficacy.

Conclusion: Leveraging TVS and employing an optimized framework significantly enhances unsupervised human motion segmentation using subspace clustering.

Abstract: Unsupervised human motion segmentation (HMS) can be effectively achieved using subspace clustering techniques. However, traditional methods overlook the role of temporal semantic exploration in HMS. This paper explores the use of temporal vision semantics (TVS) derived from human motion sequences, leveraging the image-to-text capabilities of a large language model (LLM) to enhance subspace clustering performance. The core idea is to extract textual motion information from consecutive frames via LLM and incorporate this learned information into the subspace clustering framework. The primary challenge lies in learning TVS from human motion sequences using LLM and integrating this information into subspace clustering. To address this, we determine whether consecutive frames depict the same motion by querying the LLM and subsequently learn temporal neighboring information based on its response. We then develop a TVS-integrated subspace clustering approach, incorporating subspace embedding with a temporal regularizer that induces each frame to share similar subspace embeddings with its temporal neighbors. Additionally, segmentation is performed based on subspace embedding with a temporal constraint that induces the grouping of each frame with its temporal neighbors. We also introduce a feedback-enabled framework that continuously optimizes subspace embedding based on the segmentation output. Experimental results demonstrate that the proposed method outperforms existing state-of-the-art approaches on four benchmark human motion datasets.

</details>


### [363] [Trust Region Masking for Long-Horizon LLM Reinforcement Learning](https://arxiv.org/abs/2512.23075)
*Yingru Li,Jiacai Liu,Jiawei Xu,Yuxuan Tong,Ziniu Li,Baoxiang Wang*

Main category: cs.LG

TL;DR: This paper addresses fundamental issues in current policy gradient methods for large language models (LLMs) in reinforcement learning settings, providing tighter error bounds and introducing Trust Region Masking for better guarantees in long-horizon tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the approximation errors in policy gradient methods caused by off-policy mismatches, especially for long-horizon tasks in LLM-RL, where classical bounds are ineffective.

Method: This study derives two tighter error bounds (Pinsker-Marginal and Mixed), depending on the token-level maximum KL divergence across sequences, and introduces Trust Region Masking (TRM) to exclude sequences that violate the trust region.

Result: The proposed TRM method ensures non-vacuous monotonic improvement guarantees in long-horizon reinforcement learning for LLMs.

Conclusion: Trust Region Masking and the new tighter bounds provide practical and theoretical improvements in handling optimization issues in long-horizon tasks with policy gradient methods for language models.

Abstract: Policy gradient methods for large language models optimize a surrogate objective computed from samples of a rollout policy $π_{\text{roll}}$. When $π_{\text{roll}} \ne π_θ$, there is approximation error between the surrogate and the true objective. Prior work has shown that this off-policy mismatch is unavoidable in modern LLM-RL due to implementation divergence, mixture-of-experts routing discontinuities, and distributed training staleness. Classical trust region bounds on the resulting error scale as $O(T^2)$ with sequence length $T$, rendering them vacuous for long-horizon tasks. We derive two tighter bounds: a Pinsker-Marginal bound scaling as $O(T^{3/2})$ and a Mixed bound scaling as $O(T)$. Crucially, both bounds depend on $D_{kl}^{tok,max}$ -- the maximum token-level KL divergence across all positions in a sequence. This is inherently a sequence-level quantity: it requires examining the entire trajectory to compute, and therefore cannot be controlled by token-independent methods like PPO clipping. We propose Trust Region Masking (TRM), which excludes entire sequences from gradient computation if any token violates the trust region, providing the first non-vacuous monotonic improvement guarantees for long-horizon LLM-RL.

</details>


### [364] [Interpretable Perturbation Modeling Through Biomedical Knowledge Graphs](https://arxiv.org/abs/2512.22251)
*Pascal Passigan,Kevin zhu,Angelina Ning*

Main category: cs.LG

TL;DR: The paper proposes a method to predict how drugs affect gene expression by using graph attention networks (GATs) and biomedical knowledge graphs.


<details>
  <summary>Details</summary>
Motivation: Current deep learning models focus on binary drug-disease associations instead of exploring drug-driven gene expression changes, which are crucial for understanding drug mechanisms and transcriptomic effects.

Method: The authors built an integrated biomedical graph combining PrimeKG++ embeddings and LINCS L1000 drug/cell data, initialized with multimodal embeddings from foundation models. They trained a GAT to predict gene expression changes for drug-cell pairs.

Result: The framework outperformed baseline methods in delta expression prediction and demonstrated the importance of biomedical graph edges via ablation studies.

Conclusion: This approach refines drug modeling by advancing beyond binary associations to mechanistic understanding of transcriptional outcomes.

Abstract: Understanding how small molecules perturb gene expression is essential for uncovering drug mechanisms, predicting off-target effects, and identifying repurposing opportunities. While prior deep learning frameworks have integrated multimodal embeddings into biomedical knowledge graphs (BKGs) and further improved these representations through graph neural network message-passing paradigms, these models have been applied to tasks such as link prediction and binary drug-disease association, rather than the task of gene perturbation, which may unveil more about mechanistic transcriptomic effects. To address this gap, we construct a merged biomedical graph that integrates (i) PrimeKG++, an augmentation of PrimeKG containing semantically rich embeddings for nodes with (ii) LINCS L1000 drug and cell line nodes, initialized with multimodal embeddings from foundation models such as MolFormerXL and BioBERT. Using this heterogeneous graph, we train a graph attention network (GAT) with a downstream prediction head that learns the delta expression profile of over 978 landmark genes for a given drug-cell pair. Our results show that our framework outperforms MLP baselines for differentially expressed genes (DEG) -- which predict the delta expression given a concatenated embedding of drug features, target features, and baseline cell expression -- under the scaffold and random splits. Ablation experiments with edge shuffling and node feature randomization further demonstrate that the edges provided by biomedical KGs enhance perturbation-level prediction. More broadly, our framework provides a path toward mechanistic drug modeling: moving beyond binary drug-disease association tasks to granular transcriptional effects of therapeutic intervention.

</details>


### [365] [Taming the Tail: Stable LLM Reinforcement Learning via Dynamic Vocabulary Pruning](https://arxiv.org/abs/2512.23087)
*Yingru Li,Jiawei Xu,Jiacai Liu,Yuxuan Tong,Ziniu Li,Tianle Cai,Ge Zhang,Qian Liu,Baoxiang Wang*

Main category: cs.LG

TL;DR: The paper addresses the problem of training-inference mismatch in reinforcement learning for large language models (LLMs) and proposes dynamically pruning a "safe" vocabulary to stabilize training.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning for LLMs faces training-inference mismatch due to the different probability distributions produced by inference engines and training systems, which destabilize gradient estimation and hinder stable optimization.

Method: The proposed method dynamically prunes "unsafe" low-probability tokens in the vocabulary during reinforcement learning optimization to mitigate mismatches and stabilize training.

Result: Empirical validation demonstrates stable training outcomes, and theoretical analysis bounds the optimization bias introduced by vocabulary pruning.

Conclusion: Dynamically pruning unsafe tokens offers a trade-off between bias and stability, effectively addressing mismatch issues in reinforcement learning for LLMs.

Abstract: Reinforcement learning for large language models (LLMs) faces a fundamental tension: high-throughput inference engines and numerically-precise training systems produce different probability distributions from the same parameters, creating a training-inference mismatch. We prove this mismatch has an asymmetric effect: the bound on log-probability mismatch scales as $(1-p)$ where $p$ is the token probability. For high-probability tokens, this bound vanishes, contributing negligibly to sequence-level mismatch. For low-probability tokens in the tail, the bound remains large, and moreover, when sampled, these tokens exhibit systematically biased mismatches that accumulate over sequences, destabilizing gradient estimation. Rather than applying post-hoc corrections, we propose constraining the RL objective to a dynamically-pruned ``safe'' vocabulary that excludes the extreme tail. By pruning such tokens, we trade large, systematically biased mismatches for a small, bounded optimization bias. Empirically, our method achieves stable training; theoretically, we bound the optimization bias introduced by vocabulary pruning.

</details>


### [366] [Graph Attention-based Adaptive Transfer Learning for Link Prediction](https://arxiv.org/abs/2512.22252)
*Huashen Lu,Wensheng Gan,Guoting Chen,Zhichao Huang,Philip S. Yu*

Main category: cs.LG

TL;DR: The paper presents GAATNet, a Graph Attention Adaptive Transfer Network, which integrates GNNs and transfer learning to improve link prediction (LP) performance on large-scale, sparse graphs.


<details>
  <summary>Details</summary>
Motivation: Existing GNN methods struggle with large-scale sparse graphs and lack effective transfer learning capabilities across different graph datasets, especially for LP tasks.

Method: The proposed GAATNet combines pre-training and fine-tuning while utilizing two key strategies: incorporating distant neighbor embeddings in the self-attention module and introducing a lightweight self-adapter module for efficient fine-tuning.

Result: GAATNet outperforms existing methods in LP tasks, with experiments on seven public datasets showing state-of-the-art performance.

Conclusion: GAATNet enhances GNN-based transfer learning by addressing efficiency and scalability in LP tasks, offering a general solution with publicly available implementation and datasets.

Abstract: Graph neural networks (GNNs) have brought revolutionary advancements to the field of link prediction (LP), providing powerful tools for mining potential relationships in graphs. However, existing methods face challenges when dealing with large-scale sparse graphs and the need for a high degree of alignment between different datasets in transfer learning. Besides, although self-supervised methods have achieved remarkable success in many graph tasks, prior research has overlooked the potential of transfer learning to generalize across different graph datasets. To address these limitations, we propose a novel Graph Attention Adaptive Transfer Network (GAATNet). It combines the advantages of pre-training and fine-tuning to capture global node embedding information across datasets of different scales, ensuring efficient knowledge transfer and improved LP performance. To enhance the model's generalization ability and accelerate training, we design two key strategies: 1) Incorporate distant neighbor embeddings as biases in the self-attention module to capture global features. 2) Introduce a lightweight self-adapter module during fine-tuning to improve training efficiency. Comprehensive experiments on seven public datasets demonstrate that GAATNet achieves state-of-the-art performance in LP tasks. This study provides a general and scalable solution for LP tasks to effectively integrate GNNs with transfer learning. The source code and datasets are publicly available at https://github.com/DSI-Lab1/GAATNet

</details>


### [367] [How Much Data Is Enough? Uniform Convergence Bounds for Generative & Vision-Language Models under Low-Dimensional Structure](https://arxiv.org/abs/2512.23109)
*Paul M. Thompson*

Main category: cs.LG

TL;DR: This paper explores the conditions under which generative and vision-language models (VLMs) achieve uniformly accurate and well-calibrated predictions, using theoretical and finite-sample analyses.


<details>
  <summary>Details</summary>
Motivation: To address the lack of understanding around when generative and vision-language models provide uniformly accurate predictions across different inputs, classes, or subpopulations, especially in critical biomedical applications.

Method: The study adopts a theoretical approach by focusing on classifiers induced by VLMs and analyzing their behavior under structural assumptions such as smooth dependence on low-dimensional semantic representations.

Result: The paper derives finite-sample uniform convergence bounds for accuracy and calibration, showing that the sample complexity is governed by effective dimension and spectral structure rather than high-dimensional embedding sizes.

Conclusion: Uniformly reliable predictions in data-limited domains such as biomedicine depend on intrinsic dimensions and spectral properties. Average calibration metrics may miss worst-case errors, underscoring the need for careful model evaluation.

Abstract: Modern generative and vision-language models (VLMs) are increasingly used in scientific and medical decision support, where predicted probabilities must be both accurate and well calibrated. Despite strong empirical results with moderate data, it remains unclear when such predictions generalize uniformly across inputs, classes, or subpopulations, rather than only on average-a critical issue in biomedicine, where rare conditions and specific groups can exhibit large errors even when overall loss is low.
  We study this question from a finite-sample perspective and ask: under what structural assumptions can generative and VLM-based predictors achieve uniformly accurate and calibrated behavior with practical sample sizes? Rather than analyzing arbitrary parameterizations, we focus on induced families of classifiers obtained by varying prompts or semantic embeddings within a restricted representation space. When model outputs depend smoothly on a low-dimensional semantic representation-an assumption supported by spectral structure in text and joint image-text embeddings-classical uniform convergence tools yield meaningful non-asymptotic guarantees.
  Our main results give finite-sample uniform convergence bounds for accuracy and calibration functionals of VLM-induced classifiers under Lipschitz stability with respect to prompt embeddings. The implied sample complexity depends on intrinsic/effective dimension, not ambient embedding dimension, and we further derive spectrum-dependent bounds that make explicit how eigenvalue decay governs data requirements. We conclude with implications for data-limited biomedical modeling, including when current dataset sizes can support uniformly reliable predictions and why average calibration metrics may miss worst-case miscalibration.

</details>


### [368] [Cardiac mortality prediction in patients undergoing PCI based on real and synthetic data](https://arxiv.org/abs/2512.22259)
*Daniil Burakov,Ivan Petrov,Dmitrii Khelimskii,Ivan Bessonov,Mikhail Lazarev*

Main category: cs.LG

TL;DR: The paper develops a predictive model using real and synthetic data to assess cardiac death risk in patients after PCI, focusing on improving predictions for minority cases and understanding key influencing factors.


<details>
  <summary>Details</summary>
Motivation: The study aims to predict long-term risks of cardiac death after PCI using clinical and procedural data while addressing the class imbalance problem in prediction outcomes.

Method: A dataset of 2,044 PCI patients was analyzed alongside 500 synthetic samples to tackle class imbalance. Machine learning models and permutation feature importance were used for prediction and feature analysis, alongside experiments removing non-informative features.

Result: Model accuracy was high (0.92-0.93) without oversampling but failed in minority-class recall. Augmentation improved performance on minority cases, accuracy, risk estimation, and reduced brittleness. Four key patient and procedural factors were identified.

Conclusion: Augmenting imbalanced datasets with realistic cases improves predictive performance and provides better risk estimation in clinical settings, making it a valuable approach for routine application in clinical predictions.

Abstract: Patient status, angiographic and procedural characteristics encode crucial signals for predicting long-term outcomes after percutaneous coronary intervention (PCI). The aim of the study was to develop a predictive model for assessing the risk of cardiac death based on the real and synthetic data of patients undergoing PCI and to identify the factors that have the greatest impact on mortality. We analyzed 2,044 patients, who underwent a PCI for bifurcation lesions. The primary outcome was cardiac death at 3-year follow-up. Several machine learning models were applied to predict three-year mortality after PCI. To address class imbalance and improve the representation of the minority class, an additional 500 synthetic samples were generated and added to the training set. To evaluate the contribution of individual features to model performance, we applied permutation feature importance. An additional experiment was conducted to evaluate how the model's predictions would change after removing non-informative features from the training and test datasets. Without oversampling, all models achieve high overall accuracy (0.92-0.93), yet they almost completely ignore the minority class. Across models, augmentation consistently increases minority-class recall with minimal loss of AUROC, improves probability quality, and yields more clinically reasonable risk estimates on the constructed severe profiles. According to feature importance analysis, four features emerged as the most influential: Age, Ejection Fraction, Peripheral Artery Disease, and Cerebrovascular Disease. These results show that straightforward augmentation with realistic and extreme cases can expose, quantify, and reduce brittleness in imbalanced clinical prediction using only tabular records, and motivate routine reporting of probability quality and stress tests alongside headline metrics.

</details>


### [369] [Principled Algorithms for Optimizing Generalized Metrics in Binary Classification](https://arxiv.org/abs/2512.23133)
*Anqi Mao,Mehryar Mohri,Yutao Zhong*

Main category: cs.LG

TL;DR: The paper discusses optimizing evaluation metrics for imbalanced data settings, proposes a novel approach using surrogate loss functions, and introduces the METRO algorithm with strong theoretical and empirical results.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of optimizing non-standard evaluation metrics like Fβ and Jaccard in imbalanced classification tasks where traditional methods fail to ensure optimal or computationally efficient solutions.

Method: Proposes a framework to reformulate metric optimization as cost-sensitive learning, designing surrogate loss functions with provable consistency and finite-sample guarantees.

Result: Introduces the METRO algorithm, supported by theory, and demonstrates its effectiveness over baselines in experiments.

Conclusion: The proposed METRO algorithm and framework present significant advancements in metric optimization, offering theoretical guarantees and reliable performance for imbalanced classification.

Abstract: In applications with significant class imbalance or asymmetric costs, metrics such as the $F_β$-measure, AM measure, Jaccard similarity coefficient, and weighted accuracy offer more suitable evaluation criteria than standard binary classification loss. However, optimizing these metrics present significant computational and statistical challenges. Existing approaches often rely on the characterization of the Bayes-optimal classifier, and use threshold-based methods that first estimate class probabilities and then seek an optimal threshold. This leads to algorithms that are not tailored to restricted hypothesis sets and lack finite-sample performance guarantees. In this work, we introduce principled algorithms for optimizing generalized metrics, supported by $H$-consistency and finite-sample generalization bounds. Our approach reformulates metric optimization as a generalized cost-sensitive learning problem, enabling the design of novel surrogate loss functions with provable $H$-consistency guarantees. Leveraging this framework, we develop new algorithms, METRO (Metric Optimization), with strong theoretical performance guarantees. We report the results of experiments demonstrating the effectiveness of our methods compared to prior baselines.

</details>


### [370] [The Physics Constraint Paradox: When Removing Explicit Constraints Improves Physics-Informed Data for Machine Learning](https://arxiv.org/abs/2512.22261)
*Rahul D Ray*

Main category: cs.LG

TL;DR: The paper investigates a physics-informed grating coupler spectrum generator, highlighting the significance of selectively applying physics constraints to improve data generation and downstream machine learning performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance machine learning in scientific applications where real data is scarce by systematically evaluating which physical constraints are necessary for accurate and efficient data generation.

Method: The authors conduct a systematic ablation study by selectively removing explicit physics constraints (e.g., energy conservation enforcement, Fabry-Perot oscillations, etc.) and analyzing the impact on a spectrum generator's performance.

Result: They discovered that explicit energy conservation enforcement is redundant when equations are physically consistent, while Fabry-Perot oscillations significantly affect bandwidth variation. Removing these oscillations improves downstream bandwidth prediction accuracy notably.

Conclusion: Physics-informed dataset design benefits from selectively applying constraints. Machine learning metrics can serve as diagnostic tools for evaluating constraint relevance. The approach enables efficient and accurate generation of training datasets.

Abstract: Physics-constrained data generation is essential for machine learning in scientific domains where real data are scarce; however, existing approaches often over-constrain models without identifying which physical components are necessary. We present a systematic ablation study of a physics-informed grating coupler spectrum generator that maps five geometric parameters to 100-point spectral responses. By selectively removing explicit energy conservation enforcement, Fabry-Perot oscillations, bandwidth variation, and noise, we uncover a physics constraint paradox: explicit energy conservation enforcement is mathematically redundant when the underlying equations are physically consistent, with constrained and unconstrained variants achieving identical conservation accuracy (mean error approximately 7 x 10^-9). In contrast, Fabry-Perot oscillations dominate threshold-based bandwidth variability, accounting for a 72 percent reduction in half-maximum bandwidth spread when removed (with bandwidth spread reduced from 132.3 nm to 37.4 nm). We further identify a subtle pitfall: standard noise-addition-plus-renormalization pipelines introduce 0.5 percent unphysical negative absorption values. The generator operates at 200 samples per second, enabling high-throughput data generation and remaining orders of magnitude faster than typical full-wave solvers reported in the literature. Finally, downstream machine learning evaluation reveals a clear physics-learnability trade-off: while central wavelength prediction remains unaffected, removing Fabry-Perot oscillations improves bandwidth prediction accuracy by 31.3 percent in R-squared and reduces RMSE by 73.8 percent. These findings provide actionable guidance for physics-informed dataset design and highlight machine learning performance as a diagnostic tool for assessing constraint relevance.

</details>


### [371] [Valori: A Deterministic Memory Substrate for AI Systems](https://arxiv.org/abs/2512.22280)
*Varshith Gudur*

Main category: cs.LG

TL;DR: The abstract introduces Valori, a system that ensures deterministic AI memory, avoiding non-determinism issues in AI systems by using fixed-point arithmetic instead of floating-point operations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the fundamental non-determinism in modern AI systems due to floating-point arithmetic and hardware discrepancies, which cause issues in replayability, verification, and audit trails.

Method: The paper proposes Valori, which uses fixed-point arithmetic (Q16.16) and models memory as a deterministic state machine.

Result: Valori guarantees bit-identical memory states and search results across platforms, ensuring deterministic behavior and enabling trustworthy AI system design.

Conclusion: Deterministic memory is essential for trustworthy AI systems, and Valori proves to be a viable solution to achieve this. The open-source implementation facilitates further development and usage.

Abstract: Modern AI systems rely on vector embeddings stored and searched using floating-point arithmetic. While effective for approximate similarity search, this design introduces fundamental non-determinism: identical models, inputs, and code can produce different memory states and retrieval results across hardware architectures (e.g., x86 vs. ARM). This prevents replayability and safe deployment, leading to silent data divergence that prevents post-hoc verification and compromises audit trails in regulated sectors. We present Valori, a deterministic AI memory substrate that replaces floating-point memory operations with fixed-point arithmetic (Q16.16) and models memory as a replayable state machine. Valori guarantees bit-identical memory states, snapshots, and search results across platforms. We demonstrate that non-determinism arises before indexing or retrieval and show how Valori enforces determinism at the memory boundary. Our results suggest that deterministic memory is a necessary primitive for trustworthy AI systems. The reference implementation is open-source and available at https://github.com/varshith-Git/Valori-Kernel (archived at https://zenodo.org/records/18022660).

</details>


### [372] [LuxIA: A Lightweight Unitary matriX-based Framework Built on an Iterative Algorithm for Photonic Neural Network Training](https://arxiv.org/abs/2512.22264)
*Tzamn Melendez Carmona,Federico Marchesin,Marco P. Abrate,Peter Bienstman,Stefano Di Carlo,Alessandro Savino Senior*

Main category: cs.LG

TL;DR: The paper introduces the Slicing method and the LuxIA framework to address scalability and computational challenges in photonic neural network (PNN) simulations, enabling faster and more efficient training of large PNNs.


<details>
  <summary>Details</summary>
Motivation: Current PNN simulation tools struggle with scalability due to the computational demands of transfer matrix calculations, impeding the training of large-scale PNNs.

Method: The proposed solution is the Slicing method, an efficient transfer matrix computation compatible with back-propagation, integrated into the LuxIA framework for unified simulation and training.

Result: LuxIA, utilizing the Slicing method, significantly reduces memory and time requirements, outperforming existing tools in speed and scalability across diverse photonic architectures and datasets.

Conclusion: This work advances PNN simulation, making larger and more complex architectures feasible, and fosters innovation in AI hardware with photonic technologies, promoting broader adoption and exploration.

Abstract: PNNs present promising opportunities for accelerating machine learning by leveraging the unique benefits of photonic circuits. However, current state of the art PNN simulation tools face significant scalability challenges when training large-scale PNNs, due to the computational demands of transfer matrix calculations, resulting in high memory and time consumption. To overcome these limitations, we introduce the Slicing method, an efficient transfer matrix computation approach compatible with back-propagation. We integrate this method into LuxIA, a unified simulation and training framework. The Slicing method substantially reduces memory usage and execution time, enabling scalable simulation and training of large PNNs. Experimental evaluations across various photonic architectures and standard datasets, including MNIST, Digits, and Olivetti Faces, show that LuxIA consistently surpasses existing tools in speed and scalability. Our results advance the state of the art in PNN simulation, making it feasible to explore and optimize larger, more complex architectures. By addressing key computational bottlenecks, LuxIA facilitates broader adoption and accelerates innovation in AI hardware through photonic technologies. This work paves the way for more efficient and scalable photonic neural network research and development.

</details>


### [373] [LLMTM: Benchmarking and Optimizing LLMs for Temporal Motif Analysis in Dynamic Graphs](https://arxiv.org/abs/2512.22266)
*Bing Hao,Minglai Shao,Zengyi Wo,Yunlong Chu,Yuhang Liu,Ruijie Wang*

Main category: cs.LG

TL;DR: The paper explores the use of Large Language Models (LLMs) to analyze temporal motifs—key elements in dynamic graphs reflecting anomalies and structural dynamics. It introduces a benchmark called LLMTM and evaluates performance across various tasks and models, proposing a cost-efficient dispatch strategy.


<details>
  <summary>Details</summary>
Motivation: To understand the potential of LLMs in analyzing temporal motifs of dynamic graphs, which are crucial for detecting anomalies and unique phenomena in graph structures.

Method: Introduced a benchmark (LLMTM), tested nine LLMs across tailored tasks, developed a tool-augmented LLM agent, and proposed a structure-aware dispatcher that balances accuracy and cost.

Result: The tool-augmented LLM agent achieves high accuracy in temporal motif tasks, and the structure-aware dispatcher reduces computational costs while maintaining accuracy.

Conclusion: LLMs show promise in dynamic graph analysis, and intelligent query dispatching mechanisms can optimize performance-cost balance.

Abstract: The widespread application of Large Language Models (LLMs) has motivated a growing interest in their capacity for processing dynamic graphs. Temporal motifs, as an elementary unit and important local property of dynamic graphs which can directly reflect anomalies and unique phenomena, are essential for understanding their evolutionary dynamics and structural features. However, leveraging LLMs for temporal motif analysis on dynamic graphs remains relatively unexplored. In this paper, we systematically study LLM performance on temporal motif-related tasks. Specifically, we propose a comprehensive benchmark, LLMTM (Large Language Models in Temporal Motifs), which includes six tailored tasks across nine temporal motif types. We then conduct extensive experiments to analyze the impacts of different prompting techniques and LLMs (including nine models: openPangu-7B, the DeepSeek-R1-Distill-Qwen series, Qwen2.5-32B-Instruct, GPT-4o-mini, DeepSeek-R1, and o3) on model performance. Informed by our benchmark findings, we develop a tool-augmented LLM agent that leverages precisely engineered prompts to solve these tasks with high accuracy. Nevertheless, the high accuracy of the agent incurs a substantial cost. To address this trade-off, we propose a simple yet effective structure-aware dispatcher that considers both the dynamic graph's structural properties and the LLM's cognitive load to intelligently dispatch queries between the standard LLM prompting and the more powerful agent. Our experiments demonstrate that the structure-aware dispatcher effectively maintains high accuracy while reducing cost.

</details>


### [374] [A Simple, Optimal and Efficient Algorithm for Online Exp-Concave Optimization](https://arxiv.org/abs/2512.23190)
*Yi-Han Wang,Peng Zhao,Zhi-Hua Zhou*

Main category: cs.LG

TL;DR: The paper introduces LightONS, a computationally efficient variant of Online Newton Step (ONS) reducing runtime for eXp-concave problems while maintaining optimal regret, solving a COLT'13 open SXO problem.


<details>
  <summary>Details</summary>
Motivation: Address computational complexities of Online and Stochastic eXp-concave Optimization, particularly the runtime bottleneck of ONS and improving its scalability.

Method: LightONS introduces a hysteresis mechanism that delays expensive Mahalanobis projections until necessary, leveraging domain-conversion techniques from parameter-free online learning.

Result: LightONS reduces runtime to $O(d^2T + d^ω\sqrt{T\log T})$, achieves $O(d\log T)$ regret, and answers a COLT'13 open problem by reducing SXO runtime to $\tilde{O}(d^{3}/ε)$.

Conclusion: LightONS preserves the efficacy of ONS while significantly enhancing computational efficiency, making it a versatile solution for broader optimization problems and scenarios.

Abstract: Online eXp-concave Optimization (OXO) is a fundamental problem in online learning. The standard algorithm, Online Newton Step (ONS), balances statistical optimality and computational practicality, guaranteeing an optimal regret of $O(d \log T)$, where $d$ is the dimension and $T$ is the time horizon. ONS faces a computational bottleneck due to the Mahalanobis projections at each round. This step costs $Ω(d^ω)$ arithmetic operations for bounded domains, even for the unit ball, where $ω\in (2,3]$ is the matrix-multiplication exponent. As a result, the total runtime can reach $\tilde{O}(d^ωT)$, particularly when iterates frequently oscillate near the domain boundary. For Stochastic eXp-concave Optimization (SXO), computational cost is also a challenge. Deploying ONS with online-to-batch conversion for SXO requires $T = \tilde{O}(d/ε)$ rounds to achieve an excess risk of $ε$, and thereby necessitates an $\tilde{O}(d^{ω+1}/ε)$ runtime. A COLT'13 open problem posed by Koren [2013] asks for an SXO algorithm with runtime less than $\tilde{O}(d^{ω+1}/ε)$.
  This paper proposes a simple variant of ONS, LightONS, which reduces the total runtime to $O(d^2 T + d^ω\sqrt{T \log T})$ while preserving the optimal $O(d \log T)$ regret. LightONS implies an SXO method with runtime $\tilde{O}(d^3/ε)$, thereby answering the open problem. Importantly, LightONS preserves the elegant structure of ONS by leveraging domain-conversion techniques from parameter-free online learning to introduce a hysteresis mechanism that delays expensive Mahalanobis projections until necessary. This design enables LightONS to serve as an efficient plug-in replacement of ONS in broader scenarios, even beyond regret minimization, including gradient-norm adaptive regret, parametric stochastic bandits, and memory-efficient online learning.

</details>


### [375] [Hierarchical Stacking Optimization Using Dirichlet's Process (SoDip): Towards Accelerated Design for Graft Polymerization](https://arxiv.org/abs/2512.22279)
*Amgad Ahmed Ali Ibrahim,Hein Htet,Ryoji Asahi*

Main category: cs.LG

TL;DR: The paper presents a novel 4-layer hierarchical optimization framework (SoDip) for improving reproducibility in radiation-induced grafting (RIG) by addressing challenges caused by variability in base-film morphology.


<details>
  <summary>Details</summary>
Motivation: To tackle reproducibility issues in RIG processes caused by unreported variability in polymer film morphology, which affects the diffusion of monomers and the uniformity of grafting.

Method: Proposes a data-driven hierarchical framework called SoDip which integrates transformer-based encoding, machine learning models (TabNet and XGBoost), Gaussian Process Regression with uncertainty quantification, and Bayesian Optimization, trained on a curated multimodal dataset.

Result: Achieved ~33% improvement over standard Gaussian Process Regression models in cross-validation, with calibrated confidence intervals to identify low-reproducibility regimes.

Conclusion: The SoDip framework enables morphology-aware, reproducible research in graft polymerization by effectively integrating and learning from diverse data sources, outperforming prior models.

Abstract: Radiation-induced grafting (RIG) enables precise functionalization of polymer films for ion-exchange membranes, CO2-separation membranes, and battery electrolytes by generating radicals on robust substrates to graft desired monomers. However, reproducibility remains limited due to unreported variability in base-film morphology (crystallinity, grain orientation, free volume), which governs monomer diffusion, radical distribution, and the Trommsdorff effect, leading to spatial graft gradients and performance inconsistencies. We present a hierarchical stacking optimization framework with a Dirichlet's Process (SoDip), a hierarchical data-driven framework integrating: (1) a decoder-only Transformer (DeepSeek-R1) to encode textual process descriptors (irradiation source, grafting type, substrate manufacturer); (2) TabNet and XGBoost for modelling multimodal feature interactions; (3) Gaussian Process Regression (GPR) with Dirichlet Process Mixture Models (DPMM) for uncertainty quantification and heteroscedasticity; and (4) Bayesian Optimization for efficient exploration of high-dimensional synthesis space. A diverse dataset was curated using ChemDataExtractor 2.0 and WebPlotDigitizer, incorporating numerical and textual variables across hundreds of RIG studies. In cross-validation, SoDip achieved ~33% improvement over GPR while providing calibrated confidence intervals that identify low-reproducibility regimes. Its stacked architecture integrates sparse textual and numerical inputs of varying quality, outperforming prior models and establishing a foundation for reproducible, morphology-aware design in graft polymerization research.

</details>


### [376] [On the Sample Complexity of Learning for Blind Inverse Problems](https://arxiv.org/abs/2512.23405)
*Nathan Buskulic,Luca Calatroni,Lorenzo Rosasco,Silvia Villa*

Main category: cs.LG

TL;DR: The paper investigates a theoretical framework for solving blind inverse problems using Linear Minimum Mean Square Estimators (LMMSEs), deriving closed-form solutions and finite-sample error bounds.


<details>
  <summary>Details</summary>
Motivation: Blind inverse problems occur when the forward operator is partially or fully unknown, making traditional methods unsuitable. The need for interpretable and theoretically supported solutions in fields like imaging drives this research.

Method: The paper uses a theoretical analysis of Linear Minimum Mean Square Estimators (LMMSEs), deriving closed-form expressions, exploring Tikhonov-regularized formulations, and proving convergence and finite-sample error bounds.

Result: The authors derive key theoretical results for blind inverse problems, including optimal estimators, convergence proofs, and error bounds. Practical numerical experiments validate the theoretical predictions.

Conclusion: The results provide a deeper understanding of blind inverse problems through rigorous theoretical insights and numerical validation, offering a foundation for more reliable methods in applied domains.

Abstract: Blind inverse problems arise in many experimental settings where the forward operator is partially or entirely unknown. In this context, methods developed for the non-blind case cannot be adapted in a straightforward manner. Recently, data-driven approaches have been proposed to address blind inverse problems, demonstrating strong empirical performance and adaptability. However, these methods often lack interpretability and are not supported by rigorous theoretical guarantees, limiting their reliability in applied domains such as imaging inverse problems. In this work, we shed light on learning in blind inverse problems within the simplified yet insightful framework of Linear Minimum Mean Square Estimators (LMMSEs). We provide an in-depth theoretical analysis, deriving closed-form expressions for optimal estimators and extending classical results. In particular, we establish equivalences with suitably chosen Tikhonov-regularized formulations, where the regularization depends explicitly on the distributions of the unknown signal, the noise, and the random forward operators. We also prove convergence results under appropriate source condition assumptions. Furthermore, we derive rigorous finite-sample error bounds that characterize the performance of learned estimators as a function of the noise level, problem conditioning, and number of available samples. These bounds explicitly quantify the impact of operator randomness and reveal the associated convergence rates as this randomness vanishes. Finally, we validate our theoretical findings through illustrative numerical experiments that confirm the predicted convergence behavior.

</details>


### [377] [DBAW-PIKAN: Dynamic Balance Adaptive Weight Kolmogorov-Arnold Neural Network for Solving Partial Differential Equations](https://arxiv.org/abs/2512.22283)
*Guokan Chen,Yao Xiao*

Main category: cs.LG

TL;DR: The paper introduces a novel approach, DBAW-PIKAN, to address challenges in Physics-informed neural networks (PINNs) for multi-scale or high-frequency problems, improving accuracy and convergence.


<details>
  <summary>Details</summary>
Motivation: To overcome stiffness in gradient flow and spectral bias in PINNs, which hinder their ability to handle multi-scale or high-frequency features.

Method: Proposes DBAW-PIKAN integrating a Kolmogorov-Arnold network with learnable B-splines and adaptive dynamic weighting strategy to improve representation and convergence.

Result: DBAW-PIKAN achieves faster convergence and significantly better accuracy than existing models in solving equations (e.g., Klein-Gordon and Burgers equations).

Conclusion: DBAW-PIKAN effectively addresses key limitations in PINNs, showcasing superior performance and generalization without added computational cost.

Abstract: Physics-informed neural networks (PINNs) have led to significant advancements in scientific computing by integrating fundamental physical principles with advanced data-driven techniques. However, when dealing with problems characterized by multi-scale or high-frequency features, PINNs encounter persistent and severe challenges related to stiffness in gradient flow and spectral bias, which significantly limit their predictive capabilities. To address these issues, this paper proposes a Dynamic Balancing Adaptive Weighting Physics-Informed Kolmogorov-Arnold Network (DBAW-PIKAN), designed to mitigate such gradient-related failure modes and overcome the bottlenecks in function representation. The core of DBAW-PIKAN combines the Kolmogorov-Arnold network architecture, based on learnable B-splines, with an adaptive weighting strategy that incorporates a dynamic decay upper bound. Compared to baseline models, the proposed method accelerates the convergence process and improves solution accuracy by at least an order of magnitude without introducing additional computational complexity. A series of numerical benchmarks, including the Klein-Gordon, Burgers, and Helmholtz equations, demonstrate the significant advantages of DBAW-PIKAN in enhancing both accuracy and generalization performance.

</details>


### [378] [Cluster Aggregated GAN (CAG): A Cluster-Based Hybrid Model for Appliance Pattern Generation](https://arxiv.org/abs/2512.22287)
*Zikun Guoa,Adeyinka. P. Adedigbaa,Rammohan Mallipeddi*

Main category: cs.LG

TL;DR: The paper introduces Cluster Aggregated GAN, a framework to address current inefficiencies in synthesizing appliance load patterns by treating intermittent and continuous appliances differently during data generation.


<details>
  <summary>Details</summary>
Motivation: There is a scarcity of labeled datasets for developing non-intrusive load monitoring algorithms, and current GAN-based methods fail to handle behavioral differences in appliance types, leading to low-fidelity results.

Method: The Cluster Aggregated GAN framework uses separate specialized branches for intermittent and continuous appliances. Intermittent appliances are further grouped by an activation clustering module, while continuous appliances are modeled using an LSTM-based generator.

Result: Experimental evaluation on the UVIC smart plug dataset demonstrated that the framework outperforms existing methods across realism, diversity, and training stability. Clustering greatly improved data generation interpretability and scalability.

Conclusion: The proposed Cluster Aggregated GAN framework is highly effective for generating synthetic appliance data, addressing key issues in non-intrusive load monitoring research.

Abstract: Synthetic appliance data are essential for developing non-intrusive load monitoring algorithms and enabling privacy preserving energy research, yet the scarcity of labeled datasets remains a significant barrier. Recent GAN-based methods have demonstrated the feasibility of synthesizing load patterns, but most existing approaches treat all devices uniformly within a single model, neglecting the behavioral differences between intermittent and continuous appliances and resulting in unstable training and limited output fidelity. To address these limitations, we propose the Cluster Aggregated GAN framework, a hybrid generative approach that routes each appliance to a specialized branch based on its behavioral characteristics. For intermittent appliances, a clustering module groups similar activation patterns and allocates dedicated generators for each cluster, ensuring that both common and rare operational modes receive adequate modeling capacity. Continuous appliances follow a separate branch that employs an LSTM-based generator to capture gradual temporal evolution while maintaining training stability through sequence compression. Extensive experiments on the UVIC smart plug dataset demonstrate that the proposed framework consistently outperforms baseline methods across metrics measuring realism, diversity, and training stability, and that integrating clustering as an active generative component substantially improves both interpretability and scalability. These findings establish the proposed framework as an effective approach for synthetic load generation in non-intrusive load monitoring research.

</details>


### [379] [ML Compass: Navigating Capability, Cost, and Compliance Trade-offs in AI Model Deployment](https://arxiv.org/abs/2512.23487)
*Vassilis Digalakis,Ramayya Krishnan,Gonzalo Martin Fernandez,Agni Orfanoudaki*

Main category: cs.LG

TL;DR: The paper proposes 'ML Compass,' a framework for selecting AI models considering utility, costs, and compliance together, rather than relying only on capability-focused leaderboards.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between AI model rankings based on capability leaderboards and real-world deployment decisions affected by utility, constraints, and compliance considerations.

Method: Models are selected through constrained optimization over a capability-cost frontier, using a four-step pipeline to analyze model descriptors, compute an empirical frontier, learn a user-specific utility function, and recommend optimized models.

Result: The framework is validated in conversational AI and healthcare settings, producing deployment-based recommendations that differ from capability-only rankings.

Conclusion: By integrating deployment constraints and trade-offs, this framework offers a more practical and nuanced approach to AI model selection for real-world applications.

Abstract: We study how organizations should select among competing AI models when user utility, deployment costs, and compliance requirements jointly matter. Widely used capability leaderboards do not translate directly into deployment decisions, creating a capability -- deployment gap; to bridge it, we take a systems-level view in which model choice is tied to application outcomes, operating constraints, and a capability-cost frontier. We develop ML Compass, a framework that treats model selection as constrained optimization over this frontier. On the theory side, we characterize optimal model configurations under a parametric frontier and show a three-regime structure in optimal internal measures: some dimensions are pinned at compliance minima, some saturate at maximum levels, and the remainder take interior values governed by frontier curvature. We derive comparative statics that quantify how budget changes, regulatory tightening, and technological progress propagate across capability dimensions and costs. On the implementation side, we propose a pipeline that (i) extracts low-dimensional internal measures from heterogeneous model descriptors, (ii) estimates an empirical frontier from capability and cost data, (iii) learns a user- or task-specific utility function from interaction outcome data, and (iv) uses these components to target capability-cost profiles and recommend models. We validate ML Compass with two case studies: a general-purpose conversational setting using the PRISM Alignment dataset and a healthcare setting using a custom dataset we build using HealthBench. In both environments, our framework produces recommendations -- and deployment-aware leaderboards based on predicted deployment value under constraints -- that can differ materially from capability-only rankings, and clarifies how trade-offs between capability, cost, and safety shape optimal model choice.

</details>


### [380] [Co-GRPO: Co-Optimized Group Relative Policy Optimization for Masked Diffusion Model](https://arxiv.org/abs/2512.22288)
*Renping Zhou,Zanlin Ni,Tianyi Chen,Zeyu Liu,Yang Yue,Yulin Wang,Yuxuan Wang,Jingshu Liu,Gao Huang*

Main category: cs.LG

TL;DR: Masked Diffusion Models (MDMs) currently face a disconnect between training and inference processes. The paper introduces Co-GRPO to unify these processes using a Markov Decision Process (MDP) framework, optimizing both model and inference schedules together.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the disparity between the simplified training process of MDMs and their complex iterative inference procedures to enhance generation quality.

Method: Co-GRPO reformulates MDM generation as a Markov Decision Process (MDP) and employs Group Relative Policy Optimization at the trajectory level to optimize model and inference schedules jointly with a shared reward mechanism.

Result: The Co-GRPO method demonstrates substantial improvements in generation quality across multiple benchmarks, including ImageReward, HPS, GenEval, and DPG-Bench.

Conclusion: This approach effectively aligns MDM training with inference through holistic optimization, showcasing enhanced generation results and demonstrating the utility of the proposed framework.

Abstract: Recently, Masked Diffusion Models (MDMs) have shown promising potential across vision, language, and cross-modal generation. However, a notable discrepancy exists between their training and inference procedures. In particular, MDM inference is a multi-step, iterative process governed not only by the model itself but also by various schedules that dictate the token-decoding trajectory (e.g., how many tokens to decode at each step). In contrast, MDMs are typically trained using a simplified, single-step BERT-style objective that masks a subset of tokens and predicts all of them simultaneously. This step-level simplification fundamentally disconnects the training paradigm from the trajectory-level nature of inference, leaving the inference schedules never optimized during training. In this paper, we introduce Co-GRPO, which reformulates MDM generation as a unified Markov Decision Process (MDP) that jointly incorporates both the model and the inference schedule. By applying Group Relative Policy Optimization at the trajectory level, Co-GRPO cooperatively optimizes model parameters and schedule parameters under a shared reward, without requiring costly backpropagation through the multi-step generation process. This holistic optimization aligns training with inference more thoroughly and substantially improves generation quality. Empirical results across four benchmarks-ImageReward, HPS, GenEval, and DPG-Bench-demonstrate the effectiveness of our approach. For more details, please refer to our project page: https://co-grpo.github.io/ .

</details>


### [381] [Osmotic Learning: A Self-Supervised Paradigm for Decentralized Contextual Data Representation](https://arxiv.org/abs/2512.23096)
*Mario Colosi,Reza Farahani,Maria Fazio,Radu Prodan,Massimo Villari*

Main category: cs.LG

TL;DR: The paper introduces Osmotic Learning (OSM-L), a decentralized learning paradigm that synthesizes compact data representations without raw data sharing, achieving strong performance in identifying contextual patterns.


<details>
  <summary>Details</summary>
Motivation: Distributed systems contain interdependent data sources with hidden relationships; understanding them is crucial but challenging without sharing raw data.

Method: The OSM-L method uses self-supervised learning to iteratively align local data representations, synthesizing contextual patterns across distributed entities while functioning as a decentralized clustering mechanism.

Result: Experimental results show that OSM-L achieves over 0.99 accuracy in aligning local data representations and preserving contextual understanding.

Conclusion: OSM-L effectively learns and uncovers latent contextual knowledge in distributed systems while maintaining privacy and data efficiency.

Abstract: Data within a specific context gains deeper significance beyond its isolated interpretation. In distributed systems, interdependent data sources reveal hidden relationships and latent structures, representing valuable information for many applications. This paper introduces Osmotic Learning (OSM-L), a self-supervised distributed learning paradigm designed to uncover higher-level latent knowledge from distributed data. The core of OSM-L is osmosis, a process that synthesizes dense and compact representation by extracting contextual information, eliminating the need for raw data exchange between distributed entities. OSM-L iteratively aligns local data representations, enabling information diffusion and convergence into a dynamic equilibrium that captures contextual patterns. During training, it also identifies correlated data groups, functioning as a decentralized clustering mechanism. Experimental results confirm OSM-L's convergence and representation capabilities on structured datasets, achieving over 0.99 accuracy in local information alignment while preserving contextual integrity.

</details>


### [382] [Trustworthy Machine Learning under Distribution Shifts](https://arxiv.org/abs/2512.23524)
*Zhuo Huang*

Main category: cs.LG

TL;DR: Focuses on ensuring trustworthiness in AI under distribution shifts by studying robustness, explainability, and adaptability.


<details>
  <summary>Details</summary>
Motivation: Address the challenges posed by distribution shifts and enhance trust in machine learning systems.

Method: Analyzes three types of distribution shifts (perturbation, domain, and modality) and proposes solutions targeting robustness, explainability, and adaptability.

Result: Provides insights and approaches to improve reliability, versatility, and trustworthiness in AI systems.

Conclusion: Advances in machine learning must emphasize trustworthy systems capable of operating reliably under varied conditions.

Abstract: Machine Learning (ML) has been a foundational topic in artificial intelligence (AI), providing both theoretical groundwork and practical tools for its exciting advancements. From ResNet for visual recognition to Transformer for vision-language alignment, the AI models have achieved superior capability to humans. Furthermore, the scaling law has enabled AI to initially develop general intelligence, as demonstrated by Large Language Models (LLMs). To this stage, AI has had an enormous influence on society and yet still keeps shaping the future for humanity. However, distribution shift remains a persistent ``Achilles' heel'', fundamentally limiting the reliability and general usefulness of ML systems. Moreover, generalization under distribution shift would also cause trust issues for AIs. Motivated by these challenges, my research focuses on \textit{Trustworthy Machine Learning under Distribution Shifts}, with the goal of expanding AI's robustness, versatility, as well as its responsibility and reliability. We carefully study the three common distribution shifts into: (1) Perturbation Shift, (2) Domain Shift, and (3) Modality Shift. For all scenarios, we also rigorously investigate trustworthiness via three aspects: (1) Robustness, (2) Explainability, and (3) Adaptability. Based on these dimensions, we propose effective solutions and fundamental insights, meanwhile aiming to enhance the critical ML problems, such as efficiency, adaptability, and safety.

</details>


### [383] [When Algorithms Manage Humans: A Double Machine Learning Approach to Estimating Nonlinear Effects of Algorithmic Control on Gig Worker Performance and Wellbeing](https://arxiv.org/abs/2512.22290)
*Arunkumar V,Nivethitha S,Sharan Srinivas,Gangadharan G. R*

Main category: cs.LG

TL;DR: The paper examines how algorithmic management affects gig workers and finds that transparency in oversight improves both worker wellbeing and performance. It uses a Double Machine Learning framework to uncover nonmonotonic patterns.


<details>
  <summary>Details</summary>
Motivation: To understand whether person-centered management can coexist with algorithmic oversight, especially as worker responses to algorithms are rarely linear.

Method: Employing a Double Machine Learning framework to estimate a moderated mediation model using survey data from 464 gig workers.

Result: Supportive HR practices enhance worker wellbeing, but their impact on performance weakens under unclear algorithmic oversight. Transparent oversight strengthens the link to performance.

Conclusion: Platform designs with clear and credible rules for algorithmic control improve worker outcomes and performance while addressing complexity in non-linear data analysis.

Abstract: A central question for the future of work is whether person centered management can survive when algorithms take on managerial roles. Standard tools often miss what is happening because worker responses to algorithmic systems are rarely linear. We use a Double Machine Learning framework to estimate a moderated mediation model without imposing restrictive functional forms. Using survey data from 464 gig workers, we find a clear nonmonotonic pattern. Supportive HR practices improve worker wellbeing, but their link to performance weakens in a murky middle where algorithmic oversight is present yet hard to interpret. The relationship strengthens again when oversight is transparent and explainable. These results show why simple linear specifications can miss the pattern and sometimes suggest the opposite conclusion. For platform design, the message is practical: control that is only partly defined creates confusion, but clear rules and credible recourse can make strong oversight workable. Methodologically, the paper shows how Double Machine Learning can be used to estimate conditional indirect effects in organizational research without forcing the data into a linear shape.

</details>


### [384] [FairGFL: Privacy-Preserving Fairness-Aware Federated Learning with Overlapping Subgraphs](https://arxiv.org/abs/2512.23235)
*Zihao Zhou,Shusen Yang,Fangyuan Zhao,Xuebin Ren*

Main category: cs.LG

TL;DR: The paper proposes FairGFL, an algorithm to address fairness issues due to imbalanced overlapping subgraphs in graph federated learning while preserving model utility and privacy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore and mitigate the unexplored negative effects of imbalanced overlapping subgraphs in graph federated learning, which lead to unfairness across clients.

Method: The authors design FairGFL with a weighted aggregation method for fairness using privacy-preserving estimations of overlapping subgraphs and a regularizer to balance model utility and fairness.

Result: FairGFL demonstrates superior model utility and fairness compared to four baseline algorithms on four graph benchmark datasets.

Conclusion: FairGFL effectively addresses the fairness issue in graph federated learning caused by imbalanced overlaps while retaining privacy and enhancing model performance.

Abstract: Graph federated learning enables the collaborative extraction of high-order information from distributed subgraphs while preserving the privacy of raw data. However, graph data often exhibits overlap among different clients. Previous research has demonstrated certain benefits of overlapping data in mitigating data heterogeneity. However, the negative effects have not been explored, particularly in cases where the overlaps are imbalanced across clients. In this paper, we uncover the unfairness issue arising from imbalanced overlapping subgraphs through both empirical observations and theoretical reasoning. To address this issue, we propose FairGFL (FAIRness-aware subGraph Federated Learning), a novel algorithm that enhances cross-client fairness while maintaining model utility in a privacy-preserving manner. Specifically, FairGFL incorporates an interpretable weighted aggregation approach to enhance fairness across clients, leveraging privacy-preserving estimation of their overlapping ratios. Furthermore, FairGFL improves the tradeoff between model utility and fairness by integrating a carefully crafted regularizer into the federated composite loss function. Through extensive experiments on four benchmark graph datasets, we demonstrate that FairGFL outperforms four representative baseline algorithms in terms of both model utility and fairness.

</details>


### [385] [Multi-Head Spectral-Adaptive Graph Anomaly Detection](https://arxiv.org/abs/2512.22291)
*Qingyue Cao,Bo Jin,Changwei Gong,Xin Tong,Wenzheng Li,Xiaodong Zhou*

Main category: cs.LG

TL;DR: The paper introduces MHSA-GNN, a graph neural network model designed for anomaly detection, overcoming the limitations of existing methods, including fixed filters and over-smoothing, and demonstrates superior performance on diverse graph datasets.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenges of detecting anomalous nodes in graphs characterized by complex and mixed patterns (homophily and heterophily), where existing methods fail due to over-smoothing and a lack of adaptability.

Method: The paper proposes the Multi-Head Spectral-Adaptive Graph Neural Network (MHSA-GNN), with a hypernetwork that generates adaptive filter parameters based on spectral fingerprints, and a regularization strategy combining teacher-student contrastive learning and Barlow Twins diversity loss.

Result: Experimental results on four real-world datasets show that MHSA-GNN effectively captures high-frequency abnormal signals and outperforms state-of-the-art methods, particularly excelling in highly heterogeneous scenarios.

Conclusion: The MHSA-GNN model successfully addresses the challenges of graph anomaly detection, offering an adaptive, instance-specific, and high-frequency preserving approach, setting new benchmarks in the field.

Abstract: Graph anomaly detection technology has broad applications in financial fraud and risk control. However, existing graph anomaly detection methods often face significant challenges when dealing with complex and variable abnormal patterns, as anomalous nodes are often disguised and mixed with normal nodes, leading to the coexistence of homophily and heterophily in the graph domain. Recent spectral graph neural networks have made notable progress in addressing this issue; however, current techniques typically employ fixed, globally shared filters. This 'one-size-fits-all' approach can easily cause over-smoothing, erasing critical high-frequency signals needed for fraud detection, and lacks adaptive capabilities for different graph instances. To solve this problem, we propose a Multi-Head Spectral-Adaptive Graph Neural Network (MHSA-GNN). The core innovation is the design of a lightweight hypernetwork that, conditioned on a 'spectral fingerprint' containing structural statistics and Rayleigh quotient features, dynamically generates Chebyshev filter parameters tailored to each instance. This enables a customized filtering strategy for each node and its local subgraph. Additionally, to prevent mode collapse in the multi-head mechanism, we introduce a novel dual regularization strategy that combines teacher-student contrastive learning (TSC) to ensure representation accuracy and Barlow Twins diversity loss (BTD) to enforce orthogonality among heads. Extensive experiments on four real-world datasets demonstrate that our method effectively preserves high-frequency abnormal signals and significantly outperforms existing state-of-the-art methods, especially showing excellent robustness on highly heterogeneous datasets.

</details>


### [386] [Splitwise: Collaborative Edge-Cloud Inference for LLMs via Lyapunov-Assisted DRL](https://arxiv.org/abs/2512.23310)
*Abolfazl Younesi,Abbas Shabrang Maryan,Elyas Oustad,Zahra Najafabadi Samani,Mohsen Ansari,Thomas Fahringer*

Main category: cs.LG

TL;DR: The paper introduces Splitwise, a new method for adaptively partitioning large language models between edge and cloud to optimize performance while managing resources efficiently.


<details>
  <summary>Details</summary>
Motivation: Deploying large language models on resource-constrained edge devices is difficult, as cloud-based solutions cause latency and cost issues, and static partitioning fails under changing conditions.

Method: The authors propose Splitwise, a deep reinforcement learning framework that uses fine-grained partitioning of model components guided by Lyapunov optimization, ensuring effective performance across edge and cloud environments while managing latency, energy, and stability.

Result: Experiments demonstrate that Splitwise outperforms existing partitioning methods, significantly reducing latency, energy consumption, and improving robustness under variable network conditions.

Conclusion: Splitwise offers an efficient, adaptive, and robust solution for deploying large language models across edge and cloud platforms, optimizing performance and resource use.

Abstract: Deploying large language models (LLMs) on edge devices is challenging due to their limited memory and power resources. Cloud-only inference reduces device burden but introduces high latency and cost. Static edge-cloud partitions optimize a single metric and struggle when bandwidth fluctuates. We propose Splitwise, a novel Lyapunov-assisted deep reinforcement learning (DRL) framework for fine-grained, adaptive partitioning of LLMs across edge and cloud environments. Splitwise decomposes transformer layers into attention heads and feed-forward sub-blocks, exposing more partition choices than layer-wise schemes. A hierarchical DRL policy, guided by Lyapunov optimization, jointly minimizes latency, energy consumption, and accuracy degradation while guaranteeing queue stability under stochastic workloads and variable network bandwidth. Splitwise also guarantees robustness via partition checkpoints with exponential backoff recovery in case of communication failures. Experiments on Jetson Orin NX, Galaxy S23, and Raspberry Pi 5 with GPT-2 (1.5B), LLaMA-7B, and LLaMA-13B show that Splitwise reduces end-to-end latency by 1.4x-2.8x and cuts energy consumption by up to 41% compared with existing partitioners. It lowers the 95th-percentile latency by 53-61% relative to cloud-only execution, while maintaining accuracy and modest memory requirements.

</details>


### [387] [Le Cam Distortion: A Decision-Theoretic Framework for Robust Transfer Learning](https://arxiv.org/abs/2512.23617)
*Deniz Akdemir*

Main category: cs.LG

TL;DR: This paper critiques the traditional Unsupervised Domain Adaptation (UDA) approach and proposes a new framework, Le Cam Distortion, for safer and more effective transfer learning.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of distribution shift and negative transfer risks in real-world machine learning applications, especially in safety-critical domains.

Method: Proposes a decision-theoretic framework using Le Cam's statistical experiments, introducing the Deficiency Distance ($δ$) to measure transfer risk and enable directional simulability instead of strict feature invariance.

Result: Achieved superior performance across five experiments in genomics, vision, and reinforcement learning by avoiding information loss and ensuring risk-controlled transfer.

Conclusion: Le Cam Distortion offers a principled, effective alternative to UDA for transfer learning in domains requiring high reliability and where negative transfer is unacceptable, such as in medical imaging and autonomous systems.

Abstract: Distribution shift is the defining challenge of real-world machine learning. The dominant paradigm--Unsupervised Domain Adaptation (UDA)--enforces feature invariance, aligning source and target representations via symmetric divergence minimization [Ganin et al., 2016]. We demonstrate that this approach is fundamentally flawed: when domains are unequally informative (e.g., high-quality vs degraded sensors), strict invariance necessitates information destruction, causing "negative transfer" that can be catastrophic in safety-critical applications [Wang et al., 2019].
  We propose a decision-theoretic framework grounded in Le Cam's theory of statistical experiments [Le Cam, 1986], using constructive approximations to replace symmetric invariance with directional simulability. We introduce Le Cam Distortion, quantified by the Deficiency Distance $δ(E_1, E_2)$, as a rigorous upper bound for transfer risk conditional on simulability. Our framework enables transfer without source degradation by learning a kernel that simulates the target from the source. Across five experiments (genomics, vision, reinforcement learning), Le Cam Distortion achieves: (1) near-perfect frequency estimation in HLA genomics (correlation $r=0.999$, matching classical methods), (2) zero source utility loss in CIFAR-10 image classification (81.2% accuracy preserved vs 34.7% drop for CycleGAN), and (3) safe policy transfer in RL control where invariance-based methods suffer catastrophic collapse. Le Cam Distortion provides the first principled framework for risk-controlled transfer learning in domains where negative transfer is unacceptable: medical imaging, autonomous systems, and precision medicine.

</details>


### [388] [Learning from Negative Examples: Why Warning-Framed Training Data Teaches What It Warns Against](https://arxiv.org/abs/2512.22293)
*Tsogt-Ochir Enkhbayar*

Main category: cs.LG

TL;DR: Warnings in training data do not prevent language models from reproducing warned-against behaviors due to overlapping latent activations and statistical co-occurrence biases.


<details>
  <summary>Details</summary>
Motivation: To investigate why language models disregard warning-framed training data and explore the mechanisms behind this failure.

Method: The study used sparse autoencoder analysis and experiments comparing response behaviors of models exposed to warning-framed data versus direct data.

Result: Models showed statistically similar reproduction rates for warned-against content (76.7%) and directly provided content (83.3%), due to shared latent activations for describing and performing actions.

Conclusion: Language models prioritize statistical co-occurrence over pragmatic understanding, and current architectures need improvements like training-time feature ablation to address this limitation.

Abstract: Warning-framed content in training data (e.g., "DO NOT USE - this code is vulnerable") does not, it turns out, teach language models to avoid the warned-against behavior. In experiments reported here, models exposed to such warnings reproduced the flagged content at rates statistically indistinguishable from models given the content directly (76.7% vs. 83.3%). Why? Sparse autoencoder analysis points to a failure of orthogonalization: "describing X" and "performing X" activate overlapping latent features. Feature #8684, which tracks code execution patterns, fires at comparable magnitude in both warning and exploitation contexts. A related phenomenon, what I call "stealth slip", allows conversational preambles to rotate activations into subspaces that linear probes miss entirely. Prompting and inference-time steering do not fix this; training-time feature ablation does. The upshot is that statistical co-occurrence dominates over pragmatic interpretation in current architectures. Models learn what tends to follow a context, not why it appeared there.

</details>


### [389] [Hybrid Quantum-Classical Mixture of Experts: Unlocking Topological Advantage via Interference-Based Routing](https://arxiv.org/abs/2512.22296)
*Reda Heddad,Lamiae Bouanane*

Main category: cs.LG

TL;DR: The paper proposes a Hybrid Quantum-Classical Mixture of Experts (QMoE) architecture utilizing a Quantum Router to enhance deep learning by addressing challenges like expert imbalance and computational constraints, while showing superiority on non-linear tasks and robustness in noisy quantum hardware.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of existing Mixture-of-Experts (MoE) architectures such as expert imbalance and computational complexity of routing mechanisms through advanced methods like Quantum Machine Learning.

Method: The study introduces a Hybrid Quantum-Classical Mixture of Experts using Quantum Gating Networks (Quantum Router) and classical experts. These Quantum Routers employ quantum feature maps (Angle Embedding) and wave interference to model high-dimensional, non-linear decision boundaries efficiently.

Result: The Quantum Router achieves superior parameter efficiency, outperforms classical routers on non-linearly separable tasks such as the Two Moons dataset by untangling data distributions, and is robust against simulated quantum noise, making it feasible for near-term quantum hardware.

Conclusion: The findings validate the quantum advantage and practical promise of QMoE for tasks requiring complex decision boundaries, with further potential applications in federated learning, privacy-preserving machine learning, and adaptive systems.

Abstract: The Mixture-of-Experts (MoE) architecture has emerged as a powerful paradigm for scaling deep learning models, yet it is fundamentally limited by challenges such as expert imbalance and the computational complexity of classical routing mechanisms. This paper investigates the potential of Quantum Machine Learning (QML) to address these limitations through a novel Hybrid Quantum-Classical Mixture of Experts (QMoE) architecture. Specifically, we conduct an ablation study using a Quantum Gating Network (Router) combined with classical experts to isolate the source of quantum advantage. Our central finding validates the Interference Hypothesis: by leveraging quantum feature maps (Angle Embedding) and wave interference, the Quantum Router acts as a high-dimensional kernel method, enabling the modeling of complex, non-linear decision boundaries with superior parameter efficiency compared to its classical counterparts. Experimental results on non-linearly separable data, such as the Two Moons dataset, demonstrate that the Quantum Router achieves a significant topological advantage, effectively "untangling" data distributions that linear classical routers fail to separate efficiently. Furthermore, we analyze the architecture's robustness against simulated quantum noise, confirming its feasibility for near-term intermediate-scale quantum (NISQ) hardware. We discuss practical applications in federated learning, privacy-preserving machine learning, and adaptive systems that could benefit from this quantum-enhanced routing paradigm.

</details>


### [390] [Random Controlled Differential Equations](https://arxiv.org/abs/2512.23670)
*Francesco Piatti,Thomas Cass,William F. Turner*

Main category: cs.LG

TL;DR: The paper introduces a time-series learning framework that uses random features with controlled differential equations (CDEs) for efficient training and achieves competitive or state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Improve the training efficiency and performance of time-series learning models by using continuous-time representations while leveraging inductive biases.

Method: Large random parameterized CDEs are used to create representations, and only a linear readout layer is trained. Two variants are introduced: (i) Random Fourier CDEs augment inputs with random Fourier features, and (ii) Random Rough DEs use log-signature techniques for representing temporal patterns.

Result: The proposed models demonstrate competitive or state-of-the-art performance on diverse time-series benchmarks by combining efficiency and strong inductive biases.

Conclusion: The models effectively provide scalable and practical alternatives to explicit signature calculations, maintaining theoretical strength while being computationally efficient.

Abstract: We introduce a training-efficient framework for time-series learning that combines random features with controlled differential equations (CDEs). In this approach, large randomly parameterized CDEs act as continuous-time reservoirs, mapping input paths to rich representations. Only a linear readout layer is trained, resulting in fast, scalable models with strong inductive bias. Building on this foundation, we propose two variants: (i) Random Fourier CDEs (RF-CDEs): these lift the input signal using random Fourier features prior to the dynamics, providing a kernel-free approximation of RBF-enhanced sequence models; (ii) Random Rough DEs (R-RDEs): these operate directly on rough-path inputs via a log-ODE discretization, using log-signatures to capture higher-order temporal interactions while remaining stable and efficient. We prove that in the infinite-width limit, these model induces the RBF-lifted signature kernel and the rough signature kernel, respectively, offering a unified perspective on random-feature reservoirs, continuous-time deep architectures, and path-signature theory.
  We evaluate both models across a range of time-series benchmarks, demonstrating competitive or state-of-the-art performance. These methods provide a practical alternative to explicit signature computations, retaining their inductive bias while benefiting from the efficiency of random features.

</details>


### [391] [Statistical and Machine Learning Analysis of Traffic Accidents on US 158 in Currituck County: A Comparison with HSM Predictions](https://arxiv.org/abs/2512.22302)
*Jennifer Sawyer,Julian Allagan*

Main category: cs.LG

TL;DR: This study employs advanced statistical and machine learning methods on traffic accident data to predict injury severity and identify safety issues on a rural US highway.


<details>
  <summary>Details</summary>
Motivation: The study aims to advance rural highway safety analysis by building upon simpler statistical methods to deliver actionable insights for traffic safety improvement.

Method: The paper uses advanced techniques, including Kernel Density Estimation, Negative Binomial Regression, Random Forest classification, and spatial modeling, to analyze traffic data and identify crash patterns.

Result: A Random Forest classifier predicts injury severity with 67% accuracy, outperforming traditional methods, and confirms spatial clustering while identifying accident hotspots.

Conclusion: Applying advanced analytical techniques can enhance the understanding of traffic patterns, aiding targeted intervention for traffic safety on US highways.

Abstract: This study extends previous hotspot and Chi-Square analysis by Sawyer \cite{sawyer2025hotspot} by integrating advanced statistical analysis, machine learning, and spatial modeling techniques to analyze five years (2019--2023) of traffic accident data from an 8.4-mile stretch of US 158 in Currituck County, NC. Building upon foundational statistical work, we apply Kernel Density Estimation (KDE), Negative Binomial Regression, Random Forest classification, and Highway Safety Manual (HSM) Safety Performance Function (SPF) comparisons to identify comprehensive temporal and spatial crash patterns. A Random Forest classifier predicts injury severity with 67\% accuracy, outperforming HSM SPF. Spatial clustering is confirmed via Moran's I test ($I = 0.32$, $p < 0.001$), and KDE analysis reveals hotspots near major intersections, validating and extending earlier hotspot identification methods. These results support targeted interventions to improve traffic safety on this vital transportation corridor. Our objective is to provide actionable insights for improving safety on US 158 while contributing to the broader understanding of rural highway safety analysis through methodological advancement beyond basic statistical techniques.

</details>


### [392] [PDx -- Adaptive Credit Risk Forecasting Model in Digital Lending using Machine Learning Operations](https://arxiv.org/abs/2512.22305)
*Sultan Amed,Chan Yu Hang,Sayantan Banerjee*

Main category: cs.LG

TL;DR: PDx is a dynamic MLOps-driven decision system developed to improve credit risk forecasting through adaptive probability of default modeling. It addresses the limitations of static models by ensuring continuous updates and resilience against data drift.


<details>
  <summary>Details</summary>
Motivation: Traditional credit risk prediction models focus on predictive accuracy but often fail to adapt to changing borrower behavior, leading to performance degradation over time. Many institutions also struggle with transitioning ML models into production and maintaining their quality.

Method: PDx implements an MLOps-driven, end-to-end lifecycle management approach involving continuous model monitoring, retraining, and validation. It uses a dynamic champion-challenger framework to recalibrate parameters and select the best-performing model regularly.

Result: Empirical results indicate that decision tree-based models outperform others in identifying defaulters but need frequent updates to maintain accuracy. PDx demonstrated better performance adaptability compared to static linear models and neural networks.

Conclusion: PDx mitigates value loss for digital lenders, especially in short-term loans where behavior changes rapidly. It is validated as scalable and adaptable across various lending datasets, making it a robust solution for modern credit risk forecasting.

Abstract: This paper presents PDx, an adaptive, machine learning operations (MLOps) driven decision system for forecasting credit risk using probability of default (PD) modeling in digital lending. While conventional PD models prioritize predictive accuracy during model development with complex machine learning algorithms, they often overlook continuous adaptation to changing borrower behaviour, resulting in static models that degrade over time in production and generate inaccurate default predictions. Many financial institutes also find it difficult transitioning ML models from development environment to production and maintaining their health. With PDx we aimed to addresses these limitations using a dynamic, end-to-end model lifecycle management approach that integrates continuous model monitoring, retraining, and validation through a robust MLOps pipeline. We introduced a dynamic champion-challenger framework for PDx to regularly update baseline models to recalibrate independent parameters with the latest data and select the best-performing model through out-of-time validation, ensuring resilience against data drift and changing credit risk patterns. Our empirical analysis shows that decision tree-based ensemble models consistently outperform others in classifying defaulters but require frequent updates to sustain performance. Linear models (e.g., logistic regression) and neural networks exhibit greater performance degradation. The study demonstrate with PDx we can mitigates value erosion for digital lenders, particularly in short-term, small-ticket loans, where borrower behavior shifts rapidly. We have validated the effectiveness of PDx using datasets from peer-to-peer lending, business loans, and auto loans, demonstrating its scalability and adaptability for modern credit risk forecasting.

</details>


### [393] [LLMBoost: Make Large Language Models Stronger with Boosting](https://arxiv.org/abs/2512.22309)
*Zehao Chen,Tianxiang Ai,Yifei Li,Gongxun Li,Yuyang Wei,Wang Zhou,Guanghui Li,Bin Yu,Zhijun Chen,Hailong Sun,Fuzhen Zhuang,Jianxin Li,Deqing Wang,Yikun Ban*

Main category: cs.LG

TL;DR: LLMBoost is a new ensemble fine-tuning framework for LLMs improving performance by using intermediate states rather than just outputs.


<details>
  <summary>Details</summary>
Motivation: To enhance ensemble learning by incorporating rich internal representations and interactions among large language models.

Method: LLMBoost employs cross-model attention for fusing hidden states, chain training for error correction, and near-parallel inference for efficiency.

Result: Experiments show LLMBoost improves accuracy and reduces inference latency in commonsense and arithmetic reasoning tasks.

Conclusion: LLMBoost ensures monotonic improvements in performance through sequential integration and provides both accuracy and efficiency gains.

Abstract: Ensemble learning of LLMs has emerged as a promising alternative to enhance performance, but existing approaches typically treat models as black boxes, combining the inputs or final outputs while overlooking the rich internal representations and interactions across models.In this work, we introduce LLMBoost, a novel ensemble fine-tuning framework that breaks this barrier by explicitly leveraging intermediate states of LLMs. Inspired by the boosting paradigm, LLMBoost incorporates three key innovations. First, a cross-model attention mechanism enables successor models to access and fuse hidden states from predecessors, facilitating hierarchical error correction and knowledge transfer. Second, a chain training paradigm progressively fine-tunes connected models with an error-suppression objective, ensuring that each model rectifies the mispredictions of its predecessor with minimal additional computation. Third, a near-parallel inference paradigm design pipelines hidden states across models layer by layer, achieving inference efficiency approaching single-model decoding. We further establish the theoretical foundations of LLMBoost, proving that sequential integration guarantees monotonic improvements under bounded correction assumptions. Extensive experiments on commonsense reasoning and arithmetic reasoning tasks demonstrate that LLMBoost consistently boosts accuracy while reducing inference latency.

</details>


### [394] [Optimistic Feasible Search for Closed-Loop Fair Threshold Decision-Making](https://arxiv.org/abs/2512.22313)
*Wenzhang Du*

Main category: cs.LG

TL;DR: The paper introduces Optimistic Feasible Search (OFS), a learning algorithm for threshold policies in fairness-constrained closed-loop systems, achieving better utility and constraint management compared to baselines.


<details>
  <summary>Details</summary>
Motivation: Closed-loop decision-making systems face challenges with fairness, service constraints, and non-stationarity due to feedback effects. These issues necessitate effective online learning algorithms that account for fairness and service constraints while maximizing utility.

Method: The authors propose a method called Optimistic Feasible Search (OFS), which uses grid-based confidence bounds to select thresholds that are both feasible under constraints and optimistically reward-maximizing. The method is tested in synthetic and semi-synthetic benchmarks.

Result: OFS shows superior performance in achieving higher rewards and maintaining lower cumulative constraint violations compared to other methods like unconstrained or primal-dual bandit approaches. It approaches near-optimal solutions relative to feasible thresholds under performance sweeps.

Conclusion: OFS is effective for low-dimensional, interpretable policy scenarios in closed-loop systems, offering reproducible, fair, and utility-maximizing solutions while minimizing constraint violations.

Abstract: Closed-loop decision-making systems (e.g., lending, screening, or recidivism risk assessment) often operate under fairness and service constraints while inducing feedback effects: decisions change who appears in the future, yielding non-stationary data and potentially amplifying disparities. We study online learning of a one-dimensional threshold policy from bandit feedback under demographic parity (DP) and, optionally, service-rate constraints. The learner observes only a scalar score each round and selects a threshold; reward and constraint residuals are revealed only for the chosen threshold.
  We propose Optimistic Feasible Search (OFS), a simple grid-based method that maintains confidence bounds for reward and constraint residuals for each candidate threshold. At each round, OFS selects a threshold that appears feasible under confidence bounds and, among those, maximizes optimistic reward; if no threshold appears feasible, OFS selects the threshold minimizing optimistic constraint violation. This design directly targets feasible high-utility thresholds and is particularly effective for low-dimensional, interpretable policy classes where discretization is natural.
  We evaluate OFS on (i) a synthetic closed-loop benchmark with stable contraction dynamics and (ii) two semi-synthetic closed-loop benchmarks grounded in German Credit and COMPAS, constructed by training a score model and feeding group-dependent acceptance decisions back into population composition. Across all environments, OFS achieves higher reward with smaller cumulative constraint violation than unconstrained and primal-dual bandit baselines, and is near-oracle relative to the best feasible fixed threshold under the same sweep procedure. Experiments are reproducible and organized with double-blind-friendly relative outputs.

</details>


### [395] [LangPrecip: Language-Aware Multimodal Precipitation Nowcasting](https://arxiv.org/abs/2512.22317)
*Xudong Ling,Tianxi Huang,Qian Dong,Tao He,Chaorong Li,Guiduo Duan*

Main category: cs.LG

TL;DR: This paper introduces LangPrecip, a language-aware multimodal framework for short-term precipitation nowcasting, improving performance using meteorological text as a semantic constraint.


<details>
  <summary>Details</summary>
Motivation: The study aims to improve short-term precipitation nowcasting accuracy, particularly for extreme weather, by addressing weak constraints on future motion in existing visual conditioning methods.

Method: LangPrecip integrates textual and radar data using the Rectified Flow approach, treating meteorological descriptions as semantic constraints in motion trajectory generation.

Result: LangPrecip demonstrated significant performance improvements, with gains of over 60% in heavy-rainfall CSI at an 80-minute lead time.

Conclusion: The proposed framework effectively handles the challenges of spatiotemporal forecasting by combining textual and radar data, showcasing a novel way to improve prediction accuracy in weather nowcasting.

Abstract: Short-term precipitation nowcasting is an inherently uncertain and under-constrained spatiotemporal forecasting problem, especially for rapidly evolving and extreme weather events. Existing generative approaches rely primarily on visual conditioning, leaving future motion weakly constrained and ambiguous. We propose a language-aware multimodal nowcasting framework(LangPrecip) that treats meteorological text as a semantic motion constraint on precipitation evolution. By formulating nowcasting as a semantically constrained trajectory generation problem under the Rectified Flow paradigm, our method enables efficient and physically consistent integration of textual and radar information in latent space.We further introduce LangPrecip-160k, a large-scale multimodal dataset with 160k paired radar sequences and motion descriptions. Experiments on Swedish and MRMS datasets show consistent improvements over state-of-the-art methods, achieving over 60 \% and 19\% gains in heavy-rainfall CSI at an 80-minute lead time.

</details>


### [396] [Decomposing Uncertainty in Probabilistic Knowledge Graph Embeddings: Why Entity Variance Is Not Enough](https://arxiv.org/abs/2512.22318)
*Chorok Lee*

Main category: cs.LG

TL;DR: This paper addresses a limitation in probabilistic knowledge graph embeddings and proposes a novel method to improve out-of-distribution uncertainty detection.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of current methods in probabilistic knowledge graph embeddings, specifically their inability to distinguish between emerging entities and novel relational contexts.

Method: The authors introduce a formal decomposition of uncertainty into semantic and structural components, proposing a new method (CAGP) that uses learned weights to combine these components for better uncertainty estimation.

Result: The proposed CAGP method significantly outperforms relation-agnostic baselines, achieving a 60-80% improvement in temporal out-of-distribution detection AUROC and reducing selective prediction errors by 43%.

Conclusion: Incorporating both semantic and structural uncertainty yields stronger and more accurate out-of-distribution detection in probabilistic knowledge graph embeddings, addressing previous method shortcomings.

Abstract: Probabilistic knowledge graph embeddings represent entities as distributions, using learned variances to quantify epistemic uncertainty. We identify a fundamental limitation: these variances are relation-agnostic, meaning an entity receives identical uncertainty regardless of relational context. This conflates two distinct out-of-distribution phenomena that behave oppositely: emerging entities (rare, poorly-learned) and novel relational contexts (familiar entities in unobserved relationships). We prove an impossibility result: any uncertainty estimator using only entity-level statistics independent of relation context achieves near-random OOD detection on novel contexts. We empirically validate this on three datasets, finding 100 percent of novel-context triples have frequency-matched in-distribution counterparts. This explains why existing probabilistic methods achieve 0.99 AUROC on random corruptions but only 0.52-0.64 on temporal distribution shift. We formalize uncertainty decomposition into complementary components: semantic uncertainty from entity embedding variance (detecting emerging entities) and structural uncertainty from entity-relation co-occurrence (detecting novel contexts). Our main theoretical result proves these signals are non-redundant, and that any convex combination strictly dominates either signal alone. Our method (CAGP) combines semantic and structural uncertainty via learned weights, achieving 0.94-0.99 AUROC on temporal OOD detection across multiple benchmarks, a 60-80 percent relative improvement over relation-agnostic baselines. Empirical validation confirms complete frequency overlap on three datasets (FB15k-237, WN18RR, YAGO3-10). On selective prediction, our method reduces errors by 43 percent at 85 percent answer rate.

</details>


### [397] [Expert System for Bitcoin Forecasting: Integrating Global Liquidity via TimeXer Transformers](https://arxiv.org/abs/2512.22326)
*Sravan Karthick T*

Main category: cs.LG

TL;DR: The paper integrates global liquidity metrics into Bitcoin price forecasting using the TimeXer model, showing improved accuracy and stability over traditional methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of forecasting Bitcoin's highly volatile and non-stationary prices, especially over long horizons.

Method: Introduces the TimeXer-Exog architecture, combining Bitcoin prices with global M2 liquidity data (12-week lag) and compares its performance against various state-of-the-art benchmarks.

Result: The TimeXer-Exog model achieves a significant improvement, with a 70-day MSE 89% lower than a univariate TimeXer, demonstrating stabilized forecasts through macroeconomic conditioning.

Conclusion: Macroeconomic conditioning, such as global liquidity integration, serves as an effective approach for improving long-horizon forecasts in volatile markets like Bitcoin.

Abstract: Bitcoin price forecasting is characterized by extreme volatility and non-stationarity, often defying traditional univariate time-series models over long horizons. This paper addresses a critical gap by integrating Global M2 Liquidity, aggregated from 18 major economies, as a leading exogenous variable with a 12-week lag structure. Using the TimeXer architecture, we compare a liquidity-conditioned forecasting model (TimeXer-Exog) against state-of-the-art benchmarks including LSTM, N-BEATS, PatchTST, and a standard univariate TimeXer. Experiments conducted on daily Bitcoin price data from January 2020 to August 2025 demonstrate that explicit macroeconomic conditioning significantly stabilizes long-horizon forecasts. At a 70-day forecast horizon, the proposed TimeXer-Exog model achieves a mean squared error (MSE) 1.08e8, outperforming the univariate TimeXer baseline by over 89 percent. These results highlight that conditioning deep learning models on global liquidity provides substantial improvements in long-horizon Bitcoin price forecasting.

</details>


### [398] [The Effectiveness of Approximate Regularized Replay for Efficient Supervised Fine-Tuning of Large Language Models](https://arxiv.org/abs/2512.22337)
*Matthew Riemer,Erik Miehling,Miao Liu,Djallel Bouneffouf,Murray Campbell*

Main category: cs.LG

TL;DR: The paper examines the adverse effects of LoRA-based fine-tuning on model capabilities and proposes a regularized training procedure to mitigate these effects, preserving general knowledge while allowing adaptability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the problem where LoRA-based fine-tuning, despite being parameter-efficient and trained on small datasets, can result in significant degradation of model capabilities.

Method: The paper introduces a regularized approximate replay approach. It penalizes KL divergence with respect to the original model and interleaves next-token prediction data from a similar pre-training corpus into the fine-tuning process.

Result: Applying the proposed method to Qwen instruction-tuned models ensures the preservation of general knowledge while maintaining the ability to adapt to new tasks. This approach only incurs a modest computational overhead.

Conclusion: Small but deliberate training tweaks can resolve catastrophic degradation issues caused by LoRA fine-tuning, enabling efficient fine-tuning while preserving original model capabilities.

Abstract: Although parameter-efficient fine-tuning methods, such as LoRA, only modify a small subset of parameters, they can have a significant impact on the model. Our instruction-tuning experiments show that LoRA-based supervised fine-tuning can catastrophically degrade model capabilities, even when trained on very small datasets for relatively few steps. With that said, we demonstrate that while the most straightforward approach (that is likely the most used in practice) fails spectacularly, small tweaks to the training procedure with very little overhead can virtually eliminate the problem. Particularly, in this paper we consider a regularized approximate replay approach which penalizes KL divergence with respect to the initial model and interleaves in data for next token prediction from a different, yet similar, open access corpus to what was used in pre-training. When applied to Qwen instruction-tuned models, we find that this recipe preserves general knowledge in the model without hindering plasticity to new tasks by adding a modest amount of computational overhead.

</details>


### [399] [AFA-LoRA: Enabling Non-Linear Adaptations in LoRA with Activation Function Annealing](https://arxiv.org/abs/2512.22455)
*Jiacheng Li,Jianchao Tan,Zhidong Yang,Feiye Huo,Yerui Sun,Yuchen Xie,Xunliang Cai*

Main category: cs.LG

TL;DR: The paper proposes AFA-LoRA, a parameter-efficient fine-tuning method enhancing LoRA by incorporating non-linear expressivity while maintaining mergeability.


<details>
  <summary>Details</summary>
Motivation: Current Low-Rank Adaptation (LoRA) methods are limited by their linear adaptation, forbidding them from reaching the full potential of non-linear training expressivity.

Method: AFA-LoRA introduces an annealed activation function that starts as non-linear during training and gradually transitions into linear, enhancing representational power while preserving mergeability.

Result: AFA-LoRA successfully reduces the performance gap between LoRA and full-parameter training in supervised fine-tuning, reinforcement learning, and speculative decoding tasks.

Conclusion: AFA-LoRA bridges the gap between linear and non-linear training, enabling a more robust and practical parameter-efficient adaptation paradigm.

Abstract: Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient fine-tuning (PEFT) method. However, its linear adaptation process limits its expressive power. This means there is a gap between the expressive power of linear training and non-linear training. To bridge this gap, we propose AFA-LoRA, a novel training strategy that brings non-linear expressivity to LoRA while maintaining its seamless mergeability. Our key innovation is an annealed activation function that transitions from a non-linear to a linear transformation during training, allowing the adapter to initially adopt stronger representational capabilities before converging to a mergeable linear form. We implement our method on supervised fine-tuning, reinforcement learning, and speculative decoding. The results show that AFA-LoRA reduces the performance gap between LoRA and full-parameter training. This work enables a more powerful and practical paradigm of parameter-efficient adaptation.

</details>


### [400] [AMBIT: Augmenting Mobility Baselines with Interpretable Trees](https://arxiv.org/abs/2512.22466)
*Qizhi Wang*

Main category: cs.LG

TL;DR: The paper introduces AMBIT, a gray-box framework that enhances physical mobility models with interpretable tree models for OD flow prediction, offering both accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: To address the conflict between the needs for high accuracy and clear interpretability in GIS-based origin-destination flow predictions.

Method: The authors use physical mobility baselines augmented with gradient-boosted residual tree learners and SHAP analysis, while conducting audits of classical spatial interaction models and analyzing errors for urban decision-making.

Result: AMBIT achieves near-accurate prediction like strong tree-based predictors with interpretable physics-grounded residuals and robustness under spatial generalization.

Conclusion: AMBIT provides a reliable and interpretable tool for urban decision-making, offering a reproducible pipeline and useful diagnostics while balancing interpretability and accuracy.

Abstract: Origin-destination (OD) flow prediction remains a core task in GIS and urban analytics, yet practical deployments face two conflicting needs: high accuracy and clear interpretability. This paper develops AMBIT, a gray-box framework that augments physical mobility baselines with interpretable tree models. We begin with a comprehensive audit of classical spatial interaction models on a year-long, hourly NYC taxi OD dataset. The audit shows that most physical models are fragile at this temporal resolution; PPML gravity is the strongest physical baseline, while constrained variants improve when calibrated on full OD margins but remain notably weaker. We then build residual learners on top of physical baselines using gradient-boosted trees and SHAP analysis, demonstrating that (i) physics-grounded residuals approach the accuracy of a strong tree-based predictor while retaining interpretable structure, and (ii) POI-anchored residuals are consistently competitive and most robust under spatial generalization. We provide a reproducible pipeline, rich diagnostics, and spatial error analysis designed for urban decision-making.

</details>


### [401] [GLUE: Gradient-free Learning to Unify Experts](https://arxiv.org/abs/2512.22467)
*Jong-Ik Park,Shreyas Chaudhari,Srinivasa Pranav,Carlee Joe-Wong,José M. F. Moura*

Main category: cs.LG

TL;DR: This paper introduces GLUE, a gradient-free method to unify multiple pretrained specialist models for domain expansion, leading to improved performance and sample efficiency compared to prior methods.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for blending expert models to generalize across target domains often rely on heuristics or computationally-expensive backpropagation, resulting in lower accuracy and inefficiency.

Method: GLUE employs a convex combination of expert models and optimizes mixture coefficients using a gradient-free two-point update (SPSA), requiring only two forward passes per step.

Result: GLUE shows improved test accuracy (up to 8.5% over data-size weighting and 9.1% over proxy metrics), matching or outperforming full-gradient mixing while being more efficient.

Conclusion: GLUE effectively initializes models for domain expansion, offering a robust and efficient alternative to heuristic blending and backpropagation-based methods.

Abstract: In many deployed systems (multilingual ASR, cross-hospital imaging, region-specific perception), multiple pretrained specialist models coexist. Yet, new target domains often require domain expansion: a generalized model that performs well beyond any single specialist's domain. Given such a new target domain, prior works seek a single strong initialization prior for the model parameters by first blending expert models to initialize a target model. However, heuristic blending -- using coefficients based on data size or proxy metrics -- often yields lower target-domain test accuracy, and learning the coefficients on the target loss typically requires computationally-expensive full backpropagation through the network. We propose GLUE, Gradient-free Learning To Unify Experts, which initializes the target model as a convex combination of fixed experts, learning the mixture coefficients of this combination via a gradient-free two-point (SPSA) update that requires only two forward passes per step. Across experiments on three datasets and three network architectures, GLUE produces a single prior that can be fine-tuned effectively to outperform baselines. GLUE improves test accuracy by up to 8.5% over data-size weighting and by up to 9.1% over proxy-metric selection. GLUE either outperforms backpropagation-based full-gradient mixing or matches its performance within 1.4%.

</details>


### [402] [Collaborative Optimization of Multiclass Imbalanced Learning: Density-Aware and Region-Guided Boosting](https://arxiv.org/abs/2512.22478)
*Chuantao Li,Zhi Li,Jiahao Xu,Jie Li,Sheng Li*

Main category: cs.LG

TL;DR: This study proposes a collaborative optimization Boosting model for multiclass imbalanced learning, which achieves improved performance by integrating density and confidence factors with mechanisms tightly integrated for weight updates and sampling.


<details>
  <summary>Details</summary>
Motivation: To address classification bias caused by class imbalance and improve performance by integrating imbalanced learning with model training.

Method: The study designs a collaborative optimization model using a noise-resistant weight update mechanism and dynamic sampling strategy, integrating modules for weight updates, sample region partitioning, and region-guided sampling.

Result: The proposed model significantly outperforms eight state-of-the-art baselines in extensive experiments on 20 public imbalanced datasets.

Conclusion: The collaborative optimization Boosting model offers a robust and effective solution for multiclass imbalanced learning and model training integration, with demonstrated superiority over existing methods.

Abstract: Numerous studies attempt to mitigate classification bias caused by class imbalance. However, existing studies have yet to explore the collaborative optimization of imbalanced learning and model training. This constraint hinders further performance improvements. To bridge this gap, this study proposes a collaborative optimization Boosting model of multiclass imbalanced learning. This model is simple but effective by integrating the density factor and the confidence factor, this study designs a noise-resistant weight update mechanism and a dynamic sampling strategy. Rather than functioning as independent components, these modules are tightly integrated to orchestrate weight updates, sample region partitioning, and region-guided sampling. Thus, this study achieves the collaborative optimization of imbalanced learning and model training. Extensive experiments on 20 public imbalanced datasets demonstrate that the proposed model significantly outperforms eight state-of-the-art baselines. The code for the proposed model is available at: https://github.com/ChuantaoLi/DARG.

</details>


### [403] [Toward Real-World IoT Security: Concept Drift-Resilient IoT Botnet Detection via Latent Space Representation Learning and Alignment](https://arxiv.org/abs/2512.22488)
*Hassan Wasswa,Timothy Lynar*

Main category: cs.LG

TL;DR: This paper introduces a scalable framework for IoT threat detection that adapts to concept drift without frequent retraining, using latent-space representations and graph neural networks.


<details>
  <summary>Details</summary>
Motivation: IoT threat detection faces challenges like dynamic NetFlow traffic and concept drift, which prevents effective deployment in enterprise environments relying on traditional static datasets.

Method: The framework uses one-time training of a classifier on latent representations of historical traffic, while an alignment model maps incoming traffic to the historical latent space. Attack samples are transformed into a graph-structured format for classification via graph neural networks.

Result: Experimental results show that the framework offers robust threat detection under concept drift using real-world heterogeneous IoT traffic datasets.

Conclusion: The framework is suitable for deployment in dynamic and large-scale IoT environments, offering practical solutions to challenges in adaptive threat detection.

Abstract: Although AI-based models have achieved high accuracy in IoT threat detection, their deployment in enterprise environments is constrained by reliance on stationary datasets that fail to reflect the dynamic nature of real-world IoT NetFlow traffic, which is frequently affected by concept drift. Existing solutions typically rely on periodic classifier retraining, resulting in high computational overhead and the risk of catastrophic forgetting. To address these challenges, this paper proposes a scalable framework for adaptive IoT threat detection that eliminates the need for continuous classifier retraining. The proposed approach trains a classifier once on latent-space representations of historical traffic, while an alignment model maps incoming traffic to the learned historical latent space prior to classification, thereby preserving knowledge of previously observed attacks. To capture inter-instance relationships among attack samples, the low-dimensional latent representations are further transformed into a graph-structured format and classified using a graph neural network. Experimental evaluations on real-world heterogeneous IoT traffic datasets demonstrate that the proposed framework maintains robust detection performance under concept drift. These results highlight the framework's potential for practical deployment in dynamic and large-scale IoT environments.

</details>


### [404] [The Quest for Winning Tickets in Low-Rank Adapters](https://arxiv.org/abs/2512.22495)
*Hamed Damirchi,Cristian Rodriguez-Opazo,Ehsan Abbasnejad,Zhen Zhang,Javen Shi*

Main category: cs.LG

TL;DR: The paper extends the Lottery Ticket Hypothesis (LTH) to parameter-efficient fine-tuning via LoRA methods, proposing Partial-LoRA to enhance efficiency while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: The study aims to explore if the Lottery Ticket Hypothesis applies to parameter-efficient fine-tuning, especially in the context of LoRA methods, and seeks to enhance efficiency in large pretrained models.

Method: The authors propose Partial-LoRA, a systematic approach to identify sparse subnetworks within LoRAs, allowing sparse low-rank adapters to focus on task-relevant subspaces of pretrained models.

Result: Across 8 vision and 12 language tasks, Partial-LoRA reduced trainable parameters by up to 87% while maintaining or improving model accuracy in both single-task and multi-task scenarios.

Conclusion: The findings suggest that LTH holds for LoRAs, offering insights into transfer learning and efficient fine-tuning methodologies, and introduce Partial-LoRA as a powerful strategy for parameter optimization.

Abstract: The Lottery Ticket Hypothesis (LTH) suggests that over-parameterized neural networks contain sparse subnetworks ("winning tickets") capable of matching full model performance when trained from scratch. With the growing reliance on fine-tuning large pretrained models, we investigate whether LTH extends to parameter-efficient fine-tuning (PEFT), specifically focusing on Low-Rank Adaptation (LoRA) methods. Our key finding is that LTH holds within LoRAs, revealing sparse subnetworks that can match the performance of dense adapters. In particular, we find that the effectiveness of sparse subnetworks depends more on how much sparsity is applied in each layer than on the exact weights included in the subnetwork. Building on this insight, we propose Partial-LoRA, a method that systematically identifies said subnetworks and trains sparse low-rank adapters aligned with task-relevant subspaces of the pre-trained model. Experiments across 8 vision and 12 language tasks in both single-task and multi-task settings show that Partial-LoRA reduces the number of trainable parameters by up to 87\%, while maintaining or improving accuracy. Our results not only deepen our theoretical understanding of transfer learning and the interplay between pretraining and fine-tuning but also open new avenues for developing more efficient adaptation strategies.

</details>


### [405] [Predicting LLM Correctness in Prosthodontics Using Metadata and Hallucination Signals](https://arxiv.org/abs/2512.22508)
*Lucky Susanto,Anasta Pranawijayana,Cortino Sukotjo,Soni Prasad,Derry Wijaya*

Main category: cs.LG

TL;DR: This paper investigates the feasibility of predicting the correctness of large language models' answers using metadata and hallucination signals, achieving improved accuracy and precision, but concludes that the methods are not yet robust for high-stakes applications.


<details>
  <summary>Details</summary>
Motivation: To address the critical challenge of detecting factually incorrect (hallucinated) information generated by LLMs in high-stakes domains such as healthcare.

Method: Analyzed correctness predictors for GPT-4o and OSS-120B on a prosthodontics exam using metadata and hallucination signals across three prompting strategies to assess their predictive utility.

Result: Metadata-based predictors improved accuracy by up to +7.14% and achieved a precision of 83.12%, though hallucination signals alone were not consistent predictors.

Conclusion: The paper's methods show promise for developing reliability signals in LLMs but are not yet reliable enough for critical applications.

Abstract: Large language models (LLMs) are increasingly adopted in high-stakes domains such as healthcare and medical education, where the risk of generating factually incorrect (i.e., hallucinated) information is a major concern. While significant efforts have been made to detect and mitigate such hallucinations, predicting whether an LLM's response is correct remains a critical yet underexplored problem. This study investigates the feasibility of predicting correctness by analyzing a general-purpose model (GPT-4o) and a reasoning-centric model (OSS-120B) on a multiple-choice prosthodontics exam. We utilize metadata and hallucination signals across three distinct prompting strategies to build a correctness predictor for each (model, prompting) pair. Our findings demonstrate that this metadata-based approach can improve accuracy by up to +7.14% and achieve a precision of 83.12% over a baseline that assumes all answers are correct. We further show that while actual hallucination is a strong indicator of incorrectness, metadata signals alone are not reliable predictors of hallucination. Finally, we reveal that prompting strategies, despite not affecting overall accuracy, significantly alter the models' internal behaviors and the predictive utility of their metadata. These results present a promising direction for developing reliability signals in LLMs but also highlight that the methods explored in this paper are not yet robust enough for critical, high-stakes deployment.

</details>


### [406] [Decomposing Task Vectors for Refined Model Editing](https://arxiv.org/abs/2512.22511)
*Hamed Damirchi,Ehsan Abbasnejad,Zhen Zhang,Javen Shi*

Main category: cs.LG

TL;DR: The paper introduces a decomposition method for task vectors in large pre-trained models to enhance their adaptability and improve concept-specific behaviors.


<details>
  <summary>Details</summary>
Motivation: Large pre-trained models struggle to exhibit precise, concept-focused behavior when adapted using task vectors, which often interfere with each other during arithmetic operations.

Method: A decomposition technique splits task vectors into shared components (common knowledge) and unique components (task-specific information), enabling precise concept manipulation.

Result: The proposed method improves multi-task performance in image classification, enables cleaner style mixing in diffusion models, and reduces toxicity in language models while preserving general knowledge performance.

Conclusion: This approach offers insights and control in task vector arithmetic, overcoming limitations in adapting large pre-trained models effectively.

Abstract: Large pre-trained models have transformed machine learning, yet adapting these models effectively to exhibit precise, concept-specific behaviors remains a significant challenge. Task vectors, defined as the difference between fine-tuned and pre-trained model parameters, provide a mechanism for steering neural networks toward desired behaviors. This has given rise to large repositories dedicated to task vectors tailored for specific behaviors. The arithmetic operation of these task vectors allows for the seamless combination of desired behaviors without the need for large datasets. However, these vectors often contain overlapping concepts that can interfere with each other during arithmetic operations, leading to unpredictable outcomes. We propose a principled decomposition method that separates each task vector into two components: one capturing shared knowledge across multiple task vectors, and another isolating information unique to each specific task. By identifying invariant subspaces across projections, our approach enables more precise control over concept manipulation without unintended amplification or diminution of other behaviors. We demonstrate the effectiveness of our decomposition method across three domains: improving multi-task merging in image classification by 5% using shared components as additional task vectors, enabling clean style mixing in diffusion models without generation degradation by mixing only the unique components, and achieving 47% toxicity reduction in language models while preserving performance on general knowledge tasks by negating the toxic information isolated to the unique component. Our approach provides a new framework for understanding and controlling task vector arithmetic, addressing fundamental limitations in model editing operations.

</details>


### [407] [Towards Reliable Evaluation of Adversarial Robustness for Spiking Neural Networks](https://arxiv.org/abs/2512.22522)
*Jihang Wang,Dongcheng Zhao,Ruolin Chen,Qian Zhang,Yi Zeng*

Main category: cs.LG

TL;DR: The paper presents a new framework for evaluating the adversarial robustness of Spiking Neural Networks (SNNs), addressing the issue of gradient vanishing in existing methods.


<details>
  <summary>Details</summary>
Motivation: To improve the reliability and accuracy of evaluating SNN adversarial robustness amidst challenges posed by gradient vanishing and unreliable surrogate gradient methods.

Method: Introduced Adaptive Sharpness Surrogate Gradient (ASSG) to adaptively improve gradient accuracy during attack iterations and designed a new stable adversarial attack method, SA-PGD, for better evaluation.

Result: The proposed methods show higher attack success rates, revealing that current SNN adversarial robustness has been overestimated. Extensive experiments validate the framework's general effectiveness.

Conclusion: The study highlights the need for more reliable adversarial training methods for SNNs and paves the way for improved robustness evaluation frameworks in this field.

Abstract: Spiking Neural Networks (SNNs) utilize spike-based activations to mimic the brain's energy-efficient information processing. However, the binary and discontinuous nature of spike activations causes vanishing gradients, making adversarial robustness evaluation via gradient descent unreliable. While improved surrogate gradient methods have been proposed, their effectiveness under strong adversarial attacks remains unclear. We propose a more reliable framework for evaluating SNN adversarial robustness. We theoretically analyze the degree of gradient vanishing in surrogate gradients and introduce the Adaptive Sharpness Surrogate Gradient (ASSG), which adaptively evolves the shape of the surrogate function according to the input distribution during attack iterations, thereby enhancing gradient accuracy while mitigating gradient vanishing. In addition, we design an adversarial attack with adaptive step size under the $L_\infty$ constraint-Stable Adaptive Projected Gradient Descent (SA-PGD), achieving faster and more stable convergence under imprecise gradients. Extensive experiments show that our approach substantially increases attack success rates across diverse adversarial training schemes, SNN architectures and neuron models, providing a more generalized and reliable evaluation of SNN adversarial robustness. The experimental results further reveal that the robustness of current SNNs has been significantly overestimated and highlighting the need for more dependable adversarial training methods.

</details>


### [408] [TimePerceiver: An Encoder-Decoder Framework for Generalized Time-Series Forecasting](https://arxiv.org/abs/2512.22550)
*Jaebin Lee,Hankook Lee*

Main category: cs.LG

TL;DR: This paper introduces the TimePerceiver framework, which unifies encoder-decoder architecture and training for effective time-series forecasting, outperforming previous methods.


<details>
  <summary>Details</summary>
Motivation: Prior research in time-series forecasting has mainly concentrated on improving the encoder but overlooked the importance of tightly integrating prediction (decoding) and training. This gap inspired the development of a unified approach.

Method: The authors propose TimePerceiver, a novel encoder-decoder architecture complemented by a generalized training strategy. The encoder uses latent bottleneck representations for temporal and cross-channel dependencies, while the decoder employs learnable queries for accurate retrieval of target information.

Result: Empirical results show that TimePerceiver significantly outperforms state-of-the-art baselines on diverse benchmark datasets across tasks like extrapolation, interpolation, and imputation.

Conclusion: TimePerceiver effectively integrates encoding, decoding, and training into a holistic forecasting framework, demonstrating superior performance and flexibility in handling various time-series prediction tasks.

Abstract: In machine learning, effective modeling requires a holistic consideration of how to encode inputs, make predictions (i.e., decoding), and train the model. However, in time-series forecasting, prior work has predominantly focused on encoder design, often treating prediction and training as separate or secondary concerns. In this paper, we propose TimePerceiver, a unified encoder-decoder forecasting framework that is tightly aligned with an effective training strategy. To be specific, we first generalize the forecasting task to include diverse temporal prediction objectives such as extrapolation, interpolation, and imputation. Since this generalization requires handling input and target segments that are arbitrarily positioned along the temporal axis, we design a novel encoder-decoder architecture that can flexibly perceive and adapt to these varying positions. For encoding, we introduce a set of latent bottleneck representations that can interact with all input segments to jointly capture temporal and cross-channel dependencies. For decoding, we leverage learnable queries corresponding to target timestamps to effectively retrieve relevant information. Extensive experiments demonstrate that our framework consistently and significantly outperforms prior state-of-the-art baselines across a wide range of benchmark datasets. The code is available at https://github.com/efficient-learning-lab/TimePerceiver.

</details>


### [409] [Data-Driven Analysis of Crash Patterns in SAE Level 2 and Level 4 Automated Vehicles Using K-means Clustering and Association Rule Mining](https://arxiv.org/abs/2512.22589)
*Jewel Rana Palit,Vijayalakshmi K Kumarasamy,Osama A. Osman*

Main category: cs.LG

TL;DR: This paper investigates automated vehicle crash trends using a large dataset and a new data-mining approach to uncover crash patterns and contributors.


<details>
  <summary>Details</summary>
Motivation: Address concerns about AV safety in mixed traffic environments by understanding crash trends across various SAE Levels of automation using a larger dataset.

Method: Applying a two-stage data mining framework: K-means clustering to group AV crashes into behavioral clusters, followed by Association Rule Mining to identify relationships between crash patterns and contributing factors.

Result: Identified 4 distinct behavioral clusters of AV crashes and extracted interpretable relationships between crash patterns and contributors like lighting, surface condition, and environmental factors.

Conclusion: The study's insights can aid AV developers, regulators, and policymakers in developing strategies to improve AV safety and reduce crash risks.

Abstract: Automated Vehicles (AV) hold potential to reduce or eliminate human driving errors, enhance traffic safety, and support sustainable mobility. Recently, crash data has increasingly revealed that AV behavior can deviate from expected safety outcomes, raising concerns about the technology's safety and operational reliability in mixed traffic environments. While past research has investigated AV crash, most studies rely on small-size California-centered datasets, with a limited focus on understanding crash trends across various SAE Levels of automation. This study analyzes over 2,500 AV crash records from the United States National Highway Traffic Safety Administration (NHTSA), covering SAE Levels 2 and 4, to uncover underlying crash dynamics. A two-stage data mining framework is developed. K-means clustering is first applied to segment crash records into 4 distinct behavioral clusters based on temporal, spatial, and environmental factors. Then, Association Rule Mining (ARM) is used to extract interpretable multivariate relationships between crash patterns and crash contributors including lighting conditions, surface condition, vehicle dynamics, and environmental conditions within each cluster. These insights provide actionable guidance for AV developers, safety regulators, and policymakers in formulating AV deployment strategies and minimizing crash risks.

</details>


### [410] [Energy-Guided Flow Matching Enables Few-Step Conformer Generation and Ground-State Identification](https://arxiv.org/abs/2512.22597)
*Guikun Xu,Xiaohan Yi,Peilin Zhao,Yatao Bian*

Main category: cs.LG

TL;DR: EnFlow is a framework addressing challenges in generating low-energy conformer ensembles from molecular graphs, integrating flow matching and energy-guided sampling for both diversity and fidelity.


<details>
  <summary>Details</summary>
Motivation: Current computational methods for generating conformer ensembles and identifying ground states are computationally expensive, while existing learning-based models either lack reliable energy calibration or fail to represent ensemble variability.

Method: EnFlow combines flow matching (FM) with energy-guided sampling, steering sampling trajectories towards lower-energy regions. It also uses an explicitly learned energy model for efficient energy-based ranking of generated ensembles.

Result: EnFlow achieved improved conformer generation metrics with 1--2 steps and reduced ground-state prediction errors compared to state-of-the-art methods on GEOM-QM9 and GEOM-Drugs datasets.

Conclusion: EnFlow offers an efficient and accurate framework for both generating diverse conformer ensembles and identifying ground-state structures, advancing current methodologies.

Abstract: Generating low-energy conformer ensembles and identifying ground-state conformations from molecular graphs remain computationally demanding with physics-based pipelines. Current learning-based approaches often suffer from a fragmented paradigm: generative models capture diversity but lack reliable energy calibration, whereas deterministic predictors target a single structure and fail to represent ensemble variability. Here we present EnFlow, a unified framework that couples flow matching (FM) with an explicitly learned energy model through an energy-guided sampling scheme defined along a non-Gaussian FM path. By incorporating energy-gradient guidance during sampling, our method steers trajectories toward lower-energy regions, substantially improving conformational fidelity, particularly in the few-step regime. The learned energy function further enables efficient energy-based ranking of generated ensembles for accurate ground-state identification. Extensive experiments on GEOM-QM9 and GEOM-Drugs demonstrate that EnFlow simultaneously improves generation metrics with 1--2 ODE-steps and reduces ground-state prediction errors compared with state-of-the-art methods.

</details>


### [411] [Cryptocurrency Price Prediction Using Parallel Gated Recurrent Units](https://arxiv.org/abs/2512.22599)
*Milad Asadpour,Alireza Rezaee,Farshid Hajati*

Main category: cs.LG

TL;DR: The paper introduces a deep learning model named Parallel Gated Recurrent Units (PGRU) to enhance cryptocurrency price prediction accuracy with reduced input data and computational demands.


<details>
  <summary>Details</summary>
Motivation: The paper aims to tackle the challenge of accurate cryptocurrency price forecasting, which is crucial for investors and financial analysts due to the volatile nature of cryptocurrencies.

Method: A novel Parallel Gated Recurrent Units (PGRU) model is proposed, where multiple recurrent neural networks independently process distinct input features and combine predictions via a final neural network.

Result: The model achieves a mean absolute percentage error (MAPE) of 3.243% and 2.641% for window lengths of 20 and 15, respectively, outperforming existing methods with lower computational costs.

Conclusion: The PGRU model demonstrates superior accuracy and efficiency in predicting cryptocurrency prices, making it a promising tool for financial decision-making in the cryptocurrency domain.

Abstract: According to the advent of cryptocurrencies and Bitcoin, many investments and businesses are now conducted online through cryptocurrencies. Among them, Bitcoin uses blockchain technology to make transactions secure, transparent, traceable, and immutable. It also exhibits significant price fluctuations and performance, which has attracted substantial attention, especially in financial sectors. Consequently, a wide range of investors and individuals have turned to investing in the cryptocurrency market. One of the most important challenges in economics is price forecasting for future trades. Cryptocurrencies are no exception, and investors are looking for methods to predict prices; various theories and methods have been proposed in this field. This paper presents a new deep model, called \emph{Parallel Gated Recurrent Units} (PGRU), for cryptocurrency price prediction. In this model, recurrent neural networks forecast prices in a parallel and independent way. The parallel networks utilize different inputs, each representing distinct price-related features. Finally, the outputs of the parallel networks are combined by a neural network to forecast the future price of cryptocurrencies. The experimental results indicate that the proposed model achieves mean absolute percentage errors (MAPE) of 3.243% and 2.641% for window lengths 20 and 15, respectively. Our method therefore attains higher accuracy and efficiency with fewer input data and lower computational cost compared to existing methods.

</details>


### [412] [Gold Price Prediction Using Long Short-Term Memory and Multi-Layer Perceptron with Gray Wolf Optimizer](https://arxiv.org/abs/2512.22606)
*Hesam Taghipour,Alireza Rezaee,Farshid Hajati*

Main category: cs.LG

TL;DR: The paper presents an AI-based algorithm for forecasting daily and monthly gold prices with high accuracy using LSTM and MLP networks.


<details>
  <summary>Details</summary>
Motivation: Forecasting financial markets is complex due to economic and political factors; an accurate prediction model can provide significant benefits.

Method: Uses LSTM networks for daily/monthly predictions, integrated into an MLP network. Optimization is done using GWO algorithm on RMSE error.

Result: The model achieved 171% return in three months and low forecasting errors (MAE of $0.21 for daily prices and $22.23 for monthly).

Conclusion: LSTM-MLP algorithm demonstrates high predictive accuracy for gold prices and potential practical value for trading strategies.

Abstract: The global gold market, by its fundamentals, has long been home to many financial institutions, banks, governments, funds, and micro-investors. Due to the inherent complexity and relationship between important economic and political components, accurate forecasting of financial markets has always been challenging. Therefore, providing a model that can accurately predict the future of the markets is very important and will be of great benefit to their developers. In this paper, an artificial intelligence-based algorithm for daily and monthly gold forecasting is presented. Two Long short-term memory (LSTM) networks are responsible for daily and monthly forecasting, the results of which are integrated into a Multilayer perceptrons (MLP) network and provide the final forecast of the next day prices. The algorithm forecasts the highest, lowest, and closing prices on the daily and monthly time frame. Based on these forecasts, a trading strategy for live market trading was developed, according to which the proposed model had a return of 171% in three months. Also, the number of internal neurons in each network is optimized by the Gray Wolf optimization (GWO) algorithm based on the least RMSE error. The dataset was collected between 2010 and 2021 and includes data on macroeconomic, energy markets, stocks, and currency status of developed countries. Our proposed LSTM-MLP model predicted the daily closing price of gold with the Mean absolute error (MAE) of $ 0.21 and the next month's price with $ 22.23.

</details>


### [413] [Communication Compression for Distributed Learning with Aggregate and Server-Guided Feedback](https://arxiv.org/abs/2512.22623)
*Tomas Ortega,Chun-Yin Huang,Xiaoxiao Li,Hamid Jafarkhani*

Main category: cs.LG

TL;DR: The paper addresses communication cost challenges in Federated Learning by proposing two frameworks, CAFe and CAFe-S, which enable biased compression without violating user privacy. Analytical and experimental results show their superiority over existing methods.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is to address the communication bottleneck in Federated Learning due to client-to-server update transmission costs, while preserving user privacy and accommodating stateless clients.

Method: Two novel frameworks, CAFe and CAFe-S, are introduced. CAFe aggregates updates globally to replace client-specific control variates, while CAFe-S uses a server-guided mechanism utilizing a private dataset for accurate prediction.

Result: CAFe is proven analytically and experimentally superior to DCGD for non-convex scenarios, and CAFe-S demonstrates improved convergence with representative server data.

Conclusion: The proposed methodologies significantly reduce communication costs, maintain convergence, and ensure privacy in Federated Learning environments. Experimental validation confirms their efficacy over traditional methods.

Abstract: Distributed learning, particularly Federated Learning (FL), faces a significant bottleneck in the communication cost, particularly the uplink transmission of client-to-server updates, which is often constrained by asymmetric bandwidth limits at the edge. Biased compression techniques are effective in practice, but require error feedback mechanisms to provide theoretical guarantees and to ensure convergence when compression is aggressive. Standard error feedback, however, relies on client-specific control variates, which violates user privacy and is incompatible with stateless clients common in large-scale FL. This paper proposes two novel frameworks that enable biased compression without client-side state or control variates. The first, Compressed Aggregate Feedback (CAFe), uses the globally aggregated update from the previous round as a shared control variate for all clients. The second, Server-Guided Compressed Aggregate Feedback (CAFe-S), extends this idea to scenarios where the server possesses a small private dataset; it generates a server-guided candidate update to be used as a more accurate predictor. We consider Distributed Gradient Descent (DGD) as a representative algorithm and analytically prove CAFe's superiority to Distributed Compressed Gradient Descent (DCGD) with biased compression in the non-convex regime with bounded gradient dissimilarity. We further prove that CAFe-S converges to a stationary point, with a rate that improves as the server's data become more representative. Experimental results in FL scenarios validate the superiority of our approaches over existing compression schemes.

</details>


### [414] [Scaling Unverifiable Rewards: A Case Study on Visual Insights](https://arxiv.org/abs/2512.22650)
*Shuyu Gan,James Mooney,Pan Hao,Renxiang Wang,Mingyi Hong,Qianwen Wang,Dongyeop Kang*

Main category: cs.LG

TL;DR: Selective TTS is proposed to enhance multi-stage LLM pipeline performance by pruning low-quality steps early and deploying process-specific judges, demonstrated with improvements in data visualization benchmarks.


<details>
  <summary>Details</summary>
Motivation: The study addresses challenges in automating multi-stage tasks where reward signals are unverifiable, making conventional iterative refinement prone to accumulating errors.

Method: A novel framework called Selective TTS which scales reasoning across different pipeline stages, utilizing process-specific judges to stabilize refinement and mitigate judge drift.

Result: In a data science pipeline demonstrating chart and report generation, Selective TTS improved insight quality from 61.64 to 65.86 while lowering variance under fixed computational constraints.

Conclusion: Selective TTS offers a promising approach for tackling complex, open-ended tasks, potentially enabling advancements in areas such as scientific discovery and creative content generation.

Abstract: Large Language Model (LLM) agents can increasingly automate complex reasoning through Test-Time Scaling (TTS), iterative refinement guided by reward signals. However, many real-world tasks involve multi-stage pipeline whose final outcomes lack verifiable rewards or sufficient data to train robust reward models, making judge-based refinement prone to accumulate error over stages. We propose Selective TTS, a process-based refinement framework that scales inference across different stages in multi-agent pipeline, instead of repeated refinement over time by prior work. By distributing compute across stages and pruning low-quality branches early using process-specific judges, Selective TTS mitigates the judge drift and stabilizes refinement. Grounded in the data science pipeline, we build an end-to-end multi-agent pipeline for generating visually insightful charts and report of given dataset, and design a reliable LLM-based judge model, aligned with human experts (Kendall's τ=0.55). Our proposed selective TTS then improves insight quality under a fixed compute budget, increasing mean scores from 61.64 to 65.86 while reducing variance. We hope our findings serve as the first step toward to scaling complex, open-ended tasks with unverifiable rewards, such as scientific discovery and story generation.

</details>


### [415] [Quantum Generative Models for Computational Fluid Dynamics: A First Exploration of Latent Space Learning in Lattice Boltzmann Simulations](https://arxiv.org/abs/2512.22672)
*Achraf Hsain,Fouad Mohammed Abbou*

Main category: cs.LG

TL;DR: This paper explores the first use of quantum generative models for compressed CFD data, showing that quantum methods outperformed classical approaches in certain metrics.


<details>
  <summary>Details</summary>
Motivation: To investigate the potential of quantum generative models in analyzing and modeling compressed latent representations of computational fluid dynamics data, an area not previously explored.

Method: Created a GPU-accelerated CFD simulator to generate data, compressed using VQ-VAE. Tested classical LSTM against quantum generative models (QCBM, QGAN) for modeling the latent space.

Result: Quantum generative models (QCBM and QGAN) outperformed the classical LSTM baseline in modeling compressed data, with QCBM showing the best results.

Conclusion: Demonstrates quantum generative models' effectiveness in CFD data analysis and introduces a framework for future research combining quantum computing and physics simulations.

Abstract: This paper presents the first application of quantum generative models to learned latent space representations of computational fluid dynamics (CFD) data. While recent work has explored quantum models for learning statistical properties of fluid systems, the combination of discrete latent space compression with quantum generative sampling for CFD remains unexplored. We develop a GPU-accelerated Lattice Boltzmann Method (LBM) simulator to generate fluid vorticity fields, which are compressed into a discrete 7-dimensional latent space using a Vector Quantized Variational Autoencoder (VQ-VAE). The central contribution is a comparative analysis of quantum and classical generative approaches for modeling this physics-derived latent distribution: we evaluate a Quantum Circuit Born Machine (QCBM) and Quantum Generative Adversarial Network (QGAN) against a classical Long Short-Term Memory (LSTM) baseline. Under our experimental conditions, both quantum models produced samples with lower average minimum distances to the true distribution compared to the LSTM, with the QCBM achieving the most favorable metrics. This work provides: (1)~a complete open-source pipeline bridging CFD simulation and quantum machine learning, (2)~the first empirical study of quantum generative modeling on compressed latent representations of physics simulations, and (3)~a foundation for future rigorous investigation at this intersection.

</details>


### [416] [Beyond Centralization: Provable Communication Efficient Decentralized Multi-Task Learning](https://arxiv.org/abs/2512.22675)
*Donghwa Kang,Shana Moothedath*

Main category: cs.LG

TL;DR: This paper explores decentralized multi-task representation learning by proposing a novel alternating projected gradient and minimization algorithm that reduces communication costs and provides accuracy guarantees.


<details>
  <summary>Details</summary>
Motivation: To address the lack of research in decentralized multi-task representation learning and develop efficient methods for extracting features in data-scarce environments.

Method: The authors propose an alternating projected gradient and minimization algorithm designed to recover a low-rank feature matrix while addressing challenges associated with decentralized communication constraints.

Result: Numerical simulations validate the proposed method, highlighting its efficiency in terms of time, communication, and sample complexities. Importantly, it reduces communication cost significantly while maintaining competitive performance.

Conclusion: The decentralized learning framework often outperforms centralized methods in certain regimes, making it a practical approach for distributed environments.

Abstract: Representation learning is a widely adopted framework for learning in data-scarce environments, aiming to extract common features from related tasks. While centralized approaches have been extensively studied, decentralized methods remain largely underexplored. We study decentralized multi-task representation learning in which the features share a low-rank structure. We consider multiple tasks, each with a finite number of data samples, where the observations follow a linear model with task-specific parameters. In the decentralized setting, task data are distributed across multiple nodes, and information exchange between nodes is constrained by a communication network. The goal is to recover the underlying feature matrix whose rank is much smaller than both the parameter dimension and the number of tasks. We propose a new alternating projected gradient and minimization algorithm with provable accuracy guarantees. We provide comprehensive characterizations of the time, communication, and sample complexities. Importantly, the communication complexity is independent of the target accuracy, which significantly reduces communication cost compared to prior methods. Numerical simulations validate the theoretical analysis across different dimensions and network topologies, and demonstrate regimes in which decentralized learning outperforms centralized federated approaches.

</details>


### [417] [Learning with the $p$-adics](https://arxiv.org/abs/2512.22692)
*André F. T. Martins*

Main category: cs.LG

TL;DR: This paper explores the use of $p$-adic numbers, a non-archimedean space, in machine learning as an alternative to the commonly used field of real numbers.


<details>
  <summary>Details</summary>
Motivation: The authors aim to investigate whether fields other than the real numbers, specifically $p$-adic numbers, can be utilized in machine learning due to their hierarchical structure and unique properties.

Method: The paper introduces theoretical foundations and models for classification, regression, and representation learning based on $p$-adic numbers, demonstrating their application through compact $p$-adic linear networks.

Result: The authors show that $p$-adic numbers enable representations and constructions, such as compact Quillian semantic networks, that are not possible with real numbers.

Conclusion: The work establishes $p$-adic numbers as a viable alternative to real numbers for certain types of machine learning tasks, creating new possibilities for hierarchical representation learning and opening avenues for future research.

Abstract: Existing machine learning frameworks operate over the field of real numbers ($\mathbb{R}$) and learn representations in real (Euclidean or Hilbert) vector spaces (e.g., $\mathbb{R}^d$). Their underlying geometric properties align well with intuitive concepts such as linear separability, minimum enclosing balls, and subspace projection; and basic calculus provides a toolbox for learning through gradient-based optimization.
  But is this the only possible choice? In this paper, we study the suitability of a radically different field as an alternative to $\mathbb{R}$ -- the ultrametric and non-archimedean space of $p$-adic numbers, $\mathbb{Q}_p$. The hierarchical structure of the $p$-adics and their interpretation as infinite strings make them an appealing tool for code theory and hierarchical representation learning. Our exploratory theoretical work establishes the building blocks for classification, regression, and representation learning with the $p$-adics, providing learning models and algorithms. We illustrate how simple Quillian semantic networks can be represented as a compact $p$-adic linear network, a construction which is not possible with the field of reals. We finish by discussing open problems and opportunities for future research enabled by this new framework.

</details>


### [418] [Predictive Modeling of Power Outages during Extreme Events: Integrating Weather and Socio-Economic Factors](https://arxiv.org/abs/2512.22699)
*Antar Kumar Biswas,Masoud H. Nazari*

Main category: cs.LG

TL;DR: The paper proposes a learning-based framework using machine learning models for predicting power outages from extreme events, emphasizing low-probability, high-impact scenarios. The LSTM model performs best in minimizing prediction error.


<details>
  <summary>Details</summary>
Motivation: The motivation is to predict power outages during extreme events by leveraging a diverse dataset, revealing community vulnerability, and understanding outage risks.

Method: The approach integrates EAGLE-I outage records with additional datasets such as weather, socio-economic, and infrastructure-related data, testing machine learning models (RF, SVM, AdaBoost, LSTM).

Result: Experimental validation identifies LSTM as the most effective model, with findings linking stronger economic/infrastructure conditions to fewer outages.

Conclusion: The framework underscores the effectiveness of LSTM in power outage prediction, contributing to better understanding and mitigation of outage risks during extreme conditions.

Abstract: This paper presents a novel learning-based framework for predicting power outages caused by extreme events. The proposed approach specifically targets low-probability, high-consequence outage scenarios and leverages a comprehensive set of features derived from publicly available data sources. We integrate EAGLE-I outage records (2014-2024) with weather, socio-economic, infrastructure, and seasonal event data. Incorporating social and demographic indicators reveals underlying patterns of community vulnerability and provides a clearer understanding of outage risk during extreme conditions. Four machine learning models (Random Forest (RF), Support Vector Machine (SVM), Adaptive Boosting (AdaBoost), and Long Short-Term Memory (LSTM)) are evaluated. Experimental validation is performed on a large-scale dataset covering counties in the lower peninsula of Michigan. Among all models tested, the LSTM network achieves the lowest prediction error. Additionally, the results demonstrate that stronger economic conditions and more developed infrastructure are associated with lower outage occurrence.

</details>


### [419] [What Matters in Deep Learning for Time Series Forecasting?](https://arxiv.org/abs/2512.22702)
*Valentina Moretti,Andrea Cini,Ivan Marisca,Cesare Alippi*

Main category: cs.LG

TL;DR: The paper evaluates the complex design choices in deep learning architectures for time series forecasting, emphasizing the importance of fundamental principles over specific methods.


<details>
  <summary>Details</summary>
Motivation: To address confusion and mixed results in the performance of deep learning models for time series forecasting by identifying the impactful design elements.

Method: Analyze design dimensions like locality and globality that impact forecasting, highlight the need for simplicity in architecture, and propose a model card framework.

Result: Well-designed, simpler forecasting architectures can compete with state-of-the-art methods when fundamental principles are considered.

Conclusion: Research should emphasize foundational principles, rethink benchmarking practices, and adopt tools like the proposed model card for better clarity in forecasting model design.

Abstract: Deep learning models have grown increasingly popular in time series applications. However, the large quantity of newly proposed architectures, together with often contradictory empirical results, makes it difficult to assess which components contribute significantly to final performance. We aim to make sense of the current design space of deep learning architectures for time series forecasting by discussing the design dimensions and trade-offs that can explain, often unexpected, observed results. This paper discusses the necessity of grounding model design on principles for forecasting groups of time series and how such principles can be applied to current models. In particular, we assess how concepts such as locality and globality apply to recent forecasting architectures. We show that accounting for these aspects can be more relevant for achieving accurate results than adopting specific sequence modeling layers and that simple, well-designed forecasting architectures can often match the state of the art. We discuss how overlooked implementation details in existing architectures (1) fundamentally change the class of the resulting forecasting method and (2) drastically affect the observed empirical results. Our results call for rethinking current faulty benchmarking practices and the need to focus on the foundational aspects of the forecasting problem when designing architectures. As a step in this direction, we propose an auxiliary forecasting model card, whose fields serve to characterize existing and new forecasting architectures based on key design choices.

</details>


### [420] [FoldAct: Efficient and Stable Context Folding for Long-Horizon Search Agents](https://arxiv.org/abs/2512.22733)
*Jiaqi Shao,Yufeng Miao,Wei Zhang,Bing Luo*

Main category: cs.LG

TL;DR: The paper introduces FoldAct, a framework that tackles challenges in long-horizon reinforcement learning for large language models caused by unbounded context growth.


<details>
  <summary>Details</summary>
Motivation: To address scalability issues in long-horizon RL for large language models, particularly dealing with the non-stationary observation distribution from context folding methods.

Method: The authors propose innovations like separated loss computation for better gradient signals, full context consistency loss to curb distribution shift, and selective segment training to reduce computational cost.

Result: FoldAct enables stable training of long-horizon search agents, successfully addressing non-stationary observation challenges and achieving a 5.19× computational efficiency improvement.

Conclusion: FoldAct is an effective framework for enhancing training stability and efficiency in large language models handling long-horizon RL tasks with context folding.

Abstract: Long-horizon reinforcement learning (RL) for large language models faces critical scalability challenges from unbounded context growth, leading to context folding methods that compress interaction history during task execution. However, existing approaches treat summary actions as standard actions, overlooking that summaries fundamentally modify the agent's future observation space, creating a policy-dependent, non-stationary observation distribution that violates core RL assumptions. This introduces three fundamental challenges: (1) gradient dilution where summary tokens receive insufficient training signal, (2) self-conditioning where policy updates change summary distributions, creating a vicious cycle of training collapse, and (3) computational cost from processing unique contexts at each turn. We introduce \textbf{FoldAct}\footnote{https://github.com/SHAO-Jiaqi757/FoldAct}, a framework that explicitly addresses these challenges through three key innovations: separated loss computation for independent gradient signals on summary and action tokens, full context consistency loss to reduce distribution shift, and selective segment training to reduce computational cost. Our method enables stable training of long-horizon search agents with context folding, addressing the non-stationary observation problem while improving training efficiency with 5.19$\times$ speedup.

</details>


### [421] [When Does Multi-Task Learning Fail? Quantifying Data Imbalance and Task Independence in Metal Alloy Property Prediction](https://arxiv.org/abs/2512.22740)
*Sungwoo Kang*

Main category: cs.LG

TL;DR: The paper evaluates multi-task learning (MTL) for predicting different alloy properties and discovers MTL improves classification performance but worsens regression.


<details>
  <summary>Details</summary>
Motivation: To investigate whether multi-task learning can leverage shared physics among related material properties for enhanced predictions.

Method: Comparison of single-task models with standard and structured multi-task learning models using 54,028 alloy samples for predicting resistivity, hardness, and amorphous-forming ability.

Result: MTL significantly degraded regression performance but improved classification results, showing independence between tasks, particularly due to severe data imbalance.

Conclusion: Independent modeling is suggested for regression tasks, while MTL is useful for classification tasks prioritizing recall.

Abstract: Multi-task learning (MTL) assumes related material properties share underlying physics that can be leveraged for better predictions. We test this by simultaneously predicting electrical resistivity, Vickers hardness, and amorphous-forming ability using 54,028 alloy samples. We compare single-task models against standard and structured MTL. Results reveal a striking dichotomy: MTL significantly degrades regression performance (resistivity $R^2$: 0.897 $\to$ 0.844; hardness $R^2$: 0.832 $\to$ 0.694, $p < 0.01$) but improves classification (amorphous F1: 0.703 $\to$ 0.744, $p < 0.05$; recall +17%). Analysis shows near-zero inter-task weights, indicating property independence. Regression failure is attributed to negative transfer caused by severe data imbalance (52k vs. 800 samples). We recommend independent models for precise regression, while reserving MTL for classification tasks where recall is critical.

</details>


### [422] [Bridging Global Intent with Local Details: A Hierarchical Representation Approach for Semantic Validation in Text-to-SQL](https://arxiv.org/abs/2512.22744)
*Rihong Qiu,Zhibang Yang,Xinke Jiang,Weibin Liao,Xin Gao,Xu Chu,Junfeng Zhao,Yasha Wang*

Main category: cs.LG

TL;DR: HEROSQL enhances Text-to-SQL systems by improving semantic validation using a hierarchical SQL representation and a novel neural network architecture.


<details>
  <summary>Details</summary>
Motivation: To address the semantic validation challenges in Text-to-SQL systems, specifically capturing user intent and SQL structural details while producing fine-grained annotations.

Method: Introduced HEROSQL, which uses hierarchical SQL representation combining Logical Plans (LPs) and Abstract Syntax Trees (ASTs), along with a Nested Message Passing Neural Network (NMPNN) for capturing relational and schema-guided semantics. Leveraged AST-based sub-SQL augmentation to generate high-quality negative samples.

Result: Achieved an average 9.40% improvement in AUPRC and 12.35% in AUROC for identifying semantic inconsistencies across in-domain and out-of-domain validation benchmarks, outperforming state-of-the-art techniques.

Conclusion: HEROSQL effectively detects semantic inconsistencies in Text-to-SQL systems, improves error detection, provides granular feedback for large language models, and enhances the reliability of data querying systems.

Abstract: Text-to-SQL translates natural language questions into SQL statements grounded in a target database schema. Ensuring the reliability and executability of such systems requires validating generated SQL, but most existing approaches focus only on syntactic correctness, with few addressing semantic validation (detecting misalignments between questions and SQL). As a consequence, effective semantic validation still faces two key challenges: capturing both global user intent and SQL structural details, and constructing high-quality fine-grained sub-SQL annotations. To tackle these, we introduce HEROSQL, a hierarchical SQL representation approach that integrates global intent (via Logical Plans, LPs) and local details (via Abstract Syntax Trees, ASTs). To enable better information propagation, we employ a Nested Message Passing Neural Network (NMPNN) to capture inherent relational information in SQL and aggregate schema-guided semantics across LPs and ASTs. Additionally, to generate high-quality negative samples, we propose an AST-driven sub-SQL augmentation strategy, supporting robust optimization of fine-grained semantic inconsistencies. Extensive experiments conducted on Text-to-SQL validation benchmarks (both in-domain and out-of-domain settings) demonstrate that our approach outperforms existing state-of-the-art methods, achieving an average 9.40% improvement of AUPRC and 12.35% of AUROC in identifying semantic inconsistencies. It excels at detecting fine-grained semantic errors, provides large language models with more granular feedback, and ultimately enhances the reliability and interpretability of data querying platforms.

</details>


### [423] [Debugging Tabular Log as Dynamic Graphs](https://arxiv.org/abs/2512.22903)
*Chumeng Liang,Zhanyang Jin,Zahaib Akhtar,Mona Pereira,Haofei Yu,Jiaxuan You*

Main category: cs.LG

TL;DR: The paper introduces GraphLogDebugger, a framework using dynamic graphs and Graph Neural Networks (GNNs) to debug text-enriched tabular log data, offering better scalability and flexibility than large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: Recent methods for processing tabular log data heavily depend on large language models (LLMs) and heavy computational models that lack scalability and flexibility.

Method: The proposed framework, GraphLogDebugger, transforms tabular log data into dynamic graphs by creating heterogeneous nodes for objects/events and interconnecting them. A simple dynamic Graph Neural Network (GNN) is utilized for debugging.

Result: Experimental results demonstrate that the dynamic GNN approach applied in GraphLogDebugger outperforms large language models in debugging tabular logs across various real-world datasets.

Conclusion: GraphLogDebugger offers a scalable and efficient alternative to traditional LLM-based methods for debugging tabular logs by leveraging dynamic graph modeling with GNNs.

Abstract: Tabular log abstracts objects and events in the real-world system and reports their updates to reflect the change of the system, where one can detect real-world inconsistencies efficiently by debugging corresponding log entries. However, recent advances in processing text-enriched tabular log data overly depend on large language models (LLMs) and other heavy-load models, thus suffering from limited flexibility and scalability. This paper proposes a new framework, GraphLogDebugger, to debug tabular log based on dynamic graphs. By constructing heterogeneous nodes for objects and events and connecting node-wise edges, the framework recovers the system behind the tabular log as an evolving dynamic graph. With the help of our dynamic graph modeling, a simple dynamic Graph Neural Network (GNN) is representative enough to outperform LLMs in debugging tabular log, which is validated by experimental results on real-world log datasets of computer systems and academic papers.

</details>


### [424] [From Confounding to Learning: Dynamic Service Fee Pricing on Third-Party Platforms](https://arxiv.org/abs/2512.22749)
*Rui Ai,David Simchi-Levi,Feng Zhu*

Main category: cs.LG

TL;DR: This paper explores learning demand under confounding in pricing by third-party platforms, achieving optimal regret bounds.


<details>
  <summary>Details</summary>
Motivation: The paper aims to understand how third-party platforms can optimize pricing strategies when faced with confounding factors impacting demand while observing only equilibrium price and quantity.

Method: The authors develop an algorithm with optimal regret bounds, leveraging non-i.i.d. actions as instrumental variables and introducing a novel homeomorphic construction for demand estimation using deep neural networks.

Result: The study reveals a phase transition in regret due to supply-side noise and demonstrates the algorithm's efficiency with simulations and real-world data from Zomato and Lyft.

Conclusion: The proposed methodology effectively addresses the demand learning problem, providing theoretical insights and practical value for third-party platforms in pricing optimization.

Abstract: We study the pricing behavior of third-party platforms facing strategic agents. Assuming the platform is a revenue maximizer, it observes market features that generally affect demand. Since only the equilibrium price and quantity are observable, this presents a general demand learning problem under confounding. Mathematically, we develop an algorithm with optimal regret of $\Tilde{\cO}(\sqrt{T}\wedgeσ_S^{-2})$. Our results reveal that supply-side noise fundamentally affects the learnability of demand, leading to a phase transition in regret. Technically, we show that non-i.i.d. actions can serve as instrumental variables for learning demand. We also propose a novel homeomorphic construction that allows us to establish estimation bounds without assuming star-shapedness, providing the first efficiency guarantee for learning demand with deep neural networks. Finally, we demonstrate the practical applicability of our approach through simulations and real-world data from Zomato and Lyft.

</details>


### [425] [A Micro-Macro Machine Learning Framework for Predicting Childhood Obesity Risk Using NHANES and Environmental Determinants](https://arxiv.org/abs/2512.22758)
*Eswarasanthosh Kumar Mamillapalli,Nishtha Sharma*

Main category: cs.LG

TL;DR: The paper develops a machine learning framework combining individual and environmental data to better predict and analyze obesity risk, achieving robust model performance and highlighting geographical environmental disparities.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional studies that examine individual and environmental factors of obesity independently, aiming to better understand the interplay of these factors in influencing health outcomes.

Method: The study integrates individual anthropometric/socioeconomic data from NHANES with environmental data including food access, air quality, and socioeconomic vulnerability from USDA/EPA. Machine learning models (Logistic Regression, Random Forest, XGBoost, LightGBM) were trained, with XGBoost outperforming others. An environmental vulnerability index (EnvScore) was constructed for multi-scale comparisons.

Result: XGBoost demonstrated the best model performance. The integration of environmental vulnerability index with obesity risk successfully revealed geographic disparities, linking environmental burden to obesity prevalence.

Conclusion: The multi-level machine learning framework effectively combines individual and macro-level datasets, offering a transformative approach to public health challenges like obesity. It underscores the potential for data-driven strategies in understanding and addressing health disparities.

Abstract: Childhood obesity remains a major public health challenge in the United States, strongly influenced by a combination of individual-level, household-level, and environmental-level risk factors. Traditional epidemiological studies typically analyze these levels independently, limiting insights into how structural environmental conditions interact with individual-level characteristics to influence health outcomes. In this study, we introduce a micro-macro machine learning framework that integrates (1) individual-level anthropometric and socioeconomic data from NHANES and (2) macro-level structural environment features, including food access, air quality, and socioeconomic vulnerability extracted from USDA and EPA datasets. Four machine learning models Logistic Regression, Random Forest, XGBoost, and LightGBM were trained to predict obesity using NHANES microdata. XGBoost achieved the strongest performance. A composite environmental vulnerability index (EnvScore) was constructed using normalized indicators from USDA and EPA at the state level. Multi-level comparison revealed strong geographic similarity between states with high environmental burden and the nationally predicted micro-level obesity risk distribution. This demonstrates the feasibility of integrating multi-scale datasets to identify environment-driven disparities in obesity risk. This work contributes a scalable, data-driven, multi-level modeling pipeline suitable for public health informatics, demonstrating strong potential for expansion into causal modeling, intervention planning, and real-time analytics.

</details>


### [426] [A Note on Hybrid Online Reinforcement and Imitation Learning for LLMs: Formulations and Algorithms](https://arxiv.org/abs/2512.23097)
*Yingru Li,Ziniu Li,Jiacai Liu*

Main category: cs.LG

TL;DR: A unified framework for Large Language Model fine-tuning is proposed, combining Imitation Learning and Reinforcement Learning for effective and efficient performance.


<details>
  <summary>Details</summary>
Motivation: To improve the process of fine-tuning Large Language Models by using a combined approach of trajectory-based KL divergence and task rewards for optimization.

Method: The method involves analyzing gradients of a composite objective, which decomposes into a Dense Gradient for token-level tasks (computable analytically) and a Sparse Gradient for long-horizon reward-based tasks (approximated via Monte Carlo methods).

Result: The Dense Gradient can be expressed in a closed-form formula, leading to efficient GPU implementation.

Conclusion: The framework effectively combines components of imitation and reinforcement learning, offering a streamlined and computationally efficient model fine-tuning approach.

Abstract: We present a unified framework for Large Language Model (LLM) fine-tuning that integrates Imitation Learning and Reinforcement Learning. By analyzing the gradient of a composite objective combining trajectory-level KL divergence with task rewards, we derive a natural decomposition into two components: (1) an analytically computable Dense Gradient for token-level imitation, and (2) a Monte Carlo estimated Sparse Gradient for long-horizon reward optimization. The Dense Gradient admits a closed-form logit-level formula, enabling efficient GPU implementation.

</details>


### [427] [GRExplainer: A Universal Explanation Method for Temporal Graph Neural Networks](https://arxiv.org/abs/2512.22772)
*Xuyan Li,Jie Wang,Zheng Yan*

Main category: cs.LG

TL;DR: GRExplainer is introduced as a novel, universal method for explaining Temporal Graph Neural Networks (TGNNs), focusing on improving generality, efficiency, and user-friendliness.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in TGNN explainability, including lack of generality, high computational costs, and limited user-friendliness in existing methods.

Method: GRExplainer uses node sequences as a unified feature representation for generalized applicability and employs a breadth-first search and temporal information to reduce computational redundancy. A Recurrent Neural Network (RNN)-based generative model is also designed for automated and continuous explanation generation.

Result: GRExplainer demonstrated superior performance on six datasets with three TGNNs, surpassing established methods in generality, efficiency, and user-friendliness.

Conclusion: The paper concludes that GRExplainer is a universal, efficient, and user-friendly solution for improving the explainability of TGNNs, making it applicable across various scenarios.

Abstract: Dynamic graphs are widely used to represent evolving real-world networks. Temporal Graph Neural Networks (TGNNs) have emerged as a powerful tool for processing such graphs, but the lack of transparency and explainability limits their practical adoption. Research on TGNN explainability is still in its early stages and faces several key issues: (i) Current methods are tailored to specific TGNN types, restricting generality. (ii) They suffer from high computational costs, making them unsuitable for large-scale networks. (iii) They often overlook the structural connectivity of explanations and require prior knowledge, reducing user-friendliness. To address these issues, we propose GRExplainer, the first universal, efficient, and user-friendly explanation method for TGNNs. GRExplainer extracts node sequences as a unified feature representation, making it independent of specific input formats and thus applicable to both snapshot-based and event-based TGNNs (the major types of TGNNs). By utilizing breadth-first search and temporal information to construct input node sequences, GRExplainer reduces redundant computation and improves efficiency. To enhance user-friendliness, we design a generative model based on Recurrent Neural Networks (RNNs), enabling automated and continuous explanation generation. Experiments on six real-world datasets with three target TGNNs show that GRExplainer outperforms existing baseline methods in generality, efficiency, and user-friendliness.

</details>


### [428] [Theoretical Foundations of Scaling Law in Familial Models](https://arxiv.org/abs/2512.23407)
*Huan Song,Qingfei Zhao,Ting Long,Shuyu Tian,Hongjun An,Jiawei Shao,Chi Zhang,Xuelong Li*

Main category: cs.LG

TL;DR: The paper introduces scaling laws for "Familial models," which allow generating multiple deployable sub-models from one backbone by integrating early exits and relay-style inference. It introduces Granularity (G) as a critical variable alongside model size (N) and training tokens (D).


<details>
  <summary>Details</summary>
Motivation: The motivation is to extend neural scaling laws to accommodate dynamic, deployable architectures suitable for heterogeneous device-edge-cloud hierarchies.

Method: The authors propose a unified scaling functional form, L(N, D, G), and isolate architectural impacts using IsoFLOP experiments with systematic sweeps of model size, granularity, and adaptive tokens.

Result: Results reveal that granularity penalty follows a power law with a minor exponent, establishing deployment flexibility without harming compute-optimality.

Conclusion: The study bridges fixed-compute training with dynamic architectures, confirming that "train once, deploy many" is feasible at no compromise to dense baseline performance.

Abstract: Neural scaling laws have become foundational for optimizing large language model (LLM) training, yet they typically assume a single dense model output. This limitation effectively overlooks "Familial models, a transformative paradigm essential for realizing ubiquitous intelligence across heterogeneous device-edge-cloud hierarchies. Transcending static architectures, familial models integrate early exits with relay-style inference to spawn G deployable sub-models from a single shared backbone. In this work, we theoretically and empirically extend the scaling law to capture this "one-run, many-models" paradigm by introducing Granularity (G) as a fundamental scaling variable alongside model size (N) and training tokens (D). To rigorously quantify this relationship, we propose a unified functional form L(N, D, G) and parameterize it using large-scale empirical runs. Specifically, we employ a rigorous IsoFLOP experimental design to strictly isolate architectural impact from computational scale. Across fixed budgets, we systematically sweep model sizes (N) and granularities (G) while dynamically adjusting tokens (D). This approach effectively decouples the marginal cost of granularity from the benefits of scale, ensuring high-fidelity parameterization of our unified scaling law. Our results reveal that the granularity penalty follows a multiplicative power law with an extremely small exponent. Theoretically, this bridges fixed-compute training with dynamic architectures. Practically, it validates the "train once, deploy many" paradigm, demonstrating that deployment flexibility is achievable without compromising the compute-optimality of dense baselines.

</details>


### [429] [Schrodinger AI: A Unified Spectral-Dynamical Framework for Classification, Reasoning, and Operator-Based Generalization](https://arxiv.org/abs/2512.22774)
*Truong Son Nguyen*

Main category: cs.LG

TL;DR: The paper introduces Schrödinger AI, a machine learning system inspired by quantum mechanics composed of three components for perception, dynamic reasoning, and symbolic transformations, yielding robust generalization and interpretable semantics.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore a physics-driven framework inspired by quantum mechanics as an alternative to traditional deep learning methods, addressing challenges in generalization and interpretability.

Method: Schrödinger AI utilizes three components: a wave-energy solver for spectral perception, a dynamical solver for evolving semantics, and operator calculus for symbolic reasoning.

Result: Schrödinger AI achieves emergent semantic manifolds, dynamic environmental adaptation, and exact generalization in modular arithmetic tasks.

Conclusion: The system showcases a novel learning paradigm by navigating a semantic energy landscape, offering robust generalization and dynamics for future machine learning directions.

Abstract: We introduce \textbf{Schrödinger AI}, a unified machine learning framework inspired by quantum mechanics. The system is defined by three tightly coupled components: (1) a {time-independent wave-energy solver} that treats perception and classification as spectral decomposition under a learned Hamiltonian; (2) a {time-dependent dynamical solver} governing the evolution of semantic wavefunctions over time, enabling context-aware decision revision, re-routing, and reasoning under environmental changes; and (3) a {low-rank operator calculus} that learns symbolic transformations such as modular arithmetic through learned quantum-like transition operators. Together, these components form a coherent physics-driven alternative to conventional cross-entropy training and transformer attention, providing robust generalization, interpretable semantics, and emergent topology.
  Empirically, Schrödinger AI demonstrates: (a) emergent semantic manifolds that reflect human-conceived class relations without explicit supervision; (b) dynamic reasoning that adapts to changing environments, including maze navigation with real-time potential-field perturbations; and (c) exact operator generalization on modular arithmetic tasks, where the system learns group actions and composes them across sequences far beyond training length. These results suggest a new foundational direction for machine learning, where learning is cast as discovering and navigating an underlying semantic energy landscape.

</details>


### [430] [Adapting, Fast and Slow: Transportable Circuits for Few-Shot Learning](https://arxiv.org/abs/2512.22777)
*Kasra Jalaldoust,Elias Bareinboim*

Main category: cs.LG

TL;DR: The paper introduces Circuit-TR, an algorithm leveraging causal graphs for zero-shot compositional generalization and supervised domain adaptation, enabling effective learning across domains.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenge of generalizing across unseen target domains by imposing structured constraints related to causality, enabling compositional transportability.

Method: Circuit-TR algorithm utilizes causal graphs for intra-domain structure, discrepancies oracles for inter-domain association, and module composition for prediction, along with a supervised domain adaptation mechanism using limited target data.

Result: Theoretical foundations characterize the learnability in terms of causal structure and circuit complexity, supported by controlled simulations that validate these theoretical claims.

Conclusion: Circuit-TR demonstrates that causal structure and graphical criteria are potent tools for achieving few-shot learnability and zero-shot generalization, emphasizing applicability in real-world domains.

Abstract: Generalization across the domains is not possible without asserting a structure that constrains the unseen target domain w.r.t. the source domain. Building on causal transportability theory, we design an algorithm for zero-shot compositional generalization which relies on access to qualitative domain knowledge in form of a causal graph for intra-domain structure and discrepancies oracle for inter-domain mechanism sharing. \textit{Circuit-TR} learns a collection of modules (i.e., local predictors) from the source data, and transport/compose them to obtain a circuit for prediction in the target domain if the causal structure licenses. Furthermore, circuit transportability enables us to design a supervised domain adaptation scheme that operates without access to an explicit causal structure, and instead uses limited target data. Our theoretical results characterize classes of few-shot learnable tasks in terms of graphical circuit transportability criteria, and connects few-shot generalizability with the established notion of circuit size complexity; controlled simulations corroborate our theoretical results.

</details>


### [431] [VL-RouterBench: A Benchmark for Vision-Language Model Routing](https://arxiv.org/abs/2512.23562)
*Zhehao Huang,Baijiong Lin,Jingyuan Zhang,Jingying Wang,Yuhang Liu,Ning Lu,Tao Li,Xiaolin Huang*

Main category: cs.LG

TL;DR: The paper introduces VL-RouterBench, a benchmark to evaluate vision-language model routing systems using datasets and models with systematic cost and accuracy metrics.


<details>
  <summary>Details</summary>
Motivation: To address the lack of systematic and reproducible benchmarks for evaluating vision-language model (VLM) routing systems and to improve their development.

Method: The framework assesses VLM routing systems via 14 datasets, 15 open-source models, and 2 API models using metrics such as accuracy, cost, and throughput, and ranks performance by harmonic means.

Result: Evaluated 10 routing methods and observed improved routability, though significant gaps persist compared to ideal Oracle benchmarks.

Conclusion: VL-RouterBench provides a standardized evaluation benchmark, fostering progress in multimodal routing, but highlights room for improvement in router architectures.

Abstract: Multi-model routing has evolved from an engineering technique into essential infrastructure, yet existing work lacks a systematic, reproducible benchmark for evaluating vision-language models (VLMs). We present VL-RouterBench to assess the overall capability of VLM routing systems systematically. The benchmark is grounded in raw inference and scoring logs from VLMs and constructs quality and cost matrices over sample-model pairs. In scale, VL-RouterBench covers 14 datasets across 3 task groups, totaling 30,540 samples, and includes 15 open-source models and 2 API models, yielding 519,180 sample-model pairs and a total input-output token volume of 34,494,977. The evaluation protocol jointly measures average accuracy, average cost, and throughput, and builds a ranking score from the harmonic mean of normalized cost and accuracy to enable comparison across router configurations and cost budgets. On this benchmark, we evaluate 10 routing methods and baselines and observe a significant routability gain, while the best current routers still show a clear gap to the ideal Oracle, indicating considerable room for improvement in router architecture through finer visual cues and modeling of textual structure. We will open-source the complete data construction and evaluation toolchain to promote comparability, reproducibility, and practical deployment in multimodal routing research.

</details>


### [432] [Discovering Transmission Dynamics of COVID-19 in China](https://arxiv.org/abs/2512.22787)
*Zhou Yang,Edward Dougherty,Chen Zhang,Zhenhe Pan,Fang Jin*

Main category: cs.LG

TL;DR: This study retrospectively analyzes China's COVID-19 transmission patterns using tracking data, identifying key temporal and spatial transmission dynamics and sources.


<details>
  <summary>Details</summary>
Motivation: To identify mechanisms most effective in mitigating COVID-19 by analyzing transmission patterns in China leveraging publicly available tracking and population mobility data.

Method: The authors collected case reports through local health commissions, Chinese CDC, and social media platforms, then used NLP and manual curation to build transmission chains and analyze these alongside mobility data.

Result: They found significant regional differences, with larger cities experiencing more infections driven by social activities. Symptomatic individuals were hospitalized within ~5 days of symptoms, and infection sources shifted from travel-related cases to social activities.

Conclusion: China's COVID-19 transmission evolved from travel-driven cases to more socially driven spread, emphasizing the importance of tailored interventions according to transmission dynamics.

Abstract: A comprehensive retrospective analysis of public health interventions, such as large scale testing, quarantining, and contact tracing, can help identify mechanisms most effective in mitigating COVID-19. We investigate China based SARS-CoV-2 transmission patterns (e.g., infection type and likely transmission source) using publicly released tracking data. We collect case reports from local health commissions, the Chinese CDC, and official local government social media, then apply NLP and manual curation to construct transmission/tracking chains. We further analyze tracking data together with Wuhan population mobility data to quantify and visualize temporal and spatial spread dynamics. Results indicate substantial regional differences, with larger cities showing more infections, likely driven by social activities. Most symptomatic individuals (79\%) were hospitalized within 5 days of symptom onset, and those with confirmed-case contact sought admission in under 5 days. Infection sources also shifted over time: early cases were largely linked to travel to (or contact with travelers from) Hubei Province, while later transmission was increasingly associated with social activities.

</details>


### [433] [SNM-Net: A Universal Framework for Robust Open-Set Gas Recognition via Spherical Normalization and Mahalanobis Distance](https://arxiv.org/abs/2512.22792)
*Shuai Chen,Chen Wang,Ziran Wang*

Main category: cs.LG

TL;DR: This study introduces SNM-Net, a deep learning framework for open-set gas recognition, designed to address feature distribution shifts and decision failures in electronic nose (E-nose) systems, achieving superior performance and stability.


<details>
  <summary>Details</summary>
Motivation: The study aims to overcome two key challenges in open-set gas recognition for E-nose systems: feature distribution shifts caused by signal drift and decision failures due to unknown gas interference.

Method: The proposed SNM-Net framework utilizes a geometric decoupling mechanism combining cascaded batch normalization and L2 normalization to manage signal intensity fluctuations. It employs Mahalanobis distance for scoring and constructs adaptive ellipsoidal decision boundaries.

Result: SNM-Net achieves near-theoretical performance with a Transformer backbone, attaining an AUROC of 0.9977 and an unknown gas detection rate of 99.57% (TPR at 5% FPR). It outperforms state-of-the-art methods with a 3.0% AUROC improvement and greatly reduces standard deviation.

Conclusion: The framework demonstrates excellent accuracy and robustness, resolving accuracy-stability trade-offs and providing a viable foundation for industrial E-nose system applications.

Abstract: Electronic nose (E-nose) systems face dual challenges in open-set gas recognition: feature distribution shifts caused by signal drift and decision failures induced by unknown interference. Existing methods predominantly rely on Euclidean distance, failing to adequately account for anisotropic gas feature distributions and dynamic signal intensity variations. To address these issues, this study proposes SNM-Net, a universal deep learning framework for open-set gas recognition. The core innovation lies in a geometric decoupling mechanism achieved through cascaded batch normalization and L2 normalization, which projects high-dimensional features onto a unit hypersphere to eliminate signal intensity fluctuations. Additionally, Mahalanobis distance is introduced as the scoring mechanism, utilizing class-wise statistics to construct adaptive ellipsoidal decision boundaries. SNM-Net is architecture-agnostic and seamlessly integrates with CNN, RNN, and Transformer backbones. Systematic experiments on the Vergara dataset demonstrate that the Transformer+SNM configuration attains near-theoretical performance, achieving an AUROC of 0.9977 and an unknown gas detection rate of 99.57% (TPR at 5% FPR). This performance significantly outperforms state-of-the-art methods, showing a 3.0% improvement in AUROC and a 91.0% reduction in standard deviation compared to Class Anchor Clustering. The framework exhibits exceptional robustness across sensor positions with standard deviations below 0.0028. This work effectively resolves the trade-off between accuracy and stability, providing a solid technical foundation for industrial E-nose deployment.

</details>


### [434] [ReDiF: Reinforced Distillation for Few Step Diffusion](https://arxiv.org/abs/2512.22802)
*Amirhossein Tighkhorshid,Zahra Dehghanian,Gholamali Aminian,Chengchun Shi,Hamid R. Rabiee*

Main category: cs.LG

TL;DR: This paper proposes a reinforcement learning-based distillation framework to improve diffusion models, achieving better results with fewer steps and computational resources.


<details>
  <summary>Details</summary>
Motivation: To address the slow sampling problem of diffusion models by developing a more efficient distillation process that does not rely on fixed loss functions.

Method: The authors treat distillation as a reinforcement learning problem, training the student model using reward signals from teacher alignment and allowing dynamic optimization of sampling paths.

Result: The method demonstrates improved performance, requiring fewer inference steps and less computation, while being applicable across different diffusion models.

Conclusion: The proposed RL-driven distillation provides a general, efficient optimization framework for diffusion models, overcoming limitations of traditional distillation techniques.

Abstract: Distillation addresses the slow sampling problem in diffusion models by creating models with smaller size or fewer steps that approximate the behavior of high-step teachers. In this work, we propose a reinforcement learning based distillation framework for diffusion models. Instead of relying on fixed reconstruction or consistency losses, we treat the distillation process as a policy optimization problem, where the student is trained using a reward signal derived from alignment with the teacher's outputs. This RL driven approach dynamically guides the student to explore multiple denoising paths, allowing it to take longer, optimized steps toward high-probability regions of the data distribution, rather than relying on incremental refinements. Our framework utilizes the inherent ability of diffusion models to handle larger steps and effectively manage the generative process. Experimental results show that our method achieves superior performance with significantly fewer inference steps and computational resources compared to existing distillation techniques. Additionally, the framework is model agnostic, applicable to any type of diffusion models with suitable reward functions, providing a general optimization paradigm for efficient diffusion learning.

</details>


### [435] [Training AI Co-Scientists Using Rubric Rewards](https://arxiv.org/abs/2512.23707)
*Shashwat Goel,Rishi Hazra,Dulhan Jayalath,Timon Willi,Parag Jain,William F. Shen,Ilias Leontiadis,Francesco Barbieri,Yoram Bachrach,Jonas Geiping,Chenxi Whitehouse*

Main category: cs.LG

TL;DR: The paper focuses on training language models to generate research plans by using a scalable corpus of research goals extracted from papers, with finetuning through reinforcement learning and self-grading.


<details>
  <summary>Details</summary>
Motivation: To overcome current limitations of language models in generating constraint-following, meaningful research plans and enhance their capacity as AI co-scientists.

Method: The authors created a diverse training dataset by extracting research goals and grading rubrics from papers across domains. They trained models using reinforcement learning with a self-grading mechanism to improve plan generation without external supervision.

Result: The finetuned Qwen3-30B-A3B model was preferred by experts for 70% of research goals, and 84% of automatically extracted rubrics were approved. Their approach also showed 12-22% improvement and cross-domain generalization, including success in medical research.

Conclusion: This work presents an effective, scalable method to train language models for research plan generation, advancing the development of general AI co-scientists that can assist in multiple research domains.

Abstract: AI co-scientists are emerging as a tool to assist human researchers in achieving their research goals. A crucial feature of these AI co-scientists is the ability to generate a research plan given a set of aims and constraints. The plan may be used by researchers for brainstorming, or may even be implemented after further refinement. However, language models currently struggle to generate research plans that follow all constraints and implicit requirements. In this work, we study how to leverage the vast corpus of existing research papers to train language models that generate better research plans. We build a scalable, diverse training corpus by automatically extracting research goals and goal-specific grading rubrics from papers across several domains. We then train models for research plan generation via reinforcement learning with self-grading. A frozen copy of the initial policy acts as the grader during training, with the rubrics creating a generator-verifier gap that enables improvements without external human supervision. To validate this approach, we conduct a study with human experts for machine learning research goals, spanning 225 hours. The experts prefer plans generated by our finetuned Qwen3-30B-A3B model over the initial model for 70% of research goals, and approve 84% of the automatically extracted goal-specific grading rubrics. To assess generality, we also extend our approach to research goals from medical papers, and new arXiv preprints, evaluating with a jury of frontier models. Our finetuning yields 12-22% relative improvements and significant cross-domain generalization, proving effective even in problem settings like medical research where execution feedback is infeasible. Together, these findings demonstrate the potential of a scalable, automated training recipe as a step towards improving general AI co-scientists.

</details>


### [436] [MoR: Mixture Of Representations For Mixed-Precision Training](https://arxiv.org/abs/2512.22804)
*Bor-Yiing Su,Peter Dykas,Mike Chrzanowski,Jatin Chhugani*

Main category: cs.LG

TL;DR: This paper introduces Mixture-of-Representations (MoR), a new framework for dynamic quantization in deep learning, achieving high accuracy with FP8 representation.


<details>
  <summary>Details</summary>
Motivation: To develop a robust technique for mixed-precision training to enhance scalability of deep learning models.

Method: The paper proposes a framework that dynamically selects between FP8 and BF16 representations using a tensor's numerical properties, applied at both per-tensor and sub-tensor levels.

Result: The proposed framework can quantize 98.38% of tensors to FP8, achieving comparable accuracy to existing methods without requiring fine-grain partitioning.

Conclusion: Dynamic, property-aware quantization like MoR can make low precision training more robust and improve the potential of lower precision number formats.

Abstract: Mixed-precision training is a crucial technique for scaling deep learning models, but successful mixedprecision training requires identifying and applying the right combination of training methods. This paper presents our preliminary study on Mixture-of-Representations (MoR), a novel, per-tensor and sub-tensor level quantization framework that dynamically analyzes a tensor's numerical properties to select between a variety of different representations. Based on the framework, we have proposed and experimented concrete algorithms that choose dynamically between FP8 and BF16 representations for both per-tensor and sub-tensor level granularities. Our universal approach is designed to preserve model quality across various quantization partition strategies and datasets. Our initial findings show that this approach can achieve state-of-the-art results with 98.38% of tensors quantized to the FP8 format. This work highlights the potential of dynamic, property-aware quantization while preserving model quality. We believe this approach can generally improve the robustness of low precision training, as demonstrated by achieving FP8 accuracies that are on par with existing approaches without the need for fine-grain partitioning, or can be used in combination with other training methods to improve the leverage of even lower precision number formats such as NVFP4.

</details>


### [437] [Long-Range Distillation: Distilling 10,000 Years of Simulated Climate into Long Timestep AI Weather Models](https://arxiv.org/abs/2512.22814)
*Scott A. Martin,Noah Brenowitz,Dale Durran,Michael Pritchard*

Main category: cs.LG

TL;DR: The paper addresses challenges in AI-based long-range weather forecasting, introduces long-range distillation using synthetic datasets, and achieves promising results in forecast skill.


<details>
  <summary>Details</summary>
Motivation: Long-range weather forecasting is hindered by error accumulation in autoregressive models and the limited data availability from reanalysis datasets.

Method: A long-timestep probabilistic student model is trained using synthetic data generated by a short-timestep autoregressive teacher model, producing over 10,000 years of simulated climate for training.

Result: Distilled models outperform climatology, nearly match the teacher's skill, and achieve comparable real-world S2S forecast skill to ECMWF ensemble forecasts after fine-tuning.

Conclusion: Synthetic training data generated by AI can scale long-range forecast skill effectively, showcasing a new avenue for enhancing weather predictions over extended timescales.

Abstract: Accurate long-range weather forecasting remains a major challenge for AI models, both because errors accumulate over autoregressive rollouts and because reanalysis datasets used for training offer a limited sample of the slow modes of climate variability underpinning predictability. Most AI weather models are autoregressive, producing short lead forecasts that must be repeatedly applied to reach subseasonal-to-seasonal (S2S) or seasonal lead times, often resulting in instability and calibration issues. Long-timestep probabilistic models that generate long-range forecasts in a single step offer an attractive alternative, but training on the 40-year reanalysis record leads to overfitting, suggesting orders of magnitude more training data are required. We introduce long-range distillation, a method that trains a long-timestep probabilistic "student" model to forecast directly at long-range using a huge synthetic training dataset generated by a short-timestep autoregressive "teacher" model. Using the Deep Learning Earth System Model (DLESyM) as the teacher, we generate over 10,000 years of simulated climate to train distilled student models for forecasting across a range of timescales. In perfect-model experiments, the distilled models outperform climatology and approach the skill of their autoregressive teacher while replacing hundreds of autoregressive steps with a single timestep. In the real world, they achieve S2S forecast skill comparable to the ECMWF ensemble forecast after ERA5 fine-tuning. The skill of our distilled models scales with increasing synthetic training data, even when that data is orders of magnitude larger than ERA5. This represents the first demonstration that AI-generated synthetic training data can be used to scale long-range forecast skill.

</details>


### [438] [TEACH: Temporal Variance-Driven Curriculum for Reinforcement Learning](https://arxiv.org/abs/2512.22824)
*Gaurav Chaudhary,Laxmidhar Behera*

Main category: cs.LG

TL;DR: The paper introduces a novel Student-Teacher learning paradigm using Temporal Variance-Driven Curriculum to improve goal-conditioned reinforcement learning (RL).


<details>
  <summary>Details</summary>
Motivation: The paper addresses inefficiencies in goal-conditioned RL due to uniform goal selection and draws inspiration from adaptive learning mechanisms in biological systems.

Method: A dynamic teacher module prioritizes goals with the highest temporal variance in the policy's Q-values to create an adaptive curriculum for efficient learning.

Result: The proposed method demonstrated consistent and notable improvements on 11 robotic manipulation and maze navigation tasks, outperforming state-of-the-art methods.

Conclusion: The approach enhances RL efficiency by focusing on high-uncertainty goals and integrates seamlessly with existing algorithms.

Abstract: Reinforcement Learning (RL) has achieved significant success in solving single-goal tasks. However, uniform goal selection often results in sample inefficiency in multi-goal settings where agents must learn a universal goal-conditioned policy. Inspired by the adaptive and structured learning processes observed in biological systems, we propose a novel Student-Teacher learning paradigm with a Temporal Variance-Driven Curriculum to accelerate Goal-Conditioned RL. In this framework, the teacher module dynamically prioritizes goals with the highest temporal variance in the policy's confidence score, parameterized by the state-action value (Q) function. The teacher provides an adaptive and focused learning signal by targeting these high-uncertainty goals, fostering continual and efficient progress. We establish a theoretical connection between the temporal variance of Q-values and the evolution of the policy, providing insights into the method's underlying principles. Our approach is algorithm-agnostic and integrates seamlessly with existing RL frameworks. We demonstrate this through evaluation across 11 diverse robotic manipulation and maze navigation tasks. The results show consistent and notable improvements over state-of-the-art curriculum learning and goal-selection methods.

</details>


### [439] [Federated Multi-Task Clustering](https://arxiv.org/abs/2512.22897)
*S. Dai,G. Sun,F. Li,X. Tang,Q. Wang,Y. Cong*

Main category: cs.LG

TL;DR: The paper introduces FMTC, a novel Federated Multi-Task Clustering framework, addressing limitations in decentralized spectral clustering by leveraging personalized models and shared client structures in a privacy-preserving manner.


<details>
  <summary>Details</summary>
Motivation: Existing clustering models largely target centralized settings, inadequately handling decentralized environments. Additionally, federated learning methods often perform poorly due to unreliable pseudo-labels and fail to capture client heterogeneity.

Method: FMTC integrates a client-side personalized clustering module for robust predictions without pseudo-label reliance and a server-side tensorial correlation module using low-rank regularization to identify shared structures. An efficient distributed algorithm, based on ADMM, ensures privacy-preserving global optimization.

Result: Extensive real-world experimental evaluations show FMTC's superiority over baseline and state-of-the-art federated clustering algorithms, achieving robust performance across heterogeneous clients.

Conclusion: FMTC advances federated clustering by introducing privacy-preserving personalized models and effectively leveraging client-commonalities, overcoming traditional drawbacks in decentralized clustering tasks.

Abstract: Spectral clustering has emerged as one of the most effective clustering algorithms due to its superior performance. However, most existing models are designed for centralized settings, rendering them inapplicable in modern decentralized environments. Moreover, current federated learning approaches often suffer from poor generalization performance due to reliance on unreliable pseudo-labels, and fail to capture the latent correlations amongst heterogeneous clients. To tackle these limitations, this paper proposes a novel framework named Federated Multi-Task Clustering (i.e.,FMTC), which intends to learn personalized clustering models for heterogeneous clients while collaboratively leveraging their shared underlying structure in a privacy-preserving manner. More specifically, the FMTC framework is composed of two main components: client-side personalized clustering module, which learns a parameterized mapping model to support robust out-of-sample inference, bypassing the need for unreliable pseudo-labels; and server-side tensorial correlation module, which explicitly captures the shared knowledge across all clients. This is achieved by organizing all client models into a unified tensor and applying a low-rank regularization to discover their common subspace. To solve this joint optimization problem, we derive an efficient, privacy-preserving distributed algorithm based on the Alternating Direction Method of Multipliers, which decomposes the global problem into parallel local updates on clients and an aggregation step on the server. To the end, several extensive experiments on multiple real-world datasets demonstrate that our proposed FMTC framework significantly outperforms various baseline and state-of-the-art federated clustering algorithms.

</details>


### [440] [MetaCD: A Meta Learning Framework for Cognitive Diagnosis based on Continual Learning](https://arxiv.org/abs/2512.22904)
*Jin Wu,Chanjin Zheng*

Main category: cs.LG

TL;DR: The paper introduces MetaCD, a meta-learning framework combining continual learning to improve cognitive diagnosis models for overcoming long-tailed data issues and dynamic data changes.


<details>
  <summary>Details</summary>
Motivation: Current cognitive diagnosis methods are limited by challenges like long-tailed data distributions and dynamic changes, impeding their performance in personalized intelligent education.

Method: The authors propose MetaCD, which employs meta-learning for optimal initialization and uses a parameter protection mechanism for continual learning, enabling adaptation to new data and tasks.

Result: MetaCD demonstrates superior accuracy and generalization in tackling sequential tasks, outperforming existing approaches on five real-world datasets.

Conclusion: MetaCD enhances the adaptability, stability, and generalization of cognitive diagnosis models, addressing longstanding limitations in managing dynamic and unevenly distributed data.

Abstract: Cognitive diagnosis is an essential research topic in intelligent education, aimed at assessing the level of mastery of different skills by students. So far, many research works have used deep learning models to explore the complex interactions between students, questions, and skills. However, the performance of existing method is frequently limited by the long-tailed distribution and dynamic changes in the data. To address these challenges, we propose a meta-learning framework for cognitive diagnosis based on continual learning (MetaCD). This framework can alleviate the long-tailed problem by utilizing meta-learning to learn the optimal initialization state, enabling the model to achieve good accuracy on new tasks with only a small amount of data. In addition, we utilize a continual learning method named parameter protection mechanism to give MetaCD the ability to adapt to new skills or new tasks, in order to adapt to dynamic changes in data. MetaCD can not only improve the plasticity of our model on a single task, but also ensure the stability and generalization of the model on sequential tasks. Comprehensive experiments on five real-world datasets show that MetaCD outperforms other baselines in both accuracy and generalization.

</details>


### [441] [Sat-EnQ: Satisficing Ensembles of Weak Q-Learners for Reliable and Compute-Efficient Reinforcement Learning](https://arxiv.org/abs/2512.22910)
*Ünver Çiftçi*

Main category: cs.LG

TL;DR: Sat-EnQ introduces a two-phase framework for deep Q-learning to improve stability and robustness by first focusing on achieving 'good enough' results before optimizing.


<details>
  <summary>Details</summary>
Motivation: Deep Q-learning algorithms are unstable, especially early during training, due to amplified estimation errors from the maximization operator.

Method: The proposed Sat-EnQ framework consists of two phases: Phase 1 uses lightweight Q-networks optimized under a satisficing objective with a dynamic baseline to reduce overestimation. Phase 2 distills and fine-tunes the ensemble into a larger network with Double DQN.

Result: Sat-EnQ reduces variance by 3.8x, eliminates catastrophic failures (0% compared to 50% for DQN), and maintains good performance (79%) under noise while requiring 2.5x less compute than bootstrapped ensembles.

Conclusion: Adopting a satisficing approach before aggressive optimization can significantly enhance the stability and robustness of reinforcement learning systems.

Abstract: Deep Q-learning algorithms remain notoriously unstable, especially during early training when the maximization operator amplifies estimation errors. Inspired by bounded rationality theory and developmental learning, we introduce Sat-EnQ, a two-phase framework that first learns to be ``good enough'' before optimizing aggressively. In Phase 1, we train an ensemble of lightweight Q-networks under a satisficing objective that limits early value growth using a dynamic baseline, producing diverse, low-variance estimates while avoiding catastrophic overestimation. In Phase 2, the ensemble is distilled into a larger network and fine-tuned with standard Double DQN. We prove theoretically that satisficing induces bounded updates and cannot increase target variance, with a corollary quantifying conditions for substantial reduction. Empirically, Sat-EnQ achieves 3.8x variance reduction, eliminates catastrophic failures (0% vs 50% for DQN), maintains 79% performance under environmental noise}, and requires 2.5x less compute than bootstrapped ensembles. Our results highlight a principled path toward robust reinforcement learning by embracing satisficing before optimization.

</details>


### [442] [Multiple Token Divergence: Measuring and Steering In-Context Computation Density](https://arxiv.org/abs/2512.22944)
*Vincent Herrmann,Eric Alcaide,Michael Wand,Jürgen Schmidhuber*

Main category: cs.LG

TL;DR: The paper introduces Multiple Token Divergence (MTD), a lightweight tool to measure and steer computational effort in language models.


<details>
  <summary>Details</summary>
Motivation: Next-token loss fails to capture reasoning complexity in language models, leading to a need for better computational effort metrics.

Method: MTD utilizes KL divergence between a model's output distribution and a shallow auxiliary prediction head. It is computed directly from pre-trained models with multiple prediction heads, requiring no extra training.

Result: MTD distinguishes between complex and simple tasks effectively, correlates with mathematical reasoning problem difficulty, and lower MTD links to better reasoning accuracy.

Conclusion: MTD is a practical and efficient tool for analyzing and steering language models' computational dynamics.

Abstract: Measuring the in-context computational effort of language models is a key challenge, as metrics like next-token loss fail to capture reasoning complexity. Prior methods based on latent state compressibility can be invasive and unstable. We propose Multiple Token Divergence (MTD), a simple measure of computational effort defined as the KL divergence between a model's full output distribution and that of a shallow, auxiliary prediction head. MTD can be computed directly from pre-trained models with multiple prediction heads, requiring no additional training. Building on this, we introduce Divergence Steering, a novel decoding method to control the computational character of generated text. We empirically show that MTD is more effective than prior methods at distinguishing complex tasks from simple ones. On mathematical reasoning benchmarks, MTD correlates positively with problem difficulty. Lower MTD is associated with more accurate reasoning. MTD provides a practical, lightweight tool for analyzing and steering the computational dynamics of language models.

</details>


### [443] [APO: Alpha-Divergence Preference Optimization](https://arxiv.org/abs/2512.22953)
*Wang Zixian*

Main category: cs.LG

TL;DR: The paper introduces Alpha-Divergence Preference Optimization (APO), which balances between mode-covering and mode-seeking behaviors in alignment practice using alpha-divergence.


<details>
  <summary>Details</summary>
Motivation: Current alignment methods have limitations either by under-exploiting high-reward modes or risking mode collapse. A balanced approach that leverages the advantages of both forward and reverse KL behaviors is needed.

Method: APO uses Csiszar alpha-divergence within anchored geometry to interpolate between forward and reverse KL divergence behavior, with a reward-and-confidence-guarded alpha scheduling system.

Result: Experiments on Qwen3-1.7B show APO achieves competitive performance with GRPO and GSPO baselines while maintaining training stability.

Conclusion: APO offers a stable and flexible optimization framework by interpolating divergence regimes and safeguarding policy updates.

Abstract: Two divergence regimes dominate modern alignment practice. Supervised fine-tuning and many distillation-style objectives implicitly minimize the forward KL divergence KL(q || pi_theta), yielding stable mode-covering updates but often under-exploiting high-reward modes. In contrast, PPO-style online reinforcement learning from human feedback behaves closer to reverse KL divergence KL(pi_theta || q), enabling mode-seeking improvements but risking mode collapse. Recent anchored methods, such as ADPO, show that performing the projection in anchored coordinates can substantially improve stability, yet they typically commit to a single divergence. We introduce Alpha-Divergence Preference Optimization (APO), an anchored framework that uses Csiszar alpha-divergence to continuously interpolate between forward and reverse KL behavior within the same anchored geometry. We derive unified gradient dynamics parameterized by alpha, analyze gradient variance properties, and propose a practical reward-and-confidence-guarded alpha schedule that transitions from coverage to exploitation only when the policy is both improving and confidently calibrated. Experiments on Qwen3-1.7B with math-level3 demonstrate that APO achieves competitive performance with GRPO and GSPO baselines while maintaining training stability.

</details>


### [444] [FLOW: A Feedback-Driven Synthetic Longitudinal Dataset of Work and Wellbeing](https://arxiv.org/abs/2512.22956)
*Wafaa El Husseini*

Main category: cs.LG

TL;DR: This paper introduces FLOW, a synthetic dataset modeling work-life interactions like workload, lifestyle behaviors, and wellbeing. It simulates 1,000 individuals' data over two years.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from limited access to real longitudinal, individual-level data on work-life balance and wellbeing due to privacy, ethical, and logistical issues. This limits research, benchmarking, and education in stress modeling, behavioral analysis, and machine learning.

Method: FLOW is generated using a rule-based, feedback-driven simulation to create coherent temporal interactions in variables like stress, sleep, mood, physical activity, and body weight over a two-year period.

Result: The result is a public synthetic longitudinal dataset and a configurable data generation tool to enable reproducible experimentation in work-life behavioral research.

Conclusion: FLOW provides a controlled experimental environment to support research and development in domains where real-world data is inaccessible, rather than directly substituting it.

Abstract: Access to longitudinal, individual-level data on work-life balance and wellbeing is limited by privacy, ethical, and logistical constraints. This poses challenges for reproducible research, methodological benchmarking, and education in domains such as stress modeling, behavioral analysis, and machine learning.
  We introduce FLOW, a synthetic longitudinal dataset designed to model daily interactions between workload, lifestyle behaviors, and wellbeing. FLOW is generated using a rule-based, feedback-driven simulation that produces coherent temporal dynamics across variables such as stress, sleep, mood, physical activity, and body weight. The dataset simulates 1{,}000 individuals over a two-year period with daily resolution and is released as a publicly available resource.
  In addition to the static dataset, we describe a configurable data generation tool that enables reproducible experimentation under adjustable behavioral and contextual assumptions. FLOW is intended as a controlled experimental environment rather than a proxy for observed human populations, supporting exploratory analysis, methodological development, and benchmarking where real-world data are inaccessible.

</details>


### [445] [A Context-Aware Temporal Modeling through Unified Multi-Scale Temporal Encoding and Hierarchical Sequence Learning for Single-Channel EEG Sleep Staging](https://arxiv.org/abs/2512.22976)
*Amirali Vakili,Salar Jahanshiri,Armin Salimi-Badr*

Main category: cs.LG

TL;DR: This paper addresses sleep staging using single-channel EEG signals and improves the challenging N1 stage classification through a context-aware and interpretable framework.


<details>
  <summary>Details</summary>
Motivation: The study aims to tackle issues in sleep staging, such as class imbalance, limited receptive-field modeling, and lack of interpretability in existing models, to enhance healthcare applications.

Method: The model employs compact multi-scale feature extraction, temporal modeling, class-weighted loss function, and sub-epoch chunk segmentation alongside softmax probability averaging for robust predictions.

Result: Achieved 89.72% accuracy, 85.46% macro-average F1-score, and a significant 61.7% F1-score for N1 stage on SleepEDF datasets.

Conclusion: The approach effectively improves sleep staging accuracy, particularly for N1 stages, while being interpretable and clinically applicable.

Abstract: Automatic sleep staging is a critical task in healthcare due to the global prevalence of sleep disorders. This study focuses on single-channel electroencephalography (EEG), a practical and widely available signal for automatic sleep staging. Existing approaches face challenges such as class imbalance, limited receptive-field modeling, and insufficient interpretability. This work proposes a context-aware and interpretable framework for single-channel EEG sleep staging, with particular emphasis on improving detection of the N1 stage. Many prior models operate as black boxes with stacked layers, lacking clearly defined and interpretable feature extraction roles.The proposed model combines compact multi-scale feature extraction with temporal modeling to capture both local and long-range dependencies. To address data imbalance, especially in the N1 stage, classweighted loss functions and data augmentation are applied. EEG signals are segmented into sub-epoch chunks, and final predictions are obtained by averaging softmax probabilities across chunks, enhancing contextual representation and robustness.The proposed framework achieves an overall accuracy of 89.72% and a macro-average F1-score of 85.46%. Notably, it attains an F1- score of 61.7% for the challenging N1 stage, demonstrating a substantial improvement over previous methods on the SleepEDF datasets. These results indicate that the proposed approach effectively improves sleep staging performance while maintaining interpretability and suitability for real-world clinical applications.

</details>


### [446] [Fusion or Confusion? Multimodal Complexity Is Not All You Need](https://arxiv.org/abs/2512.22991)
*Tillmann Rheude,Roland Eils,Benjamin Wild*

Main category: cs.LG

TL;DR: The paper critiques the assumption that complex multimodal learning models perform better, proposing a simpler baseline model (SimBaMM) that shows comparable results.


<details>
  <summary>Details</summary>
Motivation: To assess whether the increasing complexity of multimodal learning architectures is justified or if simpler models can achieve comparable performance.

Method: Conducted a large-scale study reimplementing 19 methods across 9 datasets with rigorous evaluation and proposed a late-fusion Transformer model, SimBaMM.

Result: SimBaMM performs comparably to more complex multimodal methods and often matches unimodal baselines, especially in low-data scenarios.

Conclusion: Complex architectures in multimodal learning often do not outperform simpler models under rigorous evaluation, emphasizing the need for methodological rigor over architectural novelty.

Abstract: Deep learning architectures for multimodal learning have increased in complexity, driven by the assumption that multimodal-specific methods improve performance. We challenge this assumption through a large-scale empirical study reimplementing 19 high-impact methods under standardized conditions, evaluating them across nine diverse datasets with up to 23 modalities, and testing their generalizability to new tasks beyond their original scope, including settings with missing modalities. We propose a Simple Baseline for Multimodal Learning (SimBaMM), a straightforward late-fusion Transformer architecture, and demonstrate that under standardized experimental conditions with rigorous hyperparameter tuning of all methods, more complex architectures do not reliably outperform SimBaMM. Statistical analysis indicates that more complex methods perform comparably to SimBaMM and frequently do not reliably outperform well-tuned unimodal baselines, especially in the small-data regime considered in many original studies. To support our findings, we include a case study of a recent multimodal learning method highlighting the methodological shortcomings in the literature. In addition, we provide a pragmatic reliability checklist to promote comparable, robust, and trustworthy future evaluations. In summary, we argue for a shift in focus: away from the pursuit of architectural novelty and toward methodological rigor.

</details>


### [447] [Merge before Forget: A Single LoRA Continual Learning via Continual Merging](https://arxiv.org/abs/2512.23017)
*Fuli Qiao,Mehrdad Mahdavi*

Main category: cs.LG

TL;DR: This paper introduces a new continual learning method for LoRAs in LLMs that reduces memory and interference issues while improving performance.


<details>
  <summary>Details</summary>
Motivation: Current LoRA continual learning techniques face challenges like growing computational memory and task interference due to lack of effective merging mechanisms.

Method: The approach orthogonally initializes LoRA updates and sequentially merges them into a unified LoRA using orthogonal basis extraction and time-aware scaling mechanisms.

Result: The method achieves constant memory complexity, minimizes task interference, and improves performance as shown through theoretical analysis and experiments on diverse benchmarks.

Conclusion: The paper's novel approach enhances efficiency and adaptability for continual learning in large language models, addressing key limitations of existing methods.

Abstract: Parameter-efficient continual learning has emerged as a promising approach for large language models (LLMs) to mitigate catastrophic forgetting while enabling adaptation to new tasks. Current Low-Rank Adaptation (LoRA) continual learning techniques often retain and freeze previously learned LoRAs or generate data representations to overcome forgetting, typically utilizing these to support new LoRAs learn new tasks. However, these methods not only ignore growing computational memory with tasks and limited storage space but also suffer from potential task interference due to the lack of effective LoRA merging mechanisms. In this paper, we propose a novel continual learning method that orthogonally initializes and sequentially merges LoRAs updates into a single unified LoRA. Our method leverages orthogonal basis extraction from previously learned LoRA to initialize the learning of new tasks, further exploits the intrinsic asymmetry property of LoRA components by using a time-aware scaling mechanism to balance new and old knowledge during continual merging. Our approach maintains constant memory complexity with respect to the number of tasks, minimizes interference between past and new tasks via orthogonal basis initialization, and improves performance over asymmetric LoRA merging via adaptive scaling. We provide theoretical analysis to justify our design and conduct extensive experiments across diverse continual learning benchmarks using various Llama models, demonstrating the effectiveness and efficiency of our method.

</details>


### [448] [Mechanistic Analysis of Circuit Preservation in Federated Learning](https://arxiv.org/abs/2512.23043)
*Muhammad Haseeb,Salaar Masood,Muhammad Abdullah Sohail*

Main category: cs.LG

TL;DR: The paper investigates why Federated Learning struggles under Non-IID data, revealing that conflicting client updates lead to destructive interference in neural network components responsible for class prediction.


<details>
  <summary>Details</summary>
Motivation: To understand the mechanistic reasons behind the performance degradation of Federated Learning under Non-IID data conditions.

Method: The study uses Mechanistic Interpretability and weight-sparse neural networks to track and analyze class prediction circuits in FedAvg across clients and communication rounds.

Result: Findings show that Non-IID data causes local circuits to structurally diverge and degrade, which affects the global model’s performance.

Conclusion: This paper reframes statistical drift in Federated Learning as a mechanistic failure of circuit preservation, suggesting a need for targeted interventions to address this issue.

Abstract: Federated Learning (FL) enables collaborative training of models on decentralized data, but its performance degrades significantly under Non-IID (non-independent and identically distributed) data conditions. While this accuracy loss is well-documented, the internal mechanistic causes remain a black box. This paper investigates the canonical FedAvg algorithm through the lens of Mechanistic Interpretability (MI) to diagnose this failure mode. We hypothesize that the aggregation of conflicting client updates leads to circuit collapse, the destructive interference of functional, sparse sub-networks responsible for specific class predictions. By training inherently interpretable, weight-sparse neural networks within an FL framework, we identify and track these circuits across clients and communication rounds. Using Intersection-over-Union (IoU) to quantify circuit preservation, we provide the first mechanistic evidence that Non-IID data distributions cause structurally distinct local circuits to diverge, leading to their degradation in the global model. Our findings reframe the problem of statistical drift in FL as a concrete, observable failure of mechanistic preservation, paving the way for more targeted solutions.

</details>


### [449] [PI-MFM: Physics-informed multimodal foundation model for solving partial differential equations](https://arxiv.org/abs/2512.23056)
*Min Zhu,Jingmin Sun,Zecheng Zhang,Hayden Schaeffer,Lu Lu*

Main category: cs.LG

TL;DR: The paper introduces a physics-informed multimodal foundation model (PI-MFM) that enforces governing PDE equations during pretraining and adaptation, improving efficiency and robustness in solving diverse PDE families.


<details>
  <summary>Details</summary>
Motivation: Existing multi-operator learning methods for solving PDEs are data-intensive and fail to incorporate physics-informed training, motivating the need for a framework that is both data-efficient and physics-compliant.

Method: The paper proposes PI-MFM, a framework that uses symbolic PDE representations to compute governing equation residuals vectorially, enforcing a unified physics-informed objective during training or adaptation across different PDE families.

Result: PI-MFM outperforms standard data-driven methods across various benchmarks, particularly with sparse labeled data or noisy conditions. It also enables effective zero-shot fine-tuning to solve new PDE families with minimal labeled data requirements.

Conclusion: PI-MFM is a scalable, data-efficient, and transferable approach for solving PDEs, offering substantial benefits over purely data-driven or physics-only models.

Abstract: Partial differential equations (PDEs) govern a wide range of physical systems, and recent multimodal foundation models have shown promise for learning PDE solution operators across diverse equation families. However, existing multi-operator learning approaches are data-hungry and neglect physics during training. Here, we propose a physics-informed multimodal foundation model (PI-MFM) framework that directly enforces governing equations during pretraining and adaptation. PI-MFM takes symbolic representations of PDEs as the input, and automatically assembles PDE residual losses from the input expression via a vectorized derivative computation. These designs enable any PDE-encoding multimodal foundation model to be trained or adapted with unified physics-informed objectives across equation families. On a benchmark of 13 parametric one-dimensional time-dependent PDE families, PI-MFM consistently outperforms purely data-driven counterparts, especially with sparse labeled spatiotemporal points, partially observed time domains, or few labeled function pairs. Physics losses further improve robustness against noise, and simple strategies such as resampling collocation points substantially improve accuracy. We also analyze the accuracy, precision, and computational cost of automatic differentiation and finite differences for derivative computation within PI-MFM. Finally, we demonstrate zero-shot physics-informed fine-tuning to unseen PDE families: starting from a physics-informed pretrained model, adapting using only PDE residuals and initial/boundary conditions, without any labeled solution data, rapidly reduces test errors to around 1% and clearly outperforms physics-only training from scratch. These results show that PI-MFM provides a practical and scalable path toward data-efficient, transferable PDE solvers.

</details>


### [450] [Breaking the Memory Wall: Exact Analytical Differentiation via Tiled Operator-Space Evolution](https://arxiv.org/abs/2512.23068)
*Shuhuan Wang,Yuzhen Xie,Jiayi Li,Yinliang Diao*

Main category: cs.LG

TL;DR: The paper introduces Phase Gradient Flow (PGF), a framework for memory-efficient, exact gradient-based analysis in Selective State Space Models, enabling genomic-scale modeling on consumer-grade hardware.


<details>
  <summary>Details</summary>
Motivation: Current Selective State Space Models (SSMs) face memory bottlenecks during backpropagation, limiting their scalability for genomic-scale modeling on consumer hardware.

Method: The authors propose PGF, which computes exact analytical derivatives by directly operating within the state-space manifold and utilizing Tiled Operator-Space Evolution (TOSE) dynamics to achieve O(1) memory complexity regardless of sequence length.

Result: PGF achieves a 94% reduction in peak VRAM, a 23x increase in throughput, and maintains stability with near-machine precision, even for sequences as large as 128,000 steps, avoiding out-of-memory errors.

Conclusion: PGF overcomes hardware memory limits, enabling chromosome-scale sensitivity analysis on single GPUs and making infinite-context modeling feasible on practical devices.

Abstract: Selective State Space Models (SSMs) achieve linear-time inference, yet their gradient-based sensitivity analysis remains bottlenecked by O(L) memory scaling during backpropagation. This memory constraint precludes genomic-scale modeling (L > 10^5) on consumer-grade hardware. We introduce Phase Gradient Flow (PGF), a framework that computes exact analytical derivatives by operating directly in the state-space manifold, bypassing the need to materialize the intermediate computational graph. By reframing SSM dynamics as Tiled Operator-Space Evolution (TOSE), our method delivers O(1) memory complexity relative to sequence length, yielding a 94% reduction in peak VRAM and a 23x increase in throughput compared to standard Autograd. Unlike parallel prefix scans that exhibit numerical divergence in stiff ODE regimes, PGF ensures stability through invariant error scaling, maintaining near-machine precision across extreme sequences. We demonstrate the utility of PGF on an impulse-response benchmark with 128,000-step sequences - a scale where conventional Autograd encounters prohibitive memory overhead, often leading to out-of-memory (OOM) failures in multi-layered models. Our work enables chromosome-scale sensitivity analysis on a single GPU, bridging the gap between theoretical infinite-context models and practical hardware limitations.

</details>


### [451] [FLEX-MoE: Federated Mixture-of-Experts with Load-balanced Expert Assignment](https://arxiv.org/abs/2512.23070)
*Boyang Zhang,Xiaobing Chen,Songyang Zhang,Shuai Zhang,Xiangwei Zhou,Mingxuan Sun*

Main category: cs.LG

TL;DR: This paper presents FLEX-MoE, a federated mixture-of-experts framework that improves expert assignment and load balancing in constrained client settings.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiencies and performance degradation in deploying mixture-of-experts (MoE) models in federated learning, particularly due to constrained client resources and non-IID data distributions.

Method: FLEX-MoE introduces client-expert fitness scores to optimize client-expert assignment based on training feedback and uses an algorithm to enhance expert specialization while maintaining balanced system-wide utilization.

Result: Experiments on three datasets demonstrate FLEX-MoE's superior performance and its ability to ensure balanced expert usage across varied resource-limited scenarios.

Conclusion: FLEX-MoE effectively tackles challenges in federated MoE deployment, achieving better performance and system-wide balance compared to existing methods.

Abstract: Mixture-of-Experts (MoE) models enable scalable neural networks through conditional computation. However, their deployment with federated learning (FL) faces two critical challenges: 1) resource-constrained edge devices cannot store full expert sets, and 2) non-IID data distributions cause severe expert load imbalance that degrades model performance. To this end, we propose \textbf{FLEX-MoE}, a novel federated MoE framework that jointly optimizes expert assignment and load balancing under limited client capacity. Specifically, our approach introduces client-expert fitness scores that quantify the expert suitability for local datasets through training feedback, and employs an optimization-based algorithm to maximize client-expert specialization while enforcing balanced expert utilization system-wide. Unlike existing greedy methods that focus solely on personalization while ignoring load imbalance, our FLEX-MoE is capable of addressing the expert utilization skew, which is particularly severe in FL settings with heterogeneous data. Our comprehensive experiments on three different datasets demonstrate the superior performance of the proposed FLEX-MoE, together with its ability to maintain balanced expert utilization across diverse resource-constrained scenarios.

</details>


### [452] [Rethinking Fine-Tuning: Unlocking Hidden Capabilities in Vision-Language Models](https://arxiv.org/abs/2512.23073)
*Mingyuan Zhang,Yue Bai,Yifan Wang,Yiyang Huang,Yun Fu*

Main category: cs.LG

TL;DR: The paper explores Mask Fine-Tuning (MFT) as an alternative method for adapting Vision-Language Models (VLMs), showing its superiority over traditional fine-tuning and LoRA by reorganizing internal knowledge without weight updates.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the limitations of current fine-tuning methods for VLMs which rely heavily on explicit weight updates and overlook the representational structures already embedded in pre-trained models.

Method: The authors apply Mask Fine-Tuning (MFT) to the language and projector components of VLMs. Instead of updating model weights, MFT assigns learnable gating scores to each weight, facilitating internal network reorganization for better adaptation.

Result: MFT outperforms PEFT approaches like LoRA and even surpasses full fine-tuning in various benchmarks, achieving high performance without altering the frozen backbone of the model.

Conclusion: The study highlights that adaptation in VLMs can be effectively achieved by reorganizing internal connections rather than modifying weights, offering an efficient and performant alternative to traditional fine-tuning.

Abstract: Explorations in fine-tuning Vision-Language Models (VLMs), such as Low-Rank Adaptation (LoRA) from Parameter Efficient Fine-Tuning (PEFT), have made impressive progress. However, most approaches rely on explicit weight updates, overlooking the extensive representational structures already encoded in pre-trained models that remain underutilized. Recent works have demonstrated that Mask Fine-Tuning (MFT) can be a powerful and efficient post-training paradigm for language models. Instead of updating weights, MFT assigns learnable gating scores to each weight, allowing the model to reorganize its internal subnetworks for downstream task adaptation. In this paper, we rethink fine-tuning for VLMs from a structural reparameterization perspective grounded in MFT. We apply MFT to the language and projector components of VLMs with different language backbones and compare against strong PEFT baselines. Experiments show that MFT consistently surpasses LoRA variants and even full fine-tuning, achieving high performance without altering the frozen backbone. Our findings reveal that effective adaptation can emerge not only from updating weights but also from reestablishing connections among the model's existing knowledge. Code available at: https://github.com/Ming-K9/MFT-VLM

</details>


### [453] [Multimodal Functional Maximum Correlation for Emotion Recognition](https://arxiv.org/abs/2512.23076)
*Deyang Zheng,Tianyi Zhang,Wenming Zheng,Shujian Yu*

Main category: cs.LG

TL;DR: This paper introduces MFMC, a self-supervised learning framework for multimodal affective computing, focused on capturing higher-order joint interactions among modalities. It outperforms state-of-the-art methods across various benchmarks.


<details>
  <summary>Details</summary>
Motivation: The difficulty of multimodal learning and the scarcity of affective annotations necessitate a robust framework that captures complex interdependencies across physiological responses in affective computing.

Method: MFMC utilizes a Dual Total Correlation objective and functional maximum correlation analysis to capture higher-order multimodal dependencies, avoiding pairwise contrastive losses.

Result: MFMC shows state-of-the-art or competitive results across multiple datasets, improving accuracies in subject-dependent and subject-independent evaluations.

Conclusion: MFMC demonstrates robustness and efficiency in representing multimodal physiological interactions, offering significant improvement in affective computing benchmarking results.

Abstract: Emotional states manifest as coordinated yet heterogeneous physiological responses across central and autonomic systems, posing a fundamental challenge for multimodal representation learning in affective computing. Learning such joint dynamics is further complicated by the scarcity and subjectivity of affective annotations, which motivates the use of self-supervised learning (SSL). However, most existing SSL approaches rely on pairwise alignment objectives, which are insufficient to characterize dependencies among more than two modalities and fail to capture higher-order interactions arising from coordinated brain and autonomic responses.
  To address this limitation, we propose Multimodal Functional Maximum Correlation (MFMC), a principled SSL framework that maximizes higher-order multimodal dependence through a Dual Total Correlation (DTC) objective. By deriving a tight sandwich bound and optimizing it using a functional maximum correlation analysis (FMCA) based trace surrogate, MFMC captures joint multimodal interactions directly, without relying on pairwise contrastive losses.
  Experiments on three public affective computing benchmarks demonstrate that MFMC consistently achieves state-of-the-art or competitive performance under both subject-dependent and subject-independent evaluation protocols, highlighting its robustness to inter-subject variability. In particular, MFMC improves subject-dependent accuracy on CEAP-360VR from 78.9% to 86.8%, and subject-independent accuracy from 27.5% to 33.1% using the EDA signal alone. Moreover, MFMC remains within 0.8 percentage points of the best-performing method on the most challenging EEG subject-independent split of MAHNOB-HCI. Our code is available at https://github.com/DY9910/MFMC.

</details>


### [454] [SE-MLP Model for Predicting Prior Acceleration Features in Penetration Signals](https://arxiv.org/abs/2512.23131)
*Yankang Li,Changsheng Li*

Main category: cs.LG

TL;DR: The paper introduces SE-MLP, a specialized neural network for rapid prediction of penetration acceleration features, outperforming traditional methods in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of long simulation cycles and high computational costs in obtaining penetration acceleration feature values.

Method: The authors propose SE-MLP, a multi-layer perceptron model enhanced with a channel attention mechanism and residual connections, to predict penetration acceleration features using physical input parameters.

Result: SE-MLP outperforms traditional MLP, XGBoost, and Transformer models in accuracy, generalization, and stability, validated through comparative experiments, ablation studies, numerical simulations, and recovery tests.

Conclusion: The SE-MLP model is validated as both feasible and practical for generating prior feature values quickly and accurately for penetration fuzes, with results within acceptable engineering tolerances.

Abstract: Accurate identification of the penetration process relies heavily on prior feature values of penetration acceleration. However, these feature values are typically obtained through long simulation cycles and expensive computations. To overcome this limitation, this paper proposes a multi-layer Perceptron architecture, termed squeeze and excitation multi-layer perceptron (SE-MLP), which integrates a channel attention mechanism with residual connections to enable rapid prediction of acceleration feature values. Using physical parameters under different working conditions as inputs, the model outputs layer-wise acceleration features, thereby establishing a nonlinear mapping between physical parameters and penetration characteristics. Comparative experiments against conventional MLP, XGBoost, and Transformer models demonstrate that SE-MLP achieves superior prediction accuracy, generalization, and stability. Ablation studies further confirm that both the channel attention module and residual structure contribute significantly to performance gains. Numerical simulations and range recovery tests show that the discrepancies between predicted and measured acceleration peaks and pulse widths remain within acceptable engineering tolerances. These results validate the feasibility and engineering applicability of the proposed method and provide a practical basis for rapidly generating prior feature values for penetration fuzes.

</details>


### [455] [A Weak Signal Learning Dataset and Its Baseline Method](https://arxiv.org/abs/2512.23160)
*Xianqi Liu,Xiangru Li,Lefeng He,Ziyu Fang*

Main category: cs.LG

TL;DR: The paper focuses on addressing challenges in Weak Signal Learning (WSL) by creating a dedicated dataset and proposing a PDVFN model to improve weak signal feature extraction amidst noise, imbalances, and low SNR data.


<details>
  <summary>Details</summary>
Motivation: The motivation is to solve issues prevalent in weak signal learning tasks, where critical information is obscured by noise, and feature identification is challenging. Current research is limited by the lack of specialized datasets.

Method: The authors constructed a dataset with 13,158 spectral samples characterized by low SNR dominance and extreme class imbalance. They introduced the PDVFN model, which employs dual-view representations (vector and time-frequency maps) and strategies like local enhancement and multi-scale frequency extraction, designed for low SNR and imbalances.

Result: The proposed dataset and the PDVFN model demonstrated superior accuracy and robustness in experiments, particularly under conditions of weak signals, high noise, and class imbalance.

Conclusion: This work establishes a novel benchmark for weak signal learning, providing both a dedicated dataset and a high-performance model, setting the stage for advancing research in this field.

Abstract: Weak signal learning (WSL) is a common challenge in many fields like fault diagnosis, medical imaging, and autonomous driving, where critical information is often masked by noise and interference, making feature identification difficult. Even in tasks with abundant strong signals, the key to improving model performance often lies in effectively extracting weak signals. However, the lack of dedicated datasets has long constrained research. To address this, we construct the first specialized dataset for weak signal feature learning, containing 13,158 spectral samples. It features low SNR dominance (over 55% samples with SNR below 50) and extreme class imbalance (class ratio up to 29:1), providing a challenging benchmark for classification and regression in weak signal scenarios. We also propose a dual-view representation (vector + time-frequency map) and a PDVFN model tailored to low SNR, distribution skew, and dual imbalance. PDVFN extracts local sequential features and global frequency-domain structures in parallel, following principles of local enhancement, sequential modeling, noise suppression, multi-scale capture, frequency extraction, and global perception. This multi-source complementarity enhances representation for low-SNR and imbalanced data, offering a novel solution for WSL tasks like astronomical spectroscopy. Experiments show our method achieves higher accuracy and robustness in handling weak signals, high noise, and extreme class imbalance, especially in low SNR and imbalanced scenarios. This study provides a dedicated dataset, a baseline model, and establishes a foundation for future WSL research.

</details>


### [456] [Diffusion-based Decentralized Federated Multi-Task Representation Learning](https://arxiv.org/abs/2512.23161)
*Donghwa Kang,Shana Moothedath*

Main category: cs.LG

TL;DR: This paper introduces a decentralized projected gradient descent-based algorithm for multi-task representation learning, specifically for multi-task linear regression, focusing on efficient and provable performance guarantees.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work is to address the gap in decentralized multi-task representation learning approaches, particularly in scenarios where tasks share a common, low-dimensional representation, to improve learning efficiency and performance.

Method: The method involves developing an alternating projected gradient descent and minimization algorithm. The approach recovers a low-rank feature matrix in a decentralized, diffusion-based, federated framework while providing provable performance guarantees.

Result: The algorithm achieved lower bounds on the sample complexity and upper bounds on iteration complexity. Analysis showed that it is both time and communication-efficient, as validated through numerical simulations that compared its performance with benchmark algorithms.

Conclusion: The study concludes that the proposed algorithm is effective and efficient for decentralized multi-task representation learning, addressing scalability and communication challenges while providing theoretical guarantees for performance.

Abstract: Representation learning is a widely adopted framework for learning in data-scarce environments to obtain a feature extractor or representation from various different yet related tasks. Despite extensive research on representation learning, decentralized approaches remain relatively underexplored. This work develops a decentralized projected gradient descent-based algorithm for multi-task representation learning. We focus on the problem of multi-task linear regression in which multiple linear regression models share a common, low-dimensional linear representation. We present an alternating projected gradient descent and minimization algorithm for recovering a low-rank feature matrix in a diffusion-based decentralized and federated fashion. We obtain constructive, provable guarantees that provide a lower bound on the required sample complexity and an upper bound on the iteration complexity of our proposed algorithm. We analyze the time and communication complexity of our algorithm and show that it is fast and communication-efficient. We performed numerical simulations to validate the performance of our algorithm and compared it with benchmark algorithms.

</details>


### [457] [Evaluating Parameter Efficient Methods for RLVR](https://arxiv.org/abs/2512.23165)
*Qingyu Yin,Yulun Wu,Zhennan Shen,Sunbowen Li,Zhilin Wang,Yanshu Li,Chak Tou Leong,Jiale Kang,Jinjin Gu*

Main category: cs.LG

TL;DR: This paper evaluates 12 different Parameter-Efficient Fine-Tuning (PEFT) methods for language models in Reinforcement Learning with Verifiable Rewards (RLVR), aiming to improve reasoning capabilities and identifies shortcomings of the commonly adopted LoRA method.


<details>
  <summary>Details</summary>
Motivation: The motivation comes from the need to identify the optimal PEFT approach for RLVR to enhance reasoning capabilities in language models and address limitations in commonly used methods like LoRA.

Method: The authors systematically evaluated over 12 PEFT methods (e.g., LoRA, DoRA, AdaLoRA, and others) on mathematical reasoning benchmarks, performing ablation studies and scaling experiments to assess their efficacy.

Result: The study found that structural variants like DoRA, AdaLoRA, and MiSS outperform LoRA, highlighted issues like spectral collapse in SVD-informed methods, and showed that excessive parameter reduction impacts reasoning capabilities.

Conclusion: The findings advocate for alternative PEFT approaches to LoRA, encouraging exploration into more suitable parameter-efficient RL methods for enhancing reasoning in language models.

Abstract: We systematically evaluate Parameter-Efficient Fine-Tuning (PEFT) methods under the paradigm of Reinforcement Learning with Verifiable Rewards (RLVR). RLVR incentivizes language models to enhance their reasoning capabilities through verifiable feedback; however, while methods like LoRA are commonly used, the optimal PEFT architecture for RLVR remains unidentified. In this work, we conduct the first comprehensive evaluation of over 12 PEFT methodologies across the DeepSeek-R1-Distill families on mathematical reasoning benchmarks. Our empirical results challenge the default adoption of standard LoRA with three main findings. First, we demonstrate that structural variants, such as DoRA, AdaLoRA, and MiSS, consistently outperform LoRA. Second, we uncover a spectral collapse phenomenon in SVD-informed initialization strategies (\textit{e.g.,} PiSSA, MiLoRA), attributing their failure to a fundamental misalignment between principal-component updates and RL optimization. Furthermore, our ablations reveal that extreme parameter reduction (\textit{e.g.,} VeRA, Rank-1) severely bottlenecks reasoning capacity. We further conduct ablation studies and scaling experiments to validate our findings. This work provides a definitive guide for advocating for more exploration for parameter-efficient RL methods.

</details>


### [458] [HELM-BERT: A Transformer for Medium-sized Peptide Property Prediction](https://arxiv.org/abs/2512.23175)
*Seungeon Lee,Takuto Koyama,Itsuki Maeda,Shigeyuki Matsumoto,Yasushi Okuno*

Main category: cs.LG

TL;DR: The paper introduces HELM-BERT, a new peptide language model based on HELM notation, to overcome challenges in modeling therapeutic peptides, outperforming previous SMILES-based methods.


<details>
  <summary>Details</summary>
Motivation: Existing molecular language models struggle with accurate peptide representation due to limitations in SMILES and amino-acid level notations, inhibiting peptide development.

Method: The methodology involves using HELM notation for hierarchical peptide representation and pre-training HELM-BERT, an encoder-based model, on chemically diverse peptide datasets.

Result: HELM-BERT surpasses SMILES-based models in key tasks like cyclic peptide membrane permeability and peptide-protein interaction prediction.

Conclusion: HELM-BERT demonstrates the advantage of HELM-based representation in efficiently modeling therapeutic peptides, bridging the gap between small molecules and proteins.

Abstract: Therapeutic peptides have emerged as a pivotal modality in modern drug discovery, occupying a chemically and topologically rich space. While accurate prediction of their physicochemical properties is essential for accelerating peptide development, existing molecular language models rely on representations that fail to capture this complexity. Atom-level SMILES notation generates long token sequences and obscures cyclic topology, whereas amino-acid-level representations cannot encode the diverse chemical modifications central to modern peptide design. To bridge this representational gap, the Hierarchical Editing Language for Macromolecules (HELM) offers a unified framework enabling precise description of both monomer composition and connectivity, making it a promising foundation for peptide language modeling. Here, we propose HELM-BERT, the first encoder-based peptide language model trained on HELM notation. Based on DeBERTa, HELM-BERT is specifically designed to capture hierarchical dependencies within HELM sequences. The model is pre-trained on a curated corpus of 39,079 chemically diverse peptides spanning linear and cyclic structures. HELM-BERT significantly outperforms state-of-the-art SMILES-based language models in downstream tasks, including cyclic peptide membrane permeability prediction and peptide-protein interaction prediction. These results demonstrate that HELM's explicit monomer- and topology-aware representations offer substantial data-efficiency advantages for modeling therapeutic peptides, bridging a long-standing gap between small-molecule and protein language models.

</details>


### [459] [Machine Learning-Assisted Vocal Cord Ultrasound Examination: Project VIPR](https://arxiv.org/abs/2512.23177)
*Will Sebelik-Lassiter,Evan Schubert,Muhammad Alliyu,Quentin Robbins,Excel Olatunji,Mustafa Barry*

Main category: cs.LG

TL;DR: The study develops machine learning models for vocal cord ultrasound analysis, achieving high accuracy in identifying vocal cords and classifying vocal cord paralysis.


<details>
  <summary>Details</summary>
Motivation: To improve the diagnostic accuracy of vocal cord ultrasound, which is often operator-dependent, through automated analysis using machine learning.

Method: VCUS videos were collected from 30 volunteers, frames cropped uniformly, and used for training segmentation and classification models for identifying vocal cords and distinguishing normal vs paralyzed states.

Result: The segmentation model achieved 96% accuracy, while the classification model achieved 99% validation accuracy.

Conclusion: Machine learning greatly enhances the diagnostic capabilities of vocal cord ultrasound, surpassing traditional operator-dependent methods.

Abstract: Intro: Vocal cord ultrasound (VCUS) has emerged as a less invasive and better tolerated examination technique, but its accuracy is operator dependent. This research aims to apply a machine learning-assisted algorithm to automatically identify the vocal cords and distinguish normal vocal cord images from vocal cord paralysis (VCP). Methods: VCUS videos were acquired from 30 volunteers, which were split into still frames and cropped to a uniform size. Healthy and simulated VCP images were used as training data for vocal cord segmentation and VCP classification models. Results: The vocal cord segmentation model achieved a validation accuracy of 96%, while the best classification model (VIPRnet) achieved a validation accuracy of 99%. Conclusion: Machine learning-assisted analysis of VCUS shows great promise in improving diagnostic accuracy over operator-dependent human interpretation.

</details>


### [460] [PGOT: A Physics-Geometry Operator Transformer for Complex PDEs](https://arxiv.org/abs/2512.23192)
*Zhuo Zhang,Xi Yang,Yuan Zhao,Canqun Yang*

Main category: cs.LG

TL;DR: This paper introduces the Physics-Geometry Operator Transformer (PGOT) to address modeling challenges for large-scale unstructured meshes in PDEs by explicitly preserving geometry features and employing an adaptive computational approach.


<details>
  <summary>Details</summary>
Motivation: Modeling large-scale unstructured meshes with complex geometries within PDEs is challenging due to Geometric Aliasing caused by existing dimensionality reduction strategies, which lose critical physical boundary information.

Method: The authors introduce the Physics-Geometry Operator Transformer (PGOT) featuring Spectrum-Preserving Geometric Attention (SpecGeo-Attention) that uses a physics slicing-geometry injection mechanism and employs spatially adaptive computations to handle disparities in smooth and complex regions.

Result: PGOT outperforms prior approaches across four benchmarks and demonstrates high performance in large-scale industrial tasks, including airfoil and car design modeling.

Conclusion: PGOT effectively addresses geometric challenges in PDE modeling with superior performance and scalability, combining geometry-awareness and adaptive computations for real-world applications.

Abstract: While Transformers have demonstrated remarkable potential in modeling Partial Differential Equations (PDEs), modeling large-scale unstructured meshes with complex geometries remains a significant challenge. Existing efficient architectures often employ feature dimensionality reduction strategies, which inadvertently induces Geometric Aliasing, resulting in the loss of critical physical boundary information. To address this, we propose the Physics-Geometry Operator Transformer (PGOT), designed to reconstruct physical feature learning through explicit geometry awareness. Specifically, we propose Spectrum-Preserving Geometric Attention (SpecGeo-Attention). Utilizing a ``physics slicing-geometry injection" mechanism, this module incorporates multi-scale geometric encodings to explicitly preserve multi-scale geometric features while maintaining linear computational complexity $O(N)$. Furthermore, PGOT dynamically routes computations to low-order linear paths for smooth regions and high-order non-linear paths for shock waves and discontinuities based on spatial coordinates, enabling spatially adaptive and high-precision physical field modeling. PGOT achieves consistent state-of-the-art performance across four standard benchmarks and excels in large-scale industrial tasks including airfoil and car designs.

</details>


### [461] [Energy and Memory-Efficient Federated Learning With Ordered Layer Freezing](https://arxiv.org/abs/2512.23200)
*Ziru Niu,Hai Dong,A. K. Qin,Tao Gu,Pengcheng Zhang*

Main category: cs.LG

TL;DR: The paper introduces Federated Learning with Ordered Layer Freezing (FedOLF) and Tensor Operation Approximation (TOA) to address computational, memory, and communication challenges in Federated Learning, achieving improved model accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Address efficiency and scalability issues in Federated Learning for IoT edge devices with limited computational power, memory, and bandwidth, particularly in training deep neural networks.

Method: Introduce FedOLF, which freezes layers in a predefined order to reduce computation and memory costs, and TOA, a lightweight approach to further reduce communication and energy costs while preserving model accuracy.

Result: FedOLF achieves higher accuracy across various datasets and model architectures compared to existing methods, with improved energy efficiency and reduced memory requirements.

Conclusion: FedOLF and TOA provide effective solutions to challenges in Federated Learning, making it more suited for resource-constrained IoT applications while ensuring model accuracy and scalability.

Abstract: Federated Learning (FL) has emerged as a privacy-preserving paradigm for training machine learning models across distributed edge devices in the Internet of Things (IoT). By keeping data local and coordinating model training through a central server, FL effectively addresses privacy concerns and reduces communication overhead. However, the limited computational power, memory, and bandwidth of IoT edge devices pose significant challenges to the efficiency and scalability of FL, especially when training deep neural networks. Various FL frameworks have been proposed to reduce computation and communication overheads through dropout or layer freezing. However, these approaches often sacrifice accuracy or neglect memory constraints. To this end, in this work, we introduce Federated Learning with Ordered Layer Freezing (FedOLF). FedOLF consistently freezes layers in a predefined order before training, significantly mitigating computation and memory requirements. To further reduce communication and energy costs, we incorporate Tensor Operation Approximation (TOA), a lightweight alternative to conventional quantization that better preserves model accuracy. Experimental results demonstrate that over non-iid data, FedOLF achieves at least 0.3%, 6.4%, 5.81%, 4.4%, 6.27% and 1.29% higher accuracy than existing works respectively on EMNIST (with CNN), CIFAR-10 (with AlexNet), CIFAR-100 (with ResNet20 and ResNet44), and CINIC-10 (with ResNet20 and ResNet44), along with higher energy efficiency and lower memory footprint.

</details>


### [462] [PFed-Signal: An ADR Prediction Model based on Federated Learning](https://arxiv.org/abs/2512.23262)
*Tao Li,Peilin Li,Kui Lu,Yilei Wang,Junliang Shang,Guangshun Li,Huiyu Zhou*

Main category: cs.LG

TL;DR: The paper introduces PFed-signal, a federated learning model using Euclidean distance to eliminate biases in FAERS for better adverse drug reaction (ADR) prediction.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for predicting ADRs from FAERS data are biased, leading to inaccurate diagnoses; this study aims to overcome these issues by addressing dataset biases.

Method: It proposes splitting the dataset (PFed-Split), identifying biased data using Euclidean distance and federated learning, and constructing a Transformer-based ADR prediction model trained on a clean dataset.

Result: PFed-Signal outperforms traditional methods with key metrics such as accuracy (0.887), F1 score (0.890), recall rate (0.913), and AUC (0.957).

Conclusion: The proposed approach significantly improves ADR signal predictions by eliminating biased data, offering a more reliable method for addressing biases in FAERS.

Abstract: The adverse drug reactions (ADRs) predicted based on the biased records in FAERS (U.S. Food and Drug Administration Adverse Event Reporting System) may mislead diagnosis online. Generally, such problems are solved by optimizing reporting odds ratio (ROR) or proportional reporting ratio (PRR). However, these methods that rely on statistical methods cannot eliminate the biased data, leading to inaccurate signal prediction. In this paper, we propose PFed-signal, a federated learning-based signal prediction model of ADR, which utilizes the Euclidean distance to eliminate the biased data from FAERS, thereby improving the accuracy of ADR prediction. Specifically, we first propose Pfed-Split, a method to split the original dataset into a split dataset based on ADR. Then we propose ADR-signal, an ADR prediction model, including a biased data identification method based on federated learning and an ADR prediction model based on Transformer. The former identifies the biased data according to the Euclidean distance and generates a clean dataset by deleting the biased data. The latter is an ADR prediction model based on Transformer trained on the clean data set. The results show that the ROR and PRR on the clean dataset are better than those of the traditional methods. Furthermore, the accuracy rate, F1 score, recall rate and AUC of PFed-Signal are 0.887, 0.890, 0.913 and 0.957 respectively, which are higher than the baselines.

</details>


### [463] [On the Inverse Flow Matching Problem in the One-Dimensional and Gaussian Cases](https://arxiv.org/abs/2512.23265)
*Alexander Korotin,Gudmund Pammer*

Main category: cs.LG

TL;DR: The paper investigates the inverse problem of flow matching between distributions with finite exponential moments, focusing on generative AI applications.


<details>
  <summary>Details</summary>
Motivation: To address flow matching issues in generative AI applications, such as distillation in flow matching models.

Method: Uniqueness of flow matching solution is analyzed in one-dimensional and Gaussian cases.

Result: Solution uniqueness is established in 1D and Gaussian scenarios, but the multidimensional problem is yet unresolved.

Conclusion: While the paper provides solutions for specific cases, multidimensional flow matching remains an open problem for future exploration.

Abstract: This paper studies the inverse problem of flow matching (FM) between distributions with finite exponential moment, a problem motivated by modern generative AI applications such as the distillation of flow matching models. Uniqueness of the solution is established in two cases - the one-dimensional setting and the Gaussian case. The general multidimensional problem remains open for future studies.

</details>


### [464] [Spectral Analysis of Hard-Constraint PINNs: The Spatial Modulation Mechanism of Boundary Functions](https://arxiv.org/abs/2512.23295)
*Yuchen Xie,Honghang Chi,Haopeng Quan,Yahui Wang,Wei Wang,Yu Ma*

Main category: cs.LG

TL;DR: The paper investigates the theoretical dynamics of hard-constrained Physics-Informed Neural Networks (HC-PINNs), revealing boundary functions fundamentally alter the learning landscape via spectral modulation.


<details>
  <summary>Details</summary>
Motivation: To address the theoretical gap in understanding how HC-PINNs' trial functions influence training dynamics and optimization in scientific machine learning.

Method: Rigorously establishes an NTK framework for HC-PINNs, derives kernel composition laws, and uses spectral analysis to evaluate training convergence.

Result: Finds that boundary functions in HC-PINNs act as spectral filters, impacting eigenspectrum and training dynamics, and identifies predictors of optimization convergence.

Conclusion: The framework shifts boundary function design from heuristics to a spectral optimization problem, improving geometric hard constraints in scientific machine learning.

Abstract: Physics-Informed Neural Networks with hard constraints (HC-PINNs) are increasingly favored for their ability to strictly enforce boundary conditions via a trial function ansatz $\tilde{u} = A + B \cdot N$, yet the theoretical mechanisms governing their training dynamics have remained unexplored.
  Unlike soft-constrained formulations where boundary terms act as additive penalties, this work reveals that the boundary function $B$ introduces a multiplicative spatial modulation that fundamentally alters the learning landscape.
  A rigorous Neural Tangent Kernel (NTK) framework for HC-PINNs is established, deriving the explicit kernel composition law.
  This relationship demonstrates that the boundary function $B(\vec{x})$ functions as a spectral filter, reshaping the eigenspectrum of the neural network's native kernel.
  Through spectral analysis, the effective rank of the residual kernel is identified as a deterministic predictor of training convergence, superior to classical condition numbers.
  It is shown that widely used boundary functions can inadvertently induce spectral collapse, leading to optimization stagnation despite exact boundary satisfaction.
  Validated across multi-dimensional benchmarks, this framework transforms the design of boundary functions from a heuristic choice into a principled spectral optimization problem, providing a solid theoretical foundation for geometric hard constraints in scientific machine learning.

</details>


### [465] [Deep learning for pedestrians: backpropagation in Transformers](https://arxiv.org/abs/2512.23329)
*Laurent Boué*

Main category: cs.LG

TL;DR: This paper extends prior work on vectorized backpropagation, applying it to transformer-based architectures and providing lightweight gradient derivations for various components.


<details>
  <summary>Details</summary>
Motivation: To develop a deeper understanding of backpropagation in transformer-based architectures and increase the intuition behind forward and backward processes.

Method: The paper uses a lightweight, index-free methodology to derive gradients for transformer layers such as embedding, attention, layer normalization, and LoRA for fine-tuning.

Result: Analytical gradient expressions and a minimal PyTorch GPT-like implementation are offered to guide understanding and application.

Conclusion: Manually working through backpropagation reveals gaps in understanding and offers valuable insight, complementing automatic differentiation tools.

Abstract: This document is a follow-up to our previous paper dedicated to a vectorized derivation of backpropagation in CNNs. Following the same principles and notations already put in place there, we now focus on transformer-based next-token-prediction architectures. To this end, we apply our lightweight index-free methodology to new types of layers such as embedding, multi-headed self-attention and layer normalization. In addition, we also provide gradient expressions for LoRA layers to illustrate parameter-efficient fine-tuning. Why bother doing manual backpropagation when there are so many tools that do this automatically? Any gap in understanding of how values propagate forward will become evident when attempting to differentiate the loss function. By working through the backward pass manually, we gain a deeper intuition for how each operation influences the final output. A complete PyTorch implementation of a minimalistic GPT-like network is also provided along with analytical expressions for of all of its gradient updates.

</details>


### [466] [The Law of Multi-Model Collaboration: Scaling Limits of Model Ensembling for Large Language Models](https://arxiv.org/abs/2512.23340)
*Dakuan Lu,Jiaqi Zhang,Cheng Yuan,Jiawei Shao,Chi Zhang,Xuelong Li*

Main category: cs.LG

TL;DR: Multi-model collaboration in large language models (LLMs) surpasses individual model capabilities, following a power-law scaling with better performance due to model diversity.


<details>
  <summary>Details</summary>
Motivation: Current scaling laws are limited to individual LLMs, which have inherent performance limits, and a unifying performance scaling framework for multi-model collaboration is missing.

Method: This paper introduces the Law of Multi-model Collaboration, a scaling law estimating performance limits based on aggregated parameters, assuming an idealized integration oracle with method-agnostic loss calculation.

Result: Experiments demonstrate that multi-model systems perform better than single LLMs, with heterogeneous models achieving superior performance due to diversity.

Conclusion: Multi-model collaboration is essential for pushing the performance boundaries of LLMs, underscoring the importance of model diversity in achieving better results.

Abstract: Recent advances in large language models (LLMs) have been largely driven by scaling laws for individual models, which predict performance improvements as model parameters and data volume increase. However, the capabilities of any single LLM are inherently bounded. One solution originates from intricate interactions among multiple LLMs, rendering their collective performance surpasses that of any constituent model. Despite the rapid proliferation of multi-model integration techniques such as model routing and post-hoc ensembling, a unifying theoretical framework of performance scaling for multi-model collaboration remains absent. In this work, we propose the Law of Multi-model Collaboration, a scaling law that predicts the performance limits of LLM ensembles based on their aggregated parameter budget. To quantify the intrinsic upper bound of multi-model collaboration, we adopt a method-agnostic formulation and assume an idealized integration oracle where the total cross-entropy loss of each sample is determined by the minimum loss of any model in the model pool. Experimental results reveal that multi-model systems follow a power-law scaling with respect to the total parameter count, exhibiting a more significant improvement trend and a lower theoretical loss floor compared to single model scaling. Moreover, ensembles of heterogeneous model families achieve better performance scaling than those formed within a single model family, indicating that model diversity is a primary driver of collaboration gains. These findings suggest that model collaboration represents a critical axis for extending the intelligence frontier of LLMs.

</details>


### [467] [ECG-RAMBA: Zero-Shot ECG Generalization by Morphology-Rhythm Disentanglement and Long-Range Modeling](https://arxiv.org/abs/2512.23347)
*Hai Duong Nguyen,Xuan-The Tran*

Main category: cs.LG

TL;DR: This paper introduces ECG-RAMBA, a novel framework that separates and reintegrates ECG morphology and rhythm to improve classification accuracy and cross-domain robustness.


<details>
  <summary>Details</summary>
Motivation: To enhance deep learning-based ECG classification which struggles with generalization across different acquisition settings, hindering its clinical deployment and monitoring capabilities.

Method: ECG-RAMBA separates ECG waveform morphology and rhythm using MiniRocket for morphology features, heart-rate variability for rhythm descriptors, and integrates them with a bi-directional Mamba backbone. It includes Power Mean pooling for transient abnormality sensitivity and the protocol-faithful evaluation setup.

Result: ECG-RAMBA significantly surpassed baseline models in performance, achieving a macro ROC-AUC of ~0.85 on Chapman-Shaoxing and a PR-AUC of 0.708 for atrial fibrillation detection in zero-shot transfer on CPSC-2021, with consistent robustness across datasets.

Conclusion: The proposed architecture effectively improves ECG classification by leveraging explicit modular modeling, with morphology and rhythm contributing to cross-domain generalization and robustness.

Abstract: Deep learning has achieved strong performance for electrocardiogram (ECG) classification within individual datasets, yet dependable generalization across heterogeneous acquisition settings remains a major obstacle to clinical deployment and longitudinal monitoring. A key limitation of many model architectures is the implicit entanglement of morphological waveform patterns and rhythm dynamics, which can promote shortcut learning and amplify sensitivity to distribution shifts. We propose ECG-RAMBA, a framework that separates morphology and rhythm and then re-integrates them through context-aware fusion. ECG-RAMBA combines: (i) deterministic morphological features extracted by MiniRocket, (ii) global rhythm descriptors computed from heart-rate variability (HRV), and (iii) long-range contextual modeling via a bi-directional Mamba backbone. To improve sensitivity to transient abnormalities under windowed inference, we introduce a numerically stable Power Mean pooling operator ($Q=3$) that emphasizes high-evidence segments while avoiding the brittleness of max pooling and the dilution of averaging. We evaluate under a protocol-faithful setting with subject-level cross-validation, a fixed decision threshold, and no test-time adaptation. On the Chapman--Shaoxing dataset, ECG-RAMBA achieves a macro ROC-AUC $\approx 0.85$. In zero-shot transfer, it attains PR-AUC $=0.708$ for atrial fibrillation detection on the external CPSC-2021 dataset, substantially outperforming a comparable raw-signal Mamba baseline, and shows consistent cross-dataset performance on PTB-XL. Ablation studies indicate that deterministic morphology provides a strong foundation, while explicit rhythm modeling and long-range context are critical drivers of cross-domain robustness.

</details>


### [468] [ISOPO: Proximal policy gradients without pi-old](https://arxiv.org/abs/2512.23353)
*Nilin Abrahamsen*

Main category: cs.LG

TL;DR: ISOPO introduces a method to approximate the natural policy gradient with a single gradient step, unlike existing methods requiring multiple steps.


<details>
  <summary>Details</summary>
Motivation: To create a more efficient method for approximating natural policy gradients, addressing the limitations of existing multi-step proximal policy methods.

Method: ISOPO normalizes log-probability gradients relative to their Fisher metric before contracting with advantages. It also uses neural tangent kernel transformations for microbatch advantages, applied layer-wise.

Result: Proposes a single backward pass computation improving computational efficiency compared to traditional REINFORCE methods.

Conclusion: ISOPO is presented as an efficient alternative to traditional proximal methods with minimal computational overhead.

Abstract: This note introduces Isometric Policy Optimization (ISOPO), an efficient method to approximate the natural policy gradient in a single gradient step. In comparison, existing proximal policy methods such as GRPO or CISPO use multiple gradient steps with variants of importance ratio clipping to approximate a natural gradient step relative to a reference policy. In its simplest form, ISOPO normalizes the log-probability gradient of each sequence in the Fisher metric before contracting with the advantages. Another variant of ISOPO transforms the microbatch advantages based on the neural tangent kernel in each layer. ISOPO applies this transformation layer-wise in a single backward pass and can be implemented with negligible computational overhead compared to vanilla REINFORCE.

</details>


### [469] [Post-Training Quantization of OpenPangu Models for Efficient Deployment on Atlas A2](https://arxiv.org/abs/2512.23367)
*Yilun Luo,HuaQing Zheng,Haoqian Meng,Wenyuan Liu,Peng Zhang*

Main category: cs.LG

TL;DR: This paper presents a low-bit quantization approach to address computational constraints in implementing the openPangu-Embedded models on Ascend NPUs, achieving efficient reasoning while maintaining high accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the memory and latency challenges posed by Chain-of-Thought reasoning modes on Ascend NPUs, which hinder practical deployment.

Method: Introduced a unified low-bit inference framework optimized for the openPangu-Embedded models on the Atlas A2, employing INT8 and W4A8 quantization techniques.

Result: The INT8 quantization preserved over 90% accuracy of the FP16 baseline and achieved a 1.5x speedup, while W4A8 reduced memory consumption with minor accuracy trade-offs.

Conclusion: Low-bit quantization enables efficient reasoning on Ascend NPUs with improved computation speed and reduced memory usage, maintaining model reliability.

Abstract: Huawei's openPangu-Embedded-1B and openPangu-Embedded-7B, variants of the openPangu large language model, integrate three distinct Chain-of-Thought (CoT) reasoning paradigms, namely slow_think, auto_think, and no_think. While these CoT modes enhance reasoning capabilities, their generation of extended reasoning traces introduces substantial memory and latency overheads, posing challenges for practical deployment on Ascend NPUs. This paper addresses these computational constraints by leveraging low-bit quantization, which transforms FP16 computations into more efficient integer arithmetic. We introduce a unified low-bit inference framework, supporting INT8 (W8A8) and W4A8 quantization, specifically optimized for openPangu-Embedded models on the Atlas A2. Our comprehensive evaluation, conducted across all three CoT modes on code generation benchmarks (HumanEval and MBPP), demonstrates the efficacy of this approach. INT8 quantization consistently preserves over 90\% of the FP16 baseline accuracy and achieves a 1.5x prefill speedup on the Atlas A2. Furthermore, W4A8 quantization significantly reduces memory consumption, albeit with a moderate trade-off in accuracy. These findings collectively indicate that low-bit quantization effectively facilitates efficient CoT reasoning on Ascend NPUs, maintaining high model fidelity.

</details>


### [470] [Diffusion priors enhanced velocity model building from time-lag images using a neural operator](https://arxiv.org/abs/2512.23375)
*Xiao Ma,Mohammad Hasyim Taufik,Tariq Alkhalifah*

Main category: cs.LG

TL;DR: The study introduces a novel framework combining generative models and neural operators for efficient high resolution subsurface velocity model building, dramatically improving computational efficiency and precision.


<details>
  <summary>Details</summary>
Motivation: Traditional velocity model building methods for subsurface imaging are computationally expensive and slow, prompting the exploration of deep learning-based approaches to overcome these challenges.

Method: The proposed framework uses neural operators to map migration velocities to extended images efficiently. Generative models act as regularizers, ensuring higher resolution in the velocity models. The neural operator is trained for automatic differentiation to refine migration velocities iteratively.

Result: Synthetic and field data experiments confirm that the method yields cleaner, high-resolution velocity models which match with observed data effectively.

Conclusion: The generative neural operator framework offers a promising alternative for efficient and precise velocity model building, leveraging deep learning innovations to overcome traditional limitations.

Abstract: Velocity model building serves as a crucial component for achieving high precision subsurface imaging. However, conventional velocity model building methods are often computationally expensive and time consuming. In recent years, with the rapid advancement of deep learning, particularly the success of generative models and neural operators, deep learning based approaches that integrate data and their statistics have attracted increasing attention in addressing the limitations of traditional methods. In this study, we propose a novel framework that combines generative models with neural operators to obtain high resolution velocity models efficiently. Within this workflow, the neural operator functions as a forward mapping operator to rapidly generate time lag reverse time migration (RTM) extended images from the true and migration velocity models. In this framework, the neural operator is acting as a surrogate for modeling followed by migration, which uses the true and migration velocities, respectively. The trained neural operator is then employed, through automatic differentiation, to gradually update the migration velocity placed in the true velocity input channel with high resolution components so that the output of the network matches the time lag images of observed data obtained using the migration velocity. By embedding a generative model, trained on a high-resolution velocity model distribution, which corresponds to the true velocity model distribution used to train the neural operator, as a regularizer, the resulting predictions are cleaner with higher resolution information. Both synthetic and field data experiments demonstrate the effectiveness of the proposed generative neural operator based velocity model building approach.

</details>


### [471] [A unified framework for detecting point and collective anomalies in operating system logs via collaborative transformers](https://arxiv.org/abs/2512.23380)
*Mohammad Nasirzadeh,Jafar Tahmoresnezhad,Parviz Rashidi-Khazaee*

Main category: cs.LG

TL;DR: CoLog is a multimodal framework for log anomaly detection that outperforms state-of-the-art methods with a mean F1 score of 99.61% across seven benchmark datasets. It addresses log modality interactions and heterogeneity effectively using advanced transformers and attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: Unimodal methods ignore log data's different modalities, and multimodal methods fail to manage interactions between modalities, necessitating a more comprehensive detection solution.

Method: CoLog employs collaborative transformers, multi-head impressed attention, and a modality adaptation layer to encode and adapt interactions among log modalities for nuanced anomaly detection.

Result: Extensive experiments show CoLog achieves high precision (99.63%), recall (99.59%), and F1 score (99.61%), validating its superiority in log anomaly detection.

Conclusion: CoLog is a sophisticated, effective framework for detecting point and collective anomalies, ideal for cybersecurity, system monitoring, and operational efficiency. Implementation is publicly available.

Abstract: Log anomaly detection is crucial for preserving the security of operating systems. Depending on the source of log data collection, various information is recorded in logs that can be considered log modalities. In light of this intuition, unimodal methods often struggle by ignoring the different modalities of log data. Meanwhile, multimodal methods fail to handle the interactions between these modalities. Applying multimodal sentiment analysis to log anomaly detection, we propose CoLog, a framework that collaboratively encodes logs utilizing various modalities. CoLog utilizes collaborative transformers and multi-head impressed attention to learn interactions among several modalities, ensuring comprehensive anomaly detection. To handle the heterogeneity caused by these interactions, CoLog incorporates a modality adaptation layer, which adapts the representations from different log modalities. This methodology enables CoLog to learn nuanced patterns and dependencies within the data, enhancing its anomaly detection capabilities. Extensive experiments demonstrate CoLog's superiority over existing state-of-the-art methods. Furthermore, in detecting both point and collective anomalies, CoLog achieves a mean precision of 99.63%, a mean recall of 99.59%, and a mean F1 score of 99.61% across seven benchmark datasets for log-based anomaly detection. The comprehensive detection capabilities of CoLog make it highly suitable for cybersecurity, system monitoring, and operational efficiency. CoLog represents a significant advancement in log anomaly detection, providing a sophisticated and effective solution to point and collective anomaly detection through a unified framework and a solution to the complex challenges automatic log data analysis poses. We also provide the implementation of CoLog at https://github.com/NasirzadehMoh/CoLog.

</details>


### [472] [Task-driven Heterophilic Graph Structure Learning](https://arxiv.org/abs/2512.23406)
*Ayushman Raghuvanshi,Gonzalo Mateos,Sundeep Prabhakar Chepuri*

Main category: cs.LG

TL;DR: The paper introduces Frequency-guided Graph Structure Learning (FgGSL), a framework that addresses challenges in learning for heterophilic graphs by leveraging spectral encoding and frequency-driven graph inference.


<details>
  <summary>Details</summary>
Motivation: Graph neural networks struggle to learn effective node representations in heterophilic graphs due to weak structural cues in features and label dissimilarity in connected nodes.

Method: The FgGSL framework jointly learns homophilic and heterophilic graph structures using a learnable masking function, spectral encoder, and filter banks for graph frequencies. It also employs a label-based structural loss to enhance task-driven performance.

Result: FgGSL outperforms state-of-the-art GNNs and graph rewiring methods in six benchmark datasets, demonstrating its effectiveness on heterophilic graphs.

Conclusion: Plugging frequency information and supervised topology inference into GNNs brings significant improvement to heterophilic graph learning, enhancing the robustness and efficacy of node representations.

Abstract: Graph neural networks (GNNs) often struggle to learn discriminative node representations for heterophilic graphs, where connected nodes tend to have dissimilar labels and feature similarity provides weak structural cues. We propose frequency-guided graph structure learning (FgGSL), an end-to-end graph inference framework that jointly learns homophilic and heterophilic graph structures along with a spectral encoder. FgGSL employs a learnable, symmetric, feature-driven masking function to infer said complementary graphs, which are processed using pre-designed low- and high-pass graph filter banks. A label-based structural loss explicitly promotes the recovery of homophilic and heterophilic edges, enabling task-driven graph structure learning. We derive stability bounds for the structural loss and establish robustness guarantees for the filter banks under graph perturbations. Experiments on six heterophilic benchmarks demonstrate that FgGSL consistently outperforms state-of-the-art GNNs and graph rewiring methods, highlighting the benefits of combining frequency information with supervised topology inference.

</details>


### [473] [Directly Constructing Low-Dimensional Solution Subspaces in Deep Neural Networks](https://arxiv.org/abs/2512.23410)
*Yusuf Kalyoncuoglu*

Main category: cs.LG

TL;DR: The paper addresses the redundancy in high-dimensional neural networks by proposing a method to compress the classification head and bypass optimization challenges.


<details>
  <summary>Details</summary>
Motivation: Massive widths in neural networks are unnecessary for representation but crucial for solving non-convex optimization challenges.

Method: The authors introduced a constructive approach that decouples solution geometry from the ambient search space, emphasizing subspace-native distillation.

Result: The classification head of deep models like ResNet-50, ViT, and BERT was compressed by factors of 16 with minimal performance loss.

Conclusion: This novel technique allows student models to bypass high-dimensional optimization problems, advancing the idea of Train Big, Deploy Small.

Abstract: While it is well-established that the weight matrices and feature manifolds of deep neural networks exhibit a low Intrinsic Dimension (ID), current state-of-the-art models still rely on massive high-dimensional widths. This redundancy is not required for representation, but is strictly necessary to solve the non-convex optimization search problem-finding a global minimum, which remains intractable for compact networks. In this work, we propose a constructive approach to bypass this optimization bottleneck. By decoupling the solution geometry from the ambient search space, we empirically demonstrate across ResNet-50, ViT, and BERT that the classification head can be compressed by even huge factors of 16 with negligible performance degradation. This motivates Subspace-Native Distillation as a novel paradigm: by defining the target directly in this constructed subspace, we provide a stable geometric coordinate system for student models, potentially allowing them to circumvent the high-dimensional search problem entirely and realize the vision of Train Big, Deploy Small.

</details>


### [474] [Stochastic Siamese MAE Pretraining for Longitudinal Medical Images](https://arxiv.org/abs/2512.23441)
*Taha Emre,Arunava Chakravarty,Thomas Pinetz,Dmitrii Lachinov,Martin J. Menten,Hendrik Scholl,Sobha Sivaprasad,Daniel Rueckert,Andrew Lotery,Stefan Sacu,Ursula Schmidt-Erfurth,Hrvoje Bogunović*

Main category: cs.LG

TL;DR: The paper introduces STAMP, a temporally-aware self-supervised learning framework for longitudinal medical dataset analysis, improving disease progression prediction in 3D volumes.


<details>
  <summary>Details</summary>
Motivation: To address the gap in temporal representation for disease progression in 3D medical datasets, especially since existing self-supervised learning models like MAE are not temporally aware.

Method: STAMP is a Siamese MAE framework that uses a stochastic process to encode temporal information, leveraging a time-conditioned variational inference objective to handle uncertainties in disease progression.

Result: STAMP outperformed current temporal MAE methods and foundation models in predicting late-stage progression of Age-Related Macular Degeneration and Alzheimer's Disease across different datasets.

Conclusion: The proposed STAMP model is effective in learning non-deterministic temporal dynamics in diseases, demonstrating its superiority in disease progression prediction using 3D medical datasets.

Abstract: Temporally aware image representations are crucial for capturing disease progression in 3D volumes of longitudinal medical datasets. However, recent state-of-the-art self-supervised learning approaches like Masked Autoencoding (MAE), despite their strong representation learning capabilities, lack temporal awareness. In this paper, we propose STAMP (Stochastic Temporal Autoencoder with Masked Pretraining), a Siamese MAE framework that encodes temporal information through a stochastic process by conditioning on the time difference between the 2 input volumes. Unlike deterministic Siamese approaches, which compare scans from different time points but fail to account for the inherent uncertainty in disease evolution, STAMP learns temporal dynamics stochastically by reframing the MAE reconstruction loss as a conditional variational inference objective. We evaluated STAMP on two OCT and one MRI datasets with multiple visits per patient. STAMP pretrained ViT models outperformed both existing temporal MAE methods and foundation models on different late stage Age-Related Macular Degeneration and Alzheimer's Disease progression prediction which require models to learn the underlying non-deterministic temporal dynamics of the diseases.

</details>


### [475] [Dynamic Subspace Composition: Efficient Adaptation via Contractive Basis Expansion](https://arxiv.org/abs/2512.23448)
*Vladimer Khasia*

Main category: cs.LG

TL;DR: Dynamic Subspace Composition (DSC) is a framework addressing challenges in Mixture of Experts (MoE) models, enhancing scalability and stability through efficient sparse weight approximation.


<details>
  <summary>Details</summary>
Motivation: MoE models often face issues like representation collapse and unstable gradients when scaling. This paper aims to resolve these with a novel framework.

Method: Proposes DSC, leveraging a sparse expansion of shared basis banks instead of independent rank-r matrices. Employs Magnitude-Gated Simplex Interpolation and imposes Frame-Theoretic regularization and spectral constraints.

Result: DSC lowers parameter complexity and memory traffic drastically while ensuring stability and scalability with formal bounds.

Conclusion: DSC provides efficient scaling for MoE models with reduced complexity and enhanced stability, addressing prior limitations effectively.

Abstract: Mixture of Experts (MoE) models scale capacity but often suffer from representation collapse and gradient instability. We propose Dynamic Subspace Composition (DSC), a framework that approximates context-dependent weights via a state-dependent, sparse expansion of a shared basis bank. Formally, DSC models the weight update as a residual trajectory within a Star- Shaped Domain, employing a Magnitude-Gated Simplex Interpolation to ensure continuity at the identity. Unlike standard Mixture-of-LoRAs, which incurs O(M rd) parameter complexity by retrieving independent rank-r matrices, DSC constructs a compositional rank-K approximation from decoupled unit-norm basis vectors. This reduces parameter complexity to O(M d) and memory traffic to O(Kd), while Frame-Theoretic regularization and spectral constraints provide rigorous worst-case bounds on the dynamic update. The code is available at https://github. com/VladimerKhasia/DSC

</details>


### [476] [Eliminating Inductive Bias in Reward Models with Information-Theoretic Guidance](https://arxiv.org/abs/2512.23461)
*Zhuo Li,Pengyu Cheng,Zhechao Yu,Feifei Tong,Anningzhe Gao,Tsung-Hui Chang,Xiang Wan,Erchao Zhao,Xiaoxi Jiang,Guanjun Jiang*

Main category: cs.LG

TL;DR: This paper addresses reward model (RM) bias in reinforcement learning from human feedback (RLHF) by introducing the DIR method, which uses an information-theoretic approach for debiasing.


<details>
  <summary>Details</summary>
Motivation: To address the issue of low-quality RM training data that contains inductive biases, leading to overfitting and reward hacking.

Method: The proposed method, DIR, applies the information bottleneck principle, optimizing mutual information to maximize desirable signals (human preferences) while minimizing associations with biased attributes.

Result: DIR mitigates various types of inductive biases such as response length, sycophancy, and format. It also improves RLHF performance and generalization across multiple benchmarks.

Conclusion: DIR effectively debiases RMs and enhances their performance, making it applicable to broader real-world scenarios in RLHF with improved generalization ability.

Abstract: Reward models (RMs) are essential in reinforcement learning from human feedback (RLHF) to align large language models (LLMs) with human values. However, RM training data is commonly recognized as low-quality, containing inductive biases that can easily lead to overfitting and reward hacking. For example, more detailed and comprehensive responses are usually human-preferred but with more words, leading response length to become one of the inevitable inductive biases. A limited number of prior RM debiasing approaches either target a single specific type of bias or model the problem with only simple linear correlations, \textit{e.g.}, Pearson coefficients. To mitigate more complex and diverse inductive biases in reward modeling, we introduce a novel information-theoretic debiasing method called \textbf{D}ebiasing via \textbf{I}nformation optimization for \textbf{R}M (DIR). Inspired by the information bottleneck (IB), we maximize the mutual information (MI) between RM scores and human preference pairs, while minimizing the MI between RM outputs and biased attributes of preference inputs. With theoretical justification from information theory, DIR can handle more sophisticated types of biases with non-linear correlations, broadly extending the real-world application scenarios for RM debiasing methods. In experiments, we verify the effectiveness of DIR with three types of inductive biases: \textit{response length}, \textit{sycophancy}, and \textit{format}. We discover that DIR not only effectively mitigates target inductive biases but also enhances RLHF performance across diverse benchmarks, yielding better generalization abilities. The code and training recipes are available at https://github.com/Qwen-Applications/DIR.

</details>


### [477] [FRoD: Full-Rank Efficient Fine-Tuning with Rotational Degrees for Fast Convergence](https://arxiv.org/abs/2512.23485)
*Guoan Wan,Tianyu Chen,Fangzheng Feng,Haoyi Zhou,Runhua Xu*

Main category: cs.LG

TL;DR: The paper introduces FRoD, a fine-tuning method enhancing the efficiency and expressiveness of parameter-efficient fine-tuning, achieving full model accuracy using only 1.72% trainable parameters.


<details>
  <summary>Details</summary>
Motivation: Current parameter-efficient fine-tuning methods like LoRA face limitations in convergence speed and adaptation capacity due to low-rank constraints, limiting their effectiveness for diverse tasks.

Method: FRoD utilizes hierarchical joint decomposition with rotational degrees of freedom, combining shared basis across layers and learnable perturbations to achieve flexible full-rank updates.

Result: FRoD achieves faster and more robust convergence, matching the accuracy of full model fine-tuning across 20 benchmarks, while maintaining high computational efficiency with only 1.72% trainable parameters.

Conclusion: FRoD offers a practical and efficient fine-tuning solution, advancing the state of PEFT methods by overcoming low-rank constraints and balancing efficiency with expressive modeling.

Abstract: Parameter-efficient fine-tuning (PEFT) methods have emerged as a practical solution for adapting large foundation models to downstream tasks, reducing computational and memory costs by updating only a small subset of parameters. Among them, approaches like LoRA aim to strike a balance between efficiency and expressiveness, but often suffer from slow convergence and limited adaptation capacity due to their inherent low-rank constraints. This trade-off hampers the ability of PEFT methods to capture complex patterns needed for diverse tasks. To address these challenges, we propose FRoD, a novel fine-tuning method that combines hierarchical joint decomposition with rotational degrees of freedom. By extracting a globally shared basis across layers and injecting sparse, learnable perturbations into scaling factors for flexible full-rank updates, FRoD enhances expressiveness and efficiency, leading to faster and more robust convergence. On 20 benchmarks spanning vision, reasoning, and language understanding, FRoD matches full model fine-tuning in accuracy, while using only 1.72% of trainable parameters under identical training budgets.

</details>


### [478] [Joint Link Adaptation and Device Scheduling Approach for URLLC Industrial IoT Network: A DRL-based Method with Bayesian Optimization](https://arxiv.org/abs/2512.23493)
*Wei Gao,Paul Zheng,Peng Wu,Yulin Hu,Anke Schmeink*

Main category: cs.LG

TL;DR: The paper introduces a method to optimize communication rates in IIoT networks with imperfect channel data, utilizing Bayesian optimization and TD3 algorithm.


<details>
  <summary>Details</summary>
Motivation: IIoT networks face challenges ensuring ultra-reliable low-latency communication (URLLC) with imperfect channel state information (CSI). They require efficient methods to balance transmission rates and reliability.

Method: The paper proposes a Bayesian optimization (BO) driven Twin Delayed Deep Deterministic Policy Gradient (TD3) approach to optimize device scheduling and link adaptation under imperfect CSI conditions.

Result: The method shows faster convergence and improves the transmission sum-rate compared to other existing algorithms.

Conclusion: The proposed BO-enhanced TD3 algorithm is effective for improving performance and reliability in IIoT networks under imperfect CSI conditions.

Abstract: In this article, we consider an industrial internet of things (IIoT) network supporting multi-device dynamic ultra-reliable low-latency communication (URLLC) while the channel state information (CSI) is imperfect. A joint link adaptation (LA) and device scheduling (including the order) design is provided, aiming at maximizing the total transmission rate under strict block error rate (BLER) constraints. In particular, a Bayesian optimization (BO) driven Twin Delayed Deep Deterministic Policy Gradient (TD3) method is proposed, which determines the device served order sequence and the corresponding modulation and coding scheme (MCS) adaptively based on the imperfect CSI. Note that the imperfection of CSI, error sample imbalance in URLLC networks, as well as the parameter sensitivity nature of the TD3 algorithm likely diminish the algorithm's convergence speed and reliability. To address such an issue, we proposed a BO based training mechanism for the convergence speed improvement, which provides a more reliable learning direction and sample selection method to track the imbalance sample problem. Via extensive simulations, we show that the proposed algorithm achieves faster convergence and higher sum-rate performance compared to existing solutions.

</details>


### [479] [EEG-based Graph-guided Domain Adaptation for Robust Cross-Session Emotion Recognition](https://arxiv.org/abs/2512.23526)
*Maryam Mirzaei,Farzaneh Shayegh,Hamed Narimani*

Main category: cs.LG

TL;DR: This paper presents EGDA, a novel framework optimizing cross-session EEG-based emotion recognition with distribution alignment and graph regularization, achieving high accuracy and outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: The research aims to address the challenge of model generalization in EEG-based emotion recognition caused by variations across recording sessions.

Method: The proposed EGDA framework reduces session discrepancies through joint global and class-specific distribution alignment, incorporating graph regularization to preserve EEG structure.

Result: EGDA demonstrates superior performance in emotion recognition across three tasks with accuracies of 81.22%, 80.15%, and 83.27% on the SEED-IV dataset, outperforming baseline methods.

Conclusion: The study validates the effectiveness of EGDA in achieving robust cross-session emotion recognition and underscores the significance of the Gamma frequency band and key brain regions.

Abstract: Accurate recognition of human emotional states is critical for effective human-machine interaction. Electroencephalography (EEG) offers a reliable source for emotion recognition due to its high temporal resolution and its direct reflection of neural activity. Nevertheless, variations across recording sessions present a major challenge for model generalization. To address this issue, we propose EGDA, a framework that reduces cross-session discrepancies by jointly aligning the global (marginal) and class-specific (conditional) distributions, while preserving the intrinsic structure of EEG data through graph regularization. Experimental results on the SEED-IV dataset demonstrate that EGDA achieves robust cross-session performance, obtaining accuracies of 81.22%, 80.15%, and 83.27% across three transfer tasks, and surpassing several baseline methods. Furthermore, the analysis highlights the Gamma frequency band as the most discriminative and identifies the central-parietal and prefrontal brain regions as critical for reliable emotion recognition.

</details>


### [480] [Distribution-Free Process Monitoring with Conformal Prediction](https://arxiv.org/abs/2512.23602)
*Christopher Burger*

Main category: cs.LG

TL;DR: This paper improves Statistical Process Control (SPC) by integrating Conformal Prediction, enhancing quality control in manufacturing environments.


<details>
  <summary>Details</summary>
Motivation: Traditional SPC struggles due to reliance on assumptions often unmet in modern manufacturing, prompting a need for more robust tools.

Method: Development of a hybrid SPC framework integrating Conformal Prediction, including novel tools like Conformal-Enhanced Control Charts and Conformal-Enhanced Process Monitoring.

Result: Enhanced robustness and statistical rigor in quality control while preserving interpretability of classical SPC methods.

Conclusion: The framework provides a superior and intuitive approach to process monitoring and anomaly detection in complex settings.

Abstract: Traditional Statistical Process Control (SPC) is essential for quality management but is limited by its reliance on often violated statistical assumptions, leading to unreliable monitoring in modern, complex manufacturing environments. This paper introduces a hybrid framework that enhances SPC by integrating the distribution free, model agnostic guarantees of Conformal Prediction. We propose two novel applications: Conformal-Enhanced Control Charts, which visualize process uncertainty and enable proactive signals like 'uncertainty spikes', and Conformal-Enhanced Process Monitoring, which reframes multivariate control as a formal anomaly detection problem using an intuitive p-value chart. Our framework provides a more robust and statistically rigorous approach to quality control while maintaining the interpretability and ease of use of classic methods.

</details>


### [481] [BOAD: Discovering Hierarchical Software Engineering Agents via Bandit Optimization](https://arxiv.org/abs/2512.23631)
*Iris Xu,Guangtao Zeng,Zexue He,Charles Jin,Aldo Pareja,Dan Gutfreund,Chuang Gan,Zhang-Wei Hong*

Main category: cs.LG

TL;DR: The paper introduces BOAD, an automatic hierarchy discovery framework for multi-agent SWE task orchestration, improving upon single-agent and manually-designed systems.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with complex, real-world SWE problems due to monolithic designs forcing irrelevant context retention.

Method: The authors propose structuring SWE agents as orchestrators coordinating specialized sub-agents, using a Bandit Optimization for Agent Design (BOAD) framework based on multi-armed bandit problems.

Result: BOAD demonstrates superior performance on SWE-bench benchmarks, achieving high leaderboard rankings against models like GPT-4.

Conclusion: Hierarchical multi-agent systems discovered via BOAD show improved generalization for long-horizon SWE tasks.

Abstract: Large language models (LLMs) have shown strong reasoning and coding capabilities, yet they struggle to generalize to real-world software engineering (SWE) problems that are long-horizon and out of distribution. Existing systems often rely on a single agent to handle the entire workflow-interpreting issues, navigating large codebases, and implementing fixes-within one reasoning chain. Such monolithic designs force the model to retain irrelevant context, leading to spurious correlations and poor generalization. Motivated by how human engineers decompose complex problems, we propose structuring SWE agents as orchestrators coordinating specialized sub-agents for sub-tasks such as localization, editing, and validation. The challenge lies in discovering effective hierarchies automatically: as the number of sub-agents grows, the search space becomes combinatorial, and it is difficult to attribute credit to individual sub-agents within a team. We address these challenges by formulating hierarchy discovery as a multi-armed bandit (MAB) problem, where each arm represents a candidate sub-agent and the reward measures its helpfulness when collaborating with others. This framework, termed Bandit Optimization for Agent Design (BOAD), enables efficient exploration of sub-agent designs under limited evaluation budgets. On SWE-bench-Verified, BOAD outperforms single-agent and manually designed multi-agent systems. On SWE-bench-Live, featuring more recent and out-of-distribution issues, our 36B system ranks second on the leaderboard at the time of evaluation, surpassing larger models such as GPT-4 and Claude. These results demonstrate that automatically discovered hierarchical multi-agent systems significantly improve generalization on challenging long-horizon SWE tasks. Code is available at https://github.com/iamxjy/BOAD-SWE-Agent.

</details>


### [482] [End-to-End Test-Time Training for Long Context](https://arxiv.org/abs/2512.23675)
*Arnuv Tandon,Karan Dalal,Xinhao Li,Daniel Koceja,Marcel Rød,Sam Buchanan,Xiaolong Wang,Jure Leskovec,Sanmi Koyejo,Tatsunori Hashimoto,Carlos Guestrin,Jed McCaleb,Yejin Choi,Yu Sun*

Main category: cs.LG

TL;DR: The paper introduces a method for long-context language modeling using standard Transformer architecture with sliding-window attention and continual learning at test time.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of efficiently processing longer contexts in language models without resorting to specialized architectures.

Method: The method employs test-time training (TTT) that compresses context into weights via next-token prediction while leveraging meta-learning during training to optimize initialization for test-time learning.

Result: The proposed approach, TTT-E2E, achieves scaling behavior similar to full attention in Transformers but has constant inference latency regardless of context length. It outperforms other methods like Mamba 2 and Gated DeltaNet in test-time efficiency, being 2.7 times faster than full attention for long contexts (128K).

Conclusion: TTT-E2E offers an efficient and scalable framework for long-context language modeling, providing results matching full attention's accuracy but with superior computational efficiency.

Abstract: We formulate long-context language modeling as a problem in continual learning rather than architecture design. Under this formulation, we only use a standard architecture -- a Transformer with sliding-window attention. However, our model continues learning at test time via next-token prediction on the given context, compressing the context it reads into its weights. In addition, we improve the model's initialization for learning at test time via meta-learning at training time. Overall, our method, a form of Test-Time Training (TTT), is End-to-End (E2E) both at test time (via next-token prediction) and training time (via meta-learning), in contrast to previous forms. We conduct extensive experiments with a focus on scaling properties. In particular, for 3B models trained with 164B tokens, our method (TTT-E2E) scales with context length in the same way as Transformer with full attention, while others, such as Mamba 2 and Gated DeltaNet, do not. However, similar to RNNs, TTT-E2E has constant inference latency regardless of context length, making it 2.7 times faster than full attention for 128K context. Our code is publicly available.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [483] [CosineGate: Semantic Dynamic Routing via Cosine Incompatibility in Residual Networks](https://arxiv.org/abs/2512.22206)
*Yogeswar Reddy Thota*

Main category: cs.NE

TL;DR: This paper introduces CosineGate, a differentiable mechanism to reduce computational redundancy in residual networks by dynamically routing computations using geometric measures of feature incompatibility.


<details>
  <summary>Details</summary>
Motivation: To reduce the computational redundancy in deep residual networks, which compute unnecessary operations when identity mappings suffice for certain inputs.

Method: The paper proposes CosineGate, which dynamically decides to skip residual blocks using Cosine Incompatibility Ratio and Gumbel-Softmax for routing while incorporating FLOPs regularization for efficiency.

Result: CosineGate achieves significant FLOPs savings (up to 28.5%) with accuracy spanning or exceeding baseline (up to 93.2%) on CIFAR-10, without supervision, distillation, or heuristics.

Conclusion: CosineGate effectively optimizes computational efficiency in residual networks by leveraging geometric measures, providing a principled method for dynamic routing.

Abstract: Modern deep residual networks perform substantial redundant computation by evaluating all residual blocks for every input, even when identity mappings suffice. We introduce CosineGate, an end-to-end differentiable architecture for dynamic routing in residual networks that uses cosine incompatibility between identity and residual feature representations as a self-supervised skip signal. CosineGate measures semantic redundancy through the Cosine Incompatibility Ratio (CIR), defined as 1 - cos(x, F(x)), and uses Gumbel-Softmax relaxation to enable per-sample, per-block gating during training. A progressive FLOPs regularization term controls average compute usage without destabilizing optimization. On CIFAR-10, CosineGate spans the accuracy-efficiency Pareto frontier: an aggressive configuration achieves 89.9 percent accuracy with 24.1 percent FLOPs savings, a balanced configuration achieves 91.3 percent accuracy with 28.5 percent savings at epoch 160, and a conservative configuration reaches a peak of 93.2 percent accuracy with minimal compute reduction. These results match or exceed ResNet-20 (91.3 percent) while reducing computation, without auxiliary supervision, distillation, or task-specific heuristics. Our results demonstrate that simple geometric measures of feature incompatibility provide a principled and effective signal for dynamic residual routing.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [484] [Symbolic Specification and Reasoning for Quantum Data and Operations](https://arxiv.org/abs/2512.22383)
*Mingsheng Ying*

Main category: cs.PL

TL;DR: The paper introduces Symbolic Operator Logic (SOL), a framework for symbolic specification and reasoning in quantum computing, enabling integration with classical logic and automated tools.


<details>
  <summary>Details</summary>
Motivation: There is no formal theory for symbolic specification and reasoning about quantum data, limiting automated verification in quantum computing.

Method: The authors embed classical first-order logic into a framework called SOL which allows symbolic descriptions and recursive definitions of quantum data and operations.

Result: The framework integrates classical logic into quantum reasoning, leveraging classical automated verification tools for quantum programs.

Conclusion: SOL provides a foundation for formal verification and automated theorem proving in quantum computation, with applications in systems like Lean or Coq.

Abstract: In quantum information and computation research, symbolic methods have been widely used for human specification and reasoning about quantum states and operations. At the same time, they are essential for ensuring the scalability and efficiency of automated reasoning and verification tools for quantum algorithms and programs. However, a formal theory for symbolic specification and reasoning about quantum data and operations is still lacking, which significantly limits the practical applicability of automated verification techniques in quantum computing.
  In this paper, we present a general logical framework, called Symbolic Operator Logic $\mathbf{SOL}$, which enables symbolic specification and reasoning about quantum data and operations. Within this framework, a classical first-order logical language is embedded into a language of formal operators used to specify quantum data and operations, including their recursive definitions. This embedding allows reasoning about their properties modulo a chosen theory of the underlying classical data (e.g., Boolean algebra or group theory), thereby leveraging existing automated verification tools developed for classical computing. It should be emphasised that this embedding of classical first-order logic into $\mathbf{SOL}$ is precisely what makes the symbolic method possible.
  We envision that this framework can provide a conceptual foundation for the formal verification and automated theorem proving of quantum computation and information in proof assistants such as Lean, Coq, and related systems.

</details>


### [485] [Eliminate Branches by Melding IR Instructions](https://arxiv.org/abs/2512.22390)
*Yuze Li,Srinivasan Ramachandra Sharma,Charitha Saumya,Ali R. Butt,Kirshanthan Sundararajah*

Main category: cs.PL

TL;DR: The paper introduces MERIT, a compiler transformation to mitigate branch mispredictions by merging operations from divergent code paths, achieving significant performance improvements without hardware predication.


<details>
  <summary>Details</summary>
Motivation: Branch mispredictions degrade processor performance, and data-dependent branches with irregular patterns challenge existing prediction and transformation techniques.

Method: The authors propose MERIT, which uses sequence alignment to identify merging opportunities for structurally similar operations at the IR instruction level and employs guarding to ensure correctness. It is implemented as an LLVM compiler pass.

Result: MERIT was evaluated on 102 programs from four benchmark suites, yielding a geometric mean speedup of 10.9% and peak improvements of 32x over hardware branch predictors.

Conclusion: MERIT effectively eliminates branches and improves performance by aligning and merging divergent paths, offering a practical solution with reduced instruction overhead.

Abstract: Branch mispredictions cause catastrophic performance penalties in modern processors, leading to performance loss. While hardware predictors and profile-guided techniques exist, data-dependent branches with irregular patterns remain challenging. Traditional if-conversion eliminates branches via software predication but faces limitations on architectures like x86. It often fails on paths containing memory instructions or incurs excessive instruction overhead by fully speculating large branch bodies.
  This paper presents Melding IR Instructions (MERIT), a compiler transformation that eliminates branches by aligning and melding similar operations from divergent paths at the IR instruction level. By observing that divergent paths often perform structurally similar operations with different operands, MERIT adapts sequence alignment to discover merging opportunities and employs safe operand-level guarding to ensure semantic correctness without hardware predication. Implemented as an LLVM pass and evaluated on 102 programs from four benchmark suites, MERIT achieves a geometric mean speedup of 10.9% with peak improvements of 32x compared to hardware branch predictor, demonstrating the effectiveness with reduced static instruction overhead.

</details>


### [486] [A Bounded Game Semantics Checker for Precise Smart Contract Analysis](https://arxiv.org/abs/2512.22417)
*Vasileios Koutavas,Yu-Yang Lin,Nikos Tzevelekos*

Main category: cs.PL

TL;DR: The paper introduces YulToolkit, a bounded game-semantics tool for detecting smart contract vulnerabilities, achieving precision and effective scalability in real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: There is a need for precise, scalable methods to identify vulnerabilities, particularly in smart contracts prone to exploits like reentrancy.

Method: The paper uses game semantics to model interactions between the contract and its environment, enabling trace enumeration and avoiding over-approximation.

Result: YulToolkit successfully identifies vulnerabilities in well-known incidents and benchmarks, demonstrating its reliability and precision.

Conclusion: Bounded game-semantic exploration via YulToolkit is a robust and efficient addition to the smart contract analysis repertoire, offering precise detection and viable scalability.

Abstract: We present a new approach to finding smart contract vulnerabilities that is precise (no false positives up to our EVM-Yul interpreter), bounded-complete, and, when instrumented with domain knowledge, scales to real-world contracts. Our method is based on game semantics, modelling computation as an interaction between a contract and its environment, reducing reasoning about unknown or malicious external contracts to trace enumeration. We implement this in a tool we refer to as YulToolkit, a bounded game-semantics checker for Yul, the intermediate language of Solidity. By exploring only feasible interactions, YulToolkit avoids over-approximation, and by relying on the theory of game semantics it achieves bounded completeness. To make exploration tractable, YulToolkit supports instrumentation written in Solidity and propagated to Yul, comparable in effort to creating a test harness. Unlike tests, however, our technique explores all admissible traces within the chosen parameters and bounds. We evaluate YulToolkit on three real-world incidents: The DAO, PredyPool, and Lendf.Me, as well as benchmark contracts. In all cases, YulToolkit detects the known vulnerabilities (producing a violation-triggering trace), and after applying fixes, reports no further violations within bounds. These results show that bounded game semantics exploration is an effective and precise addition to the smart contract analysis toolbox, particularly for vulnerabilities such as reentrancy that are hard to detect precisely in real code.

</details>


### [487] [Compiling Gradual Types with Evidence](https://arxiv.org/abs/2512.22684)
*José Luis Romero,Cristóbal Isla,Matías Toro,Éric Tanter*

Main category: cs.PL

TL;DR: The paper introduces GrEv, an evidence-based compiler for gradual typing, and shows it can be competitive or faster than the existing coercion-based Grift compiler while opening avenues for advanced gradual typing disciplines.


<details>
  <summary>Details</summary>
Motivation: To assess whether evidence-based semantics, derived from Abstracting Gradual Typing (AGT), can be efficiently implemented for gradual typing in languages with structural types.

Method: Design, implement, and empirically evaluate the GrEv compiler, bridging AGT-inspired semantics to practical implementation and benchmarking against the Grift compiler.

Result: GrEv demonstrates competitive performance, often surpassing the coercion-based Grift compiler while maintaining stability across static-dynamic typing spectrum configurations.

Conclusion: Evidence-based semantics are a viable and efficient foundation for implementations of gradual typing, enriching compiler designs and enabling further exploration of AGT-driven gradual typing disciplines.

Abstract: Efficiently supporting sound gradual typing in a language with structural types is challenging. To date, the Grift compiler is the only close-to-the-metal implementation of gradual typing in this setting, exploiting coercions for runtime checks, and further extended with monotonic references for efficient access to statically-typed data structures. On the language design and semantics side, the Abstracting Gradual Typing (AGT) methodology has proven fruitful to elucidate existing designs and to innovate by deriving gradualizations of a wide variety of typing disciplines and language features. Grounded in abstract interpretation, the Curry-Howard inspired runtime semantics of AGT is based on the notion of evidence for consistent judgments that evolve during reduction, monitoring the plausibility of well-typedness. While expressive and versatile, it is unclear whether such evidence-based semantics are a viable route to realize an efficient implementation of gradual typing.
  In this work, we explore this question by designing, implementing, and evaluating an evidence-based compiler, called GrEv. We explain how to bridge the gap between the formal semantics and the GrEv compiler implementation, and identify novel monotonic semantics. We empirically evaluate the performance of GrEv on the Grift benchmark suite. The results show that an evidence-based compiler can be competitive with, and even faster than, a coercion-based compiler, exhibiting more stability across configurations on the static-to-dynamic spectrum. In addition to enriching the space of gradual typing compilers, this work opens a direct door to exploring efficient implementations of the many advanced gradual typing disciplines formally derived with AGT in the literature.

</details>


### [488] [Fancy Some Chips for Your TeaStore? Modeling the Control of an Adaptable Discrete System](https://arxiv.org/abs/2512.23496)
*Anna Gallone,Simon Bliudze,Sophie Cerf,Olga Kouchnarenko*

Main category: cs.PL

TL;DR: The paper introduces Chips, a language aimed at creating robust component-based systems for web application design using principles from control theory and programming languages.


<details>
  <summary>Details</summary>
Motivation: Developers face challenges in designing web applications due to the complexity of constraints across software, hardware, network, and other resources.

Method: Chips combines control theory and general-purpose programming language concepts to describe applications as functional blocks and systematically design models.

Result: Chips facilitates the modeling and analysis of complex systems, demonstrated through its application on the Adaptable TeaStore project.

Conclusion: Chips provides a systematic approach to designing robust web applications, addressing resource interdependencies effectively.

Abstract: When designing new web applications, developers must cope with different kinds of constraints relative to the resources they rely on: software, hardware, network, online micro-services, or any combination of the mentioned entities. Together, these entities form a complex system of communicating interdependent processes, physical or logical. It is very desirable that such system ensures its robustness to provide a good quality of service. In this paper we introduce Chips, a language that aims at facilitating the design of models made of various entwined components. It allows the description of applications in the form of functional blocks. Chips mixes notions  from control theory and general purpose programming languages to generate robust component-based models. This paper presents how to use Chips to systematically design, model and analyse a complex system project, using a variation of the Adaptable TeaStore application as running example.

</details>


### [489] [Adaptable TeaStore: A Choreographic Approach](https://arxiv.org/abs/2512.23497)
*Giuseppe De Palma,Saverio Giallorenzo,Ivan Lanese,Gianluigi Zavattaro*

Main category: cs.PL

TL;DR: The paper discusses using AIOCJ, a choreographic programming language, to implement the Adaptable TeaStore, a microservice reference model, showcasing its dynamic adaptation abilities and limitations.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore the use of adaptable microservice architectures, focusing on dynamic transitions between configurations while ensuring communication correctness.

Method: The authors implemented the Adaptable TeaStore using AIOCJ, a choreographic programming language, which allows runtime adaptations and ensures communication correctness.

Result: The study demonstrates how AIOCJ enables adaptation in multiparty systems and identifies its strengths and limitations in modeling cloud architectures.

Conclusion: The authors conclude that AIOCJ provides strong benefits for dynamic adaptation in microservices, but enhancements are needed for better applicability to real-world cloud systems.

Abstract: The Adaptable TeaStore has recently been proposed as a reference model for adaptable microservice architectures. It includes different configurations, as well as scenarios requiring to transition between them. We describe an implementation of the Adaptable TeaStore based on AIOCJ, a choreographic language that allows one to program multiparty systems that can adapt at runtime to different conditions. Following the choreographic tradition, AIOCJ ensures by-construction correctness of communications (e.g., no deadlocks) before, during, and after adaptation. Adaptation is dynamic, and the adaptation scenarios need to be fully specified only at runtime. Using AIOCJ to model the Adaptable TeaStore, we showcase the strengths of the approach and its current limitations, providing suggestions for future directions for refining the paradigm (and the AIOCJ language, in particular), to better align it with real-world Cloud architectures.

</details>


### [490] [Beyond Per-Thread Lock Sets: Multi-Thread Critical Sections and Dynamic Deadlock Prediction](https://arxiv.org/abs/2512.23552)
*Martin Sulzmann*

Main category: cs.PL

TL;DR: The paper improves the standard lock set model for dynamic deadlock analysis by introducing multi-thread critical sections, reducing false positives and negatives, and maintaining computational efficiency.


<details>
  <summary>Details</summary>
Motivation: The standard per-thread lock set approach causes false positives and negatives in deadlock detection due to its single-thread critical section constraint.

Method: Proposed trace-based characterization for critical sections spanning multiple threads and sound approximation via partial order relations.

Result: Improved lock set construction integrated into SPDOffline, providing more precise deadlock predictions without compromising performance.

Conclusion: The extensions offer enhanced precision and completeness for deadlock prediction while ensuring soundness and efficient computation.

Abstract: Lock sets are commonly used for dynamic analysis of deadlocks. The standard per-thread lock set construction only considers locks acquired in the same thread, but is unaware of locks acquired in another thread. This leads to false positives and false negatives. The underlying issue is that the commonly used notion of a critical section on which the lock set construction relies ignores events from other threads. We give a trace-based characterization of critical sections that drops this restriction. Critical sections are no longer restricted to a single thread and can cover multiple threads. Such forms of critical sections exist, are natural, and correct the standard formulation.
  We show how to soundly approximate the trace-based characterization via partial order relations. Thus, we obtain an improved lock set construction that can still be efficiently computed and allows us to remove false positives reported by the DIRK deadlock predictor and remove false negatives by extending the SPDOffline deadlock predictor. We integrate various lock set constructions with increased precision in an extension of SPDOffline. Our extensions remain sound (no false positives) but are more complete (fewer false negatives) w.r.t. SPDOffline. For an extensive standard benchmark suite we can also show that the performance is not affected.

</details>


### [491] [Automating the Analysis of Parsing Algorithms (and other Dynamic Programs)](https://arxiv.org/abs/2512.23665)
*Tim Vieira,Ryan Cotterell,Jason Eisner*

Main category: cs.PL

TL;DR: The paper introduces a system for analyzing NLP algorithms to infer types, detect code inefficiencies, and calculate runtime and space complexities.


<details>
  <summary>Details</summary>
Motivation: Algorithm developers in NLP seek to ensure efficiency in manipulating formal structures and need tools to analyze properties like runtime, space complexity, and type correctness.

Method: The authors propose and implement a system that helps algorithm designers analyze NLP algorithms based on specific properties, such as types and computational complexities.

Result: The system was successfully applied to several NLP algorithms, inferring code inefficiencies, types, and computational bounds.

Conclusion: The paper's system proves useful in evaluating and refining NLP algorithms, offering insights into their efficiency and correctness.

Abstract: Much algorithmic research in NLP aims to efficiently manipulate rich formal structures. An algorithm designer typically seeks to provide guarantees about their proposed algorithm -- for example, that its running time or space complexity is upper-bounded as a certain function of its input size. They may also wish to determine the necessary properties of the quantities derived by the algorithm to synthesize efficient data structures and verify type errors. In this paper, we develop a system for helping programmers to perform these types of analyses. We apply our system to a number of NLP algorithms and find that it successfully infers types, dead and redundant code, and parametric runtime and space complexity bounds.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [492] [Joint UAV-UGV Positioning and Trajectory Planning via Meta A3C for Reliable Emergency Communications](https://arxiv.org/abs/2512.22187)
*Ndagijimana Cyprien,Mehdi Sookhak,Hosein Zarini,Chandra N Sekharan,Mohammed Atiquzzaman*

Main category: cs.RO

TL;DR: The paper proposes a method for optimal UAV and UGV deployment in disaster areas to ensure Quality of Services for ground users, using Meta-A3C for trajectory planning.


<details>
  <summary>Details</summary>
Motivation: There is a need to optimize UAV and UGV deployment in disaster-affected areas to maintain communication services with high QoS, minimizing the number of UAVs used.

Method: The authors introduce a joint framework for UAV and UGV trajectory planning, use a road graph to model UGV mobility, and implement Meta-A3C—a meta-learning-enhanced asynchronous algorithm for solving the optimization problem.

Result: Meta-A3C delivers 13.1% higher throughput and 49% faster execution compared to alternatives like A3C and DDPG, while ensuring QoS requirements.

Conclusion: Deploying Meta-A3C in UAV and UGV positioning enhances throughput and efficiency compared to existing methods, effectively supporting ground users' QoS needs in dynamic disaster scenarios.

Abstract: Joint deployment of unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs) has been shown to be an effective method to establish communications in areas affected by disasters. However, ensuring good Quality of Services (QoS) while using as few UAVs as possible also requires optimal positioning and trajectory planning for UAVs and UGVs. This paper proposes a joint UAV-UGV-based positioning and trajectory planning framework for UAVs and UGVs deployment that guarantees optimal QoS for ground users. To model the UGVs' mobility, we introduce a road graph, which directs their movement along valid road segments and adheres to the road network constraints. To solve the sum rate optimization problem, we reformulate the problem as a Markov Decision Process (MDP) and propose a novel asynchronous Advantage Actor Critic (A3C) incorporated with meta-learning for rapid adaptation to new environments and dynamic conditions. Numerical results demonstrate that our proposed Meta-A3C approach outperforms A3C and DDPG, delivering 13.1\% higher throughput and 49\% faster execution while meeting the QoS requirements.

</details>


### [493] [VL-LN Bench: Towards Long-horizon Goal-oriented Navigation with Active Dialogs](https://arxiv.org/abs/2512.22342)
*Wensi Huang,Shaohao Zhu,Meng Wei,Jinming Xu,Xihui Liu,Hanqing Wang,Tai Wang,Feng Zhao,Jiangmiao Pang*

Main category: cs.RO

TL;DR: The paper introduces Interactive Instance Object Navigation (IION), enhancing embodied navigation by integrating active dialog capabilities and presents the large-scale VL-LN benchmark to train and evaluate dialog-enabled navigation models.


<details>
  <summary>Details</summary>
Motivation: Existing embodied navigation tasks focus on idealized conditions with clear instructions, neglecting the ambiguity of real-world scenarios requiring active dialog to clarify user intent.

Method: The authors propose IION, enabling natural language interaction during Instance Object Navigation, and create VL-LN, a large-scale dataset with evaluation protocol for training dialog-capable navigation models.

Result: Experiments using VL-LN show significant improvements in navigation models equipped with dialog capabilities, outperforming baselines.

Conclusion: VL-LN advances embodied navigation research by facilitating dialog-enabled capability development and demonstrating its effectiveness through extensive evaluations.

Abstract: In most existing embodied navigation tasks, instructions are well-defined and unambiguous, such as instruction following and object searching. Under this idealized setting, agents are required solely to produce effective navigation outputs conditioned on vision and language inputs. However, real-world navigation instructions are often vague and ambiguous, requiring the agent to resolve uncertainty and infer user intent through active dialog. To address this gap, we propose Interactive Instance Object Navigation (IION), a task that requires agents not only to generate navigation actions but also to produce language outputs via active dialog, thereby aligning more closely with practical settings. IION extends Instance Object Navigation (ION) by allowing agents to freely consult an oracle in natural language while navigating. Building on this task, we present the Vision Language-Language Navigation (VL-LN) benchmark, which provides a large-scale, automatically generated dataset and a comprehensive evaluation protocol for training and assessing dialog-enabled navigation models. VL-LN comprises over 41k long-horizon dialog-augmented trajectories for training and an automatic evaluation protocol with an oracle capable of responding to agent queries. Using this benchmark, we train a navigation model equipped with dialog capabilities and show that it achieves significant improvements over the baselines. Extensive experiments and analyses further demonstrate the effectiveness and reliability of VL-LN for advancing research on dialog-enabled embodied navigation. Code and dataset: https://0309hws.github.io/VL-LN.github.io/

</details>


### [494] [A Unified AI, Embedded, Simulation, and Mechanical Design Approach to an Autonomous Delivery Robot](https://arxiv.org/abs/2512.22408)
*Amro Gamar,Ahmed Abduljalil,Alargam Mohammed,Ali Elhenidy,Abeer Tawakol*

Main category: cs.RO

TL;DR: This paper develops a fully autonomous delivery robot combining mechanical engineering, embedded systems, and AI, achieving reliable real-world deployment.


<details>
  <summary>Details</summary>
Motivation: To create a robust, real-world deployable autonomous delivery robot optimizing mechanical, computational, and communication aspects.

Method: Integrated a heterogeneous computing architecture utilizing RPi 5 with ROS 2 for AI and path planning, and ESP32 for real-time motor control, solving challenges like computation optimization and reliable communication.

Result: Achieved robust, deterministic motor control using PID, optimized performance on limited resources, reliable communication, and system monitoring via AWS IoT.

Conclusion: The research established a unified methodology for designing and deploying a fully functional and reliable autonomous delivery robot for practical use.

Abstract: This paper presents the development of a fully autonomous delivery robot integrating mechanical engineering, embedded systems, and artificial intelligence. The platform employs a heterogeneous computing architecture, with RPi 5 and ROS 2 handling AI-based perception and path planning, while ESP32 running FreeRTOS ensures real-time motor control. The mechanical design was optimized for payload capacity and mobility through precise motor selection and material engineering. Key technical challenges addressed include optimizing computationally intensive AI algorithms on a resource-constrained platform and implementing a low-latency, reliable communication link between the ROS 2 host and embedded controller. Results demonstrate deterministic, PID-based motor control through rigorous memory and task management, and enhanced system reliability via AWS IoT monitoring and a firmware-level motor shutdown failsafe. This work highlights a unified, multi-disciplinary methodology, resulting in a robust and operational autonomous delivery system capable of real-world deployment.

</details>


### [495] [Emergence of Human to Robot Transfer in Vision-Language-Action Models](https://arxiv.org/abs/2512.22414)
*Simar Kareer,Karl Pertsch,James Darpinian,Judy Hoffman,Danfei Xu,Sergey Levine,Chelsea Finn,Suraj Nair*

Main category: cs.RO

TL;DR: The paper explores using human videos for training vision-language-action (VLA) models, proposing a co-training recipe and showing that diverse robot pre-training enables effective human-to-robot transfer.


<details>
  <summary>Details</summary>
Motivation: To determine if human video data, abundant and diverse, can facilitate human-to-robot skill transfer in VLA models without excessive manual engineering.

Method: The authors introduce a co-training recipe for VLAs and analyze how diverse robot pre-training can lead to embodiment-agnostic representations, enabling models to generalize from human to robot tasks.

Result: The proposed method achieves nearly double the performance on generalization tasks derived from human data after sufficient robot pre-training.

Conclusion: Training VLAs with diverse data enables emergent human-to-robot transfer, demonstrating the potential of leveraging human videos for generalizable robotic skill acquisition.

Abstract: Vision-language-action (VLA) models can enable broad open world generalization, but require large and diverse datasets. It is appealing to consider whether some of this data can come from human videos, which cover diverse real-world situations and are easy to obtain. However, it is difficult to train VLAs with human videos alone, and establishing a mapping between humans and robots requires manual engineering and presents a major research challenge. Drawing inspiration from advances in large language models, where the ability to learn from diverse supervision emerges with scale, we ask whether a similar phenomenon holds for VLAs that incorporate human video data. We introduce a simple co-training recipe, and find that human-to-robot transfer emerges once the VLA is pre-trained on sufficient scenes, tasks, and embodiments. Our analysis suggests that this emergent capability arises because diverse pretraining produces embodiment-agnostic representations for human and robot data. We validate these findings through a series of experiments probing human to robot skill transfer and find that with sufficiently diverse robot pre-training our method can nearly double the performance on generalization settings seen only in human data.

</details>


### [496] [Bugs with Features: Vision-Based Fault-Tolerant Collective Motion Inspired by Nature](https://arxiv.org/abs/2512.22448)
*Peleg Shefi,Amir Ayali,Gal A. Kaminka*

Main category: cs.RO

TL;DR: This paper enhances robustness in artificial swarms during collective motion by introducing methods inspired by locust studies: improved visual distance estimation and intermittent locomotion.


<details>
  <summary>Details</summary>
Motivation: Artificial swarms struggle with brittleness and ambiguity, especially when relying on visual perception, which contrasts the robustness of natural collective motion.

Method: The paper proposes a robust distance estimation combining perceived horizontal and vertical sizes, along with intermittent locomotion for reliably identifying and avoiding faulty robots.

Result: Physics-based simulations demonstrated significant improvements in swarm resilience across diverse experimental configurations using the proposed methodologies.

Conclusion: The mechanisms outlined ensure robust collective motion for artificial swarms by addressing issues of faulty robot detection and perception ambiguities, with broader applicability across different types of swarm models.

Abstract: In collective motion, perceptually-limited individuals move in an ordered manner, without centralized control. The perception of each individual is highly localized, as is its ability to interact with others. While natural collective motion is robust, most artificial swarms are brittle. This particularly occurs when vision is used as the sensing modality, due to ambiguities and information-loss inherent in visual perception. This paper presents mechanisms for robust collective motion inspired by studies of locusts. First, we develop a robust distance estimation method that combines visually perceived horizontal and vertical sizes of neighbors. Second, we introduce intermittent locomotion as a mechanism that allows robots to reliably detect peers that fail to keep up, and disrupt the motion of the swarm. We show how such faulty robots can be avoided in a manner that is robust to errors in classifying them as faulty. Through extensive physics-based simulation experiments, we show dramatic improvements to swarm resilience when using these techniques. We show these are relevant to both distance-based Avoid-Attract models, as well as to models relying on Alignment, in a wide range of experiment settings.

</details>


### [497] [Asymmetric Friction in Geometric Locomotion](https://arxiv.org/abs/2512.22484)
*Ross L. Hatton,Yousef Salaman,Shai Revzen*

Main category: cs.RO

TL;DR: This paper extends geometric mechanics models of locomotion from Riemannian dynamics to more general Finslerian dynamics with asymmetric drag properties.


<details>
  <summary>Details</summary>
Motivation: To explore how systems with asymmetric and anisotropic drag properties can be modeled effectively for locomotion and better understand friction-induced constraints.

Method: The authors utilize a sub-Finslerian approach to extend the sub-Riemannian motility map framework, incorporating asymmetric and anisotropic drag coefficients into locomotion modeling.

Result: The study identifies analogous properties for Finslerian systems and demonstrates how the motility map framework can naturally integrate asymmetry in friction.

Conclusion: This approach generalizes locomotion modeling to include systems with non-Riemannian drag properties, enhancing the understanding of friction's role in system motion.

Abstract: Geometric mechanics models of locomotion have provided insight into how robots and animals use environmental interactions to convert internal shape changes into displacement through the world, encoding this relationship in a ``motility map''. A key class of such motility maps arises from (possibly anisotropic) linear drag acting on the system's individual body parts, formally described via Riemannian metrics on the motions of the system's individual body parts. The motility map can then be generated by invoking a sub-Riemannian constraint on the aggregate system motion under which the position velocity induced by a given shape velocity is that which minimizes the power dissipated via friction. The locomotion of such systems is ``geometric'' in the sense that the final position reached by the system depends only on the sequence of shapes that the system passes through, but not on the rate with which the shape changes are made.
  In this paper, we consider a far more general class of systems in which the drag may be not only anisotropic (with different coefficients for forward/backward and left/right motions), but also asymmetric (with different coefficients for forward and backward motions). Formally, including asymmetry in the friction replaces the Riemannian metrics on the body parts with Finsler metrics. We demonstrate that the sub-Riemannian approach to constructing the system motility map extends naturally to a sub-Finslerian approach and identify system properties analogous to the constraint curvature of sub-Riemannian systems that allow for the characterization of the system motion capabilities.

</details>


### [498] [Topology-Preserving Scalar Field Optimization for Boundary-Conforming Spiral Toolpaths on Multiply Connected Freeform Surfaces](https://arxiv.org/abs/2512.22502)
*Shen Changqing,Xu Bingzhou,Qi Bosong,Zhang Xiaojian,Yan Sijie,Ding Han*

Main category: cs.RO

TL;DR: The paper proposes an efficient optimization strategy for ball-end milling path planning aimed at improving machining quality and efficiency on multiply connected surfaces, addressing issues like scalar-field singularities and zero-gradient discontinuities.


<details>
  <summary>Details</summary>
Motivation: To address challenges in maintaining boundary conformity and avoiding scalar-field singularities that disrupt toolpath continuity on multiply connected freeform surfaces in automotive and aerospace manufacturing.

Method: The strategy uses conformal slit mapping to construct a singularity-free scalar field and reformulates optimization as topology-preserving mesh deformation with boundary-synchronous updates.

Result: The proposed method demonstrated a 14.24% increase in machining efficiency, 5.70% improvement in scallop-height uniformity, and over 10% reduction in milling-induced vibrations compared to the existing conformal slit mapping approach.

Conclusion: The approach successfully generates continuous, boundary-conforming toolpaths free from self-intersections and offers improvements in machining efficiency, trajectory smoothness, and vibration mitigation with broad applicability in advanced manufacturing.

Abstract: Ball-end milling path planning on multiply connected freeform surfaces is pivotal for high-quality and efficient machining of components in automotive and aerospace manufacturing. Although scalar-field-based optimization provides a unified framework for multi-objective toolpath generation, maintaining boundary conformity while eliminating zero-gradient singularities that cause iso-curve branching or termination and disrupt toolpath continuity remains challenging on multiply connected surfaces. We propose an efficient strategy to robustly enforce these constraints throughout optimization. Conformal slit mapping is employed to construct a feasible, singularity-free initial scalar field. The optimization is reformulated as a topology-preserving mesh deformation governed by boundary-synchronous updates, enabling globally optimized spacing, scallop-height uniformity, and smooth trajectory transitions. Consequently, the toolpaths are continuous, boundary-conforming, and free of self-intersections. Milling experiments demonstrate that, compared with a state-of-the-art conformal slit mapping-based method, the proposed approach increases machining efficiency by 14.24%, improves scallop-height uniformity by 5.70%, and reduces milling impact-induced vibrations by over 10%. The strategy offers broad applicability in high-performance machining scenarios.

</details>


### [499] [The body is not there to compute: Comment on "Informational embodiment: Computational role of information structure in codes and robots" by Pitti et al](https://arxiv.org/abs/2512.22868)
*Matej Hoffmann*

Main category: cs.RO

TL;DR: The paper explores the application of computation and information theory to understanding and designing bodies but argues that bodies primarily do not serve to compute.


<details>
  <summary>Details</summary>
Motivation: The aim is to comprehend why animal bodies have evolved in specific ways and how these principles can inform the optimal design of robotic bodies.

Method: The study uses the theoretical framework of computation and information theory to analyze animal bodies and design robotic forms.

Result: The research questions the prevailing paradigm by suggesting that the primary purpose of bodies may not be computational.

Conclusion: While employing computation and information theory provides insights into body evolution and design, bodies should not be predominantly viewed as computational entities.

Abstract: Applying the lens of computation and information has been instrumental in driving the technological progress of our civilization as well as in empowering our understanding of the world around us. The digital computer was and for many still is the leading metaphor for how our mind operates. Information theory (IT) has also been important in our understanding of how nervous systems encode and process information. The target article deploys information and computation to bodies: to understand why they have evolved in particular ways (animal bodies) and to design optimal bodies (robots). In this commentary, I argue that the main role of bodies is not to compute.

</details>


### [500] [Clutter-Resistant Vision-Language-Action Models through Object-Centric and Geometry Grounding](https://arxiv.org/abs/2512.22519)
*Khoa Vo,Taisei Hanyu,Yuki Ikebe,Trong Thang Pham,Nhat Chung,Minh Nhat Vu,Duy Nguyen Ho Minh,Anh Nguyen,Anthony Gunderman,Chase Rainwater,Ngan Le*

Main category: cs.RO

TL;DR: This paper introduces OBEYED-VLA, a framework that improves robotic manipulation by disentangling perception from action reasoning, focusing on object-centric and geometry-aware grounding.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address issues with current Vision-Language-Action (VLA) models, which struggle to maintain robust language-conditioned grounding in cluttered or absent-target scenarios, often due to entangled perception and action pipelines.

Method: OBEYED-VLA augments VLA systems with a perception module that processes multi-view inputs into task-conditioned, object-centric, and geometry-aware observations. The framework combines VLM-based semantic grounding and geometric 3D grounding before feeding the refined data to a pretrained VLA policy fine-tuned with single-object demonstrations.

Result: OBEYED-VLA achieves significant robustness improvements on a UR10e robotic tabletop setup across cluttered environments, distractor presence, absent-target conditions, and unseen-object manipulation. Ablation studies validate the importance of both semantic and geometric grounding.

Conclusion: Incorporating explicit, object-centric perceptual modules is crucial for enhancing and generalizing the performance of VLA-based robotic systems for manipulation tasks.

Abstract: Recent Vision-Language-Action (VLA) models have made impressive progress toward general-purpose robotic manipulation by post-training large Vision-Language Models (VLMs) for action prediction. Yet most VLAs entangle perception and control in a monolithic pipeline optimized purely for action, which can erode language-conditioned grounding. In our real-world tabletop tests, policies over-grasp when the target is absent, are distracted by clutter, and overfit to background appearance.
  To address these issues, we propose OBEYED-VLA (OBject-centric and gEometrY groundED VLA), a framework that explicitly disentangles perceptual grounding from action reasoning. Instead of operating directly on raw RGB, OBEYED-VLA augments VLAs with a perception module that grounds multi-view inputs into task-conditioned, object-centric, and geometry-aware observations. This module includes a VLM-based object-centric grounding stage that selects task-relevant object regions across camera views, along with a complementary geometric grounding stage that emphasizes the 3D structure of these objects over their appearance. The resulting grounded views are then fed to a pretrained VLA policy, which we fine-tune exclusively on single-object demonstrations collected without environmental clutter or non-target objects.
  On a real-world UR10e tabletop setup, OBEYED-VLA substantially improves robustness over strong VLA baselines across four challenging regimes and multiple difficulty levels: distractor objects, absent-target rejection, background appearance changes, and cluttered manipulation of unseen objects. Ablation studies confirm that both semantic grounding and geometry-aware grounding are critical to these gains. Overall, the results indicate that making perception an explicit, object-centric component is an effective way to strengthen and generalize VLA-based robotic manipulation.

</details>


### [501] [VLA-Arena: An Open-Source Framework for Benchmarking Vision-Language-Action Models](https://arxiv.org/abs/2512.22539)
*Borong Zhang,Jiahao Li,Jiachen Shen,Yishuai Cai,Yuhao Zhang,Yuanpei Chen,Juntao Dai,Jiaming Ji,Yaodong Yang*

Main category: cs.RO

TL;DR: The paper introduces VLA-Arena, a benchmark to evaluate and address limitations in Vision-Language-Action (VLA) models, providing tools and datasets for analysis and reproducibility.


<details>
  <summary>Details</summary>
Motivation: There is difficulty in quantitatively analyzing the limits and failure modes of Vision-Language-Action (VLA) models as they advance toward generalist robot policies.

Method: The authors developed VLA-Arena with structured task design for measuring model capabilities across task structure, language commands, and visual observations. They used 170 tasks with different difficulty levels and orthogonal perturbations for robustness evaluation.

Result: State-of-the-art VLAs showed critical limitations such as memorization biases, asymmetric robustness, neglect of safety constraints, and inability for long-horizon task composition.

Conclusion: VLA-Arena provides a comprehensive toolset to address VLA challenges, enabling reproducible measurements and fostering advancements in model generalization, robustness, and safety.

Abstract: While Vision-Language-Action models (VLAs) are rapidly advancing towards generalist robot policies, it remains difficult to quantitatively understand their limits and failure modes. To address this, we introduce a comprehensive benchmark called VLA-Arena. We propose a novel structured task design framework to quantify difficulty across three orthogonal axes: (1) Task Structure, (2) Language Command, and (3) Visual Observation. This allows us to systematically design tasks with fine-grained difficulty levels, enabling a precise measurement of model capability frontiers. For Task Structure, VLA-Arena's 170 tasks are grouped into four dimensions: Safety, Distractor, Extrapolation, and Long Horizon. Each task is designed with three difficulty levels (L0-L2), with fine-tuning performed exclusively on L0 to assess general capability. Orthogonal to this, language (W0-W4) and visual (V0-V4) perturbations can be applied to any task to enable a decoupled analysis of robustness. Our extensive evaluation of state-of-the-art VLAs reveals several critical limitations, including a strong tendency toward memorization over generalization, asymmetric robustness, a lack of consideration for safety constraints, and an inability to compose learned skills for long-horizon tasks. To foster research addressing these challenges and ensure reproducibility, we provide the complete VLA-Arena framework, including an end-to-end toolchain from task definition to automated evaluation and the VLA-Arena-S/M/L datasets for fine-tuning. Our benchmark, data, models, and leaderboard are available at https://vla-arena.github.io.

</details>


### [502] [ParaMaP: Parallel Mapping and Collision-free Motion Planning for Reactive Robot Manipulation](https://arxiv.org/abs/2512.22575)
*Xuewei Zhang,Bailing Tian,Kai Zheng,Yulin Hui,Junjie Lu,Zhiyu Li*

Main category: cs.RO

TL;DR: This paper presents a motion planning framework for robots in unknown environments, combining Euclidean Distance Transform (EDT) mapping with Sampling-based Model Predictive Control (SMPC) to ensure real-time collision-free navigation.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in enabling robotic manipulators to perform real-time collision-free motion planning in dynamic and unknown environments, due to the constant need for online perception updates and replanning.

Method: The approach uses a parallel framework integrating GPU-accelerated EDT for environment mapping with SMPC for motion planning. A dense distance-field representation and robot-masked update mechanism are employed to avoid self-collision. Motion is optimized with stochastic methods and a tracking metric on SE(3) to ensure accuracy and speed.

Result: The framework was validated with extensive simulations and real-world experiments using a 7-DoF manipulator, demonstrating its high-frequency replanning capability and robust, efficient performance in dynamic environments.

Conclusion: This framework provides a comprehensive real-time motion planning solution for robotic manipulation in unknown environments, leveraging GPU acceleration and integration of EDT mapping and SMPC. It achieves reliable collision avoidance and efficient target pose tracking.

Abstract: Real-time and collision-free motion planning remains challenging for robotic manipulation in unknown environments due to continuous perception updates and the need for frequent online replanning. To address these challenges, we propose a parallel mapping and motion planning framework that tightly integrates Euclidean Distance Transform (EDT)-based environment representation with a sampling-based model predictive control (SMPC) planner. On the mapping side, a dense distance-field-based representation is constructed using a GPU-based EDT and augmented with a robot-masked update mechanism to prevent false self-collision detections during online perception. On the planning side, motion generation is formulated as a stochastic optimization problem with a unified objective function and efficiently solved by evaluating large batches of candidate rollouts in parallel within a SMPC framework, in which a geometrically consistent pose tracking metric defined on SE(3) is incorporated to ensure fast and accurate convergence to the target pose. The entire mapping and planning pipeline is implemented on the GPU to support high-frequency replanning. The effectiveness of the proposed framework is validated through extensive simulations and real-world experiments on a 7-DoF robotic manipulator. More details are available at: https://zxw610.github.io/ParaMaP.

</details>


### [503] [Modeling of UAV Tether Aerodynamics for Real-Time Simulation](https://arxiv.org/abs/2512.22588)
*Max Beffert,Andreas Zell*

Main category: cs.RO

TL;DR: This paper addresses the limitation of multirotor UAVs' flight time by proposing real-time tether modeling methods considering aerodynamic effects for grounded, tethered systems.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enable continuous operation of multirotor UAVs by using ground-powered tethering, especially in challenging conditions like fast-moving base vehicles or strong winds.

Method: Two tether modeling approaches are proposed: an analytical method leveraging catenary theory with uniform drag and a numerical method using segment discretization solved via CasADi and IPOPT, achieving real-time performance.

Result: Both methods achieve real-time tether modeling, validated through real-world tests. Analytical method provides quick solutions (<1 ms) for typical scenarios, while numerical method ensures higher physical accuracy (solve time ~5 ms) and flexibility.

Conclusion: The presented framework offers lightweight and extensible tools for real-time tether simulation suitable for optimizing UAV performance in offline and online tasks like simulation, control, and trajectory planning.

Abstract: One of the main limitations of multirotor UAVs is their short flight time due to battery constraints. A practical solution for continuous operation is to power the drone from the ground via a tether. While this approach has been demonstrated for stationary systems, scenarios with a fast-moving base vehicle or strong wind conditions require modeling the tether forces, including aerodynamic effects. In this work, we propose two complementary approaches for real-time quasi-static tether modeling with aerodynamics. The first is an analytical method based on catenary theory with a uniform drag assumption, achieving very fast solve times below 1ms. The second is a numerical method that discretizes the tether into segments and lumped masses, solving the equilibrium equations using CasADi and IPOPT. By leveraging initialization strategies, such as warm starting and analytical initialization, real-time performance was achieved with a solve time of 5ms, while allowing for flexible force formulations. Both approaches were validated in real-world tests using a load cell to measure the tether force. The results show that the analytical method provides sufficient accuracy for most tethered UAV applications with minimal computational cost, while the numerical method offers higher flexibility and physical accuracy when required. These approaches form a lightweight and extensible framework for real-time tether simulation, applicable to both offline optimization and online tasks such as simulation, control, and trajectory planning.

</details>


### [504] [Sistema de navegación de cobertura para vehículos no holonómicos en ambientes de exterior](https://arxiv.org/abs/2512.22734)
*Michelle Valenzuela,Francisco Leiva,Javier Ruiz-del-Solar*

Main category: cs.RO

TL;DR: This paper proposes a coverage navigation system for a non-holonomic robot aimed at automating the coverage tasks, especially in mining industry scenarios.


<details>
  <summary>Details</summary>
Motivation: The motivation is to automate coverage tasks to enhance safety and ensure efficiency, addressing scenarios like cleaning tasks, material movement processes, and construction in the mining industry.

Method: The paper introduces a coverage navigation system that calculates routes for area coverage and incorporates recovery behaviors for handling dynamic or unmapped obstacles.

Result: The system demonstrated near 90% coverage in simulated and real outdoor environments.

Conclusion: The coverage navigation system proves to be a promising concept for automating coverage tasks, with plans to scale the technology for mining vehicles and validate its operation in real-world scenarios.

Abstract: In mobile robotics, coverage navigation refers to the deliberate movement of a robot with the purpose of covering a certain area or volume. Performing this task properly is fundamental for the execution of several activities, for instance, cleaning a facility with a robotic vacuum cleaner. In the mining industry, it is required to perform coverage in several unit processes related with material movement using industrial machinery, for example, in cleaning tasks, in dumps, and in the construction of tailings dam walls. The automation of these processes is fundamental to enhance the security associated with their execution. In this work, a coverage navigation system for a non-holonomic robot is presented. This work is intended to be a proof of concept for the potential automation of various unit processes that require coverage navigation like the ones mentioned before. The developed system includes the calculation of routes that allow a mobile platform to cover a specific area, and incorporates recovery behaviors in case that an unforeseen event occurs, such as the arising of dynamic or previously unmapped obstacles in the terrain to be covered, e.g., other machines or pedestrians passing through the area, being able to perform evasive maneuvers and post-recovery to ensure a complete coverage of the terrain. The system was tested in different simulated and real outdoor environments, obtaining results near 90% of coverage in the majority of experiments. The next step of development is to scale up the utilized robot to a mining machine/vehicle whose operation will be validated in a real environment. The result of one of the tests performed in the real world can be seen in the video available in https://youtu.be/gK7_3bK1P5g.

</details>


### [505] [Active Constraint Learning in High Dimensions from Demonstrations](https://arxiv.org/abs/2512.22757)
*Zheng Qiu,Chih-Yuan Chiu,Glen Chou*

Main category: cs.RO

TL;DR: This paper introduces a method to efficiently learn constraints in an environment by requesting only essential demonstration data.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency of learning constraints in an environment by minimizing the number of required demonstration trajectories for inference.

Method: The method involves an iterative process that uses Gaussian processes (GPs) to identify constraints from demonstrator data, querying specific state or goal pairs to generate new informative demonstrations.

Result: The proposed method performed better than a random-sampling baseline in experiments involving high-dimensional nonlinear dynamics, achieving more accurate constraint inference with a small, focused set of demonstrations.

Conclusion: The iterative ACL algorithm is an effective approach to constraint inference, leveraging informative demonstrations to improve accuracy and reduce data requirements.

Abstract: We present an iterative active constraint learning (ACL) algorithm, within the learning from demonstrations (LfD) paradigm, which intelligently solicits informative demonstration trajectories for inferring an unknown constraint in the demonstrator's environment. Our approach iteratively trains a Gaussian process (GP) on the available demonstration dataset to represent the unknown constraints, uses the resulting GP posterior to query start/goal states, and generates informative demonstrations which are added to the dataset. Across simulation and hardware experiments using high-dimensional nonlinear dynamics and unknown nonlinear constraints, our method outperforms a baseline, random-sampling based method at accurately performing constraint inference from an iteratively generated set of sparse but informative demonstrations.

</details>


### [506] [Two-Robot Computational Landscape: A Complete Characterization of Model Power in Minimal Mobile Robot Systems](https://arxiv.org/abs/2512.22770)
*Naoki Kitamura,Yuichi Sudo,Koichi Wada*

Main category: cs.RO

TL;DR: This paper fully characterizes the computational limits of two autonomous robots in various models under different synchrony levels.


<details>
  <summary>Details</summary>
Motivation: To resolve the unresolved question of computational capabilities specific to two autonomous robots within various models.

Method: The study uses a novel simulation-free approach to compare and determine the equivalence and separation of different models.

Result: Revealed that significant differences exist between two-robot systems and larger ones. Key revelations include special cases of equivalence and incomparability among models.

Conclusion: The computational power of two-robot systems is now fully defined, and the results contrast with larger n-robot cases. This highlights the unique challenges of minimal scale coordination.

Abstract: The computational power of autonomous mobile robots under the Look-Compute-Move (LCM) model has been widely studied through an extensive hierarchy of robot models defined by the presence of memory, communication, and synchrony assumptions. While the general n-robot landscape has been largely established, the exact structure for two robots has remained unresolved. This paper presents the first complete characterization of the computational power of two autonomous robots across all major models, namely OBLOT, FSTA, FCOM, and LUMI, under the full spectrum of schedulers (FSYNCH, SSYNCH, ASYNCH, and their atomic variants). Our results reveal a landscape that fundamentally differs from the general case. Most notably, we prove that FSTA^F and LUMI^F coincide under full synchrony, a surprising collapse indicating that perfect synchrony can substitute both memory and communication when only two robots exist. We also show that FSTA and FCOM are orthogonal: there exists a problem solvable in the weakest communication model but impossible even in the strongest finite-state model, completing the bidirectional incomparability. All equivalence and separation results are derived through a novel simulation-free method, providing a unified and constructive view of the two-robot hierarchy. This yields the first complete and exact computational landscape for two robots, highlighting the intrinsic challenges of coordination at the minimal scale.

</details>


### [507] [P-FABRIK: A General Intuitive and Robust Inverse Kinematics Method for Parallel Mechanisms Using FABRIK Approach](https://arxiv.org/abs/2512.22927)
*Daqian Cao,Quan Yuan,Weibang Bai*

Main category: cs.RO

TL;DR: This paper introduces P-FABRIK, an extended inverse kinematics method based on the FABRIK algorithm, to address challenges in solving kinematics for redundant parallel mechanisms.


<details>
  <summary>Details</summary>
Motivation: Traditional methods struggle with redundant parallel mechanisms due to complex constraints. Additionally, they fail when the target pose is outside the mechanism's workspace, creating control issues.

Method: P-FABRIK decomposes parallel mechanisms into serial sub-chains through a new topological strategy and uses iterative revision of sub-chain targets to compute inverse kinematics solutions efficiently.

Result: The method proved effective through case studies with various mechanisms and showed robustness in handling out-of-workspace targets during simulations.

Conclusion: P-FABRIK is a general, robust, and computationally efficient solution for inverse kinematics across diverse parallel mechanisms, addressing workspace limitations effectively.

Abstract: Traditional geometric inverse kinematics methods for parallel mechanisms rely on specific spatial geometry constraints. However, their application to redundant parallel mechanisms is challenged due to the increased constraint complexity. Moreover, it will output no solutions and cause unpredictable control problems when the target pose lies outside its workspace. To tackle these challenging issues, this work proposes P-FABRIK, a general, intuitive, and robust inverse kinematics method to find one feasible solution for diverse parallel mechanisms based on the FABRIK algorithm. By decomposing the general parallel mechanism into multiple serial sub-chains using a new topological decomposition strategy, the end targets of each sub-chain can be subsequently revised to calculate the inverse kinematics solutions iteratively. Multiple case studies involving planar, standard, and redundant parallel mechanisms demonstrated the proposed method's generality across diverse parallel mechanisms. Furthermore, numerical simulation studies verified its efficacy and computational efficiency, as well as its robustness ability to handle out-of-workspace targets.

</details>


### [508] [PreGME: Prescribed Performance Control of Aerial Manipulators based on Variable-Gain ESO](https://arxiv.org/abs/2512.22957)
*Mengyu Ji,Shiliang Guo,Zhengzhen Li,Jiahao Shen,Huazi Cao,Shiyu Zhao*

Main category: cs.RO

TL;DR: This paper proposes a novel motion control framework called PreGME for aerial manipulators, effectively addressing dynamic coupling between a multirotor base and robotic arm through variable-gain extended state observers (ESOs) and prescribed performance control.


<details>
  <summary>Details</summary>
Motivation: The dynamic coupling between an aerial manipulator's multirotor base and robotic arm poses challenges for precise and robust motion control, which is critical for complex aerial manipulation tasks.

Method: The authors developed a motion control framework, PreGME, involving variable-gain ESOs to estimate dynamic coupling in real time and a prescribed performance control strategy to ensure tracking errors stay within predefined limits.

Result: Through experiments on real platforms with tasks such as aerial staff twirling, aerial mixology, and cart-pulling, the proposed method demonstrated high-precision tracking even under significant dynamic coupling from rapid robotic arm motions.

Conclusion: The PreGME framework is effective in ensuring robust and precise motion control for aerial manipulators, making it suitable for demanding manipulation tasks involving aggressive arm movements.

Abstract: An aerial manipulator, comprising a multirotor base and a robotic arm, is subject to significant dynamic coupling between these two components. Therefore, achieving precise and robust motion control is a challenging yet important objective. Here, we propose a novel prescribed performance motion control framework based on variable-gain extended state observers (ESOs), referred to as PreGME. The method includes variable-gain ESOs for real-time estimation of dynamic coupling and a prescribed performance flight control that incorporates error trajectory constraints. Compared with existing methods, the proposed approach exhibits the following two characteristics. First, the adopted variable-gain ESOs can accurately estimate rapidly varying dynamic coupling. This enables the proposed method to handle manipulation tasks that require aggressive motion of the robotic arm. Second, by prescribing the performance, a preset error trajectory is generated to guide the system evolution along this trajectory. This strategy allows the proposed method to ensure the tracking error remains within the prescribed performance envelope, thereby achieving high-precision control. Experiments on a real platform, including aerial staff twirling, aerial mixology, and aerial cart-pulling experiments, are conducted to validate the effectiveness of the proposed method.
  Experimental results demonstrate that even under the dynamic coupling caused by rapid robotic arm motion (end-effector velocity: 1.02 m/s, acceleration: 5.10 m/s$^2$), the proposed method achieves high tracking performance.

</details>


### [509] [Embodied Robot Manipulation in the Era of Foundation Models: Planning and Learning Perspectives](https://arxiv.org/abs/2512.22983)
*Shuanghao Bai,Wenxuan Song,Jiayi Chen,Yuheng Ji,Zhide Zhong,Jin Yang,Han Zhao,Wanqi Zhou,Zhe Li,Pengxiang Ding,Cheng Chi,Chang Xu,Xiaolong Zheng,Donglin Wang,Haoang Li,Shanghang Zhang,Badong Chen*

Main category: cs.RO

TL;DR: The paper reviews robot manipulation techniques, focusing on algorithmic approaches and organizing learning methods into high-level planning and low-level control paradigms.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenging problem of robot manipulation, leveraging recent advancements in multimodal learning and exploring its structured design space.

Method: The paper categorizes techniques into high-level planning (task planning, reasoning via language/code/motion) and low-level control paradigms (input modeling, latent learning, policy creation).

Result: Identifies challenges such as scalability, efficiency, multimodal interaction, and safety, highlighting structured solutions in robotic foundation models.

Conclusion: The analysis provides clarity on modern design models for robotic manipulation and suggests future research directions for improving efficiency and interaction.

Abstract: Recent advances in vision, language, and multimodal learning have substantially accelerated progress in robotic foundation models, with robot manipulation remaining a central and challenging problem. This survey examines robot manipulation from an algorithmic perspective and organizes recent learning-based approaches within a unified abstraction of high-level planning and low-level control. At the high level, we extend the classical notion of task planning to include reasoning over language, code, motion, affordances, and 3D representations, emphasizing their role in structured and long-horizon decision making. At the low level, we propose a training-paradigm-oriented taxonomy for learning-based control, organizing existing methods along input modeling, latent representation learning, and policy learning. Finally, we identify open challenges and prospective research directions related to scalability, data efficiency, multimodal physical interaction, and safety. Together, these analyses aim to clarify the design space of modern foundation models for robotic manipulation.

</details>


### [510] [Embodied Learning of Reward for Musculoskeletal Control with Vision Language Models](https://arxiv.org/abs/2512.23077)
*Saraswati Soedarmadji,Yunyue Wei,Chen Zhang,Yisong Yue,Yanan Sui*

Main category: cs.RO

TL;DR: The paper introduces MoVLR, a framework using vision-language models to derive effective reward functions for high-dimensional musculoskeletal control by iteratively exploring movement goals.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of deriving effective reward functions for high-dimensional musculoskeletal systems, which cannot be designed directly from high-level language descriptions or explicit goals.

Method: The MoVLR framework iteratively integrates control optimization and vision-language model feedback to discover and refine reward functions that align with realistic motor control principles.

Result: The approach successfully transforms language and visual assessments into structured guidance, enabling the discovery of reward functions for musculoskeletal locomotion and manipulation.

Conclusion: Vision-language models can bridge abstract motion goals with implicit motor control strategies, offering a novel path for solving reward-design challenges in motor control systems.

Abstract: Discovering effective reward functions remains a fundamental challenge in motor control of high-dimensional musculoskeletal systems. While humans can describe movement goals explicitly such as "walking forward with an upright posture," the underlying control strategies that realize these goals are largely implicit, making it difficult to directly design rewards from high-level goals and natural language descriptions. We introduce Motion from Vision-Language Representation (MoVLR), a framework that leverages vision-language models (VLMs) to bridge the gap between goal specification and movement control. Rather than relying on handcrafted rewards, MoVLR iteratively explores the reward space through iterative interaction between control optimization and VLM feedback, aligning control policies with physically coordinated behaviors. Our approach transforms language and vision-based assessments into structured guidance for embodied learning, enabling the discovery and refinement of reward functions for high-dimensional musculoskeletal locomotion and manipulation. These results suggest that VLMs can effectively ground abstract motion descriptions in the implicit principles governing physiological motor control.

</details>


### [511] [APOLLO Blender: A Robotics Library for Visualization and Animation in Blender](https://arxiv.org/abs/2512.23103)
*Peter Messina,Daniel Rakita*

Main category: cs.RO

TL;DR: The paper presents a lightweight library for robotics visualization that simplifies using Blender for graphical outputs like images and animations.


<details>
  <summary>Details</summary>
Motivation: Robotics researchers face challenges in creating high-quality graphics due to Blender's complex interface and lack of robotics-specific integration.

Method: The library provides scripting abilities to import robots/environments, keyframe states, and create primitive 3D shapes for visualization tasks.

Result: The library successfully enables rapid and effective development of publication-ready robotics visuals through proofs-of-concept.

Conclusion: The paper concludes that its tool significantly improves accessibility to high-quality graphics for robotics research and highlights future areas for enhancement.

Abstract: High-quality visualizations are an essential part of robotics research, enabling clear communication of results through figures, animations, and demonstration videos. While Blender is a powerful and freely available 3D graphics platform, its steep learning curve and lack of robotics-focused integrations make it difficult and time-consuming for researchers to use effectively. In this work, we introduce a lightweight software library that bridges this gap by providing simple scripting interfaces for common robotics visualization tasks. The library offers three primary capabilities: (1) importing robots and environments directly from standardized descriptions such as URDF; (2) Python-based scripting tools for keyframing robot states and visual attributes; and (3) convenient generation of primitive 3D shapes for schematic figures and animations. Together, these features allow robotics researchers to rapidly create publication-ready images, animations, and explanatory schematics without needing extensive Blender expertise. We demonstrate the library through a series of proof-of-concept examples and conclude with a discussion of current limitations and opportunities for future extensions.

</details>


### [512] [Beyond URDF: The Universal Robot Description Directory for Shared, Extensible, and Standardized Robot Models](https://arxiv.org/abs/2512.23135)
*Roshan Klein-Seetharaman,Daniel Rakita*

Main category: cs.RO

TL;DR: The paper introduces the Universal Robot Description Directory (URDD) to enhance robot data representation with easily parsable modules, offering more detailed information than current standard robot specification files and enabling efficient downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Current robot specification files lack detailed information, causing redundant computations and limited standardization in robotics applications.

Method: The authors developed URDD as a modular format using JSON and YAML, providing an open-source toolkit that converts URDFs into URDDs, with Rust and JavaScript-based visualization tools.

Result: URDDs were generated efficiently and encapsulated richer information than standard files, enabling core robotics subroutines while establishing a unified resource.

Conclusion: URDD reduces redundancy, enhances information richness, and supports standardization across robotics frameworks, though the limitations and broader implications are discussed.

Abstract: Robots are typically described in software by specification files (e.g., URDF, SDF, MJCF, USD) that encode only basic kinematic, dynamic, and geometric information. As a result, downstream applications such as simulation, planning, and control must repeatedly re-derive richer data, leading to redundant computations, fragmented implementations, and limited standardization. In this work, we introduce the Universal Robot Description Directory (URDD), a modular representation that organizes derived robot information into structured, easy-to-parse JSON and YAML modules. Our open-source toolkit automatically generates URDDs from URDFs, with a Rust implementation supporting Bevy-based visualization. Additionally, we provide a JavaScript/Three.js viewer for web-based inspection of URDDs. Experiments on multiple robot platforms show that URDDs can be generated efficiently, encapsulate substantially richer information than standard specification files, and directly enable the construction of core robotics subroutines. URDD provides a unified, extensible resource for reducing redundancy and establishing shared standards across robotics frameworks. We conclude with a discussion on the limitations and implications of our work.

</details>


### [513] [A New Software Tool for Generating and Visualizing Robot Self-Collision Matrices](https://arxiv.org/abs/2512.23140)
*Roshan Klein-Seetharama,Daniel Rakita*

Main category: cs.RO

TL;DR: The paper introduces an interactive tool for generating and refining self-collision matrices for robotic systems, overcoming limitations of existing methods.


<details>
  <summary>Details</summary>
Motivation: Currently, tools for generating self-collision matrices in robotics are limited by static visualization, lack of proximity support, and inefficient workflows, which restrict their flexibility and usability.

Method: The paper proposes a Rust-based interactive tool using the Bevy game engine to create and visualize self-collision matrices, offering support for multiple shape representations and dynamic filtering and refinement capabilities.

Result: The proposed tool successfully generates self-collision matrices that improve both the speed and accuracy of self-collision and self-proximity queries on various robot platforms.

Conclusion: The tool addresses the inefficiencies of current systems, providing a flexible and highly visual approach that enhances the workflow of self-collision and self-proximity checks, making it easier to integrate into robotics applications.

Abstract: In robotics, it is common to check whether a given robot state results in self-intersection (i.e., a self-collision query) or to assess its distance from such an intersection (i.e., a self-proximity query). These checks are typically performed between pairs of shapes attached to different robot links. However, many of these shape pairs can be excluded in advance, as their configurations are known to always or never result in contact. This information is typically encoded in a self-collision matrix, where each entry (i, j) indicates whether a check should be performed between shape i and shape j. While the MoveIt Setup Assistant is widely used to generate such matrices, current tools are limited by static visualization, lack of proximity support, rigid single-geometry assumptions, and tedious refinement workflows, hindering flexibility and reuse in downstream robotics applications. In this work, we introduce an interactive tool that overcomes these limitations by generating and visualizing self-collision matrices across multiple shape representations, enabling dynamic inspection, filtering, and refinement of shape pairs. Outputs are provided in both JSON and YAML for easy integration. The system is implemented in Rust and uses the Bevy game engine to deliver high-quality visualizations. We demonstrate its effectiveness on multiple robot platforms, showing that matrices generated using diverse shape types yield faster and more accurate self-collision and self-proximity queries.

</details>


### [514] [Pole-centric Descriptors for Robust Robot Localization: Evaluation under Pole-at-Distance (PaD) Observations using the Small Pole Landmark (SPL) Dataset](https://arxiv.org/abs/2512.23141)
*Wuhao Xie,Kanji Tanaka*

Main category: cs.RO

TL;DR: The paper focuses on evaluating the robustness of geometric descriptors for robot localization in urban environments using a Small Pole Landmark (SPL) dataset. It finds Contrastive Learning to be more effective for feature retrieval in sparse geometries compared to Supervised Learning.


<details>
  <summary>Details</summary>
Motivation: Urban environments with pole-like landmarks pose challenges for robot localization under long-distance observations. Existing methods lack robust evaluation frameworks for geometric descriptor reliability.

Method: The authors created a unique SPL dataset using an automated pipeline to capture multi-view/distance observations. They analyzed Contrastive Learning versus Supervised Learning for descriptor design.

Result: Contrastive Learning exhibited superior robustness in feature space development for sparse geometry, particularly improving retrieval performance at distances of 5–10m.

Conclusion: This paper introduces a scalable evaluation methodology and emphasizes Contrastive Learning for landmark descriptor robustness, enhancing long-term localization in real-world, large-scale environments.

Abstract: While pole-like structures are widely recognized as stable geometric anchors for long-term robot localization, their identification reliability degrades significantly under Pole-at-Distance (Pad) observations typical of large-scale urban environments. This paper shifts the focus from descriptor design to a systematic investigation of descriptor robustness. Our primary contribution is the establishment of a specialized evaluation framework centered on the Small Pole Landmark (SPL) dataset. This dataset is constructed via an automated tracking-based association pipeline that captures multi-view, multi-distance observations of the same physical landmarks without manual annotation. Using this framework, we present a comparative analysis of Contrastive Learning (CL) and Supervised Learning (SL) paradigms. Our findings reveal that CL induces a more robust feature space for sparse geometry, achieving superior retrieval performance particularly in the 5--10m range. This work provides an empirical foundation and a scalable methodology for evaluating landmark distinctiveness in challenging real-world scenarios.

</details>


### [515] [Towards the Automation in the Space Station: Feasibility Study and Ground Tests of a Multi-Limbed Intra-Vehicular Robot](https://arxiv.org/abs/2512.23153)
*Seiko Piotr Yamaguchi,Kentaro Uno,Yasumaru Fujii,Masazumi Imai,Kazuki Takada,Taku Okawara,Kazuya Yoshida*

Main category: cs.RO

TL;DR: The paper investigates the feasibility of using an autonomous mobile manipulator to assist astronauts with logistical tasks on the ISS, demonstrating its capabilities in simulations and prototype tests.


<details>
  <summary>Details</summary>
Motivation: Astronauts spend considerable time on logistical tasks on the ISS, reducing their focus on critical mission objectives. The study aims to enhance operational efficiency by introducing an autonomous mobile manipulator.

Method: Simulations of 3D motion planning and a 2D prototype mimicking a microgravity environment were conducted to evaluate the robot's autonomous transportation capabilities.

Result: The study shows that the mobile manipulator can effectively perform logistical tasks with minimal human intervention.

Conclusion: An autonomous mobile manipulator is a promising solution to improve efficiency and reduce astronaut workload on the ISS.

Abstract: This paper presents a feasibility study, including simulations and prototype tests, on the autonomous operation of a multi-limbed intra-vehicular robot (mobile manipulator), shortly MLIVR, designed to assist astronauts with logistical tasks on the International Space Station (ISS). Astronauts spend significant time on tasks such as preparation, close-out, and the collection and transportation of goods, reducing the time available for critical mission activities. Our study explores the potential for a mobile manipulator to support these operations, emphasizing the need for autonomous functionality to minimize crew and ground operator effort while enabling real-time task execution. We focused on the robot's transportation capabilities, simulating its motion planning in 3D space. The actual motion execution was tested with a prototype on a 2D table to mimic a microgravity environment. The results demonstrate the feasibility of performing these tasks with minimal human intervention, offering a promising solution to enhance operational efficiency on the ISS.

</details>


### [516] [A Sequential Hermaphrodite Coupling Mechanism for Lattice-based Modular Robots](https://arxiv.org/abs/2512.23154)
*Keigo Torii,Kentaro Uno,Shreya Santra,Kazuya Yoshida*

Main category: cs.RO

TL;DR: The paper introduces a novel mechanical coupling mechanism for lattice-based modular robots that allows for single-sided coupling/decoupling and satisfies critical requirements for space and extreme environments.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of designing coupling mechanisms in lattice-based modular robots for extreme environments, such as space, by meeting requirements like single-sided coupling, flat surfaces, and compatibility with passive interfaces.

Method: The authors propose a shape-matching mechanical coupling mechanism, where transitions between male and female states are controlled and sequential, allowing single-sided coupling/decoupling.

Result: The mechanism enables practical coupling behavior, supporting single-sided operations, and is compatible with different modular robot systems and robot arm tool changers.

Conclusion: The proposed mechanism satisfies complex design requirements and has versatile applications in modular robotics and extreme environment construction.

Abstract: Lattice-based modular robot systems are envisioned for large-scale construction in extreme environments, such as space. Coupling mechanisms for heterogeneous structural modules should meet all of the following requirements: single-sided coupling and decoupling, flat surfaces when uncoupled, and coupling to passive coupling interfaces as well as coupling behavior between coupling mechanisms. The design requirements for such a coupling mechanism are complex. We propose a novel shape-matching mechanical coupling mechanism that satisfies these design requirements. This mechanism enables controlled, sequential transitions between male and female states. When uncoupled, all mechanisms are in the female state. To enable single-sided coupling, one side of the mechanisms switches to the male state during the coupling process. Single-sided decoupling is possible not only from the male side but also from the female side by forcibly switching the opposite mechanism's male state to the female state. This coupling mechanism can be applied to various modular robot systems and robot arm tool changers.

</details>


### [517] [SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling](https://arxiv.org/abs/2512.23162)
*Yufan He,Pengfei Guo,Mengya Xu,Zhaoshuo Li,Andriy Myronenko,Dillan Imans,Bingjie Liu,Dongren Yang,Mingxue Gu,Yongnan Ji,Yueming Jin,Ren Zhao,Baiyong Shen,Daguang Xu*

Main category: cs.RO

TL;DR: This paper introduces a novel approach to enhance autonomous surgical robots by leveraging synthetic datasets and world modeling, overcoming the issue of data scarcity in surgical robotics.


<details>
  <summary>Details</summary>
Motivation: Surgical robotics faces challenges due to scarce datasets containing both visual and robotic kinematics data, whereas large-scale datasets exist for other fields. The paper aims to address this data scarcity problem.

Method: The authors created the Surgical Action Text Alignment (SATA) dataset, combined it with a world model called SurgeWorld, and inferred pseudokinematics using an inverse dynamics model to generate synthetic paired video-action datasets for training surgical robots.

Result: A surgical VLA policy trained with the generated datasets achieved improved performance compared to policies trained on real demonstrations, showcasing the effectiveness of the approach.

Conclusion: This work opens pathways to scalable and generalizable surgical robot policies by leveraging synthetic data and generative modeling for autonomous skill acquisition.

Abstract: Data scarcity remains a fundamental barrier to achieving fully autonomous surgical robots. While large scale vision language action (VLA) models have shown impressive generalization in household and industrial manipulation by leveraging paired video action data from diverse domains, surgical robotics suffers from the paucity of datasets that include both visual observations and accurate robot kinematics. In contrast, vast corpora of surgical videos exist, but they lack corresponding action labels, preventing direct application of imitation learning or VLA training. In this work, we aim to alleviate this problem by learning policy models from SurgWorld, a world model designed for surgical physical AI. We curated the Surgical Action Text Alignment (SATA) dataset with detailed action description specifically for surgical robots. Then we built SurgeWorld based on the most advanced physical AI world model and SATA. It's able to generate diverse, generalizable and realistic surgery videos. We are also the first to use an inverse dynamics model to infer pseudokinematics from synthetic surgical videos, producing synthetic paired video action data. We demonstrate that a surgical VLA policy trained with these augmented data significantly outperforms models trained only on real demonstrations on a real surgical robot platform. Our approach offers a scalable path toward autonomous surgical skill acquisition by leveraging the abundance of unlabeled surgical video and generative world modeling, thus opening the door to generalizable and data efficient surgical robot policies.

</details>


### [518] [A Human-Oriented Cooperative Driving Approach: Integrating Driving Intention, State, and Conflict](https://arxiv.org/abs/2512.23220)
*Qin Wang,Shanmin Pang,Jianwu Fang,Shengye Dong,Fuhao Liu,Jianru Xue,Chen Lv*

Main category: cs.RO

TL;DR: The paper introduces the Human-Oriented Cooperative Driving (HOCD) approach to minimize human-machine conflict in semi-autonomous vehicles by prioritizing driver intention and state.


<details>
  <summary>Details</summary>
Motivation: To improve human-vehicle interaction, driving flexibility, and foster driver trust and acceptance of autonomous technology.

Method: HOCD approach combines a tactical-level intention-aware trajectory planning method and an operational-level reinforcement learning-based control authority allocation strategy.

Result: Simulations and human-in-the-loop experiments show HOCD aligns trajectories with driver intention and optimizes authority allocation, outperforming other cooperative driving approaches in reducing conflicts and improving driving performance.

Conclusion: HOCD enhances driving performance, reduces human-machine conflict, and represents a step forward in cooperative driving systems towards autonomous driving.

Abstract: Human-vehicle cooperative driving serves as a vital bridge to fully autonomous driving by improving driving flexibility and gradually building driver trust and acceptance of autonomous technology. To establish more natural and effective human-vehicle interaction, we propose a Human-Oriented Cooperative Driving (HOCD) approach that primarily minimizes human-machine conflict by prioritizing driver intention and state. In implementation, we take both tactical and operational levels into account to ensure seamless human-vehicle cooperation. At the tactical level, we design an intention-aware trajectory planning method, using intention consistency cost as the core metric to evaluate the trajectory and align it with driver intention. At the operational level, we develop a control authority allocation strategy based on reinforcement learning, optimizing the policy through a designed reward function to achieve consistency between driver state and authority allocation. The results of simulation and human-in-the-loop experiments demonstrate that our proposed approach not only aligns with driver intention in trajectory planning but also ensures a reasonable authority allocation. Compared to other cooperative driving approaches, the proposed HOCD approach significantly enhances driving performance and mitigates human-machine conflict.The code is available at https://github.com/i-Qin/HOCD.

</details>


### [519] [Beyond Coverage Path Planning: Can UAV Swarms Perfect Scattered Regions Inspections?](https://arxiv.org/abs/2512.23257)
*Socratis Gkelios,Savvas D. Apostolidis,Pavlos Ch. Kapoutsis,Elias B. Kosmatopoulos,Athanasios Ch. Kapoutsis*

Main category: cs.RO

TL;DR: The paper proposes a novel multi-UAV method, mUDAI, for efficiently inspecting scattered regions by optimizing image capturing positions and UAV trajectories.


<details>
  <summary>Details</summary>
Motivation: UAVs offer advantages in inspection tasks but are hindered by battery limitations, leading to inefficiencies in covering multiple non-connected regions.

Method: The mUDAI method features a two-fold optimization process to determine optimal image capturing positions and efficient UAV trajectories, reducing redundant data and resource usage.

Result: The method proved effective in simulations and real-world tests, improving operational efficiency and maintaining high-quality data capture.

Conclusion: mUDAI enhances UAV efficiency for inspecting scattered regions, and its tools are openly accessible on GitHub, Zenodo, and an online interactive platform.

Abstract: Unmanned Aerial Vehicles (UAVs) have revolutionized inspection tasks by offering a safer, more efficient, and flexible alternative to traditional methods. However, battery limitations often constrain their effectiveness, necessitating the development of optimized flight paths and data collection techniques. While existing approaches like coverage path planning (CPP) ensure comprehensive data collection, they can be inefficient, especially when inspecting multiple non connected Regions of Interest (ROIs). This paper introduces the Fast Inspection of Scattered Regions (FISR) problem and proposes a novel solution, the multi UAV Disjoint Areas Inspection (mUDAI) method. The introduced approach implements a two fold optimization procedure, for calculating the best image capturing positions and the most efficient UAV trajectories, balancing data resolution and operational time, minimizing redundant data collection and resource consumption. The mUDAI method is designed to enable rapid, efficient inspections of scattered ROIs, making it ideal for applications such as security infrastructure assessments, agricultural inspections, and emergency site evaluations. A combination of simulated evaluations and real world deployments is used to validate and quantify the method's ability to improve operational efficiency while preserving high quality data capture, demonstrating its effectiveness in real world operations. An open source Python implementation of the mUDAI method can be found on GitHub (https://github.com/soc12/mUDAI) and the collected and processed data from the real world experiments are all hosted on Zenodo (https://zenodo.org/records/13866483). Finally, this online platform (https://sites.google.com/view/mudai-platform/) allows interested readers to interact with the mUDAI method and generate their own multi UAV FISR missions.

</details>


### [520] [Explainable Neural Inverse Kinematics for Obstacle-Aware Robotic Manipulation: A Comparative Analysis of IKNet Variants](https://arxiv.org/abs/2512.23312)
*Sheng-Kai Chen,Yi-Ling Tsai,Chun-Chih Chang,Yan-Chen Chen,Po-Chiang Lin*

Main category: cs.RO

TL;DR: The paper introduces a workflow integrating explainable AI tools with physics-based evaluations for obstacle-aware inverse kinematics in robotics, focusing on the ROBOTIS OpenManipulator-X.


<details>
  <summary>Details</summary>
Motivation: There is a need for transparency and safety in deep neural networks applied for robotics inverse kinematics, meeting emerging responsible AI standards.

Method: Two modified variants of IKNet are developed, integrating explainable AI tools (SHAP, InterpretML) with obstacle evaluation simulations to study and improve robot manipulator trajectories.

Result: Improved architectures distribute importance evenly across pose dimensions, enhancing manipulation safety while maintaining accuracy.

Conclusion: The study demonstrates explainable AI’s potential to reveal failure modes and inform safer, trustworthy deployment strategies for data-driven inverse kinematics.

Abstract: Deep neural networks have accelerated inverse-kinematics (IK) inference to the point where low cost manipulators can execute complex trajectories in real time, yet the opaque nature of these models contradicts the transparency and safety requirements emerging in responsible AI regulation. This study proposes an explainability centered workflow that integrates Shapley-value attribution with physics-based obstacle avoidance evaluation for the ROBOTIS OpenManipulator-X. Building upon the original IKNet, two lightweight variants-Improved IKNet with residual connections and Focused IKNet with position-orientation decoupling are trained on a large, synthetically generated pose-joint dataset. SHAP is employed to derive both global and local importance rankings, while the InterpretML toolkit visualizes partial-dependence patterns that expose non-linear couplings between Cartesian poses and joint angles. To bridge algorithmic insight and robotic safety, each network is embedded in a simulator that subjects the arm to randomized single and multi-obstacle scenes; forward kinematics, capsule-based collision checks, and trajectory metrics quantify the relationship between attribution balance and physical clearance. Qualitative heat maps reveal that architectures distributing importance more evenly across pose dimensions tend to maintain wider safety margins without compromising positional accuracy. The combined analysis demonstrates that explainable AI(XAI) techniques can illuminate hidden failure modes, guide architectural refinements, and inform obstacle aware deployment strategies for learning based IK. The proposed methodology thus contributes a concrete path toward trustworthy, data-driven manipulation that aligns with emerging responsible-AI standards.

</details>


### [521] [PCR-ORB: Enhanced ORB-SLAM3 with Point Cloud Refinement Using Deep Learning-Based Dynamic Object Filtering](https://arxiv.org/abs/2512.23318)
*Sheng-Kai Chen,Jie-Yu Chao,Jr-Yu Chang,Po-Lien Wu,Po-Chiang Lin*

Main category: cs.RO

TL;DR: The paper proposes PCR-ORB, an enhanced version of ORB-SLAM3 using deep learning and point cloud refinement to address vSLAM challenges in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: Address the issues faced by vSLAM systems in dynamic environments, where moving objects degrade tracking accuracy and map consistency.

Method: Integrates YOLOv8 for semantic segmentation, CUDA-based acceleration, and a multi-stage filtering approach (ground plane estimation, sky region removal, edge filtering, and temporal validation) to enhance ORB-SLAM3.

Result: Evaluation on the KITTI dataset reveals mixed results, with improvements in specific sequences such as a 25.9% enhancement in ATE RMSE for sequence 04. Performance varies depending on the scene.

Conclusion: PCR-ORB offers significant improvements for vSLAM in certain dynamic scenarios but highlights the need for further refinements for consistent performance across environments.

Abstract: Visual Simultaneous Localization and Mapping (vSLAM) systems encounter substantial challenges in dynamic environments where moving objects compromise tracking accuracy and map consistency. This paper introduces PCR-ORB (Point Cloud Refinement ORB), an enhanced ORB-SLAM3 framework that integrates deep learning-based point cloud refinement to mitigate dynamic object interference. Our approach employs YOLOv8 for semantic segmentation combined with CUDA-accelerated processing to achieve real-time performance. The system implements a multi-stage filtering strategy encompassing ground plane estimation, sky region removal, edge filtering, and temporal consistency validation. Comprehensive evaluation on the KITTI dataset (sequences 00-09) demonstrates performance characteristics across different environmental conditions and scene types. Notable improvements are observed in specific sequences, with sequence 04 achieving 25.9% improvement in ATE RMSE and 30.4% improvement in ATE median. However, results show mixed performance across sequences, indicating scenario-dependent effectiveness. The implementation provides insights into dynamic object filtering challenges and opportunities for robust navigation in complex environments.

</details>


### [522] [Optimal Scalability-Aware Allocation of Swarm Robots: From Linear to Retrograde Performance via Marginal Gains](https://arxiv.org/abs/2512.23431)
*Simay Atasoy Bingöl,Tobias Töpfer,Sven Kosub,Heiko Hamann,Andreagiovanni Reina*

Main category: cs.RO

TL;DR: The paper introduces an efficient algorithm to allocate agents optimally among tasks with concave scalability functions, tested in robotic systems with varying task difficulties.


<details>
  <summary>Details</summary>
Motivation: Optimal allocation of agents among tasks is challenging due to nonlinear scaling of task performance with agent numbers, particularly in robotic swarms where task complexity varies.

Method: A computational algorithm based on marginal performance gains is proposed to allocate agents among tasks with concave scalability functions, tested in simulations with varied task complexities.

Result: The algorithm effectively allocates robots among tasks, with performance scaling modeled as saturating or retrograde curves dependent on task conditions and interference.

Conclusion: This method is useful for advancing allocation strategies in real-world multi-robot systems, optimizing collective performance in various scenarios.

Abstract: In collective systems, the available agents are a limited resource that must be allocated among tasks to maximize collective performance. Computing the optimal allocation of several agents to numerous tasks through a brute-force approach can be infeasible, especially when each task's performance scales differently with the increase of agents. For example, difficult tasks may require more agents to achieve similar performances compared to simpler tasks, but performance may saturate nonlinearly as the number of allocated agents increases. We propose a computationally efficient algorithm, based on marginal performance gains, for optimally allocating agents to tasks with concave scalability functions, including linear, saturating, and retrograde scaling, to achieve maximum collective performance. We test the algorithm by allocating a simulated robot swarm among collective decision-making tasks, where embodied agents sample their environment and exchange information to reach a consensus on spatially distributed environmental features. We vary task difficulties by different geometrical arrangements of environmental features in space (patchiness). In this scenario, decision performance in each task scales either as a saturating curve (following the Condorcet's Jury Theorem in an interference-free setup) or as a retrograde curve (when physical interference among robots restricts their movement). Using simple robot simulations, we show that our algorithm can be useful in allocating robots among tasks. Our approach aims to advance the deployment of future real-world multi-robot systems.

</details>


### [523] [Theory of Mind for Explainable Human-Robot Interaction](https://arxiv.org/abs/2512.23482)
*Marie Bauer,Julia Gachot,Matthias Kerzel,Cornelius Weber,Stefan Wermter*

Main category: cs.RO

TL;DR: This paper discusses integrating Theory of Mind (ToM) into Explainable Artificial Intelligence (XAI) to enhance human-robot interaction and user-centric explanations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of assessment of how ToM explanations correspond to robots' internal reasoning in Human-Robot Interaction (HRI) while improving user-centered explanations in XAI.

Method: The proposed method integrates ToM within XAI frameworks and evaluates this approach using the eValuation XAI (VXAI) framework and its seven desiderata.

Result: The idea shifts the focus of XAI from centering on the AI system itself to prioritizing user-centered explanations through ToM principles.

Conclusion: Integrating ToM into XAI frameworks bridges gaps in HRI and leads to improved interpretability, transparency, and user-focused interactions in robotic systems.

Abstract: Within the context of human-robot interaction (HRI), Theory of Mind (ToM) is intended to serve as a user-friendly backend to the interface of robotic systems, enabling robots to infer and respond to human mental states. When integrated into robots, ToM allows them to adapt their internal models to users' behaviors, enhancing the interpretability and predictability of their actions. Similarly, Explainable Artificial Intelligence (XAI) aims to make AI systems transparent and interpretable, allowing humans to understand and interact with them effectively. Since ToM in HRI serves related purposes, we propose to consider ToM as a form of XAI and evaluate it through the eValuation XAI (VXAI) framework and its seven desiderata. This paper identifies a critical gap in the application of ToM within HRI, as existing methods rarely assess the extent to which explanations correspond to the robot's actual internal reasoning. To address this limitation, we propose to integrate ToM within XAI frameworks. By embedding ToM principles inside XAI, we argue for a shift in perspective, as current XAI research focuses predominantly on the AI system itself and often lacks user-centered explanations. Incorporating ToM would enable a change in focus, prioritizing the user's informational needs and perspective.

</details>


### [524] [Robust Deep Learning Control with Guaranteed Performance for Safe and Reliable Robotization in Heavy-Duty Machinery](https://arxiv.org/abs/2512.23505)
*Mehdi Heydari Shahna*

Main category: cs.RO

TL;DR: This paper tackles technical and safety challenges in electrifying and automating heavy-duty mobile machines (HDMMs), proposing a robust and modular control framework validated through case studies.


<details>
  <summary>Details</summary>
Motivation: HDMMs must balance environmental goals (electrification) and technological progress (autonomy) while addressing the challenges posed by existing diesel-hydraulic systems and strict safety requirements.

Method: The paper proposes a modular control framework adaptable to different energy sources and actuator types. It includes hierarchical control policies that integrate AI while ensuring safety and stability.

Result: Validated in case studies of HDMM systems, the framework demonstrates robust stability, responsiveness under uncertainty, fault tolerance, and compliance with international safety standards.

Conclusion: The developed framework supports transitions to electrified and autonomous HDMMs while ensuring robust, safe, and efficient operation. Findings advance nonlinear control and robotics through peer-reviewed research.

Abstract: Today's heavy-duty mobile machines (HDMMs) face two transitions: from diesel-hydraulic actuation to clean electric systems driven by climate goals, and from human supervision toward greater autonomy. Diesel-hydraulic systems have long dominated, so full electrification, via direct replacement or redesign, raises major technical and economic challenges. Although advanced artificial intelligence (AI) could enable higher autonomy, adoption in HDMMs is limited by strict safety requirements, and these machines still rely heavily on human supervision.
  This dissertation develops a control framework that (1) simplifies control design for electrified HDMMs through a generic modular approach that is energy-source independent and supports future modifications, and (2) defines hierarchical control policies that partially integrate AI while guaranteeing safety-defined performance and stability.
  Five research questions align with three lines of investigation: a generic robust control strategy for multi-body HDMMs with strong stability across actuation types and energy sources; control solutions that keep strict performance under uncertainty and faults while balancing robustness and responsiveness; and methods to interpret and trust black-box learning strategies so they can be integrated stably and verified against international safety standards.
  The framework is validated in three case studies spanning different actuators and conditions, covering heavy-duty mobile robots and robotic manipulators. Results appear in five peer-reviewed publications and one unpublished manuscript, advancing nonlinear control and robotics and supporting both transitions.

</details>


### [525] [Act2Goal: From World Model To General Goal-conditioned Policy](https://arxiv.org/abs/2512.23541)
*Pengfei Zhou,Liliang Chen,Shengcong Chen,Di Chen,Wenzhi Zhao,Rongjun Jin,Guanghui Ren,Jianlan Luo*

Main category: cs.RO

TL;DR: Act2Goal, a robotic manipulation policy, integrates goal-conditioned visual world models and multi-scale temporal control to improve long-horizon task success rates significantly.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of effectively specifying robotic manipulation tasks for precision and expressiveness, especially for long-horizon tasks.

Method: Act2Goal combines a goal-conditioned visual world model with Multi-Scale Temporal Hashing (MSTH) to generate intermediate visual task structures and translates these plans into motor control through end-to-end cross-attention.

Result: Achieved a success rate increase from 30% to 90% in challenging tasks through features like zero-shot generalization and reward-free online adaptation via LoRA-based finetuning.

Conclusion: This approach validates that integrating structured visual models with temporal control is key to addressing long-horizon manipulation tasks in robotics.

Abstract: Specifying robotic manipulation tasks in a manner that is both expressive and precise remains a central challenge. While visual goals provide a compact and unambiguous task specification, existing goal-conditioned policies often struggle with long-horizon manipulation due to their reliance on single-step action prediction without explicit modeling of task progress. We propose Act2Goal, a general goal-conditioned manipulation policy that integrates a goal-conditioned visual world model with multi-scale temporal control. Given a current observation and a target visual goal, the world model generates a plausible sequence of intermediate visual states that captures long-horizon structure. To translate this visual plan into robust execution, we introduce Multi-Scale Temporal Hashing (MSTH), which decomposes the imagined trajectory into dense proximal frames for fine-grained closed-loop control and sparse distal frames that anchor global task consistency. The policy couples these representations with motor control through end-to-end cross-attention, enabling coherent long-horizon behavior while remaining reactive to local disturbances. Act2Goal achieves strong zero-shot generalization to novel objects, spatial layouts, and environments. We further enable reward-free online adaptation through hindsight goal relabeling with LoRA-based finetuning, allowing rapid autonomous improvement without external supervision. Real-robot experiments demonstrate that Act2Goal improves success rates from 30% to 90% on challenging out-of-distribution tasks within minutes of autonomous interaction, validating that goal-conditioned world models with multi-scale temporal control provide structured guidance necessary for robust long-horizon manipulation. Project page: https://act2goal.github.io/

</details>


### [526] [Soft Robotic Technological Probe for Speculative Fashion Futures](https://arxiv.org/abs/2512.23570)
*Amy Ingold,Loong Yi Lee,Richard Suphapol Diteesawat,Ajmal Roshan,Yael Zekaria,Edith-Clare Hall,Enrico Werner,Nahian Rahman,Elaine Czech,Jonathan Rossiter*

Main category: cs.RO

TL;DR: This paper discusses the development of a soft robotic garment called Sumbrella, exploring its design, user interaction, and the societal implications of wearable robots.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the growing demand for wearable robotics that consider both functionality and social meaning, aiming to influence the design and ethical integration of these technologies into public life.

Method: The researchers designed and fabricated Sumbrella, a robotic garment integrating mechanisms like origami-inspired units, fabric actuators, and computer vision. They conducted a focus group with 12 creative technologists to analyze participant interaction and perceptions of this wearable.

Result: Sumbrella stimulated discussions on speculative futures and expressive potential for robotic wearables while uncovering concerns about exploitation, surveillance, ethical implications, and societal risks of embedding such technologies.

Conclusion: The study highlights the potential of wearable robots in kinesic communication and social dynamics. It emphasizes the necessity of ethical guidelines and suggests that speculative design enables researchers to explore the societal implications and desirability of robotic wearables in public spaces.

Abstract: Emerging wearable robotics demand design approaches that address not only function, but also social meaning. In response, we present Sumbrella, a soft robotic garment developed as a speculative fashion probe. We first detail the design and fabrication of the Sumbrella, including sequenced origami-inspired bistable units, fabric pneumatic actuation chambers, cable driven shape morphing mechanisms, computer vision components, and an integrated wearable system comprising a hat and bolero jacket housing power and control electronics. Through a focus group with twelve creative technologists, we then used Sumbrella as a technological probe to explore how people interpreted, interacted, and imagined future relationships with soft robotic wearables. While Sumbrella allowed our participants to engage in rich discussion around speculative futures and expressive potential, it also surfaced concerns about exploitation, surveillance, and the personal risks and societal ethics of embedding biosensing technologies in public life. We contribute to the Human-Robot Interaction (HRI) field key considerations and recommendations for designing soft robotic garments, including the potential for kinesic communication, the impact of such technologies on social dynamics, and the importance of ethical guidelines. Finally, we provide a reflection on our application of speculative design; proposing that it allows HRI researchers to not only consider functionality, but also how wearable robots influence definitions of what is considered acceptable or desirable in public settings.

</details>


### [527] [Unsupervised Learning for Detection of Rare Driving Scenarios](https://arxiv.org/abs/2512.23585)
*Dat Le,Thomas Manhardt,Moritz Venator,Johannes Betz*

Main category: cs.RO

TL;DR: The paper presents an unsupervised framework leveraging Deep Isolation Forest for detecting rare and hazardous driving scenarios using naturalistic driving data.


<details>
  <summary>Details</summary>
Motivation: To enhance safety and reliability in autonomous systems by effectively identifying rare and hazardous driving scenarios.

Method: Utilizes Deep Isolation Forest for anomaly detection combining neural networks with Isolation Forests. Preprocessed data is visualized via t-SNE, with evaluation through proxy ground truth and qualitative video inspections.

Result: The approach successfully detects rare and extreme driving scenarios, demonstrating scalability in autonomous driving anomaly detection systems.

Conclusion: The framework shows effectiveness but is limited by reliance on proxy ground truths and manually defined feature combinations, suggesting areas for further improvement in real-world anomaly coverage.

Abstract: The detection of rare and hazardous driving scenarios is a critical challenge for ensuring the safety and reliability of autonomous systems. This research explores an unsupervised learning framework for detecting rare and extreme driving scenarios using naturalistic driving data (NDD). We leverage the recently proposed Deep Isolation Forest (DIF), an anomaly detection algorithm that combines neural network-based feature representations with Isolation Forests (IFs), to identify non-linear and complex anomalies. Data from perception modules, capturing vehicle dynamics and environmental conditions, is preprocessed into structured statistical features extracted from sliding windows. The framework incorporates t-distributed stochastic neighbor embedding (t-SNE) for dimensionality reduction and visualization, enabling better interpretability of detected anomalies. Evaluation is conducted using a proxy ground truth, combining quantitative metrics with qualitative video frame inspection. Our results demonstrate that the proposed approach effectively identifies rare and hazardous driving scenarios, providing a scalable solution for anomaly detection in autonomous driving systems. Given the study's methodology, it was unavoidable to depend on proxy ground truth and manually defined feature combinations, which do not encompass the full range of real-world driving anomalies or their nuanced contextual dependencies.

</details>


### [528] [A Kalman Filter-Based Disturbance Observer for Steer-by-Wire Systems](https://arxiv.org/abs/2512.23593)
*Nikolai Beving,Jonas Marxen,Steffen Mueller,Johannes Betz*

Main category: cs.RO

TL;DR: The paper proposes a Kalman filter-based disturbance observer to estimate high-frequency driver torque disturbances in Steer-by-Wire systems using motor state measurements instead of costly torque sensors.


<details>
  <summary>Details</summary>
Motivation: Steer-by-Wire systems face high-frequency disturbances caused by driver impedance, which degrade performance and necessitate a solution that avoids expensive or impractical hardware.

Method: A Kalman filter-based disturbance observer was designed, integrating a PT1-lag approximation for driver torque into both linear and nonlinear system models. It was evaluated using simulations with linear and extended Kalman filters.

Result: The observer reconstructed high-frequency disturbances with minimal delay (14ms), and the extended Kalman filter surpassed the linear one in handling nonlinearities.

Conclusion: The disturbance observer shows promise for enhancing Steer-by-Wire systems, but real-world experimentation is needed for robust validation.

Abstract: Steer-by-Wire systems replace mechanical linkages, which provide benefits like weight reduction, design flexibility, and compatibility with autonomous driving. However, they are susceptible to high-frequency disturbances from unintentional driver torque, known as driver impedance, which can degrade steering performance. Existing approaches either rely on direct torque sensors, which are costly and impractical, or lack the temporal resolution to capture rapid, high-frequency driver-induced disturbances. We address this limitation by designing a Kalman filter-based disturbance observer that estimates high-frequency driver torque using only motor state measurements. We model the drivers passive torque as an extended state using a PT1-lag approximation and integrate it into both linear and nonlinear Steer-by-Wire system models. In this paper, we present the design, implementation and simulation of this disturbance observer with an evaluation of different Kalman filter variants. Our findings indicate that the proposed disturbance observer accurately reconstructs driver-induced disturbances with only minimal delay 14ms. We show that a nonlinear extended Kalman Filter outperforms its linear counterpart in handling frictional nonlinearities, improving estimation during transitions from static to dynamic friction. Given the study's methodology, it was unavoidable to rely on simulation-based validation rather than real-world experimentation. Further studies are needed to investigate the robustness of the observers under real-world driving conditions.

</details>


### [529] [Interactive Robot Programming for Surface Finishing via Task-Centric Mixed Reality Interfaces](https://arxiv.org/abs/2512.23616)
*Christoph Willibald,Lugh Martensen,Thomas Eiband,Dongheui Lee*

Main category: cs.RO

TL;DR: This paper introduces a user-friendly robot programming approach for surface finishing tasks in small-scale settings, enabling non-experts to operate robots through interactive workflows.


<details>
  <summary>Details</summary>
Motivation: The motivation is to simplify robot programming to overcome the barriers of requiring robotics expertise, making robots accessible for tasks in small-scale craft and manufacturing settings.

Method: The authors developed a surface segmentation algorithm that includes human input, providing continuous visual feedback for iterative improvements, and generates robot trajectories for defined processing areas.

Result: Through user studies with various interaction designs, the authors identified an optimal interface that reduces user workload, enhances usability, and allows effective task programming by non-experts.

Conclusion: The proposed approach successfully bridges the gap in robot programming for non-experts, facilitating the use of collaborative robots for surface finishing in small-batch production settings.

Abstract: Lengthy setup processes that require robotics expertise remain a major barrier to deploying robots for tasks involving high product variability and small batch sizes. As a result, collaborative robots, despite their advanced sensing and control capabilities, are rarely used for surface finishing in small-scale craft and manufacturing settings. To address this gap, we propose a novel robot programming approach that enables non-experts to intuitively program robots through interactive, task-focused workflows. For that, we developed a new surface segmentation algorithm that incorporates human input to identify and refine workpiece regions for processing. Throughout the programming process, users receive continuous visual feedback on the robot's learned model, enabling them to iteratively refine the segmentation result. Based on the segmented surface model, a robot trajectory is generated to cover the desired processing area. We evaluated multiple interaction designs across two comprehensive user studies to derive an optimal interface that significantly reduces user workload, improves usability and enables effective task programming even for users with limited practical experience.

</details>


### [530] [The N-5 Scaling Law: Topological Dimensionality Reduction in the Optimal Design of Fully-actuated Multirotors](https://arxiv.org/abs/2512.23619)
*Antonio Franchi*

Main category: cs.RO

TL;DR: The paper examines the optimization design of omnidirectional N-rotor aerial vehicles, focusing on the topological structure of the solution space instead of traditional parametric optimization.


<details>
  <summary>Details</summary>
Motivation: To explore the intrinsic topological structure of the optimization landscape in N-rotor aerial vehicle design beyond the conventional parametric optimization approach.

Method: The design problem is formulated on the product manifold of Projective Lines (\RP^2^N), optimizing a coordinate-invariant Log-Volume isotropy metric while fixing rotor positions to polyhedral chassis vertices and varying their orientations.

Result: The topology of global optima is strictly determined by chassis symmetry. For irregular geometries, solutions are discrete points, while regular geometries reveal continuous solution spaces like N-dimensional Tori or 1D curve branches. An empirical N-5 Scaling Law is established to predict optimal configuration spaces.

Conclusion: The findings introduce a design redundancy allowing optimal reconfiguration with preserved isotropic control, identifying topology as a critical enabler for adaptive and versatile N-rotor vehicle designs.

Abstract: The geometric design of fully-actuated and omnidirectional N-rotor aerial vehicles is conventionally formulated as a parametric optimization problem, seeking a single optimal set of N orientations within a fixed architectural family. This work departs from that paradigm to investigate the intrinsic topological structure of the optimization landscape itself. We formulate the design problem on the product manifold of Projective Lines \RP^2^N, fixing the rotor positions to the vertices of polyhedral chassis while varying their lines of action. By minimizing a coordinate-invariant Log-Volume isotropy metric, we reveal that the topology of the global optima is governed strictly by the symmetry of the chassis. For generic (irregular) vertex arrangements, the solutions appear as a discrete set of isolated points. However, as the chassis geometry approaches regularity, the solution space undergoes a critical phase transition, collapsing onto an N-dimensional Torus of the lines tangent at the vertexes to the circumscribing sphere of the chassis, and subsequently reducing to continuous 1-dimensional curves driven by Affine Phase Locking. We synthesize these observations into the N-5 Scaling Law: an empirical relationship holding for all examined regular planar polygons and Platonic solids (N <= 10), where the space of optimal configurations consists of K=N-5 disconnected 1D topological branches. We demonstrate that these locking patterns correspond to a sequence of admissible Star Polygons {N/q}, allowing for the exact prediction of optimal phases for arbitrary N. Crucially, this topology reveals a design redundancy that enables optimality-preserving morphing: the vehicle can continuously reconfigure along these branches while preserving optimal isotropic control authority.

</details>


### [531] [RoboMirror: Understand Before You Imitate for Video to Humanoid Locomotion](https://arxiv.org/abs/2512.23649)
*Zhe Li,Cheng Chi,Yangyang Wei,Boan Zhu,Tao Huang,Zhenguo Sun,Yibo Peng,Pengwei Wang,Zhongyuan Wang,Fangzhou Liu,Chang Xu,Shanghang Zhang*

Main category: cs.RO

TL;DR: The paper introduces RoboMirror, a novel video-to-locomotion framework that enables humanoid control by understanding video content before imitating, without relying on pose reconstruction or retargeting.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the gap between visual understanding and locomotion control in humanoid systems, which currently rely on curated motion data or sparse text inputs, and fall short of real visual interpretation.

Method: RoboMirror utilizes Visual Language Models (VLMs) to extract motion intents from videos and employs a diffusion-based policy for generating semantically aligned and physically plausible locomotion, bypassing traditional pose reconstruction.

Result: Experiments showed that RoboMirror achieves an 80% reduction in third-person control latency, enables better telepresence control, and improves task success rates by 3.7% compared to conventional methods.

Conclusion: RoboMirror advances humanoid locomotion by integrating video understanding with control execution, bridging a critical gap in the field and demonstrating notable improvements in efficiency and accuracy.

Abstract: Humans learn locomotion through visual observation, interpreting visual content first before imitating actions. However, state-of-the-art humanoid locomotion systems rely on either curated motion capture trajectories or sparse text commands, leaving a critical gap between visual understanding and control. Text-to-motion methods suffer from semantic sparsity and staged pipeline errors, while video-based approaches only perform mechanical pose mimicry without genuine visual understanding. We propose RoboMirror, the first retargeting-free video-to-locomotion framework embodying "understand before you imitate". Leveraging VLMs, it distills raw egocentric/third-person videos into visual motion intents, which directly condition a diffusion-based policy to generate physically plausible, semantically aligned locomotion without explicit pose reconstruction or retargeting. Extensive experiments validate the effectiveness of RoboMirror, it enables telepresence via egocentric videos, drastically reduces third-person control latency by 80%, and achieves a 3.7% higher task success rate than baselines. By reframing humanoid control around video understanding, we bridge the visual understanding and action gap.

</details>


### [532] [Do You Have Freestyle? Expressive Humanoid Locomotion via Audio Control](https://arxiv.org/abs/2512.23650)
*Zhe Li,Cheng Chi,Yangyang Wei,Boan Zhu,Tao Huang,Zhenguo Sun,Yibo Peng,Pengwei Wang,Zhongyuan Wang,Fangzhou Liu,Chang Xu,Shanghang Zhang*

Main category: cs.RO

TL;DR: RoboPerform is a framework for humanoid robots to directly generate music-driven dance or speech-driven gestures from audio, eliminating explicit motion reconstruction delays and errors.


<details>
  <summary>Details</summary>
Motivation: Humanoid robots lack improvisational motion capabilities and instead rely on predefined or sparse commands, limiting their expressiveness and responsiveness to audio.

Method: RoboPerform follows the principle of 'motion = content + style,' using audio as implicit style signals. It integrates a ResMoE teacher policy for diverse motion pattern adaptation and a diffusion-based student policy for audio style injection, avoiding explicit motion reconstruction.

Result: The framework demonstrates low latency, high fidelity, and successful robotic performances that align with audio in both physical plausibility and responsiveness.

Conclusion: RoboPerform transforms humanoid robots into responsive performers capable of generating real-time, audio-driven motions with improved accuracy and expressiveness.

Abstract: Humans intuitively move to sound, but current humanoid robots lack expressive improvisational capabilities, confined to predefined motions or sparse commands. Generating motion from audio and then retargeting it to robots relies on explicit motion reconstruction, leading to cascaded errors, high latency, and disjointed acoustic-actuation mapping. We propose RoboPerform, the first unified audio-to-locomotion framework that can directly generate music-driven dance and speech-driven co-speech gestures from audio. Guided by the core principle of "motion = content + style", the framework treats audio as implicit style signals and eliminates the need for explicit motion reconstruction. RoboPerform integrates a ResMoE teacher policy for adapting to diverse motion patterns and a diffusion-based student policy for audio style injection. This retargeting-free design ensures low latency and high fidelity. Experimental validation shows that RoboPerform achieves promising results in physical plausibility and audio alignment, successfully transforming robots into responsive performers capable of reacting to audio.

</details>


### [533] [The Bulldozer Technique: Efficient Elimination of Local Minima Traps for APF-Based Robot Navigation](https://arxiv.org/abs/2512.23672)
*Mohammed Baziyad,Manal Al Shohna,Tamer Rabie*

Main category: cs.RO

TL;DR: This paper introduces the Bulldozer path planning technique to address the local minima issue in Artificial Potential Field (APF) methods, ensuring faster execution and high-quality paths.


<details>
  <summary>Details</summary>
Motivation: To overcome the local minima trap problem in conventional Artificial Potential Field (APF) path planning while maintaining its advantages such as simplicity and low computational needs.

Method: The Bulldozer technique modifies APF with a backfilling mechanism that increases potential values in local minima regions and incorporates a ramp-based enhancement to help robots escape local traps.

Result: Experimental validations on physical robots across complex maps show that the Bulldozer technique outperforms standard APF and other algorithms (A*, PRM, RRT) in resolving local minima, execution speed, and path quality.

Conclusion: Bulldozer successfully addresses local minima problems in APF methods, ensuring smooth and traceable paths, making it practical for real-world autonomous mobile robot navigation.

Abstract: Path planning is a fundamental component in autonomous mobile robotics, enabling a robot to navigate from its current location to a desired goal while avoiding obstacles. Among the various techniques, Artificial Potential Field (APF) methods have gained popularity due to their simplicity, real-time responsiveness, and low computational requirements. However, a major limitation of conventional APF approaches is the local minima trap problem, where the robot becomes stuck in a position with no clear direction toward the goal. This paper proposes a novel path planning technique, termed the Bulldozer, which addresses the local minima issue while preserving the inherent advantages of APF. The Bulldozer technique introduces a backfilling mechanism that systematically identifies and eliminates local minima regions by increasing their potential values, analogous to a bulldozer filling potholes in a road. Additionally, a ramp-based enhancement is incorporated to assist the robot in escaping trap areas when starting within a local minimum. The proposed technique is experimentally validated using a physical mobile robot across various maps with increasing complexity. Comparative analyses are conducted against standard APF, adaptive APF, and well-established planning algorithms such as A*, PRM, and RRT. Results demonstrate that the Bulldozer technique effectively resolves the local minima problem while achieving superior execution speed and competitive path quality. Furthermore, a kinematic tracking controller is employed to assess the smoothness and traceability of the planned paths, confirming their suitability for real-world execution.

</details>


### [534] [Robo-Dopamine: General Process Reward Modeling for High-Precision Robotic Manipulation](https://arxiv.org/abs/2512.23703)
*Huajie Tan,Sixiang Chen,Yijie Xu,Zixiao Wang,Yuheng Ji,Cheng Chi,Yaoxu Lyu,Zhongxia Zhao,Xiansheng Chen,Peterson Co,Shaoxuan Xie,Guocai Yao,Pengwei Wang,Zhongyuan Wang,Shanghang Zhang*

Main category: cs.RO

TL;DR: This paper introduces Dopamine-Reward, a new general-purpose, step-aware reward modeling method with a robust RL framework (Dopamine-RL) for robotics policy learning, addressing issues with reward function design and shaping.


<details>
  <summary>Details</summary>
Motivation: Applying reinforcement learning to robotics is often hindered by the challenge of designing effective reward functions. Existing learning-based models face limitations like lack of step-awareness and reliance on single-view perceptions, leading to unreliable progress assessment and flawed reward shaping.

Method: The authors propose Dopamine-Reward with the General Reward Model (GRM) trained on a 3,400+ hour dataset, which uses Step-wise Reward Discretization and Multi-Perspective Reward Fusion. Built on this, Dopamine-RL employs Policy-Invariant Reward Shaping to enable efficient policy optimization without altering outcomes.

Result: The method achieves state-of-the-art accuracy in reward assessments. It enables policy learning improvements from near-zero to 95% success, using only 150 online rollouts (around 1 hour of robot interaction), with strong generalization across tasks.

Conclusion: Dopamine-Reward and Dopamine-RL provide significant advancements in terms of reward modeling and policy optimization in RL for robotics, solving prior issues and demonstrating efficiency and generalization in diverse tasks.

Abstract: The primary obstacle for applying reinforcement learning (RL) to real-world robotics is the design of effective reward functions. While recently learning-based Process Reward Models (PRMs) are a promising direction, they are often hindered by two fundamental limitations: their reward models lack step-aware understanding and rely on single-view perception, leading to unreliable assessments of fine-grained manipulation progress; and their reward shaping procedures are theoretically unsound, often inducing a semantic trap that misguides policy optimization. To address these, we introduce Dopamine-Reward, a novel reward modeling method for learning a general-purpose, step-aware process reward model from multi-view inputs. At its core is our General Reward Model (GRM), trained on a vast 3,400+ hour dataset, which leverages Step-wise Reward Discretization for structural understanding and Multi-Perspective Reward Fusion to overcome perceptual limitations. Building upon Dopamine-Reward, we propose Dopamine-RL, a robust policy learning framework that employs a theoretically-sound Policy-Invariant Reward Shaping method, which enables the agent to leverage dense rewards for efficient self-improvement without altering the optimal policy, thereby fundamentally avoiding the semantic trap. Extensive experiments across diverse simulated and real-world tasks validate our approach. GRM achieves state-of-the-art accuracy in reward assessment, and Dopamine-RL built on GRM significantly improves policy learning efficiency. For instance, after GRM is adapted to a new task in a one-shot manner from a single expert trajectory, the resulting reward model enables Dopamine-RL to improve the policy from near-zero to 95% success with only 150 online rollouts (approximately 1 hour of real robot interaction), while retaining strong generalization across tasks. Project website: https://robo-dopamine.github.io

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [535] [Syntax Is Not Enough: An Empirical Study of Small Transformer Models for Neural Code Repair](https://arxiv.org/abs/2512.22216)
*Shaunak Samant*

Main category: cs.SE

TL;DR: The paper evaluates small transformer models for automated program repair and finds that while grammatical correctness is often achieved, semantic correctness and effective bug fixes remain elusive.


<details>
  <summary>Details</summary>
Motivation: To evaluate the practicality of deploying small transformer-based neural models for repairing real-world Java bugs and examine the relationship between syntactic and semantic correctness.

Method: They fine-tune CodeT5-small (60.5M parameters) with 52,364 Java bug-fix pairs from CodeXGLUE, measuring token-level performance and syntactic validity using AST parsing.

Result: The model achieves ninety-four percent syntactic validity but fails at semantic correctness under exact-match evaluation, reproducing buggy input in about eighty percent of cases.

Conclusion: While small transformer models show promise in producing syntactically valid code, they are insufficient for genuinely repairing bugs in real-world scenarios.

Abstract: Automated program repair using neural models has shown promising results on benchmark datasets, yet practical deployment remains limited. In this study, we examine whether a small transformer model can meaningfully repair real-world Java bugs and whether syntactic correctness is a reliable proxy for semantic correctness.
  We fine-tune CodeT5-small (60.5M parameters) on 52,364 Java bug-fix pairs from CodeXGLUE and evaluate both token-level performance and syntactic validity using AST parsing. While the model converges cleanly and achieves high grammatical correctness, producing syntactically valid Java code in approximately ninety-four percent of cases, it fails to generate correct repairs under exact-match evaluation, achieving zero exact matches. In approximately eighty percent of cases, the model reproduces the buggy input verbatim.

</details>


### [536] [Failure Analysis of Safety Controllers in Autonomous Vehicles Under Object-Based LiDAR Attacks](https://arxiv.org/abs/2512.22244)
*Daniyal Ganiuly,Nurzhau Bolatbek,Assel Smaiyl*

Main category: cs.SE

TL;DR: This paper studies the effects of LiDAR-based attacks on autonomous vehicle safety controllers specifically during highway driving scenarios, focusing on cut in and car-following incidents.


<details>
  <summary>Details</summary>
Motivation: The study aims to assess the vulnerabilities of LiDAR perception in autonomous vehicles and its downstream effects on critical safety functions, as there is limited understanding of how perception attacks influence vehicle safety controllers.

Method: The authors conducted a failure analysis using a high-fidelity simulation framework integrating LiDAR perception, object tracking, and closed-loop vehicle control to model attack scenarios with adversarial objects that create consistent perception errors.

Result: Results show that spoofed or hallucinated objects via LiDAR attacks lead to unsafe braking, delayed responses to hazards, and unstable control behaviors, with significant increases in unsafe deceleration and time-to-collision violations during cut-in scenarios.

Conclusion: The study highlights the gap between perception robustness and vehicle control safety and provides insights for the development of attack-aware safety mechanisms and more resilient control strategies for autonomous vehicles.

Abstract: Autonomous vehicles rely on LiDAR based perception to support safety critical control functions such as adaptive cruise control and automatic emergency braking. While previous research has shown that LiDAR perception can be manipulated through object based spoofing and injection attacks, the impact of such attacks on vehicle safety controllers is still not well understood. This paper presents a systematic failure analysis of longitudinal safety controllers under object based LiDAR attacks in highway driving scenarios. The study focuses on realistic cut in and car following situations in which adversarial objects introduce persistent perception errors without directly modifying vehicle control software. A high fidelity simulation framework integrating LiDAR perception, object tracking, and closed loop vehicle control is used to evaluate how false and displaced object detections propagate through the perception planning and control pipeline. The results demonstrate that even short duration LiDAR induced object hallucinations can trigger unsafe braking, delayed responses to real hazards, and unstable control behavior. In cut in scenarios, a clear increase in unsafe deceleration events and time to collision violations is observed when compared to benign conditions, despite identical controller parameters. The analysis further shows that controller failures are more strongly influenced by the temporal consistency of spoofed objects than by spatial inaccuracies alone. These findings reveal a critical gap between perception robustness and control level safety guarantees in autonomous driving systems. By explicitly characterizing safety controller failure modes under adversarial perception, this work provides practical insights for the design of attack aware safety mechanisms and more resilient control strategies for LiDAR dependent autonomous vehicles.

</details>


### [537] [Hallucination Detection for LLM-based Text-to-SQL Generation via Two-Stage Metamorphic Testing](https://arxiv.org/abs/2512.22250)
*Bo Yang,Yinfen Xia,Weisong Sun,Yang Liu*

Main category: cs.SE

TL;DR: The paper introduces SQLHD, a method leveraging metamorphic testing to detect hallucinations in Text-to-SQL generated queries by large language models (LLMs), without relying on standard answers. It shows superior performance in detection accuracy.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the issue of hallucinations in Text-to-SQL queries generated by LLMs, which negatively affect downstream applications. Current error detection methods are insufficient, especially given the lack of ground-truth data for LLM-generated SQL.

Method: SQLHD employs metamorphic testing for hallucination detection, segmenting the detection process into two stages: schema-linking and logical-synthesis. It uses structure-aware and logic-aware metamorphic relations to identify schema mappings or SQL artifacts violations, without requiring ground-truth SQL.

Result: Experimental results highlight SQLHD's superior detection capability, achieving F1-scores ranging from 69.36% to 82.76%. The method outperforms LLM self-evaluation techniques in identifying hallucinations.

Conclusion: SQLHD presents an effective, ground-truth-independent solution for detecting hallucinations in Text-to-SQL tasks, improving correctness and application reliability.

Abstract: In Text-to-SQL generation, large language models (LLMs) have shown strong generalization and adaptability. However, LLMs sometimes generate hallucinations, i.e.,unrealistic or illogical content, which leads to incorrect SQL queries and negatively impacts downstream applications. Detecting these hallucinations is particularly challenging. Existing Text-to-SQL error detection methods, which are tailored for traditional deep learning models, face significant limitations when applied to LLMs. This is primarily due to the scarcity of ground-truth data. To address this challenge, we propose SQLHD, a novel hallucination detection method based on metamorphic testing (MT) that does not require standard answers. SQLHD splits the detection task into two sequentiial stages: schema-linking hallucination detection via eight structure-aware Metamorphic Relations (MRs) that perturb comparative words, entities, sentence structure or database schema, and logical-synthesis hallucination detection via nine logic-aware MRs that mutate prefix words, extremum expressions, comparison ranges or the entire database. In each stage the LLM is invoked separately to generate schema mappings or SQL artefacts; the follow-up outputs are cross-checked against their source counterparts through the corresponding MRs, and any violation is flagged as a hallucination without requiring ground-truth SQL. The experimental results demonstrate our method's superior performance in terms of the F1-score, which ranges from 69.36\% to 82.76\%. Additionally, SQLHD demonstrates superior performance over LLM Self-Evaluation methods, effectively identifying hallucinations in Text-to-SQL tasks.

</details>


### [538] [Agentic Software Issue Resolution with Large Language Models: A Survey](https://arxiv.org/abs/2512.22256)
*Zhonghao Jiang,David Lo,Zhongxin Liu*

Main category: cs.SE

TL;DR: This paper surveys 126 studies on advanced LLM-driven agentic systems for software issue resolution, categorizing benchmarks, techniques, and results.


<details>
  <summary>Details</summary>
Motivation: To address the complexity of real-world software issue resolution by leveraging agentic capabilities in LLMs for iterative reasoning and decision-making.

Method: A systematic survey and taxonomy creation based on 126 recent studies, analyzing workflows and emerging paradigms like agentic reinforcement learning.

Result: This work maps advancements in benchmarks, methods, and empirical studies, showcasing a paradigm shift in software engineering driven by agentic systems.

Conclusion: LLM-based agentic systems enhance software maintenance and serve as a platform for validating AI systems' reasoning, planning, and execution skills, with notable challenges and future research directions identified.

Abstract: Software issue resolution aims to address real-world issues in software repositories (e.g., bug fixing and efficiency optimization) based on natural language descriptions provided by users, representing a key aspect of software maintenance. With the rapid development of large language models (LLMs) in reasoning and generative capabilities, LLM-based approaches have made significant progress in automated software issue resolution. However, real-world software issue resolution is inherently complex and requires long-horizon reasoning, iterative exploration, and feedback-driven decision making, which demand agentic capabilities beyond conventional single-step approaches. Recently, LLM-based agentic systems have become mainstream for software issue resolution. Advancements in agentic software issue resolution not only greatly enhance software maintenance efficiency and quality but also provide a realistic environment for validating agentic systems' reasoning, planning, and execution capabilities, bridging artificial intelligence and software engineering.
  This work presents a systematic survey of 126 recent studies at the forefront of LLM-based agentic software issue resolution research. It outlines the general workflow of the task and establishes a taxonomy across three dimensions: benchmarks, techniques, and empirical studies. Furthermore, it highlights how the emergence of agentic reinforcement learning has brought a paradigm shift in the design and training of agentic systems for software engineering. Finally, it summarizes key challenges and outlines promising directions for future research.

</details>


### [539] [AI-Generated Code Is Not Reproducible (Yet): An Empirical Study of Dependency Gaps in LLM-Based Coding Agents](https://arxiv.org/abs/2512.22387)
*Bhanu Prakash Vangala,Ali Adibifar,Tanu Malik,Ashish Gehani*

Main category: cs.SE

TL;DR: This paper evaluates the reproducibility of code generated by three state-of-the-art large language models (LLMs) in different programming languages, finding that only 68.3% of the projects execute successfully without hidden dependencies.


<details>
  <summary>Details</summary>
Motivation: To understand and assess the reproducibility of code generated by LLM coding agents, addressing an unexplored aspect of their impact on software development.

Method: An empirical study using 300 projects generated from 100 prompts across Python, JavaScript, and Java. A three-layer dependency framework is introduced to analyze claimed, working, and runtime dependencies.

Result: 68.3% of the projects executed successfully out-of-the-box, with notable language variations (Python at 89.2%, Java at 44.0%). Significant hidden dependencies were discovered, with runtime dependencies expanding 13.5 times on average from declared ones.

Conclusion: Generated code from LLMs often lacks execution reproducibility, partially due to hidden dependencies. Reproducibility issues vary significantly depending on programming language.

Abstract: The rise of Large Language Models (LLMs) as coding agents promises to accelerate software development, but their impact on generated code reproducibility remains largely unexplored. This paper presents an empirical study investigating whether LLM-generated code can be executed successfully in a clean environment with only OS packages and using only the dependencies that the model specifies. We evaluate three state-of-the-art LLM coding agents (Claude Code, OpenAI Codex, and Gemini) across 300 projects generated from 100 standardized prompts in Python, JavaScript, and Java. We introduce a three-layer dependency framework (distinguishing between claimed, working, and runtime dependencies) to quantify execution reproducibility. Our results show that only 68.3% of projects execute out-of-the-box, with substantial variation across languages (Python 89.2%, Java 44.0%). We also find a 13.5 times average expansion from declared to actual runtime dependencies, revealing significant hidden dependencies.

</details>


### [540] [Building Software by Rolling the Dice: A Qualitative Study of Vibe Coding](https://arxiv.org/abs/2512.22418)
*Yi-Hung Chou,Boyuan Jiang,Yi Wen Chen,Mingyue Weng,Victoria Jackson,Thomas Zimmermann,James A. Jones*

Main category: cs.SE

TL;DR: The paper examines how software developers use large language models (LLMs) for 'vibe coding,' primarily generating software through prompts rather than writing code. It identifies varying approaches, challenges with AI randomness, and effects on practices and trust.


<details>
  <summary>Details</summary>
Motivation: To understand how 'vibe coding,' a new AI-assisted software development practice, is reshaping software engineering and to identify its implications for tool design and education.

Method: The study applied grounded theory to analyze 20 vibe-coding videos, which included 7 live-streamed coding sessions and 13 opinion videos, along with analyzing activity durations and the intents of prompts.

Result: The study categorized vibe coders' behaviors, ranging from minimal inspection of AI-generated code to deep engagement with output adaptation. It also highlighted divergent mental models influenced by coder expertise and AI trust, as well as the challenges posed by AI's stochastic nature.

Conclusion: The findings provide insights into the emerging practice of vibe coding, emphasizing the need for new research and improvements in tools and education to better support developers using AI-driven methods.

Abstract: Large language models (LLMs) are reshaping software engineering by enabling "vibe coding," in which developers build software primarily through prompts rather than writing code. Although widely publicized as a productivity breakthrough, little is known about how practitioners actually define and engage in these practices. To shed light on this emerging phenomenon, we conducted a grounded theory study of 20 vibe-coding videos, including 7 live-streamed coding sessions (about 16 hours, 254 prompts) and 13 opinion videos (about 5 hours), supported by additional analysis of activity durations and prompt intents. Our findings reveal a spectrum of behaviors: some vibe coders rely almost entirely on AI without inspecting code, while others examine and adapt generated outputs. Across approaches, all must contend with the stochastic nature of generation, with debugging and refinement often described as "rolling the dice." Further, divergent mental models, shaped by vibe coders' expertise and reliance on AI, influence prompting strategies, evaluation practices, and levels of trust. These findings open new directions for research on the future of software engineering and point to practical opportunities for tool design and education.

</details>


### [541] [GraphLocator: Graph-guided Causal Reasoning for Issue Localization](https://arxiv.org/abs/2512.22469)
*Wei Liu,Chao Peng,Pengfei Gao,Aofan Liu,Wei Zhang,Haiyan Zhao,Zhi Jin*

Main category: cs.SE

TL;DR: The paper introduces GraphLocator, a method to improve issue localization in software repositories, addressing key challenges of semantic mismatches using a Causal Issue Graph (CIG).


<details>
  <summary>Details</summary>
Motivation: The motivation is to address critical challenges in software issue localization, specifically the mismatches between issue descriptions and underlying code: symptom-to-cause mismatches and one-to-many mismatches.

Method: The method employs GraphLocator, which creates Causal Issue Graphs to capture sub-issues and dependencies, using a workflow of locating symptom vertices and dynamically expanding the graph through iterative reasoning.

Result: GraphLocator delivers significantly improved performance, with up to +19.49% higher recall and +11.89% precision over baselines. It also addresses both mismatch challenges effectively.

Conclusion: GraphLocator offers a robust model for issue localization, facilitating better identification of relevant code entities and demonstrating impactful results in bridging the semantic gap for software engineering tasks.

Abstract: The issue localization task aims to identify the locations in a software repository that requires modification given a natural language issue description. This task is fundamental yet challenging in automated software engineering due to the semantic gap between issue description and source code implementation. This gap manifests as two mismatches:(1) symptom-to-cause mismatches, where descriptions do not explicitly reveal underlying root causes; (2) one-to-many mismatches, where a single issue corresponds to multiple interdependent code entities. To address these two mismatches, we propose GraphLocator, an approach that mitigates symptom-to-cause mismatches through causal structure discovering and resolves one-to-many mismatches via dynamic issue disentangling. The key artifact is the causal issue graph (CIG), in which vertices represent discovered sub-issues along with their associated code entities, and edges encode the causal dependencies between them. The workflow of GraphLocator consists of two phases: symptom vertices locating and dynamic CIG discovering; it first identifies symptom locations on the repository graph, then dynamically expands the CIG by iteratively reasoning over neighboring vertices. Experiments on three real-world datasets demonstrates the effectiveness of GraphLocator: (1) Compared with baselines, GraphLocator achieves more accurate localization with average improvements of +19.49% in function-level recall and +11.89% in precision. (2) GraphLocator outperforms baselines on both symptom-to-cause and one-to-many mismatch scenarios, achieving recall improvement of +16.44% and +19.18%, precision improvement of +7.78% and +13.23%, respectively. (3) The CIG generated by GraphLocator yields the highest relative improvement, resulting in a 28.74% increase in performance on downstream resolving task.

</details>


### [542] [Isolating Compiler Faults via Multiple Pairs of Adversarial Compilation Configurations](https://arxiv.org/abs/2512.22538)
*Qingyang Li,Yibiao Yang,Maolin Sun,Jiangchang Wu,Qingkai Shi,Yuming Zhou*

Main category: cs.SE

TL;DR: The paper introduces MultiConf, a novel method to effectively identify and localize compiler bugs using adversarial compilation configurations, achieving notable improvements compared to existing techniques.


<details>
  <summary>Details</summary>
Motivation: Fault localization in compilers is complex due to their large and intricate infrastructures, necessitating an effective and automated method to isolate compiler faults.

Method: MultiConf constructs failing and passing configurations with fine-grained differences, applies Spectrum-Based Fault Localization (SBFL) to rank suspicious source files, and uses a weighted voting system for robust fault localization.

Result: MultiConf significantly outperforms prior techniques, successfully identifying 27 out of 60 compiler bugs at the Top-1 file level, with improvements of 35.0% and 28.6% over two state-of-the-art methods.

Conclusion: MultiConf presents an efficient and accurate approach for localizing compiler faults, advancing the state of the art and providing a valuable tool for managing compiler bugs.

Abstract: Compilers are fundamental to modern software development, making the effective identification and resolution of compiler faults essential. However, localizing these faults to specific source files remains highly challenging due to the complexity and scale of modern compiler infrastructures. In this study, we propose MultiConf, a novel approach that automatically isolates compiler faults by constructing multiple pairs of adversarial compilation configurations. Each adversarial compilation configuration pair consists of a failing configuration and its corresponding passing configuration, which differ in only a small number of fine-grained options. MultiConf generates failing configurations through a lightweight construction process and derives the corresponding passing configurations by selectively disabling bug-related fine-grained options. We then employ a Spectrum-Based Fault Localization (SBFL) formula to rank the suspiciousness of compiler source files. Each adversarial configuration pair independently produces a ranking, which is subsequently aggregated using a weighted voting scheme to derive a final suspiciousness ranking, enabling more accurate and robust fault localization. We evaluate MultiConf on a benchmark of 60 real-world GCC compiler bugs. The results demonstrate that MultiConf significantly outperforms existing compiler fault localization techniques in both effectiveness and efficiency. In particular, MultiConf successfully localizes 27 out of 60 bugs at the Top-1 file level, representing improvements of 35.0% and 28.6% over the two state-of-the-art approaches, Odfl(20) and Basic(21), respectively.

</details>


### [543] [Rethinking the Capability of Fine-Tuned Language Models for Automated Vulnerability Repair](https://arxiv.org/abs/2512.22633)
*Woorim Han,Yeongjun Kwak,Miseon Yu,Kyeongmin Kim,Younghan Lee,Hyungon Moon,Yunheung Paek*

Main category: cs.SE

TL;DR: The paper explores the limitations of current learning-based automated vulnerability repair (AVR) techniques, revealing overfitting issues and inadequacies in evaluation metrics. It introduces L-AVRBench to enhance testing effectiveness.


<details>
  <summary>Details</summary>
Motivation: To improve the robustness and evaluation of fine-tuned AVR models, addressing overfitting and the shortcomings of current match-based metrics.

Method: 1. Applied semantic-preserving transformations on test sets to assess model reliance on spurious features.
2. Re-split training, validation, and test sets to ensure mutual exclusivity for better generalization evaluation.
3. Proposed L-AVRBench, a benchmark for better testing AVR models.

Result: Identified limitations of state-of-the-art AVR models regarding unseen vulnerabilities and evaluation metrics, offering new insights and tools in the field.

Conclusion: Existing learning-based AVR models often overfit and are assessed with inadequate metrics. Novel approaches like L-AVRBench are needed to ensure robust patch-generation capabilities and proper evaluation.

Abstract: Learning-based automated vulnerability repair (AVR) techniques that utilize fine-tuned language models have shown promise in generating vulnerability patches. However, questions remain about their ability to repair unseen vulnerabilities. Our empirical study reveals that state-of-the-art models often overfit to the training set and are evaluated using training, validation, and test sets that are not mutually exclusive. Furthermore, relying on match-based metrics that compare generated patches to reference fixes at the token level has some limitations, failing to account for the possibility of various valid ways to patch the vulnerability. In this paper, we examine the capabilities of state-of-the-art fine-tuned AVR models and the adequacy of match-based evaluation metrics in three ways. First, we apply semantic-preserving transformations to test sets in order to determine whether models truly learn robust vulnerability-repair patterns or simply rely on spurious features. Second, we re-split the training, validation, and test sets to be mutually exclusive and evaluate the models on the revised test set to assess their generalization capabilities. Third, we introduce L-AVRBench, a test-based benchmark tailored for learning-based AVR, to overcome the limitations of match-based metrics and examine the AVR models' true repair capabilities.

</details>


### [544] [CFIghter: Automated Control-Flow Integrity Enablement and Evaluation for Legacy C/C++ Systems](https://arxiv.org/abs/2512.22701)
*Sabine Houy,Bruno Kreyssig,Alexandre Bartel*

Main category: cs.SE

TL;DR: The paper introduces CFIghter, an automated system to enable strict, type-based Control-Flow Integrity for large C/C++ projects, resolving unintended policy violations detected via test suites.


<details>
  <summary>Details</summary>
Motivation: Deploying strict, type-based Control-Flow Integrity is difficult in large C/C++ projects due to visibility mismatches, type inconsistencies, and behavioral failures.

Method: CFIghter introduces automated compatibility repair via whole-program analysis and guided runtime monitoring, iteratively applying minimal required adjustments to ensure CFI enforcement passes all tests.

Result: Experiments on four GNU projects, including util-linux, show CFIghter resolves all visibility-related build errors and repairs 95.8% of unintended CFI violations while retaining strict enforcement on over 89% indirect control-flow sites.

Conclusion: CFIghter makes strict compiler-based type-based CFI deployable without manual source-code modification in mature modular C software projects.

Abstract: Compiler-based Control-Flow Integrity (CFI) offers strong forward-edge protection but remains challenging to deploy in large C/C++ software due to visibility mismatches, type inconsistencies, and unintended behavioral failures. We present CFIghter, the first fully automated system that enables strict, type-based CFI in real-world projects by detecting, classifying, and repairing unintended policy violations exposed by the test suite. CFIghter integrates whole-program analysis with guided runtime monitoring and iteratively applies the minimal necessary adjustments to CFI enforcement only where required, stopping once all tests pass or remaining failures are deemed unresolvable. We evaluate CFIghter on four GNU projects. It resolves all visibility-related build errors and automatically repairs 95.8% of unintended CFI violations in the large, multi-library util-linux codebase, while retaining strict enforcement at over 89% of indirect control-flow sites. Across all subjects, CFIghter preserves strict type-based CFI for the majority of the codebase without requiring manual source-code changes, relying only on automatically generated visibility adjustments and localized enforcement scopes where necessary. These results show that automated compatibility repair makes strict compiler CFI practically deployable in mature, modular C software.

</details>


### [545] [From Rookie to Expert: Manipulating LLMs for Automated Vulnerability Exploitation in Enterprise Software](https://arxiv.org/abs/2512.22753)
*Moustapha Awwalou Diouf,Maimouna Tamah Diao,Iyiola Emmanuel Olatunji,Abdoul Kader Kaboré,Jordan Samhi,Gervais Mendy,Samuel Ouya,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.SE

TL;DR: This paper demonstrates how publicly available large language models (LLMs) can be manipulated to enable non-experts to create exploitative software attacks, challenging long-standing security assumptions in software engineering.


<details>
  <summary>Details</summary>
Motivation: The research is motivated by the increased accessibility of LLMs in software engineering. While LLMs aim to democratize app development for non-programmers, their accessibility creates new risks by removing the requirement of technical expertise to launch software attacks.

Method: The study introduces the RSA (Role-assignment, Scenario-pretexting, and Action-solicitation) strategy to bypass LLM safety mechanisms and generate functional software exploits. Five mainstream LLMs were tested against the Odoo ERP platform, and their ability to create exploits was evaluated via iterative queries.

Result: The study achieved a 100% success rate in generating working CVE exploits using the RSA strategy on all tested LLMs. This included models like GPT-4o, Gemini, Claude, Microsoft Copilot, and DeepSeek, which produced at least one effective exploit in 3-4 rounds of interaction.

Conclusion: The findings challenge fundamental security principles in software engineering, including assumptions about technical expertise, protective technical complexity, and traditional security boundaries. The paper calls for a rethinking of software security practices in response to the risks posed by LLM-assisted exploit generation.

Abstract: LLMs democratize software engineering by enabling non-programmers to create applications, but this same accessibility fundamentally undermines security assumptions that have guided software engineering for decades. We show in this work how publicly available LLMs can be socially engineered to transform novices into capable attackers, challenging the foundational principle that exploitation requires technical expertise. To that end, we propose RSA (Role-assignment, Scenario-pretexting, and Action-solicitation), a pretexting strategy that manipulates LLMs into generating functional exploits despite their safety mechanisms. Testing against Odoo -- a widely used ERP platform, we evaluated five mainstream LLMs (GPT-4o, Gemini, Claude, Microsoft Copilot, and DeepSeek) and achieved a 100% success rate: tested CVE yielded at least one working exploit within 3-4 prompting rounds. While prior work [13] found LLM-assisted attacks difficult and requiring manual effort, we demonstrate that this overhead can be eliminated entirely.
  Our findings invalidate core software engineering security principles: the distinction between technical and non-technical actors no longer provides valid threat models; technical complexity of vulnerability descriptions offers no protection when LLMs can abstract it away; and traditional security boundaries dissolve when the same tools that build software can be manipulated to break it. This represents a paradigm shift in software engineering -- we must redesign security practices for an era where exploitation requires only the ability to craft prompts, not understand code.
  Artifacts available at: https://anonymous.4open.science/r/From-Rookie-to-Attacker-D8B3.

</details>


### [546] [FasterPy: An LLM-based Code Execution Efficiency Optimization Framework](https://arxiv.org/abs/2512.22827)
*Yue Wu,Minghao Han,Ruiyin Li,Peng Liang,Amjed Tahir,Zengyang Li,Qiong Feng,Mojtaba Shahin*

Main category: cs.SE

TL;DR: This paper introduces FasterPy, a framework using LLMs for Python code optimization, by combining Retrieval-Augmented Generation and Low-Rank Adaptation.


<details>
  <summary>Details</summary>
Motivation: Code optimization is important due to the prevalence of performance bugs. Traditional rule-based methods and earlier ML approaches are labor-intensive, costly, and not scalable, necessitating a novel approach leveraging LLMs.

Method: FasterPy employs Retrieval-Augmented Generation supported by a knowledge base of code examples and Low-Rank Adaptation to improve Python code execution efficiency.

Result: FasterPy surpasses existing models in performance metrics on the PIE benchmark, showcasing its effectiveness in optimizing Python code.

Conclusion: FasterPy demonstrates the potential of LLM-driven optimization frameworks to improve Python code efficiency in a scalable, low-cost manner.

Abstract: Code often suffers from performance bugs. These bugs necessitate the research and practice of code optimization. Traditional rule-based methods rely on manually designing and maintaining rules for specific performance bugs (e.g., redundant loops, repeated computations), making them labor-intensive and limited in applicability. In recent years, machine learning and deep learning-based methods have emerged as promising alternatives by learning optimization heuristics from annotated code corpora and performance measurements. However, these approaches usually depend on specific program representations and meticulously crafted training datasets, making them costly to develop and difficult to scale. With the booming of Large Language Models (LLMs), their remarkable capabilities in code generation have opened new avenues for automated code optimization. In this work, we proposed FasterPy, a low-cost and efficient framework that adapts LLMs to optimize the execution efficiency of Python code. FasterPy combines Retrieval-Augmented Generation (RAG), supported by a knowledge base constructed from existing performance-improving code pairs and corresponding performance measurements, with Low-Rank Adaptation (LoRA) to enhance code optimization performance. Our experimental results on the Performance Improving Code Edits (PIE) benchmark demonstrate that our method outperforms existing models on multiple metrics. The FasterPy tool and the experimental results are available at https://github.com/WuYue22/fasterpy.

</details>


### [547] [Towards the analysis of team members well-being](https://arxiv.org/abs/2512.22845)
*Zan Xu,Sari Nurfauziyyah,Anastasia Romanova,Kaamesh G S,Yiqun Gao,Maria Spichkova*

Main category: cs.SE

TL;DR: This paper explores how appreciation and acknowledgment of team contributions impact software development team members' well-being, and presents a team well-being analysis along with a prototype.


<details>
  <summary>Details</summary>
Motivation: To understand and enhance the well-being of software development team members, as it affects work productivity, health, and personal life.

Method: Conducting well-being analysis and developing a prototype for examining team member well-being in software development.

Result: The paper reports findings from their well-being analysis and introduces their developed prototype.

Conclusion: Acknowledgment and appreciation of team members play a significant role in improving well-being, and the developed prototype provides insights for fostering team member satisfaction.

Abstract: Many recent research studies have focused on the well-being of software development team members, as this aspect may be critical not only for productivity and performance at work but also for the physical health and personal life of employees. Many studies agree that an important factor of team member well-being is whether team members feel appreciated and acknowledged for their contributions. This paper presents the results of a project on the team well-being analysis as well as the prototype developed within the project.

</details>


### [548] [Interpretable Gallbladder Ultrasound Diagnosis: A Lightweight Web-Mobile Software Platform with Real-Time XAI](https://arxiv.org/abs/2512.23033)
*Fuyad Hasan Bhoyan,Prashanta Sarker,Parsia Noor Ethila,Md. Emon Hossain,Md Kaviul Hossain,Md Humaion Kabir Mehedi*

Main category: cs.SE

TL;DR: The paper develops an AI-driven software employing the MobResTaNet deep learning model for classifying gallbladder diseases with high accuracy from ultrasound images.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the challenges in ultrasound interpretation for diagnosing gallbladder diseases early and accurately, providing a tool to assist clinicians.

Method: A hybrid deep learning model, MobResTaNet, is used to classify gallbladder diseases from ultrasound images. Explainable AI (XAI) visualizations offer transparency in decision-making, and the software is deployed on web and mobile platforms.

Result: The model achieves 99.85% accuracy with only 2.24M parameters and provides on-the-go diagnostic support via accessible applications.

Conclusion: This software demonstrates reliable, interpretable, and efficient AI-based diagnostic support for gallbladder diseases, aiding in improved clinical decision-making.

Abstract: Early and accurate detection of gallbladder diseases is crucial, yet ultrasound interpretation is challenging. To address this, an AI-driven diagnostic software integrates our hybrid deep learning model MobResTaNet to classify ten categories, nine gallbladder disease types and normal directly from ultrasound images. The system delivers interpretable, real-time predictions via Explainable AI (XAI) visualizations, supporting transparent clinical decision-making. It achieves up to 99.85% accuracy with only 2.24M parameters. Deployed as web and mobile applications using HTML, CSS, JavaScript, Bootstrap, and Flutter, the software provides efficient, accessible, and trustworthy diagnostic support at the point of care

</details>


### [549] [An Automated Grey Literature Extraction Tool for Software Engineering](https://arxiv.org/abs/2512.23066)
*Houcine Abdelkader Cherief,Brahim Mahmoudi,Zacharie Chenail-Larcher,Naouel Moha,Quentin Sti'evenart,Florent Avellaneda*

Main category: cs.SE

TL;DR: GLiSE is a prompt-driven tool designed to collect and assess grey literature in software engineering efficiently, supporting reproducibility and relevance ranking.


<details>
  <summary>Details</summary>
Motivation: The difficulty of collecting and assessing grey literature at scale due to their diverse sources, formats, and APIs, which impedes reproducible, large-scale research synthesis in software engineering.

Method: A prompt-driven tool named GLiSE is introduced. It generates platform-specific queries, collects data from web sources like GitHub, Stack Overflow, and Google Search, and applies embedding-based semantic classifiers to filter and rank results based on relevance. All settings and queries are fully configurable.

Result: GLiSE, a curated dataset of search results, and an empirical study on the tool's usability are presented, demonstrating effectiveness in collecting and classifying relevant grey literature.

Conclusion: GLiSE provides a reproducible, efficient method for collecting and analyzing grey literature in software engineering, contributing curated datasets and tools to support further research.

Abstract: Grey literature is essential to software engineering research as it captures practices and decisions that rarely appear in academic venues. However, collecting and assessing it at scale remains difficult because of their heterogeneous sources, formats, and APIs that impede reproducible, large-scale synthesis. To address this issue, we present GLiSE, a prompt-driven tool that turns a research topic prompt into platform-specific queries, gathers results from common software-engineering web sources (GitHub, Stack Overflow) and Google Search, and uses embedding-based semantic classifiers to filter and rank results according to their relevance. GLiSE is designed for reproducibility with all settings being configuration-based, and every generated query being accessible. In this paper, (i) we present the GLiSE tool, (ii) provide a curated dataset of software engineering grey-literature search results classified by semantic relevance to their originating search intent, and (iii) conduct an empirical study on the usability of our tool.

</details>


### [550] [An Empirical Study of Generative AI Adoption in Software Engineering](https://arxiv.org/abs/2512.23327)
*Görkem Giray,Onur Demirörs,Marcos Kalinowski,Daniel Mendez*

Main category: cs.SE

TL;DR: This paper evaluates the experience of using GenAI tools in software engineering (SE), highlighting adoption trends, benefits, challenges, and implications.


<details>
  <summary>Details</summary>
Motivation: To understand the growing adoption of GenAI in SE and its impact on practices, professionals, and the broader SE community.

Method: The paper conducts an empirical investigation, analyzing the adoption and integration of GenAI tools in day-to-day SE tasks.

Result: Wide adoption of GenAI tools is observed, aiding tasks such as implementation, validation, and maintenance. Benefits include reduced cycle time, productivity gains, and quality improvements. Challenges involve unreliable outputs, prompt engineering difficulties, and security concerns.

Conclusion: GenAI tools are transforming SE practices with notable benefits. However, challenges in governance, skill adaptation, and risk mitigation still need addressing. Practitioners foresee GenAI reshaping their roles, requiring adaptive training and solutions for sustainable adoption.

Abstract: Context. GenAI tools are being increasingly adopted by practitioners in SE, promising support for several SE activities. Despite increasing adoption, we still lack empirical evidence on how GenAI is used in practice, the benefits it provides, the challenges it introduces, and its broader organizational and societal implications. Objective. This study aims to provide an overview of the status of GenAI adoption in SE. It investigates the status of GenAI adoption, associated benefits and challenges, institutionalization of tools and techniques, and anticipated long term impacts on SE professionals and the community. Results. The results indicate a wide adoption of GenAI tools and how they are deeply integrated into daily SE work, particularly for implementation, verification and validation, personal assistance, and maintenance-related tasks. Practitioners report substantial benefits, most notably reduction in cycle time, quality improvements, enhanced support in knowledge work, and productivity gains. However, objective measurement of productivity and quality remains limited in practice. Significant challenges persist, including incorrect or unreliable outputs, prompt engineering difficulties, validation overhead, security and privacy concerns, and risks of overreliance. Institutionalization of tools and techniques seems to be common, but it varies considerably, with a strong focus on tool access and less emphasis on training and governance. Practitioners expect GenAI to redefine rather than replace their roles, while expressing moderate concern about job market contraction and skill shifts.

</details>


### [551] [Securing the AI Supply Chain: What Can We Learn From Developer-Reported Security Issues and Solutions of AI Projects?](https://arxiv.org/abs/2512.23385)
*The Anh Nguyen,Triet Huynh Minh Le,M. Ali Babar*

Main category: cs.SE

TL;DR: This paper analyzes security issues within AI projects by examining developer-reported discussions from Hugging Face and GitHub. It introduces a fine-tuned distilBERT-based pipeline to identify 312,868 security-related discussions and offers a taxonomy of 32 issues and 24 solutions, highlighting challenges in models and data components.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the existing knowledge gap regarding AI-specific security threats and resolution practices, which hinder effective development of security strategies for AI projects.

Method: The authors developed a pipeline combining keyword matching with a fine-tuned distilBERT model to identify security-related discussions in repositories like Hugging Face and GitHub. They performed thematic analysis on a sampled dataset to create a taxonomy of security concerns and solutions.

Result: The study compiled a dataset of 312,868 security-related discussions and categorically identified 32 security issues and 24 solutions across four themes: System and Software, External Tools and Ecosystem, Model, and Data.

Conclusion: AI project security is impacted by dependencies and black-box components, with models and data posing significant challenges. The insights assist developers and researchers in addressing these real-world issues effectively.

Abstract: The rapid growth of Artificial Intelligence (AI) models and applications has led to an increasingly complex security landscape. Developers of AI projects must contend not only with traditional software supply chain issues but also with novel, AI-specific security threats. However, little is known about what security issues are commonly encountered and how they are resolved in practice. This gap hinders the development of effective security measures for each component of the AI supply chain. We bridge this gap by conducting an empirical investigation of developer-reported issues and solutions, based on discussions from Hugging Face and GitHub. To identify security-related discussions, we develop a pipeline that combines keyword matching with an optimal fine-tuned distilBERT classifier, which achieved the best performance in our extensive comparison of various deep learning and large language models. This pipeline produces a dataset of 312,868 security discussions, providing insights into the security reporting practices of AI applications and projects. We conduct a thematic analysis of 753 posts sampled from our dataset and uncover a fine-grained taxonomy of 32 security issues and 24 solutions across four themes: (1) System and Software, (2) External Tools and Ecosystem, (3) Model, and (4) Data. We reveal that many security issues arise from the complex dependencies and black-box nature of AI components. Notably, challenges related to Models and Data often lack concrete solutions. Our insights can offer evidence-based guidance for developers and researchers to address real-world security threats across the AI supply chain.

</details>


### [552] [An SLO Driven and Cost-Aware Autoscaling Framework for Kubernetes](https://arxiv.org/abs/2512.23415)
*Vinoth Punniyamoorthy,Bikesh Kumar,Sumit Saha,Lokesh Butra,Mayilsamy Palanigounder,Akash Kumar Agarwal,Kabilan Kannan*

Main category: cs.SE

TL;DR: The paper proposes an AIOps-driven autoscaling framework for Kubernetes to enhance SLO fulfillment, reduce costs, and increase operational transparency.


<details>
  <summary>Details</summary>
Motivation: The study addresses limitations in Kubernetes' native autoscaling mechanisms, which lead to SLO violations and cost inefficiencies, by leveraging AIOps principles.

Method: A gap analysis of current autoscaling methods was performed. A new multi-signal autoscaling framework was developed with SLO and cost-aware mechanisms, incorporating lightweight demand forecasting.

Result: The framework reduces SLO violation duration by 31%, improves scaling response time by 24%, and cuts costs by 18% compared to existing Kubernetes autoscaling setups.

Conclusion: This AIOps-driven approach enhances the efficiency, reliability, and transparency of Kubernetes-based platforms, aligning SLO and cost objectives effectively.

Abstract: Kubernetes provides native autoscaling mechanisms, including the Horizontal Pod Autoscaler, Vertical Pod Autoscaler, and node-level autoscalers, to enable elastic resource management for cloud-native applications. However, production environments frequently experience Service Level Objective violations and cost inefficiencies due to reactive scaling behavior, limited use of application-level signals, and opaque control logic. This paper investigates how Kubernetes autoscaling can be enhanced using AIOps principles to jointly satisfy SLO and cost constraints under diverse workload patterns without compromising safety or operational transparency. We present a gap-driven analysis of existing autoscaling approaches and propose a safe and explainable multi-signal autoscaling framework that integrates SLO-aware and cost-conscious control with lightweight demand forecasting. Experimental evaluation using representative microservice and event-driven workloads shows that the proposed approach reduces SLO violation duration by up to 31 percent, improves scaling response time by 24 percent, and lowers infrastructure cost by 18 percent compared to default and tuned Kubernetes autoscaling baselines, while maintaining stable and auditable control behavior. These results demonstrate that AIOps-driven, SLO-first autoscaling can significantly improve the reliability, efficiency, and operational trustworthiness of Kubernetes-based cloud platforms.

</details>


### [553] [Embedding Quality Assurance in project-based learning](https://arxiv.org/abs/2512.23488)
*Maria Spichkova*

Main category: cs.SE

TL;DR: The paper discusses over 10 years of experience teaching software quality aspects in Agile/Scrum settings within Software Engineering courses and provides recommendations for embedding quality assurance in project-based learning.


<details>
  <summary>Details</summary>
Motivation: To improve teaching methodologies for software quality and assurance topics, particularly in Agile/Scrum projects.

Method: Reflective analysis of decade-long teaching experiences in software development and project management courses focusing on Agile/Scrum methodologies.

Result: Lessons learned and recommendations for embedding quality assurance topics in project-based learning environments.

Conclusion: The insights offer valuable guidance for educators to integrate quality assurance effectively in Agile/Scrum-oriented Software Engineering education.

Abstract: In this paper, we share our lessons learned from more than a decade of teaching software quality aspects within Software Engineering (SE) courses, where the focus is on Agile/Scrum settings: final year software development projects and the course on SE Project Management. Based on the lessons learned, we also provide a number of recommendations on embedding quality assurance topics in the project-based learning with Agile/Scrum context.

</details>


### [554] [Adaptable Teastore with Energy Consumption Awareness: A Case Study](https://arxiv.org/abs/2512.23498)
*Henrique De Medeiros,Denisse Muñante,Sophie Chabridon,César Perdigão Batista,Denis Conan*

Main category: cs.SE

TL;DR: EnCoMSAS tool is introduced to monitor energy consumption for self-adaptive systems. Its effectiveness and low overhead were demonstrated through a case study with the Adaptable TeaStore application.


<details>
  <summary>Details</summary>
Motivation: Global energy consumption from data centers is increasing due to applications migrating to the Cloud and growth in digital content users. Energy-efficient self-adaptive software systems are needed.

Method: The EnCoMSAS tool was designed for real-time energy monitoring in distributed applications. Empirical testing was conducted with the Adaptable TeaStore deployed on the Grid5000 testbed.

Result: EnCoMSAS effectively monitors energy use and shows a correlation between CPU usage and energy consumption. Its implementation has a modest impact on overall energy usage.

Conclusion: Dynamic adaptation with proper energy monitoring tools like EnCoMSAS can enhance energy efficiency in self-adaptive systems without significantly affecting overall ecosystem energy consumption.

Abstract: [Context and Motivation] Global energy consumption has been steadily increasing in recent years, with data centers emerging as major contributors. This growth is largely driven by the widespread migration of applications to the Cloud, alongside a rising number of users consuming digital content. Dynamic adaptation (or self-adaptive) approaches appear as a way to reduce, at runtime and under certain constraints, the energy consumption of software applications.
  [Question/Problem] Despite efforts to make energy-efficiency a primary goal in the dynamic adaptation of software applications, there is still a gap in understanding how to equip these self-adaptive software systems (SAS), which are dynamically adapted at runtime, with effective energy consumption monitoring tools that enable energy-awareness. Furthermore, the extent to which such an energy consumption monitoring tool impacts the overall energy consumption of the SAS ecosystem has not yet been thoroughly explored.
  [Methodology] To address this gap, we introduce the EnCoMSAS (Energy Consumption Monitoring for Self-Adaptive Systems) tool that allows to gather the energy consumed by distributed software applications deployed, for instance, in the Cloud. EnCoMSAS enables the evaluation of energy consumption of SAS variants at runtime. It allows to integrate energy-efficiency as a main goal in the analysis and execution of new adaptation plans for the SAS. In order to evaluate the effectiveness of EnCoMSAS and investigate its impact on the overall energy consumption of the SAS ecosystem, we conduct an empirical study by using the Adaptable TeaStore case study. Adaptable TeaStore is a self-adaptive extension of the TeaStore application, a microservice benchmarking application. For this study, we focus on the recommender service of Adaptable TeaStore. Regarding the experiments, we first equip Adaptable TeaStore with EnCoMSAS. Next, we execute Adaptable TeaStore by varying workload conditions that simulate users interactions. Finally, we use EnCoMSAS for gathering and assessing the energy consumption of the recommender algorithms of Adaptable TeaStore. To run these experiments, we use nodes of the Grid5000 testbed.
  [Results] The results show that EnCoMSAS is effective in collecting energy consumption of software applications for enabling dynamic adaptation at runtime. The observed correlation between CPU usage and energy consumption collected by EnCoMSAS provides evidence supporting the validity of the collected energy measurements. Moreover, we point out, through EnCoMSAS, that energy consumption is influenced not only by the algorithmic complexity but also by the characteristics of the deployment environment. Finally, the results show that the impact of EnCoMSAS on the overall energy consumption of the SAS ecosystem is comparatively modest with respect to the entire set of the TeaStore applications microservices.

</details>


### [555] [AdaptiFlow: An Extensible Framework for Event-Driven Autonomy in Cloud Microservices](https://arxiv.org/abs/2512.23499)
*Brice Arléon Zemtsop Ndadji,Simon Bliudze,Clément Quinton*

Main category: cs.SE

TL;DR: Introduces AdaptiFlow, a self-adaptive framework for microservices leveraging decentralized adaptation via autonomous computing principles.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of centralized control in managing dynamic conditions in distributed microservice architectures.

Method: Proposes a framework with Metrics Collectors, Adaptation Actions, and an event-driven mechanism to facilitate decentralized adaptation adhering to the MAPE-K model.

Result: Demonstrates effectiveness with minimum code changes in use cases of self-healing, self-protection, and self-optimization in a benchmark microservice.

Conclusion: The framework validates that decentralized adaptation is viable, bridging autonomic computing principles with practical cloud-native systems.

Abstract: Modern cloud architectures demand self-adaptive capabilities to manage dynamic operational conditions. Yet, existing solutions often impose centralized control models ill-suited to microservices decentralized nature. This paper presents AdaptiFlow, a framework that leverages well-established principles of autonomous computing to provide abstraction layers focused on the Monitor and Execute phases of the MAPE-K loop. By decoupling metrics collection and action execution from adaptation logic, AdaptiFlow enables microservices to evolve into autonomous elements through standardized interfaces, preserving their architectural independence while enabling system-wide adaptability. The framework introduces: (1) Metrics Collectors for unified infrastructure/business metric gathering, (2) Adaptation Actions as declarative actuators for runtime adjustments, and (3) a lightweight Event-Driven and rule-based mechanism for adaptation logic specification. Validation through the enhanced Adaptable TeaStore benchmark demonstrates practical implementation of three adaptation scenarios targeting three levels of autonomy self-healing (database recovery), self-protection (DDoS mitigation), and self-optimization (traffic management) with minimal code modification per service. Key innovations include a workflow for service instrumentation and evidence that decentralized adaptation can emerge from localized decisions without global coordination. The work bridges autonomic computing theory with cloud-native practice, providing both a conceptual framework and concrete tools for building resilient distributed systems. Future work includes integration with formal coordination models and application of adaptation techniques relying on AI agents for proactive adaptation to address complex adaptation scenarios.

</details>


### [556] [Beyond Correctness: Exposing LLM-generated Logical Flaws in Reasoning via Multi-step Automated Theorem Proving](https://arxiv.org/abs/2512.23511)
*Xinyi Zheng,Ningke Li,Xiaokun Luan,Kailong Wang,Ling Shi,Meng Sun,Haoyu Wang*

Main category: cs.SE

TL;DR: The study presents MATP, a framework converting language reasoning into formal logic, improving logical consistency detection in LLMs by over 42% compared to previous baselines.


<details>
  <summary>Details</summary>
Motivation: To address the issue of subtle logical errors in LLMs, particularly in critical applications such as healthcare and law, where existing methods inadequately detect multi-step reasoning flaws.

Method: MATP translates natural language reasoning into First-Order Logic (FOL) and applies automated theorem provers to systematically verify logical validity step-by-step.

Result: MATP outperforms traditional methods by over 42 percentage points in reasoning step verification on datasets from PrOntoQA-OOD, ProofWriter, and FOLIO. It also highlights differences in performance between reasoning and general language models.

Conclusion: MATP is a promising approach to improving the reliability of LLMs in high-stakes domains, ensuring more logically consistent reasoning.

Abstract: Large Language Models (LLMs) have demonstrated impressive reasoning capabilities, leading to their adoption in high-stakes domains such as healthcare, law, and scientific research. However, their reasoning often contains subtle logical errors masked by fluent language, posing significant risks for critical applications. While existing approaches like fact-checking, self-consistency methods, and rule-based validation provide partial solutions, they fail to detect complex logical flaws in multi-step reasoning.
  To overcome these challenges, we present MATP, an evaluation framework for systematically verifying LLM reasoning via Multi-step Automatic Theorem Proving. MATP translates natural language reasoning into First-Order Logic (FOL) and applies automated theorem provers to assess step-by-step logical validity. This approach identifies hidden logical errors and provides fine-grained classifications of reasoning correctness. Evaluations on a benchmark comprising 10,830 reasoning instances generated by 10 LLMs across tasks from PrOntoQA-OOD, ProofWriter, and FOLIO show that MATP surpasses prompting-based baselines by over 42 percentage points in reasoning step verification. It further reveals model-level disparities, with reasoning models generating more logically coherent outputs than general models. These results demonstrate MATP's potential to enhance the trustworthiness of LLM-generated reasoning.

</details>


### [557] [Model-based Development for Autonomous Driving Software Considering Parallelization](https://arxiv.org/abs/2512.23575)
*Kenshin Obi,Takumi Onozawa,Hiroshi Fujimoto,Takuya Azumi*

Main category: cs.SE

TL;DR: The paper proposes a parallelization method using Model-Based Development (MBD) to enhance real-time performance for autonomous driving software.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of achieving real-time performance in autonomous driving software, which manages diverse functions and complex environments.

Method: The authors extend the existing Model-Based Parallelizer (MBP) method within the Model-Based Development (MBD) process to implement parallel processing efficiently.

Result: The proposed method effectively reduces execution time, as demonstrated by evaluation results.

Conclusion: The approach is shown to be suitable for developing autonomous driving software with a focus on real-time performance optimization.

Abstract: In recent years, autonomous vehicles have attracted attention as one of the solutions to various social problems. However, autonomous driving software requires real-time performance as it considers a variety of functions and complex environments. Therefore, this paper proposes a parallelization method for autonomous driving software using the Model-Based Development (MBD) process. The proposed method extends the existing Model-Based Parallelizer (MBP) method to facilitate the implementation of complex processing. As a result, execution time was reduced. The evaluation results demonstrate that the proposed method is suitable for the development of autonomous driving software, particularly in achieving real-time performance.

</details>


### [558] [Parallelized Code Generation from Simulink Models for Event-driven and Timer-driven ROS 2 Nodes](https://arxiv.org/abs/2512.23605)
*Kenshin Obi,Ryo Yoshinaka,Hiroshi Fujimoto,Takuya Azumi*

Main category: cs.SE

TL;DR: The paper addresses challenges in parallelizing software for complex embedded systems like autonomous driving, proposing an adapted model-based development (MBD) framework for ROS 2 models, resulting in better execution time.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity and scale of embedded systems, especially for autonomous driving, require efficient parallelization methods to address concurrency issues and integrate modern frameworks like ROS 2.

Method: The framework categorizes ROS 2-compatible Simulink models into event-driven and timer-driven types, enabling targeted parallelization and automatic code generation.

Result: The framework demonstrated reduced execution time across all patterns compared to non-parallelized approaches, validating its effectiveness.

Conclusion: The proposed MBD framework is effective in improving the execution efficiency for complex ROS 2-based models with multiple inputs, overcoming traditional parallelization challenges.

Abstract: In recent years, the complexity and scale of embedded systems, especially in the rapidly developing field of autonomous driving systems, have increased significantly. This has led to the adoption of software and hardware approaches such as Robot Operating System (ROS) 2 and multi-core processors. Traditional manual program parallelization faces challenges, including maintaining data integrity and avoiding concurrency issues such as deadlocks. While model-based development (MBD) automates this process, it encounters difficulties with the integration of modern frameworks such as ROS 2 in multi-input scenarios. This paper proposes an MBD framework to overcome these issues, categorizing ROS 2-compatible Simulink models into event-driven and timer-driven types for targeted parallelization. As a result, it extends the conventional parallelization by MBD and supports parallelized code generation for ROS 2-based models with multiple inputs. The evaluation results show that after applying parallelization with the proposed framework, all patterns show a reduction in execution time, confirming the effectiveness of parallelization.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [559] [JParc: Joint cortical surface parcellation with registration](https://arxiv.org/abs/2512.22485)
*Jian Li,Karthik Gopinath,Brian L. Edlow,Adrian V. Dalca,Bruce Fischl*

Main category: q-bio.NC

TL;DR: JParc, a joint cortical registration and parcellation framework, achieves state-of-the-art performance in cortical surface parcellation tasks with Dice scores exceeding 90%, by accurately registering cortical surfaces and using a learned atlas.


<details>
  <summary>Details</summary>
Motivation: Cortical surface parcellation is crucial for neuroscience research and clinical applications, but existing methods face limitations in exploring how new approaches improve over traditional registration and atlas propagation techniques.

Method: JParc incorporates joint cortical registration and parcellation using a shallow subnetwork to fine-tune atlas labels based on basic geometric features describing cortical folding patterns.

Result: JParc outperforms other parcellation methods, demonstrating Dice scores greater than 90% on the Mindboggle dataset using sulcal depth and curvature features.

Conclusion: JParc’s high accuracy can enhance brain mapping studies' statistical power and support clinical applications such as surgical planning.

Abstract: Cortical surface parcellation is a fundamental task in both basic neuroscience research and clinical applications, enabling more accurate mapping of brain regions. Model-based and learning-based approaches for automated parcellation alleviate the need for manual labeling. Despite the advancement in parcellation performance, learning-based methods shift away from registration and atlas propagation without exploring the reason for the improvement compared to traditional methods. In this study, we present JParc, a joint cortical registration and parcellation framework, that outperforms existing state-of-the-art parcellation methods. In rigorous experiments, we demonstrate that the enhanced performance of JParc is primarily attributable to accurate cortical registration and a learned parcellation atlas. By leveraging a shallow subnetwork to fine-tune the propagated atlas labels, JParc achieves a Dice score greater than 90% on the Mindboggle dataset, using only basic geometric features (sulcal depth, curvature) that describe cortical folding patterns. The superior accuracy of JParc can significantly increase the statistical power in brain mapping studies as well as support applications in surgical planning and many other downstream neuroscientific and clinical tasks.

</details>


### [560] [Nonlinear Dynamical Modeling of Human Intracranial Brain Activity with Flexible Inference](https://arxiv.org/abs/2512.22785)
*Kiarash Vaziri,Lucine L. Oganesian,HyeongChan Jo,Roberto M. C. Vera,Charles Y. Liu,Brian Lee,Maryam M. Shanechi*

Main category: q-bio.NC

TL;DR: The paper presents an enhanced deep learning framework, DFINE, for modeling human intracranial electroencephalography (iEEG) recordings, outperforming traditional models in predicting neural activity and managing missing data.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of linear and recurrent neural network models in capturing nonlinear neural activity and handling missing observations, thereby improving the modeling and application of human iEEG recordings for BCIs.

Method: An advanced version of DFINE, a framework integrating linear state-space models and nonlinear neural networks, was applied to multisite human iEEG recordings, enabling effective modeling, forecasting, and handling of missing data.

Result: DFINE outperformed linear state-space models (LSSMs) in neural forecasting, matched or exceeded the accuracy of gated recurrent unit (GRU) models, and handled missing observations more robustly. Its advantages are particularly notable in high gamma spectral bands.

Conclusion: The study demonstrates DFINE's superior performance and flexibility for human iEEG modeling, making it a promising tool for next-generation brain-computer interface applications.

Abstract: Dynamical modeling of multisite human intracranial neural recordings is essential for developing neurotechnologies such as brain-computer interfaces (BCIs). Linear dynamical models are widely used for this purpose due to their interpretability and their suitability for BCIs. In particular, these models enable flexible real-time inference, even in the presence of missing neural samples, which often occur in wireless BCIs. However, neural activity can exhibit nonlinear structure that is not captured by linear models. Furthermore, while recurrent neural network models can capture nonlinearity, their inference does not directly address handling missing observations. To address this gap, recent work introduced DFINE, a deep learning framework that integrates neural networks with linear state-space models to capture nonlinearities while enabling flexible inference. However, DFINE was developed for intracortical recordings that measure localized neuronal populations. Here we extend DFINE to modeling of multisite human intracranial electroencephalography (iEEG) recordings. We find that DFINE significantly outperforms linear state-space models (LSSMs) in forecasting future neural activity. Furthermore, DFINE matches or exceeds the accuracy of a gated recurrent unit (GRU) model in neural forecasting, indicating that a linear dynamical backbone, when paired and jointly trained with nonlinear neural networks, can effectively describe the dynamics of iEEG signals while also enabling flexible inference. Additionally, DFINE handles missing observations more robustly than the baselines, demonstrating its flexible inference and utility for BCIs. Finally, DFINE's advantage over LSSM is more pronounced in high gamma spectral bands. Taken together, these findings highlight DFINE as a strong and flexible framework for modeling human iEEG dynamics, with potential applications in next-generation BCIs.

</details>


### [561] [An Inference-Based Architecture for Intent and Affordance Saturation in Decision-Making](https://arxiv.org/abs/2512.23144)
*Wendyam Eric Lionel Ilboudo,Saori C Tanaka*

Main category: q-bio.NC

TL;DR: The paper presents a computational explanation for decision paralysis, focusing on hierarchical decision-making challenges and its relation to autism.


<details>
  <summary>Details</summary>
Motivation: To understand the mechanisms causing decision paralysis, particularly in autism, where hesitation or failure to act is prominent despite adequate knowledge and motivation.

Method: The study separates decision-making into two layers: intent selection (goal) and affordance selection (means). It uses a computational framework combining reverse-KL (rapid commitment) and forward-KL (goal flexibility) to model decision processes.

Result: Simulations showed forward-KL-biased inference leads to slow responses and patterns like intent and affordance saturation, explaining decision inertia and shutdown in multi-option tasks.

Conclusion: Autism can be understood as an extreme, inference-based decision-making spectrum positioning, providing insights into decision paralysis and its computational underpinnings.

Abstract: Decision paralysis, i.e. hesitation, freezing, or failure to act despite full knowledge and motivation, poses a challenge for choice models that assume options are already specified and readily comparable. Drawing on qualitative reports in autism research that are especially salient, we propose a computational account in which paralysis arises from convergence failure in a hierarchical decision process. We separate intent selection (what to pursue) from affordance selection (how to pursue the goal) and formalize commitment as inference under a mixture of reverse- and forward-Kullback-Leibler (KL) objectives. Reverse KL is mode-seeking and promotes rapid commitment, whereas forward KL is mode-covering and preserves multiple plausible goals or actions. In static and dynamic (drift-diffusion) models, forward-KL-biased inference yields slow, heavy-tailed response times and two distinct failure modes, intent saturation and affordance saturation, when values are similar. Simulations in multi-option tasks reproduce key features of decision inertia and shutdown, treating autism as an extreme regime of a general, inference-based, decision-making continuum.

</details>


### [562] [A Network of Biologically Inspired Rectified Spectral Units (ReSUs) Learns Hierarchical Features Without Error Backpropagation](https://arxiv.org/abs/2512.23146)
*Shanshan Qin,Joshua L. Pughe-Sanford,Alexander Genkin,Pembe Gizem Ozdil,Philip Greengard,Anirvan M. Sengupta,Dmitri B. Chklovskii*

Main category: q-bio.NC

TL;DR: The paper introduces a biologically inspired neural architecture called Rectified Spectral Units (ReSUs) for feature construction using a self-supervised learning algorithm.


<details>
  <summary>Details</summary>
Motivation: To develop a computational framework that both mirrors biological neural circuit functions and offers a self-supervised learning paradigm for constructing deep neural networks.

Method: The authors propose a neural network architecture, ReSUs, which utilizes canonical correlation analysis (CCA) for self-supervised learning by encoding temporal input features and rectifying positive or negative components.

Result: First-layer units in the model mimic Drosophila post-photoreceptor neurons' temporal dynamics, while second-layer units exhibit direction selectivity, similar to motion-detecting T4 cells. Synaptic weight patterns of the model align with connectomic data.

Conclusion: ReSUs provide a computational model for sensory circuits with biological accuracy and an alternative to backpropagation for training neural networks in a self-supervised manner.

Abstract: We introduce a biologically inspired, multilayer neural architecture composed of Rectified Spectral Units (ReSUs). Each ReSU projects a recent window of its input history onto a canonical direction obtained via canonical correlation analysis (CCA) of previously observed past-future input pairs, and then rectifies either its positive or negative component. By encoding canonical directions in synaptic weights and temporal filters, ReSUs implement a local, self-supervised algorithm for progressively constructing increasingly complex features.
  To evaluate both computational power and biological fidelity, we trained a two-layer ReSU network in a self-supervised regime on translating natural scenes. First-layer units, each driven by a single pixel, developed temporal filters resembling those of Drosophila post-photoreceptor neurons (L1/L2 and L3), including their empirically observed adaptation to signal-to-noise ratio (SNR). Second-layer units, which pooled spatially over the first layer, became direction-selective -- analogous to T4 motion-detecting cells -- with learned synaptic weight patterns approximating those derived from connectomic reconstructions.
  Together, these results suggest that ReSUs offer (i) a principled framework for modeling sensory circuits and (ii) a biologically grounded, backpropagation-free paradigm for constructing deep self-supervised neural networks.

</details>


### [563] [Somatosensory prediction in premature neonates: iatrogenic pain experience increases repetition suppression and deviance detection of innocuous stimuli in a tactile oddball protocol](https://arxiv.org/abs/2512.23301)
*Anne-Lise Marais,Victoria Dumont,Marie Anquetil,Arnaud Mortier,Anne-Sophie Trentesaux,Nadege Roche-Labarbe*

Main category: q-bio.NC

TL;DR: The study investigates the impact of prematurity and neonatal pain on sensory prediction (SP) in the brains of premature neonates, using EEG. Findings reveal unexpected accelerated SP maturation with higher pain exposure.


<details>
  <summary>Details</summary>
Motivation: To understand how sensory prediction develops in premature neonates and its association with perinatal risk factors, particularly untimely birth and painful neonatal care, as a potential marker for neurodevelopmental disorders.

Method: A tactile oddball-omission paradigm with EEG was employed to assess neural indices of SP in premature neonates at 35 weeks corrected gestational age, evaluating the influence of gestational age and pain exposure on these indices.

Result: Repetition suppression and mismatch response, indicators of sensory prediction, were observed in premature neonates. Pain exposure was found to significantly impact SP proxies, with greater exposure correlating with more robust responses.

Conclusion: Contrary to expectations, greater ex utero pain experiences were linked to accelerated predictive somatosensory processing. Long-term developmental outcomes need further exploration in longitudinal studies.

Abstract: Sensory prediction (SP) is a fundamental mechanism of perception that supports cognitive development. Atypical SP has been reported across multiple neurodevelopmental disorders (ND), suggesting it may constitute an early cross-syndromic marker. Premature birth is a major risk factor for ND, with risk increasing as gestational age (GA) at birth decreases. However, how perinatal risk factors shape the development of SP remains poorly understood. We do not know if untimely birth itself, or exposure to iatrogenic pain during neonatal intensive care, cause neurodevelopmental impairments. In this study, we first assessed whether SP can be detected in the brains of premature neonates at 35 weeks corrected GA using a tactile oddballomission paradigm with EEG. We then examined the effects of the degree of prematurity and of the exposure to painful care procedures on neural indices of SP. Results demonstrate the presence of repetition suppression (RS) and a mismatch response (MMR) to deviant stimuli in the contralateral somatosensory cortex of premature neonates. The amplitude of these SP proxies was significantly affected by the number of painful procedures experienced since birth, independently of the effect of GA at birth. Contrary to our initial hypothesis that greater neurodevelopmental risk would be associated with less mature SP, infants with higher exposure to pain exhibited more robust indices of SP. These findings suggest that increased ex utero experience, even painful, is associated with accelerated maturation of predictive somatosensory processing. Longitudinal follow-up of participants at age 2 will explore how these early markers relate to developmental outcomes.

</details>


### [564] [Dynamical incompatibilities in paced finger tapping experiments](https://arxiv.org/abs/2512.23661)
*Ariel D. Silva,Claudia R. González,Rodrigo Laje*

Main category: q-bio.NC

TL;DR: The paper investigates sensorimotor synchronization in humans, focusing on inconsistencies in response timing to different perturbation types in finger-tapping tasks. Findings show that these responses are influenced by temporal contexts and can be unified under a recalibrated system.


<details>
  <summary>Details</summary>
Motivation: The study addresses contradictions in human sensorimotor synchronization research, specifically responses to different perturbation types during a common task.

Method: Experimental and theoretical approaches were used to compare human responses to step changes and phase shifts under different experimental setups.

Result: Experimental results showed incompatibility of responses across perturbation types in separate experiments but compatibility within the same experiment, suggesting that temporal context influences the error-correction mechanism.

Conclusion: All perturbation types and their responses can be unified under a single dynamical system recalibrated by temporal context, improving understanding and comparison of human sensorimotor synchronization behaviors.

Abstract: The behavioral description of the sensorimotor synchronization phenomenon in humans is exhaustive, mostly by using variations of the traditional paced finger-tapping task. This task helps unveil the inner workings of the error-correction mechanism responsible for the resynchronization after a perturbation to the period of the stimuli sequence. Yet, fundamental contradictions still exist among different works in the literature. One of such contradictions only emerges after comparing the two most-common period perturbation types: step changes and phase shifts. The stimulus sequence is exactly the same in both perturbation types up to and including the (unexpected) perturbed stimulus. Why then would the timing of the next response be different between perturbation types, as observed? The explanation lies in the buildup of different temporal contexts during the experiments that recalibrate the error-correction mechanism. Here we show, both experimentally and theoretically, that responses to different perturbation types are dynamically incompatible when they occur in separate experiments. That is, they can't be represented by the same underlying dynamical system, thus explaining many contradictory results and the difficulty in reproducing both types of perturbations with a single mathematical model. On the other hand, if both perturbation types are presented at random during the same experiment then the responses are compatible with each other and can be construed as produced by a unique underlying mechanism. We conclude that a single underlying dynamical system can represent the response to all perturbation types, signs, and sizes, which is nevertheless recalibrated by temporal context. Our results offer a ground for performing better comparisons in paced finger tapping and extend the usable range of data beyond the perturbed stimulus and into the information-rich resynchronization phase.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [565] [A review of NMF, PLSA, LBA, EMA, and LCA with a focus on the identifiability issue](https://arxiv.org/abs/2512.22282)
*Qianqian Qi,Peter G. M. van der Heijden*

Main category: stat.ML

TL;DR: The paper explores the similarities and identifiability of five popular nonnegative matrix factorization models (LBA, LCA, EMA, PLSA, NMF) and proves that the uniqueness of solutions among them is equivalently tied. It also reviews their algorithms and discusses closely related models like archetypal analysis, using a social science example.


<details>
  <summary>Details</summary>
Motivation: The motivation is to investigate the underlying similarities of models that factorize nonnegative matrices and ensure researchers understand their equivalence, particularly in terms of identifiability, as these models are often presented under different terminologies across fields.

Method: The study proves equivalency in identifiability across the five models (LBA, LCA, EMA, PLSA, NMF) and briefly reviews algorithms for these models. It employs a time-budget dataset from social sciences to illustrate their applications.

Result: The authors demonstrate that the uniqueness of solutions in latent budget analysis (LBA), latent class analysis (LCA), end-member analysis (EMA), and probabilistic latent semantic analysis (PLSA) is equivalent to that of nonnegative matrix factorization (NMF).

Conclusion: This work clarifies model equivalencies, emphasizing their shared identifiability properties. Researchers are guided to recognize parallels and appropriately select models depending on the context.

Abstract: Across fields such as machine learning, social science, geography, considerable attention has been given to models that factorize a nonnegative matrix into the product of two or three matrices, subject to nonnegative or row-sum-to-1 constraints. Although these models are to a large extend similar or even equivalent, they are presented under different names, and their similarity is not well known. This paper highlights similarities among five popular models, latent budget analysis (LBA), latent class analysis (LCA), end-member analysis (EMA), probabilistic latent semantic analysis (PLSA), and nonnegative matrix factorization (NMF). We focus on an essential issue-identifiability-of these models and prove that the solution of LBA, EMA, LCA, PLSA is unique if and only if the solution of NMF is unique. We also provide a brief review for algorithms of these models. We illustrate the models with a time budget dataset from social science, and end the paper with a discussion of closely related models such as archetypal analysis.

</details>


### [566] [On Fibonacci Ensembles: An Alternative Approach to Ensemble Learning Inspired by the Timeless Architecture of the Golden Ratio](https://arxiv.org/abs/2512.22284)
*Ernest Fokoué*

Main category: stat.ML

TL;DR: The paper introduces Fibonacci Ensembles, a novel framework for ensemble learning inspired by the Fibonacci sequence, aiming to enhance variance reduction and depth in learning systems.


<details>
  <summary>Details</summary>
Motivation: The work is inspired by the Fibonacci sequence, seen as a universal grammar of balance in nature, to harmonize and improve ensemble learning methodologies.

Method: The method involves leveraging Fibonacci weights with orthogonalization and Rao--Blackwell optimization for variance reduction, alongside recursive ensemble dynamics reflecting Fibonacci patterns for increased representation.

Result: Experimental results show Fibonacci Ensembles perform equally or better than uniform averaging and interact effectively with orthogonal Rao--Blackwellization in regression tasks.

Conclusion: Fibonacci Ensembles offer a natural and interpretable enhancement to ensemble learning, reflecting universal principles of harmony and stability.

Abstract: Nature rarely reveals her secrets bluntly, yet in the Fibonacci sequence she grants us a glimpse of her quiet architecture of growth, harmony, and recursive stability \citep{Koshy2001Fibonacci, Livio2002GoldenRatio}. From spiral galaxies to the unfolding of leaves, this humble sequence reflects a universal grammar of balance. In this work, we introduce \emph{Fibonacci Ensembles}, a mathematically principled yet philosophically inspired framework for ensemble learning that complements and extends classical aggregation schemes such as bagging, boosting, and random forests \citep{Breiman1996Bagging, Breiman2001RandomForests, Friedman2001GBM, Zhou2012Ensemble, HastieTibshiraniFriedman2009ESL}. Two intertwined formulations unfold: (1) the use of normalized Fibonacci weights -- tempered through orthogonalization and Rao--Blackwell optimization -- to achieve systematic variance reduction among base learners, and (2) a second-order recursive ensemble dynamic that mirrors the Fibonacci flow itself, enriching representational depth beyond classical boosting. The resulting methodology is at once rigorous and poetic: a reminder that learning systems flourish when guided by the same intrinsic harmonies that shape the natural world. Through controlled one-dimensional regression experiments using both random Fourier feature ensembles \citep{RahimiRecht2007RFF} and polynomial ensembles, we exhibit regimes in which Fibonacci weighting matches or improves upon uniform averaging and interacts in a principled way with orthogonal Rao--Blackwellization. These findings suggest that Fibonacci ensembles form a natural and interpretable design point within the broader theory of ensemble learning.

</details>


### [567] [A General Weighting Theory for Ensemble Learning: Beyond Variance Reduction via Spectral and Geometric Structure](https://arxiv.org/abs/2512.22286)
*Ernest Fokoué*

Main category: stat.ML

TL;DR: This paper focuses on a new theoretical framework for ensemble learning, which challenges the traditional variance-reduction view and demonstrates the effectiveness of structured, non-uniform weights even for stable base models.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper arises from the observation that ensemble learning is often effective even for stable predictors, which contradicts the traditional explanation of ensemble learning as a variance-reduction strategy. It seeks to provide a more general understanding of this phenomenon.

Method: The authors formalize ensembles as linear operators in a hypothesis space, applying spectral and geometric constraints to weighting schemes. They derive a refined bias-variance decomposition and use this framework to identify conditions where structured weighting outperforms uniform averaging. They also demonstrate that optimal weights can be formulated as solutions to constrained quadratic programs.

Result: The developed theory shows how structured weights reshape approximation geometry and redistribute spectral complexity. It proves conditions under which weighted ensembles dominate uniform ensembles and unifies different ensemble approaches, such as classical averaging and Fibonacci-based ensembles, under this theory.

Conclusion: The study establishes a theoretical foundation for understanding structure-driven ensemble learning, providing new insights into why ensembles are effective even for low-variance learners, and paving the way for advancing adaptive and dynamic weighting schemes.

Abstract: Ensemble learning is traditionally justified as a variance-reduction strategy, explaining its strong performance for unstable predictors such as decision trees. This explanation, however, does not account for ensembles constructed from intrinsically stable estimators-including smoothing splines, kernel ridge regression, Gaussian process regression, and other regularized reproducing kernel Hilbert space (RKHS) methods whose variance is already tightly controlled by regularization and spectral shrinkage. This paper develops a general weighting theory for ensemble learning that moves beyond classical variance-reduction arguments. We formalize ensembles as linear operators acting on a hypothesis space and endow the space of weighting sequences with geometric and spectral constraints. Within this framework, we derive a refined bias-variance approximation decomposition showing how non-uniform, structured weights can outperform uniform averaging by reshaping approximation geometry and redistributing spectral complexity, even when variance reduction is negligible. Our main results provide conditions under which structured weighting provably dominates uniform ensembles, and show that optimal weights arise as solutions to constrained quadratic programs. Classical averaging, stacking, and recently proposed Fibonacci-based ensembles appear as special cases of this unified theory, which further accommodates geometric, sub-exponential, and heavy-tailed weighting laws. Overall, the work establishes a principled foundation for structure-driven ensemble learning, explaining why ensembles remain effective for smooth, low-variance base learners and setting the stage for distribution-adaptive and dynamically evolving weighting schemes developed in subsequent work.

</details>


### [568] [Gradient Dynamics of Attention: How Cross-Entropy Sculpts Bayesian Manifolds](https://arxiv.org/abs/2512.22473)
*Naman Aggarwal,Siddhartha R. Dalal,Vishal Misra*

Main category: stat.ML

TL;DR: The paper provides a first-order analysis of how cross-entropy training adjusts attention scores and value vectors in transformer attention heads, describing a positive feedback loop that specializes queries and values, resembling an EM procedure.


<details>
  <summary>Details</summary>
Motivation: Understanding the mechanisms by which transformers perform probabilistic reasoning during gradient-based learning, as the process remains unclear.

Method: Authors analyze the reshaping of attention scores and values through equations derived from cross-entropy training, use simulations including a sticky Markov-chain task, and compare these dynamics to an EM-style update.

Result: Results show that gradient dynamics sculpt Bayesian manifolds that enable probabilistic reasoning, linking optimization with internal geometry and function.

Conclusion: Gradient-based optimization in transformers creates specialized internal geometry that facilitates Bayesian inference and probabilistic reasoning.

Abstract: Transformers empirically perform precise probabilistic reasoning in carefully constructed ``Bayesian wind tunnels'' and in large-scale language models, yet the mechanisms by which gradient-based learning creates the required internal geometry remain opaque. We provide a complete first-order analysis of how cross-entropy training reshapes attention scores and value vectors in a transformer attention head. Our core result is an \emph{advantage-based routing law} for attention scores, \[ \frac{\partial L}{\partial s_{ij}} = α_{ij}\bigl(b_{ij}-\mathbb{E}_{α_i}[b]\bigr), \qquad b_{ij} := u_i^\top v_j, \] coupled with a \emph{responsibility-weighted update} for values, \[ Δv_j = -η\sum_i α_{ij} u_i, \] where $u_i$ is the upstream gradient at position $i$ and $α_{ij}$ are attention weights. These equations induce a positive feedback loop in which routing and content specialize together: queries route more strongly to values that are above-average for their error signal, and those values are pulled toward the queries that use them. We show that this coupled specialization behaves like a two-timescale EM procedure: attention weights implement an E-step (soft responsibilities), while values implement an M-step (responsibility-weighted prototype updates), with queries and keys adjusting the hypothesis frame. Through controlled simulations, including a sticky Markov-chain task where we compare a closed-form EM-style update to standard SGD, we demonstrate that the same gradient dynamics that minimize cross-entropy also sculpt the low-dimensional manifolds identified in our companion work as implementing Bayesian inference. This yields a unified picture in which optimization (gradient flow) gives rise to geometry (Bayesian manifolds), which in turn supports function (in-context probabilistic reasoning).

</details>


### [569] [Likelihood-Preserving Embeddings for Statistical Inference](https://arxiv.org/abs/2512.22638)
*Deniz Akdemir*

Main category: stat.ML

TL;DR: The paper develops a theory for embeddings that preserve likelihood-based statistical inference, introducing the Likelihood-Ratio Distortion metric and providing guarantees for inference preservation.


<details>
  <summary>Details</summary>
Motivation: Modern machine learning embeddings compress high-dimensional data but disrupt geometric properties needed for statistical inference. There's a need to develop embeddings that preserve inferential workflows.

Method: The paper introduces the Likelihood-Ratio Distortion metric to measure embedding errors and establishes the Hinge Theorem to ensure inference preservation. Neural networks are used as approximate sufficient statistics and theoretical bounds are derived.

Result: It is shown that likelihood preservation requires nearly invertible embeddings unless tailored to specific models. Experiments validate theoretical predictions and demonstrate practical applications to clinical inference.

Conclusion: Likelihood-preserving embeddings can replace raw data in statistical inference without altering results, providing utility in practical applications like distributed clinical inference.

Abstract: Modern machine learning embeddings provide powerful compression of high-dimensional data, yet they typically destroy the geometric structure required for classical likelihood-based statistical inference. This paper develops a rigorous theory of likelihood-preserving embeddings: learned representations that can replace raw data in likelihood-based workflows -- hypothesis testing, confidence interval construction, model selection -- without altering inferential conclusions. We introduce the Likelihood-Ratio Distortion metric $Δ_n$, which measures the maximum error in log-likelihood ratios induced by an embedding. Our main theoretical contribution is the Hinge Theorem, which establishes that controlling $Δ_n$ is necessary and sufficient for preserving inference. Specifically, if the distortion satisfies $Δ_n = o_p(1)$, then (i) all likelihood-ratio based tests and Bayes factors are asymptotically preserved, and (ii) surrogate maximum likelihood estimators are asymptotically equivalent to full-data MLEs. We prove an impossibility result showing that universal likelihood preservation requires essentially invertible embeddings, motivating the need for model-class-specific guarantees. We then provide a constructive framework using neural networks as approximate sufficient statistics, deriving explicit bounds connecting training loss to inferential guarantees. Experiments on Gaussian and Cauchy distributions validate the sharp phase transition predicted by exponential family theory, and applications to distributed clinical inference demonstrate practical utility.

</details>


### [570] [JADAI: Jointly Amortizing Adaptive Design and Bayesian Inference](https://arxiv.org/abs/2512.22999)
*Niels Bracher,Lars Kühmichel,Desi R. Ivanova,Xavier Intes,Paul-Christian Bürkner,Stefan T. Radev*

Main category: stat.ML

TL;DR: This paper introduces JADAI, a framework for optimizing design variables in parameter estimation by combining experimental design and inference tasks through end-to-end training and diffusion-based inference networks.


<details>
  <summary>Details</summary>
Motivation: To address challenges in parameter estimation by actively optimizing design variables to maximize information gain and efficiently handle high-dimensional and multimodal posteriors.

Method: JADAI combines end-to-end training of a policy network, a history network, and a diffusion-based posterior estimation network, optimizing experimental design by minimizing posterior error iteratively.

Result: JADAI demonstrates superior or competitive results across standard adaptive design benchmarks, showing effectiveness in high-dimensional and multimodal posterior estimation with experimental sequences.

Conclusion: The proposed JADAI framework effectively integrates Bayesian adaptive design with inference to achieve efficient parameter estimation and robust experimental outcomes.

Abstract: We consider problems of parameter estimation where design variables can be actively optimized to maximize information gain. To this end, we introduce JADAI, a framework that jointly amortizes Bayesian adaptive design and inference by training a policy, a history network, and an inference network end-to-end. The networks minimize a generic loss that aggregates incremental reductions in posterior error along experimental sequences. Inference networks are instantiated with diffusion-based posterior estimators that can approximate high-dimensional and multimodal posteriors at every experimental step. Across standard adaptive design benchmarks, JADAI achieves superior or competitive performance.

</details>


### [571] [Federated Learning With L0 Constraint Via Probabilistic Gates For Sparsity](https://arxiv.org/abs/2512.23071)
*Krishna Harsha Kovelakuntla Huthasana,Alireza Olama,Andreas Lundell*

Main category: stat.ML

TL;DR: The paper proposes a method to apply L0 constraints for achieving sparse federated learning models, maintaining data and model sparsity while ensuring effective communication efficiency and strong statistical performance.


<details>
  <summary>Details</summary>
Motivation: Federated learning often results in dense models and poor generalizability due to data sparsity and client heterogeneity, necessitating a method to enforce model sparsity efficiently.

Method: The authors propose an L0 constraint on the density of non-zero parameters using probabilistic gates with a continuous relaxation and use federated stochastic gradient descent for optimization.

Result: The proposed method achieves target parameter densities (as low as 0.005) with minimal statistical performance loss and outperforms magnitude pruning in communication efficiency and model performance across various datasets.

Conclusion: Applying L0 constraints in federated learning is effective for controlling model sparsity while retaining strong performance and efficiency, making it suitable for scenarios with heterogeneous data and participation rates.

Abstract: Federated Learning (FL) is a distributed machine learning setting that requires multiple clients to collaborate on training a model while maintaining data privacy. The unaddressed inherent sparsity in data and models often results in overly dense models and poor generalizability under data and client participation heterogeneity. We propose FL with an L0 constraint on the density of non-zero parameters, achieved through a reparameterization using probabilistic gates and their continuous relaxation: originally proposed for sparsity in centralized machine learning. We show that the objective for L0 constrained stochastic minimization naturally arises from an entropy maximization problem of the stochastic gates and propose an algorithm based on federated stochastic gradient descent for distributed learning. We demonstrate that the target density (rho) of parameters can be achieved in FL, under data and client participation heterogeneity, with minimal loss in statistical performance for linear and non-linear models: Linear regression (LR), Logistic regression (LG), Softmax multi-class classification (MC), Multi-label classification with logistic units (MLC), Convolution Neural Network (CNN) for multi-class classification (MC). We compare the results with a magnitude pruning-based thresholding algorithm for sparsity in FL. Experiments on synthetic data with target density down to rho = 0.05 and publicly available RCV1, MNIST, and EMNIST datasets with target density down to rho = 0.005 demonstrate that our approach is communication-efficient and consistently better in statistical performance.

</details>


### [572] [Probabilistic Modelling is Sufficient for Causal Inference](https://arxiv.org/abs/2512.23408)
*Bruno Mlodozeniec,David Krueger,Richard E. Turner*

Main category: stat.ML

TL;DR: Explains that any causal inference question can be addressed using probabilistic modeling and inference without requiring specialized causal tools.


<details>
  <summary>Details</summary>
Motivation: To clarify misconceptions that specific causal frameworks or notations are mandatory for causal inference in machine learning.

Method: Uses concrete examples to demonstrate how probabilistic modeling and inference can successfully answer causal questions.

Result: Shows that causal tools emerge naturally from probabilistic modeling and explains their necessity and utility.

Conclusion: Probabilistic modeling and inference are sufficient for addressing causal inference questions, and specialized causal tools are not mandatory but emerge naturally.

Abstract: Causal inference is a key research area in machine learning, yet confusion reigns over the tools needed to tackle it. There are prevalent claims in the machine learning literature that you need a bespoke causal framework or notation to answer causal questions. In this paper, we want to make it clear that you \emph{can} answer any causal inference question within the realm of probabilistic modelling and inference, without causal-specific tools or notation. Through concrete examples, we demonstrate how causal questions can be tackled by writing down the probability of everything. Lastly, we reinterpret causal tools as emerging from standard probabilistic modelling and inference, elucidating their necessity and utility.

</details>


### [573] [The Nonstationarity-Complexity Tradeoff in Return Prediction](https://arxiv.org/abs/2512.23596)
*Agostino Capponi,Chengpiao Huang,J. Antonio Sidaoui,Kaizheng Wang,Jiacheng Zou*

Main category: stat.ML

TL;DR: The paper proposes a novel model selection method for stock return prediction in non-stationary environments by balancing tradeoffs between model complexity, training window size, and non-stationarity effects.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of predicting stock returns in non-stationary environments. Complex models reduce error but require longer training windows, which increase non-stationarity, necessitating a method to manage this tradeoff.

Method: The authors propose a tournament-based model selection method that jointly optimizes model class and training window size, evaluating candidate models adaptively on validation data under non-stationary conditions.

Result: Empirical results show the method outperforms rolling-window benchmarks, improving out-of-sample $R^2$ by 14-23%, achieving positive $R^2$ during recessions, including the Gulf War and 2008 Financial Crisis, and generating 31% higher cumulative returns in trading strategies.

Conclusion: The proposed method effectively manages non-stationarity-complexity tradeoffs, enabling better stock return prediction and superior performance in both economic and turbulent periods compared to standard benchmarks.

Abstract: We investigate machine learning models for stock return prediction in non-stationary environments, revealing a fundamental nonstationarity-complexity tradeoff: complex models reduce misspecification error but require longer training windows that introduce stronger non-stationarity. We resolve this tension with a novel model selection method that jointly optimizes model class and training window size using a tournament procedure that adaptively evaluates candidates on non-stationary validation data. Our theoretical analysis demonstrates that this approach balances misspecification error, estimation variance, and non-stationarity, performing close to the best model in hindsight. Applying our method to 17 industry portfolio returns, we consistently outperform standard rolling-window benchmarks, improving out-of-sample $R^2$ by 14-23% on average. During NBER-designated recessions, improvements are substantial: our method achieves positive $R^2$ during the Gulf War recession while benchmarks are negative, and improves $R^2$ in absolute terms by at least 80bps during the 2001 recession as well as superior performance during the 2008 Financial Crisis. Economically, a trading strategy based on our selected model generates 31% higher cumulative returns averaged across the industries.

</details>


### [574] [Calibrated Multi-Level Quantile Forecasting](https://arxiv.org/abs/2512.23671)
*Tiffany Ding,Isaac Gibbs,Ryan J. Tibshirani*

Main category: stat.ML

TL;DR: The paper introduces a method named Multi-Level Quantile Tracker (MultiQT) for ensuring calibration of multi-level quantile forecasts even under adversarial conditions.


<details>
  <summary>Details</summary>
Motivation: The paper aims to achieve multi-level quantile forecast calibration, addressing challenges such as adversarial distribution shifts that disrupt calibration.

Method: MultiQT is a lightweight wrapper applied to existing forecasters to adjust quantile predictions, ensuring they are calibrated and ordered at multiple levels.

Result: Experiments show MultiQT improves calibration significantly in forecasting applications such as epidemics and energy.

Conclusion: MultiQT guarantees calibration against distribution shifts, ensures proper ordering of forecasts, and does not asymptotically worsen an existing forecaster's performance.

Abstract: We present an online method for guaranteeing calibration of quantile forecasts at multiple quantile levels simultaneously. A sequence of $α$-level quantile forecasts is calibrated if the forecasts are larger than the target value at an $α$-fraction of time steps. We introduce a lightweight method called Multi-Level Quantile Tracker (MultiQT) that wraps around any existing point or quantile forecaster to produce corrected forecasts guaranteed to achieve calibration, even against adversarial distribution shifts, while ensuring that the forecasts are ordered -- e.g., the 0.5-level quantile forecast is never larger than the 0.6-level forecast. Furthermore, the method comes with a no-regret guarantee that implies it will not worsen the performance of an existing forecaster, asymptotically, with respect to the quantile loss. In experiments, we find that MultiQT significantly improves the calibration of real forecasters in epidemic and energy forecasting problems.

</details>


### [575] [Bellman Calibration for V-Learning in Offline Reinforcement Learning](https://arxiv.org/abs/2512.23694)
*Lars van der Laan,Nathan Kallus*

Main category: stat.ML

TL;DR: The paper introduces Iterated Bellman Calibration (IBC), a method to enhance the calibration of off-policy value predictions for Markov decision processes without relying on model-specific constraints.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of poorly calibrated off-policy value predictions in infinite-horizon Markov decision processes and improve consistency with the Bellman equation.

Method: The IBC method involves repeatedly regressing fitted Bellman targets onto predictions from a model, employing a doubly robust pseudo-outcome technique. This creates a flexible one-dimensional fitted value iteration approach adaptable to any value estimator.

Result: IBC ensures calibration and predictive accuracy under finite-sample scenarios without depending on strict assumptions like Bellman completeness or realizability.

Conclusion: The proposed technique demonstrates robust calibration capabilities and provides theoretical guarantees under relatively weak assumptions, making it broadly applicable to diverse value estimation models.

Abstract: We introduce Iterated Bellman Calibration, a simple, model-agnostic, post-hoc procedure for calibrating off-policy value predictions in infinite-horizon Markov decision processes. Bellman calibration requires that states with similar predicted long-term returns exhibit one-step returns consistent with the Bellman equation under the target policy. We adapt classical histogram and isotonic calibration to the dynamic, counterfactual setting by repeatedly regressing fitted Bellman targets onto a model's predictions, using a doubly robust pseudo-outcome to handle off-policy data. This yields a one-dimensional fitted value iteration scheme that can be applied to any value estimator. Our analysis provides finite-sample guarantees for both calibration and prediction under weak assumptions, and critically, without requiring Bellman completeness or realizability.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [576] [Simultaneous Approximation of the Score Function and Its Derivatives by Deep Neural Networks](https://arxiv.org/abs/2512.23643)
*Konstantin Yakovlev,Nikita Puchkin*

Main category: math.NA

TL;DR: This paper introduces a theory to approximate the score function and its derivatives for data distributions with unbounded support, bypassing dimensionality constraints.


<details>
  <summary>Details</summary>
Motivation: To address challenges in approximating score functions and their derivatives for data distributions, especially with low-dimensional structures and unbounded support.

Method: Developing a theoretical framework with relaxed assumption requirements and error bounds that avoid the curse of dimensionality.

Result: Provided approximation guarantees for derivatives of any order with accuracy comparable to literature while removing dimensional constraints.

Conclusion: The framework successfully expands approximation methods by relaxing bounded support assumptions, enabling broader application.

Abstract: We present a theory for simultaneous approximation of the score function and its derivatives, enabling the handling of data distributions with low-dimensional structure and unbounded support. Our approximation error bounds match those in the literature while relying on assumptions that relax the usual bounded support requirement. Crucially, our bounds are free from the curse of dimensionality. Moreover, we establish approximation guarantees for derivatives of any prescribed order, extending beyond the commonly considered first-order setting.

</details>


### [577] [Differentiable Inverse Modeling with Physics-Constrained Latent Diffusion for Heterogeneous Subsurface Parameter Fields](https://arxiv.org/abs/2512.22421)
*Zihan Lin,QiZhi He*

Main category: math.NA

TL;DR: The paper introduces LD-DIM, a method combining latent diffusion and PDE solvers for solving high-dimensional inverse problems, improving numerical stability and reconstruction accuracy compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Address challenges in PDE-constrained inverse problems with spatially distributed coefficients, especially those involving poor numerical conditioning and sparse observations.

Method: LD-DIM integrates a latent diffusion model with a differentiable numerical PDE solver, using adjoint gradients for reconstruction directly in latent space to suppress ill-conditioned degrees of freedom.

Result: LD-DIM demonstrates superior numerical stability and reconstruction accuracy compared to PINNs and physics-embedded VAE baselines in flow in porous media inverse problems.

Conclusion: The method enables robust solutions to high-dimensional inverse problems by combining latent space learning and stable numerical optimization techniques.

Abstract: We present a latent diffusion-based differentiable inversion method (LD-DIM) for PDE-constrained inverse problems involving high-dimensional spatially distributed coefficients. LD-DIM couples a pretrained latent diffusion prior with an end-to-end differentiable numerical solver to reconstruct unknown heterogeneous parameter fields in a low-dimensional nonlinear manifold, improving numerical conditioning and enabling stable gradient-based optimization under sparse observations. The proposed framework integrates a latent diffusion model (LDM), trained in a compact latent space, with a differentiable finite-volume discretization of the forward PDE. Sensitivities are propagated through the discretization using adjoint-based gradients combined with reverse-mode automatic differentiation. Inversion is performed directly in latent space, which implicitly suppresses ill-conditioned degrees of freedom while preserving dominant structural modes, including sharp material interfaces. The effectiveness of LD-DIM is demonstrated using a representative inverse problem for flow in porous media, where heterogeneous conductivity fields are reconstructed from spatially sparse hydraulic head measurements. Numerical experiments assess convergence behavior and reconstruction quality for both Gaussian random fields and bimaterial coefficient distributions. The results show that LD-DIM achieves consistently improved numerical stability and reconstruction accuracy of both parameter fields and corresponding PDE solutions compared with physics-informed neural networks (PINNs) and physics-embedded variational autoencoder (VAE) baselines, while maintaining sharp discontinuities and reducing sensitivity to initialization.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [578] [Pre-review to Peer review: Pitfalls of Automating Reviews using Large Language Models](https://arxiv.org/abs/2512.22145)
*Akhil Pandey Akella,Harish Varma Siravuri,Shaurya Rohatgi*

Main category: cs.DL

TL;DR: The paper explores the role of Large Language Models (LLMs) in scholarly peer review, particularly focusing on their use as pre-review agents. While beneficial, LLMs raise concerns about safety, integrity, and alignment in academic peer-review processes.


<details>
  <summary>Details</summary>
Motivation: To evaluate the reliability and utility of LLMs as tools in automating academic peer-review, particularly as pre-review agents, while addressing concerns of misalignment and validity in scholarly assessments.

Method: The study used frontier open-weight LLMs to generate paper reviews, comparing them with human reviewer ratings from OpenReview and post-publication metrics such as citations, novelty, and disruption.

Result: The results showed weak correlation (0.15) between LLM reviews and human peer reviewers, overestimation biases, and high confidence ratings. However, LLM reviews aligned more with post-publication metrics, indicating their utility for pre-review screening.

Conclusion: LLMs have potential as pre-review screening tools but exhibit significant misalignment risks when used autonomously. The paper provides insights into the automation of peer reviews and offers an open dataset to further expand safety frameworks.

Abstract: Large Language Models are versatile general-task solvers, and their capabilities can truly assist people with scholarly peer review as \textit{pre-review} agents, if not as fully autonomous \textit{peer-review} agents. While incredibly beneficial, automating academic peer-review, as a concept, raises concerns surrounding safety, research integrity, and the validity of the academic peer-review process. The majority of the studies performing a systematic evaluation of frontier LLMs generating reviews across science disciplines miss the mark on addressing the alignment/misalignment of reviews along with the utility of LLM generated reviews when compared against publication outcomes such as \textbf{Citations}, \textbf{Hit-papers}, \textbf{Novelty}, and \textbf{Disruption}. This paper presents an experimental study in which we gathered ground-truth reviewer ratings from OpenReview and used various frontier open-weight LLMs to generate reviews of papers to gauge the safety and reliability of incorporating LLMs into the scientific review pipeline. Our findings demonstrate the utility of frontier open-weight LLMs as pre-review screening agents despite highlighting fundamental misalignment risks when deployed as autonomous reviewers. Our results show that all models exhibit weak correlation with human peer reviewers (0.15), with systematic overestimation bias of 3-5 points and uniformly high confidence scores (8.0-9.0/10) despite prediction errors. However, we also observed that LLM reviews correlate more strongly with post-publication metrics than with human scores, suggesting potential utility as pre-review screening tools. Our findings highlight the potential and address the pitfalls of automating peer reviews with language models. We open-sourced our dataset $D_{LMRSD}$ to help the research community expand the safety framework of automating scientific reviews.

</details>


### [579] [Interpretable Link Prediction in AI-Driven Cancer Research: Uncovering Co-Authorship Patterns](https://arxiv.org/abs/2512.22181)
*Shahab Mosallaie,Andrea Schiffauerova,Ashkan Ebadi*

Main category: cs.DL

TL;DR: This study uses machine learning on co-authorship networks in AI-driven cancer research to understand and predict collaborative patterns.


<details>
  <summary>Details</summary>
Motivation: To address challenges in forming effective interdisciplinary research teams in AI-driven cancer research, aiming to enhance collaboration and optimize resources.

Method: The study analyzed 7,738 publications (2000-2017) using co-authorship networks, engineered features, and applied machine learning classifiers. Random forest and SHAP were used for interpretability.

Result: Discipline similarity positively impacts new/persistent collaborations but negatively impacts discontinued ones. High productivity and seniority are linked to discontinued collaborations.

Conclusion: Findings inform effective team formation, promote interdisciplinary collaboration, and support strategic decision-making in research policy.

Abstract: Artificial intelligence (AI) is transforming cancer diagnosis and treatment. The intricate nature of this disease necessitates the collaboration of diverse stakeholders with varied expertise to ensure the effectiveness of cancer research. Despite its importance, forming effective interdisciplinary research teams remains challenging. Understanding and predicting collaboration patterns can help researchers, organizations, and policymakers optimize resources and foster impactful research. We examined co-authorship networks as a proxy for collaboration within AI-driven cancer research. Using 7,738 publications (2000-2017) from Scopus, we constructed 36 overlapping co-authorship networks representing new, persistent, and discontinued collaborations. We engineered both attribute-based and structure-based features and built four machine learning classifiers. Model interpretability was performed using Shapley Additive Explanations (SHAP). Random forest achieved the highest recall for all three types of examined collaborations. The discipline similarity score emerged as a crucial factor, positively affecting new and persistent patterns while negatively impacting discontinued collaborations. Additionally, high productivity and seniority were positively associated with discontinued links. Our findings can guide the formation of effective research teams, enhance interdisciplinary cooperation, and inform strategic policy decisions.

</details>


### [580] [AETAS: Analysis of Evolving Temporal Affect and Semantics for Legal History](https://arxiv.org/abs/2512.22196)
*Qizhi Wang*

Main category: cs.DL

TL;DR: The paper introduces a reproducible pipeline to analyze semantic shifts in legal documents over time, using the Old Bailey Corpus.


<details>
  <summary>Details</summary>
Motivation: To address the gap between manual close readings and opaque embedding methods in studying semantic shifts in historical texts.

Method: The authors train skip-gram embeddings, align spaces using orthogonal Procrustes, and assess semantic shift through geometric displacement and neighborhood turnover, paired with visual analytics outputs.

Result: Semantic trajectories reveal shifts in words related to justice, crime, poverty, and insanity in the Old Bailey Corpus, influenced by societal and legal reforms.

Conclusion: The implemented pipeline offers an auditable and interpretable framework for analyzing lexical semantic changes over time in historical datasets.

Abstract: Digital-humanities work on semantic shift often alternates between handcrafted close readings and opaque embedding machinery. We present a reproducible expert-system style pipeline that quantifies and visualises lexical drift in the Old Bailey Corpus (1720--1913), coupling interpretable trajectories with legally meaningful axes. We bin proceedings by decade with dynamic merging for low-resource slices, train skip-gram embeddings, align spaces through orthogonal Procrustes, and measure both geometric displacement and neighborhood turnover. Three visual analytics outputs, which are drift magnitudes, semantic trajectories, and movement along a mercy-versus-retribution axis, expose how justice, crime, poverty, and insanity evolve with penal reforms, transportation debates, and Victorian moral politics. The pipeline is implemented as auditable scripts so results can be reproduced in other historical corpora.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [581] [From geometry to dynamics: Learning overdamped Langevin dynamics from sparse observations with geometric constraints](https://arxiv.org/abs/2512.23566)
*Dimitra Maoutsa*

Main category: math.DS

TL;DR: This paper introduces a framework to learn stochastic system dynamics from sparse data by reformulating the problem as stochastic control and using geometry-driven path augmentation, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: The challenge is to infer dynamics of systems from sparse temporal data, which current methods struggle with for non-conservative and stochastic systems.

Method: The proposed framework uses geometry-driven path augmentation based on a system's invariant density to reconstruct trajectories and infer dynamics without relying on parametric models.

Result: The method demonstrates superior performance in recovering dynamics in overdamped Langevin systems and works effectively even for extremely sparse data.

Conclusion: Incorporating geometric inductive biases into stochastic system identification enables accurate recovery of dynamics from undersampled data.

Abstract: How can we learn the laws underlying the dynamics of stochastic systems when their trajectories are sampled sparsely in time? Existing methods either require temporally resolved high-frequency observations, or rely on geometric arguments that apply only to conservative systems, limiting the range of dynamics they can recover. Here, we present a new framework that reconciles these two perspectives by reformulating inference as a stochastic control problem. Our method uses geometry-driven path augmentation, guided by the geometry in the system's invariant density to reconstruct likely trajectories and infer the underlying dynamics without assuming specific parametric models. Applied to overdamped Langevin systems, our approach accurately recovers stochastic dynamics even from extremely undersampled data, outperforming existing methods in synthetic benchmarks. This work demonstrates the effectiveness of incorporating geometric inductive biases into stochastic system identification methods.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [582] [Assessing behaviour coverage in a multi-agent system simulation for autonomous vehicle testing](https://arxiv.org/abs/2512.23445)
*Manuel Franco-Vivo*

Main category: cs.MA

TL;DR: This paper focuses on evaluating and improving autonomous vehicle testing simulations using behavior coverage analysis, highlighting areas for optimization and introducing an MPC pedestrian agent.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges of ensuring safety and reliability in autonomous vehicle systems by improving testing methodologies and validating their performance in diverse real-world scenarios.

Method: The study performs behavior coverage analysis in a multi-agent system simulation for autonomous driving, defines driving scenarios and agent interactions, and proposes an MPC pedestrian agent to optimize simulation tests for realistic behavior.

Result: The findings underscore the significance of behavior coverage in validating autonomous systems' robustness. The MPC pedestrian agent enhances realism and enables better testing outcomes.

Conclusion: The research advances autonomous vehicle testing, offering significant insights to enhance safety, reliability, and performance through better behavior coverage and simulation optimization.

Abstract: As autonomous vehicle technology advances, ensuring the safety and reliability of these systems becomes paramount. Consequently, comprehensive testing methodologies are essential to evaluate the performance of autonomous vehicles in diverse and complex real-world scenarios. This study focuses on the behaviour coverage analysis of a multi-agent system simulation designed for autonomous vehicle testing, and provides a systematic approach to measure and assess behaviour coverage within the simulation environment. By defining a set of driving scenarios, and agent interactions, we evaluate the extent to which the simulation encompasses a broad range of behaviours relevant to autonomous driving.
  Our findings highlight the importance of behaviour coverage in validating the effectiveness and robustness of autonomous vehicle systems. Through the analysis of behaviour coverage metrics and coverage-based testing, we identify key areas for improvement and optimization in the simulation framework. Thus, a Model Predictive Control (MPC) pedestrian agent is proposed, where its objective function is formulated to encourage \textit{interesting} tests while promoting a more realistic behaviour than other previously studied pedestrian agents. This research contributes to advancing the field of autonomous vehicle testing by providing insights into the comprehensive evaluation of system behaviour in simulated environments. The results offer valuable implications for enhancing the safety, reliability, and performance of autonomous vehicles through rigorous testing methodologies.

</details>


### [583] [ReCollab: Retrieval-Augmented LLMs for Cooperative Ad-hoc Teammate Modeling](https://arxiv.org/abs/2512.22129)
*Conor Wallace,Umer Siddique,Yongcan Cao*

Main category: cs.MA

TL;DR: The paper presents a framework using Large Language Models (LLMs) to classify and adapt to new teammate behaviors in ad-hoc teamwork scenarios, improving performance in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of conventional agent models in inferring and adapting to unseen teammate behaviors under partial observability and limited interactions.

Method: A framework, Collab, uses LLMs to classify teammate behavior with trajectory features. This is extended to ReCollab, which incorporates retrieval-augmented generation (RAG) to enhance stability through example trajectories.

Result: In the Overcooked environment, Collab effectively identifies teammate types, and ReCollab improves adaptation and achieves a balance between accurate classification and better episodic performance.

Conclusion: LLMs, especially with retrieval grounding, show great promise as behavioral world models for ad-hoc teamwork, enhancing coordination in complex environments.

Abstract: Ad-hoc teamwork (AHT) requires agents to infer the behavior of previously unseen teammates and adapt their policy accordingly. Conventional approaches often rely on fixed probabilistic models or classifiers, which can be brittle under partial observability and limited interaction. Large language models (LLMs) offer a flexible alternative: by mapping short behavioral traces into high-level hypotheses, they can serve as world models over teammate behavior. We introduce \Collab, a language-based framework that classifies partner types using a behavior rubric derived from trajectory features, and extend it to \ReCollab, which incorporates retrieval-augmented generation (RAG) to stabilize inference with exemplar trajectories. In the cooperative Overcooked environment, \Collab effectively distinguishes teammate types, while \ReCollab consistently improves adaptation across layouts, achieving Pareto-optimal trade-offs between classification accuracy and episodic return. These findings demonstrate the potential of LLMs as behavioral world models for AHT and highlight the importance of retrieval grounding in challenging coordination settings.

</details>


### [584] [Solving Multi-Agent Multi-Goal Path Finding Problems in Polynomial Time](https://arxiv.org/abs/2512.22171)
*Stefan Edelkamp*

Main category: cs.MA

TL;DR: This paper addresses multi-agent mission planning in undirected graphs with multiple goals, using a planner to assign and optimize paths autonomously, ensuring conflict-free routing.


<details>
  <summary>Details</summary>
Motivation: To enable effective routing and task assignment for a fleet of agents in undirected graphs, reducing complexity and ensuring conflict-free, optimized paths, particularly where traditional methods struggle or are NP-hard.

Method: The authors propose an autonomous planner that assigns goals to agents while optimizing routing. For grid-like and continuous cases, the solution approaches optimality; for discrete graph variants with node and edge conflicts, polynomial-time solutions are achieved. Techniques include global assignment strategies, local conflict resolution, and innovative methods like agents interleaving and departure adjustments.

Result: A functional planner was designed and implemented. It efficiently planned conflict-free, optimized routes for agents in test scenarios with major improvements in minimizing conflicts.

Conclusion: The study presents a significant improvement over traditional methods by achieving polynomial-time solutions and developing practical strategies for multi-agent planning in undirected graphs.

Abstract: In this paper, we plan missions for a fleet of agents in undirected graphs, such as grids, with multiple goals. In contrast to regular multi-agent path-finding, the solver finds and updates the assignment of goals to the agents on its own. In the continuous case for a point agent with motions in the Euclidean plane, the problem can be solved arbitrarily close to optimal. For discrete variants that incur node and edge conflicts, we show that it can be solved in polynomial time, which is unexpected, since traditional vehicle routing on general graphs is NP-hard. We implement a corresponding planner that finds conflict-free optimized routes for the agents. Global assignment strategies greatly reduce the number of conflicts, with the remaining ones resolved by elaborating on the concept of ants-on-the-stick, by solving local assignment problems, by interleaving agent paths, and by kicking agents that have already arrived out of their destinations

</details>


### [585] [Hierarchical Pedagogical Oversight: A Multi-Agent Adversarial Framework for Reliable AI Tutoring](https://arxiv.org/abs/2512.22496)
*Saisab Sadhu,Ashim Dhor*

Main category: cs.MA

TL;DR: Hierarchical Pedagogical Oversight (HPO) enhances educational reasoning in large language models (LLMs), reducing sycophancy and fostering better pedagogical interaction.


<details>
  <summary>Details</summary>
Motivation: Address educator shortages by improving automated tutoring capabilities of LLMs, tackling issues such as validation of incorrect answers and overly direct solutions.

Method: Hierarchical Pedagogical Oversight (HPO) framework employs dialog context distillation and moderated adversarial debates between pedagogical critics to enhance reasoning.

Result: The proposed 8B-parameter model achieves a Macro F1 of 0.845 on MRBench, outperforming GPT-4o by 3.3% using significantly fewer computational resources.

Conclusion: Adversarial reasoning in HPO proves effective for reliable pedagogical oversight in constrained environments, demonstrating promising low-resource educational applications.

Abstract: Large Language Models (LLMs) are increasingly deployed as automated tutors to address educator shortages; however, they often fail at pedagogical reasoning, frequently validating incorrect student solutions (sycophancy) or providing overly direct answers that hinder learning. We introduce Hierarchical Pedagogical Oversight (HPO), a framework that adapts structured adversarial synthesis to educational assessment. Unlike cooperative multi-agent systems that often drift toward superficial consensus, HPO enforces a dialectical separation of concerns: specialist agents first distill dialogue context, which then grounds a moderated, five-act debate between opposing pedagogical critics. We evaluate this framework on the MRBench dataset of 1,214 middle-school mathematics dialogues. Our 8B-parameter model achieves a Macro F1 of 0.845, outperforming GPT-4o (0.812) by 3.3% while using 20 times fewer parameters. These results establish adversarial reasoning as a critical mechanism for deploying reliable, low-compute pedagogical oversight in resource-constrained environments.

</details>


### [586] [Reinforcement Networks: novel framework for collaborative Multi-Agent Reinforcement Learning tasks](https://arxiv.org/abs/2512.22876)
*Maksim Kryzhanovskiy,Svetlana Glazyrina,Roman Ischenko,Konstantin Vorontsov*

Main category: cs.MA

TL;DR: The paper introduces Reinforcement Networks, a framework for collaborative Multi-Agent Reinforcement Learning (MARL) that uses directed acyclic graphs (DAGs) to enhance coordination and credit assignment.


<details>
  <summary>Details</summary>
Motivation: Address limitations in MARL, such as restrictive architectures and training assumptions, to enable scalable and flexible multi-agent training.

Method: The Reinforcement Networks framework organizes agents as vertices in a directed acyclic graph (DAG). It formalizes training and inference methods and connects to LevelEnv for reproducible setups, advancing MARL credit assignment and system design.

Result: Several Reinforcement Networks models were tested, revealing improved performance over standard MARL baselines in collaborative setups. The framework unifies hierarchical, modular, and graph-structured MARL concepts.

Conclusion: Reinforcement Networks present a promising foundation for scalable and structured MARL, opening new possibilities in graph morphologies, curricula design, and exploration strategies.

Abstract: Modern AI systems often comprise multiple learnable components that can be naturally organized as graphs. A central challenge is the end-to-end training of such systems without restrictive architectural or training assumptions. Such tasks fit the theory and approaches of the collaborative Multi-Agent Reinforcement Learning (MARL) field. We introduce Reinforcement Networks, a general framework for MARL that organizes agents as vertices in a directed acyclic graph (DAG). This structure extends hierarchical RL to arbitrary DAGs, enabling flexible credit assignment and scalable coordination while avoiding strict topologies, fully centralized training, and other limitations of current approaches. We formalize training and inference methods for the Reinforcement Networks framework and connect it to the LevelEnv concept to support reproducible construction, training, and evaluation. We demonstrate the effectiveness of our approach on several collaborative MARL setups by developing several Reinforcement Networks models that achieve improved performance over standard MARL baselines. Beyond empirical gains, Reinforcement Networks unify hierarchical, modular, and graph-structured views of MARL, opening a principled path toward designing and training complex multi-agent systems. We conclude with theoretical and practical directions - richer graph morphologies, compositional curricula, and graph-aware exploration. That positions Reinforcement Networks as a foundation for a new line of research in scalable, structured MARL.

</details>


### [587] [Heterogeneity in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2512.22941)
*Tianyi Hu,Zhiqiang Pu,Yuan Wang,Tenghai Qiu,Min Chen,Xin Yu*

Main category: cs.MA

TL;DR: This paper systematically studies heterogeneity in multi-agent reinforcement learning (MARL), defines five types and proposes a way to quantify it, along with an algorithm for dynamic parameter sharing based on heterogeneity.


<details>
  <summary>Details</summary>
Motivation: The research aims to address the lack of rigorous definition and understanding of heterogeneity in the MARL field to advance theoretical and practical aspects.

Method: The paper categorizes heterogeneity into five types, proposes a heterogeneity distance quantification method, and develops a heterogeneity-based dynamic parameter sharing algorithm.

Result: Case studies confirm the method's ability to identify and quantify heterogeneity types. The algorithm demonstrates superior interpretability and adaptability over existing baselines.

Conclusion: The study provides the MARL community with tools to understand and leverage heterogeneity in agents and facilitates the development of better-performing, interpretable algorithms.

Abstract: Heterogeneity is a fundamental property in multi-agent reinforcement learning (MARL), which is closely related not only to the functional differences of agents, but also to policy diversity and environmental interactions. However, the MARL field currently lacks a rigorous definition and deeper understanding of heterogeneity. This paper systematically discusses heterogeneity in MARL from the perspectives of definition, quantification, and utilization. First, based on an agent-level modeling of MARL, we categorize heterogeneity into five types and provide mathematical definitions. Second, we define the concept of heterogeneity distance and propose a practical quantification method. Third, we design a heterogeneity-based multi-agent dynamic parameter sharing algorithm as an example of the application of our methodology. Case studies demonstrate that our method can effectively identify and quantify various types of agent heterogeneity. Experimental results show that the proposed algorithm, compared to other parameter sharing baselines, has better interpretability and stronger adaptability. The proposed methodology will help the MARL community gain a more comprehensive and profound understanding of heterogeneity, and further promote the development of practical algorithms.

</details>


<div id='math.AT'></div>

# math.AT [[Back]](#toc)

### [588] [Persistent Homology via Finite Topological Spaces](https://arxiv.org/abs/2512.23348)
*Selçuk Kayacan*

Main category: math.AT

TL;DR: This paper introduces a new functorial framework for persistent homology using finite topological spaces and posets, ensuring stability of persistence diagrams under metric perturbations.


<details>
  <summary>Details</summary>
Motivation: To create a functorial mechanism for persistent homology that avoids the need for inclusion relations between simplicial complexes, thereby enhancing flexibility and stability.

Method: The authors construct a filtration of finite topologies from a finite metric space, use functorial mappings to posets and crosscut constructions to simplicial complexes, and analyze the simplified persistence modules and their diagrams.

Result: The paper demonstrates that the proposed framework preserves persistent invariants and ensures stability of persistence diagrams under density-based perturbations of the input metric.

Conclusion: The proposed method offers an innovative, stable, and functorial approach to persistent homology, broadening its applicability and robustness.

Abstract: We propose a functorial framework for persistent homology based on finite topological spaces and their associated posets. Starting from a finite metric space, we associate a filtration of finite topologies whose structure maps are continuous identity maps. By passing functorially to posets and to simplicial complexes via crosscut constructions, we obtain persistence modules without requiring inclusion relations between the resulting complexes. We show that standard poset-level simplifications preserve persistent invariants and prove stability of the resulting persistence diagrams under perturbations of the input metric in a density-based instantiation.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [589] [Sampling with Shielded Langevin Monte Carlo Using Navigation Potentials](https://arxiv.org/abs/2512.22153)
*Nicolas Zilberstein,Santiago Segarra,Luiz Chamon*

Main category: stat.CO

TL;DR: The paper introduces shielded Langevin Monte Carlo (LMC), a method for constrained sampling in non-convex spaces with convex holes using adaptive temperature and repulsive drift.


<details>
  <summary>Details</summary>
Motivation: To tackle the challenge of sampling from unnormalized target distributions defined over complex non-convex spaces, which are difficult to handle using conventional samplers.

Method: The paper develops a technique combining spatially adaptive temperature and a repulsive drift to ensure samples remain within constrained non-convex spaces.

Result: The shielded LMC is tested on a 2D Gaussian mixture and MIMO symbol detection, demonstrating better performance compared to unconstrained sampling methods.

Conclusion: Shielded LMC is effective for constrained sampling in punctured supports and provides practical advantages in complex applications like MIMO symbol detection.

Abstract: We introduce shielded Langevin Monte Carlo (LMC), a constrained sampler inspired by navigation functions, capable of sampling from unnormalized target distributions defined over punctured supports. In other words, this approach samples from non-convex spaces defined as convex sets with convex holes. This defines a novel and challenging problem in constrained sampling. To do so, the sampler incorporates a combination of a spatially adaptive temperature and a repulsive drift to ensure that samples remain within the feasible region. Experiments on a 2D Gaussian mixture and multiple-input multiple-output (MIMO) symbol detection showcase the advantages of the proposed shielded LMC in contrast to unconstrained cases.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [590] [Integrating Wide and Deep Neural Networks with Squeeze-and-Excitation Blocks for Multi-Target Property Prediction in Additively Manufactured Fiber Reinforced Composites](https://arxiv.org/abs/2512.22397)
*Behzad Parvaresh,Rahmat K. Adesunkanmi,Adel Alaeddini*

Main category: physics.comp-ph

TL;DR: This paper proposes a data-efficient machine learning approach to predict mechanical and manufacturing properties of continuous fiber-reinforced composites made via additive manufacturing.


<details>
  <summary>Details</summary>
Motivation: Additive manufacturing of continuous fiber-reinforced composites provides lightweight, high-strength materials but finding optimal process parameters is challenging due to extensive experimental requirements.

Method: The study developed a squeeze-and-excitation wide and deep neural network (SE-WDNN) combined with Latin Hypercube Sampling (LHS) to predict mechanical and manufacturing properties from a dataset of 155 tested specimens.

Result: The proposed SE-WDNN model outperformed other machine learning approaches with the lowest test error (MAPE = 12.33%) and significant improvements in several key variables compared to baseline models.

Conclusion: Integrating LHS experimentation with SE-WDNN enables efficient and interpretable predictions for parameter selection, balancing mechanical and manufacturing outcomes in CFRC-AM applications.

Abstract: Continuous fiber-reinforced composite manufactured by additive manufacturing (CFRC-AM) offers opportunities for printing lightweight materials with high specific strength. However, their performance is sensitive to the interaction of process and material parameters, making exhaustive experimental testing impractical. In this study, we introduce a data-efficient, multi-input, multi-target learning approach that integrates Latin Hypercube Sampling (LHS)-guided experimentation with a squeeze-and-excitation wide and deep neural network (SE-WDNN) to jointly predict multiple mechanical and manufacturing properties of CFRC-AMs based on different manufacturing parameters. We printed and tested 155 specimens selected from a design space of 4,320 combinations using a Markforged Mark Two 3D printer. The processed data formed the input-output set for our proposed model. We compared the results with those from commonly used machine learning models, including feedforward neural networks, Kolmogorov-Arnold networks, XGBoost, CatBoost, and random forests. Our model achieved the lowest overall test error (MAPE = 12.33%) and showed statistically significant improvements over the baseline wide and deep neural network for several target variables (paired t-tests, p <= 0.05). SHapley Additive exPlanations (SHAP) analysis revealed that reinforcement strategy was the major influence on mechanical performance. Overall, this study demonstrates that the integration of LHS and SE-WDNN enables interpretable and sample-efficient multi-target predictions, guiding parameter selection in CFRC-AM with a balance between mechanical behavior and manufacturing metrics.

</details>


### [591] [PINNs for Electromagnetic Wave Propagation](https://arxiv.org/abs/2512.23396)
*Nilufer K. Bulut*

Main category: physics.comp-ph

TL;DR: This study explores hybrid training techniques for Physics-Informed Neural Networks (PINNs) to enhance their accuracy and energy performance, making them competitive with traditional FDTD methods in electromagnetism.


<details>
  <summary>Details</summary>
Motivation: PINNs struggle with accuracy and energy consistency compared to established FDTD methods. The paper aims to overcome these limitations using hybrid training techniques to improve PINNs for electromagnetism applications.

Method: It introduces time marching, causality-aware weighting, a two-stage interface continuity loss, and a local Poynting-based regularizer to address deficiencies in PINNs for wave propagation scenarios.

Result: High accuracy was achieved with a 0.09% NRMSE and 1.01% L2 error, alongside energy conservation with a 0.024% relative energy mismatch in a 2D PEC cavity test, validated without labeled field data.

Conclusion: PINNs, enriched with hybrid strategies, can closely match FDTD’s performance in electromagnetism, offering a mesh-free, physics-based alternative for solving physical systems.

Abstract: Physics-Informed Neural Networks (PINNs) are a methodology that aims to solve physical systems by directly embedding PDE constraints into the neural network training process. In electromagnetism, where well-established methodologies such as FDTD and FEM already exist, new methodologies are expected to provide clear advantages to be accepted. Despite their mesh-free nature and applicability to inverse problems, PINNs can exhibit deficiencies in terms of accuracy and energy metrics when compared to FDTD solutions. This study demonstrates hybrid training strategies can bring PINNs closer to FDTD-level accuracy and energy consistency.
  This study presents a hybrid methodology addressing common challenges in wave propagation scenarios. The causality collapse problem in time-dependent PINN training is addressed via time marching and causality-aware weighting. In order to mitigate the discontinuities that are introduced by time marching, a two-stage interface continuity loss is applied. In order to suppress loss accumulation, which is manifested as cumulative energy drift in electromagnetic waves, a local Poynting-based regularizer has been developed.
  In the developed PINN model, high field accuracy is achieved with an average 0.09\% $NRMSE$ and 1.01\% $L^2$ error over time. Energy conservation is achieved on the PINN side with only a 0.024\% relative energy mismatch in the 2D PEC cavity scenario. Training is performed without labeled field data, using only physics-based residual losses; FDTD is used solely for post-training evaluation. The results demonstrate that PINNs can achieve competitive results with FDTD in canonical electromagnetic examples and are a viable alternative.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [592] [Deep classifier kriging for probabilistic spatial prediction of air quality index](https://arxiv.org/abs/2512.23474)
*Junyu Chen,Pratik Nag,Huixia Judy-Wang,Ying Sun*

Main category: stat.ME

TL;DR: The paper introduces "deep classifier kriging" (DCK), a deep learning approach for spatial prediction of air quality, offering better accuracy and uncertainty quantification compared to classical methods.


<details>
  <summary>Details</summary>
Motivation: Air quality index (AQI) fields show non-Gaussian, nonlinear spatial structures which classical methods like kriging cannot fully handle. There is a need for more flexible tools to improve regulatory decision-making.

Method: The authors developed DCK, a distribution-free deep learning framework capable of estimating full predictive distributions for spatial processes, with a data fusion mechanism for integrating diverse air pollution data sources.

Result: Through simulations, DCK demonstrated superior performance over conventional approaches in predictive accuracy and uncertainty quantification. It successfully integrated high-quality sparse data with biased continuous model outputs.

Conclusion: DCK is a more effective tool for probabilistic AQI spatial prediction, aiding regulatory tasks like risk assessment and policy formulation by providing spatially resolved predictive distributions.

Abstract: Accurate spatial interpolation of the air quality index (AQI), computed from concentrations of multiple air pollutants, is essential for regulatory decision-making, yet AQI fields are inherently non-Gaussian and often exhibit complex nonlinear spatial structure. Classical spatial prediction methods such as kriging are linear and rely on Gaussian assumptions, which limits their ability to capture these features and to provide reliable predictive distributions. In this study, we propose \textit{deep classifier kriging} (DCK), a flexible, distribution-free deep learning framework for estimating full predictive distribution functions for univariate and bivariate spatial processes, together with a \textit{data fusion} mechanism that enables modeling of non-collocated bivariate processes and integration of heterogeneous air pollution data sources. Through extensive simulation experiments, we show that DCK consistently outperforms conventional approaches in predictive accuracy and uncertainty quantification. We further apply DCK to probabilistic spatial prediction of AQI by fusing sparse but high-quality station observations with spatially continuous yet biased auxiliary model outputs, yielding spatially resolved predictive distributions that support downstream tasks such as exceedance and extreme-event probability estimation for regulatory risk assessment and policy formulation.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [593] [A first-order method for nonconvex-strongly-concave constrained minimax optimization](https://arxiv.org/abs/2512.22909)
*Zhaosong Lu,Sanyou Mei*

Main category: math.OC

TL;DR: This paper addresses a constrained minimax problem using an augmented Lagrangian method and introduces an improved first-order method for solving subproblems, achieving a better operation complexity.


<details>
  <summary>Details</summary>
Motivation: The study aims to improve existing methods for solving nonconvex-strongly-concave constrained minimax problems by addressing efficiency and complexity issues.

Method: A first-order augmented Lagrangian approach is proposed, complemented by a novel method that utilizes the strong concavity structure for solving nonconvex-strongly-concave subproblems.

Result: The method achieves an operation complexity of $O(\varepsilon^{-3.5}\log\varepsilon^{-1})$, outperforming previous approaches by a factor of $\varepsilon^{-0.5}$.

Conclusion: The proposed approach provides an enhanced solution technique for constrained minimax problems, delivering a significant improvement in operation complexity and efficiency.

Abstract: In this paper we study a nonconvex-strongly-concave constrained minimax problem. Specifically, we propose a first-order augmented Lagrangian method for solving it, whose subproblems are nonconvex-strongly-concave unconstrained minimax problems and suitably solved by a first-order method developed in this paper that leverages the strong concavity structure. Under suitable assumptions, the proposed method achieves an \emph{operation complexity} of $O(\varepsilon^{-3.5}\log\varepsilon^{-1})$, measured in terms of its fundamental operations, for finding an $\varepsilon$-KKT solution of the constrained minimax problem, which improves the previous best-known operation complexity by a factor of $\varepsilon^{-0.5}$.

</details>


### [594] [Clipped Gradient Methods for Nonsmooth Convex Optimization under Heavy-Tailed Noise: A Refined Analysis](https://arxiv.org/abs/2512.23178)
*Zijian Liu*

Main category: math.OC

TL;DR: This paper refines the analysis of Clipped Stochastic Gradient Descent (SGD), achieving faster rates under heavy-tailed noise using the generalized effective dimension. It also provides new convergence bounds and demonstrates optimality of the refined analysis.


<details>
  <summary>Details</summary>
Motivation: Heavy-tailed gradient noise is common in modern machine learning tasks, prompting a need for optimization techniques that work effectively under these conditions.

Method: The study improves upon Clipped SGD by leveraging Freedman's inequality and identifying a generalized effective dimension to develop refined bounds for both high-probability and expectation convergence under heavy-tailed noise.

Result: Faster convergence rates are derived, surpassing previous results. Additionally, the study establishes matching lower and upper bounds for in-expectation convergence, emphasizing the refined method's optimality.

Conclusion: The proposed refined analysis offers significant improvements in Clipped SGD, providing optimal convergence rates and addressing the challenges posed by heavy-tailed noise efficiently.

Abstract: Optimization under heavy-tailed noise has become popular recently, since it better fits many modern machine learning tasks, as captured by empirical observations. Concretely, instead of a finite second moment on gradient noise, a bounded ${\frak p}$-th moment where ${\frak p}\in(1,2]$ has been recognized to be more realistic (say being upper bounded by $σ_{\frak l}^{\frak p}$ for some $σ_{\frak l}\ge0$). A simple yet effective operation, gradient clipping, is known to handle this new challenge successfully. Specifically, Clipped Stochastic Gradient Descent (Clipped SGD) guarantees a high-probability rate ${\cal O}(σ_{\frak l}\ln(1/δ)T^{1/{\frak p}-1})$ (resp. ${\cal O}(σ_{\frak l}^2\ln^2(1/δ)T^{2/{\frak p}-2})$) for nonsmooth convex (resp. strongly convex) problems, where $δ\in(0,1]$ is the failure probability and $T\in\mathbb{N}$ is the time horizon. In this work, we provide a refined analysis for Clipped SGD and offer two faster rates, ${\cal O}(σ_{\frak l}d_{\rm eff}^{-1/2{\frak p}}\ln^{1-1/{\frak p}}(1/δ)T^{1/{\frak p}-1})$ and ${\cal O}(σ_{\frak l}^2d_{\rm eff}^{-1/{\frak p}}\ln^{2-2/{\frak p}}(1/δ)T^{2/{\frak p}-2})$, than the aforementioned best results, where $d_{\rm eff}\ge1$ is a quantity we call the $\textit{generalized effective dimension}$. Our analysis improves upon the existing approach on two sides: better utilization of Freedman's inequality and finer bounds for clipping error under heavy-tailed noise. In addition, we extend the refined analysis to convergence in expectation and obtain new rates that break the known lower bounds. Lastly, to complement the study, we establish new lower bounds for both high-probability and in-expectation convergence. Notably, the in-expectation lower bounds match our new upper bounds, indicating the optimality of our refined analysis for convergence in expectation.

</details>


### [595] [Constraint programming model and biased random-key genetic algorithm for the single-machine coupled task scheduling problem with exact delays to minimize the makespan](https://arxiv.org/abs/2512.23150)
*Vítor A. Barbosa,Rafael A. Melo*

Main category: math.OC

TL;DR: This paper addresses a single-machine coupled task scheduling problem aiming to minimize makespan, proposing a Constraint Programming (CP) model and a Biased Random-Key Genetic Algorithm (BRKGA) with enhanced components for efficiency.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the computational complexity and efficiency of solving the strongly NP-hard scheduling problem with exact delays, focusing on minimizing the makespan in non-preemptive scheduling scenarios.

Method: The paper employs a Constraint Programming (CP) model incorporating well-established global constraints, alongside a Biased Random-Key Genetic Algorithm (BRKGA). BRKGA integrates advanced techniques like initial solution generation, periodic restarts, shakes, and local search, with a focus on efficient solution exploration.

Result: Experiments show that BRKGA provides high-quality approximate solutions within low computational times for instances with up to 100 jobs. Under the same computational constraints, BRKGA outperformed the CP model, while CP achieved superior solutions with extended running times and multi-threading.

Conclusion: The proposed BRKGA is effective for quick and high-quality scheduling solutions. Combined components like shake and local search enhance results, and extended CP computation times yield near-optimal solutions for most instances, showcasing distinct advantages of both methods.

Abstract: We consider the strongly NP-hard single-machine coupled task scheduling problem with exact delays to minimize the makespan. In this problem, a set of jobs has to be scheduled, each composed of two tasks interspersed by an exact delay. Given that no preemption is allowed, the goal consists of minimizing the completion time of the last scheduled task. We model the problem using constraint programming (CP) and propose a biased random-key genetic algorithm (BRKGA). Our CP model applies well-established global constraints. Our BRKGA combines some successful components in the literature: an initial solution generator, periodical restarts and shakes, and a local search algorithm. Furthermore, the BRKGA's decoder is focused on efficiency rather than optimality, which accelerates the solution space exploration. Computational experiments on a benchmark set containing instances with up to 100 jobs (200 tasks) indicate that the proposed BRKGA can efficiently explore the problem solution space, providing high-quality approximate solutions within low computational times. It can also provide better solutions than the CP model under the same computational settings, i.e., three minutes of time limit and a single thread. The CP model, when offered a longer running time of 3600 seconds and multiple threads, significantly improved the results, reaching the current best-known solution for 90.56% of these instances. Finally, our experiments highlight the importance of the shake and local search components in the BRKGA, whose combination significantly improves the results of a standard BRKGA.

</details>


### [596] [Deep Learning for the Multiple Optimal Stopping Problem](https://arxiv.org/abs/2512.22961)
*Mathieu Laurière,Mehdi Talbi*

Main category: math.OC

TL;DR: This paper proposes a deep learning framework to address multiple optimal stopping problems using neural network value approximation, demonstrating scalability and efficiency.


<details>
  <summary>Details</summary>
Motivation: To tackle the challenges of solving multiple optimal stopping problems in high dimensions, which involve complex recursive dependencies.

Method: The framework combines the Dynamic Programming Principle with neural network approximation of the value function, analyzing neural network training error for discrete-time problems and discretization error for continuous problems.

Result: The method performs well on high-dimensional American basket options and nonlinear utility maximization problems, proving efficiency and scalability.

Conclusion: The proposed framework successfully resolves the complexities of multiple optimal stopping problems in high dimensions by leveraging neural networks and dynamic programming principles.

Abstract: This paper presents a novel deep learning framework for solving multiple optimal stopping problems in high dimensions. While deep learning has recently shown promise for single stopping problems, the multiple exercise case involves complex recursive dependencies that remain challenging. We address this by combining the Dynamic Programming Principle with neural network approximation of the value function. Unlike policy-search methods, our algorithm explicitly learns the value surface. We first consider the discrete-time problem and analyze neural network training error. We then turn to continuous problems and analyze the additional error due to the discretization of the underlying stochastic processes. Numerical experiments on high-dimensional American basket options and nonlinear utility maximization demonstrate that our method provides an efficient and scalable method for the multiple optimal stopping problem.

</details>


### [597] [Risk-Averse Learning with Varying Risk Levels](https://arxiv.org/abs/2512.22986)
*Siyi Wang,Zifan Wang,Karl H. Johansson*

Main category: math.OC

TL;DR: This paper investigates online optimization in dynamic and risk-sensitive contexts, proposing algorithms that minimize regret while adapting to changing conditions.


<details>
  <summary>Details</summary>
Motivation: To address the need for adaptable decision-making in dynamic, safety-critical environments with varying risk levels.

Method: The study develops algorithms using Conditional Value-at-Risk (CVaR) for both first-order (gradient and function observations) and zeroth-order (function evaluations only) scenarios. It introduces a new risk-level variation metric and analyzes regret bounds based on function and risk variations.

Result: The algorithms adapt effectively in dynamic, non-stationary environments, as shown through regret analysis and numerical experiments.

Conclusion: The proposed risk-averse learning algorithms are effective in dynamic optimization and offer a practical approach for managing change and risk in critical decision-making systems.

Abstract: In safety-critical decision-making, the environment may evolve over time, and the learner adjusts its risk level accordingly. This work investigates risk-averse online optimization in dynamic environments with varying risk levels, employing Conditional Value-at-Risk (CVaR) as the risk measure. To capture the dynamics of the environment and risk levels, we employ the function variation metric and introduce a novel risk-level variation metric. Two information settings are considered: a first-order scenario, where the learner observes both function values and their gradients; and a zeroth-order scenario, where only function evaluations are available. For both cases, we develop risk-averse learning algorithms with a limited sampling budget and analyze their dynamic regret bounds in terms of function variation, risk-level variation, and the total number of samples. The regret analysis demonstrates the adaptability of the algorithms in non-stationary and risk-sensitive settings. Finally, numerical experiments are presented to demonstrate the efficacy of the methods.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [598] [Literature Mining System for Nutraceutical Biosynthesis: From AI Framework to Biological Insight](https://arxiv.org/abs/2512.22225)
*Xinyang Sun,Nipon Sarmah,Miao Guo*

Main category: q-bio.QM

TL;DR: This paper introduces an AI-driven system using large language models and prompt engineering to identify microbes involved in nutraceutical compound biosynthesis from scientific literature.


<details>
  <summary>Details</summary>
Motivation: The bottleneck in identifying microbial strains for nutraceutical biosynthesis due to unstructured scientific text inspired the development of an automated system.

Method: Developed a domain-adapted system with few-shot prompting and tailored queries, leveraging large language models like DeepSeekV3 and LLaMA2 for text mining.

Result: Generated a validated dataset of 35 nutraceutical-strain associations, revealing diverse microbial contributors, including Corynebacterium glutamicum and synthetic consortia.

Conclusion: The framework improves scalability and interpretability in literature mining, aiding microbial strain selection and precision fermentation in nutraceutical production.

Abstract: The extraction of structured knowledge from scientific literature remains a major bottleneck in nutraceutical research, particularly when identifying microbial strains involved in compound biosynthesis. This study presents a domain-adapted system powered by large language models (LLMs) and guided by advanced prompt engineering techniques to automate the identification of nutraceutical-producing microbes from unstructured scientific text. By leveraging few-shot prompting and tailored query designs, the system demonstrates robust performance across multiple configurations, with DeepSeekV3 outperforming LLaMA2 in accuracy, especially when domain-specific strain information is included. A structured and validated dataset comprising 35 nutraceutical-strain associations was generated, spanning amino acids, fibers, phytochemicals, and vitamins. The results reveal significant microbial diversity across monoculture and co-culture systems, with dominant contributions from Corynebacterium glutamicum, Escherichia coli, and Bacillus subtilis, alongside emerging synthetic consortia. This AI-driven framework not only enhances the scalability and interpretability of literature mining but also provides actionable insights for microbial strain selection, synthetic biology design, and precision fermentation strategies in the production of high-value nutraceuticals.

</details>


### [599] [INSIGHT: Spatially resolved survival modelling from routine histology crosslinked with molecular profiling reveals prognostic epithelial-immune axes in stage II/III colorectal cancer](https://arxiv.org/abs/2512.22262)
*Piotr Keller,Mark Eastwood,Zedong Hu,Aimée Selten,Ruqayya Awan,Gertjan Rasschaert,Sara Verbandt,Vlad Popovici,Hubert Piessevaux,Hayley T Morris,Petros Tsantoulis,Thomas Alexander McKee,André D'Hoore,Cédric Schraepen,Xavier Sagaert,Gert De Hertogh,Sabine Tejpar,Fayyaz Minhas*

Main category: q-bio.QM

TL;DR: The paper introduces a graph neural network-based model, INSIGHT, which predicts survival in stage II/III colorectal cancer patients from histology images with superior accuracy compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: To harness the rich prognostic information in spatial tissue organization from routine histology for improved survival prediction in colorectal cancer.

Method: A graph neural network named INSIGHT was trained and cross-validated using TCGA and SURGEN datasets to generate spatially resolved risk scores and interpret spatial and molecular correlates of risk in colorectal cancer.

Result: INSIGHT significantly outperformed traditional methods in prognostic performance (C-index 0.68-0.69 vs 0.44-0.58) and identified specific tissue-level and molecular risk correlates, revealing epithelial and immune state heterogeneity.

Conclusion: The study highlights the potential of spatial risk analysis and INSIGHT to improve prognostic assessment in colorectal cancer, identify molecular vulnerabilities, and suggest personalized therapeutic strategies.

Abstract: Routine histology contains rich prognostic information in stage II/III colorectal cancer, much of which is embedded in complex spatial tissue organisation. We present INSIGHT, a graph neural network that predicts survival directly from routine histology images. Trained and cross-validated on TCGA (n=342) and SURGEN (n=336), INSIGHT produces patient-level spatially resolved risk scores. Large independent validation showed superior prognostic performance compared with pTNM staging (C-index 0.68-0.69 vs 0.44-0.58). INSIGHT spatial risk maps recapitulated canonical prognostic histopathology and identified nuclear solidity and circularity as quantitative risk correlates. Integrating spatial risk with data-driven spatial transcriptomic signatures, spatial proteomics, bulk RNA-seq, and single-cell references revealed an epithelium-immune risk manifold capturing epithelial dedifferentiation and fetal programs, myeloid-driven stromal states including $\mathrm{SPP1}^{+}$ macrophages and $\mathrm{LAMP3}^{+}$ dendritic cells, and adaptive immune dysfunction. This analysis exposed patient-specific epithelial heterogeneity, stratification within MSI-High tumours, and high-risk routes of CDX2/HNF4A loss and CEACAM5/6-associated proliferative programs, highlighting coordinated therapeutic vulnerabilities.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [600] [Uncertainty-Aware Flow Field Reconstruction Using SVGP Kolmogorov-Arnold Networks](https://arxiv.org/abs/2512.22426)
*Y. Sungtaek Ju*

Main category: physics.flu-dyn

TL;DR: This paper introduces a machine learning framework (SVGP-KAN) for reconstructing flow fields from sparse measurements with uncertainty quantification, providing accuracy and reliability comparable to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Flow field reconstruction from sparse measurements is essential for analyzing complex thermal-fluid systems but traditional methods lack uncertainty quantification.

Method: The method combines sparse variational Gaussian processes with a Kolmogorov-Arnold network topology (SVGP-KAN), extending classical Linear Stochastic Estimation and Spectral Analysis Modal Methods.

Result: Using pulsed impingement jet flow data, the SVGP-KAN achieves similar accuracy to traditional methods with improved uncertainty calibration across sampling rates.

Conclusion: SVGP-KAN is a robust machine learning framework for flow field reconstruction, offering reliable uncertainty quantification and guiding experimental designs in periodic flows.

Abstract: Reconstructing time-resolved flow fields from temporally sparse velocimetry measurements is critical for characterizing many complex thermal-fluid systems. We introduce a machine learning framework for uncertainty-aware flow reconstruction using sparse variational Gaussian processes in the Kolmogorov-Arnold network topology (SVGP-KAN). This approach extends the classical foundations of Linear Stochastic Estimation (LSE) and Spectral Analysis Modal Methods (SAMM) while enabling principled epistemic uncertainty quantification. We perform a systematic comparison of our framework with the classical reconstruction methods as well as Kalman filtering. Using synthetic data from pulsed impingement jet flows, we assess performance across fractional PIV sampling rates ranging from 0.5% to 10%. Evaluation metrics include reconstruction error, generalization gap, structure preservation, and uncertainty calibration. Our SVGP-KAN methods achieve reconstruction accuracy comparable to established methods, while also providing well-calibrated uncertainty estimates that reliably indicate when and where predictions degrade. The results demonstrate a robust, data-driven framework for flow field reconstruction with meaningful uncertainty quantification and offer practical guidance for experimental design in periodic flows.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [601] [Canonical correlation regression with noisy data](https://arxiv.org/abs/2512.22697)
*Isaac Meza,Rahul Singh*

Main category: econ.EM

TL;DR: The paper studies instrumental variable regression in high-dimensional settings by addressing noisy covariates and instruments and proposes a method using two-stage least squares with spectral regularization.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of estimating a linear model when covariates and instruments are numerous and noisy, and true signals are repetitive but difficult to align.

Method: The method uses two-stage least squares with spectral regularization to learn canonical correlations between covariates and instruments, which are then used in regression.

Result: The paper provides theoretical bounds proving the optimality of the proposed method under noisy data conditions and offers practical guidance on using different types of spectral regularization.

Conclusion: The proposed approach effectively estimates linear models in data-rich and noisy environments, backed by theoretical validation and practical insights.

Abstract: We study instrumental variable regression in data rich environments. The goal is to estimate a linear model from many noisy covariates and many noisy instruments. Our key assumption is that true covariates and true instruments are repetitive, though possibly different in nature; they each reflect a few underlying factors, however those underlying factors may be misaligned. We analyze a family of estimators based on two stage least squares with spectral regularization: canonical correlations between covariates and instruments are learned in the first stage, which are used as regressors in the second stage. As a theoretical contribution, we derive upper and lower bounds on estimation error, proving optimality of the method with noisy data. As a practical contribution, we provide guidance on which types of spectral regularization to use in different regimes.

</details>


### [602] [Causal-Policy Forest for End-to-End Policy Learning](https://arxiv.org/abs/2512.22846)
*Masahiro Kato*

Main category: econ.EM

TL;DR: The study presents a causal-policy forest algorithm for policy learning by modifying the existing causal forest approach to recommend optimal treatments for individuals based on causal inference.


<details>
  <summary>Details</summary>
Motivation: To enhance policy learning in causal inference by bridging the gap between policy learning and conditional average treatment effect (CATE) estimation through a more efficient and practical algorithm.

Method: The proposed algorithm reformulates policy learning as minimizing the mean squared error for CATE under $\{-1,1\}$-restricted regression models and modifies the causal forest approach to train policies in an end-to-end manner.

Result: The causal-policy forest method provides a simpler, more efficient end-to-end approach for training policies while maintaining computational feasibility and leveraging standard decision tree structures.

Conclusion: This algorithm improves policy learning by efficiently bridging theoretical and practical aspects of CATE estimation and policy optimization in causal inference projects.

Abstract: This study proposes an end-to-end algorithm for policy learning in causal inference. We observe data consisting of covariates, treatment assignments, and outcomes, where only the outcome corresponding to the assigned treatment is observed. The goal of policy learning is to train a policy from the observed data, where a policy is a function that recommends an optimal treatment for each individual, to maximize the policy value. In this study, we first show that maximizing the policy value is equivalent to minimizing the mean squared error for the conditional average treatment effect (CATE) under $\{-1, 1\}$ restricted regression models. Based on this finding, we modify the causal forest, an end-to-end CATE estimation algorithm, for policy learning. We refer to our algorithm as the causal-policy forest. Our algorithm has three advantages. First, it is a simple modification of an existing, widely used CATE estimation method, therefore, it helps bridge the gap between policy learning and CATE estimation in practice. Second, while existing studies typically estimate nuisance parameters for policy learning as a separate task, our algorithm trains the policy in a more end-to-end manner. Third, as in standard decision trees and random forests, we train the models efficiently, avoiding computational intractability.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [603] [A Rapid GeoSAM-Based Workflow for Multi-Temporal Glacier Delineation: Case Study from Svalbard](https://arxiv.org/abs/2512.22855)
*Alexandru Hegyi*

Main category: physics.geo-ph

TL;DR: This paper presents a GeoSAM-based semi-automatic approach for glacier boundary mapping, demonstrating accuracy and efficiency across diverse conditions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges of consistent glacier delineation across heterogeneous environments and long time periods, which are difficult using existing methods.

Method: The proposed workflow uses Sentinel-2 imagery and combines late-summer image compositing, spectral index-based candidate identification, GeoSAM-guided segmentation, and physically based post-processing to delineate glacier boundaries.

Result: The method was applied in western Svalbard, providing coherent and consistent glacier outlines. Errors were mainly linked to small features with water bodies, shadows, or high surface variability.

Conclusion: This workflow offers a fast and flexible alternative for glacier mapping, adaptable to other datasets but requiring user inspection for specific adjustments.

Abstract: Consistent glacier boundary delineation is essential for monitoring glacier change, yet many existing approaches are difficult to scale across long time series and heterogeneous environments. In this report, we present a GeoSAM-based, semi-automatic workflow for rapid glacier delineation from Sentinel-2 surface reflectance imagery. The method combines late-summer image compositing, spectral-index-based identification of candidate ice areas, prompt-guided segmentation using GeoSAM, and physically based post-processing to derive annual glacier outlines. The workflow is demonstrated in the Ny-Alesund and Kongsfjorden region of western Svalbard across multiple years of the Sentinel-2 era. Results show that the approach produces spatially coherent and temporally consistent outlines for major glacier bodies, while most errors are associated with small features affected by water bodies, terrain shadows, or high surface variability. The reliance on derived RGB imagery makes the method flexible and transferable to other optical datasets, with improved performance expected at higher spatial resolution. Although user inspection remains necessary to filter incorrect polygons and adjust thresholds for local conditions, the workflow provides a fast and practical alternative for multi-temporal glacier mapping and ice-loss assessment.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [604] [AI tutoring can safely and effectively support students: An exploratory RCT in UK classrooms](https://arxiv.org/abs/2512.23633)
*LearnLM Team,Eedi,:,Albert Wang,Aliya Rysbek,Andrea Huber,Anjali Nambiar,Anna Kenolty,Ben Caulfield,Beth Lilley-Draper,Bibi Groot,Brian Veprek,Chelsea Burdett,Claire Willis,Craig Barton,Digory Smith,George Mu,Harriet Walters,Irina Jurenka,Iris Hulls,James Stalley-Moores,Jonathan Caton,Julia Wilkowski,Kaiz Alarakyia,Kevin R. McKee,Liam McCafferty,Lucy Dalton,Markus Kunesch,Pauline Malubay,Rachel Kidson,Rich Wells,Sam Wheeler,Sara Wiltberger,Shakir Mohamed,Simon Woodhead,Vasco Brazão*

Main category: cs.CY

TL;DR: The paper investigates the use of a generative AI model, LearnLM, for personalized tutoring in mathematics, showing promising results comparable or superior to human tutors.


<details>
  <summary>Details</summary>
Motivation: To explore how generative AI can make personalized one-to-one tutoring scalable and accessible, addressing the prohibitive cost of traditional human tutoring.

Method: An exploratory randomized controlled trial (RCT) was conducted with 165 UK secondary school students. LearnLM was integrated into a tutoring platform, with expert tutors supervising and revising its responses as needed.

Result: LearnLM's responses were approved by tutors with little to no edits in 76.4% of cases. Students using LearnLM performed as well or better, solving novel problems 5.5% more often than those with human tutors alone.

Conclusion: Generative AI models like LearnLM, fine-tuned for pedagogy, show strong potential to provide scalable, effective, and individualized educational support.

Abstract: One-to-one tutoring is widely considered the gold standard for personalized education, yet it remains prohibitively expensive to scale. To evaluate whether generative AI might help expand access to this resource, we conducted an exploratory randomized controlled trial (RCT) with $N = 165$ students across five UK secondary schools. We integrated LearnLM -- a generative AI model fine-tuned for pedagogy -- into chat-based tutoring sessions on the Eedi mathematics platform. In the RCT, expert tutors directly supervised LearnLM, with the remit to revise each message it drafted until they would be satisfied sending it themselves. LearnLM proved to be a reliable source of pedagogical instruction, with supervising tutors approving 76.4% of its drafted messages making zero or minimal edits (i.e., changing only one or two characters). This translated into effective tutoring support: students guided by LearnLM performed at least as well as students chatting with human tutors on each learning outcome we measured. In fact, students who received support from LearnLM were 5.5 percentage points more likely to solve novel problems on subsequent topics (with a success rate of 66.2%) than those who received tutoring from human tutors alone (rate of 60.7%). In interviews, tutors highlighted LearnLM's strength at drafting Socratic questions that encouraged deeper reflection from students, with multiple tutors even reporting that they learned new pedagogical practices from the model. Overall, our results suggest that pedagogically fine-tuned AI tutoring systems may play a promising role in delivering effective, individualized learning support at scale.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [605] [Synthesis of signal processing algorithms with constraints on minimal parallelism and memory space](https://arxiv.org/abs/2512.22676)
*Sergey Salishev*

Main category: eess.SP

TL;DR: This thesis focuses on developing signal-processing algorithms for energy-efficient low-power computing hardware, addressing parallelism and memory constraints.


<details>
  <summary>Details</summary>
Motivation: To improve energy efficiency in low-power computing devices by minimizing memory and parallelism demands.

Method: It introduces a power model for CMOS, integer-friendly methods for function approximations, and optimized FFT and Schur algorithm scheduling techniques.

Result: Constructive theorems and designs for efficient hardware have been developed, improving energy efficiency and computational accuracy.

Conclusion: The work provides valuable design methodologies to achieve efficient specialized accelerators in low-power computing systems.

Abstract: This thesis develops signal-processing algorithms and implementation schemes under constraints of minimal parallelism and memory space, with the goal of improving energy efficiency of low-power computing hardware. We propose (i) a power/energy consumption model for clocked CMOS logic that supports selecting optimal parallelism, (ii) integer-friendly approximation methods for elementary functions that reduce lookup-table size via constrained piecewise-polynomial (quasi-spline) constructions with accuracy guarantees, (iii) provably conflict-free data placement and execution order for mixed-radix streaming FFT on multi-bank and single-port memories, including a self-sorting FFT variant, and (iv) a parallelism/memory analysis of the fast Schur algorithm for superfast Toeplitz system solving, motivated by echo-cancellation workloads. The results provide constructive theorems, schedules, and design trade-offs enabling efficient specialized accelerators.

</details>


### [606] [UniFi: Combining Irregularly Sampled CSI from Diverse Communication Packets and Frequency Bands for Wi-Fi Sensing](https://arxiv.org/abs/2512.22143)
*Gaofeng Dong,Kang Yang,Mani Srivastava*

Main category: eess.SP

TL;DR: UniFi is a novel Wi-Fi sensing framework eliminating intrusive packet injection by exploiting irregular CSI from multiple frequency bands, achieving state-of-the-art performance while preserving communication throughput.


<details>
  <summary>Details</summary>
Motivation: To develop a Wi-Fi sensing system that eliminates communication degradation and poor deployability caused by reliance on high-rate probing packets for CSI extraction.

Method: UniFi introduces a CSI sanitization pipeline to handle heterogeneous packets, a time-aware attention model for learning from non-uniform CSI sequences, and uses irregularly sampled CSI without auxiliary packet injection.

Result: The framework achieved state-of-the-art accuracy on both a new real-world dataset (CommCSI-HAR) and four public benchmarks, with a compact model size while maintaining full communication throughput.

Conclusion: UniFi improves Wi-Fi sensing by removing the need for intrusive packet injection, demonstrating practical deployability and high performance in real-world applications.

Abstract: Existing Wi-Fi sensing systems rely on injecting high-rate probing packets to extract channel state information (CSI), leading to communication degradation and poor deployability. Although Integrated Sensing and Communication (ISAC) is a promising direction, existing solutions still rely on auxiliary packet injection because they exploit only CSI from data frames. We present UniFi, the first Wi-Fi-based ISAC framework that fully eliminates intrusive packet injection by directly exploiting irregularly sampled CSI from diverse communication packets across multiple frequency bands. UniFi integrates a CSI sanitization pipeline to harmonize heterogeneous packets and remove burst-induced redundancy, together with a time-aware attention model that learns directly from non-uniform CSI sequences without resampling. We further introduce CommCSI-HAR, the first dataset with irregularly sampled CSI from real-world dual-band communication traffic. Extensive evaluations on this dataset and four public benchmarks show that UniFi achieves state-of-the-art accuracy with a compact model size, while fully preserving communication throughput.

</details>


### [607] [EEG-to-Voice Decoding of Spoken and Imagined speech Using Non-Invasive EEG](https://arxiv.org/abs/2512.22146)
*Hanbeot Park,Yunjeong Cho,Hunhee Kim*

Main category: eess.SP

TL;DR: This study introduces a method to reconstruct speech directly from EEG signals without requiring temporal alignment, achieving accuracy for both spoken and imagined speech.


<details>
  <summary>Details</summary>
Motivation: The paper addresses challenges in reconstructing speech from EEG signals due to noise, low spatial resolution, and lack of alignment in imagined speech data.

Method: An EEG-to-Voice paradigm was developed, leveraging subject-specific mel-spectrogram generators, pretrained vocoders, and ASR systems. Transfer learning adapted spoken speech models to imagined speech. Linguistic correction was achieved through a language model post-processing step.

Result: The framework showed stable acoustic reconstruction and accurate linguistic decoding in both spoken and imagined speech trials, effectively reducing ASR errors with language model corrections.

Conclusion: Direct EEG-to-Voice speech reconstruction is viable, even for imagined speech, achieving substantial accuracy without temporal alignment techniques.

Abstract: Restoring speech communication from neural signals is a central goal of brain-computer interface research, yet EEG-based speech reconstruction remains challenging due to limited spatial resolution, susceptibility to noise, and the absence of temporally aligned acoustic targets in imagined speech. In this study, we propose an EEG-to-Voice paradigm that directly reconstructs speech from non-invasive EEG signals without dynamic time warping (DTW) or explicit temporal alignment. The proposed pipeline generates mel-spectrograms from EEG in an open-loop manner using a subject-specific generator, followed by pretrained vocoder and automatic speech recognition (ASR) modules to synthesize speech waveforms and decode text. Separate generators were trained for spoken speech and imagined speech, and transfer learning-based domain adaptation was applied by pretraining on spoken speech and adapting to imagined speech. A minimal language model-based correction module was optionally applied to correct limited ASR errors while preserving semantic structure. The framework was evaluated under 2 s and 4 s speech conditions using acoustic-level metrics (PCC, RMSE, MCD) and linguistic-level metrics (CER, WER). Stable acoustic reconstruction and comparable linguistic accuracy were observed for both spoken speech and imagined speech. While acoustic similarity decreased for longer utterances, text-level decoding performance was largely preserved, and word-position analysis revealed a mild increase in decoding errors toward later parts of sentences. The language model-based correction consistently reduced CER and WER without introducing semantic distortion. These results demonstrate the feasibility of direct, open-loop EEG-to-Voice reconstruction for spoken speech and imagined speech without explicit temporal alignment.

</details>


### [608] [Machine Learning-Based Basil Yield Prediction in IoT-Enabled Indoor Vertical Hydroponic Farms](https://arxiv.org/abs/2512.22151)
*Emna Bouzid,Noura Baccar,Kamran Iqbal,Yassine Chaouch,Fares Ben Youssef,Amine Regayeg,Sarra Toumi,Houda Nsir,Amina Mseddi,Leila Costelle*

Main category: eess.SP

TL;DR: The study integrates indoor vertical hydroponics with Machine Learning to optimize basil yield and water efficiency, comparing three ML models for predictive performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of water scarcity in agriculture by implementing resource-efficient, high-yield solutions.

Method: Indoor vertical hydroponics were combined with IoT sensors to collect environmental data, which was used to train and evaluate ML models like Linear Regression, Long Short-Term Memory, and Deep Neural Networks.

Result: LSTM achieved 99% accuracy but was resource-intensive, DNN provided 98% accuracy with better computational efficiency, and LR had fast execution suitable for low-resource scenarios.

Conclusion: DNN is the most balanced solution for high-accuracy, computationally efficient agricultural applications and holds potential for edge-device deployment.

Abstract: As agriculture faces increasing pressure from water scarcity, especially in regions like Tunisia, innovative, resource-efficient solutions are urgently needed. This work explores the integration of indoor vertical hydroponics with Machine Learning (ML) techniques to optimize basil yield while saving water. This research develops a prediction system that uses different ML models and assesses their performance. The models were systematically trained and tested using data collected from IoT sensors of various environmental parameters like CO2, light. The experimental setup features 21 basil crops and uses Raspberry Pi and Arduino. 10k data points were collected and used to train and evaluate three ML models: Linear Regression (LR), Long Short-Term Memory (LSTM), and Deep Neural Networks (DNN). The comparative analysis of the performance of each model revealed that, while LSTM showed high predictive capability and accuracy of 99%, its execution time was 10 times longer than LR and its RAM usage was about 3 times higher than DNN's when simulated on a standard CPU environment. Conversely, the DNN model had an accuracy rate of 98%. This proves an efficient balance between computational speed and prediction quality, which makes this model well-suited for real-life deployment. Moreover, LR excelled in fast processing of basic prediction with an execution time of 11 seconds. This makes the LR model more suitable for low-complexity or resource-limited applications. These performance trade-offs highlight the potential of DNN-based solutions for building responsive, high-accuracy decision-support systems tailored to agricultural environments, making it suitable for future edge-device deployment.

</details>


### [609] [PaperNet: Efficient Temporal Convolutions and Channel Residual Attention for EEG Epilepsy Detection](https://arxiv.org/abs/2512.22172)
*Md Shahriar Sajid,Abhijit Kumar Ghosh,Fariha Nusrat*

Main category: eess.SP

TL;DR: The paper introduces PaperNet, a lightweight hybrid deep learning model for EEG classification, yielding strong accuracy and efficiency on an epilepsy dataset.


<details>
  <summary>Details</summary>
Motivation: To address challenges in EEG signal modeling, including noise, subject variability, and multi-scale dynamics, while ensuring computational efficiency.

Method: Developed a hybrid architecture called PaperNet, combining temporal convolutions, a residual attention module, and lightweight bidirectional recurrent blocks. Evaluated on a public epilepsy dataset using a subject-independent protocol.

Result: Achieved a macro-F1 score of 0.96 on the dataset with 0.6M parameters, maintaining balanced class performance and demonstrating computational efficiency.

Conclusion: PaperNet effectively combines temporal filtering, channel attention, and recurrent modeling, offering both accuracy and efficiency for EEG signals, suitable for resource-constrained systems.

Abstract: Electroencephalography (EEG) signals contain rich temporal-spectral structure but are difficult to model due to noise, subject variability, and multi-scale dynamics. Lightweight deep learning models have shown promise, yet many either rely solely on local convolutions or require heavy recurrent modules. This paper presents PaperNet, a compact hybrid architecture that combines temporal convolutions, a channel-wise residual attention module, and a lightweight bidirectional recurrent block which is used for short-window classification. Using the publicly available BEED: Bangalore EEG Epilepsy Dataset, we evaluate PaperNet under a clearly defined subject-independent training protocol and compare it against established and widely used lightweight baselines. The model achieves a macro-F1 of 0.96 on the held-out test set with approximately 0.6M parameters, while maintaining balanced performance across all four classes. An ablation study demonstrates the contribution of temporal convolutions, residual attention, and recurrent aggregation. Channel-wise attention weights further offer insights into electrode relevance. Computational profiling shows that PaperNet remains efficient enough for practical deployment on resource-constrained systems through out the whole process. These results indicate that carefully combining temporal filtering, channel reweighting, and recurrent context modeling can yield strong EEG classification performance without excessive computational cost.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [610] [Field strength-dependent performance variability in deep learning-based analysis of magnetic resonance imaging](https://arxiv.org/abs/2512.22176)
*Muhammad Ibtsaam Qadir,Duane Schonlau,Ulrike Dydak,Fiona R. Kolbinger*

Main category: eess.IV

TL;DR: This paper investigates how MRI scanner field strength (1.5T vs 3.0T) affects deep learning-based segmentation models across three datasets (breast tumor, pancreas, cervical spine). It finds that field strength can significantly impact segmentation performance.


<details>
  <summary>Details</summary>
Motivation: The study aims to determine how differences in MRI scanner magnetic field strength impact the performance and generalizability of AI-based medical image segmentation models.

Method: The authors used three MRI datasets to train three models (1.5T-only, 3.0T-only, combined 1.5T+3.0T) for each segmentation task. They evaluated these models on separate validation sets and performed radiomic analysis and clustering for field-strength-dependent differences.

Result: For breast tumor and pancreas segmentation, 3.0T-trained models outperformed others. Cervical spine models showed less sensitivity to field strength changes, maintaining high performance across validation sets. Radiomic analysis revealed soft-tissue-related clustering by field strength.

Conclusion: Magnetic field strength significantly affects the performance of AI segmentation models, especially in soft tissues, highlighting the need to account for this variability in AI evaluations on MRI scans.

Abstract: This study quantitatively evaluates the impact of MRI scanner magnetic field strength on the performance and generalizability of deep learning-based segmentation algorithms. Three publicly available MRI datasets (breast tumor, pancreas, and cervical spine) were stratified by scanner field strength (1.5T vs. 3.0T). For each segmentation task, three nnU-Net-based models were developed: A model trained on 1.5T data only (m-1.5T), a model trained on 3.0T data only (m-3.0T), and a model trained on pooled 1.5T and 3.0T data (m-combined). Each model was evaluated on both 1.5T and 3.0T validation sets. Field-strength-dependent performance differences were investigated via Uniform Manifold Approximation and Projection (UMAP)-based clustering and radiomic analysis, including 23 first-order and texture features. For breast tumor segmentation, m-3.0T (DSC: 0.494 [1.5T] and 0.433 [3.0T]) significantly outperformed m-1.5T (DSC: 0.411 [1.5T] and 0.289 [3.0T]) and m-combined (DSC: 0.373 [1.5T] and 0.268[3.0T]) on both validation sets (p<0.0001). Pancreas segmentation showed similar trends: m-3.0T achieved the highest DSC (0.774 [1.5T], 0.840 [3.0T]), while m-1.5T underperformed significantly (p<0.0001). For cervical spine, models performed optimally on same-field validation sets with minimal cross-field performance degradation (DSC>0.92 for all comparisons). Radiomic analysis revealed moderate field-strength-dependent clustering in soft tissues (silhouette scores 0.23-0.29) but minimal separation in osseous structures (0.12). These results indicate that magnetic field strength in the training data substantially influences the performance of deep learning-based segmentation models, particularly for soft-tissue structures (e.g., small lesions). This warrants consideration of magnetic field strength as a confounding factor in studies evaluating AI performance on MRI.

</details>


### [611] [Super-Resolution Enhancement of Medical Images Based on Diffusion Model: An Optimization Scheme for Low-Resolution Gastric Images](https://arxiv.org/abs/2512.22209)
*Haozhe Jia*

Main category: eess.IV

TL;DR: This paper explores enhancing capsule endoscopy images using a diffusion-based super-resolution framework, significantly improving image quality compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Capsule endoscopy suffers from low image resolution due to hardware constraints, restricting the ability to detect subtle textures and pathological features critical for early diagnosis.

Method: The study uses the SR3 framework based on Denoising Diffusion Probabilistic Models to learn the mapping from low-resolution to high-resolution images. The HyperKvasir dataset is employed for training and evaluation.

Result: The diffusion-based approach outperformed bicubic interpolation and GAN methods with quantitative metrics like PSNR of 29.3 dB and SSIM of 0.71, along with qualitative improvements in anatomical structures.

Conclusion: Diffusion-based super-resolution methods show promise in enhancing minimally invasive medical imaging, overcoming resolution limitations, especially for capsule endoscopy.

Abstract: Capsule endoscopy has enabled minimally invasive gastrointestinal imaging, but its clinical utility is limited by the inherently low resolution of captured images due to hardware, power, and transmission constraints. This limitation hampers the identification of fine-grained mucosal textures and subtle pathological features essential for early diagnosis.
  This work investigates a diffusion-based super-resolution framework to enhance capsule endoscopy images in a data-driven and anatomically consistent manner. We adopt the SR3 (Super-Resolution via Repeated Refinement) framework built upon Denoising Diffusion Probabilistic Models (DDPMs) to learn a probabilistic mapping from low-resolution to high-resolution images. Unlike GAN-based approaches that often suffer from training instability and hallucination artifacts, diffusion models provide stable likelihood-based training and improved structural fidelity. The HyperKvasir dataset, a large-scale publicly available gastrointestinal endoscopy dataset, is used for training and evaluation.
  Quantitative results demonstrate that the proposed method significantly outperforms bicubic interpolation and GAN-based super-resolution methods such as ESRGAN, achieving PSNR of 27.5 dB and SSIM of 0.65 for a baseline model, and improving to 29.3 dB and 0.71 with architectural enhancements including attention mechanisms. Qualitative results show improved preservation of anatomical boundaries, vascular patterns, and lesion structures. These findings indicate that diffusion-based super-resolution is a promising approach for enhancing non-invasive medical imaging, particularly in capsule endoscopy where image resolution is fundamentally constrained.

</details>


### [612] [AI-Enhanced Virtual Biopsies for Brain Tumor Diagnosis in Low Resource Settings](https://arxiv.org/abs/2512.22184)
*Areeb Ehsan*

Main category: eess.IV

TL;DR: The paper proposes a virtual biopsy pipeline for brain tumor classification combining a lightweight CNN and handcrafted radiomics features via a late fusion strategy.


<details>
  <summary>Details</summary>
Motivation: To address challenges in brain tumor diagnosis in low-resource environments lacking advanced tools and expertise.

Method: Combines MobileNetV2 CNN for classification, radiomics feature extraction, and a RandomForest classifier utilizing fused representations. Uses Grad-CAM for explainability.

Result: Achieved enhanced validation performance in experiments compared to single-branch methods; robustness shown in challenging low-resource conditions.

Conclusion: The system acts as a decision support tool, improving performance and offering interpretability but not replacing clinical diagnosis or histopathology.

Abstract: Timely brain tumor diagnosis remains challenging in low-resource clinical environments where expert neuroradiology interpretation, high-end MRI hardware, and invasive biopsy procedures may be limited. Although deep learning has achieved strong performance in brain tumor analysis, real-world adoption is constrained by computational demands, dataset shift across scanners, and limited interpretability. This paper presents a prototype virtual biopsy pipeline for four-class classification of 2D brain MRI images using a lightweight convolutional neural network (CNN) and complementary radiomics-style handcrafted features. A MobileNetV2-based CNN is trained for classification, while an interpretable radiomics branch extracts eight features capturing lesion shape, intensity statistics, and gray-level co-occurrence matrix (GLCM) texture descriptors. A late fusion strategy concatenates CNN embeddings with radiomics features and trains a RandomForest classifier on the fused representation. Explainability is provided via Grad-CAM visualizations and radiomics feature importance analysis. Experiments on a public Kaggle brain tumor MRI dataset show improved validation performance for fusion relative to single-branch baselines, while robustness tests under reduced resolution and additive noise highlight sensitivity relevant to low-resource imaging conditions. The system is framed as decision support and not a substitute for clinical diagnosis or histopathology.

</details>


### [613] [Complex Swin Transformer for Accelerating Enhanced SMWI Reconstruction](https://arxiv.org/abs/2512.22202)
*Muhammad Usman,Sung-Min Gho*

Main category: eess.IV

TL;DR: The paper proposes a complex Swin Transformer network for efficient super-resolution reconstruction of multi-echo MRI data to generate high-quality SMWI images from reduced k-space data.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of long scan times in full resolution SMWI acquisition and enable efficient neuroimaging without compromising diagnostic quality.

Method: Developed a complex-valued Swin Transformer network to perform super-resolution reconstruction of low-resolution k-space data, specifically designed for multi-echo MRI applications.

Result: Achieved a structural similarity index of 0.9116 and a mean squared error of 0.076 while maintaining critical diagnostic features in reconstructed SMWI images.

Conclusion: The approach shortens SMWI scan times without sacrificing diagnostic detail, offering potential for practical application in Parkinson’s disease diagnosis and faster neuroimaging workflows.

Abstract: Susceptibility Map Weighted Imaging (SMWI) is an advanced magnetic resonance imaging technique used to detect nigral hyperintensity in Parkinsons disease. However, full resolution SMWI acquisition is limited by long scan times. Efficient reconstruction methods are therefore required to generate high quality SMWI from reduced k space data while preserving diagnostic relevance. In this work, we propose a complex valued Swin Transformer based network for super resolution reconstruction of multi echo MRI data. The proposed method reconstructs high quality SMWI images from low resolution k space inputs. Experimental results demonstrate that the method achieves a structural similarity index of 0.9116 and a mean squared error of 0.076 when reconstructing SMWI from 256 by 256 k space data, while maintaining critical diagnostic features. This approach enables high quality SMWI reconstruction from reduced k space sampling, leading to shorter scan times without compromising diagnostic detail. The proposed method has the potential to improve the clinical applicability of SMWI for Parkinsons disease and support faster and more efficient neuroimaging workflows.

</details>


### [614] [MEGA-PCC: A Mamba-based Efficient Approach for Joint Geometry and Attribute Point Cloud Compression](https://arxiv.org/abs/2512.22463)
*Kai-Hsiang Hsieh,Monyneath Yim,Wen-Hsiao Peng,Jui-Chiu Chiang*

Main category: eess.IV

TL;DR: The paper introduces MEGA-PCC, a learning-based, end-to-end point cloud compression framework that eliminates manual bitrate allocation and recoloring, delivering improved efficiency and effectiveness.


<details>
  <summary>Details</summary>
Motivation: The need for efficient and joint compression of point cloud geometry and attributes while reducing complexity and removing reliance on manual bitrate allocation and recoloring.

Method: MEGA-PCC utilizes a shared encoder and dual decoders for compressing geometry and attributes, alongside a Mamba-based Entropy Model for enhanced entropy coding through spatial and channel-wise correlation modeling.

Result: The proposed method outperforms traditional and learning-based baselines in rate-distortion performance and runtime efficiency.

Conclusion: MEGA-PCC simplifies the compression pipeline, offers superior compression performance, and highlights the potential of AI-driven methods for point cloud compression.

Abstract: Joint compression of point cloud geometry and attributes is essential for efficient 3D data representation. Existing methods often rely on post-hoc recoloring procedures and manually tuned bitrate allocation between geometry and attribute bitstreams in inference, which hinders end-to-end optimization and increases system complexity. To overcome these limitations, we propose MEGA-PCC, a fully end-to-end, learning-based framework featuring two specialized models for joint compression. The main compression model employs a shared encoder that encodes both geometry and attribute information into a unified latent representation, followed by dual decoders that sequentially reconstruct geometry and then attributes. Complementing this, the Mamba-based Entropy Model (MEM) enhances entropy coding by capturing spatial and channel-wise correlations to improve probability estimation. Both models are built on the Mamba architecture to effectively model long-range dependencies and rich contextual features. By eliminating the need for recoloring and heuristic bitrate tuning, MEGA-PCC enables data-driven bitrate allocation during training and simplifies the overall pipeline. Extensive experiments demonstrate that MEGA-PCC achieves superior rate-distortion performance and runtime efficiency compared to both traditional and learning-based baselines, offering a powerful solution for AI-driven point cloud compression.

</details>


### [615] [Semantic contrastive learning for orthogonal X-ray computed tomography reconstruction](https://arxiv.org/abs/2512.22674)
*Jiashu Dong,Jiabing Xiang,Lisheng Geng,Suqing Tian,Wei Zhao*

Main category: eess.IV

TL;DR: This paper introduces a CT reconstruction method using a novel semantic feature contrastive learning loss to enhance reconstruction quality while keeping computation efficient.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve sparse-view CT reconstruction quality and overcome challenges like streak artifacts caused by ill-posed conditions.

Method: The paper proposes a three-stage U-Net-based model using coarse reconstruction, detail refinement, and semantic similarity measurement. It also introduces a semantic feature contrastive learning loss function for better latent space evaluations.

Result: Tests on chest datasets with orthogonal projections showed superior image quality and faster processing compared to existing methods.

Conclusion: The study achieves significant advancements in CT reconstruction quality with low computational complexity, offering a practical and effective solution.

Abstract: X-ray computed tomography (CT) is widely used in medical imaging, with sparse-view reconstruction offering an effective way to reduce radiation dose. However, ill-posed conditions often result in severe streak artifacts. Recent advances in deep learning-based methods have improved reconstruction quality, but challenges still remain. To address these challenges, we propose a novel semantic feature contrastive learning loss function that evaluates semantic similarity in high-level latent spaces and anatomical similarity in shallow latent spaces. Our approach utilizes a three-stage U-Net-based architecture: one for coarse reconstruction, one for detail refinement, and one for semantic similarity measurement. Tests on a chest dataset with orthogonal projections demonstrate that our method achieves superior reconstruction quality and faster processing compared to other algorithms. The results show significant improvements in image quality while maintaining low computational complexity, making it a practical solution for orthogonal CT reconstruction.

</details>


### [616] [SwinCCIR: An end-to-end deep network for Compton camera imaging reconstruction](https://arxiv.org/abs/2512.22766)
*Minghao Dong,Xinyang Luo,Xujian Ouyang,Yongshun Xiao*

Main category: eess.IV

TL;DR: The paper proposes SwinCCIR, an end-to-end deep learning framework for improving Compton camera imaging by overcoming reconstruction artifacts and errors.


<details>
  <summary>Details</summary>
Motivation: Existing Compton camera imaging methods suffer from severe artifacts, deformation due to back-projection reconstruction principles, and systematic errors that degrade imaging quality. Current iterative and deep learning methods still rely on back-projection optimization, which limits progress.

Method: The authors proposed SwinCCIR, a deep learning framework using Swin-transformer blocks and a transposed convolution-based image generation module to directly map list-mode events to radioactive source distributions, eliminating dependency on back-projection.

Result: SwinCCIR was trained and validated on simulated and practical datasets. Results show the framework significantly improves imaging quality, resolving the issues with conventional Compton camera imaging methods.

Conclusion: The study demonstrates that SwinCCIR effectively improves Compton camera imaging performance and has potential for practical applications in gamma imaging.

Abstract: Compton cameras (CCs) are a kind of gamma cameras which are designed to determine the directions of incident gammas based on the Compton scatter. However, the reconstruction of CCs face problems of severe artifacts and deformation due to the fundamental reconstruction principle of back-projection of Compton cones. Besides, a part of systematic errors originated from the performance of devices are hard to remove through calibration, leading to deterioration of imaging quality. Iterative algorithms and deep-learning based methods have been widely used to improve reconstruction. But most of them are optimization based on the results of back-projection. Therefore, we proposed an end-to-end deep learning framework, SwinCCIR, for CC imaging. Through adopting swin-transformer blocks and a transposed convolution-based image generation module, we established the relationship between the list-mode events and the radioactive source distribution. SwinCCIR was trained and validated on both simulated and practical dataset. The experimental results indicate that SwinCCIR effectively overcomes problems of conventional CC imaging, which are expected to be implemented in practical applications.

</details>


### [617] [EIR: Enhanced Image Representations for Medical Report Generation](https://arxiv.org/abs/2512.23185)
*Qiang Sun,Zongcheng Ji,Yinlong Xiao,Peng Chang,Jun Yu*

Main category: eess.IV

TL;DR: This paper proposes an approach called Enhanced Image Representations (EIR) for automatically generating accurate chest X-ray reports.


<details>
  <summary>Details</summary>
Motivation: To reduce the workload on radiologists and decrease potential misdiagnoses by automating chest X-ray report generation with a focus on eliminating information asymmetry and bridging domain gaps.

Method: The authors propose Enhanced Image Representations (EIR), which employs cross-modal transformers to integrate metadata with image representations and uses medical domain pre-trained models for encoding medical images, addressing domain gaps and asymmetry issues.

Result: Experimental results on MIMIC and Open-I datasets confirm the efficacy of the proposed method in generating accurate medical reports.

Conclusion: The EIR approach effectively addresses existing challenges in automated chest X-ray report generation by resolving information asymmetry and domain gaps, leading to enhanced accuracy and practical utility.

Abstract: Generating medical reports from chest X-ray images is a critical and time-consuming task for radiologists, especially in emergencies. To alleviate the stress on radiologists and reduce the risk of misdiagnosis, numerous research efforts have been dedicated to automatic medical report generation in recent years. Most recent studies have developed methods that represent images by utilizing various medical metadata, such as the clinical document history of the current patient and the medical graphs constructed from retrieved reports of other similar patients. However, all existing methods integrate additional metadata representations with visual representations through a simple "Add and LayerNorm" operation, which suffers from the information asymmetry problem due to the distinct distributions between them. In addition, chest X-ray images are usually represented using pre-trained models based on natural domain images, which exhibit an obvious domain gap between general and medical domain images. To this end, we propose a novel approach called Enhanced Image Representations (EIR) for generating accurate chest X-ray reports. We utilize cross-modal transformers to fuse metadata representations with image representations, thereby effectively addressing the information asymmetry problem between them, and we leverage medical domain pre-trained models to encode medical images, effectively bridging the domain gap for image representation. Experimental results on the widely used MIMIC and Open-I datasets demonstrate the effectiveness of our proposed method.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [618] [Practical challenges of control monitoring in frontier AI deployments](https://arxiv.org/abs/2512.22154)
*David Lindner,Charlie Griffin,Tomek Korbak,Roland S. Zimmermann,Geoffrey Irving,Sebastian Farquhar,Alan Cooney*

Main category: cs.CR

TL;DR: The paper explores how to improve automated control monitors for overseeing advanced AI in real-world settings, addressing challenges like oversight latency, incremental attacks, and identifying harmful agents.


<details>
  <summary>Details</summary>
Motivation: To develop reliable monitoring protocols for advanced AI systems that can't be fully trusted in real-world deployments.

Method: The authors analyze three monitoring protocols (synchronous, semi-synchronous, asynchronous), sketch a safety framework, and investigate their performance through case studies.

Result: Identification of key challenges (oversight, latency, recovery) and insights into monitoring design trade-offs for future AI systems.

Conclusion: Automated control monitoring can address safety challenges posed by advanced AI agents through appropriate protocol designs balancing latency and reliability.

Abstract: Automated control monitors could play an important role in overseeing highly capable AI agents that we do not fully trust. Prior work has explored control monitoring in simplified settings, but scaling monitoring to real-world deployments introduces additional dynamics: parallel agent instances, non-negligible oversight latency, incremental attacks between agent instances, and the difficulty of identifying scheming agents based on individual harmful actions. In this paper, we analyse design choices to address these challenges, focusing on three forms of monitoring with different latency-safety trade-offs: synchronous, semi-synchronous, and asynchronous monitoring. We introduce a high-level safety case sketch as a tool for understanding and comparing these monitoring protocols. Our analysis identifies three challenges -- oversight, latency, and recovery -- and explores them in four case studies of possible future AI deployments.

</details>


### [619] [Beyond Single Bugs: Benchmarking Large Language Models for Multi-Vulnerability Detection](https://arxiv.org/abs/2512.22306)
*Chinmay Pushkar,Sanchit Kabra,Dhruv Kumar,Jagat Sesh Challa*

Main category: cs.CR

TL;DR: The paper introduces a new benchmark for detecting multiple vulnerabilities in large programming files, addressing biases in current methods, and evaluates various LLMs across different programming languages.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks focus only on isolated vulnerabilities or function-level evaluations, ignoring the complexity of real-world software. The study aims to address count and selection biases in multi-vulnerability detection.

Method: The researchers develop a dataset of 40,000 files with controlled vulnerability counts (1, 3, 5, 9) injected into long-context code samples, and evaluate five state-of-the-art LLMs including GPT-4o-mini and Llama-3.3-70B.

Result: Performance degrades sharply as vulnerability density increases. For instance, Llama-3.3-70B performs well on single vulnerabilities in C but faces up to 40% performance drops in high-density scenarios, with Python and JavaScript showing unique issues such as 'under-counting.'

Conclusion: LLMs need improvement for robust multi-vulnerability detection in large and diverse code samples, especially to handle high vulnerability density and complex Python/JavaScript cases.

Abstract: Large Language Models (LLMs) have demonstrated significant potential in automated software security, particularly in vulnerability detection. However, existing benchmarks primarily focus on isolated, single-vulnerability samples or function-level classification, failing to reflect the complexity of real-world software where multiple interacting vulnerabilities often coexist within large files. Recent studies indicate that LLMs suffer from "count bias" and "selection bias" in multi-label tasks, yet this has not been rigorously quantified in the domain of code security. In this work, we introduce a comprehensive benchmark for Multi-Vulnerability Detection across four major languages: C, C++, Python, and JavaScript. We construct a dataset of 40,000 files by systematically injecting controlled counts of vulnerabilities (1, 3, 5, and 9) into long-context code samples (7.5k-10k tokens) sourced from CodeParrot. We evaluate five state-of-the-art LLMs, including GPT-4o-mini, Llama-3.3-70B, and the Qwen-2.5 series. Our results reveal a sharp degradation in performance as vulnerability density increases. While Llama-3.3-70B achieves near-perfect F1 scores (approximately 0.97) on single-vulnerability C tasks, performance drops by up to 40% in high-density settings. Notably, Python and JavaScript show distinct failure modes compared to C/C++, with models exhibiting severe "under-counting" (Recall dropping to less than 0.30) in complex Python files.

</details>


### [620] [LLA: Enhancing Security and Privacy for Generative Models with Logic-Locked Accelerators](https://arxiv.org/abs/2512.22307)
*You Li,Guannan Zhao,Yuhao Ju,Yunqi He,Jie Gu,Hai Zhou*

Main category: cs.CR

TL;DR: The paper introduces LLA, a hardware-software co-designed approach, to protect generative AI models from threats such as theft and corruption, using key-based protections and lightweight hardware locking.


<details>
  <summary>Details</summary>
Motivation: To address increasing risks of intellectual property theft and model corruption in generative AI due to supply chain vulnerabilities.

Method: The LLA scheme embeds secret key bits into neurons for model locking and transforms these keys for additional security. It also incorporates a lightweight locking module into AI accelerators, ensuring compatibility with AI toolchains and dataflows.

Result: LLA effectively resists oracle-guided key optimization attacks and achieves high security with negligible computational overhead (less than 0.1% for 7,168 key bits).

Conclusion: LLA is a robust and efficient intellectual property protection mechanism that combines hardware and software to safeguard generative AI models against diverse threats while maintaining system performance.

Abstract: We introduce LLA, an effective intellectual property (IP) protection scheme for generative AI models. LLA leverages the synergy between hardware and software to defend against various supply chain threats, including model theft, model corruption, and information leakage. On the software side, it embeds key bits into neurons that can trigger outliers to degrade performance and applies invariance transformations to obscure the key values. On the hardware side, it integrates a lightweight locking module into the AI accelerator while maintaining compatibility with various dataflow patterns and toolchains. An accelerator with a pre-stored secret key acts as a license to access the model services provided by the IP owner. The evaluation results show that LLA can withstand a broad range of oracle-guided key optimization attacks, while incurring a minimal computational overhead of less than 0.1% for 7,168 key bits.

</details>


### [621] [Agentic AI for Cyber Resilience: A New Security Paradigm and Its System-Theoretic Foundations](https://arxiv.org/abs/2512.22883)
*Tao Li,Quanyan Zhu*

Main category: cs.CR

TL;DR: The abstract discusses shifting cybersecurity to focus on resilience, utilizing AI-driven agentic workflows for adaptive protection and recovery, and highlights a framework for designing such systems.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional static and human-centric cybersecurity systems by leveraging foundation-model-based AI for dynamic, resilient, and autonomous responses.

Method: An agentic system-level framework was developed, introducing a general agentic architecture and using game-theoretic formulations to design adaptive attacker/defender workflows. Case studies highlight practical applications.

Result: Proposed a paradigm where AI autonomous agents integrate in cybersecurity workflows for enhanced resiliency. Demonstrated through case studies like automated penetration testing and cyber deception.

Conclusion: Cybersecurity must evolve to embrace agentic AI systems that ensure not just prevention but adaptability, continuous learning, and efficient recovery from cyber threats.

Abstract: Cybersecurity is being fundamentally reshaped by foundation-model-based artificial intelligence. Large language models now enable autonomous planning, tool orchestration, and strategic adaptation at scale, challenging security architectures built on static rules, perimeter defenses, and human-centered workflows. This chapter argues for a shift from prevention-centric security toward agentic cyber resilience. Rather than seeking perfect protection, resilient systems must anticipate disruption, maintain critical functions under attack, recover efficiently, and learn continuously. We situate this shift within the historical evolution of cybersecurity paradigms, culminating in an AI-augmented paradigm where autonomous agents participate directly in sensing, reasoning, action, and adaptation across cyber and cyber-physical systems. We then develop a system-level framework for designing agentic AI workflows. A general agentic architecture is introduced, and attacker and defender workflows are analyzed as coupled adaptive processes, and game-theoretic formulations are shown to provide a unifying design language for autonomy allocation, information flow, and temporal composition. Case studies in automated penetration testing, remediation, and cyber deception illustrate how equilibrium-based design enables system-level resiliency design.

</details>


### [622] [DECEPTICON: How Dark Patterns Manipulate Web Agents](https://arxiv.org/abs/2512.22894)
*Phil Cuvin,Hao Zhu,Diyi Yang*

Main category: cs.CR

TL;DR: Dark patterns, deceptive UI designs, manipulate web agents extensively. They successfully alter agent trajectories towards malicious outcomes in 70% of tasks, and defenses prove insufficient.


<details>
  <summary>Details</summary>
Motivation: The motivation is to evaluate and address the risk posed by dark patterns (deceptive designs) to the effectiveness and safety of web agents.

Method: The authors introduce DECEPTICON, a testing environment with 700 tasks (600 generated and 100 real-world) to isolate and study dark patterns' effects on instruction-following and manipulation.

Result: Dark patterns manipulated state-of-the-art web agents over 70% of the time, with the manipulation more pronounced in larger, more capable models.

Conclusion: Dark patterns present a significant, unaddressed risk to web agents' robustness, requiring the urgent development of robust defenses.

Abstract: Deceptive UI designs, widely instantiated across the web and commonly known as dark patterns, manipulate users into performing actions misaligned with their goals. In this paper, we show that dark patterns are highly effective in steering agent trajectories, posing a significant risk to agent robustness. To quantify this risk, we introduce DECEPTICON, an environment for testing individual dark patterns in isolation. DECEPTICON includes 700 web navigation tasks with dark patterns -- 600 generated tasks and 100 real-world tasks, designed to measure instruction-following success and dark pattern effectiveness. Across state-of-the-art agents, we find dark patterns successfully steer agent trajectories towards malicious outcomes in over 70% of tested generated and real-world tasks -- compared to a human average of 31%. Moreover, we find that dark pattern effectiveness correlates positively with model size and test-time reasoning, making larger, more capable models more susceptible. Leading countermeasures against adversarial attacks, including in-context prompting and guardrail models, fail to consistently reduce the success rate of dark pattern interventions. Our findings reveal dark patterns as a latent and unmitigated risk to web agents, highlighting the urgent need for robust defenses against manipulative designs.

</details>


### [623] [EquaCode: A Multi-Strategy Jailbreak Approach for Large Language Models via Equation Solving and Code Completion](https://arxiv.org/abs/2512.23173)
*Zhen Liang,Hai Huang,Zhengkui Chen*

Main category: cs.CR

TL;DR: Equacode introduces a multi-strategy jailbreak method for LLMs, converting malicious requests into mathematical problems solved via coding.


<details>
  <summary>Details</summary>
Motivation: The paper addresses LLM vulnerabilities to jailbreak attacks, aiming to improve attack strategies for better robustness assessment.

Method: The proposed method transforms malicious intent into mathematical problems and uses equation-solving and code completion to bypass safety constraints.

Result: Equacode achieved 91.19% success on GPT models and 98.65% success across 3 state-of-the-art LLMs in single-query attacks.

Conclusion: The multi-strategy Equacode technique is more effective compared to single-strategy modules, showing synergy between its components.

Abstract: Large language models (LLMs), such as ChatGPT, have achieved remarkable success across a wide range of fields. However, their trustworthiness remains a significant concern, as they are still susceptible to jailbreak attacks aimed at eliciting inappropriate or harmful responses. However, existing jailbreak attacks mainly operate at the natural language level and rely on a single attack strategy, limiting their effectiveness in comprehensively assessing LLM robustness. In this paper, we propose Equacode, a novel multi-strategy jailbreak approach for large language models via equation-solving and code completion. This approach transforms malicious intent into a mathematical problem and then requires the LLM to solve it using code, leveraging the complexity of cross-domain tasks to divert the model's focus toward task completion rather than safety constraints. Experimental results show that Equacode achieves an average success rate of 91.19% on the GPT series and 98.65% across 3 state-of-the-art LLMs, all with only a single query. Further, ablation experiments demonstrate that EquaCode outperforms either the mathematical equation module or the code module alone. This suggests a strong synergistic effect, thereby demonstrating that multi-strategy approach yields results greater than the sum of its parts.

</details>


### [624] [Adaptive Trust Consensus for Blockchain IoT: Comparing RL, DRL, and MARL Against Naive, Collusive, Adaptive, Byzantine, and Sleeper Attacks](https://arxiv.org/abs/2512.22860)
*Soham Padia,Dhananjay Vaidya,Ramchandra Mangrulkar*

Main category: cs.CR

TL;DR: This paper presents a trust-based delegated consensus framework for blockchain IoT networks, integrating Fully Homomorphic Encryption and Attribute-Based Access Control alongside learning-based defense mechanisms to counter adversarial attacks. It compares three reinforcement learning strategies against five attack types, highlighting varied effectiveness.


<details>
  <summary>Details</summary>
Motivation: The increasing sophistication of adversarial attacks in blockchain-enabled IoT networks demands robust defenses to secure decentralized systems.

Method: The paper integrates Fully Homomorphic Encryption and Attribute-Based Access Control with privacy-preserving mechanisms. It evaluates multiple reinforcement learning methods—tabular Q-learning, Deep RL, and Multi-Agent RL—against distinct adversarial attacks in a simulated IoT setting.

Result: Experimental results show that Multi-Agent RL outperforms other methods in detecting certain attacks (e.g., collusive attacks with an F1 score of 0.85). However, all methods failed against Time-Delayed Poisoning attacks, revealing critical vulnerabilities.

Conclusion: Multi-Agent RL demonstrates significant advantages in combating sophisticated attacks; however, new strategies are needed to address catastrophic vulnerabilities like Time-Delayed Poisoning in blockchain IoT environments.

Abstract: Securing blockchain-enabled IoT networks against sophisticated adversarial attacks remains a critical challenge. This paper presents a trust-based delegated consensus framework integrating Fully Homomorphic Encryption (FHE) with Attribute-Based Access Control (ABAC) for privacy-preserving policy evaluation, combined with learning-based defense mechanisms. We systematically compare three reinforcement learning approaches -- tabular Q-learning (RL), Deep RL with Dueling Double DQN (DRL), and Multi-Agent RL (MARL) -- against five distinct attack families: Naive Malicious Attack (NMA), Collusive Rumor Attack (CRA), Adaptive Adversarial Attack (AAA), Byzantine Fault Injection (BFI), and Time-Delayed Poisoning (TDP). Experimental results on a 16-node simulated IoT network reveal significant performance variations: MARL achieves superior detection under collusive attacks (F1=0.85 vs. DRL's 0.68 and RL's 0.50), while DRL and MARL both attain perfect detection (F1=1.00) against adaptive attacks where RL fails (F1=0.50). All agents successfully defend against Byzantine attacks (F1=1.00). Most critically, the Time-Delayed Poisoning attack proves catastrophic for all agents, with F1 scores dropping to 0.11-0.16 after sleeper activation, demonstrating the severe threat posed by trust-building adversaries. Our findings indicate that coordinated multi-agent learning provides measurable advantages for defending against sophisticated trust manipulation attacks in blockchain IoT environments.

</details>


### [625] [Multi-Agent Framework for Threat Mitigation and Resilience in AI-Based Systems](https://arxiv.org/abs/2512.23132)
*Armstrong Foundjem,Lionel Nganyewou Tidjon,Leuson Da Silva,Foutse Khomh*

Main category: cs.CR

TL;DR: This paper evaluates machine learning (ML) security risks by extracting and analyzing threats. Key insights include vulnerabilities like jailbreaking, model stealing, and preference-based attacks, especially in foundation and multimodal systems.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of ML-specific threat modeling in cybersecurity for foundation models and related systems, which are increasingly targets of various sophisticated attacks.

Method: The authors examine datasets like MITRE ATLAS, AI Incident Database, and literature to extract threats, analyze 854 repositories, and use a multi-agent system for threat graph assembly.

Result: They identify numerous unreported threats, such as commercial LLM API model stealing and federated backdoors, while uncovering concentrated vulnerability hotspots in libraries.

Conclusion: An adaptive ML security framework is proposed to tackle risks, combining dependency management, threat intelligence, and monitoring for secure ML lifecycle management.

Abstract: Machine learning (ML) underpins foundation models in finance, healthcare, and critical infrastructure, making them targets for data poisoning, model extraction, prompt injection, automated jailbreaking, and preference-guided black-box attacks that exploit model comparisons. Larger models can be more vulnerable to introspection-driven jailbreaks and cross-modal manipulation. Traditional cybersecurity lacks ML-specific threat modeling for foundation, multimodal, and RAG systems. Objective: Characterize ML security risks by identifying dominant TTPs, vulnerabilities, and targeted lifecycle stages. Methods: We extract 93 threats from MITRE ATLAS (26), AI Incident Database (12), and literature (55), and analyze 854 GitHub/Python repositories. A multi-agent RAG system (ChatGPT-4o, temp 0.4) mines 300+ articles to build an ontology-driven threat graph linking TTPs, vulnerabilities, and stages. Results: We identify unreported threats including commercial LLM API model stealing, parameter memorization leakage, and preference-guided text-only jailbreaks. Dominant TTPs include MASTERKEY-style jailbreaking, federated poisoning, diffusion backdoors, and preference optimization leakage, mainly impacting pre-training and inference. Graph analysis reveals dense vulnerability clusters in libraries with poor patch propagation. Conclusion: Adaptive, ML-specific security frameworks, combining dependency hygiene, threat intelligence, and monitoring, are essential to mitigate supply-chain and inference risks across the ML lifecycle.

</details>


### [626] [Agentic AI for Autonomous Defense in Software Supply Chain Security: Beyond Provenance to Vulnerability Mitigation](https://arxiv.org/abs/2512.23480)
*Toqeer Ali Syed,Mohammad Riyaz Belgaum,Salman Jan,Asadullah Abdullah Khan,Saad Said Alqahtani*

Main category: cs.CR

TL;DR: This paper presents an agentic AI system integrating LLMs, RL, and multi-agent coordination for securing software supply chains, tested with real-world CI/CD pipelines.


<details>
  <summary>Details</summary>
Motivation: To address the growing risk of software supply chain attacks, which are bypassing conventional integrity mechanisms and focus on trusted software development and delivery procedures.

Method: The system utilizes LLMs for reasoning, RL for adaptive security measures, multi-agent coordination, and a blockchain ledger for logging. Real CI/CD tools like GitHub Actions and Jenkins were tested with simulated attacks.

Result: The experimental results showed improved accuracy, reduced mitigation time, and acceptable overhead compared to other baselines, indicating strong performance.

Conclusion: Agentic AI for software supply chains can transition the industry to proactive, self-defending systems instead of reactive approaches reliant on verification methods.

Abstract: The software supply chain attacks are becoming more and more focused on trusted development and delivery procedures, so the conventional post-build integrity mechanisms cannot be used anymore. The available frameworks like SLSA, SBOM and in toto are majorly used to offer provenance and traceability but do not have the capabilities of actively identifying and removing vulnerabilities in software production. The current paper includes an example of agentic artificial intelligence (AI) based on autonomous software supply chain security that combines large language model (LLM)-based reasoning, reinforcement learning (RL), and multi-agent coordination. The suggested system utilizes specialized security agents coordinated with the help of LangChain and LangGraph, communicates with actual CI/CD environments with the Model Context Protocol (MCP), and documents all the observations and actions in a blockchain security ledger to ensure integrity and auditing. Reinforcement learning can be used to achieve adaptive mitigation strategies that consider the balance between security effectiveness and the operational overhead, and LLMs can be used to achieve semantic vulnerability analysis, as well as explainable decisions. This framework is tested based on simulated pipelines, as well as, actual world CI/CD integrations on GitHub Actions and Jenkins, including injection attacks, insecure deserialization, access control violations, and configuration errors. Experimental outcomes indicate better detection accuracy, shorter mitigation latency and reasonable build-time overhead than rule-based, provenance only and RL only baselines. These results show that agentic AI can facilitate the transition to self defending, proactive software supply chains rather than reactive verification ones.

</details>


### [627] [Certifying the Right to Be Forgotten: Primal-Dual Optimization for Sample and Label Unlearning in Vertical Federated Learning](https://arxiv.org/abs/2512.23171)
*Yu Jiang,Xindi Tong,Ziyao Liu,Xiaoxi Zhang,Kwok-Yan Lam,Chee Wei Tan*

Main category: cs.CR

TL;DR: This paper addresses challenges in removing sensitive data influences from Vertical Federated Learning (VFL), proposing FedORA for effective and efficient unlearning.


<details>
  <summary>Details</summary>
Motivation: The need to address privacy concerns and comply with the "right to be forgotten" by enabling AI models to remove sensitive data influences.

Method: FedORA is proposed, leveraging a primal-dual algorithm for unlearning sensitive data through a new loss function, adaptive step sizes, and asymmetric batch handling.

Result: FedORA matches Train-from-scratch in effectiveness and utility preservation, while reducing computational and communication costs in experiments.

Conclusion: FedORA addresses privacy concerns in VFL, offering an effective and computationally efficient solution to unlearning sensitive data.

Abstract: Federated unlearning has become an attractive approach to address privacy concerns in collaborative machine learning, for situations when sensitive data is remembered by AI models during the machine learning process. It enables the removal of specific data influences from trained models, aligning with the growing emphasis on the "right to be forgotten." While extensively studied in horizontal federated learning, unlearning in vertical federated learning (VFL) remains challenging due to the distributed feature architecture. VFL unlearning includes sample unlearning that removes specific data points' influence and label unlearning that removes entire classes. Since different parties hold complementary features of the same samples, unlearning tasks require cross-party coordination, creating computational overhead and complexities from feature interdependencies. To address such challenges, we propose FedORA (Federated Optimization for data Removal via primal-dual Algorithm), designed for sample and label unlearning in VFL. FedORA formulates the removal of certain samples or labels as a constrained optimization problem solved using a primal-dual framework. Our approach introduces a new unlearning loss function that promotes classification uncertainty rather than misclassification. An adaptive step size enhances stability, while an asymmetric batch design, considering the prior influence of the remaining data on the model, handles unlearning and retained data differently to efficiently reduce computational costs. We provide theoretical analysis proving that the model difference between FedORA and Train-from-scratch is bounded, establishing guarantees for unlearning effectiveness. Experiments on tabular and image datasets demonstrate that FedORA achieves unlearning effectiveness and utility preservation comparable to Train-from-scratch with reduced computation and communication overhead.

</details>


### [628] [Toward Trustworthy Agentic AI: A Multimodal Framework for Preventing Prompt Injection Attacks](https://arxiv.org/abs/2512.23557)
*Toqeer Ali Syed,Mishal Ateeq Almutairi,Mahmoud Abdel Moaty*

Main category: cs.CR

TL;DR: The paper proposes a defense framework to combat multimodal prompt injection (PI) attacks within agentic AI systems using sanitization and provenance tracking.


<details>
  <summary>Details</summary>
Motivation: With the growing use of autonomous AI systems that rely on agentic networks like LangChain and GraphChain, there is an increased risk of malicious instruction propagation which undermines system stability and policy compliance.

Method: The paper introduces a framework utilizing text sanitizer agents, visual sanitizer agents, and output validator agents, coordinated by a provenance ledger to ensure trust and verify prompts and outputs across the agent network.

Result: Experimental results demonstrate improved multimodal injection detection accuracy, reduced trust leakage across agents, and enhanced stability of execution pathways.

Conclusion: The proposed framework ensures secure, reliable, and transparent agent-to-agent communication by leveraging provenance tracking to mitigate instruction injection risks in AI systems.

Abstract: Powerful autonomous systems, which reason, plan, and converse using and between numerous tools and agents, are made possible by Large Language Models (LLMs), Vision-Language Models (VLMs), and new agentic AI systems, like LangChain and GraphChain. Nevertheless, this agentic environment increases the probability of the occurrence of multimodal prompt injection (PI) attacks, in which concealed or malicious instructions carried in text, pictures, metadata, or agent-to-agent messages may spread throughout the graph and lead to unintended behavior, a breach of policy, or corruption of state. In order to mitigate these risks, this paper suggests a Cross-Agent Multimodal Provenanc- Aware Defense Framework whereby all the prompts, either user-generated or produced by upstream agents, are sanitized and all the outputs generated by an LLM are verified independently before being sent to downstream nodes. This framework contains a Text sanitizer agent, visual sanitizer agent, and output validator agent all coordinated by a provenance ledger, which keeps metadata of modality, source, and trust level throughout the entire agent network. This architecture makes sure that agent-to-agent communication abides by clear trust frames such such that injected instructions are not propagated down LangChain or GraphChain-style-workflows. The experimental assessments show that multimodal injection detection accuracy is significantly enhanced, and the cross-agent trust leakage is minimized, as well as, agentic execution pathways become stable. The framework, which expands the concept of provenance tracking and validation to the multi-agent orchestration, enhances the establishment of secure, understandable and reliable agentic AI systems.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [629] [A Domain Decomposition-based Solver for Acoustic Wave propagation in Two-Dimensional Random Media](https://arxiv.org/abs/2512.23027)
*Sudhi Sharma Padillath Vasudevan*

Main category: cs.CE

TL;DR: The paper uses a stochastic Galerkin approach combined with domain decomposition to address computational challenges in solving acoustic wave propagation problems.


<details>
  <summary>Details</summary>
Motivation: To efficiently solve the computationally intensive problem of acoustic wave propagation with random field approximations for wave speed.

Method: The wave propagation problem is solved using a sampling-free intrusive stochastic Galerkin approach with polynomial chaos expansions, coupled with domain decomposition-based solvers and a conjugate gradient iterative solver with two-level Neumann-Neumann preconditioner.

Result: Efficient scalability and computational efficiency using the proposed approach.

Conclusion: The proposed stochastic approach successfully handles the computational intensity of wave propagation problems, especially with large-scale parameters.

Abstract: An acoustic wave propagation problem with a log normal random field approximation for wave speed is solved using a sampling-free intrusive stochastic Galerkin approach. The stochastic partial differential equation with the inputs and outputs expanded using polynomial chaos expansion (PCE) is transformed into a set of deterministic PDEs and further to a system of linear equations. Domain decomposition (DD)-based solvers are utilized to handle the overwhelming computational cost for the resulting system with increasing mesh size, time step and number of random parameters. A conjugate gradient iterative solver with a two-level Neumann-Neumann preconditioner is applied here showing their efficient scalabilities.

</details>


<div id='q-fin.GN'></div>

# q-fin.GN [[Back]](#toc)

### [630] [Deep Learning for Art Market Valuation](https://arxiv.org/abs/2512.23078)
*Jianping Mei,Michael Moses,Jan Waelty,Yucheng Yang*

Main category: q-fin.GN

TL;DR: The paper analyzes how deep learning and multi-modal models can improve artwork price predictions by using visual and historical art data, particularly effective for new art sales.


<details>
  <summary>Details</summary>
Motivation: The motivation is to advance the valuation process in the art market by utilizing deep learning techniques that incorporate visual and historical data for better predictive accuracy, especially where conventional methods face challenges.

Method: The paper uses a large dataset of repeated-sales from auction houses to compare traditional hedonic regression models and tree-based methods against deep learning architectures. Multi-modal models integrating tabular and image data were a key focus, with interpretability analyses like Grad-CAM applied.

Result: Visual embeddings significantly enhance valuation predictions, particularly for fresh-to-market artworks. Artist identity and transaction history dominate prediction, but deep learning models provide additional meaningful insights.

Conclusion: Multi-modal deep learning models improve price predictions in art valuation, especially for first-time sales, offering new academic and practical perspectives.

Abstract: We study how deep learning can improve valuation in the art market by incorporating the visual content of artworks into predictive models. Using a large repeated-sales dataset from major auction houses, we benchmark classical hedonic regressions and tree-based methods against modern deep architectures, including multi-modal models that fuse tabular and image data. We find that while artist identity and prior transaction history dominate overall predictive power, visual embeddings provide a distinct and economically meaningful contribution for fresh-to-market works where historical anchors are absent. Interpretability analyses using Grad-CAM and embedding visualizations show that models attend to compositional and stylistic cues. Our findings demonstrate that multi-modal deep learning delivers significant value precisely when valuation is hardest, namely first-time sales, and thus offers new insights for both academic research and practice in art market valuation.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [631] [Why Machine Learning Models Systematically Underestimate Extreme Values II: How to Fix It with LatentNN](https://arxiv.org/abs/2512.23138)
*Yuan-Sen Ting*

Main category: astro-ph.IM

TL;DR: This paper addresses the problem of attenuation bias in neural networks caused by measurement errors in input variables and introduces a solution called LatentNN.


<details>
  <summary>Details</summary>
Motivation: To mitigate regression coefficient underestimation (attenuation bias) in neural networks caused by measurement error, which is common in astronomical data.

Method: The authors propose LatentNN, a method that estimates true input values as latent variables alongside neural network parameters by optimizing the joint likelihood of observed inputs and outputs.

Result: LatentNN effectively reduces attenuation bias across different scenarios, including low signal-to-noise ratios, correlated input features, and real-world stellar spectroscopy data.

Conclusion: LatentNN enhances neural network inference under low signal-to-noise environments typical of astronomical data and is most effective when measurement errors are less than half the intrinsic data range.

Abstract: Attenuation bias -- the systematic underestimation of regression coefficients due to measurement errors in input variables -- affects astronomical data-driven models. For linear regression, this problem was solved by treating the true input values as latent variables to be estimated alongside model parameters. In this paper, we show that neural networks suffer from the same attenuation bias and that the latent variable solution generalizes directly to neural networks. We introduce LatentNN, a method that jointly optimizes network parameters and latent input values by maximizing the joint likelihood of observing both inputs and outputs. We demonstrate the correction on one-dimensional regression, multivariate inputs with correlated features, and stellar spectroscopy applications. LatentNN reduces attenuation bias across a range of signal-to-noise ratios where standard neural networks show large bias. This provides a framework for improved neural network inference in the low signal-to-noise regime characteristic of astronomical data. This bias correction is most effective when measurement errors are less than roughly half the intrinsic data range; in the regime of very low signal-to-noise and few informative features. Code is available at https://github.com/tingyuansen/LatentNN.

</details>


### [632] [Space AI: Leveraging Artificial Intelligence for Space to Improve Life on Earth](https://arxiv.org/abs/2512.22399)
*Ziyang Wang*

Main category: astro-ph.IM

TL;DR: The paper explores 'Space AI,' a field combining AI with space science, proposing a framework for its application across four mission contexts: earth-based, orbit-based, deep space, and multi-planetary life.


<details>
  <summary>Details</summary>
Motivation: Progress in Earth-based domains is constrained, driving the need for autonomous AI-driven operations in outer space under extreme conditions.

Method: The authors consolidate historical developments and current progress, proposing a systematic framework with four mission categories for applying AI in space.

Result: The framework provides a structure to enable advancements in AI-driven applications, from mission planning on Earth to building habitats for multi-planetary life.

Conclusion: Space AI has potential to not only revolutionize space exploration but also benefit society on Earth through advancements in multiple AI-related technologies.

Abstract: Artificial Intelligence (AI) is transforming domains from healthcare and agriculture to finance and industry. As progress on Earth meets growing constraints, the next frontier is outer space, where AI can enable autonomous, resilient operations under extreme uncertainty and limited human oversight. This paper introduces Space AI as a unified interdisciplinary field at the intersection of artificial intelligence and space science and technology. We consolidate historical developments and contemporary progress, and propose a systematic framework that organises Space AI into four mission contexts: 1 AI on Earth, covering intelligent mission planning, spacecraft design optimisation, simulation, and ground-based data analytics; 2 AI in Orbit, focusing on satellite and station autonomy, space robotics, on-board/near-real-time data processing, communication optimisation, and orbital safety; (3) AI in Deep Space, enabling autonomous navigation, adaptive scientific discovery, resource mapping, and long-duration human-AI collaboration under communication constraints; and 4 AI for Multi-Planetary Life, supporting in-situ resource utilisation, habitat and infrastructure construction, life-support and ecological management, and resilient interplanetary networks. Ultimately, Space AI can accelerate humanity's capability to explore and operate in space, while translating advances in sensing, robotics, optimisation, and trustworthy AI into broad societal impact on Earth.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [633] [Breaking Symmetry-Induced Degeneracy in Multi-Agent Ergodic Coverage via Stochastic Spectral Control](https://arxiv.org/abs/2512.23158)
*Kooktae Lee,Julian Martinez*

Main category: eess.SY

TL;DR: This paper improves the Spectral Multiscale Coverage (SMC) framework for multi-agent systems by addressing issues of symmetry-induced stalling and motion constraints through stochastic perturbation and contraction terms, verified via simulations.


<details>
  <summary>Details</summary>
Motivation: Classical SMC suffers from gradient cancellation issues, especially when agents are initialized near symmetry points of the target distribution, causing stalling or constrained motions.

Method: The authors rigorously analyzed the conditions causing directional degeneracy in agent dynamics and introduced stochastic perturbation and contraction terms to address these issues.

Result: The proposed stochastic SMC resolves stalling and axis-constrained motion, ensuring agents' trajectories remain bounded within the domain.

Conclusion: The modified SMC effectively overcomes symmetry-based issues in multi-agent coverage while ensuring robust and bounded agent trajectories.

Abstract: Multi-agent ergodic coverage via Spectral Multiscale Coverage (SMC) provides a principled framework for driving a team of agents so that their collective time-averaged trajectories match a prescribed spatial distribution. While classical SMC has demonstrated empirical success, it can suffer from gradient cancellation, particularly when agents are initialized near symmetry points of the target distribution, leading to undesirable behaviors such as stalling or motion constrained along symmetry axes. In this work, we rigorously characterize the initial conditions and symmetry-induced invariant manifolds that give rise to such directional degeneracy in first-order agent dynamics. To address this, we introduce a stochastic perturbation combined with a contraction term and prove that the resulting dynamics ensure almost-sure escape from zero-gradient manifolds while maintaining mean-square boundedness of agent trajectories. Simulations on symmetric multi-modal reference distributions demonstrate that the proposed stochastic SMC effectively mitigates transient stalling and axis-constrained motion, while ensuring that all agent trajectories remain bounded within the domain.

</details>


### [634] [Reach-Avoid Differential game with Reachability Analysis for UAVs: A decomposition approach](https://arxiv.org/abs/2512.22793)
*Minh Bui,Simon Monckton,Mo Chen*

Main category: eess.SY

TL;DR: The paper addresses the challenge of extending reach-avoid (RA) games to three spatial dimensions using a novel dimensionality reduction framework.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the need to handle RA games for UAVs in three spatial dimensions, addressing obstacles, adversarial dynamics, and optimality, which are computationally prohibitive to solve through standard methods.

Method: A novel framework decomposes the three-dimensional RA game into horizontal and vertical sub-games, solved via Hamilton-Jacobi (HJ) reachability analysis and incorporating second-order dynamics for defenders.

Result: Numerical simulations and experiments in a Gazebo physics simulator demonstrate the approach's effectiveness in guaranteeing optimality and achieving successful capture of attackers in three-dimensional space.

Conclusion: The proposed dimensionality reduction framework effectively extends reach-avoid games into three dimensions while maintaining optimality and practical applicability for UAV defense scenarios.

Abstract: Reach-avoid (RA) games have significant applications in security and defense, particularly for unmanned aerial vehicles (UAVs). These problems are inherently challenging due to the need to consider obstacles, consider the adversarial nature of opponents, ensure optimality, and account for nonlinear dynamics. Hamilton-Jacobi (HJ) reachability analysis has emerged as a powerful tool for tackling these challenges; however, while it has been applied to games involving two spatial dimensions, directly extending this approach to three spatial dimensions is impossible due to high dimensionality. On the other hand, alternative approaches for solving RA games lack the generality to consider games with three spatial dimensions involving agents with non-trivial system dynamics. In this work, we propose a novel framework for dimensionality reduction by decomposing the problem into a horizontal RA sub-game and a vertical RA sub-game. We then solve each sub-game using HJ reachability analysis and consider second-order dynamics that account for the defender's acceleration. To reconstruct the solution to the original RA game from the sub-games, we introduce a HJ-based tracking control algorithm in each sub-game that not only guarantees capture of the attacker but also tracking of the attacker thereafter. We prove the conditions under which the capture guarantees are maintained. The effectiveness of our approach is demonstrated via numerical simulations, showing that the decomposition maintains optimality and guarantees in the original problem. Our methods are also validated in a Gazebo physics simulator, achieving successful capture of quadrotors in three spatial dimensions space for the first time to the best of our knowledge.

</details>


### [635] [A Neural Network-Based Real-time Casing Collar Recognition System for Downhole Instruments](https://arxiv.org/abs/2512.22901)
*Si-Yu Xiao,Xin-Di Zhao,Xiang-Zhan Wang,Tian-Hao Mao,Ying-Kai Liao,Xing-Yu Liao,Yu-Qiao Chen,Jun-Jie Wang,Shuang Liu,Tu-Pei Chen,Yang Liu*

Main category: eess.SY

TL;DR: The paper introduces a real-time collar recognition system for oil and gas operations using neural networks optimized for constrained microprocessors, achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: To tackle signal degradation in traditional CCL monitoring methods and improve downhole positioning accuracy in oil and gas operations.

Method: Proposed lightweight "Collar Recognition Nets" (CRNs) utilizing temporal and depthwise separable convolutions, optimized for ARM Cortex-M7 microprocessors.

Result: Achieved computational complexity of 8,208 MACs with an F1 score of 0.972 and an inference latency of 343.2 μs on hardware.

Conclusion: Robust real-time collar recognition is achievable within power and space limitations of downhole instrumentation.

Abstract: Accurate downhole positioning is critical in oil and gas operations but is often compromised by signal degradation in traditional surface-based Casing Collar Locator (CCL) monitoring. To address this, we present an in-situ, real-time collar recognition system using embedded neural network. We introduce lightweight "Collar Recognition Nets" (CRNs) optimized for resource-constrained ARM Cortex-M7 microprocessors. By leveraging temporal and depthwise separable convolutions, our most compact model reduces computational complexity to just 8,208 MACs while maintaining an F1 score of 0.972. Hardware validation confirms an average inference latency of 343.2 μs, demonstrating that robust, autonomous signal processing is feasible within the severe power and space limitations of downhole instrumentation.

</details>


### [636] [Tree Meets Transformer: A Hybrid Architecture for Scalable Power Allocation in Cell-Free Networks](https://arxiv.org/abs/2512.22639)
*Irched Chafaa,Giacomo Bacci,Luca Sanguinetti*

Main category: eess.SY

TL;DR: The paper introduces a Tree-Transformer architecture for efficient scalable power allocation in wireless communication networks, surpassing traditional Transformer models in computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Address the computational inefficiencies of Transformer models in large-scale wireless networks, ensuring scalable solutions for dynamic user loads.

Method: Develop a Tree-Transformer model that compresses user features into a global root via a binary tree, employs a Transformer encoder for the root, and decodes power allocations using a shared decoder.

Result: The model achieves near-optimal performance, reduces inference time compared to Transformer baselines, and optimally handles variable and large user sets.

Conclusion: The novel Tree-Transformer architecture offers a practical solution for scalable and efficient power allocation, addressing real-world challenges in massive MIMO systems.

Abstract: Power allocation remains a fundamental challenge in wireless communication networks, particularly under dynamic user loads and large-scale deployments. While Transformerbased models have demonstrated strong performance, their computational cost scales poorly with the number of users. In this work, we propose a novel hybrid Tree-Transformer architecture that achieves scalable per-user power allocation. Our model compresses user features via a binary tree into a global root representation, applies a Transformer encoder solely to this root, and decodes per-user uplink and downlink powers through a shared decoder. This design achieves logarithmic depth and linear total complexity, enabling efficient inference across large and variable user sets without retraining or architectural changes. We evaluate our model on the max-min fairness problem in cellfree massive MIMO systems and demonstrate that it achieves near-optimal performance while significantly reducing inference time compared to full-attention baselines.

</details>


### [637] [Revealing design archetypes and flexibility in e-molecule import pathways using Modeling to Generate Alternatives and interpretable machine learning](https://arxiv.org/abs/2512.23284)
*Mahdi Kchaou,Francesco Contino,Diederik Coppitters*

Main category: eess.SY

TL;DR: The paper addresses the challenges in cost-optimal green e-molecule import modeling for Europe's energy transition by generating diverse near-optimal alternatives and using machine learning to analyze them.


<details>
  <summary>Details</summary>
Motivation: To move beyond fragile cost-optimal solutions for green e-molecule imports that fail to consider real-world constraints, uncertainties, and implementation challenges.

Method: The authors applied Modeling to Generate Alternatives to create diverse near-optimal solutions, using interpretable machine learning for insights. The study focuses on hydrogen import pathways with multiple carrier options.

Result: The study found significant flexibility in near-optimal solutions: solar, wind, and storage are not strictly required for solutions within a 10% cost margin. Different constraints, such as wind or storage limitations, favor different pathways.

Conclusion: Relying solely on cost-optimal designs is impractical. Exploring near-optimal alternatives reveals adaptable, feasible pathways for hydrogen imports accommodating varying constraints and uncertainties.

Abstract: Given the central role of green e-molecule imports in the European energy transition, many studies optimize import pathways and identify a single cost-optimal solution. However, cost optimality is fragile, as real-world implementation depends on regulatory, spatial, and stakeholder constraints that are difficult to represent in optimization models and can render cost-optimal designs infeasible. To address this limitation, we generate a diverse set of near-cost-optimal alternatives within an acceptable cost margin using Modeling to Generate Alternatives, accounting for unmodeled uncertainties. Interpretable machine learning is then applied to extract insights from the resulting solution space. The approach is applied to hydrogen import pathways considering hydrogen, ammonia, methane, and methanol as carriers. Results reveal a broad near-optimal space with great flexibility: solar, wind, and storage are not strictly required to remain within 10% of the cost optimum. Wind constraints favor solar-storage methanol pathways, while limited storage favors wind-based ammonia or methane pathways.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [638] [QSAR-Guided Generative Framework for the Discovery of Synthetically Viable Odorants](https://arxiv.org/abs/2512.23080)
*Tim C. Pearce,Ahmed Ibrahim*

Main category: physics.chem-ph

TL;DR: This study proposes a new AI framework combining a variational autoencoder (VAE) and a QSAR model to efficiently generate novel odorant molecules, overcoming data limitations.


<details>
  <summary>Details</summary>
Motivation: Developing novel odorant molecules is critical for fragrance and flavor industries, but the vast chemical space and the requirement for extensive molecular datasets present challenges.

Method: A framework with VAE for learning SMILES grammar, augmented with a loss term from an external QSAR model, is used to structure the latent representation based on odor probability.

Result: The model demonstrates robust internal consistency, generates 94.8% unique, syntactically valid structures, and explores extensive chemical space, validated against external datasets.

Conclusion: The proposed framework efficiently generates novel odorants with desirable properties while proving capable of chemical space exploration and addressing limitations of traditional datasets.

Abstract: The discovery of novel odorant molecules is key for the fragrance and flavor industries, yet efficiently navigating the vast chemical space to identify structures with desirable olfactory properties remains a significant challenge. Generative artificial intelligence offers a promising approach for \textit{de novo} molecular design but typically requires large sets of molecules to learn from. To address this problem, we present a framework combining a variational autoencoder (VAE) with a quantitative structure-activity relationship (QSAR) model to generate novel odorants from limited training sets of odor molecules. The self-supervised learning capabilities of the VAE allow it to learn SMILES grammar from ChemBL database, while its training objective is augmented with a loss term derived from an external QSAR model to structure the latent representation according to odor probability. While the VAE demonstrated high internal consistency in learning the QSAR supervision signal, validation against an external, unseen ground truth dataset (Unique Good Scents) confirms the model generates syntactically valid structures (100\% validity achieved via rejection sampling) and 94.8\% unique structures. The latent space is effectively structured by odor likelihood, evidenced by a Fréchet ChemNet Distance (FCD) of $\approx$ 6.96 between generated molecules and known odorants, compared to $\approx$ 21.6 for the ChemBL baseline. Structural analysis via Bemis-Murcko scaffolds reveals that 74.4\% of candidates possess novel core frameworks distinct from the training data, indicating the model performs extensive chemical space exploration beyond simple derivatization of known odorants. Generated candidates display physicochemical properties ....

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [639] [LogosQ: A High-Performance and Type-Safe Quantum Computing Library in Rust](https://arxiv.org/abs/2512.23183)
*Shiwen An,Jiayi Wang,Konstantinos Slavakis*

Main category: quant-ph

TL;DR: The paper introduces LogosQ, a quantum computing library in Rust, offering high performance and runtime error prevention through compile-time type safety, surpassing existing quantum software in speed and numerical stability.


<details>
  <summary>Details</summary>
Motivation: To address challenges of runtime errors and scalability issues in dynamic, Python-based quantum computing frameworks.

Method: Developed LogosQ using Rust for compile-time type safety, implemented optimization techniques like direct state-vector manipulation, adaptive parallel processing, and an FFT-optimized Quantum Fourier Transform.

Result: Achieves up to 900x speedups for QFT state preparation, 2-5x for variational workloads (compared to Python frameworks), 6-22x (compared to Julia), and comparable performance to Q sharp. Validates numerical stability in VQE experiments.

Conclusion: LogosQ sets a new benchmark for reliability and efficiency by integrating Rust's system programming safety with advanced quantum circuit optimization techniques.

Abstract: Developing robust and high performance quantum software is challenging due to the dynamic nature of existing Python-based frameworks, which often suffer from runtime errors and scalability bottlenecks. In this work, we present LogosQ, a high performance backend agnostic quantum computing library implemented in Rust that enforces correctness through compile time type safety. Unlike existing tools, LogosQ leverages Rust static analysis to eliminate entire classes of runtime errors, particularly in parameter-shift rule gradient computations for variational algorithms. We introduce novel optimization techniques, including direct state-vector manipulation, adaptive parallel processing, and an FFT optimized Quantum Fourier Transform, which collectively deliver speedups of up to 900 times for state preparation (QFT) and 2 to 5 times for variational workloads over Python frameworks (PennyLane, Qiskit), 6 to 22 times over Julia implementations (Yao), and competitive performance with Q sharp. Beyond performance, we validate numerical stability through variational quantum eigensolver (VQE) experiments on molecular hydrogen and XYZ Heisenberg models, achieving chemical accuracy even in edge cases where other libraries fail. By combining the safety of systems programming with advanced circuit optimization, LogosQ establishes a new standard for reliable and efficient quantum simulation.

</details>


### [640] [Revisiting finite Abelian hidden subgroup problem and its distributed exact quantum algorithm](https://arxiv.org/abs/2512.22959)
*Ziyuan Dong,Xiang Fan,Tengxun Zhong,Daowen Qiu*

Main category: quant-ph

TL;DR: The paper revisits the finite Abelian hidden subgroup problem (AHSP) and provides concise and efficient quantum and classical algorithmic solutions, including a distributed framework.


<details>
  <summary>Details</summary>
Motivation: Expand and improve solutions to the finite Abelian hidden subgroup problem with more concise, efficient, and distributed computational approaches.

Method: The paper introduces an exact quantum algorithm, a distributed quantum algorithm based on the Chinese Remainder Theorem with reduced complexities, and a parallel classical algorithm with lower query requirements.

Result: Achieved reductions in qubit usage, quantum query complexity, and enabled parallelization, extending results partially to non-Abelian groups.

Conclusion: This work advances the mathematical and algorithmic understanding of AHSP, providing refined methods with potential applications in distributed and parallel computing frameworks.

Abstract: We revisit the finite Abelian hidden subgroup problem (AHSP) from a mathematical perspective and make the following contributions. First, by employing amplitude amplification, we present an exact quantum algorithm for the finite AHSP, our algorithm is more concise than the previous exact algorithm and applies to any finite Abelian group. Second, utilizing the Chinese Remainder Theorem, we propose a distributed exact quantum algorithm for finite AHSP, which requires fewer qudits, lower quantum query complexity, and no quantum communication. We further show that our distributed approach can be extended to certain classes of non-Abelian groups. Finally, we develop a parallel exact classical algorithm for finite AHSP with reduced query complexity; even without parallel execution, the total number of queries across all nodes does not exceed that of the original centralized algorithm under mild conditions.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [641] [Cost-Aware Text-to-SQL: An Empirical Study of Cloud Compute Costs for LLM-Generated Queries](https://arxiv.org/abs/2512.22364)
*Saurabh Deochake,Debajyoti Mukhopadhyay*

Main category: cs.DB

TL;DR: The paper evaluates the cloud compute costs of LLM-generated SQL queries on Google BigQuery, revealing inefficiency patterns and providing cost-sensitive deployment guidelines.


<details>
  <summary>Details</summary>
Motivation: To address the gap in understanding and evaluating the consumption-based costs of cloud SQL query processing by LLMs, beyond execution time metrics such as VES.

Method: The study evaluates six state-of-the-art LLMs by executing 180 queries on Google BigQuery using the StackOverflow dataset, measuring metrics like bytes processed, slot utilization, and estimated costs.

Result: Key findings include (1) reasoning models process 44.5% fewer bytes while maintaining accuracy, (2) execution time has weak correlation with query costs, and (3) models' cost variance can reach 3.4x, with inefficiency patterns like missing partition filters.

Conclusion: Cloud SQL query costs vary significantly across models and simple optimizations in speed do not guarantee cost efficiency. Deployment guidelines can mitigate inefficiencies in cost-sensitive enterprise settings.

Abstract: Text-to-SQL systems powered by Large Language Models (LLMs) achieve high accuracy on standard benchmarks, yet existing efficiency metrics such as the Valid Efficiency Score (VES) measure execution time rather than the consumption-based costs of cloud data warehouses. This paper presents the first systematic evaluation of cloud compute costs for LLM-generated SQL queries. We evaluate six state-of-the-art LLMs across 180 query executions on Google BigQuery using the StackOverflow dataset (230GB), measuring bytes processed, slot utilization, and estimated cost. Our analysis yields three key findings: (1) reasoning models process 44.5% fewer bytes than standard models while maintaining equivalent correctness (96.7%-100%); (2) execution time correlates weakly with query cost (r=0.16), indicating that speed optimization does not imply cost optimization; and (3) models exhibit up to 3.4x cost variance, with standard models producing outliers exceeding 36GB per query. We identify prevalent inefficiency patterns including missing partition filters and unnecessary full-table scans, and provide deployment guidelines for cost-sensitive enterprise environments.

</details>


### [642] [Robust LLM-based Column Type Annotation via Prompt Augmentation with LoRA Tuning](https://arxiv.org/abs/2512.22742)
*Hanze Meng,Jianhao Cao,Rachel Pottinger*

Main category: cs.DB

TL;DR: The paper proposes a low-cost and robust framework for column type annotation (CTA) using prompt-augmented data and Low-Rank Adaptation (LoRA), improving performance stability and accuracy across datasets and templates.


<details>
  <summary>Details</summary>
Motivation: Current methods for column type annotation either suffer performance issues from prompt sensitivity or require costly retraining due to distribution shifts, limiting scalability and practicality.

Method: The authors use Low-Rank Adaptation (LoRA) combined with prompt-augmented data, allowing parameter-efficient fine-tuning of models to mitigate sensitivity to prompt changes and reduce computational costs.

Result: The proposed method achieved higher weighted F1 scores and robust performance across diverse prompt patterns, outperforming models fine-tuned on a single prompt template.

Conclusion: Parameter-efficient training using LoRA and prompt augmentation is a practical and effective strategy for developing adaptable column type annotation systems with stable cross-domain performance.

Abstract: Column Type Annotation (CTA) is a fundamental step towards enabling schema alignment and semantic understanding of tabular data. Existing encoder-only language models achieve high accuracy when fine-tuned on labeled columns, but their applicability is limited to in-domain settings, as distribution shifts in tables or label spaces require costly re-training from scratch. Recent work has explored prompting generative large language models (LLMs) by framing CTA as a multiple-choice task, but these approaches face two key challenges: (1) model performance is highly sensitive to subtle changes in prompt wording and structure, and (2) annotation F1 scores remain modest. A natural extension is to fine-tune large language models. However, fully fine-tuning these models incurs prohibitive computational costs due to their scale, and the sensitivity to prompts is not eliminated. In this paper, we present a parameter-efficient framework for CTA that trains models over prompt-augmented data via Low-Rank Adaptation (LoRA). Our approach mitigates sensitivity to prompt variations while drastically reducing the number of necessary trainable parameters, achieving robust performance across datasets and templates. Experimental results on recent benchmarks demonstrate that models fine-tuned with our prompt augmentation strategy maintain stable performance across diverse prompt patterns during inference and yield higher weighted F1 scores than those fine-tuned on a single prompt template. These results highlight the effectiveness of parameter-efficient training and augmentation strategies in developing practical and adaptable CTA systems.

</details>


### [643] [AGRO-SQL: Agentic Group-Relative Optimization with High-Fidelity Data Synthesis](https://arxiv.org/abs/2512.23366)
*Cehua Yang,Dongyu Xiao,Junming Lin,Yuyang Song,Hanxu Yan,Shawn Guo,Wei Zhang,Jian Yang,Mingjie Tang,Bryan Dai*

Main category: cs.DB

TL;DR: The paper introduces a dual-centric framework for improving Text-to-SQL systems, addressing data scarcity and reasoning limitations through a data factory and reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of high-quality training data availability and underdeveloped reasoning capabilities in Text-to-SQL systems.

Method: They propose a dual-centric framework: 1) Data-Centric Approach involves an iterative data factory for generating verified RL-ready data. 2) Model-Centric Approach introduces Agentic Reinforcement Learning with Diversity-Aware Cold Start and Group Relative Policy Optimization.

Result: The framework achieves state-of-the-art performance on the BIRD and Spider benchmarks among single-model methods.

Conclusion: The proposed approaches effectively enhance Text-to-SQL systems by addressing both data and reasoning limitations, setting new performance standards.

Abstract: The advancement of Text-to-SQL systems is currently hindered by the scarcity of high-quality training data and the limited reasoning capabilities of models in complex scenarios. In this paper, we propose a holistic framework that addresses these issues through a dual-centric approach. From a Data-Centric perspective, we construct an iterative data factory that synthesizes RL-ready data characterized by high correctness and precise semantic-logic alignment, ensured by strict verification. From a Model-Centric perspective, we introduce a novel Agentic Reinforcement Learning framework. This framework employs a Diversity-Aware Cold Start stage to initialize a robust policy, followed by Group Relative Policy Optimization (GRPO) to refine the agent's reasoning via environmental feedback. Extensive experiments on BIRD and Spider benchmarks demonstrate that our synergistic approach achieves state-of-the-art performance among single-model methods.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [644] [A general framework for deep learning](https://arxiv.org/abs/2512.23425)
*William Kengne,Modou Wade*

Main category: math.ST

TL;DR: This paper proposes two deep neural network estimators for nonparametric regression and classification, analyzing their performance for various types of data, including independent, mixing observations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to create robust deep learning estimators capable of handling varied types of data, including dependent data characterized by mixing conditions, while providing rigorous theoretical guarantees.

Method: The authors propose two estimators: a non-penalized deep neural network estimator (NPDNN) and a sparse-penalized deep neural network estimator (SPDNN), providing bounds for their expected excess risks on specific function classes.

Result: Both NPDNN and SPDNN estimators are proven to be minimax optimal (up to logarithmic factors) for diverse data settings, including independent and mixing processes.

Conclusion: The paper demonstrates how deep neural network estimators can achieve optimal theoretical performance across multiple data scenarios, expanding applicability and reliability in nonparametric regression and classification.

Abstract: This paper develops a general approach for deep learning for a setting that includes nonparametric regression and classification. We perform a framework from data that fulfills a generalized Bernstein-type inequality, including independent, $φ$-mixing, strongly mixing and $\mathcal{C}$-mixing observations. Two estimators are proposed: a non-penalized deep neural network estimator (NPDNN) and a sparse-penalized deep neural network estimator (SPDNN). For each of these estimators, bounds of the expected excess risk on the class of Hölder smooth functions and composition Hölder functions are established. Applications to independent data, as well as to $φ$-mixing, strongly mixing, $\mathcal{C}$-mixing processes are considered. For each of these examples, the upper bounds of the expected excess risk of the proposed NPDNN and SPDNN predictors are derived. It is shown that both the NPDNN and SPDNN estimators are minimax optimal (up to a logarithmic factor) in many classical settings.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [645] [Mining the Gold: Student-AI Chat Logs as Rich Sources for Automated Knowledge Gap Detection](https://arxiv.org/abs/2512.22404)
*Quanzhi Fu,Qiyu Wu,Dan Williams*

Main category: cs.HC

TL;DR: The paper introduces QueryQuilt, a framework using LLM-driven chat data to detect knowledge gaps in large lectures by analyzing student-AI interactions.


<details>
  <summary>Details</summary>
Motivation: Growing lecture sizes hinder timely identification of student knowledge gaps, and existing systems are limited in recognizing spontaneous gaps during teaching.

Method: QueryQuilt analyzes real-time student-AI dialogues using two agents: a Dialogue Agent to respond and probe gaps, and a Knowledge Gap Identification Agent to synthesize class-wide insights.

Result: QueryQuilt achieved 100% accuracy in identifying gaps among simulated students and 95% completeness on real dialogue data.

Conclusion: The approach shows strong potential for improving teaching effectiveness in large classes, and plans for real classroom deployment are underway for further evaluation.

Abstract: With the significant increase in enrollment in computing-related programs over the past 20 years, lecture sizes have grown correspondingly. In large lectures, instructors face challenges on identifying students' knowledge gaps timely, which is critical for effective teaching. Existing classroom response systems rely on instructor-initiated interactions, which limits their ability to capture the spontaneous knowledge gaps that naturally emerge during lectures. With the widespread adoption of LLMs among students, we recognize these student-AI dialogues as a valuable, student-centered data source for identifying knowledge gaps. In this idea paper, we propose QueryQuilt, a multi-agent LLM framework that automatically detects common knowledge gaps in large-scale lectures by analyzing students' chat logs with AI assistants. QueryQuilt consists of two key components: (1) a Dialogue Agent that responds to student questions while employing probing questions to reveal underlying knowledge gaps, and (2) a Knowledge Gap Identification Agent that systematically analyzes these dialogues to identify knowledge gaps across the student population. By generating frequency distributions of identified gaps, instructors can gain comprehensive insights into class-wide understanding. Our evaluation demonstrates promising results, with QueryQuilt achieving 100% accuracy in identifying knowledge gaps among simulated students and 95% completeness when tested on real student-AI dialogue data. These initial findings indicate the system's potential for facilitate teaching in authentic learning environments. We plan to deploy QueryQuilt in actual classroom settings for comprehensive evaluation, measuring its detection accuracy and impact on instruction.

</details>


### [646] [Reimagining the Traditional Flight Computer: E6BJA as a Modern, Multi-Platform Tool for Flight Calculations and Training](https://arxiv.org/abs/2512.23055)
*Jamie J. Alnasir*

Main category: cs.HC

TL;DR: The paper introduces E6BJA, a modern software-based flight computer for multiple platforms, improving upon traditional tools through enhanced precision and educational resources.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of traditional flight computers (mechanical and electronic) which lack adaptability, modern interaction models, and extensibility, making them less aligned with digital aviation needs.

Method: The authors designed and implemented E6BJA, a multi-platform flight computer for iOS, Android, Windows, and web, reproducing traditional core calculations and enhancing them with modern modelling capabilities and embedded educational content.

Result: Compared to traditional flight computers, E6BJA shows advancements in accuracy, reduced errors, discoverability, and educational utility across functional, cognitive, and technical aspects.

Conclusion: E6BJA is a significant advancement in pilot-facing tools, integrating traditional flight planning rigor with modern digital design, thereby improving safety and usability for flight planning and aviation training.

Abstract: Traditional flight computers -- including mechanical "whiz-wheels" (e.g. E6B, CRP series) and electronic flight calculators (e.g. ASA CX-3, Sportys E6-B) -- have long played a central role in flight planning and training within general aviation (GA). While these tools remain pedagogically valuable, their fixed form factors, constrained interaction models, and limited extensibility are increasingly misaligned with the expectations and workflows of pilots operating in modern digital environments.
  This paper presents E6BJA (Jamies Flight Computer), a fully featured, multi-platform, software-based flight computer designed natively for Apple iOS, Android, and Microsoft Windows devices, with a complementary web-based implementation. E6BJA reproduces the core calculations of traditional flight computers while extending them through enhanced modelling capabilities such as the 1976 International Standard Atmosphere, carburettor icing risk estimation, and aircraft-specific weight and balance calculators. Each calculator is accompanied by embedded educational monographs that explain underlying assumptions, variables, and equations.
  We compare E6BJA with mechanical and electronic flight computers across functional, cognitive, and technical dimensions, demonstrating improvements in accuracy, error reduction, discoverability, and educational value. We also discuss design trade-offs associated with native multi-platform development and examine how contemporary mobile computing environments can support safer and more intuitive pre-flight planning for pilots, trainees, instructors, and flight planning personnel. By combining the conceptual rigour of traditional flight planning methods with modern human-computer interaction design, E6BJA represents a meaningful evolution in pilot-facing flight tools, supporting both computation and instruction in aviation training contexts.

</details>


### [647] [SPECTRE: Spectral Pre-training Embeddings with Cylindrical Temporal Rotary Position Encoding for Fine-Grained sEMG-Based Movement Decoding](https://arxiv.org/abs/2512.22481)
*Zihan Weng,Chanlin Yi,Pouya Bashivan,Jing Lu,Fali Li,Dezhong Yao,Jingming Hou,Yangsong Zhang,Peng Xu*

Main category: cs.HC

TL;DR: This paper introduces SPECTRE, a domain-specific self-supervised learning framework that enhances decoding of fine-grained movement from sEMG signals, overcoming challenges of signal non-stationarity and low signal-to-noise ratios.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address limitations of generic SSL frameworks for prosthetic control, particularly their inefficiency in handling noisy sEMG signals and lacking topology-specific modeling for electrode arrays.

Method: SPECTRE improves movement decoding using a physiologically-grounded pre-training task with STFT-based pseudo-labels and Cylindrical Rotary Position Embedding (CyRoPE) for modeling forearm sensor topology.

Result: SPECTRE significantly outperforms existing supervised and SSL methods for sEMG movement decoding, with supporting evidence from evaluations and ablation studies on various datasets.

Conclusion: SPECTRE establishes new benchmarks for robust and practical myoelectric interfaces, successfully handling real-world complexities of sEMG signals and enabling improved prosthetic control.

Abstract: Decoding fine-grained movement from non-invasive surface Electromyography (sEMG) is a challenge for prosthetic control due to signal non-stationarity and low signal-to-noise ratios. Generic self-supervised learning (SSL) frameworks often yield suboptimal results on sEMG as they attempt to reconstruct noisy raw signals and lack the inductive bias to model the cylindrical topology of electrode arrays. To overcome these limitations, we introduce SPECTRE, a domain-specific SSL framework. SPECTRE features two primary contributions: a physiologically-grounded pre-training task and a novel positional encoding. The pre-training involves masked prediction of discrete pseudo-labels from clustered Short-Time Fourier Transform (STFT) representations, compelling the model to learn robust, physiologically relevant frequency patterns. Additionally, our Cylindrical Rotary Position Embedding (CyRoPE) factorizes embeddings along linear temporal and annular spatial dimensions, explicitly modeling the forearm sensor topology to capture muscle synergies. Evaluations on multiple datasets, including challenging data from individuals with amputation, demonstrate that SPECTRE establishes a new state-of-the-art for movement decoding, significantly outperforming both supervised baselines and generic SSL approaches. Ablation studies validate the critical roles of both spectral pre-training and CyRoPE. SPECTRE provides a robust foundation for practical myoelectric interfaces capable of handling real-world sEMG complexities.

</details>


### [648] [Emotion classification using EEG headset signals and Random Forest](https://arxiv.org/abs/2512.22333)
*Ricardo Vasquez,Diego Riofrío-Luzcando,Joe Carrion-Jumbo,Cesar Guevara*

Main category: cs.HC

TL;DR: This paper introduces a computational model to classify emotions using EEG signals through the brain-computer interface EMOTIV EPOC, achieving 97.21% accuracy for happiness and 76% accuracy for sadness and relaxation.


<details>
  <summary>Details</summary>
Motivation: Understanding emotions is crucial for enhancing communication between humans and machines. Computational systems can facilitate better emotion recognition and understanding to improve human-machine interaction.

Method: The study collected EEG data from 50 individuals using EMOTIV EPOC while exposing them to audiovisual stimuli. A Random Forest model was developed for classifying three emotional states (happiness, sadness, and relaxation).

Result: The Random Forest model provided high accuracy in emotion prediction: 97.21% for happiness, 76% for sadness, and 76% for relaxation. A real-time emotion prediction algorithm was also implemented.

Conclusion: The study showcases a reliable and efficient method to classify emotions based on EEG signals, with potential applications in enhancing human-computer interaction systems.

Abstract: Emotions are one of the important components of the human being, thus they are a valuable part of daily activities such as interaction with people, decision making and learning. For this reason, it is important to detect, recognize and understand emotions using computational systems to improve communication between people and machines, which would facilitate the ability of computers to understand the communication between humans. This study proposes the creation of a model that allows the classification of people's emotions based on their EEG signals, for which the brain-computer interface EMOTIV EPOC was used. This allowed the collection of electroencephalographic information from 50 people, all of whom were shown audiovisual resources that helped to provoke the desired mood. The information obtained was stored in a database for the generation of the model and the corresponding classification analysis. Random Forest model was created for emotion prediction (happiness, sadness and relaxation), based on the signals of any person. The results obtained were 97.21% accurate for happiness, 76% for relaxation and 76% for sadness. Finally, the model was used to generate a real-time emotion prediction algorithm; it captures the person's EEG signals, executes the generated algorithm and displays the result on the screen with the help of images representative of each emotion.

</details>


### [649] [It's a TRAP! Task-Redirecting Agent Persuasion Benchmark for Web Agents](https://arxiv.org/abs/2512.23128)
*Karolina Korgul,Yushi Yang,Arkadiusz Drohomirecki,Piotr Błaszczyk,Will Howard,Lukas Aichberger,Chris Russell,Philip H. S. Torr,Adam Mahdi,Adel Bibi*

Main category: cs.HC

TL;DR: The paper examines the vulnerability of web-based agents powered by large language models to prompt injection attacks and introduces the Task-Redirecting Agent Persuasion Benchmark (TRAP) for evaluation.


<details>
  <summary>Details</summary>
Motivation: Web agents face significant risks from adversarial prompt injections due to their dependence on dynamic web content, and addressing these vulnerabilities is imperative for robust agent performance.

Method: The study evaluates six state-of-the-art models using the TRAP framework, conducting controlled experiments on website clones to assess susceptibility to prompt injections.

Result: Prompt injection attacks succeeded in diverting agents from their tasks in 25% of cases on average, with susceptibility varying across models (e.g., GPT-5 at 13%, DeepSeek-R1 at 43%).

Conclusion: Web-based agents exhibit systemic vulnerabilities to prompt injection attacks, which can be mitigated through targeted study and improved frameworks like TRAP.

Abstract: Web-based agents powered by large language models are increasingly used for tasks such as email management or professional networking. Their reliance on dynamic web content, however, makes them vulnerable to prompt injection attacks: adversarial instructions hidden in interface elements that persuade the agent to divert from its original task. We introduce the Task-Redirecting Agent Persuasion Benchmark (TRAP), an evaluation for studying how persuasion techniques misguide autonomous web agents on realistic tasks. Across six frontier models, agents are susceptible to prompt injection in 25\% of tasks on average (13\% for GPT-5 to 43\% for DeepSeek-R1), with small interface or contextual changes often doubling success rates and revealing systemic, psychologically driven vulnerabilities in web-based agents. We also provide a modular social-engineering injection framework with controlled experiments on high-fidelity website clones, allowing for further benchmark expansion.

</details>


### [650] [Clinically Calibrated Machine Learning Benchmarks for Large-Scale Multi-Disorder EEG Classification](https://arxiv.org/abs/2512.22656)
*Argha Kamal Samanta,Deepak Mewada,Monalisa Sarma,Debasis Samanta*

Main category: cs.HC

TL;DR: This paper develops and evaluates an automated system to classify EEG recordings for eleven neurological disorders, focusing on improving diagnostic sensitivity for scalable screening.


<details>
  <summary>Details</summary>
Motivation: To address the manual, time-intensive, and variable process of EEG interpretation for multiple neurological conditions, and to provide automated support for multi-disorder screening rather than just isolated problems like seizure detection.

Method: The authors processed EEG recordings using a standard longitudinal bipolar montage and a multi-domain feature set. Machine learning models were trained, accounting for severe class imbalance, to prioritize sensitivity and calibrated thresholds.

Result: The sensitivity-focused models achieved recall above 80% for most disorder categories, with significant recall improvements (15-30%) for rare conditions. The system demonstrated physiologically plausible patterns reflective of known clinical markers.

Conclusion: This study sets a performance benchmark for multi-disorder EEG classification. It shows automated, sensitivity-prioritized systems have potential to assist in scalable EEG screening and triage in clinical environments.

Abstract: Clinical electroencephalography is routinely used to evaluate patients with diverse and often overlapping neurological conditions, yet interpretation remains manual, time-intensive, and variable across experts. While automated EEG analysis has been widely studied, most existing methods target isolated diagnostic problems, particularly seizure detection, and provide limited support for multi-disorder clinical screening.
  This study examines automated EEG-based classification across eleven clinically relevant neurological disorder categories, encompassing acute time-critical conditions, chronic neurocognitive and developmental disorders, and disorders with indirect or weak electrophysiological signatures. EEG recordings are processed using a standard longitudinal bipolar montage and represented through a multi-domain feature set capturing temporal statistics, spectral structure, signal complexity, and inter-channel relationships. Disorder-aware machine learning models are trained under severe class imbalance, with decision thresholds explicitly calibrated to prioritize diagnostic sensitivity.
  Evaluation on a large, heterogeneous clinical EEG dataset demonstrates that sensitivity-oriented modeling achieves recall exceeding 80% for the majority of disorder categories, with several low-prevalence conditions showing absolute recall gains of 15-30% after threshold calibration compared to default operating points. Feature importance analysis reveals physiologically plausible patterns consistent with established clinical EEG markers.
  These results establish realistic performance baselines for multi-disorder EEG classification and provide quantitative evidence that sensitivity-prioritized automated analysis can support scalable EEG screening and triage in real-world clinical settings.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [651] [The Big Three in Marriage Talk: LLM-Assisted Analysis of Moral Ethics and Sentiment on Weibo and Xiaohongshu](https://arxiv.org/abs/2512.23609)
*Frank Tian-Fang Ye,Xiaozi Gao*

Main category: econ.GN

TL;DR: The study examined marriage-related sentiments on Chinese social media, revealing platform differences and moral reasoning associations with sentiment toward marriage.


<details>
  <summary>Details</summary>
Motivation: The decline in marriage registrations in China raises questions about public attitudes and underlying moral reasoning influencing these views.

Method: Content analysis of 219,358 marriage-related posts from Sina Weibo and Xiaohongshu, using LLMs and Shweder's Big Three moral ethics framework.

Result: Sentiment varied by platform (Weibo-positive, Xiaohongshu-neutral). Posts invoking Autonomy and Community morals were largely negative, while Divinity-framed posts skewed neutral or positive.

Conclusion: Negative marriage attitudes are linked to concerns about autonomy and community obligations, and the study highlights LLMs' utility for large-scale qualitative analysis.

Abstract: China's marriage registrations have declined dramatically, dropping from 13.47 million couples in 2013 to 6.1 million in 2024. Understanding public attitudes toward marriage requires examining not only emotional sentiment but also the moral reasoning underlying these evaluations. This study analyzed 219,358 marriage-related posts from two major Chinese social media platforms (Sina Weibo and Xiaohongshu) using large language model (LLM)-assisted content analysis. Drawing on Shweder's Big Three moral ethics framework, posts were coded for sentiment (positive, negative, neutral) and moral dimensions (Autonomy, Community, Divinity). Results revealed platform differences: Weibo discourse skewed positive, while Xiaohongshu was predominantly neutral. Most posts across both platforms lacked explicit moral framing. However, when moral ethics were invoked, significant associations with sentiment emerged. Posts invoking Autonomy ethics and Community ethics were predominantly negative, whereas Divinity-framed posts tended toward neutral or positive sentiment. These findings suggest that concerns about both personal autonomy constraints and communal obligations drive negative marriage attitudes in contemporary China. The study demonstrates LLMs' utility for scaling qualitative analysis and offers insights for developing culturally informed policies addressing marriage decline in Chinese contexts.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [652] [The Complete Anatomy of the Madden-Julian Oscillation Revealed by Artificial Intelligence](https://arxiv.org/abs/2512.22144)
*Xiao Zhou,Yuze Sun,Jie Wu,Xiaomeng Huang*

Main category: physics.ao-ph

TL;DR: The paper introduces a deep learning model called PhysAnchor-MJO-AE to redefine the Madden-Julian Oscillation (MJO) lifecycle by discovering intrinsic structures and enabling more accurate phase diagnostics.


<details>
  <summary>Details</summary>
Motivation: Accurately defining the life cycle of MJO remains challenging due to propagation complexities and issues in current linear projection methods.

Method: The authors employ PhysAnchor-MJO-AE, a deep learning model that learns latent representations to objectively cluster MJO states and discover a complete six-phase anatomical map of its lifecycle.

Result: The study identifies two transitional lifecycle phases and creates a new monitoring framework that minimizes spurious propagation and reduces convective misplacement errors by an order of magnitude.

Conclusion: This paper demonstrates the transformative potential of AI as a discovery tool for extracting fundamental dynamics from complex systems such as the MJO lifecycle.

Abstract: Accurately defining the life cycle of the Madden-Julian Oscillation (MJO), the dominant mode of intraseasonal climate variability, remains a foundational challenge due to its propagating nature. The established linear-projection method (RMM index) often conflates mathematical artifacts with physical states, while direct clustering in raw data space is confounded by a "propagation penalty." Here, we introduce an "AI-for-theory" paradigm to objectively discover the MJO's intrinsic structure. We develop a deep learning model, PhysAnchor-MJO-AE, to learn a latent representation where vector distance corresponds to physical-feature similarity, enabling objective clustering of MJO dynamical states. Clustering these "MJO fingerprints" reveals the first complete, six-phase anatomical map of its life cycle. This taxonomy refines and critically completes the classical view by objectively isolating two long-hypothesized transitional phases: organizational growth over the Indian Ocean and the northward shift over the Philippine Sea. Derived from this anatomy, we construct a new physics-coherent monitoring framework that decouples location and intensity diagnostics. This framework reduces the rates of spurious propagation and convective misplacement by over an order of magnitude compared to the classical index. Our work transforms AI from a forecasting tool into a discovery microscope, establishing a reproducible template for extracting fundamental dynamical constructs from complex systems.

</details>


### [653] [Neural ocean forecasting from sparse satellite-derived observations: a case-study for SSH dynamics and altimetry data](https://arxiv.org/abs/2512.22152)
*Daria Botvynko,Pierre Haslée,Lucile Gaultier,Bertrand Chapron,Clement de Boyer Montégut,Anass El Aouni,Julien Le Sommer,Ronan Fablet*

Main category: physics.ao-ph

TL;DR: The paper develops a deep learning framework for predicting sea surface dynamics using sparse satellite data, outperforming traditional methods in accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy and applicability of short-term oceanographic forecasting using sparse observational data.

Method: Adapted U-Net and 4DVarNet architectures for sequence-to-sequence forecasting, trained on GLORYS12 reanalysis data, and evaluated against operational forecasts using several accuracy metrics.

Result: The developed model outperformed traditional forecasting methods, particularly in regions with high variability, exhibiting superior predictive performance on sparse data.

Conclusion: The study highlights the feasibility and effectiveness of using end-to-end deep learning models for forecasting sea surface dynamics, emphasizing reproducibility and application in operational settings.

Abstract: We present an end-to-end deep learning framework for short-term forecasting of global sea surface dynamics based on sparse satellite altimetry data. Building on two state-of-the-art architectures: U-Net and 4DVarNet, originally developed for image segmentation and spatiotemporal interpolation respectively, we adapt the models to forecast the sea level anomaly and sea surface currents over a 7-day horizon using sequences of sparse nadir altimeters observations. The model is trained on data from the GLORYS12 operational ocean reanalysis, with synthetic nadir sampling patterns applied to simulate realistic observational coverage. The forecasting task is formulated as a sequence-to-sequence mapping, with the input comprising partial sea level anomaly (SLA) snapshots and the target being the corresponding future full-field SLA maps. We evaluate model performance using (i) normalized root mean squared error (nRMSE), (ii) averaged effective resolution, (iii) percentage of correctly predicted velocities magnitudes and angles, and benchmark results against the operational Mercator Ocean forecast product. Results show that end-to-end neural forecasts outperform the baseline across all lead times, with particularly notable improvements in high variability regions. Our framework is developed within the OceanBench benchmarking initiative, promoting reproducibility and standardized evaluation in ocean machine learning. These results demonstrate the feasibility and potential of end-to-end neural forecasting models for operational oceanography, even in data-sparse conditions.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [654] [PHANTOM: Physics-Aware Adversarial Attacks against Federated Learning-Coordinated EV Charging Management System](https://arxiv.org/abs/2512.22381)
*Mohammad Zakaria Haider,Amit Kumar Podder,Prabin Mali,Aranya Chakrabortty,Sumit Paudyal,Mohammad Ashiqur Rahman*

Main category: cs.ET

TL;DR: The paper proposes PHANTOM, a physics-aware adversarial network utilizing reinforcement learning to optimize and assess the security of electric vehicle charging stations (EVCS) in managing grid stability and operations.


<details>
  <summary>Details</summary>
Motivation: The rapid integration of EVCS into distribution networks requires intelligent systems to manage grid resilience and safeguard against cybersecurity threats.

Method: PHANTOM involves a physics-informed neural network (PINN) enhanced by federated learning as a digital twin for EVCS, leveraging deep Q-networks (DQN) and soft actor-critic (SAC) within a multi-agent reinforcement learning framework to simulate adversarial attacks and their grid-level impacts.

Result: The developed framework demonstrates how adversarial attacks on EVCS operations can cause load imbalances and voltage instabilities that cascade through transmission and distribution systems.

Conclusion: The work underscores the necessity of physics-aware cybersecurity measures to protect the resilience of electric grid systems amid large-scale adoption of electric vehicles.

Abstract: The rapid deployment of electric vehicle charging stations (EVCS) within distribution networks necessitates intelligent and adaptive control to maintain the grid's resilience and reliability. In this work, we propose PHANTOM, a physics-aware adversarial network that is trained and optimized through a multi-agent reinforcement learning model. PHANTOM integrates a physics-informed neural network (PINN) enabled by federated learning (FL) that functions as a digital twin of EVCS-integrated systems, ensuring physically consistent modeling of operational dynamics and constraints. Building on this digital twin, we construct a multi-agent RL environment that utilizes deep Q-networks (DQN) and soft actor-critic (SAC) methods to derive adversarial false data injection (FDI) strategies capable of bypassing conventional detection mechanisms. To examine the broader grid-level consequences, a transmission and distribution (T and D) dual simulation platform is developed, allowing us to capture cascading interactions between EVCS disturbances at the distribution level and the operations of the bulk transmission system. Results demonstrate how learned attack policies disrupt load balancing and induce voltage instabilities that propagate across T and D boundaries. These findings highlight the critical need for physics-aware cybersecurity to ensure the resilience of large-scale vehicle-grid integration.

</details>


<div id='q-fin.PR'></div>

# q-fin.PR [[Back]](#toc)

### [655] [Machine learning models for predicting catastrophe bond coupons using climate data](https://arxiv.org/abs/2512.22660)
*Julia Kończal,Michał Balcerek,Krzysztof Burnecki*

Main category: q-fin.PR

TL;DR: The paper analyzes the influence of climate variability on catastrophe (CAT) bond pricing, using machine learning models to forecast bond coupons.


<details>
  <summary>Details</summary>
Motivation: To address the growing risks from natural disasters and explore alternative financial instruments like CAT bonds tailored by climate variability.

Method: The study employs linear regression and advanced machine learning models, incorporating traditional and climate-based features, for pricing prediction.

Result: Including climate-related variables improves predictive accuracy in CAT bond pricing, with extremely randomized trees achieving the best results.

Conclusion: Climate variability significantly impacts CAT bond pricing, and machine learning methods are effective tools for capturing these relationships and forecasting bond coupons.

Abstract: In recent years, the growing frequency and severity of natural disasters have increased the need for effective tools to manage catastrophe risk. Catastrophe (CAT) bonds allow the transfer of part of this risk to investors, offering an alternative to traditional reinsurance. This paper examines the role of climate variability in CAT bond pricing and evaluates the predictive performance of various machine learning models in forecasting CAT bond coupons. We combine features typically used in the literature with a new set of climate indicators, including Oceanic Ni{ñ}o Index, Arctic Oscillation, North Atlantic Oscillation, Outgoing Longwave Radiation, Pacific-North American pattern, Pacific Decadal Oscillation, Southern Oscillation Index, and sea surface temperatures. We compare the performance of linear regression with several machine learning algorithms, such as random forest, gradient boosting, extremely randomized trees, and extreme gradient boosting. Our results show that including climate-related variables improves predictive accuracy across all models, with extremely randomized trees achieving the lowest root mean squared error (RMSE). These findings suggest that large-scale climate variability has a measurable influence on CAT bond pricing and that machine learning methods can effectively capture these complex relationships.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [656] [Verifying Asynchronous Hyperproperties in Reactive Systems](https://arxiv.org/abs/2512.23344)
*Raven Beutner,Bernd Finkbeiner*

Main category: cs.LO

TL;DR: The paper introduces a variant of HyperLTL called asynchronous HyperLTL (A-HLTL), addressing limitations in synchronously comparing execution traces and enabling asynchronous comparisons with stutterings. They propose a novel game-based model-checking approach for verifying these hyperproperties.


<details>
  <summary>Details</summary>
Motivation: Existing HyperLTL logic often fails to express certain asynchronous comparisons of traces, limiting its ability to handle crucial hyperproperties like observational determinism and stuttering refinement.

Method: The authors use a game-based approach where verification is treated as a game between a verifier and a refuter, enabling the identification of suitable stutterings and traces for arbitrary formulas.

Result: The paper provides a game-based solution for model-checking $orall^*orall^*$ A-HLTL formulas, identifying fragments of the logic that produce finite-state decision procedures.

Conclusion: Their method facilitates the verification of complex asynchronous hyperproperties that were previously difficult or infeasible, enhancing the expressive power of HyperLTL for reactive systems.

Abstract: Hyperproperties are system properties that relate multiple execution traces and commonly occur when specifying information-flow and security policies. Logics like HyperLTL utilize explicit quantification over execution traces to express temporal hyperproperties in reactive systems, i.e., hyperproperties that reason about the temporal behavior along infinite executions. An often unwanted side-effect of such logics is that they compare the quantified traces synchronously. This prohibits the logics from expressing properties that compare multiple traces asynchronously, such as Zdancewic and Myers's observational determinism, McLean's non-inference, or stuttering refinement. We study the model-checking problem for a variant of asynchronous HyperLTL (A-HLTL), a temporal logic that can express hyperproperties where multiple traces are compared across timesteps. In addition to quantifying over system traces, A-HLTL features secondary quantification over stutterings of these traces. Consequently, A-HLTL allows for a succinct specification of many widely used asynchronous hyperproperties. Model-checking A-HLTL requires finding suitable stutterings, which, thus far, has been only possible for very restricted fragments or terminating systems. In this paper, we propose a novel game-based approach for the verification of arbitrary $\forall^*\exists^*$ A-HLTL formulas in reactive systems. In our method, we consider the verification as a game played between a verifier and a refuter, who challenge each other by controlling parts of the underlying traces and stutterings. A winning strategy for the verifier then corresponds to concrete witnesses for existentially quantified traces and asynchronous alignments for existentially quantified stutterings. We identify fragments for which our game-based interpretation is complete and thus constitutes a finite-state decision procedure.

</details>


### [657] [ReVEAL: GNN-Guided Reverse Engineering for Formal Verification of Optimized Multipliers](https://arxiv.org/abs/2512.22260)
*Chen Chen,Daniela Kaufmann,Chenhui Deng,Zhan Song,Hongce Zhang,Cunxi Yu*

Main category: cs.LO

TL;DR: ReVEAL introduces a graph-learning-based method for reverse-engineering multiplier architectures to enhance circuit verification.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address limitations in scalability and accuracy of traditional rule-based methods for verifying large optimized multipliers.

Method: The approach uses graph-learning techniques to identify architectural patterns and integrates these insights into existing verification workflows.

Result: ReVEAL shows improved scalability and accuracy across diverse multiplier benchmarks, outperforming conventional methods.

Conclusion: The method effectively supports algebraic proof strategies and enhances verification processes for complex multiplier designs.

Abstract: We present ReVEAL, a graph-learning-based method for reverse engineering of multiplier architectures to improve algebraic circuit verification techniques. Our framework leverages structural graph features and learning-driven inference to identify architecture patterns at scale, enabling robust handling of large optimized multipliers. We demonstrate applicability across diverse multiplier benchmarks and show improvements in scalability and accuracy compared to traditional rule-based approaches. The method integrates smoothly with existing verification flows and supports downstream algebraic proof strategies.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [658] [Beyond-Diagonal Reconfigurable Intelligent Surfaces for 6G Networks: Principles, Challenges, and Quantum Horizons](https://arxiv.org/abs/2512.23400)
*Abd Ullah Khan,Uman Khalid,Muhammad Tanveer,Trung Q. Duong,Hyundong Shin*

Main category: cs.SI

TL;DR: A new type of reconfigurable intelligent surface (BD-RIS) enables better wave manipulation using inter-element connections, tackling various challenges in beamforming through advanced algorithms and quantum-enhanced ML models.


<details>
  <summary>Details</summary>
Motivation: Innovate wave manipulation technologies to enhance communication systems, particularly using BD-RIS for better amplitude, phase adjustments, and practical deployment in complex environments.

Method: Introduce BD-RIS design principles and functionalities, evaluate beamforming with four algorithms, and apply hybrid quantum-classical ML models to enhance prediction in real-world communication scenarios.

Result: Comparison of four algorithms for beamforming demonstrates their impact on sum rate and computation cost. Quantum-enhanced machine learning models improve beam prediction performance in 6G scenarios.

Conclusion: BD-RIS offers transformative advantages, but realizing its potential requires addressing significant technical challenges in beamforming and hybrid quantum-classical integration for 6G systems.

Abstract: A beyond-diagonal reconfigurable intelligent surface (BD-RIS) is an innovative type of reconfigurable intelligent surface (RIS) that has recently been proposed and is considered a revolutionary advancement in wave manipulation. Unlike the mutually disconnected arrangement of elements in traditional RISs, BD-RIS creates cost-effective and simple inter-element connections, allowing for greater freedom in configuring the amplitude and phase of impinging waves. However, there are numerous underlying challenges in realizing the advantages associated with BD-RIS, prompting the research community to actively investigate cutting-edge schemes and algorithms in this direction. Particularly, the passive beamforming design for BD-RIS under specific environmental conditions has become a major focus in this research area. In this article, we provide a systematic introduction to BD-RIS, elaborating on its functional principles concerning architectural design, promising advantages, and classification. Subsequently, we present recent advances and identify a series of challenges and opportunities. Additionally, we consider a specific case study where beamforming is designed using four different algorithms, and we analyze their performance with respect to sum rate and computation cost. To augment the beamforming capabilities in 6G BD-RIS with quantum enhancement, we analyze various hybrid quantum-classical machine learning (ML) models to improve beam prediction performance, employing real-world communication Scenario 8 from the DeepSense 6G dataset. Consequently, we derive useful insights about the practical implications of BD-RIS.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [659] [Mesquite MoCap: Democratizing Real-Time Motion Capture with Affordable, Bodyworn IoT Sensors and WebXR SLAM](https://arxiv.org/abs/2512.22690)
*Poojan Vanani,Darsh Patel,Danyal Khorami,Siva Munaganuru,Pavan Reddy,Varun Reddy,Bhargav Raghunath,Ishrat Lallmamode,Romir Patel,Assegid Kidané,Tejaswi Gowda*

Main category: cs.MM

TL;DR: Mesquite is a low-cost, open-source inertial motion capture system built with modern web technologies, achieving high accuracy at a fraction of the cost of commercial systems.


<details>
  <summary>Details</summary>
Motivation: Motion capture systems are typically expensive and complex, restricting their use to specialized labs. This paper aims to develop an accessible and affordable system for broader applications.

Method: The authors developed a motion capture system with a network of 15 IMU sensors, a smartphone, and a browser-based visualization tool. It leverages WebGL, WebXR, WebSerial, and WebSockets for rendering, communication, and reduced latency.

Result: Mesquite achieves a mean joint-angle error of 2-5 degrees, 30 FPS, latency under 15ms, and a 99.7% packet delivery rate, all at around 5% of the cost of commercial systems.

Conclusion: The Mesquite system addresses the challenges of cost and complexity, making motion capture more accessible for applications like healthcare, virtual reality, and biomechanics. It's released as open-source under GNU GPL.

Abstract: Motion capture remains costly and complex to deploy, limiting use outside specialized laboratories. We present Mesquite, an open-source, low-cost inertial motion-capture system that combines a body-worn network of 15 IMU sensor nodes with a hip-worn Android smartphone for position tracking. A low-power wireless link streams quaternion orientations to a central USB dongle and a browser-based application for real-time visualization and recording. Built on modern web technologies -- WebGL for rendering, WebXR for SLAM, WebSerial and WebSockets for device and network I/O, and Progressive Web Apps for packaging -- the system runs cross-platform entirely in the browser. In benchmarks against a commercial optical system, Mesquite achieves mean joint-angle error of 2-5 degrees while operating at approximately 5% of the cost. The system sustains 30 frames per second with end-to-end latency under 15ms and a packet delivery rate of at least 99.7% in standard indoor environments. By leveraging IoT principles, edge processing, and a web-native stack, Mesquite lowers the barrier to motion capture for applications in entertainment, biomechanics, healthcare monitoring, human-computer interaction, and virtual reality. We release hardware designs, firmware, and software under an open-source license (GNU GPL).

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [660] [Analyzing Skill Element in Online Fantasy Cricket](https://arxiv.org/abs/2512.22254)
*Sarthak Sarkar,Supratim Das,Purushottam Saha,Diganta Mukherjee,Tridib Mukherjee*

Main category: cs.GT

TL;DR: This paper examines whether outcomes in online fantasy cricket are determined by skill or chance using statistical analysis and numerical experiments.


<details>
  <summary>Details</summary>
Motivation: To understand the extent to which skill, rather than chance, determines success in online fantasy cricket platforms.

Method: The authors used deterministic and stochastic strategies, statistical optimization techniques, and dynamic tournament models to examine success under different contest structures.

Result: The numerical experiments on the IPL 2024 dataset suggest that skill plays a significant role in the outcomes on these platforms.

Conclusion: This study provides evidence that skill elements significantly influence success in online fantasy cricket, emphasizing its strategic aspects.

Abstract: Online fantasy cricket has emerged as large-scale competitive systems in which participants construct virtual teams and compete based on real-world player performances. This massive growth has been accompanied by important questions about whether outcomes are primarily driven by skill or chance. We develop a statistical framework to assess the role of skill in determining success on these platforms. We construct and analyze a range of deterministic and stochastic team selection strategies, based on recent form, historical statistics, statistical optimization, and multi-criteria decision making. Strategy performance is evaluated based on points, ranks, and payoff under two contest structures Mega and 4x or Nothing. An extensive comparison between different strategies is made to find an optimal set of strategies. To capture adaptive behavior, we further introduce a dynamic tournament model in which agent populations evolve through a softmax reweighting mechanism proportional to positive payoff realizations. We demonstrate our work by running extensive numerical experiments on the IPL 2024 dataset. The results provide quantitative evidence in favor of the skill element present in online fantasy cricket platforms.

</details>


### [661] [Computing Pure-Strategy Nash Equilibria in a Two-Party Policy Competition: Existence and Algorithmic Approaches](https://arxiv.org/abs/2512.22552)
*Chuang-Chieh Lin,Chi-Jen Lu,Po-An Chen,Chih-Chieh Hung*

Main category: cs.GT

TL;DR: The paper models two-party policy competition as a non-cooperative game where parties choose strategies from a policy space, proving the existence of Nash equilibrium and proposing algorithms to find approximations.


<details>
  <summary>Details</summary>
Motivation: To explore political policy competition as a mathematical framework, addressing uncertainty and decision-making informed by voters' preferences.

Method: Policy competition is modeled as a game where parties choose policy vectors, and utility comparisons drive voter preferences. Affine isotonicity is introduced as a critical assumption. Simulations and mathematical proofs are applied to validate hypotheses.

Result: The authors validate isotonicity, prove the existence of a pure-strategy Nash equilibrium, and demonstrate the effectiveness of gradient-based and grid-based algorithms for finding approximate equilibria.

Conclusion: The paper provides a robust mathematical model of policy competition, offering theoretical guarantees and practical algorithms to compute equilibria under general conditions.

Abstract: We formulate two-party policy competition as a two-player non-cooperative game, generalizing Lin et al.'s work (2021). Each party selects a real-valued policy vector as its strategy from a compact subset of Euclidean space, and a voter's utility for a policy is given by the inner product with their preference vector. To capture the uncertainty in the competition, we assume that a policy's winning probability increases monotonically with its total utility across all voters, and we formalize this via an affine isotonic function. A player's payoff is defined as the expected utility received by its supporters. In this work, we first test and validate the isotonicity hypothesis through voting simulations. Next, we prove the existence of a pure-strategy Nash equilibrium (PSNE) in both one- and multi-dimensional settings. Although we construct a counterexample demonstrating the game's non-monotonicity, our experiments show that a decentralized gradient-based algorithm typically converges rapidly to an approximate PSNE. Finally, we present a grid-based search algorithm that finds an $ε$-approximate PSNE of the game in time polynomial in the input size and $1/ε$.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [662] [Rethinking Leveraging Pre-Trained Multi-Layer Representations for Speaker Verification](https://arxiv.org/abs/2512.22148)
*Jin Sob Kim,Hyun Joon Park,Wooseok Shin,Sung Won Han*

Main category: cs.SD

TL;DR: This paper introduces a novel Layer Attentive Pooling (LAP) method for speaker verification, combining it with a lightweight backend model to achieve state-of-the-art performance and faster training.


<details>
  <summary>Details</summary>
Motivation: Few studies have explored advancements beyond static weighted averages for aggregating multi-level features from pre-trained Transformer models in speaker verification.

Method: The proposed method, LAP, dynamically assesses layer significance using max pooling. It is integrated into a lightweight backend speaker model with Attentive Statistical Temporal Pooling (ASTP).

Result: Experiments on the VoxCeleb benchmark show state-of-the-art performance with significantly reduced training time compared to existing methods.

Conclusion: LAP provides an effective way to aggregate inter-layer features dynamically, enhancing speaker verification performance with improved efficiency.

Abstract: Recent speaker verification studies have achieved notable success by leveraging layer-wise output from pre-trained Transformer models. However, few have explored the advancements in aggregating these multi-level features beyond the static weighted average. We present Layer Attentive Pooling (LAP), a novel strategy for aggregating inter-layer representations from pre-trained speech models for speaker verification. LAP assesses the significance of each layer from multiple perspectives time-dynamically, and employs max pooling instead of averaging. Additionally, we propose a lightweight backend speaker model comprising LAP and Attentive Statistical Temporal Pooling (ASTP) to extract speaker embeddings from pre-trained model output. Experiments on the VoxCeleb benchmark reveal that our compact architecture achieves state-of-the-art performance while greatly reducing the training time. We further analyzed LAP design and its dynamic weighting mechanism for capturing speaker characteristics.

</details>


### [663] [Chord Recognition with Deep Learning](https://arxiv.org/abs/2512.22621)
*Pierre Mackenzie*

Main category: cs.SD

TL;DR: Progress in automatic chord recognition has been slow with deep learning. Experiments show classifiers struggle with rare chords, pitch augmentation aids accuracy, synthetic data holds promise, and model interpretability is improved with beat detection.


<details>
  <summary>Details</summary>
Motivation: To investigate the slow progress of automatic chord recognition and explore ways to improve it using generative models and other advancements.

Method: Experiments conducted include analyzing errors in classifiers, testing pitch augmentation, assessing features from generative models, and exploring synthetic data potential. Beat detection is incorporated to enhance model interpretability.

Result: Findings reveal poor performance on rare chords, benefits of pitch augmentation, ineffectiveness of features from generative models, promise of synthetic data, and enhanced interpretability via beat detection.

Conclusion: Much progress is needed in solving automatic chord recognition. While the study provides some of the best results and highlights promising directions, challenges remain. The work offers a guide for future exploration.

Abstract: Progress in automatic chord recognition has been slow since the advent of deep learning in the field. To understand why, I conduct experiments on existing methods and test hypotheses enabled by recent developments in generative models. Findings show that chord classifiers perform poorly on rare chords and that pitch augmentation boosts accuracy. Features extracted from generative models do not help and synthetic data presents an exciting avenue for future work. I conclude by improving the interpretability of model outputs with beat detection, reporting some of the best results in the field and providing qualitative analysis. Much work remains to solve automatic chord recognition, but I hope this thesis will chart a path for others to try.

</details>


### [664] [Mobile-Efficient Speech Emotion Recognition Using DistilHuBERT: A Cross-Corpus Validation Study](https://arxiv.org/abs/2512.23435)
*Saifelden M. Ismail*

Main category: cs.SD

TL;DR: The paper introduces a Speech Emotion Recognition system using a distilled and quantized transformer called DistilHuBERT, which significantly reduces computational demands while maintaining competitive accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to make state-of-the-art SER systems feasible for mobile applications by addressing the challenge of computational constraints.

Method: The method involves using DistilHuBERT for SER, conducting LOSO cross-validation on the IEMOCAP dataset, and training on CREMA-D for enhanced generalization. Cross-corpus evaluation is performed for robustness insights.

Result: Results include 61.4% Unweighted Accuracy, parameter reduction of 92%, improvements in Weighted Accuracy and Macro F1-score, and effective performance even under challenging datasets like RAVDESS.

Conclusion: This work demonstrates how a carefully distilled and quantized model can achieve a balance between size and accuracy, making SER systems practical for mobile deployment.

Abstract: Speech Emotion Recognition (SER) has significant potential for mobile applications, yet deployment remains constrained by the computational demands of state-of-the-art transformer architectures. This paper presents a mobile-efficient SER system based on DistilHuBERT, a distilled and 8-bit quantized transformer that achieves 92% parameter reduction compared to full-scale Wav2Vec 2.0 models while maintaining competitive accuracy. We conduct a rigorous 5-fold Leave-One-Session-Out (LOSO) cross-validation on the IEMOCAP dataset to ensure speaker independence, augmented with cross-corpus training on CREMA-D to enhance generalization. Cross-corpus training with CREMA-D yields a 1.2% improvement in Weighted Accuracy, a 1.4% gain in Macro F1-score, and a 32% reduction in cross-fold variance, with the Neutral class showing the most substantial benefit at 5.4% F1-score improvement. Our approach achieves an Unweighted Accuracy of 61.4% with a quantized model footprint of only 23 MB, representing approximately 91% of full-scale baseline performance. Cross-corpus evaluation on RAVDESS reveals that the theatrical nature of acted emotions causes predictions to cluster by arousal level rather than valence: happiness is systematically confused with anger due to acoustic saturation in high-energy expressions. Despite this theatricality effect reducing overall RAVDESS accuracy to 43.29%, the model maintains robust arousal detection with 97% recall for anger and 64% for sadness. These findings establish a Pareto-optimal tradeoff between model size and accuracy, enabling practical affect recognition on resource-constrained mobile devices.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [665] [Geometry-Aware Optimization for Respiratory Sound Classification: Enhancing Sensitivity with SAM-Optimized Audio Spectrogram Transformers](https://arxiv.org/abs/2512.22564)
*Atakan Işık,Selin Vulga Işık,Ahmet Feridun Işık,Mahşuk Taylan*

Main category: eess.AS

TL;DR: The paper introduces a modified Audio Spectrogram Transformer (AST) enhanced with Sharpness-Aware Minimization (SAM) to improve generalization on the ICBHI 2017 benchmark for respiratory sound classification.


<details>
  <summary>Details</summary>
Motivation: Address limitations of existing medical datasets (limited size, high noise, class imbalance) and improve generalization capabilities of Transformer-based models while reducing overfitting.

Method: The proposed framework utilizes Sharpness-Aware Minimization (SAM) to optimize the loss landscape and incorporates a weighted sampling strategy to manage class imbalance. Evaluation with t-SNE and attention maps demonstrates robust feature learning.

Result: Achieved state-of-the-art score of 68.10% and a sensitivity score of 68.31% on the ICBHI 2017 dataset, outperforming previous benchmarks.

Conclusion: The proposed approach effectively addresses dataset challenges and enhances feature robustness, leading to improved performance and reliability in clinical applications.

Abstract: Respiratory sound classification is hindered by the limited size, high noise levels, and severe class imbalance of benchmark datasets like ICBHI 2017. While Transformer-based models offer powerful feature extraction capabilities, they are prone to overfitting and often converge to sharp minima in the loss landscape when trained on such constrained medical data. To address this, we introduce a framework that enhances the Audio Spectrogram Transformer (AST) using Sharpness-Aware Minimization (SAM). Instead of merely minimizing the training loss, our approach optimizes the geometry of the loss surface, guiding the model toward flatter minima that generalize better to unseen patients. We also implement a weighted sampling strategy to handle class imbalance effectively. Our method achieves a state-of-the-art score of 68.10% on the ICBHI 2017 dataset, outperforming existing CNN and hybrid baselines. More importantly, it reaches a sensitivity of 68.31%, a crucial improvement for reliable clinical screening. Further analysis using t-SNE and attention maps confirms that the model learns robust, discriminative features rather than memorizing background noise.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [666] [Alpha-R1: Alpha Screening with LLM Reasoning via Reinforcement Learning](https://arxiv.org/abs/2512.23515)
*Zuoyou Jiang,Li Zhao,Rui Sun,Ruohan Sun,Zhongjian Li,Jing Li,Daxin Jiang,Zuo Bai,Cheng Hua*

Main category: q-fin.TR

TL;DR: This paper introduces Alpha-R1, a reasoning model that improves investment strategies by evaluating the economic relevance of alpha factors using context-aware reasoning and real-time news.


<details>
  <summary>Details</summary>
Motivation: Traditional investment models often fail to adapt to non-stationary markets and changing economic conditions, as they don't incorporate dynamic, semantic reasoning.

Method: Alpha-R1 is an 8-billion-parameter reasoning model trained with reinforcement learning to assess and activate/deactivate alpha factors based on economic context and real-time news.

Result: Alpha-R1 achieves better performance and robustness compared to benchmark models, showing resilience to alpha signal decay across multiple asset pools.

Conclusion: The paper demonstrates that reasoning models like Alpha-R1, which incorporate economic logic and real-time information, can significantly enhance robustness and performance in factor-based investment strategies.

Abstract: Signal decay and regime shifts pose recurring challenges for data-driven investment strategies in non-stationary markets. Conventional time-series and machine learning approaches, which rely primarily on historical correlations, often struggle to generalize when the economic environment changes. While large language models (LLMs) offer strong capabilities for processing unstructured information, their potential to support quantitative factor screening through explicit economic reasoning remains underexplored. Existing factor-based methods typically reduce alphas to numerical time series, overlooking the semantic rationale that determines when a factor is economically relevant. We propose Alpha-R1, an 8B-parameter reasoning model trained via reinforcement learning for context-aware alpha screening. Alpha-R1 reasons over factor logic and real-time news to evaluate alpha relevance under changing market conditions, selectively activating or deactivating factors based on contextual consistency. Empirical results across multiple asset pools show that Alpha-R1 consistently outperforms benchmark strategies and exhibits improved robustness to alpha decay. The full implementation and resources are available at https://github.com/FinStep-AI/Alpha-R1.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [667] [Adaptive Fusion Graph Network for 3D Strain Field Prediction in Solid Rocket Motor Grains](https://arxiv.org/abs/2512.23443)
*Jiada Huang,Hao Ma,Zhibin Shen,Yizhou Qiao,Haiyang Li*

Main category: physics.app-ph

TL;DR: This paper introduces GrainGNet, an adaptive graph network that addresses local high strain in solid rocket motor grains, significantly improving prediction accuracy and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to mitigate structural failure in solid rocket motor grains caused by local high strain, where existing numerical methods are computationally intense and surrogate models are inadequate for precise geometric modeling and high-strain region detection.

Method: GrainGNet employs adaptive pooling for dynamic node selection to retain critical mechanical features and uses feature fusion for transmitting deep features, effectively enhancing the model's predictive ability.

Result: GrainGNet achieves a 62.8% reduction in mean squared error compared to baseline methods, improves training efficiency sevenfold, and lowers prediction error in high-strain regions by 33%.

Conclusion: GrainGNet provides an accurate, efficient tool for evaluating the structural safety of rocket motors, advancing computational methods in this domain.

Abstract: Local high strain in solid rocket motor grains is a primary cause of structural failure. However, traditional numerical simulations are computationally expensive, and existing surrogate models cannot explicitly establish geometric models and accurately capture high-strain regions. Therefore, this paper proposes an adaptive graph network, GrainGNet, which employs an adaptive pooling dynamic node selection mechanism to effectively preserve the key mechanical features of structurally critical regions, while concurrently utilising feature fusion to transmit deep features and enhance the model's representational capacity. In the joint prediction task involving four sequential conditions--curing and cooling, storage, overloading, and ignition--GrainGNet reduces the mean squared error by 62.8% compared to the baseline graph U-Net model, with only a 5.2% increase in parameter count and an approximately sevenfold improvement in training efficiency. Furthermore, in the high-strain regions of debonding seams, the prediction error is further reduced by 33% compared to the second-best method, offering a computationally efficient and high-fidelity approach to evaluate motor structural safety.

</details>
