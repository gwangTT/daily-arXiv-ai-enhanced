<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 8]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.CL](#cs.CL) [Total: 8]
- [cs.CV](#cs.CV) [Total: 7]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.LG](#cs.LG) [Total: 9]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.PF](#cs.PF) [Total: 2]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.RO](#cs.RO) [Total: 7]
- [cs.SE](#cs.SE) [Total: 8]
- [q-bio.NC](#q-bio.NC) [Total: 4]
- [stat.ML](#stat.ML) [Total: 6]
- [cs.DB](#cs.DB) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Nearest-Better Network for Visualizing and Analyzing Combinatorial Optimization Problems: A Unified Tool](https://arxiv.org/abs/2507.22440)
*Yiya Diao,Changhe Li,Sanyou Zeng,Xinye Cai,Wenjian Luo,Shengxiang Yang,Carlos A. Coello Coello*

Main category: cs.AI

TL;DR: The paper introduces an efficient method to compute Nearest-Better Networks (NBN) and applies it to uncover insights into OneMax and TSP problems, highlighting challenges for specific algorithms.


<details>
  <summary>Details</summary>
Motivation: Analyze algorithm behaviors by addressing computational inefficiency of Nearest-Better Networks and extending its application to combinatorial optimization problems.

Method: Proposes a logarithmic linear time complexity algorithm for efficient computation of NBN and demonstrates it through application to OneMax and TSP.

Result: Findings reveal that OneMax fitness landscapes exhibit neutrality, ruggedness, and modality. TSP challenges include ruggedness, modality, and deception, affecting algorithm performance like EAX and LKH.

Conclusion: The efficient NBN algorithm provides novel insights into optimization problems, revealing challenges affecting algorithm efficacy and suggesting improvement areas.

Abstract: The Nearest-Better Network (NBN) is a powerful method to visualize sampled
data for continuous optimization problems while preserving multiple landscape
features. However, the calculation of NBN is very time-consuming, and the
extension of the method to combinatorial optimization problems is challenging
but very important for analyzing the algorithm's behavior. This paper provides
a straightforward theoretical derivation showing that the NBN network
essentially functions as the maximum probability transition network for
algorithms. This paper also presents an efficient NBN computation method with
logarithmic linear time complexity to address the time-consuming issue. By
applying this efficient NBN algorithm to the OneMax problem and the Traveling
Salesman Problem (TSP), we have made several remarkable discoveries for the
first time: The fitness landscape of OneMax exhibits neutrality, ruggedness,
and modality features. The primary challenges of TSP problems are ruggedness,
modality, and deception. Two state-of-the-art TSP algorithms (i.e., EAX and
LKH) have limitations when addressing challenges related to modality and
deception, respectively. LKH, based on local search operators, fails when there
are deceptive solutions near global optima. EAX, which is based on a single
population, can efficiently maintain diversity. However, when multiple
attraction basins exist, EAX retains individuals within multiple basins
simultaneously, reducing inter-basin interaction efficiency and leading to
algorithm's stagnation.

</details>


### [2] [When Truthful Representations Flip Under Deceptive Instructions?](https://arxiv.org/abs/2507.22149)
*Xianxuan Long,Yao Fu,Runchao Li,Mu Sheng,Haotian Yu,Xiaotian Han,Pan Li*

Main category: cs.AI

TL;DR: This paper investigates how malicious instructions impact the internal representations of large language models (LLMs), using techniques like Sparse Autoencoders and linear probes to identify representational shifts, particularly under deceptive instructions.


<details>
  <summary>Details</summary>
Motivation: The study aims to address safety concerns with LLMs by understanding how deceptive instructions affect their internal states beyond output-level analysis, which is currently underexplored.

Method: The researchers analyze internal representations of two LLMs (Llama-3.1-8B-Instruct and Gemma-2-9B-Instruct) using a factual verification task. Methods include predicting model outputs using linear probes and analyzing representational shifts through Sparse Autoencoders across different instruction settings.

Result: Deceptive instructions induce significant shifts in internal representations, concentrated in early-to-mid layers. While Truthful and Neutral instructions result in similar representations, specific Sparse Autoencoder features are highly sensitive to deception, showing distinct subspaces between truthful and deceptive instructions.

Conclusion: This research identifies layer-wise and feature-level indicators of instructed dishonesty, providing insights to enhance detection and control of deceptive behaviors in LLMs.

Abstract: Large language models (LLMs) tend to follow maliciously crafted instructions
to generate deceptive responses, posing safety challenges. How deceptive
instructions alter the internal representations of LLM compared to truthful
ones remains poorly understood beyond output analysis. To bridge this gap, we
investigate when and how these representations ``flip'', such as from truthful
to deceptive, under deceptive versus truthful/neutral instructions. Analyzing
the internal representations of Llama-3.1-8B-Instruct and Gemma-2-9B-Instruct
on a factual verification task, we find the model's instructed True/False
output is predictable via linear probes across all conditions based on the
internal representation. Further, we use Sparse Autoencoders (SAEs) to show
that the Deceptive instructions induce significant representational shifts
compared to Truthful/Neutral representations (which are similar), concentrated
in early-to-mid layers and detectable even on complex datasets. We also
identify specific SAE features highly sensitive to deceptive instruction and
use targeted visualizations to confirm distinct truthful/deceptive
representational subspaces. % Our analysis pinpoints layer-wise and
feature-level correlates of instructed dishonesty, offering insights for LLM
detection and control. Our findings expose feature- and layer-level signatures
of deception, offering new insights for detecting and mitigating instructed
dishonesty in LLMs.

</details>


### [3] [Explainability Through Systematicity: The Hard Systematicity Challenge for Artificial Intelligence](https://arxiv.org/abs/2507.22197)
*Matthieu Queloz*

Main category: cs.AI

TL;DR: The paper examines the concept of systematicity in AI, arguing that it goes beyond mere explainability and introduces a richer framework for understanding the systematicity of thought, offering a new challenge for AI systems.


<details>
  <summary>Details</summary>
Motivation: To address the long-standing tension between systematicity and connectionism in AI thought and to refine what systematicity means beyond the traditional Fodorian perspective.

Method: The author develops a conceptual framework that identifies four senses of the systematicity of thought, contrasts them with the traditional 'systematicity challenge' to connectionism, and applies five rationales for systematization to evaluate AI models.

Result: The study reframes the perceived conflict between systematicity and connectionism, introduces the 'hard systematicity challenge' for AI, and provides a dynamic guide to determine how much systematicity AI systems should exhibit.

Conclusion: AI models need to strive towards systematicity in a nuanced and regulated manner, based on rationales for systematization, rather than adhering to a rigid or overly simplistic ideal.

Abstract: This paper argues that explainability is only one facet of a broader ideal
that shapes our expectations towards artificial intelligence (AI).
Fundamentally, the issue is to what extent AI exhibits systematicity--not
merely in being sensitive to how thoughts are composed of recombinable
constituents, but in striving towards an integrated body of thought that is
consistent, coherent, comprehensive, and parsimoniously principled. This richer
conception of systematicity has been obscured by the long shadow of the
"systematicity challenge" to connectionism, according to which network
architectures are fundamentally at odds with what Fodor and colleagues termed
"the systematicity of thought." I offer a conceptual framework for thinking
about "the systematicity of thought" that distinguishes four senses of the
phrase. I use these distinctions to defuse the perceived tension between
systematicity and connectionism and show that the conception of systematicity
that historically shaped our sense of what makes thought rational,
authoritative, and scientific is more demanding than the Fodorian notion. To
determine whether we have reason to hold AI models to this ideal of
systematicity, I then argue, we must look to the rationales for systematization
and explore to what extent they transfer to AI models. I identify five such
rationales and apply them to AI. This brings into view the "hard systematicity
challenge." However, the demand for systematization itself needs to be
regulated by the rationales for systematization. This yields a dynamic
understanding of the need to systematize thought, which tells us how systematic
we need AI models to be and when.

</details>


### [4] [CoEx -- Co-evolving World-model and Exploration](https://arxiv.org/abs/2507.22281)
*Minsoo Kim,Seung-won Hwang*

Main category: cs.AI

TL;DR: Modern LLM agents struggle to adapt their static world models to dynamic environments, leading to flawed plans. CoEx is introduced as a hierarchical architecture enabling adaptive planning and persistent world model updates, showing superior performance in complex scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing LLM agent designs fail to account for dynamically changing environments, leading to misaligned plans due to a reliance on static internal models.

Method: The paper introduces CoEx, a hierarchical architecture leveraging state abstraction, LLM reasoning, and a neurosymbolic belief state for dynamic adaptation and persistent memory updates.

Result: CoEx demonstrates superior planning and exploration capabilities compared to existing paradigms across diverse and complex tasks.

Conclusion: Through its dynamic planning and belief state mechanism, CoEx bridges the gap between static world models and evolving environments, improving task performance.

Abstract: Planning in modern LLM agents relies on the utilization of LLM as an internal
world model, acquired during pretraining. However, existing agent designs fail
to effectively assimilate new observations into dynamic updates of the world
model. This reliance on the LLM's static internal world model is progressively
prone to misalignment with the underlying true state of the world, leading to
the generation of divergent and erroneous plans. We introduce a hierarchical
agent architecture, CoEx, in which hierarchical state abstraction allows LLM
planning to co-evolve with a dynamically updated model of the world. CoEx plans
and interacts with the world by using LLM reasoning to orchestrate dynamic
plans consisting of subgoals, and its learning mechanism continuously
incorporates these subgoal experiences into a persistent world model in the
form of a neurosymbolic belief state, comprising textual inferences and
code-based symbolic memory. We evaluate our agent across a diverse set of agent
scenarios involving rich environments and complex tasks including ALFWorld,
PDDL, and Jericho. Our experiments show that CoEx outperforms existing agent
paradigms in planning and exploration.

</details>


### [5] [An Explainable Emotion Alignment Framework for LLM-Empowered Agent in Metaverse Service Ecosystem](https://arxiv.org/abs/2507.22326)
*Qun Ma,Xiao Xue,Ming Zhang,Yifan Shen,Zihan Zhao*

Main category: cs.AI

TL;DR: This paper introduces an explainable emotion alignment framework to enhance large language model-based agents in the Metaverse service ecosystem, focusing on bridging virtual and real-world services through improved decision-making and alignment.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges faced by LLM-based agents in Metaverse services, such as character data fusion, knowledge association, and ethical safety, which hinder seamless integration between virtual and real-world services.

Method: The proposed method is an explainable emotion alignment framework designed to integrate factual factors into the decision-making loop of LLM-based agents. It demonstrates relational fact alignment and is evaluated through a simulation experiment.

Result: The simulation experiment, based on an Offline-to-Offline food delivery scenario, shows that the framework enhances the realism and social emergence of agent interactions.

Conclusion: The framework effectively improves the decision-making of LLM-based agents within the Metaverse service ecosystem, facilitating better integration of services and addressing key challenges.

Abstract: Metaverse service is a product of the convergence between Metaverse and
service systems, designed to address service-related challenges concerning
digital avatars, digital twins, and digital natives within Metaverse. With the
rise of large language models (LLMs), agents now play a pivotal role in
Metaverse service ecosystem, serving dual functions: as digital avatars
representing users in the virtual realm and as service assistants (or NPCs)
providing personalized support. However, during the modeling of Metaverse
service ecosystems, existing LLM-based agents face significant challenges in
bridging virtual-world services with real-world services, particularly
regarding issues such as character data fusion, character knowledge
association, and ethical safety concerns. This paper proposes an explainable
emotion alignment framework for LLM-based agents in Metaverse Service
Ecosystem. It aims to integrate factual factors into the decision-making loop
of LLM-based agents, systematically demonstrating how to achieve more
relational fact alignment for these agents. Finally, a simulation experiment in
the Offline-to-Offline food delivery scenario is conducted to evaluate the
effectiveness of this framework, obtaining more realistic social emergence.

</details>


### [6] [Magentic-UI: Towards Human-in-the-loop Agentic Systems](https://arxiv.org/abs/2507.22358)
*Hussein Mozannar,Gagan Bansal,Cheng Tan,Adam Fourney,Victor Dibia,Jingya Chen,Jack Gerrits,Tyler Payne,Matheus Kunzler Maldaner,Madeleine Grunde-McLaughlin,Eric Zhu,Griffin Bassman,Jacob Alber,Peter Chang,Ricky Loynd,Friederike Niedtner,Ece Kamar,Maya Murad,Rafah Hosn,Saleema Amershi*

Main category: cs.AI

TL;DR: Magentic-UI is a human-centered system combining AI agents and user oversight for safer, efficient task execution and collaboration.


<details>
  <summary>Details</summary>
Motivation: To address the limitations and risks of autonomous AI agents, including inefficiencies and safety issues in performing complex multi-step tasks.

Method: Developing Magentic-UI, a flexible open-source interface allowing human-AI collaboration through interaction mechanisms such as co-planning and action guards.

Result: Magentic-UI demonstrates improved human-AI collaboration via evaluations on task benchmarks, simulated and real user testing, and safety assessments.

Conclusion: Human-controlled AI systems like Magentic-UI offer a promising solution for advancing safe, efficient collaboration between humans and AI agents.

Abstract: AI agents powered by large language models are increasingly capable of
autonomously completing complex, multi-step tasks using external tools. Yet,
they still fall short of human-level performance in most domains including
computer use, software development, and research. Their growing autonomy and
ability to interact with the outside world, also introduces safety and security
risks including potentially misaligned actions and adversarial manipulation. We
argue that human-in-the-loop agentic systems offer a promising path forward,
combining human oversight and control with AI efficiency to unlock productivity
from imperfect systems. We introduce Magentic-UI, an open-source web interface
for developing and studying human-agent interaction. Built on a flexible
multi-agent architecture, Magentic-UI supports web browsing, code execution,
and file manipulation, and can be extended with diverse tools via Model Context
Protocol (MCP). Moreover, Magentic-UI presents six interaction mechanisms for
enabling effective, low-cost human involvement: co-planning, co-tasking,
multi-tasking, action guards, and long-term memory. We evaluate Magentic-UI
across four dimensions: autonomous task completion on agentic benchmarks,
simulated user testing of its interaction capabilities, qualitative studies
with real users, and targeted safety assessments. Our findings highlight
Magentic-UI's potential to advance safe and efficient human-agent
collaboration.

</details>


### [7] [LLM-Crowdsourced: A Benchmark-Free Paradigm for Mutual Evaluation of Large Language Models](https://arxiv.org/abs/2507.22359)
*Qianhong Guo,Wei Xie,Xiaofang Cai,Enze Wang,Shuoyoucheng Ma,Kai Chen,Xiaofeng Wang,Baosheng Wang*

Main category: cs.AI

TL;DR: This paper proposes a new benchmark-free evaluation method for large language models (LLMs) by crowdsourcing their capabilities to assess themselves, addressing current evaluation challenges like data contamination, subjectivity, and black-box issues.


<details>
  <summary>Details</summary>
Motivation: Existing methods for evaluating LLMs face difficulties such as lack of comprehensive benchmarks and issues like data contamination and subjectivity, preventing fair assessment.

Method: The authors designed an evaluation paradigm called LLM-Crowdsourced, where LLMs generate, answer, and evaluate questions independently using criteria like transparency, objectivity, and professionalism.

Result: The method was tested on eight major LLMs for mathematics and programming tasks, showing its effectiveness in distinguishing LLM performances while detecting unique behaviors like memorization-based answering.

Conclusion: The LLM-Crowdsourced evaluation paradigm is dynamic, transparent, and objective, making it superior to traditional evaluation methods while highlighting novel findings about LLM behaviors.

Abstract: Although large language models (LLMs) demonstrate remarkable capabilities
across various tasks, evaluating their capabilities remains a challenging task.
Existing evaluation methods suffer from issues such as data contamination,
black-box operation, and subjective preference. These issues make it difficult
to evaluate the LLMs' true capabilities comprehensively. To tackle these
challenges, we propose a novel benchmark-free evaluation paradigm,
LLM-Crowdsourced. It utilizes LLMs to generate questions, answer independently,
and evaluate mutually. This method integrates four key evaluation criteria:
dynamic, transparent, objective, and professional, which existing evaluation
methods cannot satisfy simultaneously. Experiments on eight mainstream LLMs
across mathematics and programming verify the advantages of our method in
distinguishing LLM performance. Furthermore, our study reveals several novel
findings that are difficult for traditional methods to detect, including but
not limited to: (1) Gemini demonstrates the highest original and professional
question-design capabilities among others; (2) Some LLMs exhibit
''memorization-based answering'' by misrecognizing questions as familiar ones
with a similar structure; (3) LLM evaluation results demonstrate high
consistency (robustness).

</details>


### [8] [Beyond Accuracy: How AI Metacognitive Sensitivity improves AI-assisted Decision Making](https://arxiv.org/abs/2507.22365)
*ZhaoBin Li,Mark Steyvers*

Main category: cs.AI

TL;DR: This paper emphasizes the importance of AI's metacognitive sensitivity, i.e., its accurate confidence assessments, in improving human decision-making in addition to its predictive accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to understand how AI confidence estimates, alongside its predictive accuracy, impact human decision-making quality in hybrid AI-human systems.

Method: Introduces a theoretical framework, identifies conditions influencing AI-human joint decision accuracy, and validates findings through a behavioral experiment.

Result: Finds that higher AI metacognitive sensitivity, even in scenarios with lower predictive accuracy, can enhance human decision-making performance.

Conclusion: AI assistance is best evaluated and optimized by both predictive accuracy and metacognitive sensitivity to improve decision outcomes.

Abstract: In settings where human decision-making relies on AI input, both the
predictive accuracy of the AI system and the reliability of its confidence
estimates influence decision quality. We highlight the role of AI metacognitive
sensitivity -- its ability to assign confidence scores that accurately
distinguish correct from incorrect predictions -- and introduce a theoretical
framework for assessing the joint impact of AI's predictive accuracy and
metacognitive sensitivity in hybrid decision-making settings. Our analysis
identifies conditions under which an AI with lower predictive accuracy but
higher metacognitive sensitivity can enhance the overall accuracy of human
decision making. Finally, a behavioral experiment confirms that greater AI
metacognitive sensitivity improves human decision performance. Together, these
findings underscore the importance of evaluating AI assistance not only by
accuracy but also by metacognitive sensitivity, and of optimizing both to
achieve superior decision outcomes.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [9] [A Customized Memory-aware Architecture for Biological Sequence Alignment](https://arxiv.org/abs/2507.22221)
*Nasrin Akbari,Mehdi Modarressi,Alireza Khadem*

Main category: cs.AR

TL;DR: The paper proposes a processing-in-memory architecture to address memory bandwidth limitations in sequence alignment algorithms, achieving up to 2.4x speedup over GPU designs and 37% average power reduction.


<details>
  <summary>Details</summary>
Motivation: The motivation is rooted in the challenges posed by the growth of bioinformatics data, which demands higher throughput while overcoming the memory bandwidth limitations of sequence alignment algorithms.

Method: The method involves designing a memory-aware architecture integrated into the logic layer of 3D DRAM for processing-in-memory, optimizing bandwidth and computational efficiency.

Result: Experimental results show the proposed architecture delivers up to 2.4x speed improvement over GPU-based designs and reduces power consumption by 37% on average.

Conclusion: The study concludes that integrating computation closer to memory through processing-in-memory allows significant performance and power efficiency improvements in sequence alignment tasks.

Abstract: Sequence alignment is a fundamental process in computational biology which
identifies regions of similarity in biological sequences. With the exponential
growth in the volume of data in bioinformatics databases, the time, processing
power, and memory bandwidth for comparing a query sequence with the available
databases grows proportionally. The sequence alignment algorithms often involve
simple arithmetic operations and feature high degrees of inherent fine-grained
and coarse-grained parallelism. These features can be potentially exploited by
a massive parallel processor, such as a GPU, to increase throughput. In this
paper, we show that the excessive memory bandwidth demand of the sequence
alignment algorithms prevents exploiting the maximum achievable throughput on
conventional parallel machines. We then propose a memory-aware architecture to
reduce the bandwidth demand of the sequence alignment algorithms, effectively
pushing the memory wall to extract higher throughput. The design is integrated
at the logic layer of an emerging 3D DRAM as a processing-in-memory
architecture to further increase the available bandwidth. The experimental
results show that the proposed architecture results in up to 2.4x speedup over
a GPU-based design. Moreover, by moving the computation closer to the memory,
power consumption is reduced by 37%, on average.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [10] [IndoPref: A Multi-Domain Pairwise Preference Dataset for Indonesian](https://arxiv.org/abs/2507.22159)
*Vanessa Rebecca Wiyono,David Anugraha,Ayu Purwarianti,Genta Indra Winata*

Main category: cs.CL

TL;DR: IndoPref is the first human-authored Indonesian preference dataset for evaluating LLM text quality.


<details>
  <summary>Details</summary>
Motivation: Indonesian's underrepresentation in LLM preference research due to reliance on translations.

Method: Created IndoPref, a multi-domain dataset with native Indonesian annotations evaluated for inter-annotator agreement.

Result: High inter-annotator agreement (Krippendorff's alpha) and LLM benchmarking on the dataset.

Conclusion: IndoPref addresses authenticity issues in multilingual datasets and provides quality benchmarks for Indonesian LLM research.

Abstract: Over 200 million people speak Indonesian, yet the language remains
significantly underrepresented in preference-based research for large language
models (LLMs). Most existing multilingual datasets are derived from English
translations, often resulting in content that lacks cultural and linguistic
authenticity. To address this gap, we introduce IndoPref, the first fully
human-authored and multi-domain Indonesian preference dataset specifically
designed to evaluate the naturalness and quality of LLM-generated text. All
annotations are natively written in Indonesian and evaluated using
Krippendorff's alpha, demonstrating strong inter-annotator agreement.
Additionally, we benchmark the dataset across multiple LLMs and assess the
output quality of each model.

</details>


### [11] [Persona-Augmented Benchmarking: Evaluating LLMs Across Diverse Writing Styles](https://arxiv.org/abs/2507.22168)
*Kimberly Le Truong,Riccardo Fogliato,Hoda Heidari,Zhiwei Steven Wu*

Main category: cs.CL

TL;DR: Current benchmarks fail to evaluate Large Language Models (LLMs) against diverse writing styles, and this paper proposes a scalable approach using persona-based prompting to address this limitation.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks mainly follow standardized conventions and do not represent the diverse communication patterns found in human language, leading to potentially fragile LLM performance when exposed to non-standard inputs.

Method: The authors introduce persona-based LLM prompting, a cost-effective method to rewrite evaluation prompts in diverse writing styles while keeping semantic content consistent.

Result: The study reveals that variations in writing style and prompt formatting strongly influence LLM performance and identifies distinct writing styles that yield consistently high or low results, regardless of model family, size, or recency.

Conclusion: Incorporating stylistic diversity into benchmarks using this scalable method could improve the external validity of LLM evaluations and provide a better measure of their robustness across various linguistic patterns.

Abstract: Current benchmarks for evaluating Large Language Models (LLMs) often do not
exhibit enough writing style diversity, with many adhering primarily to
standardized conventions. Such benchmarks do not fully capture the rich variety
of communication patterns exhibited by humans. Thus, it is possible that LLMs,
which are optimized on these benchmarks, may demonstrate brittle performance
when faced with "non-standard" input. In this work, we test this hypothesis by
rewriting evaluation prompts using persona-based LLM prompting, a low-cost
method to emulate diverse writing styles. Our results show that, even with
identical semantic content, variations in writing style and prompt formatting
significantly impact the estimated performance of the LLM under evaluation.
Notably, we identify distinct writing styles that consistently trigger either
low or high performance across a range of models and tasks, irrespective of
model family, size, and recency. Our work offers a scalable approach to augment
existing benchmarks, improving the external validity of the assessments they
provide for measuring LLM performance across linguistic variations.

</details>


### [12] [A Scalable Pipeline for Estimating Verb Frame Frequencies Using Large Language Models](https://arxiv.org/abs/2507.22187)
*Adam M. Morgan,Adeen Flinker*

Main category: cs.CL

TL;DR: This paper introduces an automated pipeline using large language models (LLMs) for calculating Verb Frame Frequencies (VFFs) with enhanced accuracy, scalability, and accessibility compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Verb Frame Frequencies (VFFs) play a significant role in understanding syntax in both human and machine languages, but existing methods for calculating VFFs face limitations in scale, accuracy, or accessibility.

Method: The authors leverage LLMs to generate a corpus of sentences for 476 English verbs and then analyze these sentences syntactically using LLMs trained to mimic expert linguists. This pipeline is benchmarked against widely used syntactic parsers.

Result: The pipeline outperformed existing syntactic parsers in multiple evaluations and required fewer resources compared to manual parsing methods. It enabled the creation of an extensive VFF database with finer syntactic distinctions and broader verb coverage.

Conclusion: This approach demonstrates the feasibility of automated VFF estimation using LLMs, offering high scalability and customization potential for various applications, including psycholinguistics and cross-linguistic studies.

Abstract: We present an automated pipeline for estimating Verb Frame Frequencies
(VFFs), the frequency with which a verb appears in particular syntactic frames.
VFFs provide a powerful window into syntax in both human and machine language
systems, but existing tools for calculating them are limited in scale,
accuracy, or accessibility. We use large language models (LLMs) to generate a
corpus of sentences containing 476 English verbs. Next, by instructing an LLM
to behave like an expert linguist, we had it analyze the syntactic structure of
the sentences in this corpus. This pipeline outperforms two widely used
syntactic parsers across multiple evaluation datasets. Furthermore, it requires
far fewer resources than manual parsing (the gold-standard), thereby enabling
rapid, scalable VFF estimation. Using the LLM parser, we produce a new VFF
database with broader verb coverage, finer-grained syntactic distinctions, and
explicit estimates of the relative frequencies of structural alternates
commonly studied in psycholinguistics. The pipeline is easily customizable and
extensible to new verbs, syntactic frames, and even other languages. We present
this work as a proof of concept for automated frame frequency estimation, and
release all code and data to support future research.

</details>


### [13] [The role of media memorability in facilitating startups' access to venture capital funding](https://arxiv.org/abs/2507.22201)
*L. Toschi,S. Torrisi,A. Fronzetti Colladon*

Main category: cs.CL

TL;DR: The study explores how media memorability, beyond sheer media exposure, influences venture capital investment decisions for startups.


<details>
  <summary>Details</summary>
Motivation: To address the gap in understanding how nuanced aspects of media content impact funding decisions by venture capitalists.

Method: Analysis of 197 UK startups in the micro and nanotechnology sector, funded between 1995 and 2004, assessing their media memorability and its relation to investment outcomes.

Result: Media memorability—focused on the distinctiveness and connectivity of startups in semantic networks—significantly influences venture capital investment decisions.

Conclusion: Startups should prioritize meaningful media coverage to enhance memorability, emphasizing their distinctiveness and relevance within industry narratives to attract venture funding.

Abstract: Media reputation plays an important role in attracting venture capital
investment. However, prior research has focused too narrowly on general media
exposure, limiting our understanding of how media truly influences funding
decisions. As informed decision-makers, venture capitalists respond to more
nuanced aspects of media content. We introduce the concept of media
memorability - the media's ability to imprint a startup's name in the memory of
relevant investors. Using data from 197 UK startups in the micro and
nanotechnology sector (funded between 1995 and 2004), we show that media
memorability significantly influences investment outcomes. Our findings suggest
that venture capitalists rely on detailed cues such as a startup's
distinctiveness and connectivity within news semantic networks. This
contributes to research on entrepreneurial finance and media legitimation. In
practice, startups should go beyond frequent media mentions to strengthen brand
memorability through more targeted, meaningful coverage highlighting their
uniqueness and relevance within the broader industry conversation.

</details>


### [14] [How Well Does First-Token Entropy Approximate Word Entropy as a Psycholinguistic Predictor?](https://arxiv.org/abs/2507.22209)
*Christian Clark,Byung-Doh Oh,William Schuler*

Main category: cs.CL

TL;DR: The paper explores entropy measures in language processing and finds that using a word's first-token for entropy estimates can lead to inaccuracies, proposing Monte Carlo methods for better estimates.


<details>
  <summary>Details</summary>
Motivation: To address the inaccuracy and distortion in measuring word entropy using first-token probability distributions in psycholinguistics.

Method: Monte Carlo (MC) methods were used to estimate word entropy by accounting for the variability in the number of tokens a word spans.

Result: MC word entropy estimates led to divergent findings in regression experiments on reading times compared to first-token word entropy estimates.

Conclusion: First-token approximations for contextual entropy are flawed, and Monte Carlo methods offer a better estimation mechanism for psycholinguistic research.

Abstract: Contextual entropy is a psycholinguistic measure capturing the anticipated
difficulty of processing a word just before it is encountered. Recent studies
have tested for entropy-related effects as a potential complement to well-known
effects from surprisal. For convenience, entropy is typically estimated based
on a language model's probability distribution over a word's first subword
token. However, this approximation results in underestimation and potential
distortion of true word entropy. To address this, we generate Monte Carlo (MC)
estimates of word entropy that allow words to span a variable number of tokens.
Regression experiments on reading times show divergent results between
first-token and MC word entropy, suggesting a need for caution in using
first-token approximations of contextual entropy.

</details>


### [15] [RL from Teacher-Model Refinement: Gradual Imitation Learning for Machine Translation](https://arxiv.org/abs/2507.22219)
*Dongyub Jude Lee,Zhenyi Ye,Pengcheng He*

Main category: cs.CL

TL;DR: This paper proposes RLfR, an innovative machine translation method that replaces static datasets with dynamic feedback from a teacher model (GPT-4) to achieve superior translation quality.


<details>
  <summary>Details</summary>
Motivation: Current preference-learning methods heavily rely on static, curated datasets and struggle to generalize when applied outside tuning domains.

Method: The paper introduces RLfR, where a teacher model refines translations and provides rewards based on alignment with its improvement, using edit distance and COMET scores as learning signals.

Result: Experimental results on the FLORES-200 benchmark show RLfR surpasses common baselines (MT-SFT and preference-based approaches) by improving COMET and M-ETA scores.

Conclusion: RLfR demonstrates how leveraging dynamic teacher feedback can enhance translation quality while mimicking human-like incremental learning.

Abstract: Preference-learning methods for machine translation (MT)--such as Direct
Preference Optimization (DPO)--have achieved impressive gains but depend
heavily on large, carefully curated triplet datasets and often struggle to
generalize beyond their tuning domains. We propose Reinforcement Learning from
Teacher-Model Refinement (RLfR), a novel framework that removes reliance on
static triplets by leveraging continuous, high-quality feedback from an
external teacher model (GPT-4o). RLfR frames each translation step as a
micro-tutorial: the actor generates a hypothesis, the teacher refines it, and
the actor is rewarded based on how closely it aligns with the teacher's
refinement. Guided by two complementary signals--(i) negative edit distance,
promoting lexical and structural fidelity, and (ii) COMET score, ensuring
semantic adequacy--the actor progressively learns to emulate the teacher,
mirroring a human learning process through incremental, iterative improvement.
On the FLORES-200 benchmark (English to and from German, Spanish, Chinese,
Korean, and Japanese), RLfR consistently outperforms both MT-SFT and
preference-based baselines, significantly improving COMET (semantic adequacy)
and M-ETA (entity preservation) scores.

</details>


### [16] [Meaning-infused grammar: Gradient Acceptability Shapes the Geometric Representations of Constructions in LLMs](https://arxiv.org/abs/2507.22286)
*Supantho Rakshit,Adele Goldberg*

Main category: cs.CL

TL;DR: This study evaluates whether Large Language Models (LLMs) capture meaning-based gradients in language, focusing on English dative constructions. The analysis reveals that LLMs possess gradient, meaningful representations of constructions.


<details>
  <summary>Details</summary>
Motivation: Language is composed of constructions with meanings that guide their usage. The study aims to determine if LLMs represent these meaning-based, graded structures similarly to human cognition.

Method: Using the English dative constructions, the neural representations in the Pythia-1.4B LLM were analyzed through geometric measures like Energy Distance and Jensen-Shannon Divergence based on a dataset of 5000 systematically varied sentences.

Result: The analysis shows that LLMs encode constructions in a way modulated by preference strength, with more prototypical exemplars having more distinct neural representations in activation space.

Conclusion: LLMs exhibit rich, graded, meaning-infused representations of linguistic constructions, and geometric measures can quantify constructionist principles in LLMs.

Abstract: The usage-based constructionist (UCx) approach posits that language comprises
a network of learned form-meaning pairings (constructions) whose use is largely
determined by their meanings or functions, requiring them to be graded and
probabilistic. This study investigates whether the internal representations in
Large Language Models (LLMs) reflect the proposed function-infused gradience.
We analyze the neural representations of the English dative constructions
(Double Object and Prepositional Object) in Pythia-$1.4$B, using a dataset of
$5000$ sentence pairs systematically varied for human-rated preference
strength. A macro-level geometric analysis finds that the separability between
construction representations, as measured by Energy Distance or Jensen-Shannon
Divergence, is systematically modulated by gradient preference strength. More
prototypical exemplars of each construction occupy more distinct regions in the
activation space of LLMs. These results provide strong evidence that LLMs learn
rich, meaning-infused, graded representations of constructions and offer
support for geometric measures of basic constructionist principles in LLMs.

</details>


### [17] [Intent Recognition and Out-of-Scope Detection using LLMs in Multi-party Conversations](https://arxiv.org/abs/2507.22289)
*Galo Castillo-López,Gaël de Chalendar,Nasredine Semmar*

Main category: cs.CL

TL;DR: This paper proposes a hybrid approach combining BERT and large language models (LLMs) for zero and few-shot intent recognition in task-oriented dialogue systems, focusing on Out-of-Scope (OOS) detection.


<details>
  <summary>Details</summary>
Motivation: Traditional task-oriented dialogue systems require large amounts of annotated data for intent recognition and OOS detection, which can be resource-intensive.

Method: The authors combine the generalization capabilities of LLMs with BERT's computational efficiency in a hybrid framework, sharing information between the two models for better performance.

Result: Evaluation on multi-party conversation corpora shows that sharing outputs between BERT and LLMs improves the system's performance in intent recognition and OOS detection.

Conclusion: The hybrid approach effectively enhances intent recognition and OOS detection, demonstrating the advantages of integrating LLMs and BERT in low-resource settings.

Abstract: Intent recognition is a fundamental component in task-oriented dialogue
systems (TODS). Determining user intents and detecting whether an intent is
Out-of-Scope (OOS) is crucial for TODS to provide reliable responses. However,
traditional TODS require large amount of annotated data. In this work we
propose a hybrid approach to combine BERT and LLMs in zero and few-shot
settings to recognize intents and detect OOS utterances. Our approach leverages
LLMs generalization power and BERT's computational efficiency in such
scenarios. We evaluate our method on multi-party conversation corpora and
observe that sharing information from BERT outputs to LLMs leads to system
performance improvement.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [18] [Runtime Failure Hunting for Physics Engine Based Software Systems: How Far Can We Go?](https://arxiv.org/abs/2507.22099)
*Shuqing Li,Qiang Chen,Xiaoxue Ren,Michael R. Lyu*

Main category: cs.CV

TL;DR: The paper studies physics failures in Physics Engines, developing a taxonomy, evaluating detection methods, and sharing developer insights.


<details>
  <summary>Details</summary>
Motivation: Physics Engines are critical for simulations but suffer from physics failures that impact reliability, especially in safety-critical systems.

Method: A large-scale empirical study was conducted, evaluating various detection methodologies like deep learning and prompt-based approaches.

Result: Contributions include a taxonomy of physics failure manifestations, an evaluation of detection techniques, and insights from developers.

Conclusion: The study offers tools and insights that address weaknesses in detecting physics failures in software using Physics Engines and promotes future research with publicly available resources.

Abstract: Physics Engines (PEs) are fundamental software frameworks that simulate
physical interactions in applications ranging from entertainment to
safety-critical systems. Despite their importance, PEs suffer from physics
failures, deviations from expected physical behaviors that can compromise
software reliability, degrade user experience, and potentially cause critical
failures in autonomous vehicles or medical robotics. Current testing approaches
for PE-based software are inadequate, typically requiring white-box access and
focusing on crash detection rather than semantically complex physics failures.
This paper presents the first large-scale empirical study characterizing
physics failures in PE-based software. We investigate three research questions
addressing the manifestations of physics failures, the effectiveness of
detection techniques, and developer perceptions of current detection practices.
Our contributions include: (1) a taxonomy of physics failure manifestations;
(2) a comprehensive evaluation of detection methods including deep learning,
prompt-based techniques, and large multimodal models; and (3) actionable
insights from developer experiences for improving detection approaches. To
support future research, we release PhysiXFails, code, and other materials at
https://sites.google.com/view/physics-failure-detection.

</details>


### [19] [Trade-offs in Image Generation: How Do Different Dimensions Interact?](https://arxiv.org/abs/2507.22100)
*Sicheng Zhang,Binzhu Xie,Zhonghao Yan,Yuli Zhang,Donghao Zhou,Xiaofei Chen,Shi Qiu,Jiaqi Liu,Guoyang Xie,Zhichao Lu*

Main category: cs.CV

TL;DR: This paper introduces a benchmark (TRIG-Bench) and metric (TRIGScore) to evaluate trade-offs between multiple dimensions in image generation tasks, offering tools to diagnose and mitigate weaknesses in generative models.


<details>
  <summary>Details</summary>
Motivation: To address the limited understanding of trade-offs among various performance dimensions in image generation models due to dataset and measurement gaps.

Method: The researchers created TRIG-Bench, a dataset with 40,200 samples across 10 dimensions, and TRIGScore, a metric for automatic dimensional evaluation. They also developed a Dimension Trade-off Map to visualize generative model trade-offs.

Result: TRIG-Bench, TRIGScore, and DTM successfully evaluated 14 models, providing insights into their dimensional weaknesses and trade-offs and confirming the usefulness of fine-tuning to improve model performance.

Conclusion: A new framework (TRIG-Bench, TRIGScore, and DTM) enables a systematic understanding of generative model trade-offs, facilitating performance enhancement through targeted fine-tuning.

Abstract: Model performance in text-to-image (T2I) and image-to-image (I2I) generation
often depends on multiple aspects, including quality, alignment, diversity, and
robustness. However, models' complex trade-offs among these dimensions have
rarely been explored due to (1) the lack of datasets that allow fine-grained
quantification of these trade-offs, and (2) the use of a single metric for
multiple dimensions. To bridge this gap, we introduce TRIG-Bench (Trade-offs in
Image Generation), which spans 10 dimensions (Realism, Originality, Aesthetics,
Content, Relation, Style, Knowledge, Ambiguity, Toxicity, and Bias), contains
40,200 samples, and covers 132 pairwise dimensional subsets. Furthermore, we
develop TRIGScore, a VLM-as-judge metric that automatically adapts to various
dimensions. Based on TRIG-Bench and TRIGScore, we evaluate 14 models across T2I
and I2I tasks. In addition, we propose the Relation Recognition System to
generate the Dimension Trade-off Map (DTM) that visualizes the trade-offs among
model-specific capabilities. Our experiments demonstrate that DTM consistently
provides a comprehensive understanding of the trade-offs between dimensions for
each type of generative model. Notably, we show that the model's
dimension-specific weaknesses can be mitigated through fine-tuning on DTM to
enhance overall performance. Code is available at:
https://github.com/fesvhtr/TRIG

</details>


### [20] [AI in Agriculture: A Survey of Deep Learning Techniques for Crops, Fisheries and Livestock](https://arxiv.org/abs/2507.22101)
*Umair Nawaz,Muhammad Zaigham Zaheer,Fahad Shahbaz Khan,Hisham Cholakkal,Salman Khan,Rao Muhammad Anwer*

Main category: cs.CV

TL;DR: The paper reviews over 200 studies on the use of AI in agriculture, highlighting its application in crop disease detection, livestock health, and aquatic monitoring, while analyzing challenges like data variability and proposing open research directions.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges faced by agriculture, fisheries, and livestock industries, such as climate variability, resource limitations, and sustainability, by exploring the role of AI as an efficient and scalable solution.

Method: The authors conduct a systematic survey of more than 200 research works, focusing on conventional machine learning, advanced deep learning, and vision-language models in agricultural applications. They also review datasets, performance metrics, and implementation hurdles.

Result: The survey provides a comprehensive overview of the state of AI in agriculture, identifying key applications, challenges like data variability, and gaps such as the need for multimodal data integration and edge-device deployment.

Conclusion: AI offers transformative potential for agriculture, but further research is essential to overcome challenges like data variability and to develop domain-adaptable models for practical use in diverse farming environments.

Abstract: Crops, fisheries and livestock form the backbone of global food production,
essential to feed the ever-growing global population. However, these sectors
face considerable challenges, including climate variability, resource
limitations, and the need for sustainable management. Addressing these issues
requires efficient, accurate, and scalable technological solutions,
highlighting the importance of artificial intelligence (AI). This survey
presents a systematic and thorough review of more than 200 research works
covering conventional machine learning approaches, advanced deep learning
techniques (e.g., vision transformers), and recent vision-language foundation
models (e.g., CLIP) in the agriculture domain, focusing on diverse tasks such
as crop disease detection, livestock health management, and aquatic species
monitoring. We further cover major implementation challenges such as data
variability and experimental aspects: datasets, performance evaluation metrics,
and geographical focus. We finish the survey by discussing potential open
research directions emphasizing the need for multimodal data integration,
efficient edge-device deployment, and domain-adaptable AI models for diverse
farming environments. Rapid growth of evolving developments in this field can
be actively tracked on our project page:
https://github.com/umair1221/AI-in-Agriculture

</details>


### [21] [Color as the Impetus: Transforming Few-Shot Learner](https://arxiv.org/abs/2507.22136)
*Chaofei Qi,Zhitai Liu,Jianbin Qiu*

Main category: cs.CV

TL;DR: This paper introduces the ColorSense Learner, a bio-inspired meta-learning framework emphasizing human-like color perception for few-shot learning, along with a ColorSense Distiller to enhance model performance.


<details>
  <summary>Details</summary>
Motivation: To leverage human-like color perception mechanisms, which are often overlooked in conventional meta-learning, for improving feature extraction in few-shot learning.

Method: The paper proposes a framework called ColorSense Learner that utilizes color-channel interactions to highlight important features for few-shot learning. In addition, a ColorSense Distiller uses knowledge distillation to enhance the framework's effectiveness.

Result: The approach demonstrated strong generalizability, robustness, and transferability in few-shot learning experiments conducted across eleven benchmarks.

Conclusion: Modeling human-inspired color perception into meta-learning significantly boosts performance in few-shot classification tasks by enhancing discriminative feature extraction across classes and channels.

Abstract: Humans possess innate meta-learning capabilities, partly attributable to
their exceptional color perception. In this paper, we pioneer an innovative
viewpoint on few-shot learning by simulating human color perception mechanisms.
We propose the ColorSense Learner, a bio-inspired meta-learning framework that
capitalizes on inter-channel feature extraction and interactive learning. By
strategically emphasizing distinct color information across different channels,
our approach effectively filters irrelevant features while capturing
discriminative characteristics. Color information represents the most intuitive
visual feature, yet conventional meta-learning methods have predominantly
neglected this aspect, focusing instead on abstract feature differentiation
across categories. Our framework bridges the gap via synergistic color-channel
interactions, enabling better intra-class commonality extraction and larger
inter-class differences. Furthermore, we introduce a meta-distiller based on
knowledge distillation, ColorSense Distiller, which incorporates prior teacher
knowledge to augment the student network's meta-learning capacity. We've
conducted comprehensive coarse/fine-grained and cross-domain experiments on
eleven few-shot benchmarks for validation. Numerous experiments reveal that our
methods have extremely strong generalization ability, robustness, and
transferability, and effortless handle few-shot classification from the
perspective of color perception.

</details>


### [22] [Enhancing efficiency in paediatric brain tumour segmentation using a pathologically diverse single-center clinical dataset](https://arxiv.org/abs/2507.22152)
*A. Piffer,J. A. Buchner,A. G. Gennari,P. Grehten,S. Sirin,E. Ross,I. Ezhov,M. Rosier,J. C. Peeken,M. Piraud,B. Menze,A. Guerreiro Stücklin,A. Jakab,F. Kofler*

Main category: cs.CV

TL;DR: This paper evaluates the use of a 3D nnU-Net deep learning model for segmenting paediatric brain tumours (PBTs) in MRI scans. The model achieves strong performance for certain tumour regions but struggles with others, suggesting the need for further refinement and potential simplification of MRI protocols.


<details>
  <summary>Details</summary>
Motivation: To address the diagnostic and therapeutic challenges of paediatric brain tumours and evaluate the robustness of deep learning (DL)-based segmentation methods across various tumour subtypes and MRI protocols.

Method: A retrospective study involving 174 paediatric patients and MRI scans. A 3D nnU-Net deep learning model was trained on annotated tumour subregions and evaluated using the Dice similarity coefficient (DSC) against human annotators.

Result: The DL model performed well for whole tumour (WT) and T2-hyperintensity (T2H) regions (mean DSC: 0.85), moderately for enhancing tumour (ET) (mean DSC: 0.75), and poorly for cystic components (CC). Its performance varies by tumour type and MRI sequence, and a simplified protocol delivered near-equivalent results to a full protocol.

Conclusion: Deep learning shows promise for PBT segmentation, especially for certain tumour subregions. Limitations in ET and CC segmentation highlight the need for further model refinement. Simplifying MRI protocols could streamline workflows in paediatric neuro-oncology.

Abstract: Background Brain tumours are the most common solid malignancies in children,
encompassing diverse histological, molecular subtypes and imaging features and
outcomes. Paediatric brain tumours (PBTs), including high- and low-grade
gliomas (HGG, LGG), medulloblastomas (MB), ependymomas, and rarer forms, pose
diagnostic and therapeutic challenges. Deep learning (DL)-based segmentation
offers promising tools for tumour delineation, yet its performance across
heterogeneous PBT subtypes and MRI protocols remains uncertain. Methods A
retrospective single-centre cohort of 174 paediatric patients with HGG, LGG,
medulloblastomas (MB), ependymomas, and other rarer subtypes was used. MRI
sequences included T1, T1 post-contrast (T1-C), T2, and FLAIR. Manual
annotations were provided for four tumour subregions: whole tumour (WT),
T2-hyperintensity (T2H), enhancing tumour (ET), and cystic component (CC). A 3D
nnU-Net model was trained and tested (121/53 split), with segmentation
performance assessed using the Dice similarity coefficient (DSC) and compared
against intra- and inter-rater variability. Results The model achieved robust
performance for WT and T2H (mean DSC: 0.85), comparable to human annotator
variability (mean DSC: 0.86). ET segmentation was moderately accurate (mean
DSC: 0.75), while CC performance was poor. Segmentation accuracy varied by
tumour type, MRI sequence combination, and location. Notably, T1, T1-C, and T2
alone produced results nearly equivalent to the full protocol. Conclusions DL
is feasible for PBTs, particularly for T2H and WT. Challenges remain for ET and
CC segmentation, highlighting the need for further refinement. These findings
support the potential for protocol simplification and automation to enhance
volumetric assessment and streamline paediatric neuro-oncology workflows.

</details>


### [23] [Temporally Consistent Unsupervised Segmentation for Mobile Robot Perception](https://arxiv.org/abs/2507.22194)
*Christian Ellis,Maggie Wigness,Craig Lennon,Lance Fiondella*

Main category: cs.CV

TL;DR: This paper introduces Frontier-Seg, an unsupervised segmentation method for terrain-aware navigation that ensures temporal consistency using mobile robot video streams.


<details>
  <summary>Details</summary>
Motivation: Current autonomous navigation systems rely on supervised models requiring costly data labeling, making them infeasible for dynamic, unstructured environments without labeled data.

Method: They use features extracted from DINOv2 backbones and introduce a clustering technique combined with temporal consistency enforcement to identify terrain boundaries across video frames.

Result: Frontier-Seg's performance is validated on benchmark datasets like RUGD and RELLIS-3D, showing effective unsupervised segmentation in unstructured off-road scenarios.

Conclusion: Frontier-Seg successfully addresses the challenge of unsupervised terrain segmentation in dynamic environments, reducing reliance on manually labeled data and improving real-world applicability.

Abstract: Rapid progress in terrain-aware autonomous ground navigation has been driven
by advances in supervised semantic segmentation. However, these methods rely on
costly data collection and labor-intensive ground truth labeling to train deep
models. Furthermore, autonomous systems are increasingly deployed in
unrehearsed, unstructured environments where no labeled data exists and
semantic categories may be ambiguous or domain-specific. Recent zero-shot
approaches to unsupervised segmentation have shown promise in such settings but
typically operate on individual frames, lacking temporal consistency-a critical
property for robust perception in unstructured environments. To address this
gap we introduce Frontier-Seg, a method for temporally consistent unsupervised
segmentation of terrain from mobile robot video streams. Frontier-Seg clusters
superpixel-level features extracted from foundation model
backbones-specifically DINOv2-and enforces temporal consistency across frames
to identify persistent terrain boundaries or frontiers without human
supervision. We evaluate Frontier-Seg on a diverse set of benchmark
datasets-including RUGD and RELLIS-3D-demonstrating its ability to perform
unsupervised segmentation across unstructured off-road environments.

</details>


### [24] [SmartCLIP: Modular Vision-language Alignment with Identification Guarantees](https://arxiv.org/abs/2507.22264)
*Shaoan Xie,Lingjing Kong,Yujia Zheng,Yu Yao,Zeyu Tang,Eric P. Xing,Guangyi Chen,Kun Zhang*

Main category: cs.CV

TL;DR: This paper addresses limitations in CLIP, such as misaligned and entangled image-text representations, proposing a framework and novel method to achieve better alignment and disentanglement, leading to improved performance.


<details>
  <summary>Details</summary>
Motivation: Current CLIP models struggle with information misalignment in image-text datasets due to issues like short or long captions that either fail to fully represent or over-entangle visual features, limiting downstream application performance.

Method: The authors develop a theoretical framework for flexible visual-text alignment and propose a new modular approach called \ours, which identifies and aligns relevant visual and textual representations in a disentangled manner.

Result: The proposed model achieves superior performance across various tasks, successfully addressing challenges of information misalignment and validating its theoretical identification method.

Conclusion: The framework and \ours approach enhance CLIP's ability to disentangle and align visual-textual data effectively, improving generalization across diverse tasks while addressing critical alignment issues.

Abstract: Contrastive Language-Image Pre-training (CLIP)~\citep{radford2021learning}
has emerged as a pivotal model in computer vision and multimodal learning,
achieving state-of-the-art performance at aligning visual and textual
representations through contrastive learning. However, CLIP struggles with
potential information misalignment in many image-text datasets and suffers from
entangled representation. On the one hand, short captions for a single image in
datasets like MSCOCO may describe disjoint regions in the image, leaving the
model uncertain about which visual features to retain or disregard. On the
other hand, directly aligning long captions with images can lead to the
retention of entangled details, preventing the model from learning
disentangled, atomic concepts -- ultimately limiting its generalization on
certain downstream tasks involving short prompts.
  In this paper, we establish theoretical conditions that enable flexible
alignment between textual and visual representations across varying levels of
granularity. Specifically, our framework ensures that a model can not only
\emph{preserve} cross-modal semantic information in its entirety but also
\emph{disentangle} visual representations to capture fine-grained textual
concepts. Building on this foundation, we introduce \ours, a novel approach
that identifies and aligns the most relevant visual and textual representations
in a modular manner. Superior performance across various tasks demonstrates its
capability to handle information misalignment and supports our identification
theory. The code is available at https://github.com/Mid-Push/SmartCLIP.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [25] [Minimizing CGYRO HPC Communication Costs in Ensembles with XGYRO by Sharing the Collisional Constant Tensor Structure](https://arxiv.org/abs/2507.22245)
*Igor Sfiligoi,Emily A. Belli,Jeff Candy*

Main category: cs.DC

TL;DR: The paper introduces XGYRO, a tool that optimizes memory and communication overhead for ensemble fusion plasma simulations by sharing data structures across simulations.


<details>
  <summary>Details</summary>
Motivation: Fusion plasma simulations require high computational and memory resources, leading to bottlenecks such as significant communication overhead during multi-node usage. This is especially challenging when analyzing ensembles of simulations.

Method: The authors developed XGYRO, which organizes and executes ensembles of CGYRO simulations as a single HPC job. This allows for global optimization, including memory-sharing strategies for data structures like collisional constant tensors.

Result: The proposed approach drastically reduces memory consumption per simulation by sharing data structures across simulations in the ensemble, leading to lower communication overhead.

Conclusion: XGYRO effectively tackles the inefficiencies in processing ensembles of fusion plasma simulations, enabling more efficient resource utilization and faster computations on high-performance computing environments.

Abstract: First-principles fusion plasma simulations are both compute and memory
intensive, and CGYRO is no exception. The use of many HPC nodes to fit the
problem in the available memory thus results in significant communication
overhead, which is hard to avoid for any single simulation. That said, most
fusion studies are composed of ensembles of simulations, so we developed a new
tool, named XGYRO, that executes a whole ensemble of CGYRO simulations as a
single HPC job. By treating the ensemble as a unit, XGYRO can alter the global
buffer distribution logic and apply optimizations that are not feasible on any
single simulation, but only on the ensemble as a whole. The main saving comes
from the sharing of the collisional constant tensor structure, since its values
are typically identical between parameter-sweep simulations. This data
structure dominates the memory consumption of CGYRO simulations, so
distributing it among the whole ensemble results in drastic memory savings for
each simulation, which in turn results in overall lower communication overhead.

</details>


### [26] [Towards Experiment Execution in Support of Community Benchmark Workflows for HPC](https://arxiv.org/abs/2507.22294)
*Gregor von Laszewski,Wesley Brewer,Sean R. Wilkinson,Andrew Shao,J. P. Fleischer,Harshad Pitkar,Christine R. Kirkpatrick,Geoffrey C. Fox*

Main category: cs.DC

TL;DR: The paper proposes workflow templates to demonstrate computational resource capabilities and validate its concept through tools like Cloudmesh's Experiment Executor and HPE's SmartSim in various scientific applications.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of demonstrating computational resource capability with limited benchmarks in scientific applications.

Method: The paper identifies common usage patterns for adaptable workflow templates, derived from extensive HPC experience and validated through tools Cloudmesh's Experiment Executor and HPE's SmartSim.

Result: The proposed workflow templates improve adaptability, especially in education, and were successfully tested on various scientific applications such as cloudmask conduction and earthquake prediction.

Conclusion: Benchmark carpentry through workflow templates and simple experiment management tools improves adaptability and validates the frameworks' utility in scientific computational workflows.

Abstract: A key hurdle is demonstrating compute resource capability with limited
benchmarks. We propose workflow templates as a solution, offering adaptable
designs for specific scientific applications. Our paper identifies common usage
patterns for these templates, drawn from decades of HPC experience, including
recent work with the MLCommons Science working group.
  We found that focusing on simple experiment management tools within the
broader computational workflow improves adaptability, especially in education.
This concept, which we term benchmark carpentry, is validated by two
independent tools: Cloudmesh's Experiment Executor and Hewlett Packard
Enterprise's SmartSim. Both frameworks, with significant functional overlap,
have been tested across various scientific applications, including conduction
cloudmask, earthquake prediction, simulation-AI/ML interactions, and the
development of computational fluid dynamics surrogates.

</details>


### [27] [A Semi-Supervised Federated Learning Framework with Hierarchical Clustering Aggregation for Heterogeneous Satellite Networks](https://arxiv.org/abs/2507.22339)
*Zhuocheng Liu,Zhishu Shen,Qiushi Zheng,Tiehua Zhang,Zheng Lei,Jiong Jin*

Main category: cs.DC

TL;DR: This paper proposes a novel semi-supervised federated learning framework for Low Earth Orbit (LEO) satellite networks to improve convergence, minimize processing time, and reduce energy consumption.


<details>
  <summary>Details</summary>
Motivation: LEO satellites are increasingly central to 6G networks and supporting distributed tasks. However, utilizing Federated Learning in these dynamic, heterogeneous satellite networks comes with challenges like reliable convergence, processing time optimization, and energy efficiency.

Method: The framework combines hierarchical clustering aggregation, sparsification, adaptive weight quantization, and hierarchical clustering in two stages: satellite cluster aggregation and ground station aggregation. Supervised learning at the ground stations guides the federated training process for Parameter Server satellites and fully unlabeled satellites.

Result: The experimental results reveal significant improvements: processing time is reduced by up to 3x, and energy consumption is reduced by up to 4x compared to existing methods, all while maintaining model accuracy.

Conclusion: The proposed semi-supervised federated learning framework is effective and efficient for LEO satellite networks, enabling distributed intelligence with enhanced performance metrics.

Abstract: Low Earth Orbit (LEO) satellites are emerging as key components of 6G
networks, with many already deployed to support large-scale Earth observation
and sensing related tasks. Federated Learning (FL) presents a promising
paradigm for enabling distributed intelligence in these resource-constrained
and dynamic environments. However, achieving reliable convergence, while
minimizing both processing time and energy consumption, remains a substantial
challenge, particularly in heterogeneous and partially unlabeled satellite
networks. To address this challenge, we propose a novel semi-supervised
federated learning framework tailored for LEO satellite networks with
hierarchical clustering aggregation. To further reduce communication overhead,
we integrate sparsification and adaptive weight quantization techniques. In
addition, we divide the FL clustering into two stages: satellite cluster
aggregation stage and Ground Stations (GSs) aggregation stage. The supervised
learning at GSs guides selected Parameter Server (PS) satellites, which in turn
support fully unlabeled satellites during the federated training process.
Extensive experiments conducted on a satellite network testbed demonstrate that
our proposal can significantly reduce processing time (up to 3x) and energy
consumption (up to 4x) compared to other comparative methods while maintaining
model accuracy.

</details>


### [28] [Leveraging Caliper and Benchpark to Analyze MPI Communication Patterns: Insights from AMG2023, Kripke, and Laghos](https://arxiv.org/abs/2507.22372)
*Grace Nansamba,Evelyn Namugwanya,David Boehme,Dewi Yokelson,Riley Shipley,Derek Schafer,Michael McKinsey,Olga Pearce,Anthony Skjellum*

Main category: cs.DC

TL;DR: The paper enhances the Caliper HPC profiling tool by introducing communication regions to capture detailed communication behaviors and visualize MPI patterns, offering insights such as communication bottlenecks.


<details>
  <summary>Details</summary>
Motivation: To address the lack of detailed analysis and visualization capabilities for MPI communication patterns in existing HPC profiling tools, including Caliper.

Method: The authors introduced communication regions into Caliper that enable capturing metrics about communicated data and MPI processes. They evaluated the tool's utility using applications from the Benchpark suite like AMG2023, Kripke, and Laghos.

Result: The enhanced Caliper tool successfully identifies communication bottlenecks, visualizes MPI communication patterns, and differentiates scalability and message-traffic metrics for CPU and GPU-oriented systems.

Conclusion: The addition of communication regions significantly enhances Caliper's ability to analyze and visualize detailed HPC communication behaviors, proving valuable for profiling and optimization.

Abstract: We introduce ``communication regions'' into the widely used Caliper HPC
profiling tool. A communication region is an annotation enabling capture of
metrics about the data being communicated (including statistics of these
metrics), and metrics about the MPI processes involved in the communications,
something not previously possible in Caliper. We explore the utility of
communication regions with three representative modeling and simulation
applications, AMG2023, Kripke, and Laghos, all part of the comprehensive
Benchpark suite that includes Caliper annotations. Enhanced Caliper reveals
detailed communication behaviors. Using Caliper and Thicket in tandem, we
create new visualizations of MPI communication patterns, including halo
exchanges. Our findings reveal communication bottlenecks and detailed
behaviors, indicating significant utility of the special-regions addition to
Caliper. The comparative scaling behavior of both CPU and GPU oriented systems
are shown; we are able to look at different regions within a given application,
and see how scalability and message-traffic metrics differ.

</details>


### [29] [DSPE: Profit Maximization in Edge-Cloud Storage System using Dynamic Space Partitioning with Erasure Code](https://arxiv.org/abs/2507.22801)
*Shubhradeep Roy,Suvarthi Sarkar,Vivek Verma,Aryabartta Sahu*

Main category: cs.DC

TL;DR: This paper proposes a profit-driven framework using collaborative caching, erasure coding, and elastic storage partitioning to improve data access in edge storage systems under dynamic workloads.


<details>
  <summary>Details</summary>
Motivation: Edge storage's limited capacity complicates handling high volumes of latency-sensitive requests in dynamic cloud environments.

Method: The paper combines collaborative caching, erasure coding, and dynamic private/public partitioning with adaptive data placement and replacement policies.

Result: Dynamic Space Partitioning and Elastic caching improves system profitability by 5-8%, tested on synthetic and real-world traces.

Conclusion: The proposed method enhances edge storage system efficiency under variable workloads, outperforming state-of-the-art approaches.

Abstract: Edge Storage Systems have emerged as a critical enabler of low latency data
access in modern cloud networks by bringing storage and computation closer to
end users. However, the limited storage capacity of edge servers poses
significant challenges in handling high volume and latency sensitive data
access requests, particularly under dynamic workloads. In this work, we propose
a profit driven framework that integrates three key mechanisms which are
collaborative caching, erasure coding, and elastic storage partitioning. Unlike
traditional replication, erasure coding enables space efficient redundancy,
allowing data to be reconstructed from any subset of K out of K plus M coded
blocks. We dynamically partition each edge server s storage into private and
public regions. The private region is further subdivided among access points
based on their incoming request rates, enabling adaptive control over data
locality and ownership. We design a data placement and replacement policy that
determines how and where to store or evict coded data blocks to maximize data
access within deadlines. While the private region serves requests from local
APs, the public region handles cooperative storage requests from neighboring
servers. Our proposed Dynamic Space Partitioning and Elastic caching strategy
is evaluated on both synthetic and real world traces from Netflix and Spotify.
Experimental results show that our method improves overall system profitability
by approximately 5 to 8% compared to state of the art approaches under varied
workload conditions.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [30] [Hybrid activation functions for deep neural networks: S3 and S4 -- a novel approach to gradient flow optimization](https://arxiv.org/abs/2507.22090)
*Sergii Kavun*

Main category: cs.LG

TL;DR: This paper introduces two new hybrid activation functions, S3 and S4, which address issues like dead neurons and vanishing gradients prevalent in traditional activation functions.


<details>
  <summary>Details</summary>
Motivation: The traditional activation functions like ReLU, Sigmoid, and Tanh face significant issues such as dead neuron problems and vanishing gradients, hindering neural network performance.

Method: The proposed hybrid activation function, S3, combines sigmoid for negative inputs and softsign for positive inputs, while its improved version, S4, incorporates a smooth transition mechanism regulated by a parameter k.

Result: S4 demonstrated higher accuracy on tasks like MNIST (97.4%), Iris classification (96.0%), and better regression performance on Boston Housing (MSE 18.7). It outperformed nine baseline functions in training speed, convergence, and maintaining stable gradients.

Conclusion: The findings suggest that S4's hybrid design and tunable parameter make it a versatile activation function that addresses key limitations of existing methods and improves neural network training dynamics.

Abstract: Activation functions are critical components in deep neural networks,
directly influencing gradient flow, training stability, and model performance.
Traditional functions like ReLU suffer from dead neuron problems, while sigmoid
and tanh exhibit vanishing gradient issues. We introduce two novel hybrid
activation functions: S3 (Sigmoid-Softsign) and its improved version S4
(smoothed S3). S3 combines sigmoid for negative inputs with softsign for
positive inputs, while S4 employs a smooth transition mechanism controlled by a
steepness parameter k. We conducted comprehensive experiments across binary
classification, multi-class classification, and regression tasks using three
different neural network architectures. S4 demonstrated superior performance
compared to nine baseline activation functions, achieving 97.4% accuracy on
MNIST, 96.0% on Iris classification, and 18.7 MSE on Boston Housing regression.
The function exhibited faster convergence (-19 for ReLU) and maintained stable
gradient flow across network depths. Comparative analysis revealed S4's
gradient range of [0.24, 0.59] compared to ReLU's 18% dead neurons in deep
networks. The S4 activation function addresses key limitations of existing
functions through its hybrid design and smooth transition mechanism. The
tunable parameter k allows adaptation to different tasks and network depths,
making S4 a versatile choice for deep learning applications. These findings
suggest that hybrid activation functions represent a promising direction for
improving neural network training dynamics.

</details>


### [31] [CIMR: Contextualized Iterative Multimodal Reasoning for Robust Instruction Following in LVLMs](https://arxiv.org/abs/2507.22074)
*Yangshu Yuan,Heng Chen,Xinyi Jiang,Christian Ng,Kexin Qiu*

Main category: cs.LG

TL;DR: CIMR enhances reasoning and self-correction abilities in multimodal tasks, outperforming existing models like GPT-4V and MiniGPT-4.


<details>
  <summary>Details</summary>
Motivation: LLMs and LVLMs struggle with multi-step and complex multimodal instructions requiring iterative reasoning and feedback.

Method: The proposed CIMR framework incorporates initial reasoning followed by iterative refinement using multimodal feedback and fine-tunes models on specific datasets.

Result: CIMR achieved 91.5% accuracy on the Multi-modal Action Planning dataset, surpassing leading models such as GPT-4V and others.

Conclusion: CIMR demonstrates effective iterative reasoning and self-correction for advanced multimodal tasks.

Abstract: The rapid advancement of Large Language Models (LLMs) and Large
Vision-Language Models (LVLMs) has enhanced our ability to process and generate
human language and visual information. However, these models often struggle
with complex, multi-step multi-modal instructions that require logical
reasoning, dynamic feedback integration, and iterative self-correction. To
address this, we propose CIMR: Contextualized Iterative Multimodal Reasoning, a
novel framework that introduces a context-aware iterative reasoning and
self-correction module. CIMR operates in two stages: initial reasoning and
response generation, followed by iterative refinement using parsed multi-modal
feedback. A dynamic fusion module deeply integrates textual, visual, and
contextual features at each step. We fine-tune LLaVA-1.5-7B on the Visual
Instruction Tuning (VIT) dataset and evaluate CIMR on the newly introduced
Multi-modal Action Planning (MAP) dataset. CIMR achieves 91.5% accuracy,
outperforming state-of-the-art models such as GPT-4V (89.2%), LLaVA-1.5
(78.5%), MiniGPT-4 (75.3%), and InstructBLIP (72.8%), demonstrating the
efficacy of its iterative reasoning and self-correction capabilities in complex
tasks.

</details>


### [32] [Prototype-Guided Pseudo-Labeling with Neighborhood-Aware Consistency for Unsupervised Adaptation](https://arxiv.org/abs/2507.22075)
*Eman Ali,Chetan Arora,Muhammad Haris Khan*

Main category: cs.LG

TL;DR: This paper introduces an adaptive pseudo-labeling framework for improving CLIP's adaptation using prototype and neighborhood consistency.


<details>
  <summary>Details</summary>
Motivation: Address challenges posed by noisy pseudo-labels during unsupervised domain adaptation of vision-language models like CLIP.

Method: Develop components focusing on prototype in-class/cross-class consistency (PICS), neighborhood refinement (NALR), and adaptive weighting of pseudo-label contributions based on confidence.

Result: Extensive testing across 11 benchmarks shows state-of-the-art performance in generating accurate pseudo-labels while retaining efficiency.

Conclusion: The proposed framework effectively addresses pseudo-label noise in unsupervised adaptation, significantly improving performance and robustness.

Abstract: In unsupervised adaptation for vision-language models such as CLIP,
pseudo-labels derived from zero-shot predictions often exhibit significant
noise, particularly under domain shifts or in visually complex scenarios.
Conventional pseudo-label filtering approaches, which rely on fixed confidence
thresholds, tend to be unreliable in fully unsupervised settings. In this work,
we propose a novel adaptive pseudo-labeling framework that enhances CLIP's
adaptation performance by integrating prototype consistency and
neighborhood-based consistency. The proposed method comprises two key
components: PICS, which assesses pseudo-label accuracy based on in-class
feature compactness and cross-class feature separation; and NALR, which
exploits semantic similarities among neighboring samples to refine
pseudo-labels dynamically. Additionally, we introduce an adaptive weighting
mechanism that adjusts the influence of pseudo-labeled samples during training
according to their estimated correctness. Extensive experiments on 11 benchmark
datasets demonstrate that our method achieves state-of-the-art performance in
unsupervised adaptation scenarios, delivering more accurate pseudo-labels while
maintaining computational efficiency.

</details>


### [33] [Tapping into the Black Box: Uncovering Aligned Representations in Pretrained Neural Networks](https://arxiv.org/abs/2507.22832)
*Maciej Satkiewicz*

Main category: cs.LG

TL;DR: The paper shows ReLU networks implicitly learn linear models and proposes a method to uncover their interpretable decision-making process using a modified backward pass.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks are often viewed as black boxes, and there's a desire to understand their decision-making processes in a more interpretable way.

Method: The authors formalize an implicit linear model in ReLU networks, apply certain modifications to the backward pass, and introduce 'excitation pullbacks' to uncover target-specific input features.

Result: Excitation pullbacks present perceptually aligned high-resolution features, indicating that neural networks rely on interpretable learned patterns.

Conclusion: The study suggests neural networks learn interpretable patterns, which can be extracted post-training, advancing knowledge discovery and the development of trustworthy AI systems.

Abstract: In this paper we argue that ReLU networks learn an implicit linear model we
can actually tap into. We describe that alleged model formally and show that we
can approximately pull its decision boundary back to the input space with
certain simple modification to the backward pass. The resulting gradients
(called excitation pullbacks) reveal high-resolution input- and target-specific
features of remarkable perceptual alignment on a number of popular
ImageNet-pretrained deep architectures. This strongly suggests that neural
networks do, in fact, rely on learned interpretable patterns that can be
recovered after training. Thus, our findings may have profound implications for
knowledge discovery and the development of dependable artificial systems.

</details>


### [34] [Test-time Prompt Refinement for Text-to-Image Models](https://arxiv.org/abs/2507.22076)
*Mohammad Abdul Hafeez Khan,Yash Jain,Siddhartha Bhattacharyya,Vibhav Vineet*

Main category: cs.LG

TL;DR: The paper introduces TIR, a test-time prompt refinement framework to improve text-to-image generation by iteratively correcting prompt misalignments without retraining the model.


<details>
  <summary>Details</summary>
Motivation: Text-to-image models suffer from sensitivity to prompt wording, leading to inconsistent outputs. Addressing this issue can enhance their reliability and utility.

Method: A closed-loop refinement framework, TIR, uses a pretrained multimodal large language model (MLLM) to analyze generated images and prompts, detect misalignments, and refine the prompts iteratively during the generation process.

Result: TIR was tested across multiple benchmark datasets, showing improved alignment and visual coherence without requiring retraining of the underlying T2I model.

Conclusion: The proposed framework effectively enhances the performance of T2I models, demonstrating its viability as a plug-and-play solution for prompt refinement.

Abstract: Text-to-image (T2I) generation models have made significant strides but still
struggle with prompt sensitivity: even minor changes in prompt wording can
yield inconsistent or inaccurate outputs. To address this challenge, we
introduce a closed-loop, test-time prompt refinement framework that requires no
additional training of the underlying T2I model, termed TIR. In our approach,
each generation step is followed by a refinement step, where a pretrained
multimodal large language model (MLLM) analyzes the output image and the user's
prompt. The MLLM detects misalignments (e.g., missing objects, incorrect
attributes) and produces a refined and physically grounded prompt for the next
round of image generation. By iteratively refining the prompt and verifying
alignment between the prompt and the image, TIR corrects errors, mirroring the
iterative refinement process of human artists. We demonstrate that this
closed-loop strategy improves alignment and visual coherence across multiple
benchmark datasets, all while maintaining plug-and-play integration with
black-box T2I models.

</details>


### [35] [Multi-fidelity Bayesian Data-Driven Design of Energy Absorbing Spinodoid Cellular Structures](https://arxiv.org/abs/2507.22079)
*Leo Guo,Hirak Kansara,Siamak F. Khosroshahi,GuoQi Zhang,Wei Tan*

Main category: cs.LG

TL;DR: This paper explores and compares Bayesian Optimization (BO) and Multi-Fidelity Bayesian Optimization (MFBO) for maximizing energy absorption in spinodoid cellular structures, showing that MFBO outperforms BO by up to 11%.


<details>
  <summary>Details</summary>
Motivation: To address the increasing computational expense of finite element simulations while responding to the growing demand for data-efficient design methods. It also aims to fill gaps in sensitivity analysis and sample quality representation in data-driven engineering.

Method: The study employs Sobol' samples with variance-based sensitivity analysis to reduce problem complexity, and compares BO and MFBO performances for maximizing energy absorption in spinodoid structures using finite element analysis.

Result: MFBO outperformed BO by up to 11% across various hyperparameter settings in optimizing the energy absorption of spinodoid structures.

Conclusion: Multi-Fidelity Bayesian Optimization (MFBO) is an effective and superior approach for addressing expensive, data-driven design problems like energy absorption optimization in metamaterials. The results, available open-source, highlight the utility of multi-fidelity techniques.

Abstract: Finite element (FE) simulations of structures and materials are getting
increasingly more accurate, but also more computationally expensive as a
collateral result. This development happens in parallel with a growing demand
of data-driven design. To reconcile the two, a robust and data-efficient
optimization method called Bayesian optimization (BO) has been previously
established as a technique to optimize expensive objective functions. In
parallel, the mesh width of an FE model can be exploited to evaluate an
objective at a lower or higher fidelity (cost & accuracy) level. The
multi-fidelity setting applied to BO, called multi-fidelity BO (MFBO), has also
seen previous success. However, BO and MFBO have not seen a direct comparison
with when faced with with a real-life engineering problem, such as metamaterial
design for deformation and absorption qualities. Moreover, sampling quality and
assessing design parameter sensitivity is often an underrepresented part of
data-driven design. This paper aims to address these shortcomings by employing
Sobol' samples with variance-based sensitivity analysis in order to reduce
design problem complexity. Furthermore, this work describes, implements,
applies and compares the performance BO with that MFBO when maximizing the
energy absorption (EA) problem of spinodoid cellular structures is concerned.
The findings show that MFBO is an effective way to maximize the EA of a
spinodoid structure and is able to outperform BO by up to 11% across various
hyperparameter settings. The results, which are made open-source, serve to
support the utility of multi-fidelity techniques across expensive data-driven
design problems.

</details>


### [36] [Shape Invariant 3D-Variational Autoencoder: Super Resolution in Turbulence flow](https://arxiv.org/abs/2507.22082)
*Anuraj Maurya*

Main category: cs.LG

TL;DR: The paper discusses how deep learning enhances turbulence modeling by addressing challenges in multiscale integration and super-resolution tasks.


<details>
  <summary>Details</summary>
Motivation: To leverage deep learning for understanding and modeling turbulence using high-dimensional data from experiments, observations, and simulations.

Method: The paper reviews classical and deep learning approaches to turbulence modeling, focusing on multiscale integration and deep generative model application.

Result: The paper identifies advancements and challenges in applying deep learning, specifically with multiscale models and super-resolution reconstruction.

Conclusion: Deep learning serves as a valuable tool for turbulence modeling, unlocking new insights and addressing computational challenges in fluid dynamics.

Abstract: Deep learning provides a versatile suite of methods for extracting structured
information from complex datasets, enabling deeper understanding of underlying
fluid dynamic phenomena. The field of turbulence modeling, in particular,
benefits from the growing availability of high-dimensional data obtained
through experiments, field observations, and large-scale simulations spanning
multiple spatio-temporal scales. This report presents a concise overview of
both classical and deep learningbased approaches to turbulence modeling. It
further investigates two specific challenges at the intersection of fluid
dynamics and machine learning: the integration of multiscale turbulence models
with deep learning architectures, and the application of deep generative models
for super-resolution reconstruction

</details>


### [37] [Principled Curriculum Learning using Parameter Continuation Methods](https://arxiv.org/abs/2507.22089)
*Harsh Nilesh Pathak,Randy Paffenroth*

Main category: cs.LG

TL;DR: The paper introduces a parameter continuation method to optimize neural networks for improved generalization, outperforming ADAM.


<details>
  <summary>Details</summary>
Motivation: To explore optimization alternatives for neural networks that achieve better generalization and performance than existing methods such as ADAM.

Method: Developing and applying a parameter continuation technique, which builds on principles of homotopies and curriculum learning.

Result: The proposed method demonstrates superior generalization performance compared to ADAM in supervised and unsupervised learning tasks.

Conclusion: Parameter continuation methods offer both theoretical validity and practical advantages in neural network optimization, marking an improvement over traditional methods.

Abstract: In this work, we propose a parameter continuation method for the optimization
of neural networks. There is a close connection between parameter continuation,
homotopies, and curriculum learning. The methods we propose here are
theoretically justified and practically effective for several problems in deep
neural networks. In particular, we demonstrate better generalization
performance than state-of-the-art optimization techniques such as ADAM for
supervised and unsupervised learning tasks.

</details>


### [38] [Spatial-Temporal Reinforcement Learning for Network Routing with Non-Markovian Traffic](https://arxiv.org/abs/2507.22174)
*Molly Wang*

Main category: cs.LG

TL;DR: This paper proposes a spatial-temporal RL approach, integrating Graph Neural Networks (GNNs) and Recurrent Neural Networks (RNNs), to improve packet routing in communication networks with dynamic topologies and traffic patterns.


<details>
  <summary>Details</summary>
Motivation: Standard RL methods for packet routing often fail in scenarios where the Markovian assumption does not hold and do not explicitly address spatial relationships within complex network topologies.

Method: The approach combines GNNs for capturing spatial dynamics of the network topology and RNNs for addressing temporal traffic patterns, forming a spatial-temporal RL strategy.

Result: The proposed method demonstrated superior performance and robustness compared to traditional RL algorithms, especially under dynamic network topology changes.

Conclusion: Integrating spatial and temporal awareness into RL algorithms enhances decision-making for packet routing in communication networks, addressing limitations in traditional techniques.

Abstract: Reinforcement Learning (RL) has become a well-established approach for
optimizing packet routing in communication networks. Standard RL algorithms
typically are based on the Markov Decision Process (MDP), which assumes that
the current state of the environment provides all the necessary information for
system evolution and decision-making. However, this Markovian assumption is
invalid in many practical scenarios, making the MDP and RL frameworks
inadequate to produce the optimal solutions. Additionally, traditional RL
algorithms often employ function approximations (e.g., by neural networks) that
do not explicitly capture the spatial relationships inherent in environments
with complex network topologies. Communication networks are characterized by
dynamic traffic patterns and arbitrary numbers of nodes and links, which
further complicate the decision-making process. To address these challenges, we
propose a spatial-temporal RL approach that integrates Graph Neural Networks
(GNNs) and Recurrent Neural Networks (RNNs) to adequately capture the spatial
dynamics regarding network topology and temporal traffic patterns,
respectively, to enhance routing decisions. Our evaluation demonstrates that
the proposed method outperforms and is more robust to changes in the network
topology when compared with traditional RL techniques.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [39] [Pendulum Model of Spiking Neurons](https://arxiv.org/abs/2507.22146)
*Joy Bose*

Main category: cs.NE

TL;DR: The paper introduces a novel spiking neuron model inspired by a damped, driven pendulum, offering richer temporal computations and phase-based spike encoding for tasks like sequence processing.


<details>
  <summary>Details</summary>
Motivation: Standard spiking neuron models, like LIF, lack the capacity for oscillatory behavior and sophisticated timing-sensitive processing necessary for sequence-related and symbolic tasks.

Method: The authors propose a second-order dynamic model of spiking neurons resembling a pendulum and analyze its dynamics, extend the framework with STDP learning rules, and showcase implementations using Python, Brian2, and neuromorphic hardware approximations.

Result: The model captures complex temporal characteristics, performs timing-sensitive computations, and is deployable on energy-efficient neuromorphic platforms.

Conclusion: The pendulum neuron model provides a biologically plausible, computationally rich, and energy-efficient approach for sequence processing and neuromorphic applications.

Abstract: We propose a biologically inspired model of spiking neurons based on the
dynamics of a damped, driven pendulum. Unlike traditional models such as the
Leaky Integrate-and-Fire (LIF) neurons, the pendulum neuron incorporates
second-order, nonlinear dynamics that naturally give rise to oscillatory
behavior and phase-based spike encoding. This model captures richer temporal
features and supports timing-sensitive computations critical for sequence
processing and symbolic learning. We present an analysis of single-neuron
dynamics and extend the model to multi-neuron layers governed by Spike-Timing
Dependent Plasticity (STDP) learning rules. We demonstrate practical
implementation with python code and with the Brian2 spiking neural simulator,
and outline a methodology for deploying the model on neuromorphic hardware
platforms, using an approximation of the second-order equations. This framework
offers a foundation for developing energy-efficient neural systems for
neuromorphic computing and sequential cognition tasks.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [40] [Dissecting RISC-V Performance: Practical PMU Profiling and Hardware-Agnostic Roofline Analysis on Emerging Platforms](https://arxiv.org/abs/2507.22451)
*Alexander Batashev*

Main category: cs.PF

TL;DR: A methodology to optimize performance on RISC-V systems using tooling to address hardware bugs and compiler-guided Roofline analysis.


<details>
  <summary>Details</summary>
Motivation: RISC-V's increasing adoption faces issues like fragmented tools, immature features, and hardware defects, complicating performance optimization.

Method: Proposes workarounds for hardware bugs and introduces LLVM-based Roofline tooling for operational intensity and throughput metrics.

Result: Developed an open-source toolchain unifying corrected PMU data and Roofline analysis into a consolidated procedure.

Conclusion: This approach facilitates actionable performance insights and efficient optimizations for challenging RISC-V hardware scenarios.

Abstract: As RISC-V architectures proliferate across embedded and high-performance
domains, developers face persistent challenges in performance optimization due
to fragmented tooling, immature hardware features, and platform-specific
defects. This paper delivers a pragmatic methodology for extracting actionable
performance insights on RISC-V systems, even under constrained or unreliable
hardware conditions. We present a workaround to circumvent hardware bugs in one
of the popular RISC-V implementations, enabling robust event sampling. For
memory-compute bottleneck analysis, we introduce compiler-driven Roofline
tooling that operates without hardware PMU dependencies, leveraging LLVM-based
instrumentation to derive operational intensity and throughput metrics directly
from application IR. Our open source toolchain automates these workarounds,
unifying PMU data correction and compiler-guided Roofline construction into a
single workflow.

</details>


### [41] [Ecoscape: Fault Tolerance Benchmark for Adaptive Remediation Strategies in Real-Time Edge ML](https://arxiv.org/abs/2507.22702)
*Hendrik Reiter,Ahmad Rzgar Hamid,Florian Schlösser,Mikkel Baun Kjærgaard,Wilhelm Hasselbring*

Main category: cs.PF

TL;DR: The paper presents Ecoscape, a benchmark to evaluate the effectiveness of remediation strategies in fault-prone edge computing environments.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of fair comparison methods for remediation strategies in edge environments that are prone to faults.

Method: The authors developed Ecoscape, a benchmark utilizing Chaos Engineering techniques to simulate faults and measure remediation efficacy based on quantified scores.

Result: Ecoscape was shown to be effective in evaluating remediation strategies for edge machine learning inference without requiring physical edge testbeds.

Conclusion: Ecoscape provides a structured and configurable framework to optimize fault tolerance in edge computing systems, ensuring better handling of Service Level Objectives in fault-prone scenarios.

Abstract: Edge computing offers significant advantages for realtime data processing
tasks, such as object recognition, by reducing network latency and bandwidth
usage. However, edge environments are susceptible to various types of fault. A
remediator is an automated software component designed to adjust the
configuration parameters of a software service dynamically. Its primary
function is to maintain the services operational state within predefined
Service Level Objectives by applying corrective actions in response to
deviations from these objectives. Remediators can be implemented based on the
Kubernetes container orchestration tool by implementing remediation strategies
such as rescheduling or adjusting application parameters. However, currently,
there is no method to compare these remediation strategies fairly. This paper
introduces Ecoscape, a comprehensive benchmark designed to evaluate the
performance of remediation strategies in fault-prone environments. Using Chaos
Engineering techniques, Ecoscape simulates realistic fault scenarios and
provides a quantifiable score to assess the efficacy of different remediation
approaches. In addition, it is configurable to support domain-specific Service
Level Objectives. We demonstrate the capabilities of Ecoscape in edge machine
learning inference, offering a clear framework to optimize fault tolerance in
these systems without needing a physical edge testbed.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [42] [A Compute-Matched Re-Evaluation of TroVE on MATH](https://arxiv.org/abs/2507.22069)
*Tobias Sesterhenn,Ian Berlot-Attwell,Janis Zenkner,Christian Bartelt*

Main category: cs.PL

TL;DR: The paper re-evaluates the TroVE model, which combines direct code generation, tool creation, and reuse for mathematical problem-solving. It finds that the supposed advantage of this approach is mainly a result of higher computational budget rather than effective mechanisms.


<details>
  <summary>Details</summary>
Motivation: To assess whether the use of toolboxes in the TroVE model truly improves mathematical problem-solving, as claimed, or if the gains are due to other factors.

Method: The authors analyze the TroVE model's performance on the MATH benchmark, adjusting its implementation for improved accuracy and matching the computational budget with the baseline PRIMITIVE model.

Result: Upon equalizing compute budgets, the performance advantage of TroVE diminishes to a marginal 1% improvement, indicating that the toolbox approach yields limited benefits.

Conclusion: The toolbox framework in TroVE does not significantly enhance mathematical problem-solving performance on MATH, and its advantage stems mostly from computational resource allocation.

Abstract: Reusing established theorems and formulas is central to mathematical problem
solving, serving as essential building blocks for tackling increasingly complex
challenges. Recent work, TroVE, argues that code-generating Large Language
Models (LLMs) can benefit similarly on the MATH benchmark by inducing and
reusing higher-level toolboxes. By allocating computational budget across an
ensemble of three modes -- directly generating code, creating tools, and
reusing tools -- TroVE claims to outperform a PRIMITIVE baseline that only
performs direct generation. However, recent analysis (Berlot-Attwell et al.,
2024) casts doubt on these gains, noting that the tools created are often
trivial or rarely reused, suggesting that improvements may stem from
self-consistency or self-correction. In this work, we re-evaluate TroVE on
MATH, analyze the impact of each of its modes, and show that its benefit does
not come from these mechanisms, but simply from a higher computational budget
spent for TroVE compared to PRIMITIVE. To this end, we also perform a small
correction in the original implementation of TroVE's selection mechanism,
boosting TroVE's performance on MATH by 3\% in accuracy. After matching for
compute, the benefit of TroVE reduces to a marginal improvement of 1\%,
suggesting that this toolbox approach does not provide a significant benefit on
MATH.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [43] [Deployment of Objects with a Soft Everting Robot](https://arxiv.org/abs/2507.22188)
*Ethan DeVries,Jack Ferlazzo,Mustafa Ugur,Laura H. Blumenschein*

Main category: cs.RO

TL;DR: The paper explores the capability of soft everting robots to transport larger and heavier payloads effectively in various challenging terrains.


<details>
  <summary>Details</summary>
Motivation: The authors aim to expand the utility of soft everting robots beyond exploratory tasks, studying their potential to safely deliver payloads in hazardous environments.

Method: The paper develops models to quantify payload effects on robot growth, self-support, and predict payload slip. Researchers conducted experiments for payload transport across diverse shapes, sizes, and terrains.

Result: Experiments demonstrated successful transport capabilities of payloads up to 1.5kg and navigation through tight apertures, steep turns, and unsupported gaps of 1.15m length.

Conclusion: Soft everting robots are capable of carrying and delivering payloads efficiently under extreme conditions, thereby broadening their application potential.

Abstract: Soft everting robots present significant advantages over traditional rigid
robots, including enhanced dexterity, improved environmental interaction, and
safe navigation in unpredictable environments. While soft everting robots have
been widely demonstrated for exploration type tasks, their potential to move
and deploy payloads in such tasks has been less investigated, with previous
work focusing on sensors and tools for the robot. Leveraging the navigation
capabilities, and deployed body, of the soft everting robot to deliver payloads
in hazardous areas, e.g. carrying a water bottle to a person stuck under
debris, would represent a significant capability in many applications. In this
work, we present an analysis of how soft everting robots can be used to deploy
larger, heavier payloads through the inside of the robot. We analyze both what
objects can be deployed and what terrain features they can be carried through.
Building on existing models, we present methods to quantify the effects of
payloads on robot growth and self-support, and develop a model to predict
payload slip. We then experimentally quantify payload transport using soft
everting robot with a variety of payload shapes, sizes, and weights and though
a series of tasks: steering, vertical transport, movement through holes, and
movement across gaps. Overall, the results show that we can transport payloads
in a variety of shapes and up to 1.5kg in weight and that we can move through
circular apertures with as little as 0.01cm clearance around payloads, carry
out discrete turns up to 135 degrees, and move across unsupported gaps of 1.15m
in length.

</details>


### [44] [FLORES: A Reconfigured Wheel-Legged Robot for Enhanced Steering and Adaptability](https://arxiv.org/abs/2507.22345)
*Zhicheng Song,Jinglan Xu,Chunxin Zheng,Yulin Li,Zhihai Bi,Jun Ma*

Main category: cs.RO

TL;DR: FLORES is a novel wheel-legged robot with enhanced adaptability and steering capabilities, featuring a unique front-leg configuration and reinforcement learning controller.


<details>
  <summary>Details</summary>
Motivation: Existing wheel-legged robot designs lack integration of flexibility and efficiency for varied terrains.

Method: FLORES employs innovative front-leg hip-yaw design and a tailored reinforcement learning controller for adaptive loco-motion.

Result: Experiments show superior steering, terrain adaptability, and efficiency compared to conventional designs.

Conclusion: FLORES demonstrates the potential of integrating reconfigured mechanics with advanced controllers for versatile robotics.

Abstract: Wheel-legged robots integrate the agility of legs for navigating rough
terrains while harnessing the efficiency of wheels for smooth surfaces.
However, most existing designs do not fully capitalize on the benefits of both
legged and wheeled structures, which limits overall system flexibility and
efficiency. We present FLORES (reconfigured wheel-legged robot for enhanced
steering and adaptability), a novel wheel-legged robot design featuring a
distinctive front-leg configuration that sets it beyond standard design
approaches. Specifically, FLORES replaces the conventional hip-roll degree of
freedom (DoF) of the front leg with hip-yaw DoFs, and this allows for efficient
movement on flat surfaces while ensuring adaptability when navigating complex
terrains. This innovative design facilitates seamless transitions between
different locomotion modes (i.e., legged locomotion and wheeled locomotion) and
optimizes the performance across varied environments. To fully exploit FLORES's
mechanical capabilities, we develop a tailored reinforcement learning (RL)
controller that adapts the Hybrid Internal Model (HIM) with a customized reward
structure optimized for our unique mechanical configuration. This framework
enables the generation of adaptive, multi-modal locomotion strategies that
facilitate smooth transitions between wheeled and legged movements.
Furthermore, our distinctive joint design enables the robot to exhibit novel
and highly efficient locomotion gaits that capitalize on the synergistic
advantages of both locomotion modes. Through comprehensive experiments, we
demonstrate FLORES's enhanced steering capabilities, improved navigation
efficiency, and versatile locomotion across various terrains. The open-source
project can be found at
https://github.com/ZhichengSong6/FLORES-A-Reconfigured-Wheel-Legged-Robot-for-Enhanced-Steering-and-Adaptability.git.

</details>


### [45] [In-Situ Soil-Property Estimation and Bayesian Mapping with a Simulated Compact Track Loader](https://arxiv.org/abs/2507.22356)
*W. Jacob Wagner,Ahmet Soylemezoglu,Katherine Driggs-Campbell*

Main category: cs.RO

TL;DR: This paper introduces an advanced soil-property mapping system to enhance autonomous earthmoving by tracking disturbed and undisturbed soil layers using GPU-accelerated mapping and a physics-infused neural network.


<details>
  <summary>Details</summary>
Motivation: The work is motivated by the limitations of current autonomous earthmoving systems, which struggle in complex and unknown terrain due to partial observability and variable soil conditions.

Method: A GPU-accelerated elevation mapping system combined with a blind mapping component tracks soil dynamics. A physics-infused neural network (PINN) predicts soil properties using data from vehicle-soil interaction simulations, applying Bayesian updates in real-time.

Result: The initial experiments validate the system's ability to pinpoint areas requiring higher interaction forces for terrain shaping, demonstrating promising results for soil-aware autonomous planning.

Conclusion: The proposed system offers a powerful approach to improve autonomous earthmoving processes by accurately mapping and anticipating soil-property dynamics, paving the way for operational efficiency in diverse terrains.

Abstract: Existing earthmoving autonomy is largely confined to highly controlled and
well-characterized environments due to the complexity of vehicle-terrain
interaction dynamics and the partial observability of the terrain resulting
from unknown and spatially varying soil conditions. In this chapter, a a
soil-property mapping system is proposed to extend the environmental state, in
order to overcome these restrictions and facilitate development of more robust
autonomous earthmoving. A GPU accelerated elevation mapping system is extended
to incorporate a blind mapping component which traces the movement of the blade
through the terrain to displace and erode intersected soil, enabling separately
tracking undisturbed and disturbed soil. Each interaction is approximated as a
flat blade moving through a locally homogeneous soil, enabling modeling of
cutting forces using the fundamental equation of earthmoving (FEE). Building
upon our prior work on in situ soil-property estimation, a method is devised to
extract approximate geometric parameters of the model given the uneven terrain,
and an improved physics infused neural network (PINN) model is developed to
predict soil properties and uncertainties of these estimates. A simulation of a
compact track loader (CTL) with a blade attachment is used to collect data to
train the PINN model. Post-training, the model is leveraged online by the
mapping system to track soil property estimates spatially as separate layers in
the map, with updates being performed in a Bayesian manner. Initial experiments
show that the system accurately highlights regions requiring higher relative
interaction forces, indicating the promise of this approach in enabling
soil-aware planning for autonomous terrain shaping.

</details>


### [46] [Improving Generalization Ability of Robotic Imitation Learning by Resolving Causal Confusion in Observations](https://arxiv.org/abs/2507.22380)
*Yifei Chen,Yuzhe Zhang,Giovanni D'urso,Nicholas Lawrance,Brendan Tidd*

Main category: cs.RO

TL;DR: The paper addresses the poor generalization in imitation learning for robotic manipulation by proposing a causal structure learning framework to improve robustness in domain shifts.


<details>
  <summary>Details</summary>
Motivation: Enhancing generalization capabilities of imitation learning algorithms for robotic manipulation to handle unpredictable domain shifts.

Method: They propose a causal structure learning framework involving intervention on imitation learning policies without requiring disentangled feature representations.

Result: The framework is tested on simulations of bimanual robot arms in Mujoco, showing significant mitigation of generalization problems in imitation learning.

Conclusion: Explicitly learning causal relationships can improve generalization in complex robotic manipulation tasks and can be seamlessly integrated into existing architectures.

Abstract: Recent developments in imitation learning have considerably advanced robotic
manipulation. However, current techniques in imitation learning can suffer from
poor generalization, limiting performance even under relatively minor domain
shifts. In this work, we aim to enhance the generalization capabilities of
complex imitation learning algorithms to handle unpredictable changes from the
training environments to deployment environments. To avoid confusion caused by
observations that are not relevant to the target task, we propose to explicitly
learn the causal relationship between observation components and expert
actions, employing a framework similar to [6], where a causal structural
function is learned by intervention on the imitation learning policy.
Disentangling the feature representation from image input as in [6] is hard to
satisfy in complex imitation learning process in robotic manipulation, we
theoretically clarify that this requirement is not necessary in causal
relationship learning. Therefore, we propose a simple causal structure learning
framework that can be easily embedded in recent imitation learning
architectures, such as the Action Chunking Transformer [31]. We demonstrate our
approach using a simulation of the ALOHA [31] bimanual robot arms in Mujoco,
and show that the method can considerably mitigate the generalization problem
of existing complex imitation learning algorithms.

</details>


### [47] [Safety Evaluation of Motion Plans Using Trajectory Predictors as Forward Reachable Set Estimators](https://arxiv.org/abs/2507.22389)
*Kaustav Chakraborty,Zeyuan Feng,Sushant Veer,Apoorva Sharma,Wenhao Ding,Sever Topan,Boris Ivanovic,Marco Pavone,Somil Bansal*

Main category: cs.RO

TL;DR: This paper introduces a safety monitor for autonomous systems by leveraging trajectory predictors to ensure the motion plan is collision-free and safe.


<details>
  <summary>Details</summary>
Motivation: Ensuring the safety of end-to-end autonomy stacks, which lack interpretable intermediate modules, requires a reliable safety monitor to validate their outputs.

Method: The method involves approximating forward reachable sets (FRS) using trajectory predictors, using conformal prediction for calibration, and employing a Bayesian filter for dynamic adjustments in out-of-distribution scenarios.

Result: Experiments on the nuScenes dataset demonstrate significant improvements in ensuring both the soundness and completeness of the safety monitoring system.

Conclusion: The proposed approach provides a practical and reliable safety monitoring method for autonomous systems, ensuring safety through enhanced prediction and calibration methods.

Abstract: The advent of end-to-end autonomy stacks - often lacking interpretable
intermediate modules - has placed an increased burden on ensuring that the
final output, i.e., the motion plan, is safe in order to validate the safety of
the entire stack. This requires a safety monitor that is both complete (able to
detect all unsafe plans) and sound (does not flag safe plans). In this work, we
propose a principled safety monitor that leverages modern multi-modal
trajectory predictors to approximate forward reachable sets (FRS) of
surrounding agents. By formulating a convex program, we efficiently extract
these data-driven FRSs directly from the predicted state distributions,
conditioned on scene context such as lane topology and agent history. To ensure
completeness, we leverage conformal prediction to calibrate the FRS and
guarantee coverage of ground-truth trajectories with high probability. To
preserve soundness in out-of-distribution (OOD) scenarios or under predictor
failure, we introduce a Bayesian filter that dynamically adjusts the FRS
conservativeness based on the predictor's observed performance. We then assess
the safety of the ego vehicle's motion plan by checking for intersections with
these calibrated FRSs, ensuring the plan remains collision-free under plausible
future behaviors of others. Extensive experiments on the nuScenes dataset show
our approach significantly improves soundness while maintaining completeness,
offering a practical and reliable safety monitor for learned autonomy stacks.

</details>


### [48] [Comparing Normalizing Flows with Kernel Density Estimation in Estimating Risk of Automated Driving Systems](https://arxiv.org/abs/2507.22429)
*Erwin de Gelder,Maren Buermann,Olaf Op den Camp*

Main category: cs.RO

TL;DR: The paper explores the use of Normalizing Flows (NF) for estimating scenario exposure in safety validation of Automated Driving Systems (ADSs), showing improved accuracy compared to traditional kernel methods.


<details>
  <summary>Details</summary>
Motivation: To improve the quantification of risk and risk uncertainty in Automated Driving Systems (ADS) by overcoming limitations of current methods for estimating scenario exposure from real-world driving data.

Method: The authors propose using Normalizing Flows (NF), a type of generative model, for estimating the Probability Density Function (PDF) of scenario parameters. These models provide flexible, high-dimensional density estimation without restrictive assumptions about PDF shape.

Result: NF showed improved performance over Kernel Density Estimation (KDE) in handling high-dimensional data, reducing the impact of dimensionality, and providing more accurate risk assessments for ADS.

Conclusion: The study highlights the potential of NF in enhancing safety validation of ADSs by improving risk uncertainty estimation. Future work will explore further NF optimizations for scenario generation and parameter estimation.

Abstract: The development of safety validation methods is essential for the safe
deployment and operation of Automated Driving Systems (ADSs). One of the goals
of safety validation is to prospectively evaluate the risk of an ADS dealing
with real-world traffic. Scenario-based assessment is a widely-used approach,
where test cases are derived from real-world driving data. To allow for a
quantitative analysis of the system performance, the exposure of the scenarios
must be accurately estimated. The exposure of scenarios at parameter level is
expressed using a Probability Density Function (PDF). However, assumptions
about the PDF, such as parameter independence, can introduce errors, while
avoiding assumptions often leads to oversimplified models with limited
parameters to mitigate the curse of dimensionality.
  This paper considers the use of Normalizing Flows (NF) for estimating the PDF
of the parameters. NF are a class of generative models that transform a simple
base distribution into a complex one using a sequence of invertible and
differentiable mappings, enabling flexible, high-dimensional density estimation
without restrictive assumptions on the PDF's shape. We demonstrate the
effectiveness of NF in quantifying risk and risk uncertainty of an ADS,
comparing its performance with Kernel Density Estimation (KDE), a traditional
method for non-parametric PDF estimation. While NF require more computational
resources compared to KDE, NF is less sensitive to the curse of dimensionality.
As a result, NF can improve risk uncertainty estimation, offering a more
precise assessment of an ADS's safety.
  This work illustrates the potential of NF in scenario-based safety. Future
work involves experimenting more with using NF for scenario generation and
optimizing the NF architecture, transformation types, and training
hyperparameters to further enhance their applicability.

</details>


### [49] [Operationalization of Scenario-Based Safety Assessment of Automated Driving Systems](https://arxiv.org/abs/2507.22433)
*Olaf Op den Camp,Erwin de Gelder*

Main category: cs.RO

TL;DR: The paper discusses methods for conducting safety assessments of Automated Driving Systems (ADS) using scenario databases in alignment with the UNECE WP.29 NATM framework.


<details>
  <summary>Details</summary>
Motivation: To ensure the safe deployment of Automated Driving Systems at scale and standardize the safety assessment process globally, leveraging the guidelines of UNECE WP.29's NATM framework.

Method: The paper proposes integrating scenario databases and enriching them with practices inspired by Horizon Europe projects to operationalize and enhance the NATM approach for ADS safety evaluations.

Result: Effective utilization of scenario databases is demonstrated, and additional steps for fully operationalizing NATM in ADS safety assessments are identified.

Conclusion: Scenario databases are vital tools for structuring ADS safety assessments under the NATM framework, and collaboration with Horizon Europe projects enhances their implementation.

Abstract: Before introducing an Automated Driving System (ADS) on the road at scale,
the manufacturer must conduct some sort of safety assurance. To structure and
harmonize the safety assurance process, the UNECE WP.29 Working Party on
Automated/Autonomous and Connected Vehicles (GRVA) is developing the New
Assessment/Test Method (NATM) that indicates what steps need to be taken for
safety assessment of an ADS. In this paper, we will show how to practically
conduct safety assessment making use of a scenario database, and what
additional steps must be taken to fully operationalize the NATM. In addition,
we will elaborate on how the use of scenario databases fits with methods
developed in the Horizon Europe projects that focus on safety assessment
following the NATM approach.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [50] [RedCoder: Automated Multi-Turn Red Teaming for Code LLMs](https://arxiv.org/abs/2507.22063)
*Wenjie Jacky Mo,Qin Liu,Xiaofei Wen,Dongwon Jung,Hadi Askari,Wenxuan Zhou,Zhe Zhao,Muhao Chen*

Main category: cs.SE

TL;DR: The paper introduces RedCoder, a red-teaming agent that autonomously engages in multi-turn conversations to test and expose vulnerabilities in code generated by large language models (LLMs), outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: The study addresses the challenge of identifying vulnerabilities or malicious outputs in code generated by LLMs under adversarial conditions. Prior approaches require extensive human effort and lack scalability, while also failing to consider the multi-turn interactive nature of real-world programming scenarios.

Method: The authors developed RedCoder using a multi-agent gaming approach to generate prototype conversations and attack strategies. This was followed by fine-tuning an LLM on the simulated conversations to enable dynamic and autonomous multi-turn dialogues, retrieving relevant strategies from a pre-built arsenal to induce vulnerabilities in code outputs.

Result: RedCoder outperformed prior single-turn and multi-turn methods in successfully inducing vulnerable code across multiple Code LLMs. It also demonstrated scalability and effectiveness as a security evaluation tool for code-generation systems.

Conclusion: The study highlights the potential of RedCoder as a scalable, autonomous agent for stress-testing the security boundaries of code-generation LLMs, emphasizing its application in improving the safety of AI-assisted software development.

Abstract: Large Language Models (LLMs) for code generation (i.e., Code LLMs) have
demonstrated impressive capabilities in AI-assisted software development and
testing. However, recent studies have shown that these models are prone to
generating vulnerable or even malicious code under adversarial settings.
Existing red-teaming approaches rely on extensive human effort, limiting their
scalability and practicality, and generally overlook the interactive nature of
real-world AI-assisted programming, which often unfolds over multiple turns. To
bridge these gaps, we present RedCoder, a red-teaming agent that engages victim
models in multi-turn conversation to elicit vulnerable code. The pipeline to
construct RedCoder begins with a multi-agent gaming process that simulates
adversarial interactions, yielding a set of prototype conversations and an
arsenal of reusable attack strategies. We then fine-tune an LLM on these
prototype conversations to serve as the backbone of RedCoder. Once deployed,
RedCoder autonomously engages Code LLMs in multi-turn conversations,
dynamically retrieving relevant strategies from the arsenal to steer the
dialogue toward vulnerability-inducing outputs. Experiments across multiple
Code LLMs show that our approach outperforms prior single-turn and multi-turn
red-team methods in inducing vulnerabilities in code generation, offering a
scalable and effective tool for evaluating the security boundaries of modern
code-generation systems.

</details>


### [51] [Fuzzing: Randomness? Reasoning! Efficient Directed Fuzzing via Large Language Models](https://arxiv.org/abs/2507.22065)
*Xiaotao Feng,Xiaogang Zhu,Kun Hu,Jincheng Wang,Yingjie Cao,Guang Gong,Jianfeng Pan*

Main category: cs.SE

TL;DR: RandLuzz integrates large language models (LLMs) with directed fuzzing to improve seed and mutator quality, significantly accelerating bug detection in programs.


<details>
  <summary>Details</summary>
Motivation: The inefficiency caused by randomness in fuzzing slows down bug detection, even in directed fuzzers. This randomness primarily stems from seed and mutator selection, which are critical to exposing bugs efficiently.

Method: RandLuzz uses LLMs for generating targeted seeds and constructing bug-specific mutators. It guides LLMs with function call chains and bug analysis to reduce randomness in fuzzing. This leads to faster bug exposure.

Result: RandLuzz outperforms four state-of-the-art fuzzers (AFLGo, Beacon, WindRanger, and SelectFuzz), achieving speedups from 2.1× to 4.8× on average and exposing some bugs in as little as 60 seconds.

Conclusion: Integrating LLMs with directed fuzzing through RandLuzz can drastically improve the efficiency of bug detection by reducing randomness in seeds and mutators while maintaining high effectiveness.

Abstract: Fuzzing is highly effective in detecting bugs due to the key contribution of
randomness. However, randomness significantly reduces the efficiency of
fuzzing, causing it to cost days or weeks to expose bugs. Even though directed
fuzzing reduces randomness by guiding fuzzing towards target buggy locations,
the dilemma of randomness still challenges directed fuzzers. Two critical
components, which are seeds and mutators, contain randomness and are closely
tied to the conditions required for triggering bugs. Therefore, to address the
challenge of randomness, we propose to use large language models (LLMs) to
remove the randomness in seeds and reduce the randomness in mutators. With
their strong reasoning and code generation capabilities, LLMs can be used to
generate reachable seeds that target pre-determined locations and to construct
bug-specific mutators tailored for specific bugs. We propose RandLuzz, which
integrates LLMs and directed fuzzing, to improve the quality of seeds and
mutators, resulting in efficient bug exposure. RandLuzz analyzes function call
chain or functionality to guide LLMs in generating reachable seeds. To
construct bug-specific mutators, RandLuzz uses LLMs to perform bug analysis,
obtaining information such as bug causes and mutation suggestions, which
further help generate code that performs bug-specific mutations. We evaluate
RandLuzz by comparing it with four state-of-the-art directed fuzzers, AFLGo,
Beacon, WindRanger, and SelectFuzz. With RandLuzz-generated seeds, the fuzzers
achieve an average speedup ranging from 2.1$\times$ to 4.8$\times$ compared to
using widely-used initial seeds. Additionally, when evaluated on individual
bugs, RandLuzz achieves up to a 2.7$\times$ speedup compared to the
second-fastest exposure. On 8 bugs, RandLuzz can even expose them within 60
seconds.

</details>


### [52] [Machine Learning Experiences: A story of learning AI for use in enterprise software testing that can be used by anyone](https://arxiv.org/abs/2507.22064)
*Michael Cohoon,Debbie Furman*

Main category: cs.SE

TL;DR: The paper describes a machine learning workflow applied by a software testing team, detailing key steps for project implementation.


<details>
  <summary>Details</summary>
Motivation: To document and share the team's progression through a machine learning workflow and provide a template for others.

Method: The paper outlines a step-by-step ML implementation process including data gathering, cleaning, feature engineering, model training, and evaluation.

Result: The described ML workflow was applied successfully, demonstrating its effectiveness.

Conclusion: The workflow is a practical guide that can be replicated by anyone wishing to apply machine learning in their projects.

Abstract: This paper details the machine learning (ML) journey of a group of people
focused on software testing. It tells the story of how this group progressed
through a ML workflow (similar to the CRISP-DM process). This workflow consists
of the following steps and can be used by anyone applying ML techniques to a
project: gather the data; clean the data; perform feature engineering on the
data; splitting the data into two sets, one for training and one for testing;
choosing a machine learning model; training the model; testing the model and
evaluating the model performance. By following this workflow, anyone can
effectively apply ML to any project that they are doing.

</details>


### [53] [Automated Test Data Generation for Enterprise Protobuf Systems: A Metaclass-Enhanced Statistical Approach](https://arxiv.org/abs/2507.22070)
*Y. Du*

Main category: cs.SE

TL;DR: The paper introduces a framework for efficient test data generation in enterprise systems using Protocol Buffers, achieving significant improvements in preparation time and test coverage.


<details>
  <summary>Details</summary>
Motivation: Traditional methods struggle with the complexity of nested and hierarchical protobuf structures, leading to inefficiencies in performance testing.

Method: The proposed framework utilizes Python's metaclass system, schema introspection, statistical value domain extraction from logs, and recursive descent algorithms for generating test data.

Result: Experiments show a 95% reduction in data preparation time, 80% increase in test coverage, and capability to handle 15 levels of nesting.

Conclusion: The framework is highly efficient, scalable, and effective in generating comprehensive test cases for complex protobuf structures in enterprise environments.

Abstract: Large-scale enterprise systems utilizing Protocol Buffers (protobuf) present
significant challenges for performance testing, particularly when targeting
intermediate business interfaces with complex nested data structures.
Traditional test data generation approaches are inadequate for handling the
intricate hierarchical and graph-like structures inherent in enterprise
protobuf schemas. This paper presents a novel test data generation framework
that leverages Python's metaclass system for dynamic type enhancement and
statistical analysis of production logs for realistic value domain extraction.
Our approach combines automatic schema introspection, statistical value
distribution analysis, and recursive descent algorithms for handling deeply
nested structures. Experimental evaluation on three real-world enterprise
systems demonstrates up to 95\% reduction in test data preparation time and
80\% improvement in test coverage compared to existing approaches. The
framework successfully handles protobuf structures with up to 15 levels of
nesting and generates comprehensive test suites containing over 100,000 test
cases within seconds.

</details>


### [54] [TypyBench: Evaluating LLM Type Inference for Untyped Python Repositories](https://arxiv.org/abs/2507.22086)
*Honghua Dong,Jiacheng Yang,Xun Deng,Yuhe Jiang,Gennady Pekhimenko,Fan Long,Xujie Si*

Main category: cs.SE

TL;DR: The paper presents TypyBench, a benchmark for evaluating type inference capabilities of large language models (LLMs) in Python repositories, highlighting their struggles with type consistency and complex types.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the persistent challenge of type inference in dynamic programming languages like Python, leveraging the potential of LLMs.

Method: The researchers developed TypyBench with two metrics, TypeSim and TypeCheck, to evaluate type inference in Python repositories. They also curated a dataset of 50 high-quality repositories.

Result: The evaluation revealed LLMs performed adequately in semantic type similarity (TypeSim) but struggled with nested types and consistency (TypeCheck).

Conclusion: The paper underscores the need for research to prioritize repository-level type consistency over type similarity, with TypyBench serving as a foundation for such efforts.

Abstract: Type inference for dynamic languages like Python is a persistent challenge in
software engineering. While large language models (LLMs) have shown promise in
code understanding, their type inference capabilities remain underexplored. We
introduce TypyBench, a benchmark designed to evaluate LLMs' type inference
across entire Python repositories. TypyBench features two novel metrics:
TypeSim, which captures nuanced semantic relationships between predicted and
ground truth types, and TypeCheck, which assesses type consistency across
codebases. Our evaluation of various LLMs on a curated dataset of 50
high-quality Python repositories reveals that, although LLMs achieve decent
TypeSim scores, they struggle with complex nested types and exhibit significant
type consistency errors. These findings suggest that future research should
shift focus from improving type similarity to addressing repository-level
consistency. TypyBench provides a foundation for this new direction, offering
insights into model performance across different type complexities and usage
contexts. Our code and data are available at
https://github.com/typybench/typybench.

</details>


### [55] [CodableLLM: Automating Decompiled and Source Code Mapping for LLM Dataset Generation](https://arxiv.org/abs/2507.22066)
*Dylan Manuel,Paul Rad*

Main category: cs.SE

TL;DR: The paper introduces CodableLLM, a Python framework that automates the creation of aligned datasets for decompiled and source code, aimed at improving code-focused LLMs.


<details>
  <summary>Details</summary>
Motivation: The lack of high-quality datasets aligning decompiled binaries with source code limits the potential of LLMs in code understanding and generation.

Method: CodableLLM automates the generation of datasets by integrating with decompilers and parsers, mapping decompiled functions to their corresponding source functions across programming languages.

Result: CodableLLM outperforms existing tools in terms of robustness and efficiency in creating datasets tailored for code-centric LLMs.

Conclusion: CodableLLM provides an effective solution to improve dataset quality for LLMs in coding, supporting advancements in cross-level code understanding and generation.

Abstract: The generation of large, high-quality datasets for code understanding and
generation remains a significant challenge, particularly when aligning
decompiled binaries with their original source code. To address this, we
present CodableLLM, a Python framework designed to automate the creation and
curation of datasets by mapping decompiled functions to their corresponding
source functions. This process enhances the alignment between decompiled and
source code representations, facilitating the development of large language
models (LLMs) capable of understanding and generating code across multiple
abstraction levels. CodableLLM supports multiple programming languages and
integrates with existing decompilers and parsers to streamline dataset
generation. This paper presents the design and implementation of CodableLLM,
evaluates its performance in dataset creation, and compares it to existing
tools in the field. The results demonstrate that CodableLLM offers a robust and
efficient solution for generating datasets tailored for code-focused LLMS.

</details>


### [56] [Analyzing and Evaluating the Behavior of Git Diff and Merge](https://arxiv.org/abs/2507.22071)
*Niels Glodny*

Main category: cs.SE

TL;DR: The paper examines Git's diff and merge algorithms, discovering unexpected behaviors like pathological cases and unpredictable merge outcomes.


<details>
  <summary>Details</summary>
Motivation: Understanding Git's diff and merge algorithms to document their key functionalities and nuances for potential broader applications.

Method: The paper investigates Git's diff computation, merge operations, and complex functionalities through systematic analysis.

Result: It highlights behaviors like the histogram diff algorithm's sensitivity, exponential runtime in some merge strategies, and non-commutative operations leading to unpredictable outcomes.

Conclusion: Git's diff and merge mechanisms have quirks and non-standard behaviors that need deeper comprehension for effective use and adaptation.

Abstract: Despite being widely used, the algorithms that enable collaboration with Git
are not well understood. The diff and merge algorithms are particularly
interesting, as they could be applied in other contexts. In this thesis, I
document the main functionalities of Git: how diffs are computed, how they are
used to run merges, and how merges enable more complex operations. In the
process, I show multiple unexpected behaviors in Git, including the following:
The histogram diff algorithm has pathological cases where a single-line change
can cause the entire rest of the file to be marked as changed. The default
merge strategy (ort) can result in merges requiring exponential time in the
number of commits in the history. Merges and rebases are not commutative, and
even when merges do not result in a conflict, the result is not specified but
depends on the diff algorithm used. And finally, sometimes when two sides of a
merge add different lines at the same position, the result is not a conflict,
but a merge containing both changes after each other, in arbitrary order.

</details>


### [57] [CodeEvo: Interaction-Driven Synthesis of Code-centric Data through Hybrid and Iterative Feedback](https://arxiv.org/abs/2507.22080)
*Qiushi Sun,Jinyang Gong,Lei Li,Qipeng Guo,Fei Yuan*

Main category: cs.SE

TL;DR: CodeEvo proposes a novel framework using iterative interactions between LLM agents to enhance the quality and effectiveness of instruction-code pair generation for training code generation models.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the issues of limited scale and low quality in manually curated and existing synthetic instruction-code pair datasets for training Large Language Models (LLMs) for code generation.

Method: CodeEvo leverages two LLM agents—a Coder and a Reviewer—that interact iteratively to produce and refine code and instructions. It introduces a hybrid feedback mechanism combining compiler determinism with agent generative flexibility for improving quality control.

Result: Models trained on CodeEvo data achieve significant performance improvements over established baselines in various code generation benchmarks.

Conclusion: CodeEvo showcases the effectiveness of iterative synthesis and hybrid feedback mechanisms for generating high-quality instruction-code pairs, paving the way for advancements in code-centric data synthesis methodologies.

Abstract: Acquiring high-quality instruction-code pairs is essential for training Large
Language Models (LLMs) for code generation. Manually curated data is expensive
and inherently limited in scale, motivating the development of code-centric
synthesis methods. Yet, current approaches either focus on augmenting existing
code or rely on predefined heuristics, both lacking rigorous data validation,
which results in synthetic data that is ungrounded, repetitive, or overly
simplistic. Inspired by collaborative programming practices, we propose
CodeEvo, a framework that synthesizes code data through iterative interactions
between two LLM agents: a Coder, which generates candidate code and test cases
based on given instructions, and a Reviewer, which guides the synthesis process
by producing new instructions and feedback. We further introduce a hybrid
feedback mechanism that combines compiler determinism with the generative
flexibility of agents, enabling automatic quality control throughout synthesis.
Extensive experiments demonstrate that models fine-tuned on CodeEvo data
significantly outperform established baselines across code generation
benchmarks with various difficulties. In-depth analyses further provide
insights from multiple perspectives into effective code-centric data synthesis.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [58] [Dimensions of Vulnerability in Visual Working Memory: An AI-Driven Approach to Perceptual Comparison](https://arxiv.org/abs/2507.22067)
*Yuang Cao,Jiachen Zou,Chen Wei,Quanying Liu*

Main category: q-bio.NC

TL;DR: The study investigates how perceptual comparisons affect memory distortions and reveals that visual dimensions are more prone to distortion than semantic dimensions.


<details>
  <summary>Details</summary>
Motivation: To understand how perceptual comparisons influence memory distortions, particularly in real-world objects, and to identify the visual features contributing to memory vulnerabilities.

Method: Developed an AI-driven framework to generate two types of stimuli -- image wheels and dimension wheels. Conducted three visual working memory experiments to test memory distortions under various perceptual comparison conditions.

Result: Memory was distorted by similar dimensions and images, with visual dimensions being more vulnerable to distortion than semantic dimensions.

Conclusion: Object dimensions in naturalistic visual stimuli significantly influence memory vulnerabilities, and perceptual comparisons with similar dimensions exacerbate these distortions.

Abstract: Human memory exhibits significant vulnerability in cognitive tasks and daily
life. Comparisons between visual working memory and new perceptual input (e.g.,
during cognitive tasks) can lead to unintended memory distortions. Previous
studies have reported systematic memory distortions after perceptual
comparison, but understanding how perceptual comparison affects memory
distortions in real-world objects remains a challenge. Furthermore, identifying
what visual features contribute to memory vulnerability presents a novel
research question. Here, we propose a novel AI-driven framework that generates
naturalistic visual stimuli grounded in behaviorally relevant object dimensions
to elicit similarity-induced memory biases. We use two types of stimuli --
image wheels created through dimension editing and dimension wheels generated
by dimension activation values -- in three visual working memory (VWM)
experiments. These experiments assess memory distortions under three
conditions: no perceptual comparison, perceptual comparison with image wheels,
and perceptual comparison with dimension wheels. The results show that similar
dimensions, like similar images, can also induce memory distortions.
Specifically, visual dimensions are more prone to distortion than semantic
dimensions, indicating that the object dimensions of naturalistic visual
stimuli play a significant role in the vulnerability of memory.

</details>


### [59] [Representation biases: will we achieve complete understanding by analyzing representations?](https://arxiv.org/abs/2507.22216)
*Andrew Kyle Lampinen,Stephanie C. Y. Chan,Yuxuan Li,Katherine Hermann*

Main category: q-bio.NC

TL;DR: Neural representations analysis may be biased, affecting computational and neuroscientific insights.


<details>
  <summary>Details</summary>
Motivation: To understand biases in feature representations that could hinder interpreting neural and computational systems.

Method: The paper illustrates biases using analyses such as PCA, regression, and RSA, and uses homomorphic encryption as a case study.

Result: Feature representation biases lead to skewed inferences, demonstrating challenges in representational comparisons.

Conclusion: Researchers should account for biases in representation to avoid flawed system interpretations.

Abstract: A common approach in neuroscience is to study neural representations as a
means to understand a system -- increasingly, by relating the neural
representations to the internal representations learned by computational
models. However, a recent work in machine learning (Lampinen, 2024) shows that
learned feature representations may be biased to over-represent certain
features, and represent others more weakly and less-consistently. For example,
simple (linear) features may be more strongly and more consistently represented
than complex (highly nonlinear) features. These biases could pose challenges
for achieving full understanding of a system through representational analysis.
In this perspective, we illustrate these challenges -- showing how feature
representation biases can lead to strongly biased inferences from common
analyses like PCA, regression, and RSA. We also present homomorphic encryption
as a simple case study of the potential for strong dissociation between
patterns of representation and computation. We discuss the implications of
these results for representational comparisons between systems, and for
neuroscience more generally.

</details>


### [60] [Decoding Neural Signatures of Semantic Evaluations in Depression and Suicidality](https://arxiv.org/abs/2507.22313)
*Woojae Jeong,Aditya Kommineni,Kleanthis Avramidis,Colin McDaniel,Donald Berry,Myzelle Hughes,Thomas McGee,Elsi Kaiser,Dani Byrd,Assal Habibi,B. Rael Cahn,Idan A. Blank,Kristina Lerman,Dimitrios Pantazis,Sudarsana R. Kadiri,Takfarinas Medani,Shrikanth Narayanan,Richard M. Leahy*

Main category: q-bio.NC

TL;DR: This paper explores EEG-detected neural dynamics in emotional sentence evaluation tasks to identify biomarkers for depression and suicidality.


<details>
  <summary>Details</summary>
Motivation: To address the lack of objective neurophysiological biomarkers for depression and suicidality by examining neural responses to emotional semantic content.

Method: Using EEG data on 137 participants, the study employed multivariate decoding during emotional sentence evaluation tasks to analyze spatiotemporal neural dynamics tied to depression and suicidality.

Result: People with depression and suicidality displayed earlier onset, longer duration, and intensified neural responses, along with broader activation in specific brain regions, compared to healthy controls.

Conclusion: The findings highlight altered neural sensitivity and disengagement impairments in clinical groups, paving the way for EEG-based biomarkers of depression and suicidality.

Abstract: Depression and suicidality profoundly impact cognition and emotion, yet
objective neurophysiological biomarkers remain elusive. We investigated the
spatiotemporal neural dynamics underlying affective semantic processing in
individuals with varying levels of clinical severity of depression and
suicidality using multivariate decoding of electroencephalography (EEG) data.
Participants (N=137) completed a sentence evaluation task involving emotionally
charged self-referential statements while EEG was recorded. We identified
robust, neural signatures of semantic processing, with peak decoding accuracy
between 300-600 ms -- a window associated with automatic semantic evaluation
and conflict monitoring. Compared to healthy controls, individuals with
depression and suicidality showed earlier onset, longer duration, and greater
amplitude decoding responses, along with broader cross-temporal generalization
and increased activation of frontocentral and parietotemporal components. These
findings suggest altered sensitivity and impaired disengagement from
emotionally salient content in the clinical groups, advancing our understanding
of the neurocognitive basis of mental health and providing a principled basis
for developing reliable EEG-based biomarkers of depression and suicidality.

</details>


### [61] [An Uncertainty Principle for Probabilistic Computation in the Retina](https://arxiv.org/abs/2507.22785)
*Jayanth R Taranath,Salim M'Jahad*

Main category: q-bio.NC

TL;DR: The paper presents a probabilistic model of early visual processing that incorporates quantum-inspired computation principles and biological variability.


<details>
  <summary>Details</summary>
Motivation: The paper aims to challenge the traditional deterministic view of retinal processing and establish a new framework based on probabilistic principles influenced by quantum photon statistics and intrinsic biological variability.

Method: The authors develop a probabilistic model describing light's transformation into neural signals in the retina, formalized using an uncertainty relation. They base their approach on prior experimental findings and propose future experiments to test their hypotheses.

Result: The model explains intrinsic variability in retinal responses to fixed stimuli and introduces a framework for integrating quantum-inspired computation into neural processing.

Conclusion: By redefining the retina as a probabilistic measurement device, the study provides a basis for exploring cortical dynamics with quantum-inspired computational principles, bridging classical and probabilistic systems.

Abstract: We introduce a probabilistic model of early visual processing, beginning with
the interaction between a light wavefront and the retina. We argue that
perception originates not with deterministic transduction, but with
probabilistic threshold crossings shaped by quantum photon arrival statistics
and biological variability. We formalize this with an uncertainty relation, \(
\Delta \alpha \cdot \Delta t \geq \eta \), through the transformation of light
into symbolic neural code through the layered retinal architecture. Our model
is supported by previous experimental results, which show intrinsic variability
in retinal responses even under fixed stimuli. We contrast this with a
classical null hypothesis of deterministic encoding and propose experiments to
further test our uncertainty relation. By re-framing the retina as a
probabilistic measurement device, we lay the foundation for future models of
cortical dynamics rooted in quantum-like computation. We are not claiming that
the brain could be working as a quantum-system, but rather putting forth the
argument that the brain as a classical system could still implement
quantum-inspired computations. We define quantum-inspired computation as a
scheme that includes both probabilistic and time-sensitive computation, clearly
separating it from classically implementable probabilistic systems.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [62] [Simulating Posterior Bayesian Neural Networks with Dependent Weights](https://arxiv.org/abs/2507.22095)
*Nicola Apollonio,Giovanni Franzina,Giovanni Luca Torrisi*

Main category: stat.ML

TL;DR: The paper analyzes Bayesian deep neural networks with dependent weights and explores their behavior under Gaussian likelihoods, presenting algorithms and numerical validations.


<details>
  <summary>Details</summary>
Motivation: To better understand the behavior and distributional properties of deep neural networks with dependent weights, especially under Bayesian frameworks.

Method: Introducing posterior Bayesian models for fully connected and feedforward networks, deriving theoretical distributions for wide-width networks, and performing numerical validations.

Result: Wide-width networks are characterized under Gaussian likelihoods, with shallow networks proven to produce output as Gaussian mixtures. Results are validated through simulations.

Conclusion: Bayesian dependent-weight neural networks exhibit Gaussian mixture properties in shallow configurations and show specific distributional behaviors as width increases, validated computationally.

Abstract: In this paper we consider posterior Bayesian fully connected and feedforward
deep neural networks with dependent weights. Particularly, if the likelihood is
Gaussian, we identify the distribution of the wide width limit and provide an
algorithm to sample from the network. In the shallow case we explicitly compute
the distribution of the output, proving that it is a Gaussian mixture. All the
theoretical results are numerically validated.

</details>


### [63] [Stacked SVD or SVD stacked? A Random Matrix Theory perspective on data integration](https://arxiv.org/abs/2507.22170)
*Tavor Z. Baharav,Phillip B. Nicol,Rafael A. Irizarry,Rong Ma*

Main category: stat.ML

TL;DR: The paper compares two approaches for extracting shared latent structures in noisy high-dimensional data—Stack-SVD and SVD-Stack—and introduces optimal weighting schemes and theoretical analyses to guide method selection.


<details>
  <summary>Details</summary>
Motivation: To understand and choose between Stack-SVD and SVD-Stack methods for integrating high-dimensional datasets in practical applications, addressing the lack of theoretical insights in the proportional asymptotic regime.

Method: Developed exact asymptotic performance metrics, phase transition expressions, and optimal weighting schemes for both methods, extending analyses to multiple components and offering practical algorithms for weight estimation.

Result: Optimally weighted Stack-SVD surpasses optimally weighted SVD-Stack. Extensive simulations and experiments on genomic data validate theoretical findings.

Conclusion: Weighted approaches improve the performance of both methods without uniform dominance in unweighted cases, providing theoretical foundations and practical guidance for data integration problems.

Abstract: Modern data analysis increasingly requires identifying shared latent
structure across multiple high-dimensional datasets. A commonly used model
assumes that the data matrices are noisy observations of low-rank matrices with
a shared singular subspace. In this case, two primary methods have emerged for
estimating this shared structure, which vary in how they integrate information
across datasets. The first approach, termed Stack-SVD, concatenates all the
datasets, and then performs a singular value decomposition (SVD). The second
approach, termed SVD-Stack, first performs an SVD separately for each dataset,
then aggregates the top singular vectors across these datasets, and finally
computes a consensus amongst them. While these methods are widely used, they
have not been rigorously studied in the proportional asymptotic regime, which
is of great practical relevance in today's world of increasing data size and
dimensionality. This lack of theoretical understanding has led to uncertainty
about which method to choose and limited the ability to fully exploit their
potential. To address these challenges, we derive exact expressions for the
asymptotic performance and phase transitions of these two methods and develop
optimal weighting schemes to further improve both methods. Our analysis reveals
that while neither method uniformly dominates the other in the unweighted case,
optimally weighted Stack-SVD dominates optimally weighted SVD-Stack. We extend
our analysis to accommodate multiple shared components, and provide practical
algorithms for estimating optimal weights from data, offering theoretical
guidance for method selection in practical data integration problems. Extensive
numerical simulations and semi-synthetic experiments on genomic data
corroborate our theoretical findings.

</details>


### [64] [LVM-GP: Uncertainty-Aware PDE Solver via coupling latent variable model and Gaussian process](https://arxiv.org/abs/2507.22493)
*Xiaodong Feng,Ling Guo,Xiaoliang Wan,Hao Wu,Tao Zhou,Wenwen Zhou*

Main category: stat.ML

TL;DR: The paper presents LVM-GP, a novel probabilistic framework for uncertainty quantification in solving PDEs with noisy data, integrating a Gaussian process and neural operator, and surpassing existing methods like B-PINNs in prediction accuracy and uncertainty handling.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of uncertainty quantification in solving forward and inverse PDEs using noisy data, improving upon existing methods like B-PINNs in terms of efficiency, accuracy, and robustness.

Method: The framework uses a stochastic mapping via a confidence-aware encoder with a Gaussian process-based latent variable model and a probabilistic decoder. Soft constraints enforce physical law consistency, and a neural operator enables flexible function-to-function mappings.

Result: Demonstrated improved predictive accuracy and reliable uncertainty quantification through numerical experiments, outperforming methods like B-PINNs and deep ensembles.

Conclusion: LVM-GP effectively integrates probabilistic modeling and neural operators, achieving robust and accurate solutions to PDEs, making it a competitive and reliable solution compared to existing methods.

Abstract: We propose a novel probabilistic framework, termed LVM-GP, for uncertainty
quantification in solving forward and inverse partial differential equations
(PDEs) with noisy data. The core idea is to construct a stochastic mapping from
the input to a high-dimensional latent representation, enabling
uncertainty-aware prediction of the solution. Specifically, the architecture
consists of a confidence-aware encoder and a probabilistic decoder. The encoder
implements a high-dimensional latent variable model based on a Gaussian process
(LVM-GP), where the latent representation is constructed by interpolating
between a learnable deterministic feature and a Gaussian process prior, with
the interpolation strength adaptively controlled by a confidence function
learned from data. The decoder defines a conditional Gaussian distribution over
the solution field, where the mean is predicted by a neural operator applied to
the latent representation, allowing the model to learn flexible
function-to-function mapping. Moreover, physical laws are enforced as soft
constraints in the loss function to ensure consistency with the underlying PDE
structure. Compared to existing approaches such as Bayesian physics-informed
neural networks (B-PINNs) and deep ensembles, the proposed framework can
efficiently capture functional dependencies via merging a latent Gaussian
process and neural operator, resulting in competitive predictive accuracy and
robust uncertainty quantification. Numerical experiments demonstrate the
effectiveness and reliability of the method.

</details>


### [65] [A Unified Analysis of Generalization and Sample Complexity for Semi-Supervised Domain Adaptation](https://arxiv.org/abs/2507.22632)
*Elif Vural,Huseyin Karaca*

Main category: stat.ML

TL;DR: This paper provides a theoretical study of domain adaptation algorithms based on feature alignment, analyzing sample complexity in semi-supervised settings and offering generalization bounds.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address gaps in the theoretical understanding of domain adaptation, particularly for modern approaches involving feature transformations and shared spaces.

Method: The paper employs rigorous mathematical analysis, deriving generalization bounds and characterizing sample complexity for MMD-based and adversarial domain-adaptive neural networks.

Result: The study finds that sample complexity scales quadratically with network depth and width, and suggests proportional scaling of target loss to optimize limited labeled target data in semi-supervised settings.

Conclusion: This work deepens understanding of domain-adaptive algorithms' theoretical properties, providing insights into their behavior and guidance for their practical use.

Abstract: Domain adaptation seeks to leverage the abundant label information in a
source domain to improve classification performance in a target domain with
limited labels. While the field has seen extensive methodological development,
its theoretical foundations remain relatively underexplored. Most existing
theoretical analyses focus on simplified settings where the source and target
domains share the same input space and relate target-domain performance to
measures of domain discrepancy. Although insightful, these analyses may not
fully capture the behavior of modern approaches that align domains into a
shared space via feature transformations. In this paper, we present a
comprehensive theoretical study of domain adaptation algorithms based on domain
alignment. We consider the joint learning of domain-aligning feature
transformations and a shared classifier in a semi-supervised setting. We first
derive generalization bounds in a broad setting, in terms of covering numbers
of the relevant function classes. We then extend our analysis to characterize
the sample complexity of domain-adaptive neural networks employing maximum mean
discrepancy (MMD) or adversarial objectives. Our results rely on a rigorous
analysis of the covering numbers of these architectures. We show that, for both
MMD-based and adversarial models, the sample complexity admits an upper bound
that scales quadratically with network depth and width. Furthermore, our
analysis suggests that in semi-supervised settings, robustness to limited
labeled target data can be achieved by scaling the target loss proportionally
to the square root of the number of labeled target samples. Experimental
evaluation in both shallow and deep settings lends support to our theoretical
findings.

</details>


### [66] [Subgrid BoostCNN: Efficient Boosting of Convolutional Networks via Gradient-Guided Feature Selection](https://arxiv.org/abs/2507.22842)
*Biyi Fang,Jean Utke,Truong Vo,Diego Klabjan*

Main category: stat.ML

TL;DR: The paper proposes a framework, BoostCNN, that enhances CNN performance through dynamic feature selection and boosting weights.


<details>
  <summary>Details</summary>
Motivation: Conventional CNNs require extensive time and manual effort due to their deep architectures and computational expense, limiting efficiency.

Method: A novel framework using dynamic feature selection with subgrid selection, importance sampling, and embedding boosting weights via least squares loss formulation.

Result: Boosted CNN variants surpass traditional CNNs in predictive accuracy and training speed across numerous classification benchmarks.

Conclusion: The integration of dynamic selection and boosting effectively reduces manual design complexity while improving efficiency and performance.

Abstract: Convolutional Neural Networks (CNNs) have achieved remarkable success across
a wide range of machine learning tasks by leveraging hierarchical feature
learning through deep architectures. However, the large number of layers and
millions of parameters often make CNNs computationally expensive to train,
requiring extensive time and manual tuning to discover optimal architectures.
In this paper, we introduce a novel framework for boosting CNN performance that
integrates dynamic feature selection with the principles of BoostCNN. Our
approach incorporates two key strategies: subgrid selection and importance
sampling, to guide training toward informative regions of the feature space. We
further develop a family of algorithms that embed boosting weights directly
into the network training process using a least squares loss formulation. This
integration not only alleviates the burden of manual architecture design but
also enhances accuracy and efficiency. Experimental results across several
fine-grained classification benchmarks demonstrate that our boosted CNN
variants consistently outperform conventional CNNs in both predictive
performance and training speed.

</details>


### [67] [Consistency of Feature Attribution in Deep Learning Architectures for Multi-Omics](https://arxiv.org/abs/2507.22877)
*Daniel Claborne,Javier Flores,Samantha Erwin,Luke Durell,Rachel Richardson,Ruby Fore,Lisa Bramer*

Main category: stat.ML

TL;DR: This paper explores the use of Shapley Additive Explanations (SHAP) in multi-view deep learning models applied to multi-omics data for feature attribution but finds sensitivity in feature rankings due to factors like architecture and random initialization.


<details>
  <summary>Details</summary>
Motivation: To address challenges in interpretability of machine and deep learning models in biological research, particularly for identifying key biomolecules in multi-omics data analysis.

Method: The authors used SHAP on multi-view deep learning models to rank features and compared these rankings across architectures. They conducted computational experiments to evaluate SHAP's robustness and effectiveness using metrics like random forest accuracy and clustering quality.

Result: The study found that SHAP's feature rankings are sensitive to the choice of model architecture and random weight initializations, raising concerns about the method's reliability in this context.

Conclusion: While SHAP offers potential for feature attribution in complex models, its sensitivity to architectural variations demands caution. An alternative, simpler method for robustness assessment is presented.

Abstract: Machine and deep learning have grown in popularity and use in biological
research over the last decade but still present challenges in interpretability
of the fitted model. The development and use of metrics to determine features
driving predictions and increase model interpretability continues to be an open
area of research. We investigate the use of Shapley Additive Explanations
(SHAP) on a multi-view deep learning model applied to multi-omics data for the
purposes of identifying biomolecules of interest. Rankings of features via
these attribution methods are compared across various architectures to evaluate
consistency of the method. We perform multiple computational experiments to
assess the robustness of SHAP and investigate modeling approaches and
diagnostics to increase and measure the reliability of the identification of
important features. Accuracy of a random-forest model fit on subsets of
features selected as being most influential as well as clustering quality using
only these features are used as a measure of effectiveness of the attribution
method. Our findings indicate that the rankings of features resulting from SHAP
are sensitive to the choice of architecture as well as different random
initializations of weights, suggesting caution when using attribution methods
on multi-view deep learning models applied to multi-omics data. We present an
alternative, simple method to assess the robustness of identification of
important biomolecules.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [68] [CleANN: Efficient Full Dynamism in Graph-based Approximate Nearest Neighbor Search](https://arxiv.org/abs/2507.19802)
*Ziyu Zhang,Yuanhao Wei,Joshua Engels,Julian Shun*

Main category: cs.DB

TL;DR: The paper proposes CleANN, a novel dynamic graph-based approximate nearest neighbor search (ANNS) system. CleANN maintains high-quality query results efficiently despite concurrent insertions, deletions, and searches.


<details>
  <summary>Details</summary>
Motivation: Dynamic ANNS systems are needed for applications requiring up-to-date responses. Existing methods suffer either from degraded query quality or expensive global updates.

Method: CleANN integrates three components: workload-aware linking, query-adaptive neighborhood consolidation, and semi-lazy memory cleaning.

Result: In tests on 7 diverse datasets, CleANN maintained query quality comparable to static indexes while achieving dramatically improved throughput (7-1200x) under fully dynamic workloads.

Conclusion: CleANN represents a breakthrough in dynamic ANNS indexes, achieving high efficiency and query quality in concurrent and fully dynamic environments.

Abstract: Approximate nearest neighbor search (ANNS) has become a quintessential
algorithmic problem for various other foundational data tasks for AI workloads.
Graph-based ANNS indexes have superb empirical trade-offs in indexing cost,
query efficiency, and query approximation quality. Most existing graph-based
indexes are designed for the static scenario, where there are no updates to the
data after the index is constructed. However, full dynamism (insertions,
deletions, and searches) is crucial to providing up-to-date responses in
applications using vector databases. It is desirable that the index efficiently
supports updates and search queries concurrently. Existing dynamic graph-based
indexes suffer from at least one of the following problems: (1) the query
quality degrades as updates happen; and (2) the graph structure updates used to
maintain the index quality upon updates are global and thus expensive. To solve
these problems, we propose the CleANN system which consists of three main
components: (1) workload-aware linking of diverse search tree descendants to
combat distribution shift; (2)query-adaptive on-the-fly neighborhood
consolidation to efficiently handle deleted nodes; and (3) semi-lazy memory
cleaning to clean up stale information in the data structure and reduce the
work spent by the first two components. We evaluate CleANN on 7 diverse
datasets on fully dynamic workloads and find that CleANN has query quality at
least as good as if the index had been built statically using the corresponding
data. In the in-memory setting using 56 hyper-threads, with all types of
queries running concurrently, at the same recall level, CleANN achieves 7-1200x
throughput improvement on million-scale real-world datasets. To the best of our
knowledge, CleANN is the first concurrent ANNS index to achieve such efficiency
while maintaining quality under full dynamism.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [69] [CNN-based Surface Temperature Forecasts with Ensemble Numerical Weather Prediction over Medium-range Forecast Periods](https://arxiv.org/abs/2507.18937)
*Takuya Inoue,Takuya Kawabata*

Main category: physics.ao-ph

TL;DR: This work presents a method combining CNNs with ensemble NWP models to improve medium-range temperature forecasting, addressing both systematic and random errors efficiently.


<details>
  <summary>Details</summary>
Motivation: Operational medium-range temperature forecasts often rely on low-resolution NWP models, leading to systematic and random errors due to computational constraints.

Method: The method applies CNN-based post-processing to correct systematic errors (bias correction, spatial super-resolution) and uses ensemble averaging to reduce random errors in NWP outputs.

Result: CNN correction before ensemble averaging achieved higher accuracy than the reverse approach, and the method outperformed high-resolution deterministic NWP models while using low-resolution ensemble forecasts.

Conclusion: The method is an effective, scalable solution for improving medium-range temperature forecasts, especially useful for operational centers with limited computational resources.

Abstract: This study proposes a method that integrates convolutional neural networks
(CNNs) with ensemble numerical weather prediction (NWP) models, enabling
surface temperature forecasting at lead times beyond the short-range (five-day)
forecast period. Owing to limited computational resources, operational
medium-range temperature forecasts typically rely on low-resolution NWP models,
which are prone to systematic and random errors. To resolve these limitations,
the proposed method first reduces systematic errors through CNN-based
post-processing (bias correction and spatial super-resolution) on each ensemble
member, reconstructing high-resolution temperature fields from low-resolution
model outputs. Second, it reduces random errors through ensemble averaging of
the CNN-corrected members. This study also investigates whether the sequence of
CNN correction and ensemble averaging affects the forecast accuracy. For
comparison with the proposed method, we additionally conducted experiments with
the CNN trained on ensemble-averaged forecasts. The first approach--CNN
correction before ensemble averaging--consistently achieved higher accuracy
than the reverse approach. Although based on low-resolution ensemble forecasts,
the proposed method notably outperformed the high-resolution deterministic NWP
models. These findings indicate that combining CNN-based correction with
ensemble averaging effectively reduces both the systematic and random errors in
NWP model outputs. The proposed approach is a practical and scalable solution
for improving medium-range temperature forecasts, and is particularly valuable
at operational centers with limited computational resources.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [70] [Amorphous Solid Model of Vectorial Hopfield Neural Networks](https://arxiv.org/abs/2507.22787)
*F. Gallavotti,A. Zaccone*

Main category: cond-mat.dis-nn

TL;DR: This paper introduces a modified Hopfield associative memory model using unit vectors, inspired by amorphous solids. It demonstrates enhanced memory capacity and structural similarities with disordered materials.


<details>
  <summary>Details</summary>
Motivation: The authors sought to explore connections between neural associative memory models and amorphous solid physics, particularly focusing on systems with continuous orientational degrees of freedom.

Method: A generalized Hebbian learning rule is applied, creating a block-structured weight matrix with stored patterns as unit vectors on a sphere. Various system properties and transitions were analyzed using energy landscape features, anisotropy correlations, and pattern density.

Result: Enhancements in memory capacity ($\gamma_c \approx 0.55$) compared to binary networks ($\gamma_c \approx 0.138$), anisotropy ratios around $10^2$, energy gaps of $\sim 7$ units, and connections to elastic network physics were identified.

Conclusion: The findings establish a novel connection between associative memory models and amorphous solids, suggesting that mechanisms with continuous degrees of freedom enhance memory capacity and mimic structural physics behaviors.

Abstract: We present a vectorial extension of the Hopfield associative memory model
inspired by the theory of amorphous solids, where binary neural states are
replaced by unit vectors $\mathbf{s}_i \in \mathbb{R}^3$ on the sphere $S^2$.
The generalized Hebbian learning rule creates a block-structured weight matrix
through outer products of stored pattern vectors, analogous to the Hessian
matrix structure in amorphous solids. We demonstrate that this model exhibits
quantifiable structural properties characteristic of disordered materials:
energy landscapes with deep minima for stored patterns versus random
configurations (energy gaps $\sim 7$ units), strongly anisotropic correlations
encoded in the weight matrix (anisotropy ratios $\sim 10^2$), and
order-disorder transitions controlled by the pattern density $\gamma = P/(N
\cdot d)$. The enhanced memory capacity ($\gamma_c \approx 0.55$ for a
fully-connected network) compared to binary networks ($\gamma_c \approx 0.138$)
and the emergence of orientational correlations establish connections between
associative memory mechanisms and amorphous solid physics, particularly in
systems with continuous orientational degrees of freedom. We also unveil the
scaling with the coordination number $Z$ of the memory capacity: $\gamma_c \sim
(Z-6)$ from the isostatic point $Z_c =6$ of the 3D elastic network, which
closely mirrors the scaling of the shear modulus $G \sim (Z-6)$ in 3D
central-force spring networks.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [71] [OpenRASE: Service Function Chain Emulation](https://arxiv.org/abs/2507.22131)
*Theviyanthan Krishnamohan,Paul Harvey*

Main category: cs.NI

TL;DR: The paper introduces OpenRASE, an SFC emulator for evaluating resource allocation algorithms in dynamic network settings.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of existing tools used to evaluate resource allocation algorithms for Service Function Chains (SFCs), such as inaccuracy, inflexibility, and scalability.

Method: An emulator named OpenRASE was designed and implemented using Mininet and Docker. It allows real-time measurement of CPU usage and latency to test resource allocation algorithms for SFCs.

Result: OpenRASE was used to experimentally evaluate two resource allocation algorithms, including an online Genetic Algorithm, demonstrating its practical application and effectiveness in dynamic settings.

Conclusion: OpenRASE is a practical and effective emulator for studying resource allocation algorithms in SFCs, addressing issues faced by prior tools.

Abstract: Service Function Chains (SFCs) are one of the key enablers in providing
programmable computer networks, paving the way for network autonomy. However,
this also introduces new challenges, such as resource allocation and
optimisation related to their operation, requiring new algorithms to address
these challenges. Various tools have been used in the literature to evaluate
these algorithms. However, these tools suffer from inaccuracy, low fidelity,
unscalability, inflexibility, or additional code requirements. This paper
introduces an emulator based on Mininet and Docker for SFCs called OpenRASE.
The goal of OpenRASE is to enable the exploration of resource allocation
algorithms for SFCs in a dynamic setting, allowing real CPU usage and latency
to be measured. We describe the design and implementation of OpenRASE and
discuss its characteristics. We also experimentally evaluate two different
algorithms to address the SFC resource allocation challenge, including an
online Genetic Algorithm, using OpenRASE to show its effectiveness and
practicality for dynamic network conditions.

</details>
