<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 18]
- [cs.CL](#cs.CL) [Total: 15]
- [cs.CV](#cs.CV) [Total: 16]
- [cs.DC](#cs.DC) [Total: 12]
- [cs.LG](#cs.LG) [Total: 22]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.RO](#cs.RO) [Total: 16]
- [cs.SE](#cs.SE) [Total: 16]
- [q-bio.NC](#q-bio.NC) [Total: 6]
- [stat.ML](#stat.ML) [Total: 10]
- [quant-ph](#quant-ph) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Learning to Lead Themselves: Agentic AI in MAS using MARL](https://arxiv.org/abs/2510.00022)
*Ansh Kamthan*

Main category: cs.AI

TL;DR: This paper explores decentralized decision-making in multi-agent systems, focusing on drone delivery, using a proposed IPPO reinforcement learning approach.


<details>
  <summary>Details</summary>
Motivation: Autonomous systems need efficient task allocation and coordination for real-world deployment, especially for applications like drone delivery and warehouse automation.

Method: They model the problem within a cooperative multi-agent reinforcement learning setting and propose a specialized lightweight Proximal Policy Optimization (IPPO) under centralized-training and decentralized-execution. Simulations were conducted in the PettingZoo environment with homogeneous drones.

Result: Drones or agents effectively self-organized to cover distinct targets without explicit communication, demonstrating IPPO's capabilities.

Conclusion: Agentic artificial intelligence, using IPPO, can significantly enhance task coordination and allocation in multi-agent systems.

Abstract: As autonomous systems move from prototypes to real deployments, the ability
of multiple agents to make decentralized, cooperative decisions becomes a core
requirement. This paper examines how agentic artificial intelligence, agents
that act independently, adaptively and proactively can improve task allocation
and coordination in multi-agent systems, with primary emphasis on drone
delivery and secondary relevance to warehouse automation. We formulate the
problem in a cooperative multi-agent reinforcement learning setting and
implement a lightweight multi-agent Proximal Policy Optimization, called IPPO,
approach in PyTorch under a centralized-training, decentralized-execution
paradigm. Experiments are conducted in PettingZoo environment, where multiple
homogeneous drones or agents must self-organize to cover distinct targets
without explicit communication.

</details>


### [2] [ToolBrain: A Flexible Reinforcement Learning Framework for Agentic Tools](https://arxiv.org/abs/2510.00023)
*Quy Minh Le,Minh Sao Khue Luu,Khanh-Tung Tran,Duc-Hai Nguyen,Hoang-Quoc-Viet Pham,Quan Le,Hoang Thanh Lam,Hoang D. Nguyen*

Main category: cs.AI

TL;DR: ToolBrain is a lightweight RL-based framework designed to train AI agents for effective tool use without manual rewards or excessive resources. It supports flexible methods and showcases fast improvements in tailored tool-use tasks.


<details>
  <summary>Details</summary>
Motivation: The paper addresses challenges in training AI agents for versatile tool use, focusing on inefficiencies caused by manual reward design, scarce data, and poor multi-tool selection.

Method: The framework leverages reinforcement learning (GRPO/DPO) and supervised learning to coach agents. It offers knowledge distillation, automated task generation, fine-tuning pipelines, and quantized inference features.

Result: Using ToolBrain, agents demonstrated up to a 30% improvement in tool-use performance, validated through tasks like CodeAct email search.

Conclusion: ToolBrain simplifies training AI tool use across domains, making the process more efficient and accessible. The framework and codebase are open for public use.

Abstract: Effective tool use is essential for agentic AI, yet training agents to
utilize tools remains challenging due to manually designed rewards, limited
training data, and poor multi-tool selection, resulting in slow adaptation,
wasted computational resources, and suboptimal performance. We introduce
ToolBrain, a lightweight and user-friendly framework for coaching tool use in
agentic models with flexible reinforcement learning (RL), easing the barriers
for researchers and practitioners to adapt LLM-based agents to specific
domains. It supports a wide range of training strategies, including RL
algorithms such as GRPO and DPO, as well as supervised learning. ToolBrain
enables custom reward callables directly on an agent's execution traces or
simply utilizes an automated LLM-as-a-judge system for reward generation. It is
packed with useful capabilities, including knowledge distillation from large to
small models for efficient development, automatic task generation from tool
descriptions, seamless tool retrieval, efficient fine-tuning pipelines with
QLoRA through Unsloth, and quantized inference via bitsandbytes. We demonstrate
ToolBrain through diverse use cases, such as training a CodeAct agent to
autonomously execute email search tasks, showing fast, targeted improvements
(up to 30.0%) in tool-use skills while keeping the codebase simple and
extensible in Agentic AI. Our framework is publicly available at
https://toolbrain.org.

</details>


### [3] [ARS: Adaptive Reasoning Suppression for Efficient Large Reasoning Language Models](https://arxiv.org/abs/2510.00071)
*Dongqi Zheng*

Main category: cs.AI

TL;DR: This paper introduces Adaptive Reasoning Suppression (ARS), a method to reduce computational inefficiency in large reasoning language models, achieving up to 57.9% energy savings without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: Large reasoning models perform complex tasks but face computational inefficiencies due to overthinking, which this paper aims to address.

Method: The ARS method dynamically suppresses redundant reasoning steps using adaptive certainty monitoring and multi-checkpoint certainty estimation mechanisms.

Result: ARS achieves significant efficiency improvements in token usage (up to 53%), latency (46.1%), and energy reduction (57.9%), while maintaining or improving model accuracy.

Conclusion: ARS successfully tackles inefficiencies in reasoning models, offering a scalable approach to improving computational performance without compromising reasoning quality.

Abstract: Large Reasoning Language Models (LRLMs or LRMs) demonstrate remarkable
capabilities in complex reasoning tasks, but suffer from significant
computational inefficiencies due to overthinking phenomena. Existing efficient
reasoning methods face the challenge of balancing reasoning quality with
inference cost reduction. We propose \textbf{Adaptive Reasoning Suppression
(ARS)}, a novel training-free approach that dynamically suppresses redundant
reasoning steps while preserving accuracy through adaptive certainty
monitoring. ARS introduces a multi-checkpoint certainty estimation mechanism
with progressive suppression thresholds, achieving superior efficiency compared
to static suppression methods. Our extensive evaluation across mathematical
reasoning benchmarks using multiple model architectures demonstrates that ARS
achieves up to 53%, 46.1%, and 57.9% in token, latency and energy reduction,
while maintaining or improving accuracy.

</details>


### [4] [NeurIPS should lead scientific consensus on AI policy](https://arxiv.org/abs/2510.00075)
*Rishi Bommasani*

Main category: cs.AI

TL;DR: The paper argues for NeurIPS to lead in forming scientific consensus on AI policy, inspired by the IPCC's approach to climate policy.


<details>
  <summary>Details</summary>
Motivation: There is a void in mechanisms for consensus formation in AI policy, despite existing mechanisms for evidence generation and synthesis.

Method: Proposes NeurIPS take the lead by piloting initiatives inspired by the IPCC's methods to drive consensus formation.

Result: No direct empirical results are provided; the paper outlines a position and argues the case for NeurIPS involvement.

Conclusion: NeurIPS should leverage its leadership role to fill the gap in scientific consensus mechanisms for AI policy.

Abstract: Designing wise AI policy is a grand challenge for society. To design such
policy, policymakers should place a premium on rigorous evidence and scientific
consensus. While several mechanisms exist for evidence generation, and nascent
mechanisms tackle evidence synthesis, we identify a complete void on consensus
formation. In this position paper, we argue NeurIPS should actively catalyze
scientific consensus on AI policy. Beyond identifying the current deficit in
consensus formation mechanisms, we argue that NeurIPS is the best option due
its strengths and the paucity of compelling alternatives. To make progress, we
recommend initial pilots for NeurIPS by distilling lessons from the IPCC's
leadership to build scientific consensus on climate policy. We dispel
predictable counters that AI researchers disagree too much to achieve consensus
and that policy engagement is not the business of NeurIPS. NeurIPS leads AI on
many fronts, and it should champion scientific consensus to create higher
quality AI policy.

</details>


### [5] [Towards a Framework for Supporting the Ethical and Regulatory Certification of AI Systems](https://arxiv.org/abs/2510.00084)
*Fabian Kovac,Sebastian Neumaier,Timea Pahi,Torsten Priebe,Rafael Rodrigues,Dimitrios Christodoulou,Maxime Cordy,Sylvain Kubler,Ali Kordia,Georgios Pitsiladis,John Soldatos,Petros Zervoudakis*

Main category: cs.AI

TL;DR: The CERTAIN project proposes a framework integrating compliance, ethics, and transparency into AI systems, focusing on semantic MLOps, data lineage tracking, and RegOps workflows.


<details>
  <summary>Details</summary>
Motivation: To address ethical, legal, and regulatory challenges posed by AI proliferation in Europe's societal and economic contexts.

Method: The paper outlines semantic MLOps for AI lifecycle management, ontology-driven data lineage tracking, and RegOps workflows for compliance.

Result: CERTAIN solutions are implemented and validated across various pilots to enhance regulatory compliance and ethical AI innovation.

Conclusion: The project advances EU-aligned responsible AI development by integrating compliance, ethics, and operational transparency.

Abstract: Artificial Intelligence has rapidly become a cornerstone technology,
significantly influencing Europe's societal and economic landscapes. However,
the proliferation of AI also raises critical ethical, legal, and regulatory
challenges. The CERTAIN (Certification for Ethical and Regulatory Transparency
in Artificial Intelligence) project addresses these issues by developing a
comprehensive framework that integrates regulatory compliance, ethical
standards, and transparency into AI systems. In this position paper, we outline
the methodological steps for building the core components of this framework.
Specifically, we present: (i) semantic Machine Learning Operations (MLOps) for
structured AI lifecycle management, (ii) ontology-driven data lineage tracking
to ensure traceability and accountability, and (iii) regulatory operations
(RegOps) workflows to operationalize compliance requirements. By implementing
and validating its solutions across diverse pilots, CERTAIN aims to advance
regulatory compliance and to promote responsible AI innovation aligned with
European standards.

</details>


### [6] [A Neuro-Fuzzy System for Interpretable Long-Term Stock Market Forecasting](https://arxiv.org/abs/2510.00960)
*Miha Ožbot,Igor Škrjanc,Vitomir Štruc*

Main category: cs.AI

TL;DR: The paper presents Fuzzformer, a novel architecture combining fuzzy inference systems and self-attention mechanisms for interpretable multivariate time series forecasting in stock market data.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of achieving both accuracy and interpretability in multivariate time series forecasting, particularly for stock market data.

Method: The approach integrates LSTM networks with temporal attention to condense multivariate data into interpretable features, leveraging fuzzy inference systems for enhanced interpretability.

Result: The Fuzzformer shows comparable forecasting performance to standard models like ARIMA and LSTM, while maintaining interpretability in feature analysis, tested on S&P500 stock market data.

Conclusion: The initial results demonstrate the feasibility of interpretable forecasting in stock market analysis, suggesting performance tradeoffs but highlighting practical applicability.

Abstract: In the complex landscape of multivariate time series forecasting, achieving
both accuracy and interpretability remains a significant challenge. This paper
introduces the Fuzzy Transformer (Fuzzformer), a novel recurrent neural network
architecture combined with multi-head self-attention and fuzzy inference
systems to analyze multivariate stock market data and conduct long-term time
series forecasting. The method leverages LSTM networks and temporal attention
to condense multivariate data into interpretable features suitable for fuzzy
inference systems. The resulting architecture offers comparable forecasting
performance to conventional models such as ARIMA and LSTM while providing
meaningful information flow within the network. The method was examined on the
real world stock market index S\&P500. Initial results show potential for
interpretable forecasting and identify current performance tradeoffs,
suggesting practical application in understanding and forecasting stock market
behavior.

</details>


### [7] [Judging by Appearances? Auditing and Intervening Vision-Language Models for Bail Prediction](https://arxiv.org/abs/2510.00088)
*Sagnik Basu,Shubham Prakash,Ashish Maruti Barge,Siddharth D Jaiswal,Abhisek Dash,Saptarshi Ghosh,Animesh Mukherjee*

Main category: cs.AI

TL;DR: The paper analyzes the use of vision-language models (VLMs) for bail decision prediction and finds poor performance and potential biases. The authors design interventions to improve their functionality.


<details>
  <summary>Details</summary>
Motivation: To explore the efficiency and potential ethical and functional pitfalls of vision-language models in judicial applications, particularly bail decision predictions.

Method: The authors audit standalone VLMs for bail prediction, assess their biases and performance flaws, and then introduce intervention algorithms, including a retrieval-augmented generation (RAG) pipeline and fine-tuning strategies, to enhance their accuracy.

Result: The proposed interventions notably boost the VLMs' performance, addressing critical deficiencies in the bail decision task.

Conclusion: While VLMs currently show poor performance and biases in legal prediction tasks, the study's interventions demonstrate their potential when designed thoughtfully, urging the need for smarter and ethical AI systems before real-world deployment.

Abstract: Large language models (LLMs) have been extensively used for legal judgment
prediction tasks based on case reports and crime history. However, with a surge
in the availability of large vision language models (VLMs), legal judgment
prediction systems can now be made to leverage the images of the criminals in
addition to the textual case reports/crime history. Applications built in this
way could lead to inadvertent consequences and be used with malicious intent.
In this work, we run an audit to investigate the efficiency of standalone VLMs
in the bail decision prediction task. We observe that the performance is poor
across multiple intersectional groups and models \textit{wrongly deny bail to
deserving individuals with very high confidence}. We design different
intervention algorithms by first including legal precedents through a RAG
pipeline and then fine-tuning the VLMs using innovative schemes. We demonstrate
that these interventions substantially improve the performance of bail
prediction. Our work paves the way for the design of smarter interventions on
VLMs in the future, before they can be deployed for real-world legal judgment
prediction.

</details>


### [8] [AuditAgent: Expert-Guided Multi-Agent Reasoning for Cross-Document Fraudulent Evidence Discovery](https://arxiv.org/abs/2510.00156)
*Songran Bai,Bingzhe Wu,Yiwei Zhang,Chengke Wu,Xiaolong Zheng,Yaze Yuan,Ke Wu,Jianqiang Li*

Main category: cs.AI

TL;DR: The paper introduces AuditAgent, a multi-agent framework for detecting financial fraud by localizing evidence in multi-year disclosures, which proves effective and transparent in practice.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of detecting subtle and dispersed evidence of financial fraud across complex disclosures.

Method: The framework leverages an expert-annotated dataset, subject-level risk priors, a hybrid retrieval strategy, and specialized agent models.

Result: AuditAgent shows superior performance in recall and interpretability compared to General-Purpose Agents, setting a new benchmark in automated financial forensics.

Conclusion: Domain-specific reasoning and well-constructed datasets play a critical role in advancing transparent and effective financial fraud detection tools.

Abstract: Financial fraud detection in real-world scenarios presents significant
challenges due to the subtlety and dispersion of evidence across complex,
multi-year financial disclosures. In this work, we introduce a novel
multi-agent reasoning framework AuditAgent, enhanced with auditing domain
expertise, for fine-grained evidence chain localization in financial fraud
cases. Leveraging an expert-annotated dataset constructed from enforcement
documents and financial reports released by the China Securities Regulatory
Commission, our approach integrates subject-level risk priors, a hybrid
retrieval strategy, and specialized agent modules to efficiently identify and
aggregate cross-report evidence. Extensive experiments demonstrate that our
method substantially outperforms General-Purpose Agent paradigm in both recall
and interpretability, establishing a new benchmark for automated, transparent
financial forensics. Our results highlight the value of domain-specific
reasoning and dataset construction for advancing robust financial fraud
detection in practical, real-world regulatory applications.

</details>


### [9] [Drones that Think on their Feet: Sudden Landing Decisions with Embodied AI](https://arxiv.org/abs/2510.00167)
*Diego Ortiz Barbosa,Mohit Agrawal,Yash Malegaonkar,Luis Burbano,Axel Andersson,György Dán,Henrik Sandberg,Alvaro A. Cardenas*

Main category: cs.AI

TL;DR: Autonomous drones often face unpredictable events requiring quick decisions. Traditional methods fall short, but recent embodied AI using visual language models offers adaptive and commonsense reasoning for safe reaction.


<details>
  <summary>Details</summary>
Motivation: Traditional hand-coded recovery rules for drones are incomplete as they cannot address all real-world contingencies.

Method: Utilizing embodied AI with large visual language models to interpret surroundings and adapt drone maneuvers in a simulated urban setting.

Result: In simulation, embodied AI allows drones to dynamically make safe decisions like landing during emergencies, demonstrating resilience and flexibility.

Conclusion: Embodied AI introduces an adaptive approach for drone decision-making and safety, surpassing traditional rule-based methods.

Abstract: Autonomous drones must often respond to sudden events, such as alarms,
faults, or unexpected changes in their environment, that require immediate and
adaptive decision-making. Traditional approaches rely on safety engineers
hand-coding large sets of recovery rules, but this strategy cannot anticipate
the vast range of real-world contingencies and quickly becomes incomplete.
Recent advances in embodied AI, powered by large visual language models,
provide commonsense reasoning to assess context and generate appropriate
actions in real time. We demonstrate this capability in a simulated urban
benchmark in the Unreal Engine, where drones dynamically interpret their
surroundings and decide on sudden maneuvers for safe landings. Our results show
that embodied AI makes possible a new class of adaptive recovery and
decision-making pipelines that were previously infeasible to design by hand,
advancing resilience and safety in autonomous aerial systems.

</details>


### [10] [Object-Centric Case-Based Reasoning via Argumentation](https://arxiv.org/abs/2510.00185)
*Gabriel de Olim Gaul,Adam Gould,Avinash Kori,Francesca Toni*

Main category: cs.AI

TL;DR: SAA-CBR is a hybrid image classification system merging Slot Attention-based object-centric learning with Abstract Argumentation for reasoning, tested on CLEVR-Hans datasets.


<details>
  <summary>Details</summary>
Motivation: The paper aims to advance image classification techniques by combining symbolic reasoning and neural network-based object-centric learning.

Method: The proposed approach integrates Slot Attention for feature extraction and Abstract Argumentation for reasoning, alongside enhancements like casebase reduction, partial orders, and multi-class classification strategies.

Result: SAA-CBR achieved competitive performance in classification tasks on CLEVR-Hans datasets compared to baseline models.

Conclusion: The integration of neuro-symbolic approaches like SAA-CBR can effectively address complex image classification tasks while leveraging the strengths of symbolic reasoning and neural networks.

Abstract: We introduce Slot Attention Argumentation for Case-Based Reasoning (SAA-CBR),
a novel neuro-symbolic pipeline for image classification that integrates
object-centric learning via a neural Slot Attention (SA) component with
symbolic reasoning conducted by Abstract Argumentation for Case-Based Reasoning
(AA-CBR). We explore novel integrations of AA-CBR with the neural component,
including feature combination strategies, casebase reduction via representative
samples, novel count-based partial orders, a One-Vs-Rest strategy for extending
AA-CBR to multi-class classification, and an application of Supported AA-CBR, a
bipolar variant of AA-CBR. We demonstrate that SAA-CBR is an effective
classifier on the CLEVR-Hans datasets, showing competitive performance against
baseline models.

</details>


### [11] [Thinkquel: A Model Dedicated to Text-to-dbt Using Synthetic Data and a Span-Aware Objective](https://arxiv.org/abs/2510.00186)
*Anni Li,Aria Attar,Paul Dong*

Main category: cs.AI

TL;DR: The paper introduces Thinkquel, a fine-tuned model for generating reliable database queries, focusing on schema linking and SQL dialects. It integrates a synthetic data pipeline (TS-SQL) and a span-aware reinforcement learning method (TS-GRPO). Results on a test set show a significant improvement in execution success and exact-result matches.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of transforming natural language requests into accurate database queries, which involve schema linking, SQL dialects, and limited supervision for training models.

Method: Thinkquel employs a synthetic data pipeline (TS-SQL), dbt as an intermediate representation, and a span-aware reinforcement learning objective (TS-GRPO) to bridge the gap between token-level and sequence-level rewards, ensuring improved training stability and optimization.

Result: Thinkquel (32B) achieved 93.2% execution success and 61.8% exact-result match on the TS-SQL test set, outperforming previous models by significant margins. TS-GRPO also boosts training stability in Spider (14B) experiments.

Conclusion: Thinkquel's methodologies demonstrate robust improvements in generating reliable database queries, making it a valuable approach for enhancing training and execution quality in the field of natural-language-to-SQL transformation.

Abstract: Transforming natural-language requests into reliable, production-ready data
transformations remains challenging: correctness depends on precise schema
linking and warehouse-specific SQL dialects, while the strongest supervision
available during training--execution success and result matching--are provided
only at the sequence level. At the same time, assembling large,
execution-validated corpora is costly, and token-level objectives misalign with
these global signals, yielding unstable optimization and limited portability.
We introduce Thinkquel, a fine-tuned model for producing robust, portable, and
execution-validated database queries. Methodologies in Thinkquel integrates a
novel synthetic data pipeline, TS-SQL, that leverages dbt as a portable
intermediate representation with a span-aware reinforcement learning objective,
and Token-Sequence GRPO (TS-GRPO), specifically designed to bridge the gap
between token-level training signals and sequence-level execution rewards when
finetuning LLMs. On the 500-example TS-SQL test set, Thinkquel (32B) reaches
93.2\% execution success and 61.8\% exact-result match with a two-stage SFT
curriculum, improving over the base model by 67.2\% (exec.) and 44.4\% (match).
In Spider (14B) experiments, TS-GRPO increases training stability and speeds
convergence of the execution-match reward relative to GRPO and GSPO.

</details>


### [12] [DualTune: Decoupled Fine-Tuning for On-Device Agentic Systems](https://arxiv.org/abs/2510.00229)
*Rohan Kadekodi,Zhan Jin,Keisuke Kamahori,Yile Gu,Sean Khatiri,Noah H. Bayindirli,Sergey Gorbunov,Baris Kasikci*

Main category: cs.AI

TL;DR: The paper introduces a new method for improving tool-calling accuracy in local Large Language Models (LLMs) through a framework called "DualTune," which enhances both tool selection and argument generation.


<details>
  <summary>Details</summary>
Motivation: Local LLMs are vital for privacy-preserving and cost-effective task automation but often fail in tool-calling scenarios. They struggle with selecting the correct tools and generating arguments for complex tasks, highlighting the need for better solutions.

Method: The authors propose 'decoupled fine-tuning,' which uses LoRA fine-tuning to separately train adapters for tool selection and argument generation. These adapters are used in an inference framework named DualTune, which optimizes tool call generation by dynamically loading appropriate adapters and limiting the number of tools considered.

Result: The methodology, when applied to the Qwen-2.5-7B model, improves tool-calling accuracy by 46% and outperforms similarly sized models as well as some models twice its size.

Conclusion: Decoupled fine-tuning and the DualTune framework significantly enhance the local model's performance in tool-calling tasks, enabling efficient, privacy-preserving task automation on end-user devices.

Abstract: The deployment of Large Language Models (LLMs) as agentic orchestrators has
revolutionized task automation, but the need for privacy-preserving,
cost-effective solutions demands on-device inference capabilities. However,
local LLMs consistently underperform compared to frontier models in tool
calling scenarios, struggling with both tool selection from large tool sets and
accurate argument generation for complex parameter structures. We introduce a
methodology that disaggregates a tool-calling task into two distinct subtasks:
tool selection and argument generation. We propose "decoupled fine-tuning", a
novel post-training approach that employs LoRA fine-tuning to create dedicated
LoRA adapters for tool selection and tool-specific argument generation using
separate loss masking for each of the subtasks. Furthermore, we present
DualTune, an inference framework that leverages the LoRA adapters created using
decoupled fine-tuning to perform efficient agent orchestration with the help of
local models on end-user devices. DualTune decomposes the tool-call generation
step into tool selection and argument generation, and dynamically loads the
corresponding LoRA adapters to generate tool calls. Additionally, DualTune
implements hierarchical orchestration to restrict the number of tools required
for tool selection. Our experiments on the MCP-Bench benchmark demonstrate that
the Qwen-2.5-7B model trained using decoupled fine-tuning improves the tool
calling accuracy of the base model by 46%, and outperforms other local
reasoning, non-reasoning and fine-tuned models of similar size in all cases,
and models that are 2x larger, in most cases.

</details>


### [13] [MAGIC-MASK: Multi-Agent Guided Inter-Agent Collaboration with Mask-Based Explainability for Reinforcement Learning](https://arxiv.org/abs/2510.00274)
*Maisha Maliha,Dean Hougen*

Main category: cs.AI

TL;DR: The paper introduces MAGIC-MASK, a framework to enhance explainability in Multi-Agent Reinforcement Learning while addressing computational efficiency and adaptability.


<details>
  <summary>Details</summary>
Motivation: Current explainability methods struggle with computational inefficiency, limited exploration coverage, and lack of suitability for multi-agent environments.

Method: MAGIC-MASK integrates techniques like Proximal Policy Optimization, adaptive epsilon-greedy exploration, and inter-agent collaboration based on masked state sharing.

Result: Experiments demonstrate MAGIC-MASK outperforms existing baselines in learning efficiency, policy robustness, and explanation fidelity in both single-agent and multi-agent settings.

Conclusion: The proposed framework generalizes explainability for multi-agent systems, providing localized, probabilistic explanations while enhancing learning and robustness.

Abstract: Understanding the decision-making process of Deep Reinforcement Learning
agents remains a key challenge for deploying these systems in safety-critical
and multi-agent environments. While prior explainability methods like
StateMask, have advanced the identification of critical states, they remain
limited by computational cost, exploration coverage, and lack of adaptation to
multi-agent settings. To overcome these limitations, we propose a
mathematically grounded framework, MAGIC-MASK (Multi-Agent Guided Inter-agent
Collaboration with Mask-Based Explainability for Reinforcement Learning), that
extends perturbation-based explanation to Multi-Agent Reinforcement Learning.
Our method integrates Proximal Policy Optimization, adaptive epsilon-greedy
exploration, and lightweight inter-agent collaboration to share masked state
information and peer experience. This collaboration enables each agent to
perform saliency-guided masking and share reward-based insights with peers,
reducing the time required for critical state discovery, improving explanation
fidelity, and leading to faster and more robust learning. The core novelty of
our approach lies in generalizing explainability from single-agent to
multi-agent systems through a unified mathematical formalism built on
trajectory perturbation, reward fidelity analysis, and Kullback-Leibler
divergence regularization. This framework yields localized, interpretable
explanations grounded in probabilistic modeling and multi-agent Markov decision
processes. We validate our framework on both single-agent and multi-agent
benchmarks, including a multi-agent highway driving environment and Google
Research Football, demonstrating that MAGIC-MASK consistently outperforms
state-of-the-art baselines in fidelity, learning efficiency, and policy
robustness while offering interpretable and transferable explanations.

</details>


### [14] [Adaptive Federated Few-Shot Rare-Disease Diagnosis with Energy-Aware Secure Aggregation](https://arxiv.org/abs/2510.00976)
*Aueaphum Aueawatthanaphisut*

Main category: cs.AI

TL;DR: This paper introduces the AFFR framework for rare-disease diagnosis, combining meta-learning, energy-aware scheduling, and secure differential privacy in federated learning to enhance accuracy, balance participation, and protect privacy.


<details>
  <summary>Details</summary>
Motivation: Rare-disease diagnosis faces challenges such as limited data, privacy concerns, and constrained resources on edge devices, necessitating innovative solutions.

Method: The proposed AFFR framework employs few-shot federated optimization with meta-learning, energy-aware client scheduling, and secure aggregation with calibrated differential privacy in a unified, modular pipeline.

Result: AFFR achieves up to 10% higher accuracy than baseline federated learning, reduces client dropouts by over 50%, and maintains acceptable privacy-utility trade-offs.

Conclusion: AFFR is a practical and efficient framework for trustworthy federated rare-disease diagnosis, addressing key challenges in scalability, privacy, and robustness.

Abstract: Rare-disease diagnosis remains one of the most pressing challenges in digital
health, hindered by extreme data scarcity, privacy concerns, and the limited
resources of edge devices. This paper proposes the Adaptive Federated Few-Shot
Rare-Disease Diagnosis (AFFR) framework, which integrates three pillars: (i)
few-shot federated optimization with meta-learning to generalize from limited
patient samples, (ii) energy-aware client scheduling to mitigate device
dropouts and ensure balanced participation, and (iii) secure aggregation with
calibrated differential privacy to safeguard sensitive model updates. Unlike
prior work that addresses these aspects in isolation, AFFR unifies them into a
modular pipeline deployable on real-world clinical networks. Experimental
evaluation on simulated rare-disease detection datasets demonstrates up to 10%
improvement in accuracy compared with baseline FL, while reducing client
dropouts by over 50% without degrading convergence. Furthermore,
privacy-utility trade-offs remain within clinically acceptable bounds. These
findings highlight AFFR as a practical pathway for equitable and trustworthy
federated diagnosis of rare conditions.

</details>


### [15] [ICL Optimized Fragility](https://arxiv.org/abs/2510.00300)
*Serena Gomez Wannaz*

Main category: cs.AI

TL;DR: The study examines how In-Context Learning (ICL) guides alter reasoning abilities across domains in GPT-OSS:20b models, revealing higher accuracy in general knowledge but reduced reasoning flexibility in complex tasks.


<details>
  <summary>Details</summary>
Motivation: Assess the impact of ICL guides on cross-domain reasoning abilities in AI models.

Method: Six GPT-OSS:20b variants were tested on 840 cross-domain tasks, analyzed using statistical methods like ANOVA.

Result: ICL guides improved accuracy in general knowledge (91%-99%) but degraded reasoning abilities in riddles (10-43%) and maintained baseline-level performance in math olympiad problems.

Conclusion: ICL trade-offs exist, improving efficiency but compromising reasoning flexibility, with implications for AI deployment and safety.

Abstract: ICL guides are known to improve task-specific performance, but their impact
on cross-domain cognitive abilities remains unexplored. This study examines how
ICL guides affect reasoning across different knowledge domains using six
variants of the GPT-OSS:20b model: one baseline model and five ICL
configurations (simple, chain-of-thought, random, appended text, and symbolic
language). The models were subjected to 840 tests spanning general knowledge
questions, logic riddles, and a mathematical olympiad problem. Statistical
analysis (ANOVA) revealed significant behavioral modifications (p less than
0.001) across ICL variants, demonstrating a phenomenon termed "optimized
fragility." ICL models achieved 91%-99% accuracy on general knowledge tasks
while showing degraded performance on complex reasoning problems, with accuracy
dropping to 10-43% on riddles compared to 43% for the baseline model. Notably,
no significant differences emerged on the olympiad problem (p=0.2173),
suggesting that complex mathematical reasoning remains unaffected by ICL
optimization. These findings indicate that ICL guides create systematic
trade-offs between efficiency and reasoning flexibility, with important
implications for LLM deployment and AI safety.

</details>


### [16] [BiasBusters: Uncovering and Mitigating Tool Selection Bias in Large Language Models](https://arxiv.org/abs/2510.00307)
*Thierry Blankenstein,Jialin Yu,Zixuan Li,Vassilis Plachouras,Sunando Sengupta,Philip Torr,Yarin Gal,Alasdair Paren,Adel Bibi*

Main category: cs.AI

TL;DR: The paper investigates selection bias in large language models (LLMs) relying on external tools and proposes a mitigation strategy.


<details>
  <summary>Details</summary>
Motivation: To address fairness issues in LLMs that select external tools, where biased selection could degrade user experience and distort competition.

Method: The authors utilized a benchmark of diverse tool categories and conducted experiments to evaluate selection bias across seven models, analyzing tool features, metadata, and pre-training exposure.

Result: Key results show that semantic query-metadata alignment is the strongest factor influencing selection, perturbing metadata impacts bias, and pre-training exposure to specific tools amplifies bias. A lightweight filtering and sampling mitigation method reduces this bias.

Conclusion: Tool-selection bias poses significant fairness challenges for LLMs, but targeted mitigation strategies can improve fairness and maintain task performance.

Abstract: Agents backed by large language models (LLMs) often rely on external tools
drawn from marketplaces where multiple providers offer functionally equivalent
options. This raises a critical point concerning fairness: if selection is
systematically biased, it can degrade user experience and distort competition
by privileging some providers over others. We introduce a benchmark of diverse
tool categories, each containing multiple functionally equivalent tools, to
evaluate tool-selection bias. Using this benchmark, we test seven models and
show that unfairness exists with models either fixating on a single provider or
disproportionately preferring earlier-listed tools in context. To investigate
the origins of this bias, we conduct controlled experiments examining tool
features, metadata (name, description, parameters), and pre-training exposure.
We find that: (1) semantic alignment between queries and metadata is the
strongest predictor of choice; (2) perturbing descriptions significantly shifts
selections; and (3) repeated pre-training exposure to a single endpoint
amplifies bias. Finally, we propose a lightweight mitigation that first filters
the candidate tools to a relevant subset and then samples uniformly, reducing
bias while preserving good task coverage. Our findings highlight tool-selection
bias as a key obstacle for the fair deployment of tool-augmented LLMs.

</details>


### [17] [When Hallucination Costs Millions: Benchmarking AI Agents in High-Stakes Adversarial Financial Markets](https://arxiv.org/abs/2510.00332)
*Zeshi Dai,Zimo Peng,Zerui Cheng,Ryan Yihe Li*

Main category: cs.AI

TL;DR: This paper introduces CAIA, a benchmark that evaluates AI models' performance in adversarial, high-stakes environments, highlighting significant capability gaps.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the inability of AI models to perform reliably in adversarial and high-stakes scenarios, such as misinformation-heavy environments.

Method: The authors use 178 time-anchored crypto market tasks to assess 17 models in distinguishing truth from manipulation under adversarial conditions.

Result: Their findings show that without tools, models perform at only 28% accuracy; tools improve this to 67.4%, but systematic biases in tool selection limit performance.

Conclusion: The paper concludes that current AI models are not adequately prepared for adversarial environments and calls for benchmarks like CAIA to assess robustness in such contexts.

Abstract: We present CAIA, a benchmark exposing a critical blind spot in AI evaluation:
the inability of state-of-the-art models to operate in adversarial, high-stakes
environments where misinformation is weaponized and errors are irreversible.
While existing benchmarks measure task completion in controlled settings,
real-world deployment demands resilience against active deception. Using crypto
markets as a testbed where $30 billion was lost to exploits in 2024, we
evaluate 17 models on 178 time-anchored tasks requiring agents to distinguish
truth from manipulation, navigate fragmented information landscapes, and make
irreversible financial decisions under adversarial pressure.
  Our results reveal a fundamental capability gap: without tools, even frontier
models achieve only 28% accuracy on tasks junior analysts routinely handle.
Tool augmentation improves performance but plateaus at 67.4% versus 80% human
baseline, despite unlimited access to professional resources. Most critically,
we uncover a systematic tool selection catastrophe: models preferentially
choose unreliable web search over authoritative data, falling for SEO-optimized
misinformation and social media manipulation. This behavior persists even when
correct answers are directly accessible through specialized tools, suggesting
foundational limitations rather than knowledge gaps. We also find that Pass@k
metrics mask dangerous trial-and-error behavior for autonomous deployment.
  The implications extend beyond crypto to any domain with active adversaries,
e.g. cybersecurity, content moderation, etc. We release CAIA with contamination
controls and continuous updates, establishing adversarial robustness as a
necessary condition for trustworthy AI autonomy. The benchmark reveals that
current models, despite impressive reasoning scores, remain fundamentally
unprepared for environments where intelligence must survive active opposition.

</details>


### [18] [Hierarchical Reasoning Model: A Critical Supplementary Material](https://arxiv.org/abs/2510.00355)
*Renee Ge,Qianli Liao,Tomaso Poggio*

Main category: cs.AI

TL;DR: Transformers excel in sequential tasks but struggle with logical reasoning due to unexplored latent space and recurrent reasoning. The paper reviews and improves on Hierarchical Reasoning Models, showcasing enhanced success in 2D reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Investigate the limitations of transformers in reasoning and explore latent space and recurrent reasoning to improve their capabilities.

Method: Analyzing and modifying Hierarchical Reasoning Models for better logical reasoning in challenging tasks like Sudoku-Extreme and Maze-Hard.

Result: Enhanced performance on complex reasoning tasks, surpassing previous benchmarks in these models and uncovering insights for future research.

Conclusion: Critical exploration and refinement of hierarchical reasoning models can lead to significant advancements in logical reasoning capabilities for transformers.

Abstract: Transformers have demonstrated remarkable performance in natural language
processing and related domains, as they largely focus on sequential,
autoregressive next-token prediction tasks. Yet, they struggle in logical
reasoning, not necessarily because of a fundamental limitation of these models,
but possibly due to the lack of exploration of more creative uses, such as
latent space and recurrent reasoning. An emerging exploration in this direction
is the Hierarchical Reasoning Model (Wang et al., 2025), which introduces a
novel type of recurrent reasoning in the latent space of transformers,
achieving remarkable performance on a wide range of 2D reasoning tasks. Despite
the promising results, this line of models is still at an early stage and calls
for in-depth investigation. In this work, we perform a critical review on this
class of models, examine key design choices and present intriguing variants
that achieve significantly better performance on the Sudoku-Extreme and
Maze-Hard tasks than previously reported. Our results also raise surprising
observations and intriguing directions for further research.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [19] [Direct Token Optimization: A Self-contained Approach to Large Language Model Unlearning](https://arxiv.org/abs/2510.00125)
*Hong kyu Lee,Ruixuan Liu,Li Xiong*

Main category: cs.CL

TL;DR: The paper introduces Direct Token Optimization (DTO), a self-contained method for machine unlearning in large language models, eliminating the need for external resources while maintaining model utility.


<details>
  <summary>Details</summary>
Motivation: To develop a method that ensures a model forgets specific training data (ensuring privacy and other concerns) while maintaining overall utility, without relying on external resources that may introduce additional privacy risks.

Method: Direct Token Optimization (DTO) identifies two categories of tokens when unlearning a sequence: target tokens, which optimize the unlearning objective, and non-target tokens, which help preserve model performance.

Result: DTO achieves up to 16.8× better forget quality compared to existing baselines, while maintaining a similar level of model utility, as evidenced by testing on multiple benchmark datasets.

Conclusion: DTO provides an effective, privacy-preserving, and resource-independent solution for unlearning in large language models, outperforming existing methods while maintaining model utility.

Abstract: Machine unlearning is an emerging technique that removes the influence of a
subset of training data (forget set) from a model without full retraining, with
applications including privacy protection, content moderation, and model
correction. The key challenge lies in ensuring that the model completely
forgets the knowledge of the forget set without compromising its overall
utility. Existing unlearning methods for large language models (LLMs) often
utilize auxiliary language models, retain datasets, or even commercial AI
services for effective unlearning and maintaining the model utility. However,
dependence on these external resources is often impractical and could
potentially introduce additional privacy risks. In this work, we propose direct
token optimization (DTO), a novel self-contained unlearning approach for LLMs
that directly optimizes the token level objectives and eliminates the need for
external resources. Given a sequence to unlearn, we identify two categories of
tokens: target tokens, which capture critical knowledge for unlearning, and the
remaining non-target tokens, which are crucial for maintaining the model
utility. The former are used to optimize the unlearning objective, while the
latter serve to preserve the model's performance. The experimental results show
that the proposed DTO achieves up to 16.8$\times$ improvement in forget quality
on several benchmark datasets than the latest baselines while maintaining a
comparable level of model utility.

</details>


### [20] [TAMA: Tool-Augmented Multimodal Agent for Procedural Activity Understanding](https://arxiv.org/abs/2510.00161)
*Kimihiro Hasegawa,Wiradee Imrattanatrai,Masaki Asada,Ken Fukuda,Teruko Mitamura*

Main category: cs.CL

TL;DR: The paper introduces TAMA, a framework for better understanding procedural activities using a tool-augmented multimodal approach without requiring additional training.


<details>
  <summary>Details</summary>
Motivation: Procedural activity assistants could aid humans in various environments, yet their development remains underexplored, motivating the design of a new framework.

Method: The proposed framework, TAMA, leverages multimedia tools to allow interleaved multimodal reasoning in a training-free manner and evaluates its efficacy using the ProMQA-Assembly dataset.

Result: TAMA enhances the performance of vision-language models like GPT-5 and MiMo-VL on procedural QA tasks. Ablation studies validate its multimedia tools and flexible tool selection features.

Conclusion: The framework supports thinking with images for multimodal tasks and advances procedural activity assistants, offering a foundation for future developments in this area.

Abstract: Procedural activity assistants potentially support humans in a variety of
settings, from our daily lives, e.g., cooking or assembling flat-pack
furniture, to professional situations, e.g., manufacturing or biological
experiments. Despite its potential use cases, the system development tailored
for such an assistant is still underexplored. In this paper, we propose a novel
framework, called TAMA, a Tool-Augmented Multimodal Agent, for procedural
activity understanding. TAMA enables interleaved multimodal reasoning by making
use of multimedia-returning tools in a training-free setting. Our experimental
result on the multimodal procedural QA dataset, ProMQA-Assembly, shows that our
approach can improve the performance of vision-language models, especially
GPT-5 and MiMo-VL. Furthermore, our ablation studies provide empirical support
for the effectiveness of two features that characterize our framework,
multimedia-returning tools and agentic flexible tool selection. We believe our
proposed framework and experimental results facilitate the thinking with images
paradigm for video and multimodal tasks, let alone the development of
procedural activity assistants.

</details>


### [21] [DRBench: A Realistic Benchmark for Enterprise Deep Research](https://arxiv.org/abs/2510.00172)
*Amirhossein Abaskohi,Tianyi Chen,Miguel Muñoz-Mármol,Curtis Fox,Amrutha Varshini Ramesh,Étienne Marcotte,Xing Han Lù,Nicolas Chapados,Spandana Gella,Christopher Pal,Alexandre Drouin,Issam H. Laradji*

Main category: cs.CL

TL;DR: DRBench is a benchmark created to assess AI agents in handling complex, multi-step deep research tasks specific to enterprise environments, requiring a mix of public and private data sources.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the gap in benchmarks designed to evaluate AI agents on advanced enterprise-focused research tasks, moving beyond simple or web-only queries.

Method: DRBench introduces tasks grounded in realistic personas and enterprise settings, spanning heterogeneous data sources. Task generation involves synthesis pipelines with human verification, and agent evaluation considers recall accuracy, factual correctness, and report coherence.

Result: DRBench offers 15 tasks across domains such as Sales and Cybersecurity, evaluates AI agents, and identifies strengths, weaknesses, and future directions for improvement across models like GPT and Llama.

Conclusion: DRBench serves as a comprehensive tool to push the boundaries of enterprise-focused AI research by assessing multi-domain deep research capabilities and releasing code for further exploration.

Abstract: We introduce DRBench, a benchmark for evaluating AI agents on complex,
open-ended deep research tasks in enterprise settings. Unlike prior benchmarks
that focus on simple questions or web-only queries, DRBench evaluates agents on
multi-step queries (for example, ``What changes should we make to our product
roadmap to ensure compliance with this standard?") that require identifying
supporting facts from both the public web and private company knowledge base.
Each task is grounded in realistic user personas and enterprise context,
spanning a heterogeneous search space that includes productivity software,
cloud file systems, emails, chat conversations, and the open web. Tasks are
generated through a carefully designed synthesis pipeline with
human-in-the-loop verification, and agents are evaluated on their ability to
recall relevant insights, maintain factual accuracy, and produce coherent,
well-structured reports. We release 15 deep research tasks across 10 domains,
such as Sales, Cybersecurity, and Compliance. We demonstrate the effectiveness
of DRBench by evaluating diverse DR agents across open- and closed-source
models (such as GPT, Llama, and Qwen) and DR strategies, highlighting their
strengths, weaknesses, and the critical path for advancing enterprise deep
research. Code is available at https://github.com/ServiceNow/drbench.

</details>


### [22] [PrimeX: A Dataset of Worldview, Opinion, and Explanation](https://arxiv.org/abs/2510.00174)
*Rik Koncel-Kedziorski,Brihi Joshi,Tim Paek*

Main category: cs.CL

TL;DR: This paper introduces PrimeX, a dataset enhancing opinion prediction by incorporating belief explanations and worldview data to better personalize language models.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore how individual belief systems can improve the alignment and personalization of language models.

Method: PrimeX was developed using survey data from 858 US respondents, combining public opinion data, written explanations for opinions, and worldview assessments via the Primal World Belief survey.

Result: The findings indicate that incorporating belief explanations and worldview data enriches the personalization capabilities of language models.

Conclusion: PrimeX provides valuable insights for both NLP and psychological research, encouraging further exploration into the use of belief representations in models.

Abstract: As the adoption of language models advances, so does the need to better
represent individual users to the model. Are there aspects of an individual's
belief system that a language model can utilize for improved alignment?
Following prior research, we investigate this question in the domain of opinion
prediction by developing PrimeX, a dataset of public opinion survey data from
858 US residents with two additional sources of belief information: written
explanations from the respondents for why they hold specific opinions, and the
Primal World Belief survey for assessing respondent worldview. We provide an
extensive initial analysis of our data and show the value of belief
explanations and worldview for personalizing language models. Our results
demonstrate how the additional belief information in PrimeX can benefit both
the NLP and psychological research communities, opening up avenues for further
study.

</details>


### [23] [Personalized Reasoning: Just-In-Time Personalization and Why LLMs Fail At It](https://arxiv.org/abs/2510.00177)
*Shuyue Stella Li,Avinandan Bose,Faeze Brahman,Simon Shaolei Du,Pang Wei Koh,Maryam Fazel,Yulia Tsvetkov*

Main category: cs.CL

TL;DR: The paper presents PREFDISCO, a methodology to evaluate the personalized reasoning ability of large language models, revealing their limitations in adapting to individual user preferences.


<details>
  <summary>Details</summary>
Motivation: To highlight the importance of personalized reasoning in LLMs, which goes beyond objective correctness and human preference alignment, especially in scenarios with no prior user context.

Method: The paper introduces PREFDISCO, a framework using psychologically-grounded personas and sparse preferences to evaluate LLMs in interactive personalization tasks.

Result: Evaluation of 21 models showed that naive personalization often resulted in worse outcomes than generic responses, and current LLMs struggle in personalized reasoning.

Conclusion: Dedicated development toward personalized reasoning is necessary, as current LLMs fail to naturally adapt to individual preferences, particularly in critical fields like education and healthcare.

Abstract: Current large language model (LLM) development treats task-solving and
preference alignment as separate challenges, optimizing first for objective
correctness, then for alignment to aggregated human preferences. This paradigm
fails in human-facing applications where solving a problem correctly is
insufficient if the response mismatches the user's needs. This challenge
intensifies in just-in-time scenarios where no prior user interaction history
exists due to cold-start conditions or privacy constraints. LLMs need to
identify what they don't know about user preferences, strategically elicit
preference values through questioning, then adapt their reasoning processes and
responses accordingly -- a complicated chain of cognitive processes which we
term personalized reasoning. We introduce PREFDISCO, an evaluation methodology
that transforms static benchmarks into interactive personalization tasks using
psychologically-grounded personas with sparse preferences. Our framework
creates scenarios where identical questions require different reasoning chains
depending on user context, as optimal explanation approaches vary by individual
expertise and preferences while maintaining factual accuracy. Evaluation of 21
frontier models across 10 tasks reveals 29.0% of naive personalization attempts
produce worse preference alignment than generic responses, yet generic
responses also fail to serve individual user needs effectively. These findings
suggest personalized reasoning requires dedicated development rather than
emerging naturally. PREFDISCO establishes personalized reasoning as a
measurable research frontier and reveals fundamental limitations in current
LLMs' interactive capabilities, providing a foundation for developing systems
that can adapt to individual users in education, healthcare, and technical
domains where personalization is critical.

</details>


### [24] [BiasFreeBench: a Benchmark for Mitigating Bias in Large Language Model Responses](https://arxiv.org/abs/2510.00232)
*Xin Xu,Xunzhi He,Churan Zhi,Ruizhe Chen,Julian McAuley,Zexue He*

Main category: cs.CL

TL;DR: BiasFreeBench proposes a unified benchmark for evaluating bias mitigation methods in LLMs using a new response-level metric and systematic comparison of prompting-based and training-based methods.


<details>
  <summary>Details</summary>
Motivation: Current bias mitigation evaluations suffer from inconsistent baselines, metrics, and a disconnect with real-world interactions with LLMs.

Method: The paper reorganized datasets into a unified query-response format and introduced the Bias-Free Score to assess fairness, safety, and anti-stereotypical properties of LLM outputs.

Result: Eight bias mitigation techniques were compared across multiple dimensions, revealing insights and generalization limitations for unseen bias types.

Conclusion: BiasFreeBench offers a unified testbed that addresses inconsistencies in the field and provides standardized metrics for evaluating bias mitigation approaches.

Abstract: Existing studies on bias mitigation methods for large language models (LLMs)
use diverse baselines and metrics to evaluate debiasing performance, leading to
inconsistent comparisons among them. Moreover, their evaluations are mostly
based on the comparison between LLMs' probabilities of biased and unbiased
contexts, which ignores the gap between such evaluations and real-world use
cases where users interact with LLMs by reading model responses and expect fair
and safe outputs rather than LLMs' probabilities. To enable consistent
evaluation across debiasing methods and bridge this gap, we introduce
BiasFreeBench, an empirical benchmark that comprehensively compares eight
mainstream bias mitigation techniques (covering four prompting-based and four
training-based methods) on two test scenarios (multi-choice QA and open-ended
multi-turn QA) by reorganizing existing datasets into a unified query-response
setting. We further introduce a response-level metric, Bias-Free Score, to
measure the extent to which LLM responses are fair, safe, and
anti-stereotypical. Debiasing performances are systematically compared and
analyzed across key dimensions: the prompting vs. training paradigm, model
size, and generalization of different training strategies to unseen bias types.
We will publicly release our benchmark, aiming to establish a unified testbed
for bias mitigation research.

</details>


### [25] [TASER: Translation Assessment via Systematic Evaluation and Reasoning](https://arxiv.org/abs/2510.00255)
*Monishwaran Maheswaran,Marco Carini,Christian Federmann,Tony Diaz*

Main category: cs.CL

TL;DR: TASER introduces a novel translation quality assessment metric that utilizes Large Reasoning Models (LRMs) for systematic evaluation, achieving state-of-the-art performance in automated translation assessment.


<details>
  <summary>Details</summary>
Motivation: The study seeks to address the limitations of traditional metrics for translation quality assessment by leveraging transparent reasoning capabilities of LRMs.

Method: TASER employs structured prompting templates to guide LRMs in systematic reasoning for both reference-based and reference-free evaluation scenarios.

Result: TASER demonstrates top accuracy in system-level evaluations across reference-based and reference-free setups, with a reference-free version achieving superior segment-level performance among similar metrics.

Conclusion: Large Reasoning Models provide measurable improvements in translation assessment accuracy while offering greater interpretability, making them effective for diverse language evaluations.

Abstract: We introduce TASER (Translation Assessment via Systematic Evaluation and
Reasoning), a metric that uses Large Reasoning Models (LRMs) for automated
translation quality assessment. TASER harnesses the explicit reasoning
capabilities of LRMs to conduct systematic, step-by-step evaluation of
translation quality. We evaluate TASER on the WMT24 Metrics Shared Task across
both reference-based and reference-free scenarios, demonstrating
state-of-the-art performance. In system-level evaluation, TASER achieves the
highest soft pairwise accuracy in both reference-based and reference-free
settings, outperforming all existing metrics. At the segment level, TASER
maintains competitive performance with our reference-free variant ranking as
the top-performing metric among all reference-free approaches. Our experiments
reveal that structured prompting templates yield superior results with LRMs
compared to the open-ended approaches that proved optimal for traditional LLMs.
We evaluate o3, a large reasoning model from OpenAI, with varying reasoning
efforts, providing insights into the relationship between reasoning depth and
evaluation quality. The explicit reasoning process in LRMs offers
interpretability and visibility, addressing a key limitation of existing
automated metrics. Our results demonstrate that Large Reasoning Models show a
measurable advancement in translation quality assessment, combining improved
accuracy with transparent evaluation across diverse language pairs.

</details>


### [26] [Retrieval-Augmented Generation for Electrocardiogram-Language Models](https://arxiv.org/abs/2510.00261)
*Xiaoyu Song,William Han,Tony Chen,Chaojing Duan,Michael A. Rosenberg,Emerson Liu,Ding Zhao*

Main category: cs.CL

TL;DR: The paper introduces an open-source RAG pipeline for Electrocardiogram-Language Models (ELMs) and evaluates its impact on natural language generation (NLG) tasks.


<details>
  <summary>Details</summary>
Motivation: To address the absence of open-source implementations and systematic studies of RAG pipeline design for ELMs, enabling improved textual responses conditioned on ECG data.

Method: Developed an open-source Retrieval-Augmented Generation (RAG) pipeline for ELMs, conducted ablation studies, and evaluated performance on three public datasets compared to non-RAG baselines.

Result: RAG integration consistently enhanced ELM performance in NLG tasks over non-RAG baselines.

Conclusion: Incorporating RAG into ELMs improves their versatility and output quality, providing design insights for future implementations.

Abstract: Interest in generative Electrocardiogram-Language Models (ELMs) is growing,
as they can produce textual responses conditioned on ECG signals and textual
queries. Unlike traditional classifiers that output label probabilities, ELMs
are more versatile, supporting domain-specific tasks (e.g., waveform analysis,
diagnosis, prognosis) as well as general tasks (e.g., open-ended questions,
dialogue). Retrieval-Augmented Generation (RAG), widely used in Large Language
Models (LLMs) to ground LLM outputs in retrieved knowledge, helps reduce
hallucinations and improve natural language generation (NLG). However, despite
its promise, no open-source implementation or systematic study of RAG pipeline
design for ELMs currently exists. To address this gap, we present the first
open-source RAG pipeline for ELMs, along with baselines and ablation studies
for NLG. Experiments on three public datasets show that ELMs with RAG
consistently improves performance over non-RAG baselines and highlights key ELM
design considerations. Our code is available at:
https://github.com/willxxy/ECG-Bench.

</details>


### [27] [Judging with Confidence: Calibrating Autoraters to Preference Distributions](https://arxiv.org/abs/2510.00263)
*Zhuohang Li,Xiaowei Li,Chengyu Huang,Guowang Li,Katayoon Goshvadi,Bo Dai,Dale Schuurmans,Paul Zhou,Hamid Palangi,Yiwen Song,Palash Goyal,Murat Kantarcioglu,Bradley A. Malin,Yuan Xue*

Main category: cs.CL

TL;DR: This paper introduces a framework for calibrating probabilistic autoraters to better align large language models with diverse human preference distributions.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of current autoraters, which rely on discrete labels that fail to capture the complexity of subjective and nuanced human preferences.

Method: The paper formalizes a framework with two learning methods: supervised fine-tuning for dense labels and reinforcement learning for sparse binary labels.

Result: Empirical results show improved calibration, lower positional bias, and better alignment with target preference distributions without compromising performance on objective tasks.

Conclusion: Calibrating autoraters with a distribution-matching objective enhances their reliability and flexibility for aligning LLMs with human values.

Abstract: The alignment of large language models (LLMs) with human values increasingly
relies on using other LLMs as automated judges, or ``autoraters''. However,
their reliability is limited by a foundational issue: they are trained on
discrete preference labels, forcing a single ground truth onto tasks that are
often subjective, ambiguous, or nuanced. We argue that a reliable autorater
must learn to model the full distribution of preferences defined by a target
population. In this paper, we propose a general framework for calibrating
probabilistic autoraters to any given preference distribution. We formalize the
problem and present two learning methods tailored to different data conditions:
1) a direct supervised fine-tuning for dense, probabilistic labels, and 2) a
reinforcement learning approach for sparse, binary labels. Our empirical
results show that finetuning autoraters with a distribution-matching objective
leads to verbalized probability predictions that are better aligned with the
target preference distribution, with improved calibration and significantly
lower positional bias, all while preserving performance on objective tasks.

</details>


### [28] [Efficient Layer-wise LLM Fine-tuning for Revision Intention Prediction](https://arxiv.org/abs/2510.00268)
*Zhexiong Liu,Diane Litman*

Main category: cs.CL

TL;DR: The paper presents a method called IR-Tuning, a parameter-efficient fine-tuning approach for Large Language Models (LLMs), addressing their challenge in nuanced text classification tasks like text revision.


<details>
  <summary>Details</summary>
Motivation: Large Language Models are excellent at text generation but less effective at nuanced text classification tasks, such as text revision classification, due to their pre-training bias and lack of sufficient annotated data.

Method: The authors propose IR-Tuning, a layer-wise parameter-efficient fine-tuning approach that dynamically selects important layers for tuning based on their gradient norm distribution, reducing the need for comprehensive annotation and computational resources.

Result: Experiments demonstrate that IR-Tuning outperforms other layer-wise PEFT methods, exhibiting faster convergence, reduced GPU memory usage, and effectiveness even with small datasets.

Conclusion: IR-Tuning offers a promising solution for resource-constrained and nuanced classification tasks, enhancing LLM performance without extensive annotated datasets.

Abstract: Large Language Models (LLMs) have shown extraordinary success across various
text generation tasks; however, their potential for simple yet essential text
classification remains underexplored, as LLM pre-training tends to emphasize
generation over classification. While LLMs with instruction tuning can
transform classification into a generation task, they often struggle to
categorize nuanced texts. One such example is text revision, which involves
nuanced edits between pairs of texts. Although simply fine-tuning LLMs for
revision classification seems plausible, it requires a large amount of revision
annotations, which are exceptionally expensive and scarce in the community. To
address this issue, we introduce a plug-and-play layer-wise parameter-efficient
fine-tuning (PEFT) framework, i.e., IR-Tuning, which fine-tunes a subset of
important LLM layers that are dynamically selected based on their gradient norm
distribution, while freezing those of redundant layers. Extensive experiments
suggest that IR-Tuning surpasses several layer-wise PEFT baselines over diverse
text revisions, while achieving fast convergence, low GPU memory consumption,
and effectiveness on small revision corpora.

</details>


### [29] [SafePassage: High-Fidelity Information Extraction with Black Box LLMs](https://arxiv.org/abs/2510.00276)
*Joe Barrow,Raj Patel,Misha Kharkovski,Ben Davies,Ryan Schmitt*

Main category: cs.CL

TL;DR: The paper introduces "SafePassage," a pipeline to ensure information extracted by large language models (LLMs) is grounded in the document to reduce hallucinations in information extraction tasks.


<details>
  <summary>Details</summary>
Motivation: To address the issue of hallucinations and ensure extracted information by LLMs is trustworthy and grounded within the document itself.

Method: The proposed pipeline includes three steps: (1) structured entity extraction by LLM, (2) global alignment of these entities using string-based methods, and (3) scoring the groundedness of extracted information.

Result: The SafePassage pipeline reduces hallucinations by 85% in information extraction tasks, with high agreement with human evaluation. A transformer encoder fine-tuned on task-specific examples outperformed an LLM scoring model in flagging unsafe passages.

Conclusion: SafePassage ensures safer and evaluable information extraction, showcasing its robustness in reducing hallucinations and the potential of fine-tuned transformer encoders for specific tasks.

Abstract: Black box large language models (LLMs) make information extraction (IE) easy
to configure, but hard to trust. Unlike traditional information extraction
pipelines, the information "extracted" is not guaranteed to be grounded in the
document. To prevent this, this paper introduces the notion of a "safe
passage": context generated by the LLM that is both grounded in the document
and consistent with the extracted information. This is operationalized via a
three-step pipeline, SafePassage, which consists of: (1) an LLM extractor that
generates structured entities and their contexts from a document, (2) a
string-based global aligner, and (3) a scoring model. Results show that using
these three parts in conjunction reduces hallucinations by up to 85% on
information extraction tasks with minimal risk of flagging non-hallucinations.
High agreement between the SafePassage pipeline and human judgments of
extraction quality mean that the pipeline can be dually used to evaluate LLMs.
Surprisingly, results also show that using a transformer encoder fine-tuned on
a small number of task-specific examples can outperform an LLM scoring model at
flagging unsafe passages. These annotations can be collected in as little as
1-2 hours.

</details>


### [30] [ReEvalMed: Rethinking Medical Report Evaluation by Aligning Metrics with Real-World Clinical Judgment](https://arxiv.org/abs/2510.00280)
*Ruochen Li,Jun Li,Bailiang Jian,Kun Yuan,Youxiang Zhu*

Main category: cs.CL

TL;DR: This paper highlights flaws in current automated radiology report evaluation metrics and proposes a clinically grounded Meta-Evaluation framework for improvement.


<details>
  <summary>Details</summary>
Motivation: Current metrics for assessing automated radiology reports are inadequate, as they don't align with clinical requirements or accurately interpret clinical semantics.

Method: The authors introduce a Meta-Evaluation framework built on clinically grounded criteria, supported by a fine-grained dataset of annotated ground truth and rewritten report pairs.

Result: The evaluation identified critical limitations in existing metrics, such as failing to detect clinically significant errors, over-penalizing harmless variations, and showing inconsistency across error severity levels.

Conclusion: The proposed framework can help construct more clinically reliable evaluation methods, bridging the gap between metric scores and clinician trust.

Abstract: Automatically generated radiology reports often receive high scores from
existing evaluation metrics but fail to earn clinicians' trust. This gap
reveals fundamental flaws in how current metrics assess the quality of
generated reports. We rethink the design and evaluation of these metrics and
propose a clinically grounded Meta-Evaluation framework. We define clinically
grounded criteria spanning clinical alignment and key metric capabilities,
including discrimination, robustness, and monotonicity. Using a fine-grained
dataset of ground truth and rewritten report pairs annotated with error types,
clinical significance labels, and explanations, we systematically evaluate
existing metrics and reveal their limitations in interpreting clinical
semantics, such as failing to distinguish clinically significant errors,
over-penalizing harmless variations, and lacking consistency across error
severity levels. Our framework offers guidance for building more clinically
reliable evaluation methods.

</details>


### [31] [o-MEGA: Optimized Methods for Explanation Generation and Analysis](https://arxiv.org/abs/2510.00288)
*Ľuboš Kriš,Jaroslav Kopčan,Qiwei Peng,Andrej Ridzik,Marcel Veselý,Martin Tamajka*

Main category: cs.CL

TL;DR: o-mega is a tool designed to optimize explainable AI methods for semantic matching, improving transparency in fact-checking systems.


<details>
  <summary>Details</summary>
Motivation: The challenge in achieving transparency and trust in transformer-based NLP models and the need to select optimal explainability methods.

Method: Introduction of o-mega, a tool that uses hyperparameter optimization to identify effective explainable AI methods for semantic matching.

Result: o-mega demonstrated improved transparency in fact-checking pipelines by systematically optimizing explainability methods and their configurations.

Conclusion: Automated optimization of explanation methods using tools like o-mega can enhance trustworthiness and transparency in AI systems for misinformation detection.

Abstract: The proliferation of transformer-based language models has revolutionized NLP
domain while simultaneously introduced significant challenges regarding model
transparency and trustworthiness. The complexity of achieving explainable
systems in this domain is evidenced by the extensive array of explanation
methods and evaluation metrics developed by researchers. To address the
challenge of selecting optimal explainability approaches, we present
\textbf{\texttt{o-mega}}, a hyperparameter optimization tool designed to
automatically identify the most effective explainable AI methods and their
configurations within the semantic matching domain. We evaluate o-mega on a
post-claim matching pipeline using a curated dataset of social media posts
paired with refuting claims. Our tool systematically explores different
explainable methods and their hyperparameters, demonstrating improved
transparency in automated fact-checking systems. As a result, such automated
optimization of explanation methods can significantly enhance the
interpretability of claim-matching models in critical applications such as
misinformation detection, contributing to more trustworthy and transparent AI
systems.

</details>


### [32] [CORTEX: Collaborative LLM Agents for High-Stakes Alert Triage](https://arxiv.org/abs/2510.00311)
*Bowen Wei,Yuan Shen Tay,Howard Liu,Jinhao Pan,Kun Luo,Ziwei Zhu,Chris Jordan*

Main category: cs.CL

TL;DR: The paper introduces CORTEX, a multi-agent LLM architecture to alleviate alert fatigue in SOCs by improving investigation accuracy and reducing false positives.


<details>
  <summary>Details</summary>
Motivation: SOCs face alert fatigue due to a high volume of daily alerts with limited genuine threats, leading to missed attacks and analyst burnout.

Method: CORTEX employs specialized agents: behavior-analysis, evidence-gathering, and reasoning agents working collaboratively to analyze evidence. The paper also introduces a dataset reflecting SOC investigations.

Result: CORTEX demonstrated a significant reduction in false positives and enhanced the quality of investigations compared to single-agent LLM systems across various enterprise scenarios.

Conclusion: CORTEX offers a more transparent and resilient solution to alert triage than previous approaches, improving SOC efficiency and threat detection accuracy.

Abstract: Security Operations Centers (SOCs) are overwhelmed by tens of thousands of
daily alerts, with only a small fraction corresponding to genuine attacks. This
overload creates alert fatigue, leading to overlooked threats and analyst
burnout. Classical detection pipelines are brittle and context-poor, while
recent LLM-based approaches typically rely on a single model to interpret logs,
retrieve context, and adjudicate alerts end-to-end -- an approach that
struggles with noisy enterprise data and offers limited transparency. We
propose CORTEX, a multi-agent LLM architecture for high-stakes alert triage in
which specialized agents collaborate over real evidence: a behavior-analysis
agent inspects activity sequences, evidence-gathering agents query external
systems, and a reasoning agent synthesizes findings into an auditable decision.
To support training and evaluation, we release a dataset of fine-grained SOC
investigations from production environments, capturing step-by-step analyst
actions and linked tool outputs. Across diverse enterprise scenarios, CORTEX
substantially reduces false positives and improves investigation quality over
state-of-the-art single-agent LLMs.

</details>


### [33] [TokMem: Tokenized Procedural Memory for Large Language Models](https://arxiv.org/abs/2510.00444)
*Zijun Wu,Yongchang Hao,Lili Mou*

Main category: cs.CL

TL;DR: TokMem is introduced as a tokenized procedural memory for large language models that offers efficient task management and avoids prompt overhead while improving performance.


<details>
  <summary>Details</summary>
Motivation: Address inefficiencies in large language models due to repetitive prompts and lack of modular reuse.

Method: Develop TokMem, a tokenized procedural memory storing procedures as compact embeddings with controlled generation signals.

Result: TokMem outperformed retrieval-augmented generation and fine-tuning across recall tasks with fewer parameters and without context overhead.

Conclusion: TokMem provides a scalable, modular alternative to prompt engineering, enhancing efficiency and adaptability in language models.

Abstract: Large language models rely heavily on prompts to specify tasks, recall
knowledge and guide reasoning. However, this reliance is inefficient as prompts
must be re-read at each step, scale poorly across tasks, and lack mechanisms
for modular reuse. We introduce TokMem, a tokenized procedural memory that
stores recurring procedures as compact, trainable embeddings. Each memory token
encodes both an address to a procedure and a control signal that steers
generation, enabling targeted behavior with constant-size overhead. To support
continual adaptation, TokMem keeps the backbone model frozen, allowing new
procedures to be added without interfering with existing ones. We evaluate
TokMem on 1,000 tasks for atomic recall, and on function-calling tasks for
compositional recall, where it consistently outperforms retrieval-augmented
generation while avoiding repeated context overhead, and fine-tuning with far
fewer parameters. These results establish TokMem as a scalable and modular
alternative to prompt engineering and fine-tuning, offering an explicit
procedural memory for LLMs.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [34] [Hybrid Deep Learning for Hyperspectral Single Image Super-Resolution](https://arxiv.org/abs/2510.00033)
*Usman Muhammad,Jorma Laaksonen*

Main category: cs.CV

TL;DR: The paper introduces Spectral-Spatial Unmixing Fusion (SSUF) and a custom Spatial-Spectral Gradient Loss to enhance hyperspectral single image super-resolution (SISR) performance, achieving competitive results on public datasets while reducing model complexity.


<details>
  <summary>Details</summary>
Motivation: Hyperspectral SISR struggles with maintaining fine spatial details and spectral fidelity using conventional deep learning methods.

Method: Spectral-Spatial Unmixing Fusion (SSUF) is integrated into 2D convolutional architectures, combined with a ResNet-based model and a custom loss function incorporating mean squared error, spatial, and spectral gradients.

Result: The hybrid deep learning model delivers competitive performance on three public hyperspectral datasets and reduces model complexity.

Conclusion: The proposed SSUF module and custom loss function effectively address hyperspectral SISR challenges, offering a balance between performance and complexity.

Abstract: Hyperspectral single image super-resolution (SISR) is a challenging task due
to the difficulty of restoring fine spatial details while preserving spectral
fidelity across a wide range of wavelengths, which limits the performance of
conventional deep learning models. To address this challenge, we introduce
Spectral-Spatial Unmixing Fusion (SSUF), a novel module that can be seamlessly
integrated into standard 2D convolutional architectures to enhance both spatial
resolution and spectral integrity. The SSUF combines spectral unmixing with
spectral--spatial feature extraction and guides a ResNet-based convolutional
neural network for improved reconstruction. In addition, we propose a custom
Spatial-Spectral Gradient Loss function that integrates mean squared error with
spatial and spectral gradient components, encouraging accurate reconstruction
of both spatial and spectral features. Experiments on three public remote
sensing hyperspectral datasets demonstrate that the proposed hybrid deep
learning model achieves competitive performance while reducing model
complexity.

</details>


### [35] [Review of Hallucination Understanding in Large Language and Vision Models](https://arxiv.org/abs/2510.00034)
*Zhengyi Ho,Siyuan Liang,Dacheng Tao*

Main category: cs.CV

TL;DR: This paper addresses hallucinations in large language and vision models by presenting a unified framework for understanding and mitigating them.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in generative AI systems can cause misinformation, financial, and operational risks, yet current understanding and solutions remain fragmented.

Method: A unified framework characterizes image and text hallucinations, links them to model lifecycle mechanisms using a task-modality interleaved approach.

Result: Hallucinations are found to stem from predictable data distribution patterns and inherited biases.

Conclusion: The study lays a foundation for creating robust solutions and enhancing reliability in generative AI systems.

Abstract: The widespread adoption of large language and vision models in real-world
applications has made urgent the need to address hallucinations -- instances
where models produce incorrect or nonsensical outputs. These errors can
propagate misinformation during deployment, leading to both financial and
operational harm. Although much research has been devoted to mitigating
hallucinations, our understanding of it is still incomplete and fragmented.
Without a coherent understanding of hallucinations, proposed solutions risk
mitigating surface symptoms rather than underlying causes, limiting their
effectiveness and generalizability in deployment. To tackle this gap, we first
present a unified, multi-level framework for characterizing both image and text
hallucinations across diverse applications, aiming to reduce conceptual
fragmentation. We then link these hallucinations to specific mechanisms within
a model's lifecycle, using a task-modality interleaved approach to promote a
more integrated understanding. Our investigations reveal that hallucinations
often stem from predictable patterns in data distributions and inherited
biases. By deepening our understanding, this survey provides a foundation for
developing more robust and effective solutions to hallucinations in real-world
generative AI systems.

</details>


### [36] [On Robustness of Vision-Language-Action Model against Multi-Modal Perturbations](https://arxiv.org/abs/2510.00037)
*Jianing Guo,Zhenhong Wu,Chang Tu,Yiyao Ma,Xiangqi Kong,Zhiqian Liu,Jiaming Ji,Shuning Zhang,Yuanpei Chen,Kai Chen,Xianglong Liu,Qi Dou,Yaodong Yang,Huijie Zhao,Weifeng Lv,Simin Li*

Main category: cs.CV

TL;DR: The paper introduces RobustVLA, a robust model for Vision-Language-Action (VLA) tasks against diverse, multi-modal real-world perturbations. It delivers superior performance gains across 17 evaluated perturbations.


<details>
  <summary>Details</summary>
Motivation: Multi-modal Vision-Language-Action models need robustness against real-world perturbations in actions, environments, and observations, which is overlooked in prior research focusing on visual disturbances.

Method: RobustVLA uses offline robust optimization for output noise, consistent action enforcement for input changes while leveraging a multi-armed bandit approach to identify harmful perturbations. The pi0 backbone with diffusion-based action head enhances robustness.

Result: Experiments on the LIBERO benchmark showed RobustVLA achieving 12.6% better results on the pi0 backbone, 50.6x faster inference, and significant gains in mixed perturbation and real-world robot scenarios by up to 65.6%.

Conclusion: RobustVLA effectively builds multi-modal robustness for Vision-Language-Action tasks, outperforming baselines under diverse real-world perturbations, providing advancements in both efficiency and accuracy.

Abstract: In Vision-Language-Action (VLA) models, robustness to real-world
perturbations is critical for deployment. Existing methods target simple visual
disturbances, overlooking the broader multi-modal perturbations that arise in
actions, instructions, environments, and observations. Here, we first evaluate
the robustness of mainstream VLAs under 17 perturbations across four
modalities. We find (1) actions as the most fragile modality, (2) Existing
visual-robust VLA do not gain robustness in other modality, and (3) pi0
demonstrates superior robustness with a diffusion-based action head. To build
multi-modal robust VLAs, we propose RobustVLA against perturbations in VLA
inputs and outputs. For output robustness, we perform offline robust
optimization against worst-case action noise that maximizes mismatch in flow
matching objective. This can be seen as adversarial training, label smoothing,
and outlier penalization. For input robustness, we enforce consistent actions
across input variations that preserve task semantics. To account for multiple
perturbations, we formulate robustness as a multi-armed bandit problem and
apply an upper confidence bound algorithm to automatically identify the most
harmful noise. Experiments on LIBERO demonstrate our RobustVLA delivers
absolute gains over baselines of 12.6% on the pi0 backbone and 10.4% on the
OpenVLA backbone across all 17 perturbations, achieving 50.6x faster inference
than existing visual-robust VLAs, and a 10.4% gain under mixed perturbations.
Our RobustVLA is particularly effective on real-world FR5 robot with limited
demonstrations, showing absolute gains by 65.6% under perturbations of four
modalities.

</details>


### [37] [Uncovering Intrinsic Capabilities: A Paradigm for Data Curation in Vision-Language Models](https://arxiv.org/abs/2510.00040)
*Junjie Li,Ziao Wang,Jianghong Ma,Xiaofeng Zhang*

Main category: cs.CV

TL;DR: The paper introduces CADC, a framework to improve instruction tuning in vision-language models by leveraging intrinsic capability analysis, achieving better performance with smaller datasets.


<details>
  <summary>Details</summary>
Motivation: Instruction tuning for vision-language models is difficult, and budget reductions in datasets lead to performance regressions due to reliance on heuristic, black-box strategies.

Method: CADC leverages unsupervised intrinsic capability discovery, gradient-based learning trajectories, influence estimation, and curriculum design for data curation in instruction tuning.

Result: CADC achieves benchmark-beating performance with just 5% of the original dataset, proving the value of its capability-driven approach.

Conclusion: Intrinsic capabilities are fundamental to model learning, and CADC demonstrates a controllable, capability-focused approach for data curation in instruction tuning.

Abstract: Large vision-language models (VLMs) achieve strong benchmark performance, but
controlling their behavior through instruction tuning remains difficult.
Reducing the budget of instruction tuning dataset often causes regressions, as
heuristic strategies treat models as black boxes and overlook the latent
capabilities that govern learning. We introduce Capability-Attributed Data
Curation (CADC), a framework that shifts curation from task-specific heuristics
to intrinsic capability analysis. CADC discovers intrinsic capabilities in an
unsupervised manner from gradient-based learning trajectories, attributes
training data to these capabilities via influence estimation, and curates
capability-aware curricula through balanced selection and staged sequencing.
This transforms black-box instruction tuning into a controllable,
capability-driven process. With as little as 5% of the original data, CADC
surpasses full-data training on multimodal benchmarks. These results validate
intrinsic capabilities as the fundamental building blocks of model learning and
establish CADC as a principle paradigm for instruction data curation.

</details>


### [38] [Culture In a Frame: C$^3$B as a Comic-Based Benchmark for Multimodal Culturally Awareness](https://arxiv.org/abs/2510.00041)
*Yuchen Song,Andong Chen,Wenxin Zhu,Kehai Chen,Xuefeng Bai,Muyun Yang,Tiejun Zhao*

Main category: cs.CV

TL;DR: The paper introduces C$^3$B, a benchmark designed to evaluate cultural awareness in Multimodal Large Language Models (MLLMs), revealing significant gaps between model and human performance.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for cultural awareness in MLLMs are not sufficiently complex, lack cross-lingual tasks, and predominantly use single-culture images, making them less effective for evaluating advanced capabilities.

Method: The authors created the C$^3$B benchmark with over 2000 images and 18,000 QA pairs, structured across three tasks with increasing difficulty: visual recognition, cultural conflict understanding, and cultural content generation.

Result: Evaluation of 11 open-source MLLMs showed that they performed significantly worse compared to humans on the C$^3$B benchmark, indicating substantial challenges in current model capabilities.

Conclusion: C$^3$B highlights a critical gap in MLLM cultural awareness and offers a robust tool to drive advancements in this area, encouraging more sophisticated model development.

Abstract: Cultural awareness capabilities has emerged as a critical capability for
Multimodal Large Language Models (MLLMs). However, current benchmarks lack
progressed difficulty in their task design and are deficient in cross-lingual
tasks. Moreover, current benchmarks often use real-world images. Each
real-world image typically contains one culture, making these benchmarks
relatively easy for MLLMs. Based on this, we propose C$^3$B ($\textbf{C}$omics
$\textbf{C}$ross-$\textbf{C}$ultural $\textbf{B}$enchmark), a novel
multicultural, multitask and multilingual cultural awareness capabilities
benchmark. C$^3$B comprises over 2000 images and over 18000 QA pairs,
constructed on three tasks with progressed difficulties, from basic visual
recognition to higher-level cultural conflict understanding, and finally to
cultural content generation. We conducted evaluations on 11 open-source MLLMs,
revealing a significant performance gap between MLLMs and human performance.
The gap demonstrates that C$^3$B poses substantial challenges for current
MLLMs, encouraging future research to advance the cultural awareness
capabilities of MLLMs.

</details>


### [39] [Beyond the Prompt: Gender Bias in Text-to-Image Models, with a Case Study on Hospital Professions](https://arxiv.org/abs/2510.00045)
*Franck Vandewiele,Remi Synave,Samuel Delepoulle,Remi Cozot*

Main category: cs.CV

TL;DR: The paper studies gender bias in six advanced text-to-image models when generating images for hospital professions and finds systematic occupational stereotypes, highlighting the need for bias-aware designs.


<details>
  <summary>Details</summary>
Motivation: Highlighting the issue of social biases embedded in text-to-image models, particularly stereotypes in professional roles.

Method: Generated 100 images for five hospital professions using customized prompts and examined the impact of portrait qualifiers across six models.

Result: All models exhibited gender stereotypes in occupations. Qwen-Image and SDXL enforced male dominance, while FLUX.1-dev skewed female. Prompt sensitivity varied significantly across models.

Conclusion: Systematic gender bias exists across models but is model-specific and tied to prompt influence. Better default designs and user guidance are needed to address stereotypes.

Abstract: Text-to-image (TTI) models are increasingly used in professional,
educational, and creative contexts, yet their outputs often embed and amplify
social biases. This paper investigates gender representation in six
state-of-the-art open-weight models: HunyuanImage 2.1, HiDream-I1-dev,
Qwen-Image, FLUX.1-dev, Stable-Diffusion 3.5 Large, and Stable-Diffusion-XL.
Using carefully designed prompts, we generated 100 images for each combination
of five hospital-related professions (cardiologist, hospital director, nurse,
paramedic, surgeon) and five portrait qualifiers ("", corporate, neutral,
aesthetic, beautiful).
  Our analysis reveals systematic occupational stereotypes: all models produced
nurses exclusively as women and surgeons predominantly as men. However,
differences emerge across models: Qwen-Image and SDXL enforce rigid male
dominance, HiDream-I1-dev shows mixed outcomes, and FLUX.1-dev skews female in
most roles. HunyuanImage 2.1 and Stable-Diffusion 3.5 Large also reproduce
gender stereotypes but with varying degrees of sensitivity to prompt
formulation. Portrait qualifiers further modulate gender balance, with terms
like corporate reinforcing male depictions and beautiful favoring female ones.
Sensitivity varies widely: Qwen-Image remains nearly unaffected, while
FLUX.1-dev, SDXL, and SD3.5 show strong prompt dependence.
  These findings demonstrate that gender bias in TTI models is both systematic
and model-specific. Beyond documenting disparities, we argue that prompt
wording plays a critical role in shaping demographic outcomes. The results
underscore the need for bias-aware design, balanced defaults, and user guidance
to prevent the reinforcement of occupational stereotypes in generative AI.

</details>


### [40] [Reinforcement Learning-Based Prompt Template Stealing for Text-to-Image Models](https://arxiv.org/abs/2510.00046)
*Xiaotian Zou*

Main category: cs.CV

TL;DR: The paper introduces RLStealer, a framework capable of stealing prompts used in multimodal large language models from example images.


<details>
  <summary>Details</summary>
Motivation: Addressing the overlooked security risk in prompt trading, where proprietary prompts can be stolen in multimodal large language models (MLLMs).

Method: The framework uses reinforcement learning to treat prompt stealing as a sequential decision-making problem, employing similarity-based reward signals for efficient template recovery.

Result: RLStealer outperforms existing baselines in prompt inversion, reducing the attack cost to below 13% while generalizing across various image styles.

Conclusion: The study highlights the security risks in prompt trading markets for MLLMs and calls for protective standards to mitigate prompt theft vulnerabilities.

Abstract: Multimodal Large Language Models (MLLMs) have transformed text-to-image
workflows, allowing designers to create novel visual concepts with
unprecedented speed. This progress has given rise to a thriving prompt trading
market, where curated prompts that induce trademark styles are bought and sold.
Although commercially attractive, prompt trading also introduces a largely
unexamined security risk: the prompts themselves can be stolen.
  In this paper, we expose this vulnerability and present RLStealer, a
reinforcement learning based prompt inversion framework that recovers its
template from only a small set of example images. RLStealer treats template
stealing as a sequential decision making problem and employs multiple
similarity based feedback signals as reward functions to effectively explore
the prompt space. Comprehensive experiments on publicly available benchmarks
demonstrate that RLStealer gets state-of-the-art performance while reducing the
total attack cost to under 13% of that required by existing baselines. Our
further analysis confirms that RLStealer can effectively generalize across
different image styles to efficiently steal unseen prompt templates. Our study
highlights an urgent security threat inherent in prompt trading and lays the
groundwork for developing protective standards in the emerging MLLMs
marketplace.

</details>


### [41] [Explanation-Driven Counterfactual Testing for Faithfulness in Vision-Language Model Explanations](https://arxiv.org/abs/2510.00047)
*Sihao Ding,Santosh Vasa,Aditi Ramadwar*

Main category: cs.CV

TL;DR: The paper introduces EDCT, a method to test if Vision-Language Models' explanations align with the causal factors driving their predictions.


<details>
  <summary>Details</summary>
Motivation: Vision-Language Models (VLMs) provide convincing explanations, but these explanations may not truly reflect the causal factors behind their predictions, posing technical and governance risks.

Method: The proposed Explanation-Driven Counterfactual Testing (EDCT) automatically verifies VLMs by using their explanations as hypotheses. It parses NLEs into visual concepts, applies generative inpainting to create counterfactuals, and evaluates changes in answers and explanations with the Counterfactual Consistency Score (CCS).

Result: EDCT applied across 120 OK-VQA examples and various VLMs reveals significant faithfulness issues, indicating gaps where explanations fail causal verification.

Conclusion: The framework effectively identifies when VLM explanations are unfaithful, offering a systematic way to audit and improve their reliability.

Abstract: Vision-Language Models (VLMs) often produce fluent Natural Language
Explanations (NLEs) that sound convincing but may not reflect the causal
factors driving predictions. This mismatch of plausibility and faithfulness
poses technical and governance risks. We introduce Explanation-Driven
Counterfactual Testing (EDCT), a fully automated verification procedure for a
target VLM that treats the model's own explanation as a falsifiable hypothesis.
Given an image-question pair, EDCT: (1) obtains the model's answer and NLE, (2)
parses the NLE into testable visual concepts, (3) generates targeted
counterfactual edits via generative inpainting, and (4) computes a
Counterfactual Consistency Score (CCS) using LLM-assisted analysis of changes
in both answers and explanations. Across 120 curated OK-VQA examples and
multiple VLMs, EDCT uncovers substantial faithfulness gaps and provides
regulator-aligned audit artifacts indicating when cited concepts fail causal
tests.

</details>


### [42] [HiDe: Rethinking The Zoom-IN method in High Resolution MLLMs via Hierarchical Decoupling](https://arxiv.org/abs/2510.00054)
*Xianjie Liu,Yiman Hu,Yixiong Zou,Liang Wu,Jian Xu,Bo Zheng*

Main category: cs.CV

TL;DR: The paper introduces a novel framework, HiDe, addressing the limitations of Multimodal Large Language Models (MLLMs) in processing high-resolution images by targeting background interference, rather than object size, improving visual understanding performance significantly.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs struggle with high-resolution image tasks due to complex background interference, which limits their ability to focus on relevant visual regions.

Method: The proposed Hierarchical Decoupling Framework (HiDe) uses Token-wise Attention Decoupling (TAD) to isolate question and key information tokens, and Layout-Preserving Decoupling (LPD) to reconstruct compact representations by removing background interference while preserving spatial layouts.

Result: HiDe achieved state-of-the-art (SOTA) performance on benchmarks like V*Bench, HRBench4K, and HRBench8K, outperforming prior training-free approaches with significant memory optimization (75% less memory usage).

Conclusion: HiDe effectively enhances the visual processing capabilities of MLLMs for high-resolution images by addressing background interference, demonstrating superior performance and efficiency.

Abstract: Multimodal Large Language Models (MLLMs) have made significant strides in
visual understanding tasks. However, their performance on high-resolution
images remains suboptimal. While existing approaches often attribute this
limitation to perceptual constraints and argue that MLLMs struggle to recognize
small objects, leading them to use "zoom in" strategies for better detail, our
analysis reveals a different cause: the main issue is not object size, but
rather caused by complex background interference. We systematically analyze
this "zoom in" operation through a series of decoupling experiments and propose
the Hierarchical Decoupling Framework (HiDe), a training-free framework that
uses Token-wise Attention Decoupling (TAD) to decouple the question tokens and
identify the key information tokens, then leverages their attention weights to
achieve precise alignment with the target visual regions. Subsequently, it
employs Layout-Preserving Decoupling (LPD) to decouple these regions from the
background and reconstructs a compact representation that preserves essential
spatial layouts while eliminating background interference. HiDe sets a new SOTA
on V*Bench, HRBench4K, and HRBench8K, boosting Qwen2.5-VL 7B and InternVL3 8B
to SOTA (92.1% and 91.6% on V*Bench), even surpassing RL methods. After
optimization, HiDe uses 75% less memory than the previous training-free
approach. Code is provided in https://github.com/Tennine2077/HiDe.

</details>


### [43] [FSDENet: A Frequency and Spatial Domains based Detail Enhancement Network for Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2510.00059)
*Jiahao Fu,Yinfeng Yu,Liejun Wang*

Main category: cs.CV

TL;DR: The paper introduces FSDENet, a neural network for improving precision in remote sensing image segmentation by combining spatial and frequency domain enhancements.


<details>
  <summary>Details</summary>
Motivation: To address challenges in remote sensing image segmentation, especially semantic edge ambiguities caused by grayscale variations like shadows and low-contrast areas.

Method: FSDENet leverages spatial methods for multi-scale feature extraction and uses Fast Fourier Transform for integrating global frequency-domain information. Haar wavelet transform refines edge sensitivity in frequency components, combining spatial and frequency-domain insights.

Result: FSDENet significantly improves segmentation accuracy at boundaries and grayscale transitions, achieving SOTA performance on four datasets: LoveDA, Vaihingen, Potsdam, and iSAID.

Conclusion: The dual-domain integration approach successfully enhances performance in semantic segmentation tasks, particularly in challenging grayscale regions.

Abstract: To fully leverage spatial information for remote sensing image segmentation
and address semantic edge ambiguities caused by grayscale variations (e.g.,
shadows and low-contrast regions), we propose the Frequency and Spatial Domains
based Detail Enhancement Network (FSDENet). Our framework employs spatial
processing methods to extract rich multi-scale spatial features and
fine-grained semantic details. By effectively integrating global and
frequency-domain information through the Fast Fourier Transform (FFT) in global
mappings, the model's capability to discern global representations under
grayscale variations is significantly strengthened. Additionally, we utilize
Haar wavelet transform to decompose features into high- and low-frequency
components, leveraging their distinct sensitivity to edge information to refine
boundary segmentation. The model achieves dual-domain synergy by integrating
spatial granularity with frequency-domain edge sensitivity, substantially
improving segmentation accuracy in boundary regions and grayscale transition
zones. Comprehensive experimental results demonstrate that FSDENet achieves
state-of-the-art (SOTA) performance on four widely adopted datasets: LoveDA,
Vaihingen, Potsdam, and iSAID.

</details>


### [44] [Less is More: Lean yet Powerful Vision-Language Model for Autonomous Driving](https://arxiv.org/abs/2510.00060)
*Sheng Yang,Tong Zhan,Guancheng Chen,Yanfeng Lu,Jian Wang*

Main category: cs.CV

TL;DR: The paper introduces Max-V1, a one-stage end-to-end framework for autonomous driving that employs a vision-language model for trajectory prediction, achieving significant improvements in performance and generalization.


<details>
  <summary>Details</summary>
Motivation: Current challenges in autonomous driving include creating end-to-end systems capable of robust trajectory planning while managing cross-domain adaptability. The paper aims to address these issues by reconceptualizing driving as a sequential prediction task.

Method: The authors propose Max-V1, which uses a vision-language model (VLM) to predict driving trajectories directly from camera inputs. This is done through imitation learning from large-scale expert data and a statistically-informed supervision strategy.

Result: Max-V1 achieves state-of-the-art performance on the nuScenes dataset, showing over 30% improvement over existing baselines. It also demonstrates strong cross-domain generalization in datasets gathered from different vehicles.

Conclusion: The proposed Max-V1 framework presents a significant advancement in autonomous driving, offering both high performance and adaptability across different vehicles. It lays the groundwork for developing more robust and versatile self-driving systems.

Abstract: In this work, we reconceptualize autonomous driving as a generalized language
and formulate the trajectory planning task as next waypoint prediction. We
introduce Max-V1, a novel framework for one-stage end-to-end autonomous
driving. Our framework presents a single-pass generation paradigm that aligns
with the inherent sequentiality of driving. This approach leverages the
generative capacity of the VLM (Vision-Language Model) to enable end-to-end
trajectory prediction directly from front-view camera input. The efficacy of
this method is underpinned by a principled supervision strategy derived from
statistical modeling. This provides a well-defined learning objective, which
makes the framework highly amenable to master complex driving policies through
imitation learning from large-scale expert demonstrations. Empirically, our
method achieves the state-of-the-art performance on the nuScenes dataset,
delivers an overall improvement of over 30% compared to prior baselines.
Furthermore, it exhibits superior generalization performance on cross-domain
datasets acquired from diverse vehicles, demonstrating notable potential for
cross-vehicle robustness and adaptability. Due to these empirical strengths,
this work introduces a model enabling fundamental driving behaviors, laying the
foundation for the development of more capable self-driving agents. Code will
be available upon publication.

</details>


### [45] [Efficient CNN Compression via Multi-method Low Rank Factorization and Feature Map Similarity](https://arxiv.org/abs/2510.00062)
*M. Kokhazadeh,G. Keramidas,V. Kelefouras*

Main category: cs.CV

TL;DR: This paper introduces an end-to-end methodology for compressing CNNs effectively and efficiently using a novel rank selection strategy and one-shot fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Low-Rank Factorization techniques face challenges like rank selection, fine-tuning time, and compatibility across layer types.

Method: The paper proposes a Design Space Exploration framework integrating multiple LRF techniques and a rank selection based on feature map similarity.

Result: The methodology achieves substantial compression with minimal accuracy loss, performing better than state-of-the-art methods on 14 CNN models across 8 datasets.

Conclusion: Combining multiple LRF methods selectively per layer yields superior compression results, with the framework compatible with TensorFlow 2.x for practical use.

Abstract: Low-Rank Factorization (LRF) is a widely adopted technique for compressing
deep neural networks (DNNs). However, it faces several challenges, including
optimal rank selection, a vast design space, long fine-tuning times, and
limited compatibility with different layer types and decomposition methods.
This paper presents an end-to-end Design Space Exploration (DSE) methodology
and framework for compressing convolutional neural networks (CNNs) that
addresses all these issues. We introduce a novel rank selection strategy based
on feature map similarity, which captures non-linear interactions between layer
outputs more effectively than traditional weight-based approaches. Unlike prior
works, our method uses a one-shot fine-tuning process, significantly reducing
the overall fine-tuning time. The proposed framework is fully compatible with
all types of convolutional (Conv) and fully connected (FC) layers. To further
improve compression, the framework integrates three different LRF techniques
for Conv layers and three for FC layers, applying them selectively on a
per-layer basis. We demonstrate that combining multiple LRF methods within a
single model yields better compression results than using a single method
uniformly across all layers. Finally, we provide a comprehensive evaluation and
comparison of the six LRF techniques, offering practical insights into their
effectiveness across different scenarios. The proposed work is integrated into
TensorFlow 2.x, ensuring compatibility with widely used deep learning
workflows. Experimental results on 14 CNN models across eight datasets
demonstrate that the proposed methodology achieves substantial compression with
minimal accuracy loss, outperforming several state-of-the-art techniques.

</details>


### [46] [Intelligent 5S Audit: Application of Artificial Intelligence for Continuous Improvement in the Automotive Industry](https://arxiv.org/abs/2510.00067)
*Rafael da Silva Maciel,Lucio Veraldo Jr*

Main category: cs.CV

TL;DR: The paper introduces an AI-driven automated 5S audit system for industrial audits in the automotive industry, enhancing efficiency, objectivity, and alignment with Industry 4.0.


<details>
  <summary>Details</summary>
Motivation: Improve industrial organization audits by making them faster, objective, cost-efficient, and aligned with modern Industry 4.0 standards.

Method: Developed an automated 5S audit system leveraging large-scale language models, incorporating intelligent image analysis to assess the traditional 5S methodology.

Result: The system showed strong reliability (Cohen’s kappa = 0.75), reduced audit time by 50%, and achieved a 99.8% reduction in operational costs while ensuring consistent assessments.

Conclusion: This AI-driven methodology contributes significantly to the automotive industry by enabling scalable, efficient, and reliable implementation of lean systems.

Abstract: The evolution of the 5S methodology with the support of artificial
intelligence techniques represents a significant opportunity to improve
industrial organization audits in the automotive chain, making them more
objective, efficient and aligned with Industry 4.0 standards. This work
developed an automated 5S audit system based on large-scale language models
(LLM), capable of assessing the five senses (Seiri, Seiton, Seiso, Seiketsu,
Shitsuke) in a standardized way through intelligent image analysis. The
system's reliability was validated using Cohen's concordance coefficient (kappa
= 0.75), showing strong alignment between the automated assessments and the
corresponding human audits. The results indicate that the proposed solution
contributes significantly to continuous improvement in automotive manufacturing
environments, speeding up the audit process by 50% of the traditional time and
maintaining the consistency of the assessments, with a 99.8% reduction in
operating costs compared to traditional manual audits. The methodology
presented establishes a new paradigm for integrating lean systems with emerging
AI technologies, offering scalability for implementation in automotive plants
of different sizes.

</details>


### [47] [OIG-Bench: A Multi-Agent Annotated Benchmark for Multimodal One-Image Guides Understanding](https://arxiv.org/abs/2510.00069)
*Jiancong Xie,Wenjin Wang,Zhuomeng Zhang,Zihan Liu,Qi Liu,Ke Feng,Zixun Sun,Yuedong Yang*

Main category: cs.CV

TL;DR: The paper introduces OIG-Bench, a benchmark for assessing the ability of Multimodal Large Language Models (MLLMs) to understand "One-Image Guides"—a structured visual format combining text and images. Evaluating 29 state-of-the-art models, Qwen2.5-VL-72B performs best but reveals overall weaknesses in semantic understanding and reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks do not sufficiently assess the ability of MLLMs to understand "One-Image Guides," which are specifically designed to match human comprehension combining text, imagery, and symbols.

Method: The paper develops OIG-Bench, a benchmark, and employs a semi-automated annotation pipeline combining intelligent agents with human oversight to generate image-text pairs. It evaluates 29 models against the benchmark.

Result: Qwen2.5-VL-72B achieves the highest accuracy in the benchmark (77%), but all models display notable weaknesses in semantic and logical reasoning tasks. The multi-agent annotation system surpasses all evaluated MLLMs in image captioning quality.

Conclusion: Current MLLMs show limitations in interpreting complex relationships in visual-text formats like "One-Image Guides." The multi-agent annotation system emerges as a promising tool for generating high-quality image descriptions and aiding dataset development.

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have demonstrated
impressive capabilities. However, evaluating their capacity for human-like
understanding in One-Image Guides remains insufficiently explored. One-Image
Guides are a visual format combining text, imagery, and symbols to present
reorganized and structured information for easier comprehension, which are
specifically designed for human viewing and inherently embody the
characteristics of human perception and understanding. Here, we present
OIG-Bench, a comprehensive benchmark focused on One-Image Guide understanding
across diverse domains. To reduce the cost of manual annotation, we developed a
semi-automated annotation pipeline in which multiple intelligent agents
collaborate to generate preliminary image descriptions, assisting humans in
constructing image-text pairs. With OIG-Bench, we have conducted a
comprehensive evaluation of 29 state-of-the-art MLLMs, including both
proprietary and open-source models. The results show that Qwen2.5-VL-72B
performs the best among the evaluated models, with an overall accuracy of 77%.
Nevertheless, all models exhibit notable weaknesses in semantic understanding
and logical reasoning, indicating that current MLLMs still struggle to
accurately interpret complex visual-text relationships. In addition, we also
demonstrate that the proposed multi-agent annotation system outperforms all
MLLMs in image captioning, highlighting its potential as both a high-quality
image description generator and a valuable tool for future dataset
construction. Datasets are available at https://github.com/XiejcSYSU/OIG-Bench.

</details>


### [48] [Geo-R1: Unlocking VLM Geospatial Reasoning with Cross-View Reinforcement Learning](https://arxiv.org/abs/2510.00072)
*Chenhui Xu,Fuxun Yu,Michael J. Bianco,Jacob Kovarskiy,Raphael Tang,Qi Zhang,Zirui Xu,Will LeVine,Brandon Dubbs,Heming Liao,Cassandra Burgess,Suvam Bag,Jay Patravali,Rupanjali Kukal,Mikael Figueroa,Rishi Madhok,Nikolaos Karianakis,Jinjun Xiong*

Main category: cs.CV

TL;DR: Geo-R1 introduces a post-training framework for enhancing geospatial reasoning in vision-language models using synthetic exemplars and reinforcement learning, delivering state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the need for improved geospatial reasoning capabilities in vision-language models, which previously required costly human annotations and were limited to domain pretraining or supervised fine-tuning.

Method: Geo-R1 employs two stages: scaffolding, where synthetic chain-of-thought exemplars are used for supervised fine-tuning, and elevating, where GRPO-based reinforcement learning on a weakly-supervised cross-view pairing proxy supplies scalable reward signals.

Result: Geo-R1 achieves state-of-the-art performance in geospatial reasoning benchmarks, demonstrating its effectiveness through reasoning-focused training stages.

Conclusion: Geo-R1 successfully extends geospatial reasoning capabilities in models by introducing a reasoning-first post-training framework that is scalable, efficient, and surpasses prior methods.

Abstract: We introduce Geo-R1, a reasoning-centric post-training framework that unlocks
geospatial reasoning in vision-language models by combining thinking
scaffolding and elevating. In the scaffolding stage, Geo-R1 instills a
``geospatial thinking paradigm" via supervised fine-tuning on synthetic
chain-of-thought exemplars, enabling models to connect visual cues with
geographic priors without costly human reasoning annotations. In the elevating
stage, it uses GRPO-based reinforcement learning on a weakly-supervised
cross-view pairing proxy. This design supplies a verifiable and scalable reward
signal: teaching models to capture and reconcile features across modalities,
and harnessing reasoning for accurate prediction. Geo-R1 extends geospatial
modeling from domain pretraining / supervised finetuning to reasoning-first
post-training, and achieves state-of-the-art performance across various
geospatial reasoning benchmarks. Our model is available at
https://huggingface.co/miniHui/Geo-R1.

</details>


### [49] [Enhancing Certifiable Semantic Robustness via Robust Pruning of Deep Neural Networks](https://arxiv.org/abs/2510.00083)
*Hanjiang Hu,Bowei Li,Ziwei Wang,Tianhao Wei,Casidhe Hutchison,Eric Sample,Changliu Liu*

Main category: cs.CV

TL;DR: The paper presents a novel neural network pruning method to improve robustness against perturbations like brightness and contrast by introducing a metric called Unbiased and Smooth Neuron (USN) and using a new loss function based on Wasserstein distance.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks are vulnerable to robustness issues when facing semantic transformation perturbations like brightness and contrast, and existing methods struggle due to over-parameterization.

Method: The method involves defining a metric, USN, to evaluate neuron stability and variance, pruning low-USN neurons, and employing a Wasserstein loss to ensure concentration of pruned neurons across layers.

Result: Extensive experiments on robust keypoint detection tasks with brightness and contrast perturbations show that the proposed method improves both robustness certification and computational efficiency over baseline models.

Conclusion: The proposed pruning approach effectively reduces over-parameterization while maintaining robustness, leading to superior performance and efficiency in challenging perturbation scenarios.

Abstract: Deep neural networks have been widely adopted in many vision and robotics
applications with visual inputs. It is essential to verify its robustness
against semantic transformation perturbations, such as brightness and contrast.
However, current certified training and robustness certification methods face
the challenge of over-parameterization, which hinders the tightness and
scalability due to the over-complicated neural networks. To this end, we first
analyze stability and variance of layers and neurons against input
perturbation, showing that certifiable robustness can be indicated by a
fundamental Unbiased and Smooth Neuron metric (USN). Based on USN, we introduce
a novel neural network pruning method that removes neurons with low USN and
retains those with high USN, thereby preserving model expressiveness without
over-parameterization. To further enhance this pruning process, we propose a
new Wasserstein distance loss to ensure that pruned neurons are more
concentrated across layers. We validate our approach through extensive
experiments on the challenging robust keypoint detection task, which involves
realistic brightness and contrast perturbations, demonstrating that our method
achieves superior robustness certification performance and efficiency compared
to baselines.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [50] [Lattica: A Decentralized Cross-NAT Communication Framework for Scalable AI Inference and Training](https://arxiv.org/abs/2510.00183)
*Ween Yang,Jason Liu,Suli Wang,Xinyuan Song,Lynn Ai,Eric Yang,Tianyu Shi*

Main category: cs.DC

TL;DR: The paper introduces Lattica, a decentralized framework optimized for distributed AI systems operating in heterogeneous and permissionless environments.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of cross-NAT communication and the limitations of monolithic systems in supporting modern distributed AI workloads.

Method: Lattica integrates a peer-to-peer NAT traversal suite, a decentralized data store using CRDTs, and a content discovery layer with DHT-based model synchronization.

Result: The framework provides a protocol stack that ensures resilient, sovereign, and scalable AI systems suitable for edge and collaborative learning environments.

Conclusion: Lattica resolves critical communication barriers in distributed AI systems, enabling effective deployment in decentralized edge and machine learning applications without reliance on centralized intermediaries.

Abstract: The rapid expansion of distributed Artificial Intelligence (AI) workloads
beyond centralized data centers creates a demand for new communication
substrates. These substrates must operate reliably in heterogeneous and
permissionless environments, where Network Address Translators (NATs) and
firewalls impose significant constraints. Existing solutions, however, are
either designed for controlled data center deployments or implemented as
monolithic systems that tightly couple machine learning logic with networking
code. To address these limitations, we present Lattica, a decentralized
cross-NAT communication framework designed to support distributed AI systems.
Lattica integrates three core components. First, it employs a robust suite of
NAT traversal mechanisms to establish a globally addressable peer-to-peer mesh.
Second, it provides a decentralized data store based on Conflict-free
Replicated Data Types (CRDTs), ensuring verifiable and eventually consistent
state replication. Third, it incorporates a content discovery layer that
leverages distributed hash tables (DHTs) together with an optimized RPC
protocol for efficient model synchronization. By integrating these components,
Lattica delivers a complete protocol stack for sovereign, resilient, and
scalable AI systems that operate independently of centralized intermediaries.
It is directly applicable to edge intelligence, collaborative reinforcement
learning, and other large-scale distributed machine learning scenarios.

</details>


### [51] [FlowMoE: A Scalable Pipeline Scheduling Framework for Distributed Mixture-of-Experts Training](https://arxiv.org/abs/2510.00207)
*Yunqi Gao,Bing Hu,Mahdi Boloursaz Mashhadi,A-Long Jin,Yanfeng Zhang,Pei Xiao,Rahim Tafazolli,Merouane Debbah*

Main category: cs.DC

TL;DR: FlowMoE is a framework designed to enhance the training efficiency of sparsely activated Mixture-of-Experts (MoE) models through optimized task scheduling and communication mechanisms.


<details>
  <summary>Details</summary>
Motivation: To reduce the computational costs and training inefficiencies in large language models using Mixture-of-Experts (MoE) by addressing overlooked operations like multi-head attention (MHA) computing, gating, and all-reduce communication.

Method: FlowMoE constructs a unified pipeline for scheduling various tasks (MHA, gating, etc.) and incorporates tensor chunk-based priority scheduling to overlap communication with computing tasks. It is implemented over PyTorch.

Result: FlowMoE achieves significant improvements in MoE training frameworks, reducing training time by 13%-57%, energy consumption by 10%-39%, and memory usage by 7%-32%, validated on real-world models and GPU clusters.

Conclusion: FlowMoE provides an adaptive and efficient framework for distributed MoE training, setting a benchmark for scalable and energy-efficient large language model training.

Abstract: The parameter size of modern large language models (LLMs) can be scaled up
via the sparsely-activated Mixture-of-Experts (MoE) technique to avoid
excessive increase of the computational costs. To further improve training
efficiency, pipelining computation and communication has become a promising
solution for distributed MoE training. However, existing work primarily focuses
on scheduling tasks within the MoE layer, such as expert computing and
all-to-all (A2A) communication, while neglecting other key operations including
multi-head attention (MHA) computing, gating, and all-reduce communication. In
this paper, we propose FlowMoE, a scalable framework for scheduling multi-type
task pipelines. First, FlowMoE constructs a unified pipeline to consistently
scheduling MHA computing, gating, expert computing, and A2A communication.
Second, FlowMoE introduces a tensor chunk-based priority scheduling mechanism
to overlap the all-reduce communication with all computing tasks. We implement
FlowMoE as an adaptive and generic framework atop PyTorch. Extensive
experiments with 675 typical MoE layers and four real-world MoE models across
two GPU clusters demonstrate that our proposed FlowMoE framework outperforms
state-of-the-art MoE training frameworks, reducing training time by 13%-57%,
energy consumption by 10%-39%, and memory usage by 7%-32%.

</details>


### [52] [BlockSDN-VC: A SDN-Based Virtual Coordinate-Enhanced Transaction Broadcast Framework for High-Performance Blockchains](https://arxiv.org/abs/2510.00306)
*Wenyang Jia,Jingjing Wang,Kai Lei*

Main category: cs.DC

TL;DR: BlockSDN-VC is a blockchain transaction broadcast protocol that improves propagation speed and consistency through an SDN-based centralized approach, outperforming existing methods in latency, convergence, and throughput.


<details>
  <summary>Details</summary>
Motivation: To address the need for a fast, reliable, and globally consistent transaction propagation mechanism in modern blockchains, which are hindered by latency and synchronization challenges in current virtual-coordinate methods.

Method: The proposed method, BlockSDN-VC, uses an SDN controller to centralize coordinate computation and forwarding control, enabling efficient propagation with minimal path stretch and quick responses to network changes like churn or congestion.

Result: BlockSDN-VC reduces median latency by up to 62% and achieves a fourfold faster convergence in simulations compared to current methods, with under 3% control-plane overhead. In real blockchain environments, it improves transaction throughput by 17% even under adversarial conditions.

Conclusion: BlockSDN-VC demonstrates significant performance gains in speed, efficiency, and throughput, making it a practical and impactful solution for current blockchain systems without requiring client modifications.

Abstract: Modern blockchains need fast, reliable propagation to balance security and
throughput. Virtual-coordinate methods speed dissemination but rely on slow
iterative updates, leaving nodes out of sync. We present BlockSDN-VC, a
transaction-broadcast protocol that centralises coordinate computation and
forwarding control in an SDN controller, delivering global consistency, minimal
path stretch and rapid response to churn or congestion. In geo-distributed
simulations, BlockSDN-VC cuts median latency by up to 62% and accelerates
convergence fourfold over state-of-the-art schemes with under 3% control-plane
overhead. In a real blockchain environment, BlockSDN-VC boosts
confirmed-transaction throughput by 17% under adversarial workloads, requiring
no modifications to existing clients.

</details>


### [53] [ThirstyFLOPS: Water Footprint Modeling and Analysis Toward Sustainable HPC Systems](https://arxiv.org/abs/2510.00471)
*Yankai Jiang,Raghavendra Kanakagiri,Rohan Basu Roy,Devesh Tiwari*

Main category: cs.DC

TL;DR: This paper introduces ThirstyFLOPS, a framework assessing water usage in HPC systems, addressing regional-specific water footprints and impacts.


<details>
  <summary>Details</summary>
Motivation: The rising water consumption of HPC systems due to cooling and energy production needs attention, especially with underexplored water footprints compared to carbon emissions.

Method: The paper develops ThirstyFLOPS, leveraging metrics such as Water Usage Effectiveness, Power Usage Effectiveness, and Energy Water Factor, and uses data from four HPC systems.

Result: Using systems like Marconi, Fugaku, Polaris, and Frontier, they highlight the effects of water scarcity and energy strategies—offering meaningful insights for HPC planning.

Conclusion: The study emphasizes creating sustainable, water-conscious HPC infrastructures by enhancing awareness of their environmental impact.

Abstract: High-performance computing (HPC) systems are becoming increasingly
water-intensive due to their reliance on water-based cooling and the energy
used in power generation. However, the water footprint of HPC remains
relatively underexplored-especially in contrast to the growing focus on carbon
emissions. In this paper, we present ThirstyFLOPS - a comprehensive water
footprint analysis framework for HPC systems. Our approach incorporates
region-specific metrics, including Water Usage Effectiveness, Power Usage
Effectiveness, and Energy Water Factor, to quantify water consumption using
real-world data. Using four representative HPC systems - Marconi, Fugaku,
Polaris, and Frontier - as examples, we provide implications for HPC system
planning and management. We explore the impact of regional water scarcity and
nuclear-based energy strategies on HPC sustainability. Our findings aim to
advance the development of water-aware, environmentally responsible computing
infrastructures.

</details>


### [54] [Towards Efficient VM Placement: A Two-Stage ACO-PSO Approach for Green Cloud Infrastructure](https://arxiv.org/abs/2510.00541)
*Ali M. Baydoun,Ahmed S. Zekri*

Main category: cs.DC

TL;DR: The paper proposes a Hybrid ACO-PSO (HAPSO) algorithm for energy-aware VM placement and migration in cloud datacenters, achieving significant energy savings and reduced SLA violations.


<details>
  <summary>Details</summary>
Motivation: Rising energy consumption in datacenters necessitates sustainable and efficient resource management strategies.

Method: The paper introduces a two-stage hybrid algorithm: (1) Ant Colony Optimization (ACO) for initial VM placement, and (2) Particle Swarm Optimization (PSO) for refining allocations and managing migrations. Innovations include sequential hybridization, system-informed initialization, heuristic-guided constraint handling, and a multi-objective fitness function.

Result: Simulations in CloudSimPlus show HAPSO reduces energy consumption by up to 25% and SLA violations by 18% compared to existing algorithms for large-scale workloads.

Conclusion: HAPSO effectively addresses the dynamic and multi-objective challenges of cloud resource management and demonstrates the potential of bio-inspired hybrid methods in optimizing energy-aware datacenter operations.

Abstract: Datacenters consume a growing share of energy, prompting the need for
sustainable resource management. This paper presents a Hybrid ACO-PSO (HAPSO)
algorithm for energy-aware virtual machine (VM) placement and migration in
green cloud datacenters. In the first stage, Ant Colony Optimization (ACO)
performs energy-efficient initial placement across physical hosts, ensuring
global feasibility. In the second stage, a discrete Particle Swarm Optimization
(PSO) refines allocations by migrating VMs from overloaded or underutilized
hosts. HAPSO introduces several innovations: sequential hybridization of
metaheuristics, system-informed particle initialization using ACO output,
heuristic-guided discretization for constraint handling, and a multi-objective
fitness function that minimizes active servers and resource wastage.
Implemented in CloudSimPlus, extensive simulations demonstrate that HAPSO
consistently outperforms classical heuristics (BFD, FFD), Unified Ant Colony
System (UACS), and ACO-only. Notably, HAPSO achieves up to 25% lower energy
consumption and 18% fewer SLA violations compared to UACS at large-scale
workloads, while sustaining stable cost and carbon emissions. These results
highlight the effectiveness of two-stage bio-inspired hybridization in
addressing the dynamic and multi-objective nature of cloud resource management.

</details>


### [55] [ElasWave: An Elastic-Native System for Scalable Hybrid-Parallel Training](https://arxiv.org/abs/2510.00606)
*Xueze Kang,Guangyu Xiang,Yuxin Wang,Hao Zhang,Yuchu Fang,Yuhang Zhou,Zhenheng Tang,Youhui Lv,Eliran Maman,Mark Wasserman,Alon Zameret,Zhipeng Bian,Shushu Chen,Zhiyou Yu,Jin Wang,Xiaoyu Wu,Yang Zheng,Chen Tian,Xiaowen Chu*

Main category: cs.DC

TL;DR: ElasWave introduces a fault-tolerant system for large-scale LLM training, improving throughput, recovery speed, and computational consistency compared to previous methods.


<details>
  <summary>Details</summary>
Motivation: Large-scale LLM pretraining involves huge computational resources and frequent failures, making elasticity and fault tolerance integral.

Method: ElasWave employs multi-dimensional scheduling, asynchronous parameter migration, dynamic communicator modification, and in-memory snapshots to ensure efficiency and fault tolerance.

Result: Throughput improved significantly compared to baselines, communicator recovery achieved rapid completion, MTTR decreased, and convergence deviation was reduced.

Conclusion: ElasWave successfully achieves fault tolerance and computational efficiency, addressing prior limitations in large-scale LLM pretraining frameworks.

Abstract: Large-scale LLM pretraining today spans $10^{5}$--$10^{6}$ accelerators,
making failures commonplace and elasticity no longer optional. We posit that an
elastic-native training system must simultaneously ensure (i) Parameter
Consistency, (ii) low Mean Time to Recovery (MTTR), (iii) high post-change
Throughput, and (iv) Computation Consistency. This objective set not has never
been jointly attained by prior work. To achieve these goals, we present
ElasWave, which provides per-step fault tolerance via multi-dimensional
scheduling across Graph, Dataflow, Frequency, and Random Number Generation.
ElasWave resizes and reshards micro-batch workloads while preserving the global
batch size and gradient scale; it performs online pipeline resharding with
asynchronous parameter migration, interleaving ZeRO partitions so recovery
reduces to disjoint rank-to-rank transfers. It further uses DVFS to absorb
pipeline bubbles and reshards RNG to keep consistent computations. A dynamic
communicator enables in-place communication group edits, while per-step
in-memory snapshots support online verification and redistribution. We
evaluated ElasWave on 96 NPUs and benchmarked against state-of-the-art
baselines: throughput improves by $1.35\times$ over ReCycle and $1.60\times$
over TorchFT; communicator recovery completes within one second (up to
$82\times/3.6\times$ faster than full/partial rebuilds); migration MTTR drops
by as much as $51\%$; and convergence deviation is reduced by approximately
$78\%$.

</details>


### [56] [Net-Zero 6G from Earth to Orbit: Sustainable Design of Integrated Terrestrial and Non-Terrestrial Networks](https://arxiv.org/abs/2510.00678)
*Muhammad Ali Jamshed,Malik Muhammad Saad,Muhammad Ahmed Mohsin,Dongkyun Kim,Octavia A. Dobre,Halim Yanikomeroglu,Lina Mohjazi*

Main category: cs.DC

TL;DR: The paper addresses energy challenges in integrating Terrestrial and Non-Terrestrial Networks for 6G, proposes AI-driven solutions for energy efficiency, and outlines key enabling technologies and research directions.


<details>
  <summary>Details</summary>
Motivation: The need to seamlessly integrate Terrestrial and Non-Terrestrial Networks to achieve 6G's ubiquitous connectivity while addressing energy efficiency and Net-Zero targets.

Method: The paper provides a comprehensive design overview, uses AI-based adaptable solutions for varying deployment scenarios, and investigates key technologies for energy-efficient operations.

Result: The analysis identified key enabling technologies and AI as pivotal for enhancing energy efficiency in integrated TN and NTN systems under varying scenarios.

Conclusion: Achieving Net-Zero energy targets in TN and NTN integration is feasible through strategic technologies and AI, paving the way for sustainable and efficient 6G networks.

Abstract: The integration of Terrestrial Networks (TN) and Non-Terrestrial Networks
(NTN) plays a crucial role in bridging the digital divide and enabling Sixth
Generation (6G) and beyond to achieve truly ubiquitous connectivity. However,
combining TN and NTN introduces significant energy challenges due to the
diverse characteristics and operational environments of these systems. In this
paper, we present for the first time a comprehensive overview of the design
challenges associated with achieving Net-Zero energy targets in integrated TN
and NTN systems. We outline a set of key enabling technologies that can support
the energy demands of such networks while aligning with Net-Zero objectives. To
enhance the Energy Efficiency (EE) of integrated TN and NTN systems, we provide
a use case analysis that leverages Artificial Intelligence (AI) to deliver
adaptable solutions across diverse deployment scenarios. Finally, we highlight
promising research directions that can guide the sustainable evolution of
integrated TN and NTN.

</details>


### [57] [Decentralized and Self-adaptive Core Maintenance on Temporal Graphs](https://arxiv.org/abs/2510.00758)
*Davide Rucci,Emanuele Carlini,Patrizio Dazzi,Hanna Kavalionak,Matteo Mordacchini*

Main category: cs.DC

TL;DR: The paper presents a new decentralized, incremental algorithm for efficient core decomposition in temporal networks, reducing communication and computational overhead while maintaining scalability and precision.


<details>
  <summary>Details</summary>
Motivation: To improve scalability, adaptability, and efficiency in analyzing network topology and uncovering patterns in temporal and homogeneous graph-like data.

Method: Introduced a decentralized algorithm that incrementally computes core decomposition in temporal graphs, leveraging previously calculated coreness values to minimize node activation and message exchanges during dynamic network updates.

Result: The proposed method outperforms a state-of-the-art approach in experiments on large real-world networks, excelling in active node reduction, lower communication overhead, and faster convergence.

Conclusion: The method provides a scalable solution for dynamic networks with minimal precision trade-offs, addressing challenges in temporal graph analysis effectively.

Abstract: Key graph-based problems play a central role in understanding network
topology and uncovering patterns of similarity in homogeneous and temporal
data. Such patterns can be revealed by analyzing communities formed by nodes,
which in turn can be effectively modeled through temporal $k$-cores. This paper
introduces a novel decentralized and incremental algorithm for computing the
core decomposition of temporal networks. Decentralized solutions leverage the
ability of network nodes to communicate and coordinate locally, addressing
complex problems in a scalable, adaptive, and timely manner. By leveraging
previously computed coreness values, our approach significantly reduces the
activation of nodes and the volume of message exchanges when the network
changes over time. This enables scalability with only a minimal trade-off in
precision. Experimental evaluations on large real-world networks under varying
levels of dynamism demonstrate the efficiency of our solution compared to a
state-of-the-art approach, particularly in terms of active nodes, communication
overhead, and convergence speed.

</details>


### [58] [CGSim: A Simulation Framework for Large Scale Distributed Computing Environment](https://arxiv.org/abs/2510.00822)
*Sairam Sri Vatsavai,Raees Khan,Kuan-Chieh Hsu,Ozgur O. Kilic,Paul Nilsson,Tatiana Korchuganova,David K. Park,Sankha Dutta,Yihui Ren,Joseph Boudreau,Tasnuva Chowdhury,Shengyu Feng,Jaehyung Kim,Scott Klasky,Tadashi Maeno,Verena Ingrid Martinez,Norbert Podhorszki,Frédéric Suter,Wei Yang,Yiming Yang,Shinjae Yoo,Alexei Klimentov,Adolfy Hoisie*

Main category: cs.DC

TL;DR: CGSim is a simulation framework designed for large-scale distributed computing environments like the Worldwide LHC Computing Grid (WLCG), enabling scalable, accurate simulations with features like modular plugins, real-time visualization, and dataset generation for AI.


<details>
  <summary>Details</summary>
Motivation: Existing simulation tools for WLCG suffer from limitations in scalability, flexibility, real-time monitoring, and the generation of useful datasets for machine learning applications.

Method: The authors developed CGSim, based on the SimGrid framework, incorporating features like modular plugins for policies testing, interactive dashboards, and AI-compatible dataset generation. It is evaluated with ATLAS PanDA workloads.

Result: CGSim showed significant calibration accuracy improvements, near-linear scaling for multi-site simulations, and 6x performance enhancements with distributed workloads compared to single-site execution.

Conclusion: CGSim effectively addresses limitations in current tools, enabling simulations of WLCG-scale infrastructures with improved performance and scalability within practical time constraints.

Abstract: Large-scale distributed computing infrastructures such as the Worldwide LHC
Computing Grid (WLCG) require comprehensive simulation tools for evaluating
performance, testing new algorithms, and optimizing resource allocation
strategies. However, existing simulators suffer from limited scalability,
hardwired algorithms, lack of real-time monitoring, and inability to generate
datasets suitable for modern machine learning approaches. We present CGSim, a
simulation framework for large-scale distributed computing environments that
addresses these limitations. Built upon the validated SimGrid simulation
framework, CGSim provides high-level abstractions for modeling heterogeneous
grid environments while maintaining accuracy and scalability. Key features
include a modular plugin mechanism for testing custom workflow scheduling and
data movement policies, interactive real-time visualization dashboards, and
automatic generation of event-level datasets suitable for AI-assisted
performance modeling. We demonstrate CGSim's capabilities through a
comprehensive evaluation using production ATLAS PanDA workloads, showing
significant calibration accuracy improvements across WLCG computing sites.
Scalability experiments show near-linear scaling for multi-site simulations,
with distributed workloads achieving 6x better performance compared to
single-site execution. The framework enables researchers to simulate WLCG-scale
infrastructures with hundreds of sites and thousands of concurrent jobs within
practical time budget constraints on commodity hardware.

</details>


### [59] [Data Management System Analysis for Distributed Computing Workloads](https://arxiv.org/abs/2510.00828)
*Kuan-Chieh Hsu,Sairam Sri Vatsavai,Ozgur O. Kilic,Tatiana Korchuganova,Paul Nilsson,Sankha Dutta,Yihui Ren,David K. Park,Joseph Boudreau,Tasnuva Chowdhury,Shengyu Feng,Raees Khan,Jaehyung Kim,Scott Klasky,Tadashi Maeno,Verena Ingrid Martinez Outschoorn,Norbert Podhorszki,Frédéric Suter,Wei Yang,Yiming Yang,Shinjae Yoo,Alexei Klimentov,Adolfy Hoisie*

Main category: cs.DC

TL;DR: The paper addresses inefficiencies in ATLAS's PanDA and Rucio systems and proposes enhancements for better coordination and resource utilization.


<details>
  <summary>Details</summary>
Motivation: The paper aims to tackle inefficiencies arising from the lack of shared performance awareness and coordinated strategies in the PanDA and Rucio systems.

Method: A metadata-matching algorithm was developed to link PanDA jobs and Rucio datasets, providing an interconnected view for analyzing and mitigating inefficiencies.

Result: Anomalous transfer patterns violating data-centric principles were identified, leading to strategies for improved coordination and resource efficiency.

Conclusion: Enhanced collaboration between PanDA and Rucio can optimize resource use, decrease data movement, and strengthen system resilience.

Abstract: Large-scale international collaborations such as ATLAS rely on globally
distributed workflows and data management to process, move, and store vast
volumes of data. ATLAS's Production and Distributed Analysis (PanDA) workflow
system and the Rucio data management system are each highly optimized for their
respective design goals. However, operating them together at global scale
exposes systemic inefficiencies, including underutilized resources, redundant
or unnecessary transfers, and altered error distributions. Moreover, PanDA and
Rucio currently lack shared performance awareness and coordinated, adaptive
strategies.
  This work charts a path toward co-optimizing the two systems by diagnosing
data-management pitfalls and prioritizing end-to-end improvements. With the
observation of spatially and temporally imbalanced transfer activities, we
develop a metadata-matching algorithm that links PanDA jobs and Rucio datasets
at the file level, yielding a complete, fine-grained view of data access and
movement. Using this linkage, we identify anomalous transfer patterns that
violate PanDA's data-centric job-allocation principle. We then outline
mitigation strategies for these patterns and highlight opportunities for
tighter PanDA-Rucio coordination to improve resource utilization, reduce
unnecessary data movement, and enhance overall system resilience.

</details>


### [60] [Towards Verifiable Federated Unlearning: Framework, Challenges, and The Road Ahead](https://arxiv.org/abs/2510.00833)
*Thanh Linh Nguyen,Marcela Tuler de Oliveira,An Braeken,Aaron Yi Ding,Quoc-Viet Pham*

Main category: cs.DC

TL;DR: The paper proposes veriFUL, a framework aiming to ensure that federated unlearning (FUL) processes are reliable and provable, addressing the lack of verification mechanisms for data removal in existing FUL practices.


<details>
  <summary>Details</summary>
Motivation: Current federated unlearning methods lack a reliable way for clients to verify whether their data's influence has been completely removed. This undermines trust and compliance with privacy regulations.

Method: The authors introduce veriFUL, a reference framework for verifiable federated unlearning that formalizes key elements such as verification entities, goals, and metrics. They also consolidate existing research and contribute new concepts.

Result: The veriFUL framework defines methods and metrics for verifiable unlearning, offering new insights into ensuring trustworthiness in FUL. It explores the potential of its application in highly regulated sectors such as healthcare.

Conclusion: Verifiable federated unlearning is crucial for maintaining trust and meeting privacy requirements. The proposed veriFUL framework is a step forward in enhancing the reliability and acceptance of FUL processes.

Abstract: Federated unlearning (FUL) enables removing the data influence from the model
trained across distributed clients, upholding the right to be forgotten as
mandated by privacy regulations. FUL facilitates a value exchange where clients
gain privacy-preserving control over their data contributions, while service
providers leverage decentralized computing and data freshness. However, this
entire proposition is undermined because clients have no reliable way to verify
that their data influence has been provably removed, as current metrics and
simple notifications offer insufficient assurance. We envision unlearning
verification becoming a pivotal and trust-by-design part of the FUL life-cycle
development, essential for highly regulated and data-sensitive services and
applications like healthcare. This article introduces veriFUL, a reference
framework for verifiable FUL that formalizes verification entities, goals,
approaches, and metrics. Specifically, we consolidate existing efforts and
contribute new insights, concepts, and metrics to this domain. Finally, we
highlight research challenges and identify potential applications and
developments for verifiable FUL and veriFUL.

</details>


### [61] [An Efficient, Reliable and Observable Collective Communication Library in Large-scale GPU Training Clusters](https://arxiv.org/abs/2510.00991)
*Ziteng Chen,Xiaohe Hu,Menghao Zhang,Yanmin Jia,Yan Zhang,Mingjun Zhang,Da Liu,Fangzheng Jiao,Jun Chen,He Liu,Aohan Zeng,Shuaixing Duan,Ruya Gu,Yang Jing,Bowen Han,Jiahao Cao,Wei Chen,Wenqi Xie,Jinlong Hou,Yuan Cheng,Bohua Xu,Mingwei Xu,Chunming Hu*

Main category: cs.DC

TL;DR: ICCL is a proposed solution to enhance GPU communication efficiency, reliability, and observability in large-scale LLM training clusters, outperforming the existing NCCL library.


<details>
  <summary>Details</summary>
Motivation: Current collective communication library NCCL struggles with inefficient P2P communication, weak tolerance to NIC port failures, and limited monitoring capabilities in large-scale GPU setups.

Method: ICCL offloads P2P communication tasks to CPU threads, eliminates redundant memory copies, incorporates primary-backup QP mechanisms, and introduces a window-based network anomaly monitor.

Result: ICCL demonstrates a 23.4% and 28.5% improvement in P2P throughput and latency respectively, alongside a 6.02% increment in overall training throughput.

Conclusion: ICCL enhances communication efficiency, reliability, and observability, providing insightful operational experiences for large-scale GPU clusters in production-level settings for LLM training.

Abstract: Large-scale LLM training requires collective communication libraries to
exchange data among distributed GPUs. As a company dedicated to building and
operating large-scale GPU training clusters, we encounter several challenges
when using NCCL in production, including 1) limited efficiency with costly and
cumbersome P2P communication, 2) poor tolerance to frequent RNIC port failures,
and 3) insufficient observability of transient collective communication
anomalies. To address these issues, we propose ICCL, an efficient, reliable,
and observable collective communication library in large-scale GPU training
clusters. ICCL offloads the P2P communication from GPU kernels to CPU threads
for minimal SM consumption, and removes the redundant memory copies irrelevant
to the actual communicating process. ICCL also introduces a primary-backup QP
mechanism to tolerate frequent NIC port failures, and designs a window-based
monitor to observe network anomalies at O(us) level. We open-source ICCL and
deploy it in production training clusters for several months, with results
showing that compared to NCCL, ICCL achieves a 23.4%/28.5% improvement in P2P
throughput/latency as well as a 6.02% increase in training throughput. We also
share the operating experience of ICCL in large-scale clusters, hoping to give
the communities more insights on production-level collective communication
libraries in LLM training.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [62] [Methodological Framework for Quantifying Semantic Test Coverage in RAG Systems](https://arxiv.org/abs/2510.00001)
*Noah Broestl,Adel Nasser Abdalla,Rajprakash Bale,Hersh Gupta,Max Struever*

Main category: cs.LG

TL;DR: The paper introduces a methodology for quantifying the semantic coverage of test questions in RAG systems, addressing gaps in current evaluation practices.


<details>
  <summary>Details</summary>
Motivation: To address the deficiency in systematic methods to ensure the test sets adequately cover the knowledge base in RAG systems.

Method: It applies vector embeddings and clustering algorithms to embed documents and test questions into a unified vector space, enabling calculations of coverage metrics and refining test sets through outlier detection.

Result: Experimental results confirm the framework can quantify test coverage, identify content gaps, and suggest new test questions to enhance evaluation.

Conclusion: The study provides tools for RAG developers to improve test suites, enhancing reliability and quality of the systems.

Abstract: Reliably determining the performance of Retrieval-Augmented Generation (RAG)
systems depends on comprehensive test questions. While a proliferation of
evaluation frameworks for LLM-powered applications exists, current practices
lack a systematic method to ensure these test sets adequately cover the
underlying knowledge base, leaving developers with significant blind spots. To
address this, we present a novel, applied methodology to quantify the semantic
coverage of RAG test questions against their underlying documents. Our approach
leverages existing technologies, including vector embeddings and clustering
algorithms, to create a practical framework for validating test
comprehensiveness. Our methodology embeds document chunks and test questions
into a unified vector space, enabling the calculation of multiple coverage
metrics: basic proximity, content-weighted coverage, and multi-topic question
coverage. Furthermore, we incorporate outlier detection to filter irrelevant
questions, allowing for the refinement of test sets. Experimental evidence from
two distinct use cases demonstrates that our framework effectively quantifies
test coverage, identifies specific content areas with inadequate
representation, and provides concrete recommendations for generating new,
high-value test questions. This work provides RAG developers with essential
tools to build more robust test suites, thereby improving system reliability
and extending to applications such as identifying misaligned documents.

</details>


### [63] [Thoughtbubbles: an Unsupervised Method for Parallel Thinking in Latent Space](https://arxiv.org/abs/2510.00219)
*Houjun Liu,Shikhar Murty,Christopher D. Manning,Róbert Csordás*

Main category: cs.LG

TL;DR: This paper introduces Thoughtbubbles, a transformer model that performs parallel adaptive computation during pretraining, outperforming standard LMs and non-adaptive methods.


<details>
  <summary>Details</summary>
Motivation: Current inference approaches in transformers rely on explicit chain-of-thought tokens and are limited to serial natural-language verbalization.

Method: Thoughtbubbles employs latent space computation by learning to fork or delete residual streams during pretraining using only language modeling loss.

Result: Thoughtbubbles achieved superior performance on OpenWebText and peS2o perplexity as well as zero-shot evaluations like HellaSwag and LAMBADA across different parameter scales.

Conclusion: The study demonstrates that adaptive computation can be learned during pretraining, unifying train and test-time behavior in reasoning models.

Abstract: Current approaches for scaling inference-time compute in transformers rely on
training them to emit explicit chain-of-thought tokens before producing an
answer. While these methods are powerful, they are limited because they cannot
be applied during pretraining and are limited to only serially-generated,
natural-language verbalization to scale inference-time compute. In this work,
we propose Thoughtbubbles, a transformer variant that natively performs
parallel adaptive computation in latent space by learning to fork or delete
residual streams. Thus, tokens that require a large amount of computation can
form a "bubble" of cloned residuals in the middle of the network for additional
thinking. Crucially, this behavior is learned during pretraining with only
language modeling loss. Thoughtbubbles outperforms both standard decoder LMs as
well as non-adaptive parallel computation approaches on OpenWebText and peS2o
perplexity and in zero-shot evaluations such as HellaSwag and LAMBADA after
pretraining across 150M to 772M parameter scales. The implicit nature of our
method enables adaptive computation to be learned starting at pretraining time,
paving the way to unify train and test-time behavior for reasoning models.

</details>


### [64] [Learning Inter-Atomic Potentials without Explicit Equivariance](https://arxiv.org/abs/2510.00027)
*Ahmed A. Elhag,Arun Raja,Alex Morehead,Samuel M. Blau,Garrett M. Morris,Michael M. Bronstein*

Main category: cs.LG

TL;DR: The paper presents TransIP, a Transformer-based model for interatomic potentials. It achieves symmetry compliance without using hard-wired neural architecture constraints, offering improved efficiency and scalability.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of state-of-the-art equivariant neural network models for MLIPs, such as reduced flexibility, computational efficiency, and scalability when enforcing symmetries.

Method: A generic non-equivariant Transformer model is guided to learn SO(3)-equivariance by optimizing embedding space representations, trained on the diverse OMol25 dataset.

Result: TransIP matches the performance of traditional equivariant baselines and outperforms data augmentation approaches by 40–60% on OMol25, across varying dataset sizes.

Conclusion: Learned equivariance is a promising alternative to traditional equivariant architectures or augmentation-based MLIP models, benefiting molecular simulations.

Abstract: Accurate and scalable machine-learned inter-atomic potentials (MLIPs) are
essential for molecular simulations ranging from drug discovery to new material
design. Current state-of-the-art models enforce roto-translational symmetries
through equivariant neural network architectures, a hard-wired inductive bias
that can often lead to reduced flexibility, computational efficiency, and
scalability. In this work, we introduce TransIP: Transformer-based Inter-Atomic
Potentials, a novel training paradigm for interatomic potentials achieving
symmetry compliance without explicit architectural constraints. Our approach
guides a generic non-equivariant Transformer-based model to learn
SO(3)-equivariance by optimizing its representations in the embedding space.
Trained on the recent Open Molecules (OMol25) collection, a large and diverse
molecular dataset built specifically for MLIPs and covering different types of
molecules (including small organics, biomolecular fragments, and
electrolyte-like species), TransIP attains comparable performance in
machine-learning force fields versus state-of-the-art equivariant baselines.
Further, compared to a data augmentation baseline, TransIP achieves 40% to 60%
improvement in performance across varying OMol25 dataset sizes. More broadly,
our work shows that learned equivariance can be a powerful and efficient
alternative to equivariant or augmentation-based MLIP models.

</details>


### [65] [Combining Large Language Models and Gradient-Free Optimization for Automatic Control Policy Synthesis](https://arxiv.org/abs/2510.00373)
*Carlo Bosio,Matteo Guarrera,Alberto Sangiovanni-Vincentelli,Mark W. Mueller*

Main category: cs.LG

TL;DR: This paper introduces a hybrid approach combining symbolic program synthesis with numerical optimization to improve the efficiency and performance of control policy generation by large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: LLMs show promise in generating symbolic control policies but struggle with the inefficiency of separating structural synthesis and parameter optimization, necessitating a better approach.

Method: The method decouples structural synthesis from parameter optimization by introducing a local optimization layer. The LLM iteratively refines program structure while a separate optimization loop adjusts numerical parameters to improve task performance.

Result: The proposed method demonstrates higher returns and better sample efficiency on control tasks than purely LLM-guided searches, validating its effectiveness.

Conclusion: Integrating symbolic synthesis with numerical optimization leads to interpretable and high-performing control policies, effectively combining the strengths of LLM-guided design and classical control tuning.

Abstract: Large Language models (LLMs) have shown promise as generators of symbolic
control policies, producing interpretable program-like representations through
iterative search. However, these models are not capable of separating the
functional structure of a policy from the numerical values it is parametrized
by, thus making the search process slow and inefficient. We propose a hybrid
approach that decouples structural synthesis from parameter optimization by
introducing an additional optimization layer for local parameter search. In our
method, the numerical parameters of LLM-generated programs are extracted and
optimized numerically to maximize task performance. With this integration, an
LLM iterates over the functional structure of programs, while a separate
optimization loop is used to find a locally optimal set of parameters
accompanying candidate programs. We evaluate our method on a set of control
tasks, showing that it achieves higher returns and improved sample efficiency
compared to purely LLM-guided search. We show that combining symbolic program
synthesis with numerical optimization yields interpretable yet high-performing
policies, bridging the gap between language-model-guided design and classical
control tuning. Our code is available at
https://sites.google.com/berkeley.edu/colmo.

</details>


### [66] [Rethinking RoPE Scaling in Quantized LLM: Theory, Outlier, and Channel-Band Analysis with Weight Rescaling](https://arxiv.org/abs/2510.00028)
*Ye Qiao,Haocheng Xu,Xiaofan Zhang,Sitao Huang*

Main category: cs.LG

TL;DR: The paper deals with improving the compatibility of large language models with long-context inputs and quantization techniques. A new method called Q-ROAR is proposed to stabilize performance without retraining or additional deployment overhead.


<details>
  <summary>Details</summary>
Motivation: Extend the context window of large language models and address the challenges that arise when combining RoPE position interpolation with post-training quantization.

Method: Analyze the impact of combining RoPE interpolation and quantization, propose diagnostics (interpolation pressure and tail-inflation ratios), and introduce Q-ROAR, a method that adjusts weight scaling per frequency band.

Result: Q-ROAR improves model perplexity on long-context tasks by more than 14% while retaining short-context performance and inference efficiency.

Conclusion: Q-ROAR effectively stabilizes long-context support in quantized language models without requiring retraining, architectural changes, or added deployment complexity.

Abstract: Extending the context window support of large language models (LLMs) is
crucial for tasks with long-distance dependencies. RoPE-based interpolation and
extrapolation methods, such as linear scaling and frequency-aware schemes,
enable longer input length support without retraining, while post-training
quantization (PTQ) makes deployment practical. However, we show that combining
RoPE position interpolation (PI) with PTQ degrades accuracy due to coupled
effects including long-context aliasing, dynamic-range dilation, anisotropy
from axis-aligned quantizers vs. rotated RoPE pairs, and outlier shifting that
produces position-dependent logit noise. We provide, to the best of our
knowledge, the first systematic analysis of the PI+PTQ approach and introduce
two practical diagnostics: interpolation pressure (per-band sensitivity to
phase scaling) and tail-inflation ratios (outlier shift from short to long
contexts). Following the analysis results, we propose Q-ROAR (Quantization,
RoPE-interpolation, and Outlier Aware Rescaling), a weight-only,
interpolation-aware stabilization of PI for quantized LLMs. Q-ROAR groups RoPE
dimensions into a small number of frequency bands and performs a lightweight
search over per-band scales for Key and Query weights (with an optional
symmetric variant to preserve logit scale). The search is guided by our
diagnostics and uses a tiny long-context development dataset, requiring no
fine-tuning to the model, no architecture or kernel changes, and no additional
deployment overhead. Empirically, Q-ROAR reduces the model's perplexity on
long-context workloads by more than 14%, while preserving short-context
performance, inference throughput, and compatibility with existing LLM system
stacks.

</details>


### [67] [DexBench: Benchmarking LLMs for Personalized Decision Making in Diabetes Management](https://arxiv.org/abs/2510.00038)
*Maria Ana Cardei,Josephine Lamp,Mark Derdzinski,Karan Bhatia*

Main category: cs.LG

TL;DR: DexBench is a benchmark to evaluate large language models (LLMs) on real-world decision-making tasks relevant to diabetes management.


<details>
  <summary>Details</summary>
Motivation: To develop a benchmark that addresses the lack of patient-focused AI evaluation metrics in diabetes care solutions.

Method: Created a dataset with one month of continuous glucose monitor data and behavioral logs from 15,000 individuals, and generated 360,600 contextual questions across 7 task categories.

Result: Evaluated 8 LLMs on 5 metrics (accuracy, groundedness, safety, clarity, actionability), finding substantial variability in their performance across tasks and metrics.

Conclusion: DexBench establishes a comprehensive framework to refine AI solutions in diabetes care, focusing on their reliability, safety, and effectiveness.

Abstract: We present DexBench, the first benchmark designed to evaluate large language
model (LLM) performance across real-world decision-making tasks faced by
individuals managing diabetes in their daily lives. Unlike prior health
benchmarks that are either generic, clinician-facing or focused on clinical
tasks (e.g., diagnosis, triage), DexBench introduces a comprehensive evaluation
framework tailored to the unique challenges of prototyping patient-facing AI
solutions in diabetes, glucose management, metabolic health and related
domains. Our benchmark encompasses 7 distinct task categories, reflecting the
breadth of real-world questions individuals with diabetes ask, including basic
glucose interpretation, educational queries, behavioral associations, advanced
decision making and long term planning. Towards this end, we compile a rich
dataset comprising one month of time-series data encompassing glucose traces
and metrics from continuous glucose monitors (CGMs) and behavioral logs (e.g.,
eating and activity patterns) from 15,000 individuals across three different
diabetes populations (type 1, type 2, pre-diabetes/general health and
wellness). Using this data, we generate a total of 360,600 personalized,
contextual questions across the 7 tasks. We evaluate model performance on these
tasks across 5 metrics: accuracy, groundedness, safety, clarity and
actionability. Our analysis of 8 recent LLMs reveals substantial variability
across tasks and metrics; no single model consistently outperforms others
across all dimensions. By establishing this benchmark, we aim to advance the
reliability, safety, effectiveness and practical utility of AI solutions in
diabetes care.

</details>


### [68] [Physics-Informed Extreme Learning Machine (PIELM) for Tunnelling-Induced Soil-Pile Interactions](https://arxiv.org/abs/2510.00698)
*Fu-Chen Guo,Pei-Zhi Zhuang,Fei Ren,Hong-Ya Yue,He Yang*

Main category: cs.LG

TL;DR: This paper introduces a physics-informed machine learning framework for modeling soil-pile interactions during tunneling.


<details>
  <summary>Details</summary>
Motivation: To enhance the analysis and safety assessment of tunneling-induced soil-pile interactions using a physics-informed framework.

Method: The study combines physics (ODE formulation) and data (extreme learning machine network) within a Physics-Informed Extreme Learning Machine framework.

Result: The method was validated using boundary element and finite difference methods, with monitored data showing the significance of pile deflection gradient locations.

Conclusion: The framework holds great potential for real-time geotechnical monitoring and intelligent early-warning systems in tunneling applications.

Abstract: Physics-informed machine learning has been a promising data-driven and
physics-informed approach in geotechnical engineering. This study proposes a
physics-informed extreme learning machine (PIELM) framework for analyzing
tunneling-induced soil-pile interactions. The pile foundation is modeled as an
Euler-Bernoulli beam, and the surrounding soil is modeled as a Pasternak
foundation. The soil-pile interaction is formulated into a fourth-order
ordinary differential equation (ODE) that constitutes the physics-informed
component, while measured data are incorporated into PIELM as the data-driven
component. Combining physics and data yields a loss vector of the extreme
learning machine (ELM) network, which is trained within 1 second by the least
squares method. After validating the PIELM approach by the boundary element
method (BEM) and finite difference method (FDM), parametric studies are carried
out to examine the effects of ELM network architecture, data monitoring
locations and numbers on the performance of PIELM. The results indicate that
monitored data should be placed at positions where the gradients of pile
deflections are significant, such as at the pile tip/top and near tunneling
zones. Two application examples highlight the critical role of physics-informed
and data-driven approach for tunnelling-induced soil-pile interactions. The
proposed approach shows great potential for real-time monitoring and safety
assessment of pile foundations, and benefits for intelligent early-warning
systems in geotechnical engineering.

</details>


### [69] [Linear Regression in p-adic metric spaces](https://arxiv.org/abs/2510.00043)
*Gregory D. Baker,Scott McCallum,Dirk Pattinson*

Main category: cs.LG

TL;DR: The paper introduces a theoretical framework for machine learning using p-adic metrics, which naturally respect hierarchical and discrete data structures, contrasting traditional Euclidean approaches.


<details>
  <summary>Details</summary>
Motivation: Traditional machine learning methods relying on Euclidean metrics fail to capture the branching, discrete nature of hierarchical data, prompting the need for an alternative foundational approach.

Method: The authors propose using p-adic metric spaces for machine learning, theoretically proving key results about minimizing distances and constructing polynomials in this space.

Result: The authors demonstrate that p-adic regression ensures closer alignment with hierarchical data properties and provide practical applications in natural language processing, including hierarchical taxonomy analysis and grammatical modeling.

Conclusion: P-adic metrics highlight a fundamentally better approach for handling hierarchical data structures, proving their utility in both theory and machine learning practice.

Abstract: Many real-world machine learning problems involve inherently hierarchical
data, yet traditional approaches rely on Euclidean metrics that fail to capture
the discrete, branching nature of hierarchical relationships. We present a
theoretical foundation for machine learning in p-adic metric spaces, which
naturally respect hierarchical structure. Our main result proves that an
n-dimensional plane minimizing the p-adic sum of distances to points in a
dataset must pass through at least n + 1 of those points -- a striking contrast
to Euclidean regression that highlights how p-adic metrics better align with
the discrete nature of hierarchical data. As a corollary, a polynomial of
degree n constructed to minimise the p-adic sum of residuals will pass through
at least n + 1 points. As a further corollary, a polynomial of degree n
approximating a higher degree polynomial at a finite number of points will
yield a difference polynomial that has distinct rational roots. We demonstrate
the practical significance of this result through two applications in natural
language processing: analyzing hierarchical taxonomies and modeling grammatical
morphology. These results suggest that p-adic metrics may be fundamental to
properly handling hierarchical data structures in machine learning. In
hierarchical data, interpolation between points often makes less sense than
selecting actual observed points as representatives.

</details>


### [70] [Federated Learning Meets LLMs: Feature Extraction From Heterogeneous Clients](https://arxiv.org/abs/2510.00065)
*Abdelrhman Gaber,Hassan Abd-Eltawab,Youssif Abuzied,Muhammad ElMahdy,Tamer ElBatt*

Main category: cs.LG

TL;DR: The paper introduces a framework for federated learning using large language models (LLMs) as universal feature extractors to overcome schema divergence among clients.


<details>
  <summary>Details</summary>
Motivation: To tackle the challenge of divergent schemas and incompatible feature spaces in federated learning, especially in privacy-sensitive domains like healthcare and finance.

Method: FedLLM-Align uses large language models to serialize tabular data into text and extract semantic embeddings, enabling federated model training under the standard FedAvg protocol.

Result: The framework improved F1-score by up to 0.25 and reduced communication cost by 65% while performing robustly under extreme schema divergence.

Conclusion: FedLLM-Align is an effective solution for privacy-preserving and communication-efficient federated learning in heterogeneous environments, outperforming traditional methods.

Abstract: Federated learning (FL) enables collaborative model training without sharing
raw data, making it attractive for privacy-sensitive domains such as
healthcare, finance, and IoT. A major obstacle, however, is the heterogeneity
of tabular data across clients, where divergent schemas and incompatible
feature spaces prevent straightforward aggregation. To address this challenge,
we propose FedLLM-Align, a federated framework that leverages pre-trained large
language models (LLMs) as universal feature extractors. Tabular records are
serialized into text, and embeddings from models such as DistilBERT, ALBERT,
RoBERTa, and ClinicalBERT provide semantically aligned representations that
support lightweight local classifiers under the standard FedAvg protocol. This
approach removes the need for manual schema harmonization while preserving
privacy, since raw data remain strictly local. We evaluate FedLLM-Align on
coronary heart disease prediction using partitioned Framingham datasets with
simulated schema divergence. Across all client settings and LLM backbones, our
method consistently outperforms state-of-the-art baselines, achieving up to
+0.25 improvement in F1-score and a 65% reduction in communication cost. Stress
testing under extreme schema divergence further demonstrates graceful
degradation, unlike traditional methods that collapse entirely. These results
establish FedLLM-Align as a robust, privacy-preserving, and
communication-efficient solution for federated learning in heterogeneous
environments.

</details>


### [71] [Random Feature Spiking Neural Networks](https://arxiv.org/abs/2510.01012)
*Maximilian Gollwitzer,Felix Dietrich*

Main category: cs.LG

TL;DR: This paper introduces S-SWIM, a novel training algorithm for Spiking Neural Networks (SNNs), inspired by Random Feature Methods. The method avoids approximating spike function gradients, ensuring efficiency and effectiveness.


<details>
  <summary>Details</summary>
Motivation: Train SNNs effectively to harness their potential for energy-efficient machine learning models despite challenges like non-differentiability and sparsity.

Method: Adapts Random Feature Methods for Spike Response Model SNNs and proposes the S-SWIM algorithm, which avoids gradient approximation in training.

Result: S-SWIM demonstrates high accuracies in time-series forecasting and serves as a strong initialization strategy for further gradient-based training, outperforming random weight sampling.

Conclusion: S-SWIM is a fast, interpretable, and high-performance approach for training SNNs, offering practical benefits over conventional methods.

Abstract: Spiking Neural Networks (SNNs) as Machine Learning (ML) models have recently
received a lot of attention as a potentially more energy-efficient alternative
to conventional Artificial Neural Networks. The non-differentiability and
sparsity of the spiking mechanism can make these models very difficult to train
with algorithms based on propagating gradients through the spiking
non-linearity. We address this problem by adapting the paradigm of Random
Feature Methods (RFMs) from Artificial Neural Networks (ANNs) to Spike Response
Model (SRM) SNNs. This approach allows training of SNNs without approximation
of the spike function gradient. Concretely, we propose a novel data-driven,
fast, high-performance, and interpretable algorithm for end-to-end training of
SNNs inspired by the SWIM algorithm for RFM-ANNs, which we coin S-SWIM. We
provide a thorough theoretical discussion and supplementary numerical
experiments showing that S-SWIM can reach high accuracies on time series
forecasting as a standalone strategy and serve as an effective initialisation
strategy before gradient-based training. Additional ablation studies show that
our proposed method performs better than random sampling of network weights.

</details>


### [72] [Adaptive and Resource-efficient Agentic AI Systems for Mobile and Embedded Devices: A Survey](https://arxiv.org/abs/2510.00078)
*Sicong Liu,Weiye Wu,Xiangrui Xu,Teng Li,Bowen Pang,Bin Guo,Zhiwen Yu*

Main category: cs.LG

TL;DR: The paper surveys resource-efficient AI systems that integrate foundation models (FMs) for adaptive and autonomous agent performance under real-world constraints.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the dual shift where foundation models unify fragmented AI architectures while the concept of AI agents evolves into systems capable of autonomy and self-reflection.

Method: The paper systematically characterizes enabling techniques for resource-efficient AI systems, including elastic inference, dynamic multimodal integration, and test-time adaptation, identifying key methods and challenges.

Result: This survey maps the connections between cognitive capabilities, FM structures, and hardware limitations, and suggests opportunities for algorithm-system co-design and collaborative edge deployments.

Conclusion: It establishes a unified perspective toward scalable, adaptive, and resource-efficient AI agents, encouraging collaboration and innovation in agentic intelligence development.

Abstract: Foundation models have reshaped AI by unifying fragmented architectures into
scalable backbones with multimodal reasoning and contextual adaptation. In
parallel, the long-standing notion of AI agents, defined by the
sensing-decision-action loop, is entering a new paradigm: with FMs as their
cognitive core, agents transcend rule-based behaviors to achieve autonomy,
generalization, and self-reflection. This dual shift is reinforced by
real-world demands such as autonomous driving, robotics, virtual assistants,
and GUI agents, as well as ecosystem advances in embedded hardware, edge
computing, mobile deployment platforms, and communication protocols that
together enable large-scale deployment. Yet this convergence collides with
reality: while applications demand long-term adaptability and real-time
interaction, mobile and edge deployments remain constrained by memory, energy,
bandwidth, and latency. This creates a fundamental tension between the growing
complexity of FMs and the limited resources of deployment environments. This
survey provides the first systematic characterization of adaptive,
resource-efficient agentic AI systems. We summarize enabling techniques into
elastic inference, test-time adaptation, dynamic multimodal integration, and
agentic AI applications, and identify open challenges in balancing
accuracy-latency-communication trade-offs and sustaining robustness under
distribution shifts. We further highlight future opportunities in
algorithm-system co-design, cognitive adaptation, and collaborative edge
deployment. By mapping FM structures, cognition, and hardware resources, this
work establishes a unified perspective toward scalable, adaptive, and
resource-efficient agentic AI. We believe this survey can help readers to
understand the connections between enabling technologies while promoting
further discussions on the fusion of agentic intelligence and intelligent
agents.

</details>


### [73] [Approximately Unimodal Likelihood Models for Ordinal Regression](https://arxiv.org/abs/2510.00122)
*Ryoya Yamasaki*

Main category: cs.LG

TL;DR: This paper addresses biases in ordinal regression models caused by strictly unimodal likelihood conditions and proposes approximately unimodal likelihood models, capable of accommodating both unimodal and near-unimodal distributions.


<details>
  <summary>Details</summary>
Motivation: Unimodal likelihood models for ordinal regression can inaccurately represent datasets featuring explanatory variables with non-unimodal conditional probability distributions, leading to biases.

Method: The authors introduce approximately unimodal likelihood models designed to represent both unimodal and near-unimodal conditional probability distributions.

Result: Experimental evidence demonstrates the effectiveness of the proposed models in statistical modeling and ordinal regression tasks.

Conclusion: Approximately unimodal likelihood models mitigate biases in ordinal regression for datasets exhibiting non-unimodal distributions, improving statistical modeling outcomes.

Abstract: Ordinal regression (OR, also called ordinal classification) is classification
of ordinal data, in which the underlying target variable is categorical and
considered to have a natural ordinal relation for the underlying explanatory
variable. A key to successful OR models is to find a data structure `natural
ordinal relation' common to many ordinal data and reflect that structure into
the design of those models. A recent OR study found that many real-world
ordinal data show a tendency that the conditional probability distribution
(CPD) of the target variable given a value of the explanatory variable will
often be unimodal. Several previous studies thus developed unimodal likelihood
models, in which a predicted CPD is guaranteed to become unimodal. However, it
was also observed experimentally that many real-world ordinal data partly have
values of the explanatory variable where the underlying CPD will be
non-unimodal, and hence unimodal likelihood models may suffer from a bias for
such a CPD. Therefore, motivated to mitigate such a bias, we propose
approximately unimodal likelihood models, which can represent up to a unimodal
CPD and a CPD that is close to be unimodal. We also verify experimentally that
a proposed model can be effective for statistical modeling of ordinal data and
OR tasks.

</details>


### [74] [BigBang-Proton Technical Report: Next-Word-Prediction is Scientific Multitask Learner](https://arxiv.org/abs/2510.00129)
*Hengkui Wu,Liujiang Liu,Jihua He,Qihao Wang,Keke Zhao,Shuyang Hu,Renle Fu,Dahao Liang,Lingyu Zeng,Bruce Liu,Yuan Liu,Jin Zhan,Jiaqiang Niu,Xinglong Jia,Yaqin Hu,Wenjun Ji,Panpan Chi,Ken Chen,Hengyuan Wu,Yingsi Xin,Yongfeng Zhu,Yuexin Wang,Manqi Ruan,Ningtao Bian,Xiaohua Wu,Weipeng Xu*

Main category: cs.LG

TL;DR: BigBang-Proton is a novel language model for scientific computing that achieves high accuracy across multiple scientific domains.


<details>
  <summary>Details</summary>
Motivation: Develop a multitask scientific learner capable of addressing diverse real-world problems using a unified sequence-based architecture.

Method: Innovative features include Theory-Experiment Learning, Binary Patch Encoding, and Monte Carlo Attention, trained on scientific and general text corpora.

Result: Achieved exceptional accuracy across diverse tasks such as arithmetic operations, particle physics, water quality prediction, and genome modeling, matching or surpassing specialized models.

Conclusion: Language-based scientific computation can rival task-specific models while offering versatile multitask learning capabilities.

Abstract: We introduce BigBang-Proton, a unified sequence-based architecture for
auto-regressive language modeling pretrained on cross-scale, cross-structure,
cross-discipline real-world scientific tasks to construct a scientific
multi-task learner. BigBang-Proton incorporates three fundamental innovations
compared to mainstream general-purpose LLMs: Theory-Experiment Learning
paradigm aligns large-scale numerical experimental data with theoretical text
corpora; Binary Patch Encoding replaces byte pair encoding(BPE) tokenization;
Monte Carlo Attention substitutes traditional transformer architectures.
Through next-word-prediction pretraining on cross-discipline scientific
datasets of real-world problems mixed with general textual corpus, followed by
fine-tuning and inference on downstream tasks, BigBang-Proton demonstrates
100\% accuracy in up to 50-digit arithmetic addition operations, performance on
par with leading specialized models in particle physics jet tagging, matching
MAE of specialized models in inter-atomic potential simulation, performance
comparable to traditional spatiotemporal models in water quality prediction,
and benchmark-exceeding performance in genome modeling. These results prove
that language-guided scientific computing can match or exceed the performance
of task-specific scientific models while maintaining multitask learning
capabilities. We further hypothesize to scale the pretraining to the universe
scale as a fundamental step toward developing material world foundational
model.

</details>


### [75] [Large Language Models Inference Engines based on Spiking Neural Networks](https://arxiv.org/abs/2510.00133)
*Adarsha Balaji,Sandeep Madireddy*

Main category: cs.LG

TL;DR: This paper introduces NeurTransformer, a methodology to create energy-efficient transformer-based spiking neural networks (SNNs) by fine-tuning spike-based self-attention and converting feed-forward blocks.


<details>
  <summary>Details</summary>
Motivation: Address the computational challenges and inefficiency in training and deploying large-scale foundational models like transformers, especially their quadratic complexity with sequence length.

Method: Use spiking neural networks (SNNs) by replacing self-attention with spike-based self-attention, converting feed-forward blocks into SNN equivalents, and applying fine-tuning with surrogate learning algorithms.

Result: Benchmarking shows 5-12% loss in cosine similarity, 9.7% reduction in perplexity for converted GPT-2 models, and 64.71%-85.28% energy savings in the self-attention mechanism on digital hardware.

Conclusion: NeurTransformer provides scalable and energy-efficient transformer designs using spiking neural networks, offering practical advantages in computational efficiency despite trade-offs in model performance metrics.

Abstract: Foundational models based on the transformer architecture are currently the
state-of-the-art in general language modeling, as well as in scientific areas
such as material science and climate. However, training and deploying these
models is computationally challenging as the time and space complexity has a
quadratic relation to the input sequence length. Several efforts exploring
efficient computational paradigms and model architectures to address these
limitations have been made. In this work, we explore spiking neural networks
(SNNs) to design transformer models. A challenge in training large-scale SNNs,
using existing surrogate learning methods is inefficient and time-consuming. On
the other hand, techniques to convert existing transformer-based models to
their SNN equivalent are not scalable, as achieving optimal performance comes
at the cost of a large number of spike time-steps, i.e. increased latency. To
address this, we propose NeurTransformer, a methodology for designing
transformer-based SNN for inference using a supervised fine-tuning approach
with existing conversion methods. The proposed methodology works by: (1)
replacing the self-attention mechanism with a spike-based self-attention (SSA),
(2) converting the feed-forward block of the trained transformer model to its
equivalent SNN, and (3) fine-tuning the SSA block using SNN-based surrogate
learning algorithms. We benchmark the proposed methodology and demonstrate its
accuracy and scalability using three variants of the GPT-2 model of increasing
model size. We observe that the converted GPT-2 small models demonstrate a
5-12% loss in cosine similarity and a 9.7% reduction in perplexity. Finally, we
demonstrate the energy efficiency of the SSA block compared to the ASA block
and show between 64.71% and 85.28% reductions in estimated energy consumption
when implementing the self-attention mechanism on a digital hardware.

</details>


### [76] [Nonparametric Identification of Latent Concepts](https://arxiv.org/abs/2510.00136)
*Yujia Zheng,Shaoan Xie,Kun Zhang*

Main category: cs.LG

TL;DR: The paper proposes a theoretical framework for machines to identify hidden concepts using diverse observations, inspired by human cognitive mechanisms.


<details>
  <summary>Details</summary>
Motivation: Human learning leverages comparisons of diverse observations to understand concepts compositionally and extrapolate knowledge. The paper explores parallels in machine learning to ensure correct identification of hidden data concepts and provide theoretical support for concept learning.

Method: The authors develop a framework for concept identifiability using diverse class observations, without relying on predefined concept types, functional relations, or parametric models. They also propose a local comparison-based approach for scenarios where global conditions are unmet.

Result: Theoretical results demonstrate the ability to identify hidden concepts and their structures nonparametrically, validated in synthetic and real-world datasets.

Conclusion: Concept identifiability is achievable under diverse conditions, enhancing machines' capabilities to understand hidden structures. The framework offers correctness guarantees and extends applicability beyond rigid assumptions.

Abstract: We are born with the ability to learn concepts by comparing diverse
observations. This helps us to understand the new world in a compositional
manner and facilitates extrapolation, as objects naturally consist of multiple
concepts. In this work, we argue that the cognitive mechanism of comparison,
fundamental to human learning, is also vital for machines to recover true
concepts underlying the data. This offers correctness guarantees for the field
of concept learning, which, despite its impressive empirical successes, still
lacks general theoretical support. Specifically, we aim to develop a
theoretical framework for the identifiability of concepts with multiple classes
of observations. We show that with sufficient diversity across classes, hidden
concepts can be identified without assuming specific concept types, functional
relations, or parametric generative models. Interestingly, even when conditions
are not globally satisfied, we can still provide alternative guarantees for as
many concepts as possible based on local comparisons, thereby extending the
applicability of our theory to more flexible scenarios. Moreover, the hidden
structure between classes and concepts can also be identified
nonparametrically. We validate our theoretical results in both synthetic and
real-world settings.

</details>


### [77] [LoRAFusion: Efficient LoRA Fine-Tuning for LLMs](https://arxiv.org/abs/2510.00206)
*Zhanda Zhu,Qidong Su,Yaoyao Ding,Kevin Song,Shang Wang,Gennady Pekhimenko*

Main category: cs.LG

TL;DR: LoRAFusion improves LoRA fine-tuning for Large Language Models by addressing inefficiencies like memory access overhead and inability to fine-tune multiple adapters concurrently. It achieves up to 1.96x speedup compared to existing systems.


<details>
  <summary>Details</summary>
Motivation: LoRA fine-tuning systems are highly parameter-efficient but face runtime inefficiencies due to redundant memory accesses and lack of concurrent multi-adapter fine-tuning, limiting their performance.

Method: LoRAFusion introduces a graph-splitting method to optimize memory-bound operations and an adaptive batching algorithm for efficient multi-job fine-tuning. This includes dependency-aware microbatch scheduling.

Result: LoRAFusion achieves up to 1.96x speedup over Megatron-LM and 1.46x over mLoRA. Its kernel offers up to 1.39x performance improvement and can be used as a plug-and-play replacement.

Conclusion: LoRAFusion successfully addresses key inefficiencies in existing LoRA systems, offering significant speed and performance improvements while maintaining fine-tuning quality. It has been open-sourced for public use.

Abstract: Low-Rank Adaptation (LoRA) has become the leading Parameter-Efficient
Fine-Tuning (PEFT) method for Large Language Models (LLMs), as it significantly
reduces GPU memory usage while maintaining competitive fine-tuned model quality
on downstream tasks. Despite these benefits, we identify two key inefficiencies
in existing LoRA fine-tuning systems. First, they incur substantial runtime
overhead due to redundant memory accesses on large activation tensors. Second,
they miss the opportunity to concurrently fine-tune multiple independent LoRA
adapters that share the same base model on the same set of GPUs. This leads to
missed performance gains such as reduced pipeline bubbles, better communication
overlap, and improved GPU load balance.
  To address these issues, we introduce LoRAFusion, an efficient LoRA
fine-tuning system for LLMs. At the kernel level, we propose a graph-splitting
method that fuses memory-bound operations. This design eliminates unnecessary
memory accesses and preserves the performance of compute-bound GEMMs without
incurring the cost of recomputation or synchronization. At the scheduling
level, LoRAFusion introduces an adaptive batching algorithm for multi-job
fine-tuning. It first splits LoRA adapters into groups to intentionally stagger
batch execution across jobs, and then solves a bin-packing problem within each
group to generate balanced, dependency-aware microbatches. LoRAFusion achieves
up to $1.96\times$ ($1.47\times$ on average) end-to-end speedup compared to
Megatron-LM, and up to $1.46\times$ ($1.29\times$ on average) improvement over
mLoRA, the state-of-the-art multi-LoRA fine-tuning system. Our fused kernel
achieves up to $1.39\times$ ($1.27\times$ on average) kernel performance
improvement and can directly serve as a plug-and-play replacement in existing
LoRA systems. We open-source LoRAFusion at
https://github.com/CentML/lorafusion.

</details>


### [78] [Which Rewards Matter? Reward Selection for Reinforcement Learning under Limited Feedback](https://arxiv.org/abs/2510.00144)
*Shreyas Chaudhari,Renhao Zhang,Philip S. Thomas,Bruno Castro da Silva*

Main category: cs.LG

TL;DR: The paper addresses reward selection in reinforcement learning when feedback is limited, exploring strategies to maximize policy performance with fewer labeled samples.


<details>
  <summary>Details</summary>
Motivation: Obtaining large amounts of reward labels for reinforcement learning can be infeasible, prompting the need to optimize reward selection for effective training.

Method: The authors formalize reward selection for reinforcement learning with limited feedback and investigate heuristic and pre-trained strategies for impactful reward labeling.

Result: Key findings include identifying impactful rewards that guide optimal trajectories and enable recovery, achieving near-optimal policies with fewer labeled samples.

Conclusion: Reward selection is a promising strategy for scaling reinforcement learning in limited feedback scenarios, reducing the need for extensive reward labeling.

Abstract: The ability of reinforcement learning algorithms to learn effective policies
is determined by the rewards available during training. However, for practical
problems, obtaining large quantities of reward labels is often infeasible due
to computational or financial constraints, particularly when relying on human
feedback. When reinforcement learning must proceed with limited feedback --
only a fraction of samples get rewards labeled -- a fundamental question
arises: which samples should be labeled to maximize policy performance? We
formalize this problem of reward selection for reinforcement learning from
limited feedback (RLLF), introducing a new problem formulation that facilitates
the study of strategies for selecting impactful rewards. Two types of selection
strategies are investigated: (i) heuristics that rely on reward-free
information such as state visitation and partial value functions, and (ii)
strategies pre-trained using auxiliary evaluative feedback. We find that
critical subsets of rewards are those that (1) guide the agent along optimal
trajectories, and (2) support recovery toward near-optimal behavior after
deviations. Effective selection methods yield near-optimal policies with
significantly fewer reward labels than full supervision, establishing reward
selection as a powerful paradigm for scaling reinforcement learning in
feedback-limited settings.

</details>


### [79] [Partial Identification Approach to Counterfactual Fairness Assessment](https://arxiv.org/abs/2510.00163)
*Saeyoung Rho,Junzhe Zhang,Elias Bareinboim*

Main category: cs.LG

TL;DR: The paper tackles the challenge of evaluating counterfactual fairness measures in AI systems through a Bayesian partial identification approach, revealing biases in the COMPAS dataset.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the growing concerns about the fairness of AI decision-making systems in critical areas where decisions have significant societal impact, especially given the difficulty in understanding how sensitive attributes affect algorithmic outcomes.

Method: The authors employ partial identification to derive bounds over counterfactual fairness measures and propose a Bayesian algorithm to compute these bounds using observational data.

Result: The findings show a spurious positive effect on COMPAS risk scores when race changes to African-American and a direct causal negative effect when age transitions from young to old.

Conclusion: The paper concludes that the proposed approach provides a robust framework for evaluating counterfactual fairness measures, offering insights into algorithmic biases such as those present in the COMPAS dataset.

Abstract: The wide adoption of AI decision-making systems in critical domains such as
criminal justice, loan approval, and hiring processes has heightened concerns
about algorithmic fairness. As we often only have access to the output of
algorithms without insights into their internal mechanisms, it was natural to
examine how decisions would alter when auxiliary sensitive attributes (such as
race) change. This led the research community to come up with counterfactual
fairness measures, but how to evaluate the measure from available data remains
a challenging task. In many practical applications, the target counterfactual
measure is not identifiable, i.e., it cannot be uniquely determined from the
combination of quantitative data and qualitative knowledge. This paper
addresses this challenge using partial identification, which derives
informative bounds over counterfactual fairness measures from observational
data. We introduce a Bayesian approach to bound unknown counterfactual fairness
measures with high confidence. We demonstrate our algorithm on the COMPAS
dataset, examining fairness in recidivism risk scores with respect to race,
age, and sex. Our results reveal a positive (spurious) effect on the COMPAS
score when changing race to African-American (from all others) and a negative
(direct causal) effect when transitioning from young to old age.

</details>


### [80] [Why Can't Transformers Learn Multiplication? Reverse-Engineering Reveals Long-Range Dependency Pitfalls](https://arxiv.org/abs/2510.00184)
*Xiaoyan Bai,Itamar Pres,Yuntian Deng,Chenhao Tan,Stuart Shieber,Fernanda Viégas,Martin Wattenberg,Andrew Lee*

Main category: cs.LG

TL;DR: This research examines why language models struggle with multi-digit multiplication, uncovering mechanisms for encoding long-range dependencies, and proposing inductive bias for optimization.


<details>
  <summary>Details</summary>
Motivation: To understand why language models fail at tasks like multi-digit multiplication despite their increasing capabilities.

Method: Reverse-engineering a model that successfully uses implicit chain-of-thought to learn multi-digit multiplication, and introducing auxiliary losses for validation.

Result: The study revealed mechanisms like attention graphs and Fourier basis representations crucial for handling long-range dependencies, which standard fine-tuning models fail to achieve.

Conclusion: Long-range dependencies are vital for multi-digit multiplication; proper inductive biases can overcome local optima limitations in Transformer models.

Abstract: Language models are increasingly capable, yet still fail at a seemingly
simple task of multi-digit multiplication. In this work, we study why, by
reverse-engineering a model that successfully learns multiplication via
\emph{implicit chain-of-thought}, and report three findings: (1) Evidence of
long-range structure: Logit attributions and linear probes indicate that the
model encodes the necessary long-range dependencies for multi-digit
multiplication. (2) Mechanism: the model encodes long-range dependencies using
attention to construct a directed acyclic graph to ``cache'' and ``retrieve''
pairwise partial products. (3) Geometry: the model implements partial products
in attention heads by forming Minkowski sums between pairs of digits, and
digits are represented using a Fourier basis, both of which are intuitive and
efficient representations that the standard fine-tuning model lacks. With these
insights, we revisit the learning dynamics of standard fine-tuning and find
that the model converges to a local optimum that lacks the required long-range
dependencies. We further validate this understanding by introducing an
auxiliary loss that predicts the ``running sum'' via a linear regression probe,
which provides an inductive bias that enables the model to successfully learn
multi-digit multiplication. In summary, by reverse-engineering the mechanisms
of an implicit chain-of-thought model we uncover a pitfall for learning
long-range dependencies in Transformers and provide an example of how the
correct inductive bias can address this issue.

</details>


### [81] [Lipschitz Bandits with Stochastic Delayed Feedback](https://arxiv.org/abs/2510.00309)
*Zhongxuan Liu,Yue Kang,Thomas C. M. Lee*

Main category: cs.LG

TL;DR: The paper extends the Lipschitz bandit problem to account for stochastic delayed feedback in reward observations, proposing algorithms for bounded and unbounded delays.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of stochastic delayed feedback in continuous action sets where rewards are governed by Lipschitz conditions.

Method: The bounded delays are addressed using a delay-aware zooming algorithm, while unbounded delays are tackled with a phased learning strategy. Regret guarantees and lower bounds are established.

Result: The algorithms achieve sublinear regret in both bounded and unbounded delay settings, demonstrating near-optimal performance for unbounded delays.

Conclusion: The proposed solutions effectively handle delayed feedback in Lipschitz bandits, validated through theoretical guarantees and experimental results.

Abstract: The Lipschitz bandit problem extends stochastic bandits to a continuous
action set defined over a metric space, where the expected reward function
satisfies a Lipschitz condition. In this work, we introduce a new problem of
Lipschitz bandit in the presence of stochastic delayed feedback, where the
rewards are not observed immediately but after a random delay. We consider both
bounded and unbounded stochastic delays, and design algorithms that attain
sublinear regret guarantees in each setting. For bounded delays, we propose a
delay-aware zooming algorithm that retains the optimal performance of the
delay-free setting up to an additional term that scales with the maximal delay
$\tau_{\max}$. For unbounded delays, we propose a novel phased learning
strategy that accumulates reliable feedback over carefully scheduled intervals,
and establish a regret lower bound showing that our method is nearly optimal up
to logarithmic factors. Finally, we present experimental results to demonstrate
the efficiency of our algorithms under various delay scenarios.

</details>


### [82] [PrunedLoRA: Robust Gradient-Based structured pruning for Low-rank Adaptation in Fine-tuning](https://arxiv.org/abs/2510.00192)
*Xin Yu,Cong Xie,Ziyu Zhao,Tiantian Fan,Lingzhou Xue,Zhi Zhang*

Main category: cs.LG

TL;DR: The paper introduces PrunedLoRA, a new method that leverages structured pruning to improve low-rank adapters' representational capacity in LoRA fine-tuning, yielding better performance across various tasks.


<details>
  <summary>Details</summary>
Motivation: Enhance the representational capacity of low-rank adapters in LoRA fine-tuning, which currently underperforms compared to full fine-tuning.

Method: PrunedLoRA employs structured pruning to dynamically remove less useful components during fine-tuning, with fine-grained, gradient-based pruning that minimizes loss and ensures robustness.

Result: PrunedLoRA consistently outperforms LoRA and its variants in tasks like mathematical reasoning, code generation, and natural language understanding, and shows superiority over other structured pruning methods under diverse sparsity levels.

Conclusion: The approach demonstrates that PrunedLoRA not only enhances the expressiveness of low-rank adapters but also provides a theoretically robust and empirically superior alternative for parameter-efficient fine-tuning.

Abstract: Low-rank adaptation (LoRA) has become a widely used paradigm for
parameter-efficient fine-tuning of large language models, yet its
representational capacity often lags behind full fine-tuning. Within the
context of LoRA, a key open question is how to obtain expressive low-rank
adapters from over-parameterized spaces. We propose \textit{PrunedLoRA}, a new
framework that leverages structured pruning to obtain highly representative
low-rank adapters from an over-parameterized initialization. Unlike prior
approaches that impose a fixed low-rank budget, PrunedLoRA dynamically prunes
less important components during fine-tuning and prevents their reactivation,
enabling flexible and adaptive rank allocation. For structured pruning, by
minimizing the pruning error for overall loss, we provide fine-grained pruning
and recovery updates in a gradient-based pruning strategy with grounded
interpretation. We provide the first theoretical analysis of the robustness of
structured pruning and provably show that under the impact of weight
perturbation, gradient-based pruning is more robust than activation-based
pruning with respect to overall loss. Empirically, PrunedLoRA consistently
outperforms LoRA and its variants across supervised fine-tuning tasks in
mathematical reasoning, code generation, and natural language understanding,
and it also demonstrates advantages over existing structured pruning methods
across diverse sparsity levels.

</details>


### [83] [Train on Validation (ToV): Fast data selection with applications to fine-tuning](https://arxiv.org/abs/2510.00386)
*Ayush Jain,Andrea Montanari,Eren Sasoglu*

Main category: cs.LG

TL;DR: The paper introduces a novel, efficient method for data selection during fine-tuning in machine learning, which achieves better performance in reducing test loss compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of effective data selection during fine-tuning, especially when only a few samples from the target distribution are available.

Method: A data selection approach that swaps the usual role of train and validation—performing inference on the training pool before and after fine-tuning on the validation set and selecting samples whose predictions change the most.

Result: The proposed method outperforms state-of-the-art approaches in lowering test log-loss on instruction tuning and named entity recognition tasks, supported by both experiments and theoretical validation.

Conclusion: The method identifies training samples most influenced by small target-validation fine-tuning as critical, providing a simpler and faster alternative for improved fine-tuning performance.

Abstract: State-of-the-art machine learning often follows a two-stage process:
$(i)$~pre-training on large, general-purpose datasets; $(ii)$~fine-tuning on
task-specific data. In fine-tuning, selecting training examples that closely
reflect the target distribution is crucial. However, it is often the case that
only a few samples are available from the target distribution. Existing data
selection methods treat these target samples as a validation set and estimate
the effect of adding or removing a single sample from the training pool by
performing inference on the validation set.
  We propose a simpler and faster alternative that inverts the usual role of
train and validation: we perform inference on the training pool before and
after fine-tuning on the validation set. We then select samples whose
predictions change the most. Our key insight is that the training samples most
affected by fine-tuning on a small validation set tend to be the most
beneficial for reducing test loss on the target distribution. Experiments on
instruction tuning and named entity recognition tasks show that, in most cases,
our method achieves lower test log-loss than state-of-the-art approaches. We
support our findings with theoretical analysis.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [84] [A Technique Based on Trade-off Maps to Visualise and Analyse Relationships Between Objectives in Optimisation Problems](https://arxiv.org/abs/2510.00877)
*Rodrigo Lankaites Pinheiro,Dario Landa-Silva,Jason Atkin*

Main category: cs.NE

TL;DR: The paper introduces a technique for visualizing and analyzing the relationships between objectives in multiobjective optimization problems, enabling better understanding of their fitness landscape and decision-making support.


<details>
  <summary>Details</summary>
Motivation: To improve problem-solving techniques in multiobjective combinatorial optimization by understanding the often complex relationships between objectives, particularly in real-world logistical scenarios.

Method: The technique involves four steps: analyzing global pairwise relationships using the Kendall correlation method, assessing Pareto front value ranges, plotting a Gray code-based map to show trade-offs, and identifying local relationships through scatter plots.

Result: Experiments on three problems (multiobjective multidimensional knapsack, nurse scheduling, and vehicle routing with time windows) show the technique helps in understanding problem difficulty and objective relationships.

Conclusion: The proposed approach enhances insight into objective relationships in multiobjective optimization, aiding in problem-solving and decision-making efficiency.

Abstract: Understanding the relationships between objectives in a multiobjective
optimisation problem is important for developing tailored and efficient solving
techniques. In particular, when tackling combinatorial optimisation problems
with many objectives, that arise in real-world logistic scenarios, better
support for the decision maker can be achieved through better understanding of
the often complex fitness landscape. This paper makes a contribution in this
direction by presenting a technique that allows a visualisation and analysis of
the local and global relationships between objectives in optimisation problems
with many objectives. The proposed technique uses four steps: First, the global
pairwise relationships are analysed using the Kendall correlation method; then,
the ranges of the values found on the given Pareto front are estimated and
assessed; next, these ranges are used to plot a map using Gray code, similar to
Karnaugh maps, that has the ability to highlight the trade-offs between
multiple objectives; and finally, local relationships are identified using
scatter plots. Experiments are presented for three combinatorial optimisation
problems: multiobjective multidimensional knapsack problem, multiobjective
nurse scheduling problem, and multiobjective vehicle routing problem with time
windows . Results show that the proposed technique helps in the gaining of
insights into the problem difficulty arising from the relationships between
objectives.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [85] [Opal: A Modular Framework for Optimizing Performance using Analytics and LLMs](https://arxiv.org/abs/2510.00932)
*Mohammad Zaeed,Tanzima Z. Islam,Vladimir Inđić*

Main category: cs.PF

TL;DR: Opal is a framework that connects performance insights with guidance from Large Language Models (LLMs), achieving significant GPU kernel optimization speedups.


<details>
  <summary>Details</summary>
Motivation: LLMs show potential in code optimization but lack performance context for trustworthiness and reliability.

Method: Opal bridges insights from hardware counters and Roofline analysis to optimization decisions, leveraging modular frameworks and LLMs.

Result: Opal achieved significant average speedups (19.34%–52.3%) across 1640 GPU kernel experiments, with nearly flawless code generation and few unsafe suggestions.

Conclusion: Opal enables automated, expert-level GPU kernel optimization, advancing accessible performance engineering for broader use.

Abstract: Large Language Models (LLMs) show promise for automated code optimization but
struggle without performance context. This work introduces Opal, a modular
framework that connects performance analytics insights with the vast body of
published by guiding LLMs to generate informed, trustworthy optimizations.
Unlike traditional performance tools that identify bottlenecks but stop short
of actionable suggestions, Opal bridges this long-standing gap by linking
dynamic insights from hardware counters and Roofline analysis to stall events
to optimization decisions. We evaluate Opal across 1640 experiments on
real-world GPU kernels and find that in over 98.5% of cases, even a single
insight source yields speedups, ranging on average from 19.34% to 52.3%. Our
prompt template produced correct code in all but one case, where a vague
diagnostic caused an unsafe suggestion. By automatically optimizing GPU kernels
using performance analytics and LLMs, Opal marks a leap toward democratizing
expert-level performance engineering for all.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [86] [RoboPilot: Generalizable Dynamic Robotic Manipulation with Dual-thinking Modes](https://arxiv.org/abs/2510.00154)
*Xinyi Liu,Mohammadreza Fani Sani,Zewei Zhou,Julius Wirbel,Bahram Zarrin,Roberto Galeazzi*

Main category: cs.RO

TL;DR: This paper introduces RoboPilot, a novel closed-loop system for robotic manipulation that combines structured planning, feedback, and adaptable reasoning for robust performance in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: Current robotics systems struggle with executing complex or long tasks due to reliance on open-loop paradigms, which suffer from error accumulation and lack adaptability to dynamic environments.

Method: RoboPilot introduces a dual-thinking closed-loop framework that integrates structured task planning, flexible action generation, Chain-of-Thought reasoning, and dynamic switching between fast and slow thinking modes. A new benchmark, RoboPilot-Bench, evaluates this framework across 21 diverse robot manipulation tasks.

Result: RoboPilot achieves a 25.9% higher task success rate than existing systems and demonstrates real-world robustness through deployment on industrial robots.

Conclusion: RoboPilot enhances the robustness and accuracy of robotic manipulation in complex and dynamic scenarios by integrating advanced reasoning and feedback mechanisms, offering significant improvements over current approaches.

Abstract: Despite rapid progress in autonomous robotics, executing complex or
long-horizon tasks remains a fundamental challenge. Most current approaches
follow an open-loop paradigm with limited reasoning and no feedback, resulting
in poor robustness to environmental changes and severe error accumulation. We
present RoboPilot, a dual-thinking closed-loop framework for robotic
manipulation that supports adaptive reasoning for complex tasks in real-world
dynamic environments. RoboPilot leverages primitive actions for structured task
planning and flexible action generation, while introducing feedback to enable
replanning from dynamic changes and execution errors. Chain-of-Thought
reasoning further enhances high-level task planning and guides low-level action
generation. The system dynamically switches between fast and slow thinking to
balance efficiency and accuracy. To systematically evaluate the robustness of
RoboPilot in diverse robot manipulation scenarios, we introduce
RoboPilot-Bench, a benchmark spanning 21 tasks across 10 categories, including
infeasible-task recognition and failure recovery. Experiments show that
RoboPilot outperforms state-of-the-art baselines by 25.9\% in task success
rate, and the real-world deployment on an industrial robot further demonstrates
its robustness in real-world settings.

</details>


### [87] [A Systematic Study of Large Language Models for Task and Motion Planning With PDDLStream](https://arxiv.org/abs/2510.00182)
*Jorge Mendez-Mendez*

Main category: cs.RO

TL;DR: The paper examines the integration of large language models (LLMs) like Gemini 2.5 Flash into task and motion planning (TAMP) systems for robotics and discovers that LLM-based planners have limitations compared to engineered counterparts.


<details>
  <summary>Details</summary>
Motivation: Investigate LLM planning capabilities in robotics and understand how semantic knowledge can be combined with formal reasoning to enhance robotic systems.

Method: Developed 16 algorithms using Gemini 2.5 Flash and conducted zero-shot experiments across 4,950 robotics problems in three domains, analyzing their success rates and planning times.

Result: LLM-based planners demonstrated lower success rates and higher planning times compared to engineered solutions. Geometric details introduced greater errors, and non-reasoning variants generally performed better than reasoning ones.

Conclusion: Combining LLMs with TAMP systems is feasible but currently underperforms compared to traditional methods. Strategic LLM direction within TAMP systems can help mitigate errors.

Abstract: Using large language models (LLMs) to solve complex robotics problems
requires understanding their planning capabilities. Yet while we know that LLMs
can plan on some problems, the extent to which these planning capabilities
cover the space of robotics tasks is unclear. One promising direction is to
integrate the semantic knowledge of LLMs with the formal reasoning of task and
motion planning (TAMP). However, the myriad of choices for how to integrate
LLMs within TAMP complicates the design of such systems. We develop 16
algorithms that use Gemini 2.5 Flash to substitute key TAMP components. Our
zero-shot experiments across 4,950 problems and three domains reveal that the
Gemini-based planners exhibit lower success rates and higher planning times
than their engineered counterparts. We show that providing geometric details
increases the number of task-planning errors compared to pure PDDL
descriptions, and that (faster) non-reasoning LLM variants outperform (slower)
reasoning variants in most cases, since the TAMP system can direct the LLM to
correct its mistakes.

</details>


### [88] [A Novel Robust Control Method Combining DNN-Based NMPC Approximation and PI Control: Application to Exoskeleton Squat Movements](https://arxiv.org/abs/2510.00188)
*Alireza Aliyari,Gholamreza Vossoughi*

Main category: cs.RO

TL;DR: This paper presents a hybrid control method combining NMPC-DNN and a PI controller, applied to an exoskeleton robot for squat movements, achieving improved tracking and reduced computational cost.


<details>
  <summary>Details</summary>
Motivation: Traditional NMPC faces computational challenges, and existing NMPC-DNN approaches lack robustness under unexpected conditions.

Method: A hybrid control system (Hybrid NMPC-DNN-PI) that merges NMPC-DNN outputs with a PI controller was developed and validated on an exoskeleton robot with a human-robot dynamic model.

Result: The Hybrid NMPC-DNN-PI achieved significantly reduced tracking errors under unseen conditions, reduced human joint torques by over 29%, and decreased computational costs by 99.93% compared to NMPC.

Conclusion: The Hybrid NMPC-DNN-PI is a robust and efficient control solution for nonlinear robotic systems, particularly under varying and unforeseen conditions.

Abstract: Nonlinear Model Predictive Control (NMPC) is a precise controller, but its
heavy computational load often prevents application in robotic systems. Some
studies have attempted to approximate NMPC using deep neural networks
(NMPC-DNN). However, in the presence of unexpected disturbances or when
operating conditions differ from training data, this approach lacks robustness,
leading to large tracking errors. To address this issue, for the first time,
the NMPC-DNN output is combined with a PI controller (Hybrid NMPC-DNN-PI). The
proposed controller is validated by applying it to an exoskeleton robot during
squat movement, which has a complex dynamic model and has received limited
attention regarding robust nonlinear control design. A human-robot dynamic
model with three active joints (ankle, knee, hip) is developed, and more than
5.3 million training samples are used to train the DNN. The results show that,
under unseen conditions for the DNN, the tracking error in Hybrid NMPC-DNN-PI
is significantly lower compared to NMPC-DNN. Moreover, human joint torques are
greatly reduced with the use of the exoskeleton, with RMS values for the
studied case reduced by 30.9%, 41.8%, and 29.7% at the ankle, knee, and hip,
respectively. In addition, the computational cost of Hybrid NMPC-DNN-PI is
99.93% lower than that of NMPC.

</details>


### [89] [TGPO: Temporal Grounded Policy Optimization for Signal Temporal Logic Tasks](https://arxiv.org/abs/2510.00225)
*Yue Meng,Fei Chen,Chuchu Fan*

Main category: cs.RO

TL;DR: TGPO is introduced to address challenges in learning control policies for complex, long-horizon STL tasks in robotics, significantly outperforming existing baselines.


<details>
  <summary>Details</summary>
Motivation: Robotics and autonomous systems face challenges in specifying and solving complex tasks using STL due to sparse rewards and non-Markovian dynamics.

Method: TGPO uses a hierarchical learning framework, splitting tasks into timed subgoals and invariant constraints. It employs Metropolis-Hastings sampling to guide temporal allocation and policy optimization.

Result: TGPO demonstrates superior performance across five diverse environments, achieving a 31.6% higher task success rate compared to state-of-the-art baselines.

Conclusion: The proposed TGPO framework effectively handles complex STL specifications, offering a significant advancement in reinforcement learning for robotics.

Abstract: Learning control policies for complex, long-horizon tasks is a central
challenge in robotics and autonomous systems. Signal Temporal Logic (STL)
offers a powerful and expressive language for specifying such tasks, but its
non-Markovian nature and inherent sparse reward make it difficult to be solved
via standard Reinforcement Learning (RL) algorithms. Prior RL approaches focus
only on limited STL fragments or use STL robustness scores as sparse terminal
rewards. In this paper, we propose TGPO, Temporal Grounded Policy Optimization,
to solve general STL tasks. TGPO decomposes STL into timed subgoals and
invariant constraints and provides a hierarchical framework to tackle the
problem. The high-level component of TGPO proposes concrete time allocations
for these subgoals, and the low-level time-conditioned policy learns to achieve
the sequenced subgoals using a dense, stage-wise reward signal. During
inference, we sample various time allocations and select the most promising
assignment for the policy network to rollout the solution trajectory. To foster
efficient policy learning for complex STL with multiple subgoals, we leverage
the learned critic to guide the high-level temporal search via
Metropolis-Hastings sampling, focusing exploration on temporally feasible
solutions. We conduct experiments on five environments, ranging from
low-dimensional navigation to manipulation, drone, and quadrupedal locomotion.
Under a wide range of STL tasks, TGPO significantly outperforms
state-of-the-art baselines (especially for high-dimensional and long-horizon
cases), with an average of 31.6% improvement in task success rate compared to
the best baseline. The code will be available at
https://github.com/mengyuest/TGPO

</details>


### [90] [BC-MPPI: A Probabilistic Constraint Layer for Safe Model-Predictive Path-Integral Control](https://arxiv.org/abs/2510.00272)
*Odichimnma Ezeji,Michael Ziegltrum,Giulio Turrisi,Tommaso Belvedere,Valerio Modugno*

Main category: cs.RO

TL;DR: BC-MPPI introduces a probabilistic layer to adjust trajectory sampling based on safety probabilities, improving control reliability in non-linear robotic tasks.


<details>
  <summary>Details</summary>
Motivation: Enhance Model Predictive Path Integral (MPPI) control's ability to reliably handle constraints in robotic systems.

Method: Utilize probabilistic surrogates for state and input constraints, trained via 1000 offline simulations, to guide trajectory selections in MPPI.

Result: BC-MPPI maintains safety margins effectively while adhering to specific violation probabilities in quadrotor tests with varied obstacles.

Conclusion: BC-MPPI improves MPPI control robustness and integrates well into validation systems for certifiable autonomous robots, promoting safer operations.

Abstract: Model Predictive Path Integral (MPPI) control has recently emerged as a fast,
gradient-free alternative to model-predictive control in highly non-linear
robotic tasks, yet it offers no hard guarantees on constraint satisfaction. We
introduce Bayesian-Constraints MPPI (BC-MPPI), a lightweight safety layer that
attaches a probabilistic surrogate to every state and input constraint. At each
re-planning step the surrogate returns the probability that a candidate
trajectory is feasible; this joint probability scales the weight given to a
candidate, automatically down-weighting rollouts likely to collide or exceed
limits and pushing the sampling distribution toward the safe subset; no
hand-tuned penalty costs or explicit sample rejection required. We train the
surrogate from 1000 offline simulations and deploy the controller on a
quadrotor in MuJoCo with both static and moving obstacles. Across K in
[100,1500] rollouts BC-MPPI preserves safety margins while satisfying the
prescribed probability of violation. Because the surrogate is a stand-alone,
version-controlled artefact and the runtime safety score is a single scalar,
the approach integrates naturally with verification-and-validation pipelines
for certifiable autonomous systems.

</details>


### [91] [Learning Human Reaching Optimality Principles from Minimal Observation Inverse Reinforcement Learning](https://arxiv.org/abs/2510.00329)
*Sarmad Mehrdad,Maxime Sabbah,Vincent Bonnet,Ludovic Righetti*

Main category: cs.RO

TL;DR: This study uses Minimal Observation Inverse Reinforcement Learning (MO-IRL) to model human arm-reaching movements, achieving more efficient learning and robust generalization compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: To better understand dynamic, time-varying cost structures in human motor control and enhance predictions of arm-reaching movements with potential implications for humanoid robotics.

Method: The paper employs MO-IRL with a planar two-link biomechanical model, high-resolution motion-capture data, and segmented phases of trajectories to refine cost weights using maximum entropy IRL.

Result: Training on limited data yields lower joint-angle RMSE compared to static weight methods, and cross-validation shows predictive accuracy of around 8 deg RMSE, demonstrating robust inter-subject generalization.

Conclusion: MO-IRL effectively uncovers dynamic cost structures in human motor control, offering efficient learning and potential applications in humanoid robotics.

Abstract: This paper investigates the application of Minimal Observation Inverse
Reinforcement Learning (MO-IRL) to model and predict human arm-reaching
movements with time-varying cost weights. Using a planar two-link biomechanical
model and high-resolution motion-capture data from subjects performing a
pointing task, we segment each trajectory into multiple phases and learn
phase-specific combinations of seven candidate cost functions. MO-IRL
iteratively refines cost weights by scaling observed and generated trajectories
in the maximum entropy IRL formulation, greatly reducing the number of required
demonstrations and convergence time compared to classical IRL approaches.
Training on ten trials per posture yields average joint-angle Root Mean Squared
Errors (RMSE) of 6.4 deg and 5.6 deg for six- and eight-segment weight
divisions, respectively, versus 10.4 deg using a single static weight.
Cross-validation on remaining trials and, for the first time, inter-subject
validation on an unseen subject's 20 trials, demonstrates comparable predictive
accuracy, around 8 deg RMSE, indicating robust generalization. Learned weights
emphasize joint acceleration minimization during movement onset and
termination, aligning with smoothness principles observed in biological motion.
These results suggest that MO-IRL can efficiently uncover dynamic,
subject-independent cost structures underlying human motor control, with
potential applications for humanoid robots.

</details>


### [92] [DiSA-IQL: Offline Reinforcement Learning for Robust Soft Robot Control under Distribution Shifts](https://arxiv.org/abs/2510.00358)
*Linjin He,Xinda Qi,Dong Chen,Zhaojian Li,Xiaobo Tan*

Main category: cs.RO

TL;DR: The paper introduces DiSA-IQL, a novel RL approach for soft snake robots, addressing distribution shift to enhance control performance.


<details>
  <summary>Details</summary>
Motivation: Control of soft snake robots is challenging due to nonlinear dynamics and distribution shifts in offline RL training methods.

Method: The authors propose DiSA-IQL, an enhancement to IQL, with robustness modulation through penalization of unreliable state-action pairs.

Result: DiSA-IQL outperforms other methods in goal-reaching tasks, showcasing higher success rates, smoother trajectories, and better robustness.

Conclusion: DiSA-IQL offers a promising solution for offline RL in soft robot control, demonstrating superior performance and practicality for unseen scenarios.

Abstract: Soft snake robots offer remarkable flexibility and adaptability in complex
environments, yet their control remains challenging due to highly nonlinear
dynamics. Existing model-based and bio-inspired controllers rely on simplified
assumptions that limit performance. Deep reinforcement learning (DRL) has
recently emerged as a promising alternative, but online training is often
impractical because of costly and potentially damaging real-world interactions.
Offline RL provides a safer option by leveraging pre-collected datasets, but it
suffers from distribution shift, which degrades generalization to unseen
scenarios. To overcome this challenge, we propose DiSA-IQL
(Distribution-Shift-Aware Implicit Q-Learning), an extension of IQL that
incorporates robustness modulation by penalizing unreliable state-action pairs
to mitigate distribution shift. We evaluate DiSA-IQL on goal-reaching tasks
across two settings: in-distribution and out-of-distribution evaluation.
Simulation results show that DiSA-IQL consistently outperforms baseline models,
including Behavior Cloning (BC), Conservative Q-Learning (CQL), and vanilla
IQL, achieving higher success rates, smoother trajectories, and improved
robustness. The codes are open-sourced to support reproducibility and to
facilitate further research in offline RL for soft robot control.

</details>


### [93] [Physics-Informed Neural Controlled Differential Equations for Scalable Long Horizon Multi-Agent Motion Forecasting](https://arxiv.org/abs/2510.00401)
*Shounak Sural,Charles Kekeh,Wenliang Liu,Federico Pecora,Mouhacine Benosman*

Main category: cs.RO

TL;DR: The paper develops a trajectory forecasting model based on neural Controlled Differential Equations (CDEs) for predicting multi-robot motion over extended periods, achieving scalable and accurate predictions.


<details>
  <summary>Details</summary>
Motivation: Accurate long-horizon motion forecasting for multi-agent systems is vital for applications like travel time prediction, planning, and simulation. However, challenges like non-linear interactions and prediction errors make it difficult.

Method: The proposed approach, PINCoDE, integrates neural Controlled Differential Equations (CDEs) with physics-informed constraints to model continuous-time dynamical systems. The model scales to handle up to 100 robots without extra parameters.

Result: The model achieves high prediction accuracy, with average deviation errors (ADE) below 0.5 m for a one-minute horizon and improved long-term trajectory predictions compared to analytical models.

Conclusion: PINCoDE demonstrates that physics-informed neural CDEs are effective for long-horizon multi-agent forecasting, offering scalability and accuracy improvements.

Abstract: Long-horizon motion forecasting for multiple autonomous robots is challenging
due to non-linear agent interactions, compounding prediction errors, and
continuous-time evolution of dynamics. Learned dynamics of such a system can be
useful in various applications such as travel time prediction,
prediction-guided planning and generative simulation. In this work, we aim to
develop an efficient trajectory forecasting model conditioned on multi-agent
goals. Motivated by the recent success of physics-guided deep learning for
partially known dynamical systems, we develop a model based on neural
Controlled Differential Equations (CDEs) for long-horizon motion forecasting.
Unlike discrete-time methods such as RNNs and transformers, neural CDEs operate
in continuous time, allowing us to combine physics-informed constraints and
biases to jointly model multi-robot dynamics. Our approach, named PINCoDE
(Physics-Informed Neural Controlled Differential Equations), learns
differential equation parameters that can be used to predict the trajectories
of a multi-agent system starting from an initial condition. PINCoDE is
conditioned on future goals and enforces physics constraints for robot motion
over extended periods of time. We adopt a strategy that scales our model from
10 robots to 100 robots without the need for additional model parameters, while
producing predictions with an average ADE below 0.5 m for a 1-minute horizon.
Furthermore, progressive training with curriculum learning for our PINCoDE
model results in a 2.7X reduction of forecasted pose error over 4 minute
horizons compared to analytical models.

</details>


### [94] [VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified Rewards in World Simulators](https://arxiv.org/abs/2510.00406)
*Hengtao Li,Pengxiang Ding,Runze Suo,Yihao Wang,Zirui Ge,Dongyuan Zang,Kexian Yu,Mingyang Sun,Hongyin Zhang,Donglin Wang,Weihua Su*

Main category: cs.RO

TL;DR: This paper introduces VLA-RFT, a reinforcement fine-tuning framework leveraging a data-driven world model to enhance the robustness and efficiency of Vision-Language-Action models against errors and distribution shifts.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of Vision-Language-Action models that rely heavily on imitation learning, leading to compounding errors and poor robustness, while mitigating the high cost and inefficiency of traditional reinforcement learning approaches.

Method: The method involves using a data-driven world model trained from real interaction data as a controllable simulator to predict visual observations. Dense trajectory-level rewards are provided for policy rollouts, enabling efficient learning with fewer samples.

Result: VLA-RFT outperforms strong supervised baselines with less than 400 fine-tuning steps, achieves greater efficiency than traditional simulator-based reinforcement learning, and maintains stable task execution under perturbed conditions.

Conclusion: World-model-based reinforcement fine-tuning is a practical post-training paradigm that enhances the generalization and robustness of Vision-Language-Action models, offering a significant improvement in efficiency and adaptability.

Abstract: Vision-Language-Action (VLA) models enable embodied decision-making but rely
heavily on imitation learning, leading to compounding errors and poor
robustness under distribution shift. Reinforcement learning (RL) can mitigate
these issues yet typically demands costly real-world interactions or suffers
from sim-to-real gaps. We introduce VLA-RFT, a reinforcement fine-tuning
framework that leverages a data-driven world model as a controllable simulator.
Trained from real interaction data, the simulator predicts future visual
observations conditioned on actions, allowing policy rollouts with dense,
trajectory-level rewards derived from goal-achieving references. This design
delivers an efficient and action-aligned learning signal, drastically lowering
sample requirements. With fewer than 400 fine-tuning steps, VLA-RFT surpasses
strong supervised baselines and achieves greater efficiency than
simulator-based RL. Moreover, it exhibits strong robustness under perturbed
conditions, sustaining stable task execution. Our results establish
world-model-based RFT as a practical post-training paradigm to enhance the
generalization and robustness of VLA models. For more details, please refer to
https://vla-rft.github.io/.

</details>


### [95] [Seeing through Uncertainty: Robust Task-Oriented Optimization in Visual Navigation](https://arxiv.org/abs/2510.00441)
*Yiyuan Pan,Yunzhe Xu,Zhe Liu,Hesheng Wang*

Main category: cs.RO

TL;DR: NeuRO proposes a novel framework combining visual perception with robust optimization for long-horizon and generalizable navigation tasks in embodied AI.


<details>
  <summary>Details</summary>
Motivation: Embodied AI projects suffer from data scarcity issues when tackling long-horizon, multi-objective navigation tasks. Current methods overfit and fail to generalize in unseen environments.

Method: NeuRO integrates Partially Input Convex Neural Networks (PICNNs) with robust optimization techniques, transforming noisy visual data into convex uncertainty sets and reformulating planning as an uncertainty-aware optimization problem.

Result: NeuRO delivers SoTA performance in both unordered and sequential multi-object navigation tasks, significantly improving generalization in unseen environments.

Conclusion: NeuRO advances robust and generalizable autonomous agents, addressing data limitation struggles in current visual navigation systems.

Abstract: Visual navigation is a fundamental problem in embodied AI, yet practical
deployments demand long-horizon planning capabilities to address
multi-objective tasks. A major bottleneck is data scarcity: policies learned
from limited data often overfit and fail to generalize OOD. Existing neural
network-based agents typically increase architectural complexity that
paradoxically become counterproductive in the small-sample regime. This paper
introduce NeuRO, a integrated learning-to-optimize framework that tightly
couples perception networks with downstream task-level robust optimization.
Specifically, NeuRO addresses core difficulties in this integration: (i) it
transforms noisy visual predictions under data scarcity into convex uncertainty
sets using Partially Input Convex Neural Networks (PICNNs) with conformal
calibration, which directly parameterize the optimization constraints; and (ii)
it reformulates planning under partial observability as a robust optimization
problem, enabling uncertainty-aware policies that transfer across environments.
Extensive experiments on both unordered and sequential multi-object navigation
tasks demonstrate that NeuRO establishes SoTA performance, particularly in
generalization to unseen environments. Our work thus presents a significant
advancement for developing robust, generalizable autonomous agents.

</details>


### [96] [Integrating Offline Pre-Training with Online Fine-Tuning: A Reinforcement Learning Approach for Robot Social Navigation](https://arxiv.org/abs/2510.00466)
*Run Su,Hao Fu,Shuai Zhou,Yingao Fu*

Main category: cs.RO

TL;DR: This paper proposes an algorithm using offline-to-online reinforcement learning (RL) to improve robot social navigation, enhancing adaptability and robustness.


<details>
  <summary>Details</summary>
Motivation: Addressing suboptimal exploration and distribution shifts in offline RL for robot navigation due to uncertainties in pedestrian behavior and limited training interaction.

Method: Introduced a spatiotemporal fusion model with causal Transformer for precise Return-to-Go (RTG) prediction, coupled with a hybrid offline-online experience sampling mechanism for fine-tuning.

Result: The proposed method showed a higher success rate and lower collision rate compared to state-of-the-art baselines in simulated social navigation environments.

Conclusion: The algorithm improves robustness and adaptability of robot navigation, promising advancements for real-world applications.

Abstract: Offline reinforcement learning (RL) has emerged as a promising framework for
addressing robot social navigation challenges. However, inherent uncertainties
in pedestrian behavior and limited environmental interaction during training
often lead to suboptimal exploration and distributional shifts between offline
training and online deployment. To overcome these limitations, this paper
proposes a novel offline-to-online fine-tuning RL algorithm for robot social
navigation by integrating Return-to-Go (RTG) prediction into a causal
Transformer architecture. Our algorithm features a spatiotem-poral fusion model
designed to precisely estimate RTG values in real-time by jointly encoding
temporal pedestrian motion patterns and spatial crowd dynamics. This RTG
prediction framework mitigates distribution shift by aligning offline policy
training with online environmental interactions. Furthermore, a hybrid
offline-online experience sampling mechanism is built to stabilize policy
updates during fine-tuning, ensuring balanced integration of pre-trained
knowledge and real-time adaptation. Extensive experiments in simulated social
navigation environments demonstrate that our method achieves a higher success
rate and lower collision rate compared to state-of-the-art baselines. These
results underscore the efficacy of our algorithm in enhancing navigation policy
robustness and adaptability. This work paves the way for more reliable and
adaptive robotic navigation systems in real-world applications.

</details>


### [97] [From Human Hands to Robot Arms: Manipulation Skills Transfer via Trajectory Alignment](https://arxiv.org/abs/2510.00491)
*Han Zhou,Jinjin Cao,Liyuan Ma,Xueji Fang,Guo-jun Qi*

Main category: cs.RO

TL;DR: Traj2Action is a framework that enables robots to learn manipulation skills by converting human 3D trajectories into robot-specific actions, bypassing the need for costly teleoperated demonstrations.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of transferring manipulation skills from humans to robots despite the morphological differences, thereby overcoming the limitations of expensive and hard-to-scale teleoperation methods.

Method: The framework uses the 3D trajectory of the operational endpoint as an intermediate representation. It first generates a coarse trajectory using human and robot data, and then refines robot-specific actions through a co-denoising approach.

Result: Real-world experiments on a Franka robot show that Traj2Action improves task performance by up to 27% and 22.25% over baseline policies on short- and long-horizon tasks. Additionally, it scales well with increasing human data.

Conclusion: Traj2Action successfully bridges the embodiment gap and leverages human video data to teach robots diverse manipulation skills efficiently, demonstrating scalability and improved performance in real-world scenarios.

Abstract: Learning diverse manipulation skills for real-world robots is severely
bottlenecked by the reliance on costly and hard-to-scale teleoperated
demonstrations. While human videos offer a scalable alternative, effectively
transferring manipulation knowledge is fundamentally hindered by the
significant morphological gap between human and robotic embodiments. To address
this challenge and facilitate skill transfer from human to robot, we introduce
Traj2Action,a novel framework that bridges this embodiment gap by using the 3D
trajectory of the operational endpoint as a unified intermediate
representation, and then transfers the manipulation knowledge embedded in this
trajectory to the robot's actions. Our policy first learns to generate a coarse
trajectory, which forms an high-level motion plan by leveraging both human and
robot data. This plan then conditions the synthesis of precise, robot-specific
actions (e.g., orientation and gripper state) within a co-denoising framework.
Extensive real-world experiments on a Franka robot demonstrate that Traj2Action
boosts the performance by up to 27% and 22.25% over $\pi_0$ baseline on short-
and long-horizon real-world tasks, and achieves significant gains as human data
scales in robot policy learning. Our project website, featuring code and video
demonstrations, is available at
https://anonymous.4open.science/w/Traj2Action-4A45/.

</details>


### [98] [Two stage GNSS outlier detection for factor graph optimization based GNSS-RTK/INS/odometer fusion](https://arxiv.org/abs/2510.00524)
*Baoshan Song,Penggao Yan,Xiao Xia,Yihan Zhong,Weisong Wen,Li-Ta Hsu*

Main category: cs.RO

TL;DR: The paper proposes a two-stage outlier detection method for GNSS positioning in complex environments using Doppler and IMU/odometer constraints, achieving significantly improved accuracy in urban scenarios.


<details>
  <summary>Details</summary>
Motivation: Enhance GNSS positioning reliability in challenging environments plagued by NLOS, multipath, and signal blockages, to optimize integrated navigation systems.

Method: A two-stage approach: Doppler measurements detect pseudo-range inconsistencies, then IMU and odometer constraints refine outlier detection using predicted double-difference values.

Result: The proposed method significantly reduced pseudo-range errors, improving positioning accuracy, achieving a 42.3% improvement in RMSE in deep urban canyon scenarios.

Conclusion: The framework enhances robustness and positioning consistency, marking a notable advancement over baseline methods in urban navigation challenges.

Abstract: Reliable GNSS positioning in complex environments remains a critical
challenge due to non-line-of-sight (NLOS) propagation, multipath effects, and
frequent signal blockages. These effects can easily introduce large outliers
into the raw pseudo-range measurements, which significantly degrade the
performance of global navigation satellite system (GNSS) real-time kinematic
(RTK) positioning and limit the effectiveness of tightly coupled GNSS-based
integrated navigation system. To address this issue, we propose a two-stage
outlier detection method and apply the method in a tightly coupled GNSS-RTK,
inertial navigation system (INS), and odometer integration based on factor
graph optimization (FGO). In the first stage, Doppler measurements are employed
to detect pseudo-range outliers in a GNSS-only manner, since Doppler is less
sensitive to multipath and NLOS effects compared with pseudo-range, making it a
more stable reference for detecting sudden inconsistencies. In the second
stage, pre-integrated inertial measurement units (IMU) and odometer constraints
are used to generate predicted double-difference pseudo-range measurements,
which enable a more refined identification and rejection of remaining outliers.
By combining these two complementary stages, the system achieves improved
robustness against both gross pseudo-range errors and degraded satellite
measuring quality. The experimental results demonstrate that the two-stage
detection framework significantly reduces the impact of pseudo-range outliers,
and leads to improved positioning accuracy and consistency compared with
representative baseline approaches. In the deep urban canyon test, the outlier
mitigation method has limits the RMSE of GNSS-RTK/INS/odometer fusion from 0.52
m to 0.30 m, with 42.3% improvement.

</details>


### [99] [GRITS: A Spillage-Aware Guided Diffusion Policy for Robot Food Scooping Tasks](https://arxiv.org/abs/2510.00573)
*Yen-Ling Tai,Yi-Ru Yang,Kuan-Ting Yu,Yu-Wei Chao,Yi-Ting Chen*

Main category: cs.RO

TL;DR: This paper presents GRITS, a framework using guided diffusion policy to minimize food spillage in robotic food scooping tasks, significantly improving task success and reliability.


<details>
  <summary>Details</summary>
Motivation: Current robot learning algorithms struggle with diverse and dynamic food states, often causing spillage and reliability issues during manipulation.

Method: The proposed method involves a guided diffusion policy aided by a spillage predictor trained on simulated datasets, which guides robot actions to reduce spillage while achieving task success.

Result: GRITS achieved an 82% task success rate and a 4% spillage rate in real-world tests, reducing spillage by over 40% compared to unguided baselines.

Conclusion: GRITS successfully demonstrates enhanced reliability and minimized spillage in robotic food scooping, making it a promising approach for food preparation and service robots.

Abstract: Robotic food scooping is a critical manipulation skill for food preparation
and service robots. However, existing robot learning algorithms, especially
learn-from-demonstration methods, still struggle to handle diverse and dynamic
food states, which often results in spillage and reduced reliability. In this
work, we introduce GRITS: A Spillage-Aware Guided Diffusion Policy for Robot
Food Scooping Tasks. This framework leverages guided diffusion policy to
minimize food spillage during scooping and to ensure reliable transfer of food
items from the initial to the target location. Specifically, we design a
spillage predictor that estimates the probability of spillage given current
observation and action rollout. The predictor is trained on a simulated dataset
with food spillage scenarios, constructed from four primitive shapes (spheres,
cubes, cones, and cylinders) with varied physical properties such as mass,
friction, and particle size. At inference time, the predictor serves as a
differentiable guidance signal, steering the diffusion sampling process toward
safer trajectories while preserving task success. We validate GRITS on a
real-world robotic food scooping platform. GRITS is trained on six food
categories and evaluated on ten unseen categories with different shapes and
quantities. GRITS achieves an 82% task success rate and a 4% spillage rate,
reducing spillage by over 40% compared to baselines without guidance, thereby
demonstrating its effectiveness.

</details>


### [100] [Hybrid Training for Vision-Language-Action Models](https://arxiv.org/abs/2510.00600)
*Pietro Mazzaglia,Cansu Sancaktar,Markus Peschl,Daniel Dijkman*

Main category: cs.RO

TL;DR: The paper introduces Hybrid Training (HyT), a framework aimed at enabling Vision-Language-Action models (VLAs) to benefit from chain-of-thought (CoT) strategies without requiring CoT generation during inference, thus improving usability and efficiency in robotic applications.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the inefficiency caused by the need for generating long chains-of-thought during the inference process in robotics, which negatively impacts action latency in real-world tasks.

Method: The researchers propose Hybrid Training (HyT), which trains VLAs to learn from CoT strategies while allowing inference-time flexibility. This includes diverse modes of action prediction such as direct action, thoughts generation, or instruction following.

Result: Experiments using simulation benchmarks as well as real-world tests showed that HyT improves performance while removing the bottleneck of extended inference time caused by CoT generation.

Conclusion: HyT offers a practical solution by enabling flexible inference methods in robotic systems, preserving the benefits of CoT strategies while improving real-world usability through reduced latency.

Abstract: Using Large Language Models to produce intermediate thoughts, a.k.a.
Chain-of-thought (CoT), before providing an answer has been a successful recipe
for solving complex language tasks. In robotics, similar embodied CoT
strategies, generating thoughts before actions, have also been shown to lead to
improved performance when using Vision-Language-Action models (VLAs). As these
techniques increase the length of the model's generated outputs to include the
thoughts, the inference time is negatively affected. Delaying an agent's
actions in real-world executions, as in robotic manipulation settings, strongly
affects the usability of a method, as tasks require long sequences of actions.
However, is the generation of long chains-of-thought a strong prerequisite for
achieving performance improvements? In this work, we explore the idea of Hybrid
Training (HyT), a framework that enables VLAs to learn from thoughts and
benefit from the associated performance gains, while enabling the possibility
to leave out CoT generation during inference. Furthermore, by learning to
conditionally predict a diverse set of outputs, HyT supports flexibility at
inference time, enabling the model to either predict actions directly, generate
thoughts or follow instructions. We evaluate the proposed method in a series of
simulated benchmarks and real-world experiments.

</details>


### [101] [What Did I Learn? Operational Competence Assessment for AI-Based Trajectory Planners](https://arxiv.org/abs/2510.00619)
*Michiel Braat,Maren Buermann,Marijke van Weperen,Jan-Pieter Paardekooper*

Main category: cs.RO

TL;DR: This paper proposes a novel method using knowledge graphs to evaluate the training data adequacy and explainability of machine learning models in automated driving.


<details>
  <summary>Details</summary>
Motivation: To ensure the safe and reliable functionality of machine learning models used in automated driving amidst a high dependence on relevant and task-matching datasets.

Method: Driving data is modeled as knowledge graphs, where driving scenes are represented by entities and their relationships. Sub-scene configurations are queried in the graphs to measure data coverage and complexity, estimating the model’s competence.

Result: The method was applied to the NuPlan dataset, enabling analysis of dataset coverage for specific driving scenes to monitor competence of trained machine learning models.

Conclusion: The approach aids in assessing dataset adequacy and improving model trustworthiness in automated driving through enhanced explainability and operational risk understanding.

Abstract: Automated driving functions increasingly rely on machine learning for tasks
like perception and trajectory planning, requiring large, relevant datasets.
The performance of these algorithms depends on how closely the training data
matches the task. To ensure reliable functioning, it is crucial to know what is
included in the dataset to assess the trained model's operational risk. We aim
to enhance the safe use of machine learning in automated driving by developing
a method to recognize situations that an automated vehicle has not been
sufficiently trained on. This method also improves explainability by describing
the dataset at a human-understandable level. We propose modeling driving data
as knowledge graphs, representing driving scenes with entities and their
relationships. These graphs are queried for specific sub-scene configurations
to check their occurrence in the dataset. We estimate a vehicle's competence in
a driving scene by considering the coverage and complexity of sub-scene
configurations in the training set. Higher complexity scenes require greater
coverage for high competence. We apply this method to the NuPlan dataset,
modeling it with knowledge graphs and analyzing the coverage of specific
driving scenes. This approach helps monitor the competence of machine learning
models trained on the dataset, which is essential for trustworthy AI to be
deployed in automated driving.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [102] [PBFD and PDFD: Formally Defined and Verified Methodologies and Empirical Evaluation for Scalable Full-Stack Software Engineering](https://arxiv.org/abs/2510.00002)
*Dong Liu*

Main category: cs.SE

TL;DR: The paper presents PBFD and PDFD, methods combining formal methods and real-world practices for scalable software engineering, employing innovative modeling and encoding for efficiency and correctness.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between formal methods and real-world software development practices, ensuring scalability, performance, and correctness.

Method: Introduces PBFD and PDFD using layered directed graphs, unified state machines, and CSP for structural correctness, along with TLE for constant-time data updates.

Result: PBFD showed over 20x faster development than Salesforce OmniScript and superior query performance, validated in an eight-year enterprise deployment.

Conclusion: PBFD and PDFD offer a formal, scalable, and efficient approach to software development, with open-source tools and datasets provided for broader adoption.

Abstract: This paper introduces Primary Breadth-First Development (PBFD) and Primary
Depth-First Development (PDFD), two formally defined and verified methodologies
for scalable, industrial-grade full-stack software engineering. These
approaches bridge a longstanding gap between formal methods and real-world
development practice by enforcing structural correctness through
graph-theoretic modeling. Unlike prior graph-based approaches, PBFD and PDFD
operate over layered directed graphs and are formalized using unified state
machines and Communicating Sequential Processes (CSP) to ensure critical
properties, including bounded-refinement termination and structural
completeness. To coordinate hierarchical data at scale, we propose Three-Level
Encapsulation (TLE) - a novel, bitmask-based encoding scheme that delivers
provably constant-time updates. TLE's formal guarantees underpin PBFD's
industrial-scale performance and scalability. PBFD was empirically validated
through an eight-year enterprise deployment, demonstrating over 20x faster
development than Salesforce OmniScript and 7-8x faster query performance
compared to conventional relational models. Additionally, both methodologies
are supported by open-source MVPs, with PDFD's implementation conclusively
demonstrating its correctness-first design principles. Together, PBFD and PDFD
establish a reproducible, transparent framework that integrates formal
verification into practical software development. All formal specifications,
MVPs, and datasets are publicly available to foster academic research and
industrial-grade adoption.

</details>


### [103] [Semantic Zoom and Mini-Maps for Software Cities](https://arxiv.org/abs/2510.00003)
*Malte Hansen,Jens Bamberg,Noe Baumann,Wilhelm Hasselbring*

Main category: cs.SE

TL;DR: This paper addresses visual scalability in 3D software visualization.


<details>
  <summary>Details</summary>
Motivation: To improve comprehension of large, complex software visualizations by tackling visual scalability challenges.

Method: Introduced semantic zooming and a 2D mini-map augmentation and showcased in ExplorViz.

Result: User studies show both techniques enhance usability, particularly for large systems; some limitations identified.

Conclusion: The approaches improve large-scale software exploration; future work will address identified limitations.

Abstract: Software visualization tools can facilitate program comprehension by
providing visual metaphors, or abstractions that reduce the amount of textual
data that needs to be processed mentally. One way they do this is by enabling
developers to build an internal representation of the visualized software and
its architecture. However, as the amount of displayed data in the visualization
increases, the visualization itself can become more difficult to comprehend.
The ability to display small and large amounts of data in visualizations is
called visual scalability.
  In this paper, we present two approaches to address the challenge of visual
scalability in 3D software cities. First, we present an approach to semantic
zoom, in which the graphical representation of the software landscape changes
based on the virtual camera's distance from visual objects. Second, we augment
the visualization with a miniature two-dimensional top-view projection called
mini-map. We demonstrate our approach using an open-source implementation in
our software visualization tool ExplorViz. ExplorViz is web-based and uses the
3D city metaphor, focusing on live trace visualization.
  We evaluated our approaches in two separate user studies. The results
indicate that semantic zoom and the mini-map are both useful additions. User
feedback indicates that semantic zoom and mini-maps are especially useful for
large software landscapes and collaborative software exploration. The studies
indicate a good usability of our implemented approaches. However, some
shortcomings in our implementations have also been discovered, to be addressed
in future work.
  Video URL: https://youtu.be/LYtUeWvizjU

</details>


### [104] [HTML Structure Exploration in 3D Software Cities](https://arxiv.org/abs/2510.00004)
*Malte Hansen,David Moreno-Lumbreras,Wilhelm Hasselbring*

Main category: cs.SE

TL;DR: This paper enhances the ExplorViz software visualization tool by integrating a 3D visualization of the HTML DOM for web-based applications, evaluated through a preliminary user study.


<details>
  <summary>Details</summary>
Motivation: Exploring and understanding large software systems often necessitates dynamic software visualization tools, especially for web interfaces which are typically overlooked.

Method: The researchers integrated an embedded web view and 3D representation of the HTML DOM for web interfaces into ExplorViz.

Result: A preliminary user study provided insights into the usability, advantages, and drawbacks of the proposed visualization approach.

Conclusion: The study suggests future research directions for improving web interface visualization and combining software cities with HTML structure for enhanced exploration.

Abstract: Software visualization, which uses data from dynamic program analysis, can
help to explore and understand the behavior of software systems. It is common
that large software systems offer a web interface for user interaction.
Usually, available web interfaces are not regarded in software visualization
tools. This paper introduces additions to the web-based live tracing software
visualization tool ExplorViz: We add an embedded web view for instrumented
applications in the 3D visualization to ease interaction with the given
applications and enable the exploration of the thereby displayed HTML content.
Namely, the Document Object Model (DOM) is visualized via a three-dimensional
representation of the HTML structure in same-origin contexts.
  Our visualization approach is evaluated in a preliminary user study. The
study results give insights into the potential use cases, benefits, and
shortcomings of our implemented approach. Based on our study results, we
propose directions for further research to support the visual exploration of
web interfaces and explore use cases for the combined visualization of software
cities and HTML structure.
  Video URL: https://youtu.be/wBWKlbvzOOE

</details>


### [105] [VibeCodeHPC: An Agent-Based Iterative Prompting Auto-Tuner for HPC Code Generation Using LLMs](https://arxiv.org/abs/2510.00031)
*Shun-ichiro Hayashi,Koki Morita,Daichi Mukunoki,Tetsuya Hoshino,Takahiro Katagiri*

Main category: cs.SE

TL;DR: VibeCodeHPC is an automatic tuning system for HPC programs using multi-agent LLMs for code generation, achieving better results compared to solo-agent systems.


<details>
  <summary>Details</summary>
Motivation: The paper aims to optimize HPC programs through effective collaboration among multiple agents, addressing limitations in solo-agent tuning systems.

Method: The system design involves four roles: Project Manager, System Engineer, Programmer, and Continuous Delivery, with dynamic agent deployment and activity monitoring.

Result: VibeCodeHPC demonstrated higher-quality code generation per unit time and better identification of issues compared to solo-agent systems, as tested on matrix-matrix multiplication code conversion to CUDA.

Conclusion: The proposed system enhances the efficiency and effectiveness of HPC code tuning, emphasizing the benefits of a multi-agent approach.

Abstract: We propose VibeCodeHPC, an automatic tuning system for HPC programs based on
multi-agent LLMs for code generation. VibeCodeHPC tunes programs through
multi-agent role allocation and iterative prompt refinement. We describe the
system configuration with four roles: Project Manager (PM), System Engineer
(SE), Programmer (PG), and Continuous Delivery (CD). We introduce dynamic agent
deployment and activity monitoring functions to facilitate effective
multi-agent collaboration. In our case study, we convert and optimize CPU-based
matrix-matrix multiplication code written in C to GPU code using CUDA. The
multi-agent configuration of VibeCodeHPC achieved higher-quality code
generation per unit time compared to a solo-agent configuration. Additionally,
the dynamic agent deployment and activity monitoring capabilities facilitated
more effective identification of requirement violations and other issues.

</details>


### [106] [A Scalable Framework for Safety Assurance of Self-Driving Vehicles based on Assurance 2.0](https://arxiv.org/abs/2510.00092)
*Shufeng Chen,Mariat James Elizebeth,Robab Aghazadeh Chakherlou,Xingyu Zhao,Eric Barbier,Siddartha Khastgir,Paul Jennings*

Main category: cs.SE

TL;DR: The paper develops Assurance 2.0, a framework to manage safety argumentation for complex systems and demonstrates its application in self-driving vehicle development.


<details>
  <summary>Details</summary>
Motivation: Address challenges in assuring safety of advanced systems like autonomous vehicles, especially complexities surrounding evidence and transparency.

Method: Implements a structured decomposition framework using a three-tier strategy grounded in Assurance 2.0, supported by 5M1E model adaptations for safety assurance.

Result: Case study of the framework applied to self-driving vehicle development, showcasing its traceability across safety-critical lifecycle phases.

Conclusion: The framework ensures detailed and transparent safety assessment, though challenges in automation and bias management still remain.

Abstract: Assurance 2.0 is a modern framework developed to address the assurance
challenges of increasingly complex, adaptive, and autonomous systems. Building
on the traditional Claims-Argument-Evidence (CAE) model, it introduces reusable
assurance theories and explicit counterarguments (defeaters) to enhance rigor,
transparency, and adaptability. It supports continuous, incremental assurance,
enabling innovation without compromising safety. However, limitations persist
in confidence measurement, residual doubt management, automation support, and
the practical handling of defeaters and confirmation bias. This paper presents
\textcolor{black}{a set of decomposition frameworks to identify a complete set
of safety arguments and measure their corresponding evidence.} Grounded in the
Assurance 2.0 paradigm, the framework is instantiated through a structured
template and employs a three-tiered decomposition strategy. \textcolor{black}{A
case study regarding the application of the decomposition framework in the
end-to-end (E2E) AI-based Self-Driving Vehicle (SDV) development is also
presented in this paper.} At the top level, the SDV development is divided into
three critical phases: Requirements Engineering (RE), Verification and
Validation (VnV), and Post-Deployment (PD). Each phase is further decomposed
according to its Product Development Lifecycle (PDLC). To ensure comprehensive
coverage, each PDLC is analyzed using an adapted 5M1E model (Man, Machine,
Method, Material, Measurement, and Environment). Originally developed for
manufacturing quality control, the 5M1E model is reinterpreted and contextually
mapped to the assurance domain. This enables a multi-dimensional decomposition
that supports fine-grained traceability of safety claims, evidence, and
potential defeaters.

</details>


### [107] [Container Orchestration Patterns for Optimizing Resource Use](https://arxiv.org/abs/2510.00197)
*Diogo Maia,Filipe Correia,André Restivo,Paulo Queiroz*

Main category: cs.SE

TL;DR: This paper identifies and defines three key orchestration optimization patterns to address challenges in service-based architectures.


<details>
  <summary>Details</summary>
Motivation: Service orchestration in service-based architectures is challenging, especially for newcomers, due to a lack of clear, standardized resources on best practices.

Method: The authors analyzed existing literature and tools to identify common service orchestration practices and formalized three optimization patterns.

Result: Three orchestration resource optimization patterns were defined: Preemptive Scheduling, Service Balancing, and Garbage Collection, aimed at enhancing resource allocation, usage, and cleanup efficiency.

Conclusion: These patterns provide a foundational framework for improving service orchestration practices and encouraging adoption in the software industry.

Abstract: Service-based architectures provide substantial benefits, yet service
orchestration remains a challenge, particularly for newcomers. While various
resources on orchestration techniques exist, they often lack clarity and
standardization, making best practices difficult to implement and limiting
their adoption within the software industry.
  To address this gap, we analyzed existing literature and tools to identify
common orchestration practices. Based on our findings, we define three key
orchestration resource optimization patterns: {\sc Preemptive Scheduling}, {\sc
Service Balancing}, and {\sc Garbage Collection}. {\sc Preemptive Scheduling}
allows the allocation of sufficient resources for services of higher priority
in stressful situations, while {\sc Service Balancing} enables a restructuring
of the nodes to allow better resource usage. To end, {\sc Garbage Collection}
creates cleanup mechanisms to better understand the system's resource usage and
optimize it. These patterns serve as foundational elements for improving
orchestration practices and fostering broader adoption in service-based
architectures.

</details>


### [108] [Which Programming Language and Model Work Best With LLM-as-a-Judge For Code Retrieval?](https://arxiv.org/abs/2510.00324)
*Lucas Roberts,Denisa Roberts*

Main category: cs.SE

TL;DR: The study explores the use of Large Language Models (LLMs) for code search and annotations, emphasizing the role of retriever representation, programming languages (PL), and human-AI relevance alignment.


<details>
  <summary>Details</summary>
Motivation: Code search faces challenges due to the high cost of human annotations for programming queries, requiring specialized programming knowledge and domain-specific expertise.

Method: The paper studies LLMs to retrieve code functions, generate annotations, and evaluates retrieval methods (sparse vs semantic), programming languages, and human-LLM alignment using human annotations.

Result: Significant differences were found in representation methods (sparse vs semantic) and programming languages affecting human-AI relevance alignment. Transpilers were proposed to scale benchmarks across programming languages.

Conclusion: The study demonstrated that LLMs, retriever representation methods, and programming language-specific adjustments can enhance code search performance, achieving human-AI relevance agreement comparable to human-human agreement.

Abstract: Code search is an important information retrieval application. Benefits of
better code search include faster new developer on-boarding, reduced software
maintenance, and ease of understanding for large repositories. Despite
improvements in search algorithms and search benchmarks, the domain of code
search has lagged behind. One reason is the high cost of human annotation for
code queries and answers. While humans may annotate search results in general
text QA systems, code annotations require specialized knowledge of a
programming language (PL), as well as domain specific software engineering
knowledge. In this work we study the use of Large Language Models (LLMs) to
retrieve code at the level of functions and to generate annotations for code
search results. We compare the impact of the retriever representation (sparse
vs. semantic), programming language, and LLM by comparing human annotations
across several popular languages (C, Java, Javascript, Go, and Python). We
focus on repositories that implement common data structures likely to be
implemented in any PLs. For the same human annotations, we compare several
LLM-as-a-Judge models to evaluate programming language and other affinities
between LLMs. We find that the chosen retriever and PL exhibit affinities that
can be leveraged to improve alignment of human and AI relevance determinations,
with significant performance implications. We also find differences in
representation (sparse vs. semantic) across PLs that impact alignment of human
and AI relevance determinations. We propose using transpilers to bootstrap
scalable code search benchmark datasets in other PLs and in a case study
demonstrate that human-AI relevance agreement rates largely match the (worst
case) human-human agreement under study. The application code used in this work
is available at \href{https://github.com/rlucas7/code-searcher/}{this github
repo}.

</details>


### [109] [Vibe Coding in Practice: Motivations, Challenges, and a Future Outlook -- a Grey Literature Review](https://arxiv.org/abs/2510.00328)
*Ahmed Fawzy,Amjed Tahir,Kelly Blincoe*

Main category: cs.SE

TL;DR: "Vibe coding" is a new phenomenon in AI-assisted software development, favoring speed and accessibility over understanding and quality assurance. However, its reliance on AI code generation tools creates risks of flawed and unreliable software.


<details>
  <summary>Details</summary>
Motivation: To understand why developers adopt vibe coding, the challenges they face, and how they approach quality assurance in AI-assisted development environments.

Method: A systematic grey literature review analyzing 101 practitioner sources to gather 518 firsthand behavioral accounts of vibe coding practices, challenges, and limitations.

Result: Developers using vibe coding report a speed-quality trade-off where they value rapid results but often find the output flawed. Quality assurance practices such as testing are frequently neglected, creating risks for software reliability and maintainability.

Conclusion: Vibe coding eases barriers and accelerates prototyping but raises concerns about software quality and debugging. Understanding and addressing its risks are necessary to responsibly scale its adoption in software development.

Abstract: AI code generation tools are transforming software development, especially
for novice and non-software developers, by enabling them to write code and
build applications faster and with little to no human intervention. Vibe coding
is the practice where users rely on AI code generation tools through intuition
and trial-and-error without necessarily understanding the underlying code.
Despite widespread adoption, no research has systematically investigated why
users engage in vibe coding, what they experience while doing so, and how they
approach quality assurance (QA) and perceive the quality of the AI-generated
code. To this end, we conduct a systematic grey literature review of 101
practitioner sources, extracting 518 firsthand behavioral accounts about vibe
coding practices, challenges, and limitations. Our analysis reveals a
speed-quality trade-off paradox, where vibe coders are motivated by speed and
accessibility, often experiencing rapid ``instant success and flow'', yet most
perceive the resulting code as fast but flawed. QA practices are frequently
overlooked, with many skipping testing, relying on the models' or tools'
outputs without modification, or delegating checks back to the AI code
generation tools. This creates a new class of vulnerable software developers,
particularly those who build a product but are unable to debug it when issues
arise. We argue that vibe coding lowers barriers and accelerates prototyping,
but at the cost of reliability and maintainability. These insights carry
implications for tool designers and software development teams. Understanding
how vibe coding is practiced today is crucial for guiding its responsible use
and preventing a broader QA crisis in AI-assisted development.

</details>


### [110] [Beyond Pass/Fail: The Story of Learning-Based Testing](https://arxiv.org/abs/2510.00450)
*Sheikh Md. Mushfiqur Rahman,Nasir Eisty*

Main category: cs.SE

TL;DR: The paper conducts a systematic literature review on Learning-Based Testing (LBT), focusing on theoretical frameworks, tools, and applications, especially in industrial settings.


<details>
  <summary>Details</summary>
Motivation: The motivation is to highlight the potential of Learning-Based Testing (LBT) to enhance software testing by combining learning and testing processes for scalable and effective solutions.

Method: The authors review existing theoretical research, tools, libraries, frameworks, and case studies involving the application of LBT, analyzing its evolution and practical implementations.

Result: The paper identifies the efficacy of LBT in testing procedural and reactive programs, its scalability, and showcases examples where LBT was successfully implemented in industrial environments.

Conclusion: LBT is presented as a promising software testing method with significant scope for broader adoption and further development within research and industry.

Abstract: Learning-Based Testing (LBT) merges learning and testing processes to achieve
both testing and behavioral adequacy. LBT utilizes active learning to infer the
model of the System Under Test (SUT), enabling scalability for large and
complex programs by requiring only a minimal set of initial test cases. The
core principle of LBT is that the SUT's behavior can be thoroughly inferred by
progressively generating test cases and subjecting the SUT to testing, thereby
ensuring comprehensive testing. Despite being in its early stages, LBT has a
solid foundation of theoretical research demonstrating its efficacy in testing
both procedural and reactive programs. This paper provides a systematic
literature review of various LBT implementations across different program types
and evaluates the current state of research in this field. We explore diverse
theoretical frameworks, existing tools, and libraries within the LBT domain to
illustrate the concept's evolution and current research status. Additionally,
we examine case studies involving the application of LBT tools in industrial
settings, highlighting their potential and effectiveness in commercial software
testing. This systematic literature review aims to offer researchers a
comprehensive perspective on the inception and development of LBT, presenting
it as a promising technique in software testing. By unveiling LBT's
underutilized potential, this paper seeks to significantly benefit the
practitioners and research community.

</details>


### [111] [Analyzing Latent Concepts in Code Language Models](https://arxiv.org/abs/2510.00476)
*Arushi Sharma,Vedant Pungliya,Christopher J. Quinn,Ali Jannesari*

Main category: cs.SE

TL;DR: The paper presents Code Concept Analysis (CoCoA), a method for interpreting code language models by clustering contextualized token embeddings into understandable groups.


<details>
  <summary>Details</summary>
Motivation: Understanding internal behaviors of code-trained language models is crucial for transparency, trust, and improved semantic robustness.

Method: CoCoA uses post-hoc analysis and combines syntactic alignment with prompt-engineered annotations to identify and label latent concepts.

Result: Experiments show CoCoA discovers stable and predictable concepts, with its explanations providing improved interpretability (37% improvement in a user study).

Conclusion: CoCoA enhances human-centric explainability and aids understanding of latent interactions in code-focused models.

Abstract: Interpreting the internal behavior of large language models trained on code
remains a critical challenge, particularly for applications demanding trust,
transparency, and semantic robustness. We propose Code Concept Analysis
(CoCoA): a global post-hoc interpretability framework that uncovers emergent
lexical, syntactic, and semantic structures in a code language model's
representation space by clustering contextualized token embeddings into
human-interpretable concept groups. We propose a hybrid annotation pipeline
that combines static analysis tool-based syntactic alignment with
prompt-engineered large language models (LLMs), enabling scalable labeling of
latent concepts across abstraction levels. We analyse the distribution of
concepts across layers and across three finetuning tasks. Emergent concept
clusters can help identify unexpected latent interactions and be used to
identify trends and biases within the model's learned representations. We
further integrate LCA with local attribution methods to produce
concept-grounded explanations, improving the coherence and interpretability of
token-level saliency. Empirical evaluations across multiple models and tasks
show that LCA discovers concepts that remain stable under semantic-preserving
perturbations (average Cluster Sensitivity Index, CSI = 0.288) and evolve
predictably with fine-tuning. In a user study, concept-augmented explanations
disambiguate token roles. In a user study on the programming-language
classification task, concept-augmented explanations disambiguated token roles
and improved human-centric explainability by 37 percentage points compared with
token-level attributions using Integrated Gradients.

</details>


### [112] [CodeChemist: Functional Knowledge Transfer for Low-Resource Code Generation via Test-Time Scaling](https://arxiv.org/abs/2510.00501)
*Kaixin Wang,Tianlin Li,Xiaoyu Zhang,Aishan Liu,Xianglong Liu,Ziqi Liu,Zhiqiang Zhang,Jun Zhou,and Bin Shi*

Main category: cs.SE

TL;DR: CodeChemist framework improves code generation for low-resource programming languages (PLs) by transferring functional knowledge from high-resource PLs using test cases, without model retraining.


<details>
  <summary>Details</summary>
Motivation: CodeLLMs often perform inconsistently across programming languages, especially for low-resource PLs due to limited training data.

Method: CodeChemist generates test cases by executing code in high-resource PLs and uses multi-temperature hedged sampling to select the best code outputs in low-resource PLs based on test case pass rates.

Result: Experiments demonstrate CodeChemist's superior performance compared to existing test-time scaling methods, enhancing code generation for low-resource PLs.

Conclusion: CodeChemist efficiently bridges the resource gap between programming languages, enabling improved code generation for low-resource PLs without revising the underlying model.

Abstract: Code Large Language Models (CodeLLMs) are increasingly used in code
generation tasks across a wide range of applications. However, their
performance is often inconsistent across different programming languages (PLs),
with low-resource PLs suffering the most due to limited training data. In this
paper, we present CodeChemist, a novel and efficient framework for test-time
scaling that enables functional knowledge transfer from high-resource to
low-resource PLs using generated test cases. CodeChemist first generates and
executes code in high-resource PLs to create test cases that encapsulate
functional knowledge. It then uses multi-temperature hedged sampling to
generate code snippets in the low-resource PL and selects the best one based on
the pass rate of the test cases. Our extensive experiments show that
CodeChemist outperforms existing test-time scaling approaches, boosting the
performance of code generation for low-resource PLs without requiring any model
retraining.

</details>


### [113] [Architectural Transformations and Emerging Verification Demands in AI-Enabled Cyber-Physical Systems](https://arxiv.org/abs/2510.00519)
*Hadiza Umar Yusuf,Khouloud Gaaloul*

Main category: cs.SE

TL;DR: The paper explores the implications of AI integration on CPS architecture, operational complexity, and verification practices, comparing AI-driven versus traditional models in Simulink.


<details>
  <summary>Details</summary>
Motivation: To address the gap in understanding the impacts of AI integration on CPS structures, operations, and verification processes.

Method: Investigated architectural differences between AI-driven and traditional control models utilizing Simulink.

Result: Provided insights into the distinct implications of AI-based control models compared to traditional ones, especially concerning system verification.

Conclusion: AI integration introduces adaptability but also new complexity in CPS, necessitating revised verification approaches and understanding of architectural differences.

Abstract: In the world of Cyber-Physical Systems (CPS), a captivating real-time fusion
occurs where digital technology meets the physical world. This synergy has been
significantly transformed by the integration of artificial intelligence (AI), a
move that dramatically enhances system adaptability and introduces a layer of
complexity that impacts CPS control optimization and reliability. Despite
advancements in AI integration, a significant gap remains in understanding how
this shift affects CPS architecture, operational complexity, and verification
practices. The extended abstract addresses this gap by investigating
architectural distinctions between AI-driven and traditional control models
designed in Simulink and their respective implications for system verification.

</details>


### [114] [LSPFuzz: Hunting Bugs in Language Servers](https://arxiv.org/abs/2510.00532)
*Hengcheng Zhu,Songqiang Chen,Valerio Terragni,Lili Wei,Jiarong Wu,Yepang Liu,Shing-Chi Cheung*

Main category: cs.SE

TL;DR: This paper introduces LSPFuzz, a testing tool designed to detect bugs in Language Server Protocol (LSP) servers through a hybrid fuzzer approach.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the growing concerns around reliability and security in LSP servers, which can impact developer productivity and pose risks when handling untrusted code.

Method: LSPFuzz employs a two-stage mutation pipeline: first, syntax-aware mutations to source code, followed by context-aware dispatching of editor operations, enabling a systematic exploration of the input space.

Result: Evaluation on four widely used LSP servers revealed 51 bugs, out of which 42 were confirmed, 26 fixed, and 2 assigned CVE numbers. LSPFuzz outperformed baseline fuzzers.

Conclusion: LSPFuzz enhances the quality assurance of LSP servers, offering both a practical tool for developers and insights for future research in LSP server reliability and security.

Abstract: The Language Server Protocol (LSP) has revolutionized the integration of code
intelligence in modern software development. There are approximately 300 LSP
server implementations for various languages and 50 editors offering LSP
integration. However, the reliability of LSP servers is a growing concern, as
crashes can disable all code intelligence features and significantly impact
productivity, while vulnerabilities can put developers at risk even when
editing untrusted source code. Despite the widespread adoption of LSP, no
existing techniques specifically target LSP server testing. To bridge this gap,
we present LSPFuzz, a grey-box hybrid fuzzer for systematic LSP server testing.
Our key insight is that effective LSP server testing requires holistic mutation
of source code and editor operations, as bugs often manifest from their
combinations. To satisfy the sophisticated constraints of LSP and effectively
explore the input space, we employ a two-stage mutation pipeline: syntax-aware
mutations to source code, followed by context-aware dispatching of editor
operations. We evaluated LSPFuzz on four widely used LSP servers. LSPFuzz
demonstrated superior performance compared to baseline fuzzers, and uncovered
previously unknown bugs in real-world LSP servers. Of the 51 bugs we reported,
42 have been confirmed, 26 have been fixed by developers, and two have been
assigned CVE numbers. Our work advances the quality assurance of LSP servers,
providing both a practical tool and foundational insights for future research
in this domain.

</details>


### [115] [AI-Driven Self-Evolving Software: A Promising Path Toward Software Automation](https://arxiv.org/abs/2510.00591)
*Liyi Cai,Yijie Ren,Yitong Zhang,Jia Li*

Main category: cs.SE

TL;DR: The paper introduces AI-Driven Self-Evolving Software, a new form designed to autonomously evolve through user interactions without human developers. The researchers showcase a prototype capable of interpreting requirements, generating code, and handling functionalities.


<details>
  <summary>Details</summary>
Motivation: Existing AI in software development primarily assists human developers, and the paper explores stepping beyond to achieve fully autonomous software development.

Method: The authors developed a lightweight multi-agent architecture prototype that can autonomously process user requirements, create and validate code, and implement new features.

Result: The prototype demonstrated reliable performance in multiple case studies, showing promise for scaling to more complex tasks while highlighting the potential for automating software development.

Conclusion: AI-Driven Self-Evolving Software offers evidence that software can evolve autonomously, paving the way for fully automated software development with minimal human intervention.

Abstract: Software automation has long been a central goal of software engineering,
striving for software development that proceeds without human intervention.
Recent efforts have leveraged Artificial Intelligence (AI) to advance software
automation with notable progress. However, current AI functions primarily as
assistants to human developers, leaving software development still dependent on
explicit human intervention. This raises a fundamental question: Can AI move
beyond its role as an assistant to become a core component of software, thereby
enabling genuine software automation? To investigate this vision, we introduce
AI-Driven Self-Evolving Software, a new form of software that evolves
continuously through direct interaction with users. We demonstrate the
feasibility of this idea with a lightweight prototype built on a multi-agent
architecture that autonomously interprets user requirements, generates and
validates code, and integrates new functionalities. Case studies across
multiple representative scenarios show that the prototype can reliably
construct and reuse functionality, providing early evidence that such software
systems can scale to more sophisticated applications and pave the way toward
truly automated software development. We make code and cases in this work
publicly available at https://anonymous.4open.science/r/live-software.

</details>


### [116] [PyTrim: A Practical Tool for Reducing Python Dependency Bloat](https://arxiv.org/abs/2510.00674)
*Konstantinos Karakatsanis,Georgios Alexopoulos,Ioannis Karyotakis,Foivos Timotheos Proestakis,Evangelos Talos,Panos Louridas,Dimitris Mitropoulos*

Main category: cs.SE

TL;DR: The paper introduces PYTRIM, an automated system for removing unused dependencies in Python projects. It improves dependency management accuracy and reduces manual efforts.


<details>
  <summary>Details</summary>
Motivation: Dependency bloat in Python projects increases maintenance and security issues, necessitating tools for efficient detection and removal of unused dependencies.

Method: PYTRIM automates the detection and removal of unused imports and package declarations in Python source and configuration files. It incorporates a dynamic analysis component to improve recall and is designed modularly for compatibility with any detection tool.

Result: Tests on human-made changes show PYTRIM achieves 98.3% accuracy. Out of 971 tested open-source packages, 39 had bloated dependencies removed, with 6 pull requests being merged.

Conclusion: PYTRIM effectively automates the removal of dependency bloat, facilitates reproducibility, and is provided as an open-source tool for community use and advancement.

Abstract: Dependency bloat is a persistent challenge in Python projects, which
increases maintenance costs and security risks. While numerous tools exist for
detecting unused dependencies in Python, removing these dependencies across the
source code and configuration files of a project requires manual effort and
expertise.
  To tackle this challenge we introduce PYTRIM, an end-to-end system to
automate this process. PYTRIM eliminates unused imports and package
declarations across a variety of file types, including Python source and
configuration files such as requirements.txt and setup.py. PYTRIM's modular
design makes it agnostic to the source of dependency bloat information,
enabling integration with any detection tool. Beyond its contribution when it
comes to automation, PYTRIM also incorporates a novel dynamic analysis
component that improves dependency detection recall.
  Our evaluation of PYTRIM's end-to-end effectiveness on a ground-truth dataset
of 37 merged pull requests from prior work, shows that PYTRIM achieves 98.3%
accuracy in replicating human-made changes. To show its practical impact, we
run PYTRIM on 971 open-source packages, identifying and trimming bloated
dependencies in 39 of them. For each case, we submit a corresponding pull
request, 6 of which have already been accepted and merged. PYTRIM is available
as an open-source project, encouraging community contributions and further
development.
  Video demonstration: https://youtu.be/LqTEdOUbJRI
  Code repository: https://github.com/TrimTeam/PyTrim

</details>


### [117] [TShape: Rescuing Machine Learning Models from Complex Shapelet Anomalies](https://arxiv.org/abs/2510.00680)
*Hang Cui,Jingjing Li,Haotian Si,Quan Zhou,Changhua Pei,Gaogang Xie,Dan Pei*

Main category: cs.SE

TL;DR: The paper introduces TShape, a framework for time series anomaly detection (TSAD), particularly adept at identifying complex shapelet anomalies that are difficult for existing methods to detect.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need to improve anomaly detection in IT systems, where existing methods struggle with complex shape deviations that are obvious to humans but challenging for machine learning models.

Method: TShape employs a patch-wise dual attention mechanism combined with multi-scale convolution, striking a balance between fine-grained local features and global contextual information.

Result: TShape outperforms state-of-the-art methods by achieving an average 10% higher F1 score on five benchmarks, proving its robustness and adaptability in detecting shapelet anomalies.

Conclusion: The approach not only advances TSAD but also shows each component's importance through ablation studies and attention visualizations, confirming TShape's capability in complex scenarios.

Abstract: Time series anomaly detection (TSAD) is critical for maintaining the
reliability of modern IT infrastructures, where complex anomalies frequently
arise in highly dynamic environments. In this paper, we present TShape, a novel
framework designed to address the challenges in industrial time series anomaly
detection. Existing methods often struggle to detect shapelet anomalies that
manifest as complex shape deviations, which appear obvious to human experts but
prove challenging for machine learning algorithms. TShape introduces a
patch-wise dual attention mechanism with multi-scale convolution to model
intricate sub-sequence variations by balancing local, fine-grained shape
features with global contextual dependencies. Our extensive evaluation on five
diverse benchmarks demonstrates that TShape outperforms existing
state-of-the-art models, achieving an average 10\% F1 score improvement in
anomaly detection. Additionally, ablation studies and attention visualizations
confirm the essential contributions of each component, highlighting the
robustness and adaptability of TShape to complex shapelet shapes in time series
data.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [118] [Computational Advances in Taste Perception: From Ion Channels to Neural Coding](https://arxiv.org/abs/2510.00010)
*Vladimir A. Lazovsky,Sergey V. Stasenko,Victor B. Kazantsev*

Main category: q-bio.NC

TL;DR: This paper introduces a hybrid computational model combining Hodgkin-Huxley biophysical dynamics with Izhikevich spiking neurons for scalable and biologically faithful gustatory system simulations.


<details>
  <summary>Details</summary>
Motivation: The goal is to create models that maintain biophysical realism while handling the scalability demands of simulating large neural networks.

Method: The model integrates Hodgkin-Huxley dynamics for taste receptor cells with Izhikevich spiking neurons, incorporating biomorphic taste cell receptor dynamics, Goldman-Hodgkin-Katz ion currents, glutamate-driven synaptic interactions, and mechanisms like STDP. Network-level optimization focuses on temporal spike synchrony and combinatorial population coding.

Result: The framework bridges detailed single-cell biophysics with ensemble-level neural computation, allowing for efficient and biologically accurate simulations of taste pathways.

Conclusion: The study successfully combines biophysical accuracy with computational efficiency, providing a scalable and biologically realistic platform for simulating gustatory systems.

Abstract: Recent advances in computational neuroscience demand models that balance
biophysical realism with scalability. We present a hybrid neuron model
combining the biophysical fidelity of Hodgkin-Huxley (HH) dynamics for taste
receptor cells with the computational efficiency of Izhikevich spiking neurons
for large-network simulations. Our framework incorporates biomorphic taste cell
models, featuring modality-specific receptor dynamics (T1R/T2R, ENaC, PKD) and
Goldman-Hodgkin-Katz (GHK)-driven ion currents to accurately simulate gustatory
transduction. Synaptic interactions are modeled via glutamate release kinetics
with alpha-function profiles, AMPA receptor trafficking regulated by
phosphorylation, and spike-timing-dependent plasticity (STDP) to enforce
temporal coding. At the network level, we optimize multiscale learning,
leveraging both temporal spike synchrony (van Rossum metrics) and combinatorial
population coding (rank-order patterns). This approach bridges single-cell
biophysics with ensemble-level computation, enabling efficient simulation of
gustatory pathways while retaining biological fidelity.

</details>


### [119] [Robust State-space Reconstruction of Brain Dynamics via Bootstrap Monte Carlo SSA](https://arxiv.org/abs/2510.00011)
*Sir-Lord Wiafe,Carter Hinsley,Vince D. Calhoun*

Main category: q-bio.NC

TL;DR: The abstract focuses on improving the reconstruction of latent state-space geometry from noisy and short time series, particularly in neuroimaging, using a novel bootstrap Monte Carlo SSA approach.


<details>
  <summary>Details</summary>
Motivation: Understanding nonlinear dynamics in complex systems often requires reconstructing latent state-space geometry from time series data. However, certain conditions like short, noisy recordings pose a challenge for traditional methods, especially in applications like neuroimaging.

Method: The authors propose a bootstrap Monte Carlo Singular Spectrum Analysis (BMC-SSA) approach, accompanied by a red-noise null and bootstrap stability mechanisms. This method filters out noise while retaining oscillatory modes that consistently exceed noise levels, thereby stabilizing the embeddings.

Result: The proposed BMC-SSA approach enhances determinism and stabilizes embeddings in noisy and finite signals, particularly in fMRI data, making functional measures more reliable and uncovering differences in state-space dynamics.

Conclusion: BMC-SSA provides a robust framework for embedding noisy, finite signals, improving the reliability of functional measures and enabling the study of state-space dynamics in neuroimaging and other applications.

Abstract: Reconstructing latent state-space geometry from time series provides a
powerful route to studying nonlinear dynamics across complex systems.
Delay-coordinate embedding provides the theoretical basis but assumes long,
noise-free recordings, which many domains violate. In neuroimaging, for
example, fMRI is short and noisy; low sampling and strong red noise obscure
oscillations and destabilize embeddings. We propose bootstrap Monte Carlo SSA
with a red-noise null and bootstrap stability to retain only oscillatory modes
that reproducibly exceed noise. This produces reconstructions that are
red-noise-robust and mode-robust, enhancing determinism and stabilizing
subsequent embeddings. Our results show that BMC-SSA improves the reliability
of functional measures and uncovers differences in state-space dynamics in
fMRI, offering a general framework for robust embeddings of noisy, finite
signals.

</details>


### [120] [Evolutionary Kuramoto dynamics unravels origins of chimera states in neural populations](https://arxiv.org/abs/2510.00423)
*Thomas Zdyrski,Scott Pauls,Feng Fu*

Main category: q-bio.NC

TL;DR: The paper explores chimera states in neural synchronization using computational models that incorporate evolutionary game theory.


<details>
  <summary>Details</summary>
Motivation: The authors aim to understand why neurons commit to communication and how incoherent and coherent dynamics (chimera states) persist.

Method: They use a neurogame-theoretic framework involving coevolution of neuronal phases and communication strategies on directed, weighted networks.

Result: Findings indicate that connection weights and directionality affect the stability of communication strategies and synchronization. Applying the model to the C. elegans connectome reveals the role of snowdrift game payoff structures in forming chimera states.

Conclusion: The study provides insights into neuronal coordination mechanisms beyond classical synchronization models, using evolutionary graph theory and game-theoretic approaches.

Abstract: Neural synchronization is central to cognition However, incomplete
synchronization often produces chimera states where coherent and incoherent
dynamics coexist. While previous studies have explored such patterns using
networks of coupled oscillators, it remains unclear why neurons commit to
communication or how chimera states persist. Here, we investigate the
coevolution of neuronal phases and communication strategies on directed,
weighted networks, where interaction payoffs depend on phase alignment and may
be asymmetric due to unilateral communication. We find that both connection
weights and directionality influence the stability of communicative strategies
-- and, consequently, full synchronization -- as well as the strategic nature
of neuronal interactions. Applying our framework to the C. elegans connectome,
we show that emergent payoff structures, such as the snowdrift game, underpin
the formation of chimera states. Our computational results demonstrate a
promising neurogame-theoretic perspective, leveraging evolutionary graph theory
to shed light on mechanisms of neuronal coordination beyond classical
synchronization models.

</details>


### [121] [Emergence of robust looming selectivity via coordinated inhibitory neural computations](https://arxiv.org/abs/2510.00498)
*Qinbing Fu,Ziyan Qin*

Main category: q-bio.NC

TL;DR: This paper examines how multiple inhibitory mechanisms in locust neural pathways combine to fine-tune their selectivity for looming motion.


<details>
  <summary>Details</summary>
Motivation: To clarify how various inhibitory mechanisms interact to shape the precision of looming motion perception in locusts.

Method: Simulations within a feed-forward, multi-layer neural network integrating global, lateral, self-, and feed-forward inhibition.

Result: Various inhibitory mechanisms attenuate motion-induced excitation at different levels and timescales, enhancing selectivity for looming objects.

Conclusion: The research advances understanding of how multiple types of inhibition improve the precision of motion selectivity, especially for detecting objects approaching the locust’s eyes.

Abstract: In the locust's lobula giant movement detector neural pathways, four
categories of inhibition, i.e., global inhibition, self-inhibition, lateral
inhibition, and feed-forward inhibition, have been functionally explored in the
context of looming perception. However, their combined influence on shaping
selectivity to looming motion remains unclear. Driven by recent physiological
advancements, this paper offers new insights into the roles of these inhibitory
mechanisms at multiple levels and scales in simulations, refining the specific
selectivity for responding only to objects approaching the eyes while remaining
unresponsive to other forms of movement. Within a feed-forward, multi-layer
neural network framework, global inhibition, lateral inhibition,
self-inhibition, and feed-forward inhibition are integrated. Global inhibition
acts as an immediate feedback mechanism, normalising light intensities
delivered by ommatidia, particularly addressing low-contrast looming.
Self-inhibition, modelled numerically for the first time, suppresses
translational motion. Lateral inhibition is formed by delayed local excitation
spreading across a larger area. Notably, self-inhibition and lateral inhibition
are sequential in time and are combined through feed-forward inhibition, which
indicates the angular size subtended by moving objects. Together, these
inhibitory processes attenuate motion-induced excitation at multiple levels and
scales. This research suggests that self-inhibition may act earlier than
lateral inhibition to rapidly reduce excitation in situ, thereby suppressing
translational motion, and global inhibition can modulate excitation on a finer
scale, enhancing selectivity in higher contrast range.

</details>


### [122] [Emergence of Deviance Detection in Cortical Cultures through Maturation, Criticality, and Early Experience](https://arxiv.org/abs/2510.00764)
*Zhuo Zhang,Amit Yaron,Dai Akita,Tomoyo Isoguchi Shiramatsu,Zenas C. Chao,Hirokazu Takahashi*

Main category: q-bio.NC

TL;DR: This study explores how deviance detection (DD), a key neural mechanism related to predictive processing, matures in cortical networks using dissociated cultures. It highlights that DD arises intrinsically through local circuit maturation and is enhanced by critical network dynamics and early experiences.


<details>
  <summary>Details</summary>
Motivation: To investigate the fundamental principles by which deviance detection (DD) emerges and matures during early cortical development, providing potential insights into mechanisms underlying mismatch negativity (MMN) in humans.

Method: The researchers used dissociated cortical cultures grown on high-density CMOS microelectrode arrays. They stimulated these cultures with oddball and control paradigms while recording spontaneous and evoked activity over 10 to 35 days in vitro (DIV). They analyzed stimulus-evoked responses, neuronal avalanche dynamics, and the refinement of predictive processing.

Result: The study found that DD appears with the emergence of late responses at approximately DIV15–20. By DIV30, responses in the networks became faster, stronger, and more precise. While criticality significantly amplified and stabilized DD, it was not strictly necessary for DD to emerge. Early oddball stimulus exposure reinforced deviant pathways, but differentiation between deviant and frequent pathways reduced over time.

Conclusion: Deviance detection arises intrinsically through local circuit maturation and is further refined with self-organization toward criticality and early experiences. These findings deepen the understanding of predictive coding in simplified cortical networks and have potential implications for adaptive artificial systems.

Abstract: Mismatch negativity (MMN) in humans reflects deviance detection (DD), a core
neural mechanism of predictive processing. However, the fundamental principles
by which DD emerges and matures during early cortical development-potentially
providing a neuronal scaffold for MMN-remain unclear. Here, we tracked the
development of DD in dissociated cortical cultures grown on high-density CMOS
microelectrode arrays from 10 to 35 days in vitro (DIV). Cultures were
stimulated with oddball and many-standards control paradigms while spontaneous
and evoked activity were recorded longitudinally. At early stages,
stimulus-evoked responses were confined to fast components reflecting direct
activation. From DIV15-20 onward, robust late responses appeared, and deviant
stimuli progressively evoked stronger responses than frequent and control
stimuli, marking the onset of DD. By DIV30, responses became stronger, faster,
and more temporally precise. Neuronal avalanche analysis revealed a gradual
transition from subcritical to near-critical dynamics, with cultures exhibiting
power-law statistics showing the strongest deviant responses. Nonetheless, DD
was also present in non-critical networks, indicating that criticality is not
required for its emergence but instead stabilizes and amplifies predictive
processing as networks mature. Early oddball experience reinforces the deviant
pathway, resulting in faster conduction along those circuits. However, as
frequent and deviant pathways become less distinct, the deviance detection
index is reduced. Together, these findings demonstrate that DD arises
intrinsically through local circuit maturation, while self-organization toward
criticality and early experience further refine its strength and timing,
providing mechanistic insight into predictive coding in simplified cortical
networks and informing the design of adaptive, prediction-sensitive artificial
systems.

</details>


### [123] [Some Further Developments on a Neurobiologically-based Model for Color Sensations in Humans](https://arxiv.org/abs/2510.01000)
*Charles Q. Wu*

Main category: q-bio.NC

TL;DR: This paper proposes a neurobiological model mapping color sensations in humans to V1-L4 of the primary visual cortex, associating sub-layers with primary colors and applying it to aspects of color vision like the color solid, dichromatism, and ocular agnosticism.


<details>
  <summary>Details</summary>
Motivation: The author aims to establish a neurobiological basis for human color sensations and explore how V1-L4 sub-layers correspond to primary color sensations while addressing three key aspects of color vision.

Method: The paper introduces a model linking the primary visual cortex's sub-layers to primary colors and explores its implications on aspects such as the 3D color solid, chromatic color blindness (deuteranopia), and ocular agnosticism.

Result: The model suggests a tilted cuboid shape for a neurobiological color solid, explains fusion in V1-L4 layers in chromatic blindness, and offers insights into ocular integration mechanisms affecting color sensation.

Conclusion: The proposed neurobiological model promises advancements in enhancing human color experiences in future engineering applications.

Abstract: At HVEI-2012, I presented a neurobiologically-based model for trichromatic
color sensations in humans, mapping the neural substrate for color sensations
to V1-L4: the thalamic recipient layer of the primary visual cortex. In this
paper, I propose that V1-L4 itself consists of three distinct sub-layers that
directly correspond to the three primary color sensations: blue, red, and
green. Furthermore, I apply this model to three aspects of color vision: the
three-dimensional (3D) color solid, dichromatism, and ocular agnosticism.
Regarding these aspects further: (1) 3D color solid: V1-L4 is known to exhibit
a gradient of cell densities from its outermost layer (i.e., its pia side) to
its innermost layer (i.e., its white matter side). Taken together with the
proposition that the population size of a cell assembly directly corresponds
with the magnitude of a color sensation, it can be inferred that the
neurobiologically-based color solid is a tilted cuboid. (2) Chromatic color
blindness: Using deuteranopia as an example, at the retinal level, M-cones are
lost and replaced by L-cones. However, at the cortical level, deuteranopia
manifests as a fusion of the two bottom layers of V1-L4. (3) Ocular
agnosticism: Although color sensation is monocular, we normally are not aware
of which eye we are seeing with. This visual phenomenon can be explained by the
nature of ocular integration within V1-L4. A neurobiologically-based model for
human color sensations could significantly contribute to future engineering
efforts aimed at enhancing human color experiences.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [124] [Identifying All ε-Best Arms in (Misspecified) Linear Bandits](https://arxiv.org/abs/2510.00073)
*Zhekai Li,Tianyi Ma,Cheng Hua,Ruihao Zhu*

Main category: stat.ML

TL;DR: The paper proposes LinFACT, a near-optimal algorithm for identifying all ε-best arms in linear bandits, achieving instance optimality with reduced sample complexity.


<details>
  <summary>Details</summary>
Motivation: The need for efficient selection methods in high trial-and-error cost tasks such as drug discovery.

Method: Developed LinFACT algorithm and established information-theoretic lower bound for sample complexity, validated via numerical experiments.

Result: LinFACT matches the lower bound on sample complexity up to a logarithmic factor, improving identification of ε-best arms with computational efficiency.

Conclusion: LinFACT significantly accelerates exploratory tasks in linear bandit settings, particularly benefiting applications like drug discovery.

Abstract: Motivated by the need to efficiently identify multiple candidates in high
trial-and-error cost tasks such as drug discovery, we propose a near-optimal
algorithm to identify all {\epsilon}-best arms (i.e., those at most {\epsilon}
worse than the optimum). Specifically, we introduce LinFACT, an algorithm
designed to optimize the identification of all {\epsilon}-best arms in linear
bandits. We establish a novel information-theoretic lower bound on the sample
complexity of this problem and demonstrate that LinFACT achieves instance
optimality by matching this lower bound up to a logarithmic factor. A key
ingredient of our proof is to integrate the lower bound directly into the
scaling process for upper bound derivation, determining the termination round
and thus the sample complexity. We also extend our analysis to settings with
model misspecification and generalized linear models. Numerical experiments,
including synthetic and real drug discovery data, demonstrate that LinFACT
identifies more promising candidates with reduced sample complexity, offering
significant computational efficiency and accelerating early-stage exploratory
experiments.

</details>


### [125] [Private Learning of Littlestone Classes, Revisited](https://arxiv.org/abs/2510.00076)
*Xin Lyu*

Main category: stat.ML

TL;DR: The paper significantly improves private online learning and PAC learning for Littlestone classes, achieving better mistake and sample complexity bounds.


<details>
  <summary>Details</summary>
Motivation: The goal is to improve the state-of-the-art in private online and PAC learning of Littlestone classes, subject to differential privacy constraints.

Method: The method combines refined techniques such as irreducibility interpretation and private sparse selection, alongside tools like a sparse version of the Exponential Mechanism.

Result: Improved mistake bounds in online learning ($\tilde{O}(d^{9.5}\cdot \log(T))$) and sample complexity in PAC learning ($\widetilde{O}(\frac{d^5 \log(1/\delta\beta)}{\varepsilon \alpha})$), surpassing previous works.

Conclusion: The advancements bring the results closer to theoretical lower bounds and demonstrate a practical refinement of privacy-preserving machine learning techniques.

Abstract: We consider online and PAC learning of Littlestone classes subject to the
constraint of approximate differential privacy. Our main result is a private
learner to online-learn a Littlestone class with a mistake bound of
$\tilde{O}(d^{9.5}\cdot \log(T))$ in the realizable case, where $d$ denotes the
Littlestone dimension and $T$ the time horizon. This is a doubly-exponential
improvement over the state-of-the-art [GL'21] and comes polynomially close to
the lower bound for this task.
  The advancement is made possible by a couple of ingredients. The first is a
clean and refined interpretation of the ``irreducibility'' technique from the
state-of-the-art private PAC-learner for Littlestone classes [GGKM'21]. Our new
perspective also allows us to improve the PAC-learner of [GGKM'21] and give a
sample complexity upper bound of $\widetilde{O}(\frac{d^5
\log(1/\delta\beta)}{\varepsilon \alpha})$ where $\alpha$ and $\beta$ denote
the accuracy and confidence of the PAC learner, respectively. This improves
over [GGKM'21] by factors of $\frac{d}{\alpha}$ and attains an optimal
dependence on $\alpha$.
  Our algorithm uses a private sparse selection algorithm to \emph{sample} from
a pool of strongly input-dependent candidates. However, unlike most previous
uses of sparse selection algorithms, where one only cares about the utility of
output, our algorithm requires understanding and manipulating the actual
distribution from which an output is drawn. In the proof, we use a sparse
version of the Exponential Mechanism from [GKM'21] which behaves nicely under
our framework and is amenable to a very easy utility proof.

</details>


### [126] [CINDES: Classification induced neural density estimator and simulator](https://arxiv.org/abs/2510.00367)
*Dehao Dai,Jianqing Fan,Yihong Gu,Debarghya Mukherjee*

Main category: stat.ML

TL;DR: This paper introduces a neural density estimator that is easy to implement and adapts to low-dimensional structures for faster convergence rates, enhancing both theoretical understanding and practical performance.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address implementation challenges and the limited theoretical understanding of neural density estimators, specifically their adaptability to low-dimensional structures.

Method: The method involves developing a structure-agnostic neural density estimator that ensures non-negativity, unit-mass, and adaptability to low-dimensional composition structures. It is also integrated into score-based diffusion models.

Result: The proposed estimator achieves provably faster convergence rates in low-dimensional structured scenarios and performs well in simulations and a real-data application.

Conclusion: This work enhances the theoretical foundation and practicality of neural density estimators, demonstrating their adaptability and utility in structured environments and complex pipelines.

Abstract: Neural network-based methods for (un)conditional density estimation have
recently gained substantial attention, as various neural density estimators
have outperformed classical approaches in real-data experiments. Despite these
empirical successes, implementation can be challenging due to the need to
ensure non-negativity and unit-mass constraints, and theoretical understanding
remains limited. In particular, it is unclear whether such estimators can
adaptively achieve faster convergence rates when the underlying density
exhibits a low-dimensional structure. This paper addresses these gaps by
proposing a structure-agnostic neural density estimator that is (i)
straightforward to implement and (ii) provably adaptive, attaining faster rates
when the true density admits a low-dimensional composition structure. Another
key contribution of our work is to show that the proposed estimator integrates
naturally into generative sampling pipelines, most notably score-based
diffusion models, where it achieves provably faster convergence when the
underlying density is structured. We validate its performance through extensive
simulations and a real-data application.

</details>


### [127] [On the Adversarial Robustness of Learning-based Conformal Novelty Detection](https://arxiv.org/abs/2510.00463)
*Daofu Zhang,Mehrdad Pournaderi,Hanne M. Clifford,Yu Xiang,Pramod K. Varshney*

Main category: stat.ML

TL;DR: The paper examines how the adversarial robustness of AdaDetect, a novelty detection algorithm with finite-sample false discovery rate (FDR) control, is affected under adversarial conditions. It finds that adversarial attacks can significantly degrade its statistical guarantees.


<details>
  <summary>Details</summary>
Motivation: The study aims to explore the adversarial vulnerabilities of AdaDetect, a learning-based novelty detection framework that ensures finite-sample FDR control under benign settings, as its robustness under adversarial scenarios had not been examined before.

Method: The authors formulate an oracle attack to quantify the worst-case FDR degradation and derive an upper bound on its statistical cost. They then propose a practical attack method using query access to AdaDetect's output. The evaluation combines this attack with black-box adversarial algorithms on synthetic and real-world datasets.

Result: Adversarial perturbations can substantially increase the false discovery rate (FDR) while maintaining high detection power in the AdaDetect framework.

Conclusion: The study reveals critical vulnerabilities in AdaDetect under adversarial scenarios, highlighting the need for developing more robust novelty detection methods.

Abstract: This paper studies the adversarial robustness of conformal novelty detection.
In particular, we focus on AdaDetect, a powerful learning-based framework for
novelty detection with finite-sample false discovery rate (FDR) control. While
AdaDetect provides rigorous statistical guarantees under benign conditions, its
behavior under adversarial perturbations remains unexplored. We first formulate
an oracle attack setting that quantifies the worst-case degradation of FDR,
deriving an upper bound that characterizes the statistical cost of attacks.
This idealized formulation directly motivates a practical and effective attack
scheme that only requires query access to AdaDetect's output labels. Coupling
these formulations with two popular and complementary black-box adversarial
algorithms, we systematically evaluate the vulnerability of AdaDetect on
synthetic and real-world datasets. Our results show that adversarial
perturbations can significantly increase the FDR while maintaining high
detection power, exposing fundamental limitations of current error-controlled
novelty detection methods and motivating the development of more robust
alternatives.

</details>


### [128] [A universal compression theory: Lottery ticket hypothesis and superpolynomial scaling laws](https://arxiv.org/abs/2510.00504)
*Hong-Yi Wang,Di Luo,Tomaso Poggio,Isaac L. Chuang,Liu Ziyin*

Main category: stat.ML

TL;DR: The paper demonstrates that a generic permutation-invariant function can be compressed substantially in model size (polylogarithmic width) and dataset size while retaining performance.


<details>
  <summary>Details</summary>
Motivation: To explore whether comparable performance for large-scale models can be achieved with smaller models and less data, addressing cost and scalability issues.

Method: The authors prove a theorem about asymptotically compressing functions of 'd' objects, preserving learning dynamics and loss landscapes despite model or dataset size reduction.

Result: The findings validate the dynamical lottery ticket hypothesis and achieve faster neural scaling laws by proposing significant compression techniques.

Conclusion: Large-scale models and datasets can be substantially compressed without compromising learning dynamics or scaling laws, improving efficiency drastically.

Abstract: When training large-scale models, the performance typically scales with the
number of parameters and the dataset size according to a slow power law. A
fundamental theoretical and practical question is whether comparable
performance can be achieved with significantly smaller models and substantially
less data. In this work, we provide a positive and constructive answer. We
prove that a generic permutation-invariant function of $d$ objects can be
asymptotically compressed into a function of $\operatorname{polylog} d$ objects
with vanishing error. This theorem yields two key implications: (Ia) a large
neural network can be compressed to polylogarithmic width while preserving its
learning dynamics; (Ib) a large dataset can be compressed to polylogarithmic
size while leaving the loss landscape of the corresponding model unchanged.
(Ia) directly establishes a proof of the \textit{dynamical} lottery ticket
hypothesis, which states that any ordinary network can be strongly compressed
such that the learning dynamics and result remain unchanged. (Ib) shows that a
neural scaling law of the form $L\sim d^{-\alpha}$ can be boosted to an
arbitrarily fast power law decay, and ultimately to $\exp(-\alpha'
\sqrt[m]{d})$.

</details>


### [129] [Bayesian Neural Networks for Functional ANOVA model](https://arxiv.org/abs/2510.00545)
*Seokhun Park,Choeun Kim,Jihu Lee,Yunseop Shin,Insung Kong,Yongdai Kim*

Main category: stat.ML

TL;DR: Proposing Bayesian-TPNN to address limitations in ANOVA-TPNN, enabling detection of higher-order components with lower computational cost.


<details>
  <summary>Details</summary>
Motivation: High-dimensional functions need effective decomposition tools to interpret individual contributions, requiring improved methods which address limitations in existing ANOVA approaches.

Method: Introduced Bayesian-TPNN leveraging Bayesian inference and efficient MCMC algorithms for enhanced functional ANOVA modeling.

Result: Bayesian-TPNN demonstrated consistent posterior behavior and promising performance across benchmark datasets.

Conclusion: Bayesian-TPNN improves computational efficiency and addresses limitations of ANOVA-TPNN while enabling higher-order component detection with theoretical consistency.

Abstract: With the increasing demand for interpretability in machine learning,
functional ANOVA decomposition has gained renewed attention as a principled
tool for breaking down high-dimensional function into low-dimensional
components that reveal the contributions of different variable groups.
Recently, Tensor Product Neural Network (TPNN) has been developed and applied
as basis functions in the functional ANOVA model, referred to as ANOVA-TPNN. A
disadvantage of ANOVA-TPNN, however, is that the components to be estimated
must be specified in advance, which makes it difficult to incorporate
higher-order TPNNs into the functional ANOVA model due to computational and
memory constraints. In this work, we propose Bayesian-TPNN, a Bayesian
inference procedure for the functional ANOVA model with TPNN basis functions,
enabling the detection of higher-order components with reduced computational
cost compared to ANOVA-TPNN. We develop an efficient MCMC algorithm and
demonstrate that Bayesian-TPNN performs well by analyzing multiple benchmark
datasets. Theoretically, we prove that the posterior of Bayesian-TPNN is
consistent.

</details>


### [130] [Guaranteed Noisy CP Tensor Recovery via Riemannian Optimization on the Segre Manifold](https://arxiv.org/abs/2510.00569)
*Ke Xu,Yuefeng Han*

Main category: stat.ML

TL;DR: The paper presents two algorithms for recovering low-CP-rank tensors from noisy measurements using optimization on the Segre manifold.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of recovering low-CP-rank tensors from noisy linear measurements, which is crucial in applications like tensor PCA and regression.

Method: The study leverages the geometry of rank-one tensors and proposes Riemannian Gradient Descent (RGD) and Riemannian Gauss-Newton (RGN) methods, ensuring feasibility throughout iterations.

Result: RGD converges at a local linear rate under mild noise assumptions, while RGN shows initial local quadratic convergence that later transitions to a linear rate. Synthetic experiments confirm these outcomes and practical usage.

Conclusion: The proposed methods are theoretically and practically validated as effective for tensor recovery under noise conditions, offering robust convergence properties.

Abstract: Recovering a low-CP-rank tensor from noisy linear measurements is a central
challenge in high-dimensional data analysis, with applications spanning tensor
PCA, tensor regression, and beyond. We exploit the intrinsic geometry of
rank-one tensors by casting the recovery task as an optimization problem over
the Segre manifold, the smooth Riemannian manifold of rank-one tensors. This
geometric viewpoint yields two powerful algorithms: Riemannian Gradient Descent
(RGD) and Riemannian Gauss-Newton (RGN), each of which preserves feasibility at
every iteration. Under mild noise assumptions, we prove that RGD converges at a
local linear rate, while RGN exhibits an initial local quadratic convergence
phase that transitions to a linear rate as the iterates approach the
statistical noise floor. Extensive synthetic experiments validate these
convergence guarantees and demonstrate the practical effectiveness of our
methods.

</details>


### [131] [Approximation of differential entropy in Bayesian optimal experimental design](https://arxiv.org/abs/2510.00734)
*Chuntao Chen,Tapio Helin,Nuutti Hyvönen,Yuya Suzuki*

Main category: stat.ML

TL;DR: The paper focuses on reducing computational challenges in Bayesian experimental design by simplifying expected information gain estimation using maximum entropy approaches.


<details>
  <summary>Details</summary>
Motivation: Address challenges in computational cost for large-scale Bayesian inference problems where likelihood evaluations are expensive.

Method: Develop a Monte Carlo or quasi-Monte Carlo surrogate for evidence density approximation and evaluate differential entropy without additional likelihood computational demands.

Result: The proposed method demonstrates competitive or superior convergence rates compared to current state-of-the-art techniques, with numerical experiments validating theoretical results.

Conclusion: The approach alleviates computation issues, employs less restrictive assumptions, and performs effectively for large-scale inference problems.

Abstract: Bayesian optimal experimental design provides a principled framework for
selecting experimental settings that maximize obtained information. In this
work, we focus on estimating the expected information gain in the setting where
the differential entropy of the likelihood is either independent of the design
or can be evaluated explicitly. This reduces the problem to maximum entropy
estimation, alleviating several challenges inherent in expected information
gain computation.
  Our study is motivated by large-scale inference problems, such as inverse
problems, where the computational cost is dominated by expensive likelihood
evaluations. We propose a computational approach in which the evidence density
is approximated by a Monte Carlo or quasi-Monte Carlo surrogate, while the
differential entropy is evaluated using standard methods without additional
likelihood evaluations. We prove that this strategy achieves convergence rates
that are comparable to, or better than, state-of-the-art methods for full
expected information gain estimation, particularly when the cost of entropy
evaluation is negligible. Moreover, our approach relies only on mild smoothness
of the forward map and avoids stronger technical assumptions required in
earlier work. We also present numerical experiments, which confirm our
theoretical findings.

</details>


### [132] [Optimal placement of wind farms via quantile constraint learning](https://arxiv.org/abs/2510.01093)
*Wenxiu Feng,Antonio Alcántara,Carlos Ruiz*

Main category: stat.ML

TL;DR: This paper uses a probabilistic neural network reformulated as a mixed-integer linear model to optimize wind farm placements, modeled as a two-stage stochastic problem. It compares risk-averse and risk-neutral investor strategies using data from Spain.


<details>
  <summary>Details</summary>
Motivation: To improve strategic wind farm placement employing advanced data-driven techniques, accounting for spatiotemporal wind speed correlations and investor risk preferences.

Method: A probabilistic neural network is reformulated using ReLU activations into mixed-integer linear constraints. These are embedded into a two-stage stochastic optimization model, utilizing real regional data from Spain.

Result: Numerical experiments showed that risk-averse investors tend to prefer dominant wind sites with spatial diversification and are sensitive to transmission costs, while risk-neutral investors prioritize higher profit potential at further locations.

Conclusion: The novel approach provides effective solutions for wind farm placements, accommodating risk preferences and offering strategic insights for investors.

Abstract: Wind farm placement arranges the size and the location of multiple wind farms
within a given region. The power output is highly related to the wind speed on
spatial and temporal levels, which can be modeled by advanced data-driven
approaches. To this end, we use a probabilistic neural network as a surrogate
that accounts for the spatiotemporal correlations of wind speed. This neural
network uses ReLU activation functions so that it can be reformulated as
mixed-integer linear set of constraints (constraint learning). We embed these
constraints into the placement decision problem, formulated as a two-stage
stochastic optimization problem. Specifically, conditional quantiles of the
total electricity production are regarded as recursive decisions in the second
stage. We use real high-resolution regional data from a northern region in
Spain. We validate that the constraint learning approach outperforms the
classical bilinear interpolation method. Numerical experiments are implemented
on risk-averse investors. The results indicate that risk-averse investors
concentrate on dominant sites with strong wind, while exhibiting spatial
diversification and sensitive capacity spread in non-dominant sites.
Furthermore, we show that if we introduce transmission line costs in the
problem, risk-averse investors favor locations closer to the substations. On
the contrary, risk-neutral investors are willing to move to further locations
to achieve higher expected profits. Our results conclude that the proposed
novel approach is able to tackle a portfolio of regional wind farm placements
and further provide guidance for risk-averse investors.

</details>


### [133] [Theory of Scaling Laws for In-Context Regression: Depth, Width, Context and Time](https://arxiv.org/abs/2510.01098)
*Blake Bordelon,Mary I. Letey,Cengiz Pehlevan*

Main category: stat.ML

TL;DR: The paper analyzes the impact of computational and statistical factors on in-context learning (ICL) in linear regression using deep linear self-attention models, proposing a toy model for scalability.


<details>
  <summary>Details</summary>
Motivation: Understanding how deep attention models manage computational and statistical resources during tasks like ICL in linear regression is crucial for improving scalability and efficiency.

Method: The paper employs asymptotic analysis of ICL under three settings (ISO, FS, RRS) with varying resource scaling: depth, width, training steps, batch size, and data per context.

Result: Depth improves ICL performance in limited contexts for ISO/FS settings, while in RRS settings, depth consistently enhances performance even with infinite context length.

Conclusion: The toy model developed predicts transformer design for optimal compute and provides insights into scaling laws under different conditions.

Abstract: We study in-context learning (ICL) of linear regression in a deep linear
self-attention model, characterizing how performance depends on various
computational and statistical resources (width, depth, number of training
steps, batch size and data per context). In a joint limit where data dimension,
context length, and residual stream width scale proportionally, we analyze the
limiting asymptotics for three ICL settings: (1) isotropic covariates and tasks
(ISO), (2) fixed and structured covariance (FS), and (3) where covariances are
randomly rotated and structured (RRS). For ISO and FS settings, we find that
depth only aids ICL performance if context length is limited. Alternatively, in
the RRS setting where covariances change across contexts, increasing the depth
leads to significant improvements in ICL, even at infinite context length. This
provides a new solvable toy model of neural scaling laws which depends on both
width and depth of a transformer and predicts an optimal transformer shape as a
function of compute. This toy model enables computation of exact asymptotics
for the risk as well as derivation of powerlaws under source/capacity
conditions for the ICL tasks.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [134] [Advantage for Discrete Variational Quantum Algorithms in Circuit Recompilation](https://arxiv.org/abs/2510.01154)
*Oleksandr Kyriienko,Chukwudubem Umeano,Zoë Holmes*

Main category: quant-ph

TL;DR: This paper demonstrates an exponential separation between adaptive and non-adaptive quantum strategies in circuit recompilation tasks.


<details>
  <summary>Details</summary>
Motivation: To determine the relative advantage of adaptive versus non-adaptive quantum strategies in practical applications.

Method: The authors construct compilation problems with specific optimization properties and perform numerical experiments to evaluate adaptive and non-adaptive approaches.

Result: Adaptive methods efficiently uncover hidden circuit structures, while non-adaptive methods require exponential resources.

Conclusion: Adaptive access to quantum hardware offers a fundamental advantage, especially for certain optimization tasks.

Abstract: The relative power of quantum algorithms, using an adaptive access to quantum
devices, versus classical post-processing methods that rely only on an initial
quantum data set, remains the subject of active debate. Here, we present
evidence for an exponential separation between adaptive and non-adaptive
strategies in a quantum circuit recompilation task. Our construction features
compilation problems with loss landscapes for discrete optimization that are
unimodal yet non-separable, a structure known in classical optimization to
confer exponential advantages to adaptive search. Numerical experiments show
that optimization can efficiently uncover hidden circuit structure operating in
the regime of volume-law entanglement and high-magic, while non-adaptive
approaches are seemingly limited to exhaustive search requiring exponential
resources. These results indicate that adaptive access to quantum hardware
provides a fundamental advantage.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [135] [WaveMind: Towards a Conversational EEG Foundation Model Aligned to Textual and Visual Modalities](https://arxiv.org/abs/2510.00032)
*Ziyi Zeng,Zhenyang Cai,Yixi Cai,Xidong Wang,Junying Chen,Rongsheng Wang,Yipeng Liu,Siqi Cai,Benyou Wang,Zhiguo Zhang,Haizhou Li*

Main category: eess.SP

TL;DR: This paper explores a novel approach using multimodal large language models (MLLMs) for EEG interpretation and suggests mapping EEG signals into a unified semantic space for better analysis.


<details>
  <summary>Details</summary>
Motivation: EEG signals are complex as they encode both cognitive processes and intrinsic neural states, leading to mismatches in paired-data modalities. The study aims to address this challenge in cross-modal representation learning.

Method: A pivot investigation is conducted to identify complementary relationships between EEG modalities. The study proposes mapping EEG data into a unified semantic space and introduces WaveMind-Instruct-338k, a cross-task EEG dataset for instruction tuning.

Result: The proposed model achieves robust classification accuracy and enables open-ended conversations across four downstream tasks, demonstrating flexibility and practical utility.

Conclusion: This work offers valuable insights for neuroscience and supports the development of general-purpose EEG models with conversational capabilities.

Abstract: Electroencephalography (EEG) interpretation using multimodal large language
models (MLLMs) offers a novel approach for analyzing brain signals. However,
the complex nature of brain activity introduces critical challenges: EEG
signals simultaneously encode both cognitive processes and intrinsic neural
states, creating a mismatch in EEG paired-data modality that hinders effective
cross-modal representation learning. Through a pivot investigation, we uncover
complementary relationships between these modalities. Leveraging this insight,
we propose mapping EEG signals and their corresponding modalities into a
unified semantic space to achieve generalized interpretation. To fully enable
conversational capabilities, we further introduce WaveMind-Instruct-338k, the
first cross-task EEG dataset for instruction tuning. The resulting model
demonstrates robust classification accuracy while supporting flexible,
open-ended conversations across four downstream tasks, thereby offering
valuable insights for both neuroscience research and the development of
general-purpose EEG models.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [136] [Deep Learning Approaches with Explainable AI for Differentiating Alzheimer Disease and Mild Cognitive Impairment](https://arxiv.org/abs/2510.00048)
*Fahad Mostafa,Kannon Hossain,Hafiz Khan*

Main category: eess.IV

TL;DR: A hybrid ensemble deep learning framework using structural MRI achieves high accuracy in diagnosing Alzheimer's Disease, distinguishing it from Mild Cognitive Impairment (MCI), with additional interpretability techniques for improved clinical decision-making.


<details>
  <summary>Details</summary>
Motivation: Early and accurate diagnosis of Alzheimer's Disease, differentiating it from Mild Cognitive Impairment, is essential to enable effective clinical intervention, leveraging subtle structural changes in brain imaging.

Method: The study uses a hybrid deep learning framework that employs three pre-trained convolutional neural networks (ResNet50, NASNet, and MobileNet), fine-tunes them, and combines their outputs via a stacked ensemble method with a meta-learner and weighted averaging. Explainable AI is incorporated to provide heatmaps highlighting model-relevant structural biomarkers.

Result: The proposed method achieves state-of-the-art accuracy: 99.21% for Alzheimer's vs. MCI and 91.0% for MCI vs. Normal controls, tested on the Alzheimer Disease Neuroimaging Initiative dataset.

Conclusion: The framework demonstrates robust, scalable potential for clinical decision support in diagnosing Alzheimer's Disease and MCI, enhanced by interpretability for understanding its decision-making.

Abstract: Early and accurate diagnosis of Alzheimer Disease is critical for effective
clinical intervention, particularly in distinguishing it from Mild Cognitive
Impairment, a prodromal stage marked by subtle structural changes. In this
study, we propose a hybrid deep learning ensemble framework for Alzheimer
Disease classification using structural magnetic resonance imaging. Gray and
white matter slices are used as inputs to three pretrained convolutional neural
networks such as ResNet50, NASNet, and MobileNet, each fine tuned through an
end to end process. To further enhance performance, we incorporate a stacked
ensemble learning strategy with a meta learner and weighted averaging to
optimally combine the base models. Evaluated on the Alzheimer Disease
Neuroimaging Initiative dataset, the proposed method achieves state of the art
accuracy of 99.21% for Alzheimer Disease vs. Mild Cognitive Impairment and
91.0% for Mild Cognitive Impairment vs. Normal Controls, outperforming
conventional transfer learning and baseline ensemble methods. To improve
interpretability in image based diagnostics, we integrate Explainable AI
techniques by Gradient weighted Class Activation, which generates heatmaps and
attribution maps that highlight critical regions in gray and white matter
slices, revealing structural biomarkers that influence model decisions. These
results highlight the frameworks potential for robust and scalable clinical
decision support in neurodegenerative disease diagnostics.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [137] [Directed Information $γ$-covering: An Information-Theoretic Framework for Context Engineering](https://arxiv.org/abs/2510.00079)
*Hai Huang*

Main category: cs.IT

TL;DR: This paper introduces Directed Information (DI) γ-covering, a framework for redundancy-aware selection in AI, utilizing DI to optimize context representation with efficient algorithms and experimental validation.


<details>
  <summary>Details</summary>
Motivation: The paper aims to develop a framework for efficient and redundancy-aware context selection to improve AI pipelines, addressing the challenges in leveraging mutual information efficiently in real-world applications.

Method: The authors use Directed Information (DI), a causal version of mutual information, to measure asymmetry in data predictiveness. They approach context selection as a γ-cover problem and propose a greedy algorithm with bounded guarantees, including query-agnostic design.

Result: The framework outperformed competitive baselines like BM25 in experiments on HotpotQA, especially in scenarios like context compression and single-slot prompt selection, demonstrating its practical applicability.

Conclusion: DI γ-covering is presented as a robust and flexible framework for modern LLM pipelines, promoting efficient, principled, and redundancy-aware context selection while being computationally cost-effective.

Abstract: We introduce \textbf{Directed Information $\gamma$-covering}, a simple but
general framework for redundancy-aware context engineering. Directed
information (DI), a causal analogue of mutual information, measures asymmetric
predictiveness between chunks. If $\operatorname{DI}_{i \to j} \ge H(C_j) -
\gamma$, then $C_i$ suffices to represent $C_j$ up to $\gamma$ bits. Building
on this criterion, we formulate context selection as a $\gamma$-cover problem
and propose a greedy algorithm with provable guarantees: it preserves query
information within bounded slack, inherits $(1+\ln n)$ and $(1-1/e)$
approximations from submodular set cover, and enforces a diversity margin.
Importantly, building the $\gamma$-cover is \emph{query-agnostic}: it incurs no
online cost and can be computed once offline and amortized across all queries.
Experiments on HotpotQA show that $\gamma$-covering consistently improves over
BM25, a competitive baseline, and provides clear advantages in hard-decision
regimes such as context compression and single-slot prompt selection. These
results establish DI $\gamma$-covering as a principled, self-organizing
backbone for modern LLM pipelines.

</details>
