<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 82]
- [cs.AR](#cs.AR) [Total: 14]
- [cs.CL](#cs.CL) [Total: 88]
- [cs.CV](#cs.CV) [Total: 296]
- [cs.DC](#cs.DC) [Total: 40]
- [cs.LG](#cs.LG) [Total: 281]
- [cs.NE](#cs.NE) [Total: 6]
- [cs.PF](#cs.PF) [Total: 2]
- [cs.PL](#cs.PL) [Total: 4]
- [cs.RO](#cs.RO) [Total: 55]
- [cs.SE](#cs.SE) [Total: 24]
- [q-bio.NC](#q-bio.NC) [Total: 3]
- [stat.ML](#stat.ML) [Total: 12]
- [cs.HC](#cs.HC) [Total: 11]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [cs.SD](#cs.SD) [Total: 5]
- [math.ST](#math.ST) [Total: 2]
- [cs.NI](#cs.NI) [Total: 1]
- [hep-ph](#hep-ph) [Total: 1]
- [cs.MA](#cs.MA) [Total: 4]
- [cs.DL](#cs.DL) [Total: 1]
- [eess.SY](#eess.SY) [Total: 8]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 3]
- [cs.GL](#cs.GL) [Total: 1]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [cs.MM](#cs.MM) [Total: 2]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [stat.AP](#stat.AP) [Total: 3]
- [math.OC](#math.OC) [Total: 4]
- [cs.IT](#cs.IT) [Total: 1]
- [math.CO](#math.CO) [Total: 1]
- [eess.IV](#eess.IV) [Total: 12]
- [econ.GN](#econ.GN) [Total: 1]
- [cs.IR](#cs.IR) [Total: 10]
- [cs.GR](#cs.GR) [Total: 1]
- [gr-qc](#gr-qc) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 3]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [hep-ex](#hep-ex) [Total: 2]
- [eess.AS](#eess.AS) [Total: 3]
- [eess.SP](#eess.SP) [Total: 3]
- [cs.GT](#cs.GT) [Total: 1]
- [quant-ph](#quant-ph) [Total: 6]
- [stat.CO](#stat.CO) [Total: 1]
- [q-bio.MN](#q-bio.MN) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [astro-ph.HE](#astro-ph.HE) [Total: 1]
- [stat.ME](#stat.ME) [Total: 3]
- [cs.CY](#cs.CY) [Total: 12]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [cs.CE](#cs.CE) [Total: 3]
- [cs.CR](#cs.CR) [Total: 31]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Looking Forward: Challenges and Opportunities in Agentic AI Reliability](https://arxiv.org/abs/2511.11921)
*Liudong Xing,Janet,Lin*

Main category: cs.AI

TL;DR: The chapter explores challenges and future directions for ensuring reliability in agentic AI systems.


<details>
  <summary>Details</summary>
Motivation: To address and mitigate risks associated with cascading failures and various reliability-related issues in AI systems.

Method: Discussing open research problems, challenges in dynamic tasks and environments, and proposing directions for testing and evaluating reliability mechanisms.

Result: Insights into specific problems like emergent behaviors, inconsistent executions, and resource-intense reliability processes.

Conclusion: Further research is required to ensure robust and reliable performance of agentic AI systems in dynamic and challenging scenarios.

Abstract: This chapter presents perspectives for challenges and future development in building reliable AI systems, particularly, agentic AI systems. Several open research problems related to mitigating the risks of cascading failures are discussed. The chapter also sheds lights on research challenges and opportunities in aspects including dynamic environments, inconsistent task execution, unpredictable emergent behaviors, as well as resource-intensive reliability mechanisms. In addition, several research directions along the line of testing and evaluating reliability of agentic AI systems are also discussed.

</details>


### [2] [LLM-Generated Negative News Headlines Dataset: Creation and Benchmarking Against Real Journalism](https://arxiv.org/abs/2511.11591)
*Olusola Babalola,Bolanle Ojokoh,Olutayo Boyinbode*

Main category: cs.AI

TL;DR: This paper evaluates the usability of LLM-generated datasets for replacing real-world data in NLP tasks, particularly focusing on negative sentiment analysis.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges like data acquisition issues and privacy concerns associated with actual data, offering synthetic datasets as an alternative.

Method: The authors created a synthetic dataset of negative news headlines through tailored prompts to represent diverse negative sentiments. They validated these headlines via expert reviews and embedding space analysis, followed by benchmarking against real data with various evaluation metrics.

Result: The LLM-generated negative news headlines closely matched real ones in content and style, with only notable divergence in proper noun scores during POS profiling.

Conclusion: Synthetic datasets generated via LLMs show promise as feasible alternatives to real-world datasets for NLP tasks, particularly in sentiment analysis.

Abstract: This research examines the potential of datasets generated by Large Language Models (LLMs) to support Natural Language Processing (NLP) tasks, aiming to overcome challenges related to data acquisition and privacy concerns associated with real-world data. Focusing on negative valence text, a critical component of sentiment analysis, we explore the use of LLM-generated synthetic news headlines as an alternative to real-world data. A specialized corpus of negative news headlines was created using tailored prompts to capture diverse negative sentiments across various societal domains. The synthetic headlines were validated by expert review and further analyzed in embedding space to assess their alignment with real-world negative news in terms of content, tone, length, and style. Key metrics such as correlation with real headlines, perplexity, coherence, and realism were evaluated. The synthetic dataset was benchmarked against two sets of real news headlines using evaluations including the Comparative Perplexity Test, Comparative Readability Test, Comparative POS Profiling, BERTScore, and Comparative Semantic Similarity. Results show the generated headlines match real headlines with the only marked divergence being in the proper noun score of the POS profile test.

</details>


### [3] [CLINB: A Climate Intelligence Benchmark for Foundational Models](https://arxiv.org/abs/2511.11597)
*Michelle Chen Huebscher,Katharine Mach,Aleksandar Stanić,Markus Leippold,Ben Gaiarin,Zeke Hausfather,Elisa Rawat,Erich Fischer,Massimiliano Ciaramita,Joeri Rogelj,Christian Buck,Lierni Sestorain Saralegui,Reto Knutti*

Main category: cs.AI

TL;DR: The paper introduces the CLINB benchmark to assess how well large language models (LLMs) handle complex, specialized knowledge in climate change, identifying a gap between knowledge synthesis and evidence grounding.


<details>
  <summary>Details</summary>
Motivation: The intent is to evaluate the capabilities of LLMs in handling specialized, complex domains like climate change and to ensure their output is both knowledgeable and based on verifiable evidence.

Method: The authors developed CLINB, a benchmark based on real user questions and rubrics from climate scientists, to test models on open-ended, multimodal tasks. They also implemented a model-based evaluation process and assessed several advanced models.

Result: Findings highlight that advanced models are excellent at knowledge synthesis but struggle with evidential grounding, often generating hallucinations in references and images despite their high-level understanding.

Conclusion: To enable trustworthy AI in scientific workflows, the gap between knowledge synthesis and evidence attribution must be addressed. Reliable, interpretable benchmarks like CLINB are essential for this advancement.

Abstract: Evaluating how Large Language Models (LLMs) handle complex, specialized knowledge remains a critical challenge. We address this through the lens of climate change by introducing CLINB, a benchmark that assesses models on open-ended, grounded, multimodal question answering tasks with clear requirements for knowledge quality and evidential support. CLINB relies on a dataset of real users' questions and evaluation rubrics curated by leading climate scientists. We implement and validate a model-based evaluation process and evaluate several frontier models. Our findings reveal a critical dichotomy. Frontier models demonstrate remarkable knowledge synthesis capabilities, often exhibiting PhD-level understanding and presentation quality. They outperform "hybrid" answers curated by domain experts assisted by weaker models. However, this performance is countered by failures in grounding. The quality of evidence varies, with substantial hallucination rates for references and images. We argue that bridging this gap between knowledge synthesis and verifiable attribution is essential for the deployment of AI in scientific workflows and that reliable, interpretable benchmarks like CLINB are needed to progress towards building trustworthy AI systems.

</details>


### [4] [SynBullying: A Multi LLM Synthetic Conversational Dataset for Cyberbullying Detectio](https://arxiv.org/abs/2511.11599)
*Arefeh Kazemi,Hamza Qadeer,Joachim Wagner,Hossein Hosseini,Sri Balaaji Natarajan Kalaivendan,Brian Davis*

Main category: cs.AI

TL;DR: SynBullying is a conversational dataset created with multiple large language models (LLMs) to study and detect cyberbullying behaviors ethically, featuring structured interactions, context-aware annotations, and detailed labeling.


<details>
  <summary>Details</summary>
Motivation: Provide a scalable and safe alternative for studying cyberbullying with synthetic data generated through LLMs to avoid ethical and logistical challenges of human data collection.

Method: Utilize LLMs to simulate realistic bullying exchanges, structure multi-turn conversations, and annotate harmfulness assessments based on context and intent within the dataset.

Result: SynBullying successfully covers conversational dynamics, lexical and sentiment analysis, and role dynamics, and demonstrates utility as both standalone training data and for augmenting cyberbullying classification models.

Conclusion: SynBullying contributes as an ethically safe and effective tool for improving cyberbullying detection and classification systems, while offering deeper insights into conversational and behavioral aspects of bullying.

Abstract: We introduce SynBullying, a synthetic multi-LLM conversational dataset for studying and detecting cyberbullying (CB). SynBullying provides a scalable and ethically safe alternative to human data collection by leveraging large language models (LLMs) to simulate realistic bullying interactions. The dataset offers (i) conversational structure, capturing multi-turn exchanges rather than isolated posts; (ii) context-aware annotations, where harmfulness is assessed within the conversational flow considering context, intent, and discourse dynamics; and (iii) fine-grained labeling, covering various CB categories for detailed linguistic and behavioral analysis. We evaluate SynBullying across five dimensions, including conversational structure, lexical patterns, sentiment/toxicity, role dynamics, harm intensity, and CB-type distribution. We further examine its utility by testing its performance as standalone training data and as an augmentation source for CB classification.

</details>


### [5] [CausalGuard: A Smart System for Detecting and Preventing False Information in Large Language Models](https://arxiv.org/abs/2511.11600)
*Piyushkumar Patel*

Main category: cs.AI

TL;DR: CausalGuard is a novel system designed to prevent hallucinations in large language models by combining causal reasoning with symbolic logic.


<details>
  <summary>Details</summary>
Motivation: Large language models frequently generate false but plausible-sounding information, posing challenges in applications requiring high accuracy.

Method: CausalGuard detects and prevents hallucinations early by analyzing causal chains and ensuring logical consistency using automated reasoning.

Result: Testing on 12 benchmarks, CausalGuard identified 89.3% of hallucinations, missed only 8.3%, and reduced false claims by nearly 80%, especially excelling in complex reasoning tasks.

Conclusion: CausalGuard significantly mitigates hallucinations in language models, improving accuracy and transparency, making it suitable for sensitive domains such as medical or financial decision-making.

Abstract: While large language models have transformed how we interact with AI systems, they have a critical weakness: they confidently state false information that sounds entirely plausible. This "hallucination" problem has become a major barrier to using these models where accuracy matters most. Existing solutions either require retraining the entire model, add significant computational costs, or miss the root causes of why these hallucinations occur in the first place.
  We present CausalGuard, a new approach that combines causal reasoning with symbolic logic to catch and prevent hallucinations as they happen. Unlike previous methods that only check outputs after generation, our system understands the causal chain that leads to false statements and intervenes early in the process. CausalGuard works through two complementary paths: one that traces causal relationships between what the model knows and what it generates, and another that checks logical consistency using automated reasoning.
  Testing across twelve different benchmarks, we found that CausalGuard correctly identifies hallucinations 89.3\% of the time while missing only 8.3\% of actual hallucinations. More importantly, it reduces false claims by nearly 80\% while keeping responses natural and helpful. The system performs especially well on complex reasoning tasks where multiple steps of logic are required. Because CausalGuard shows its reasoning process, it works well in sensitive areas like medical diagnosis or financial analysis where understanding why a decision was made matters as much as the decision itself.

</details>


### [6] [Quantifying Skill and Chance: A Unified Framework for the Geometry of Games](https://arxiv.org/abs/2511.11611)
*David H. Silver*

Main category: cs.AI

TL;DR: The paper introduces a framework using Skill and Luck Index to quantify the balance between chance and skill in games, applying it to 30 games.


<details>
  <summary>Details</summary>
Motivation: To quantitatively measure the influence of skill and chance in various games and assess player influence, game balance, and prediction.

Method: A statistical decomposition of game outcomes into skill leverage and luck leverage, along with the introduction of the Skill-Luck Index and volatility metrics.

Result: Applications to 30 games show skill-chance spectrum from pure chance (coin toss) to pure skill (chess), and measure intermediate games like poker.

Conclusion: The framework offers a structured way to compare games, assess AI performance, and analyze uncertainty, providing a tool for game design and decision systems.

Abstract: We introduce a quantitative framework for separating skill and chance in games by modeling them as complementary sources of control over stochastic decision trees. We define the Skill-Luck Index S(G) in [-1, 1] by decomposing game outcomes into skill leverage K and luck leverage L. Applying this to 30 games reveals a continuum from pure chance (coin toss, S = -1) through mixed domains such as backgammon (S = 0, Sigma = 1.20) to pure skill (chess, S = +1, Sigma = 0). Poker exhibits moderate skill dominance (S = 0.33) with K = 0.40 +/- 0.03 and Sigma = 0.80. We further introduce volatility Sigma to quantify outcome uncertainty over successive turns. The framework extends to general stochastic decision systems, enabling principled comparisons of player influence, game balance, and predictive stability, with applications to game design, AI evaluation, and risk assessment.

</details>


### [7] [Value-Aligned Prompt Moderation via Zero-Shot Agentic Rewriting for Safe Image Generation](https://arxiv.org/abs/2511.11693)
*Xin Zhao,Xiaojun Chen,Bingshan Liu,Zeyao Liu,Zhendong Zhao,Xiaoyan Gu*

Main category: cs.AI

TL;DR: VALOR is a framework for safer text-to-image generation by addressing unsafe, offensive, or inappropriate outputs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve safety and alignment of generative vision-language models like Stable Diffusion, which can produce harmful content when misused.

Method: VALOR introduces a layered approach with a multi-level NSFW detector, cultural value alignment module, intention disambiguator, and dynamic prompt rewriting using large language models.

Result: VALOR significantly reduces unsafe outputs by up to 100% while maintaining creativity and usefulness of the generated results.

Conclusion: VALOR is effective for safe and aligned deployment of image generation systems in diverse contexts.

Abstract: Generative vision-language models like Stable Diffusion demonstrate remarkable capabilities in creative media synthesis, but they also pose substantial risks of producing unsafe, offensive, or culturally inappropriate content when prompted adversarially. Current defenses struggle to align outputs with human values without sacrificing generation quality or incurring high costs. To address these challenges, we introduce VALOR (Value-Aligned LLM-Overseen Rewriter), a modular, zero-shot agentic framework for safer and more helpful text-to-image generation. VALOR integrates layered prompt analysis with human-aligned value reasoning: a multi-level NSFW detector filters lexical and semantic risks; a cultural value alignment module identifies violations of social norms, legality, and representational ethics; and an intention disambiguator detects subtle or indirect unsafe implications. When unsafe content is detected, prompts are selectively rewritten by a large language model under dynamic, role-specific instructions designed to preserve user intent while enforcing alignment. If the generated image still fails a safety check, VALOR optionally performs a stylistic regeneration to steer the output toward a safer visual domain without altering core semantics. Experiments across adversarial, ambiguous, and value-sensitive prompts show that VALOR significantly reduces unsafe outputs by up to 100.00% while preserving prompt usefulness and creativity. These results highlight VALOR as a scalable and effective approach for deploying safe, aligned, and helpful image generation systems in open-world settings.

</details>


### [8] [Towards autonomous quantum physics research using LLM agents with access to intelligent tools](https://arxiv.org/abs/2511.11752)
*Sören Arlt,Xuemei Gu,Mario Krenn*

Main category: cs.AI

TL;DR: AI-Mandel, an advanced AI agent, generates and implements actionable scientific ideas in quantum physics, paving the way for automation in the scientific process.


<details>
  <summary>Details</summary>
Motivation: Despite AI's widespread use, human researchers remain pivotal in initiating research questions and interpreting AI ideas. Automating idea generation and execution could radically alter the human role in science.

Method: AI-Mandel leverages a language model to formulate ideas from quantum physics literature, alongside a separate AI domain tool to convert these ideas into concrete experimental designs.

Result: AI-Mandel produced implementable and scientifically intriguing ideas. Two of its generated ideas led to independent follow-up scientific papers.

Conclusion: AI-Mandel demonstrates the potential for AI to autonomously contribute to scientific discovery, offering innovative concepts and presenting challenges for achieving human-level scientific capabilities in AI.

Abstract: Artificial intelligence (AI) is used in numerous fields of science, yet the initial research questions and targets are still almost always provided by human researchers. AI-generated creative ideas in science are rare and often vague, so that it remains a human task to execute them. Automating idea generation and implementation in one coherent system would significantly shift the role of humans in the scientific process. Here we present AI-Mandel, an LLM agent that can generate and implement ideas in quantum physics. AI-Mandel formulates ideas from the literature and uses a domain-specific AI tool to turn them into concrete experiment designs that can readily be implemented in laboratories. The generated ideas by AI-Mandel are often scientifically interesting - for two of them we have already written independent scientific follow-up papers. The ideas include new variations of quantum teleportation, primitives of quantum networks in indefinite causal orders, and new concepts of geometric phases based on closed loops of quantum information transfer. AI-Mandel is a prototypical demonstration of an AI physicist that can generate and implement concrete, actionable ideas. Building such a system is not only useful to accelerate science, but it also reveals concrete open challenges on the path to human-level artificial scientists.

</details>


### [9] [Learning to Refine: An Agentic RL Approach for Iterative SPARQL Query Construction](https://arxiv.org/abs/2511.11770)
*Floris Vossebeld,Shenghui Wang*

Main category: cs.AI

TL;DR: This paper introduces a novel framework where a smaller LLM model uses reinforcement learning to iteratively generate accurate SPARQL queries, improving results for multi-hop Knowledge Graph Question Answering.


<details>
  <summary>Details</summary>
Motivation: The brittle nature of one-shot SPARQL query generation by LLMs and the lack of adaptive mechanisms for debugging queries hinder effective interaction with knowledge graphs.

Method: A 3-billion parameter LLM is trained exclusively using outcome-driven reinforcement learning (GRPO), enabling it to iteratively and adaptively construct SPARQL queries while recovering from errors.

Result: The proposed agent achieved 49.7% accuracy on a challenging dataset (LC-QuAD 2.0), significantly outperforming iterative zero-shot baselines, increasing accuracy by 17.5 percentage points.

Conclusion: The framework demonstrates the potential of compact models with reinforcement learning for robust SPARQL query construction, offering a blueprint for bridging probabilistic LLMs and structured symbolic systems.

Abstract: Generating complex, logically-sound SPARQL queries for multi-hop questions remains a critical bottleneck for Knowledge Graph Question Answering, as the brittle nature of one-shot generation by Large Language Models (LLMs) hinders reliable interaction with structured data. Current methods lack the adaptive policies needed to dynamically debug queries based on real-time execution feedback. This paper introduces a novel agentic framework where an LLM learns a resilient policy for the sequential process of iterative SPARQL construction. We show that a compact 3B-parameter model, trained exclusively via outcome-driven Reinforcement Learning (GRPO) without supervised fine-tuning, can learn effective policies for this task, discovering how to systematically recover from execution errors and refine its queries toward a correct answer. On a curated, executable single-answer subset of LC-QuAD 2.0, our agent achieves 49.7\% accuracy post-entity-linking, a significant 17.5 percentage point improvement over the strongest iterative zero-shot baseline. Further analysis reveals that while the agent's capability is driven by RL, its performance is enhanced by an explicit deliberative reasoning step that acts as a cognitive scaffold to improve policy precision. This work presents a generalizable blueprint for teaching agents to master formal, symbolic tools through interaction, bridging the gap between probabilistic LLMs and the structured world of Knowledge Graphs.

</details>


### [10] [On the Measure of a Model: From Intelligence to Generality](https://arxiv.org/abs/2511.11773)
*Ruchira Dhar,Ninell Oldenburg,Anders Soegaard*

Main category: cs.AI

TL;DR: This paper critiques intelligence-focused benchmarks in evaluating LLMs, advocating for generality as a superior evaluation criterion.


<details>
  <summary>Details</summary>
Motivation: To address limitations in intelligence-focused evaluations and propose a better framework for evaluating LLMs based on generality.

Method: Conceptual and formal analysis to assess assumptions in intelligence-based evaluations, focusing on generality.

Result: Generality is shown to be more conceptually and empirically valid than intelligence in evaluating LLMs across tasks.

Conclusion: Evaluation should prioritize generality over intelligence, providing a robust basis for assessing AI performance across diverse tasks.

Abstract: Benchmarks such as ARC, Raven-inspired tests, and the Blackbird Task are widely used to evaluate the intelligence of large language models (LLMs). Yet, the concept of intelligence remains elusive- lacking a stable definition and failing to predict performance on practical tasks such as question answering, summarization, or coding. Optimizing for such benchmarks risks misaligning evaluation with real-world utility. Our perspective is that evaluation should be grounded in generality rather than abstract notions of intelligence. We identify three assumptions that often underpin intelligence-focused evaluation: generality, stability, and realism. Through conceptual and formal analysis, we show that only generality withstands conceptual and empirical scrutiny. Intelligence is not what enables generality; generality is best understood as a multitask learning problem that directly links evaluation to measurable performance breadth and reliability. This perspective reframes how progress in AI should be assessed and proposes generality as a more stable foundation for evaluating capability across diverse and evolving tasks.

</details>


### [11] [Do LLMs Really Struggle at NL-FOL Translation? Revealing their Strengths via a Novel Benchmarking Strategy](https://arxiv.org/abs/2511.11816)
*Andrea Brunello,Luca Geatti,Michele Mignani,Angelo Montanari,Nicola Saccomanno*

Main category: cs.AI

TL;DR: Investigates and addresses the challenges of translating natural language to First-Order Logic using Large Language Models (LLMs). Proposes a new evaluation protocol to better assess true capabilities and finds dialogue-centric LLMs to excel at this task.


<details>
  <summary>Details</summary>
Motivation: First-Order Logic (FOL) is effective for understanding and specifying system properties, but translating natural language (NL) to FOL is challenging. Ambiguity in previous evaluations of LLMs' NL-FOL translation capabilities motivated the need for improvements.

Method: The authors critically analyzed existing datasets and evaluation protocols, proposed a new protocol that emphasizes true semantic and logical understanding over pattern recognition, and tested LLMs using this framework.

Result: Dialogue-oriented LLMs displayed strong NL-FOL translation abilities and logical understanding under the new evaluation protocol, whereas embedding-centric models underperformed.

Conclusion: The new evaluation framework enabled a fairer assessment of LLMs' capabilities, demonstrating that dialogue-centric models possess a deeper understanding of sentence-level logic for NL-FOL translation.

Abstract: Due to its expressiveness and unambiguous nature, First-Order Logic (FOL) is a powerful formalism for representing concepts expressed in natural language (NL). This is useful, e.g., for specifying and verifying desired system properties. While translating FOL into human-readable English is relatively straightforward, the inverse problem, converting NL to FOL (NL-FOL translation), has remained a longstanding challenge, for both humans and machines. Although the emergence of Large Language Models (LLMs) promised a breakthrough, recent literature provides contrasting results on their ability to perform NL-FOL translation. In this work, we provide a threefold contribution. First, we critically examine existing datasets and protocols for evaluating NL-FOL translation performance, revealing key limitations that may cause a misrepresentation of LLMs' actual capabilities. Second, to overcome these shortcomings, we propose a novel evaluation protocol explicitly designed to distinguish genuine semantic-level logical understanding from superficial pattern recognition, memorization, and dataset contamination. Third, using this new approach, we show that state-of-the-art, dialogue-oriented LLMs demonstrate strong NL-FOL translation skills and a genuine grasp of sentence-level logic, whereas embedding-centric models perform markedly worse.

</details>


### [12] [TopoPerception: A Shortcut-Free Evaluation of Global Visual Perception in Large Vision-Language Models](https://arxiv.org/abs/2511.11831)
*Wenhao Zhou,Hao Zheng,Rong Zhao*

Main category: cs.AI

TL;DR: This paper introduces TopoPerception, a benchmark for rigorously evaluating global visual perception in Large Vision-Language Models (LVLMs).


<details>
  <summary>Details</summary>
Motivation: Current LVLMs are limited by their reliance on aligning visual encoders with pre-trained LLMs, and conventional benchmarks overestimate their perceptual abilities due to local shortcuts.

Method: TopoPerception utilizes topological properties of images, focusing on global structure rather than local features, to provide a 'shortcut-free' assessment of visual perception at various granularities.

Result: State-of-the-art LVLMs performed at chance level even for the coarsest global features, with stronger reasoning models exhibiting lower accuracy.

Conclusion: The study highlights the inadequacy of scaling up models to enhance global visual perception and suggests the need for new training paradigms or architectures. TopoPerception serves as a diagnostic tool and guide for future improvements.

Abstract: Large Vision-Language Models (LVLMs) typically align visual features from an encoder with a pre-trained Large Language Model (LLM). However, this makes the visual perception module a bottleneck, which constrains the overall capabilities of LVLMs. Conventional evaluation benchmarks, while rich in visual semantics, often contain unavoidable local shortcuts that can lead to an overestimation of models' perceptual abilities. Here, we introduce TopoPerception, a benchmark that leverages topological properties to rigorously evaluate the global visual perception capabilities of LVLMs across various granularities. Since topology depends on the global structure of an image and is invariant to local features, TopoPerception enables a shortcut-free assessment of global perception, fundamentally distinguishing it from semantically rich tasks. We evaluate state-of-the-art models on TopoPerception and find that even at the coarsest perceptual granularity, all models perform no better than random chance, indicating a profound inability to perceive global visual features. Notably, a consistent trend emerge within model families: more powerful models with stronger reasoning capabilities exhibit lower accuracy. This suggests that merely scaling up models is insufficient to address this deficit and may even exacerbate it. Progress may require new training paradigms or architectures. TopoPerception not only exposes a critical bottleneck in current LVLMs but also offers a lens and direction for improving their global visual perception. The data and code are publicly available at: https://github.com/Wenhao-Zhou/TopoPerception.

</details>


### [13] [End to End AI System for Surgical Gesture Sequence Recognition and Clinical Outcome Prediction](https://arxiv.org/abs/2511.11899)
*Xi Li,Nicholas Matsumoto,Ujjwal Pasupulety,Atharva Deo,Cherine Yang,Jay Moran,Miguel E. Hernandez,Peter Wager,Jasmine Lin,Jeanine Kim,Alvin C. Goh,Christian Wagner,Geoffrey A. Sonn,Andrew J. Hung*

Main category: cs.AI

TL;DR: The paper introduces Frame-to-Outcome (F2O), an end-to-end system for analyzing surgical videos and predicting outcomes, showing promising results for robotic prostatectomy procedures.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of analyzing intraoperative behavior in a fine-grained manner and uncover patterns linking surgical actions to patient outcomes.

Method: The authors developed F2O, which uses transformer-based spatial and temporal modeling and frame-wise classification to analyze tissue dissection videos, detect gestures, and predict outcomes.

Result: F2O demonstrated robust gesture detection accuracy (AUC: 0.80), reliable outcome prediction comparable to human annotations, and identified specific patterns affecting erectile function recovery.

Conclusion: F2O provides an automated, interpretable method for linking intraoperative behavior to patient outcomes, paving the way for data-driven surgical feedback and clinical decision support.

Abstract: Fine-grained analysis of intraoperative behavior and its impact on patient outcomes remain a longstanding challenge. We present Frame-to-Outcome (F2O), an end-to-end system that translates tissue dissection videos into gesture sequences and uncovers patterns associated with postoperative outcomes. Leveraging transformer-based spatial and temporal modeling and frame-wise classification, F2O robustly detects consecutive short (~2 seconds) gestures in the nerve-sparing step of robot-assisted radical prostatectomy (AUC: 0.80 frame-level; 0.81 video-level). F2O-derived features (gesture frequency, duration, and transitions) predicted postoperative outcomes with accuracy comparable to human annotations (0.79 vs. 0.75; overlapping 95% CI). Across 25 shared features, effect size directions were concordant with small differences (~ 0.07), and strong correlation (r = 0.96, p < 1e-14). F2O also captured key patterns linked to erectile function recovery, including prolonged tissue peeling and reduced energy use. By enabling automatic interpretable assessment, F2O establishes a foundation for data-driven surgical feedback and prospective clinical decision support.

</details>


### [14] [Forgetting-MarI: LLM Unlearning via Marginal Information Regularization](https://arxiv.org/abs/2511.11914)
*Shizhou Xu,Yuan Ni,Stefan Broecker,Thomas Strohmer*

Main category: cs.AI

TL;DR: The paper introduces Forgetting-MarI, a framework for unlearning specific data from Large Language Models (LLMs) effectively without unnecessary loss of information and demonstrates improved performance in ensuring privacy and compliance.


<details>
  <summary>Details</summary>
Motivation: Ensuring privacy protection and regulatory compliance in AI systems by enabling the removal of influences from specific training datasets.

Method: Developed Forgetting-MarI, a framework that uses penalizing marginal information to remove only the additional influence of specific data while preserving necessary model knowledge.

Result: The Forgetting-MarI framework outperforms state-of-the-art methods in delivering reliable unlearning and maintaining general model performance based on extensive experimentation.

Conclusion: The approach provides a crucial balance between privacy and performance, making AI systems more compliant and controllable without sacrificing effectiveness.

Abstract: As AI models are trained on ever-expanding datasets, the ability to remove the influence of specific data from trained models has become essential for privacy protection and regulatory compliance. Unlearning addresses this challenge by selectively removing parametric knowledge from the trained models without retraining from scratch, which is critical for resource-intensive models such as Large Language Models (LLMs). Existing unlearning methods often degrade model performance by removing more information than necessary when attempting to ''forget'' specific data. We introduce Forgetting-MarI, an LLM unlearning framework that provably removes only the additional (marginal) information contributed by the data to be unlearned, while preserving the information supported by the data to be retained. By penalizing marginal information, our method yields an explicit upper bound on the unlearn dataset's residual influence in the trained models, providing provable undetectability. Extensive experiments confirm that our approach outperforms current state-of-the-art unlearning methods, delivering reliable forgetting and better preserved general model performance across diverse benchmarks. This advancement represents an important step toward making AI systems more controllable and compliant with privacy and copyright regulations without compromising their effectiveness.

</details>


### [15] [An Analysis of Architectural Impact on LLM-based Abstract Visual Reasoning: A Systematic Benchmark on RAVEN-FAIR](https://arxiv.org/abs/2511.11916)
*Sinan Urgun,Seçkin Arı*

Main category: cs.AI

TL;DR: This study assesses four large language models (LLMs) using four reasoning architectures on a visual reasoning dataset, highlighting GPT-4.1-Mini as the strongest performer.


<details>
  <summary>Details</summary>
Motivation: To evaluate and benchmark the abstract visual reasoning performance of different LLMs using diverse reasoning architectures.

Method: The study tested four LLMs using four reasoning approaches on the RAVEN-FAIR dataset, assessed responses with SSIM, LPIPS, and Chain-of-Thought scores, analyzed errors, and considered variations in architecture sensitivity and model performance using multi-run evaluations.

Result: GPT-4.1-Mini demonstrated the highest accuracy, while architecture sensitivity was model-dependent. Multi-run evaluations revealed variability as a key factor in assessing model performance.

Conclusion: LLM reasoning capabilities vary by architecture and model; multi-run evaluations are critical for robust performance analysis.

Abstract: This study aims to systematically evaluate the performance of large language models (LLMs) in abstract visual reasoning problems. We examined four LLM models (GPT-4.1-Mini, Claude-3.5-Haiku, Gemini-1.5-Flash, Llama-3.3-70b) utilizing four different reasoning architectures (single-shot, embedding-controlled repetition, self-reflection, and multi-agent) on the RAVEN-FAIR dataset. Visual responses generated through a three-stage process (JSON extraction, LLM reasoning, and Tool Function) were evaluated using SSIM and LPIPS metrics; Chain-of-Thought scores and error types (semantic hallucination, numeric misperception) were analyzed. Results demonstrate that GPT-4.1-Mini consistently achieved the highest overall accuracy across all architectures, indicating a strong reasoning capability. While the multi-agent architecture occasionally altered semantic and numeric balance across models, these effects were not uniformly beneficial. Instead, each model exhibited distinct sensitivity patterns to architectural design, underscoring that reasoning effectiveness remains model-specific. Variations in response coverage further emerged as a confounding factor that complicates direct cross-architecture comparison. To estimate the upper-bound performance of each configuration, we report the best of five independent runs, representing a best-case scenario rather than an averaged outcome. This multi-run strategy aligns with recent recommendations, which emphasize that single-run evaluations are fragile and may lead to unreliable conclusions.

</details>


### [16] [A Neuromorphic Architecture for Scalable Event-Based Control](https://arxiv.org/abs/2511.11924)
*Yongkang Huo,Fulvio Forni,Rodolphe Sepulchre*

Main category: cs.AI

TL;DR: The paper introduces an RWTA motif for neuromorphic control, combining discrete and continuous computation in a unified architecture.


<details>
  <summary>Details</summary>
Motivation: To create a scalable neuromorphic architecture that effectively integrates discrete computation and continuous regulation.

Method: Introduces an RWTA motif merging winner-take-all state machines and biophysical circuits. Demonstrates its utility using a snake robot.

Result: Achieves a versatile, robust, and modular architecture unifying rhythmic generation and decision-making.

Conclusion: The RWTA motif provides a reliable, tunable framework for neuromorphic control in robotic designs.

Abstract: This paper introduces the ``rebound Winner-Take-All (RWTA)" motif as the basic element of a scalable neuromorphic control architecture. From the cellular level to the system level, the resulting architecture combines the reliability of discrete computation and the tunability of continuous regulation: it inherits the discrete computation capabilities of winner-take-all state machines and the continuous tuning capabilities of excitable biophysical circuits. The proposed event-based framework addresses continuous rhythmic generation and discrete decision-making in a unified physical modeling language. We illustrate the versatility, robustness, and modularity of the architecture through the nervous system design of a snake robot.

</details>


### [17] [Augmenting The Weather: A Hybrid Counterfactual-SMOTE Algorithm for Improving Crop Growth Prediction When Climate Changes](https://arxiv.org/abs/2511.11945)
*Mohammed Temraz,Mark T Keane*

Main category: cs.AI

TL;DR: The paper introduces CFA-SMOTE, a novel AI data augmentation method to address class imbalances and improve prediction in climate-disrupted scenarios.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the ineffectiveness of traditional machine learning models in handling unpredictable, out-of-distribution climate-disrupted data, particularly in the context of extreme weather events.

Method: The proposed method, CFA-SMOTE, merges counterfactual reasoning from Explainable AI (XAI) with SMOTE to synthesize data points representing rare climate events for better predictive accuracy.

Result: Comparative experiments demonstrated that CFA-SMOTE produced better predictive results compared to other counterfactual and class-imbalance methods under varying class-imbalance conditions.

Conclusion: CFA-SMOTE effectively enhances prediction capacities in scenarios impacted by class imbalance, making it a promising tool to tackle climate change challenges in data-driven domains.

Abstract: In recent years, humanity has begun to experience the catastrophic effects of climate change as economic sectors (such as agriculture) struggle with unpredictable and extreme weather events. Artificial Intelligence (AI) should help us handle these climate challenges but its most promising solutions are not good at dealing with climate-disrupted data; specifically, machine learning methods that work from historical data-distributions, are not good at handling out-of-distribution, outlier events. In this paper, we propose a novel data augmentation method, that treats the predictive problems around climate change as being, in part, due to class-imbalance issues; that is, prediction from historical datasets is difficult because, by definition, they lack sufficient minority-class instances of "climate outlier events". This novel data augmentation method -- called Counterfactual-Based SMOTE (CFA-SMOTE) -- combines an instance-based counterfactual method from Explainable AI (XAI) with the well-known class-imbalance method, SMOTE. CFA-SMOTE creates synthetic data-points representing outlier, climate-events that augment the dataset to improve predictive performance. We report comparative experiments using this CFA-SMOTE method, comparing it to benchmark counterfactual and class-imbalance methods under different conditions (i.e., class-imbalance ratios). The focal climate-change domain used relies on predicting grass growth on Irish dairy farms, during Europe-wide drought and forage crisis of 2018.

</details>


### [18] [LLM-Assisted Formalization Enables Deterministic Detection of Statutory Inconsistency in the Internal Revenue Code](https://arxiv.org/abs/2511.11954)
*Borchuluun Yadamsuren,Steven Keith Platt,Miguel Diaz*

Main category: cs.AI

TL;DR: The paper introduces a hybrid neuro-symbolic framework combining Large Language Models (LLMs) and symbolic logic for detecting statutory inconsistencies, applied to the U.S. Internal Revenue Code.


<details>
  <summary>Details</summary>
Motivation: To address challenges in statutory inconsistency detection in complex legal texts, specifically the U.S. Internal Revenue Code, which involves hierarchical reasoning and deep structured analysis where LLMs currently lack efficiency.

Method: The study experimented with LLMs like GPT-4o and GPT-5 alongside Prolog implementation. It used a dual approach: translating legal text into Prolog rules and comparing inconsistency detection with natural language and Prolog-augmented prompting.

Result: Natural language prompts achieved better rule coverage (100%) than Prolog-augmented prompts (66%), but the hybrid Prolog model delivered deterministic and reproducible results. GPT-5 improved refinement and formalized interpretations, identifying an inconsistency zone in a reproducible manner.

Conclusion: LLM-assisted formalization using symbolic logic ensures efficient, transparent, and reproducible detection of statutory inconsistencies in complex legal frameworks.

Abstract: This study introduces a hybrid neuro-symbolic framework that achieves deterministic detection of statutory inconsistency in complex law. We use the U.S. Internal Revenue Code (IRC) as a case study because its complexity makes it a fertile domain for identifying conflicts. Our research offers a solution for detecting inconsistent provisions by combining Large Language Models (LLMs) with symbolic logic.
  LLM-based methods can support compliance, fairness, and statutory drafting, yet tax-specific applications remain sparse. A key challenge is that such models struggle with hierarchical processing and deep structured reasoning, especially over long text.
  This research addresses these gaps through experiments using GPT-4o, GPT-5, and Prolog. GPT-4o was first used to translate Section 121 into Prolog rules and refine them in SWISH. These rules were then incorporated into prompts to test whether Prolog-augmented prompting improved GPT-4o's inconsistency detection. GPT-4o, whether prompted with natural language alone or with Prolog augmentation, detected the inconsistency in only one of three strategies (33 percent accuracy), but its reasoning quality differed: natural-language prompting achieved 100 percent rule coverage, while Prolog-augmented prompting achieved 66 percent, indicating more incomplete statutory analysis.
  In contrast to probabilistic prompting, the hybrid Prolog model produced deterministic and reproducible results. Guided by GPT-5 for refinement, the model formalized the IRC section's competing interpretations and successfully detected an inconsistency zone. Validation tests confirm that the Prolog implementation is accurate, internally consistent, deterministic, and capable of autonomously identifying inconsistencies. These findings show that LLM-assisted formalization, anchored in symbolic logic, enables transparent and reliable statutory inconsistency detection.

</details>


### [19] [Improving Autoformalization Using Direct Dependency Retrieval](https://arxiv.org/abs/2511.11990)
*Shaoqi Wang,Lu Yu,Chunjie Yang*

Main category: cs.AI

TL;DR: A novel DDR-based retrieval framework is proposed for statement autoformalization, addressing challenges like lack of contextual awareness and dependency retrieval inefficiency in existing methods.


<details>
  <summary>Details</summary>
Motivation: The paper intends to enhance formal verification by improving statement autoformalization techniques, addressing issues like hallucination and inefficiency in current frameworks.

Method: The DDR framework generates candidate library dependencies from natural language descriptions and verifies them via efficient suffix array checks, followed by dataset construction and model fine-tuning.

Result: The DDR model effectively surpasses existing SOTA methods in retrieval precision and recall, enabling more accurate and stable autoformalization processes.

Conclusion: This DDR-based approach significantly improves the accuracy and stability of autoformalization, demonstrating its potential to advance formal verification through better retrieval mechanisms.

Abstract: The convergence of deep learning and formal mathematics has spurred research in formal verification. Statement autoformalization, a crucial first step in this process, aims to translate informal descriptions into machine-verifiable representations but remains a significant challenge. The core difficulty lies in the fact that existing methods often suffer from a lack of contextual awareness, leading to hallucination of formal definitions and theorems. Furthermore, current retrieval-augmented approaches exhibit poor precision and recall for formal library dependency retrieval, and lack the scalability to effectively leverage ever-growing public datasets. To bridge this gap, we propose a novel retrieval-augmented framework based on DDR (\textit{Direct Dependency Retrieval}) for statement autoformalization. Our DDR method directly generates candidate library dependencies from natural language mathematical descriptions and subsequently verifies their existence within the formal library via an efficient suffix array check. Leveraging this efficient search mechanism, we constructed a dependency retrieval dataset of over 500,000 samples and fine-tuned a high-precision DDR model. Experimental results demonstrate that our DDR model significantly outperforms SOTA methods in both retrieval precision and recall. Consequently, an autoformalizer equipped with DDR shows consistent performance advantages in both single-attempt accuracy and multi-attempt stability compared to models using traditional selection-based RAG methods.

</details>


### [20] [Look As You Think: Unifying Reasoning and Visual Evidence Attribution for Verifiable Document RAG via Reinforcement Learning](https://arxiv.org/abs/2511.12003)
*Shuochen Liu,Pengfei Luo,Chao Zhang,Yuhao Chen,Haotian Zhang,Qi Liu,Xin Kou,Tong Xu,Enhong Chen*

Main category: cs.AI

TL;DR: This paper introduces the Chain-of-Evidence (CoE) paradigm and Look As You Think (LAT) framework for visual document retrieval-augmented generation (VD-RAG), improving visual evidence attribution and reasoning in multimodal question answering.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of fine-grained supervision and progressive traceability in existing VD-RAG methods for reliable and verifiable predictions from vision-language models in multimodal settings.

Method: The paper introduces the Chain-of-Evidence (CoE) paradigm, which integrates reasoning steps with visual evidence through bounding boxes and page indices, and proposes the reinforcement learning-based LAT framework for training models to generate consistent and verifiable reasoning paths.

Result: Experiments using benchmarks like Paper- and Wiki-VISA show that LAT achieves significant improvements: a 8.23% gain in soft exact match and a 47.0% gain in IoU@0.5, outperforming existing methods, including supervised fine-tuning baselines.

Conclusion: The CoE paradigm combined with LAT enhances the traceability and verification of reasoning in VD-RAG, offering substantial performance improvements and better generalization across domains.

Abstract: Aiming to identify precise evidence sources from visual documents, visual evidence attribution for visual document retrieval-augmented generation (VD-RAG) ensures reliable and verifiable predictions from vision-language models (VLMs) in multimodal question answering. Most existing methods adopt end-to-end training to facilitate intuitive answer verification. However, they lack fine-grained supervision and progressive traceability throughout the reasoning process. In this paper, we introduce the Chain-of-Evidence (CoE) paradigm for VD-RAG. CoE unifies Chain-of-Thought (CoT) reasoning and visual evidence attribution by grounding reference elements in reasoning steps to specific regions with bounding boxes and page indexes. To enable VLMs to generate such evidence-grounded reasoning, we propose Look As You Think (LAT), a reinforcement learning framework that trains models to produce verifiable reasoning paths with consistent attribution. During training, LAT evaluates the attribution consistency of each evidence region and provides rewards only when the CoE trajectory yields correct answers, encouraging process-level self-verification. Experiments on vanilla Qwen2.5-VL-7B-Instruct with Paper- and Wiki-VISA benchmarks show that LAT consistently improves the vanilla model in both single- and multi-image settings, yielding average gains of 8.23% in soft exact match (EM) and 47.0% in IoU@0.5. Meanwhile, LAT not only outperforms the supervised fine-tuning baseline, which is trained to directly produce answers with attribution, but also exhibits stronger generalization across domains.

</details>


### [21] [Adaptive Diagnostic Reasoning Framework for Pathology with Multimodal Large Language Models](https://arxiv.org/abs/2511.12008)
*Yunqi Hong,Johnson Kao,Liam Edwards,Nein-Tzu Liu,Chung-Yen Huang,Alex Oliveira-Kowaleski,Cho-Jui Hsieh,Neil Y. C. Lin*

Main category: cs.AI

TL;DR: RECAP-PATH, a self-learning AI framework for pathology, uses two-phase learning to improve diagnostic reasoning and accuracy without extensive labeled data.


<details>
  <summary>Details</summary>
Motivation: AI tools enhance pathology but face challenges due to their lack of interpretable reasoning, limiting broader adoption in clinical settings.

Method: RECAP-PATH employs a two-phase learning process—diversification and optimization—using multimodal large language models to generate interpretable, evidence-linked diagnostic explanations.

Result: RECAP-PATH achieved expert-aligned rationales and significant gains in diagnostic accuracy, evaluated on breast and prostate cancer datasets.

Conclusion: RECAP-PATH bridges visual understanding with diagnostic reasoning, offering reliable, interpretable, and generalizable AI for clinical use.

Abstract: AI tools in pathology have improved screening throughput, standardized quantification, and revealed prognostic patterns that inform treatment. However, adoption remains limited because most systems still lack the human-readable reasoning needed to audit decisions and prevent errors. We present RECAP-PATH, an interpretable framework that establishes a self-learning paradigm, shifting off-the-shelf multimodal large language models from passive pattern recognition to evidence-linked diagnostic reasoning. At its core is a two-phase learning process that autonomously derives diagnostic criteria: diversification expands pathology-style explanations, while optimization refines them for accuracy. This self-learning approach requires only small labeled sets and no white-box access or weight updates to generate cancer diagnoses. Evaluated on breast and prostate datasets, RECAP-PATH produced rationales aligned with expert assessment and delivered substantial gains in diagnostic accuracy over baselines. By uniting visual understanding with reasoning, RECAP-PATH provides clinically trustworthy AI and demonstrates a generalizable path toward evidence-linked interpretation.

</details>


### [22] [Intelligent Collaborative Optimization for Rubber Tyre Film Production Based on Multi-path Differentiated Clipping Proximal Policy Optimization](https://arxiv.org/abs/2511.12060)
*Yinghao Ruan,Wei Pang,Shuaihao Liu,Huili Yang,Leyi Han,Xinghui Dong*

Main category: cs.AI

TL;DR: The paper introduces a novel deep reinforcement learning algorithm, MPD-PPO, to tackle scheduling and production challenges in smart tyre manufacturing systems, achieving higher efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Current systems in tyre manufacturing face challenges in coordinating complex, dynamic networks of subsystems and addressing the limitations of centralized methods, requiring innovative techniques for optimization and real-time adaptation.

Method: The research develops the Multi-path Differentiated Clipping Proximal Policy Optimization (MPD-PPO) algorithm, which features a multi-branch policy architecture and differentiated gradient clipping constraints, to handle high-dimensional, multi-objective optimization in tyre manufacturing.

Result: The MPD-PPO algorithm significantly improves tuning accuracy and operational efficiency, overcoming problems related to high dimensionality, multi-objective trade-offs, and dynamic system adaptation.

Conclusion: This framework provides enhanced performance and stability in tyre manufacturing while supporting real-time deployment, marking a significant advancement in smart manufacturing for the rubber tyre industry.

Abstract: The advent of smart manufacturing is addressing the limitations of traditional centralized scheduling and inflexible production line configurations in the rubber tyre industry, especially in terms of coping with dynamic production demands. Contemporary tyre manufacturing systems form complex networks of tightly coupled subsystems pronounced nonlinear interactions and emergent dynamics. This complexity renders the effective coordination of multiple subsystems, posing an essential yet formidable task. For high-dimensional, multi-objective optimization problems in this domain, we introduce a deep reinforcement learning algorithm: Multi-path Differentiated Clipping Proximal Policy Optimization (MPD-PPO). This algorithm employs a multi-branch policy architecture with differentiated gradient clipping constraints to ensure stable and efficient high-dimensional policy updates. Validated through experiments on width and thickness control in rubber tyre film production, MPD-PPO demonstrates substantial improvements in both tuning accuracy and operational efficiency. The framework successfully tackles key challenges, including high dimensionality, multi-objective trade-offs, and dynamic adaptation, thus delivering enhanced performance and production stability for real-time industrial deployment in tyre manufacturing.

</details>


### [23] [Bayesian Optimization in Language Space: An Eval-Efficient AI Self-Improvement Framework](https://arxiv.org/abs/2511.12063)
*Enoch Hyunwook Kang,Hema Yoganarasimhan*

Main category: cs.AI

TL;DR: The paper introduces T-BoN BO, a Bayesian optimization framework for improving evaluation efficiency in AI tasks, especially where evaluating outcomes is costly. It outperforms existing methods in empirical tests.


<details>
  <summary>Details</summary>
Motivation: Optimize evaluation efficiency in language-based AI tasks, addressing limitations where evaluation (not generating solutions) is the costliest aspect.

Method: Combines Best-of-N selection with textual gradients to emulate optimal exploration behavior, enabling efficient evaluation in Bayesian optimization.

Result: T-BoN BO achieves superior performance in automated persona ad alignment tasks compared to state-of-the-art baselines.

Conclusion: The proposed framework effectively improves evaluation efficiency, offering a practical solution for cost-intensive societal applications and advancing self-improving AI systems.

Abstract: Large Language Models (LLMs) have recently enabled self-improving AI, i.e., AI that iteratively generates, evaluates, and refines its own outcomes. Recent studies have shown that self-improving AI focusing on prompt optimization can outperform state-of-the-art reinforcement-learning fine-tuned LLMs. Here, their `performance' is typically measured by query efficiency - the number of LLM-generated solution samples required to meet a certain performance threshold. However, in many societal applications, the primary limitation is not generating new solutions but evaluating them. For instance, evaluating an ad's effectiveness requires significant human feedback, which is far more costly and time-consuming than generating a candidate ad. To optimize for the evaluation efficiency objective, a natural approach is to extend Bayesian Optimization (BO), a framework proven optimal for evaluation efficiency, to the language domain. However, the difficulty of directly estimating suitable acquisition functions in LLMs' minds makes this extension challenging. This paper overcomes this challenge by proving that the combination of the simple and widely used Best-of-N selection strategy and simple textual gradients (i.e., textual edits from a critic model) statistically emulates the behavior of the gradients on the canonical UCB acquisition function, which induces optimal exploration in terms of evaluation efficiency. Based on this result, we propose TextGrad-Best-of-N Bayesian Optimization (T-BoN BO), a simple and eval-efficient language-space Bayesian optimization framework for AI self-improvement. We also empirically validate T-BoN BO by applying it to automated ad alignment tasks for persona distribution, demonstrating its superior performance compared to popular state-of-the-art baselines.

</details>


### [24] [No-Regret Strategy Solving in Imperfect-Information Games via Pre-Trained Embedding](https://arxiv.org/abs/2511.12083)
*Yanchang Fu,Shengda Liu,Pei Xu,Kaiqi Huang*

Main category: cs.AI

TL;DR: The paper introduces Embedding CFR, a novel algorithm for solving strategies in imperfect-information extensive-form games like poker, by using low-dimensional embedding spaces to improve abstraction quality.


<details>
  <summary>Details</summary>
Motivation: Address shortcomings in state-of-the-art abstraction methods for imperfect-information extensive-form games, especially the loss of critical information during discrete clustering.

Method: Develop a strategy-solving algorithm (Embedding CFR) that embeds isolated information sets into a continuous low-dimensional space, enabling precise distinction and interconnection of features for better strategy optimization.

Result: Embedding CFR outperforms traditional cluster-based abstraction methods, achieving faster exploitability convergence while using the same spatial resources.

Conclusion: Embedding CFR successfully enhances the strategy-solving process in large-scale games, promising advancements in poker AI and other IIEFG applications.

Abstract: High-quality information set abstraction remains a core challenge in solving large-scale imperfect-information extensive-form games (IIEFGs)-such as no-limit Texas Hold'em-where the finite nature of spatial resources hinders strategy solving over the full game. State-of-the-art AI methods rely on pre-trained discrete clustering for abstraction, yet their hard classification irreversibly loses critical information: specifically, the quantifiable subtle differences between information sets-vital for strategy solving-thereby compromising the quality of such solving. Inspired by the word embedding paradigm in natural language processing, this paper proposes the Embedding CFR algorithm, a novel approach for solving strategies in IIEFGs within an embedding space. The algorithm pre-trains and embeds features of isolated information sets into an interconnected low-dimensional continuous space, where the resulting vectors more precisely capture both the distinctions and connections between information sets. Embedding CFR presents a strategy-solving process driven by regret accumulation and strategy updates within this embedding space, with accompanying theoretical analysis verifying its capacity to reduce cumulative regret. Experiments on poker show that with the same spatial overhead, Embedding CFR achieves significantly faster exploitability convergence compared to cluster-based abstraction algorithms, confirming its effectiveness. Furthermore, to our knowledge, it is the first algorithm in poker AI that pre-trains information set abstractions through low-dimensional embedding for strategy solving.

</details>


### [25] [KrwEmd: Revising the Imperfect-Recall Abstraction from Forgetting Everything](https://arxiv.org/abs/2511.12089)
*Yanchang Fu,Qiyue Yin,Shengda Liu,Pei Xu,Kaiqi Huang*

Main category: cs.AI

TL;DR: The paper proposes KrwEmd, an algorithm that tackles the problem of excessive abstraction in AI for Texas hold'em, improving its gameplay performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the performance degradation caused by excessive abstraction in large-scale imperfect-information games, specifically in how historical information is discarded.

Method: KrwEmd uses the k-recall winrate feature to consider both future and historical game information and clusters signal observation infosets based on earth mover's distance.

Result: KrwEmd significantly enhances AI performance in gameplay compared to existing approaches.

Conclusion: The KrwEmd algorithm effectively reduces excessive abstraction issues, making AI strategies more robust in handling imperfect-information games.

Abstract: Excessive abstraction is a critical challenge in hand abstraction-a task specific to games like Texas hold'em-when solving large-scale imperfect-information games, as it impairs AI performance. This issue arises from extreme implementations of imperfect-recall abstraction, which entirely discard historical information. This paper presents KrwEmd, the first practical algorithm designed to address this problem. We first introduce the k-recall winrate feature, which not only qualitatively distinguishes signal observation infosets by leveraging both future and, crucially, historical game information, but also quantitatively captures their similarity. We then develop the KrwEmd algorithm, which clusters signal observation infosets using earth mover's distance to measure discrepancies between their features. Experimental results demonstrate that KrwEmd significantly improves AI gameplay performance compared to existing algorithms.

</details>


### [26] [MetaGDPO: Alleviating Catastrophic Forgetting with Metacognitive Knowledge through Group Direct Preference Optimization](https://arxiv.org/abs/2511.12113)
*Lanxue Zhang,Yuqiang Xie,Fang Fang,Fanglong Dong,Rui Liu,Yanan Cao*

Main category: cs.AI

TL;DR: Existing fine-tuning methods for compressing reasoning capabilities of Large Language Models cause catastrophic forgetting in smaller models. This paper introduces a dataset and a training method to address this issue.


<details>
  <summary>Details</summary>
Motivation: Current datasets and fine-tuning methods fail to preserve inherent knowledge during the compression process from Large Language Models to smaller ones, especially smaller than 8B models.

Method: The authors present a dataset with 5K reasoning tasks enriched with metacognitive knowledge and propose GDPO (Group Direction Preference Optimization), a training method efficient for resource-limited scenarios.

Result: The proposed data and training methods significantly reduce catastrophic forgetting while improving reasoning performance in smaller models.

Conclusion: By combining specific data designs and the GDPO method, the paper provides an effective solution to transfer knowledge from Large Language Models to smaller ones without major performance degradation.

Abstract: Large Language Models demonstrate strong reasoning capabilities, which can be effectively compressed into smaller models. However, existing datasets and fine-tuning approaches still face challenges that lead to catastrophic forgetting, particularly for models smaller than 8B. First, most datasets typically ignore the relationship between training data knowledge and the model's inherent abilities, making it difficult to preserve prior knowledge. Second, conventional training objectives often fail to constrain inherent knowledge preservation, which can result in forgetting of previously learned skills. To address these issues, we propose a comprehensive solution that alleviates catastrophic forgetting from both the data and fine-tuning approach perspectives. On the data side, we construct a dataset of 5K instances that covers multiple reasoning tasks and incorporates metacognitive knowledge, making it more tolerant and effective for distillation into smaller models. We annotate the metacognitive knowledge required to solve each question and filter the data based on task knowledge and the model's inherent skills. On the training side, we introduce GDPO (Group Direction Preference Optimization), which is better suited for resource-limited scenarios and can efficiently approximate the performance of GRPO. Guided by the large model and by implicitly constraining the optimization path through a reference model, GDPO enables more effective knowledge transfer from the large model and constrains excessive parameter drift. Extensive experiments demonstrate that our approach significantly alleviates catastrophic forgetting and improves reasoning performance on smaller models.

</details>


### [27] [RTMol: Rethinking Molecule-text Alignment in a Round-trip View](https://arxiv.org/abs/2511.12135)
*Letian Chen,Runhan Shi,Gufeng Yu,Yang Yang*

Main category: cs.AI

TL;DR: RTMol is a self-supervised framework that aligns molecular sequences with text, improving bidirectional tasks by 47% without needing paired data.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address limitations in molecular-text alignment methods, including poor chemical accuracy, ambiguous datasets, and lack of bidirectional optimization.

Method: RTMol employs self-supervised round-trip learning to unify molecular captioning and text-based molecular generation, introducing new evaluation metrics.

Result: The approach enhances bidirectional molecular-text alignment by up to 47% across various large language models (LLMs).

Conclusion: RTMol sets a new paradigm for molecule-text understanding and generation, offering improved alignment and performance without relying on paired datasets.

Abstract: Aligning molecular sequence representations (e.g., SMILES notations) with textual descriptions is critical for applications spanning drug discovery, materials design, and automated chemical literature analysis. Existing methodologies typically treat molecular captioning (molecule-to-text) and text-based molecular design (text-to-molecule) as separate tasks, relying on supervised fine-tuning or contrastive learning pipelines. These approaches face three key limitations: (i) conventional metrics like BLEU prioritize linguistic fluency over chemical accuracy, (ii) training datasets frequently contain chemically ambiguous narratives with incomplete specifications, and (iii) independent optimization of generation directions leads to bidirectional inconsistency. To address these issues, we propose RTMol, a bidirectional alignment framework that unifies molecular captioning and text-to-SMILES generation through self-supervised round-trip learning. The framework introduces novel round-trip evaluation metrics and enables unsupervised training for molecular captioning without requiring paired molecule-text corpora. Experiments demonstrate that RTMol enhances bidirectional alignment performance by up to 47% across various LLMs, establishing an effective paradigm for joint molecule-text understanding and generation.

</details>


### [28] [Incremental Maintenance of DatalogMTL Materialisations](https://arxiv.org/abs/2511.12169)
*Kaiyue Zhao,Dingqi Chen,Shaoyu Wang,Pan Hu*

Main category: cs.AI

TL;DR: The paper introduces DRedMTL, an incremental reasoning algorithm for DatalogMTL, addressing efficient dynamic updates in temporal logic reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing methods for reasoning with DatalogMTL lack efficiency in dynamic updates, which are essential for applications involving frequent data changes.

Method: The DRedMTL algorithm extends the classical DRed algorithm, accounting for periodic intervals to efficiently represent and update DatalogMTL materialisations.

Result: DRedMTL is implemented and tested on publicly available datasets, demonstrating substantial performance improvements over rematerialisation approaches.

Conclusion: DRedMTL efficiently handles dynamic updates in DatalogMTL materialisations, making it superior for real-world applications, as evidenced by experimental results.

Abstract: DatalogMTL extends the classical Datalog language with metric temporal logic (MTL), enabling expressive reasoning over temporal data. While existing reasoning approaches, such as materialisation based and automata based methods, offer soundness and completeness, they lack support for handling efficient dynamic updates, a crucial requirement for real-world applications that involve frequent data updates. In this work, we propose DRedMTL, an incremental reasoning algorithm for DatalogMTL with bounded intervals. Our algorithm builds upon the classical DRed algorithm, which incrementally updates the materialisation of a Datalog program. Unlike a Datalog materialisation which is in essence a finite set of facts, a DatalogMTL materialisation has to be represented as a finite set of facts plus periodic intervals indicating how the full materialisation can be constructed through unfolding. To cope with this, our algorithm is equipped with specifically designed operators to efficiently handle such periodic representations of DatalogMTL materialisations. We have implemented this approach and tested it on several publicly available datasets. Experimental results show that DRedMTL often significantly outperforms rematerialisation, sometimes by orders of magnitude.

</details>


### [29] [Debate over Mixed-knowledge: A Robust Multi-Agent Framework for Incomplete Knowledge Graph Question Answering](https://arxiv.org/abs/2511.12208)
*Jilong Liu,Pengyang Shao,Wei Qin,Fei Liu,Yonghui Yang,Richang Hong*

Main category: cs.AI

TL;DR: The paper introduces the DoM framework to address Incomplete Knowledge Graph Question Answering (IKGQA) using dynamic integration of structured and unstructured knowledge, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Improve factual accuracy in KGQA by addressing knowledge graph incompleteness using adaptive integration of multiple knowledge sources.

Method: DoM uses a multi-agent paradigm with specialized agents for structured and unstructured data, iterative interactions, sub-question decomposition, evidence retrieval, and a judging agent for answer aggregation.

Result: DoM consistently outperforms state-of-the-art methods in IKGQA benchmarks and introduces a realistic dataset reflecting real-world KG incompleteness.

Conclusion: The proposed DoM framework enhances robustness and efficiently tackles KG incompleteness with dynamic knowledge integration, validated by extensive experiments and a realistic dataset.

Abstract: Knowledge Graph Question Answering (KGQA) aims to improve factual accuracy by leveraging structured knowledge. However, real-world Knowledge Graphs (KGs) are often incomplete, leading to the problem of Incomplete KGQA (IKGQA). A common solution is to incorporate external data to fill knowledge gaps, but existing methods lack the capacity to adaptively and contextually fuse multiple sources, failing to fully exploit their complementary strengths. To this end, we propose Debate over Mixed-knowledge (DoM), a novel framework that enables dynamic integration of structured and unstructured knowledge for IKGQA. Built upon the Multi-Agent Debate paradigm, DoM assigns specialized agents to perform inference over knowledge graphs and external texts separately, and coordinates their outputs through iterative interaction. It decomposes the input question into sub-questions, retrieves evidence via dual agents (KG and Retrieval-Augmented Generation, RAG), and employs a judge agent to evaluate and aggregate intermediate answers. This collaboration exploits knowledge complementarity and enhances robustness to KG incompleteness. In addition, existing IKGQA datasets simulate incompleteness by randomly removing triples, failing to capture the irregular and unpredictable nature of real-world knowledge incompleteness. To address this, we introduce a new dataset, Incomplete Knowledge Graph WebQuestions, constructed by leveraging real-world knowledge updates. These updates reflect knowledge beyond the static scope of KGs, yielding a more realistic and challenging benchmark. Through extensive experiments, we show that DoM consistently outperforms state-of-the-art baselines.

</details>


### [30] [ViTE: Virtual Graph Trajectory Expert Router for Pedestrian Trajectory Prediction](https://arxiv.org/abs/2511.12214)
*Ruochen Li,Zhanxing Zhu,Tanqiu Qiao,Hubert P. H. Shum*

Main category: cs.AI

TL;DR: The paper introduces ViTE, a framework for pedestrian trajectory prediction, addressing challenges of balancing model depth and computational efficiency. It incorporates Virtual Graph and Expert Router components to enhance interaction modeling.


<details>
  <summary>Details</summary>
Motivation: The study aims to overcome limitations in pedestrian trajectory prediction models that either fail to capture high-order interactions due to insufficient depth or suffer computational inefficiencies from excessive GNN layers.

Method: The proposed ViTE framework integrates Virtual Graphs for modeling long-range interactions dynamically, and an Expert Router employing Mixture-of-Experts design for adaptive expert selection based on social context.

Result: ViTE achieves state-of-the-art performance across ETH/UCY, NBA, and SDD benchmarks, demonstrating improved effectiveness and efficiency compared to existing approaches.

Conclusion: The model effectively addresses the trade-off between layer depth and computational cost while delivering robust high-order interaction modeling, demonstrating its value in trajectory prediction tasks.

Abstract: Pedestrian trajectory prediction is critical for ensuring safety in autonomous driving, surveillance systems, and urban planning applications. While early approaches primarily focus on one-hop pairwise relationships, recent studies attempt to capture high-order interactions by stacking multiple Graph Neural Network (GNN) layers. However, these approaches face a fundamental trade-off: insufficient layers may lead to under-reaching problems that limit the model's receptive field, while excessive depth can result in prohibitive computational costs. We argue that an effective model should be capable of adaptively modeling both explicit one-hop interactions and implicit high-order dependencies, rather than relying solely on architectural depth. To this end, we propose ViTE (Virtual graph Trajectory Expert router), a novel framework for pedestrian trajectory prediction. ViTE consists of two key modules: a Virtual Graph that introduces dynamic virtual nodes to model long-range and high-order interactions without deep GNN stacks, and an Expert Router that adaptively selects interaction experts based on social context using a Mixture-of-Experts design. This combination enables flexible and scalable reasoning across varying interaction patterns. Experiments on three benchmarks (ETH/UCY, NBA, and SDD) demonstrate that our method consistently achieves state-of-the-art performance, validating both its effectiveness and practical efficiency.

</details>


### [31] [Beyond World Models: Rethinking Understanding in AI Models](https://arxiv.org/abs/2511.12239)
*Tarun Gupta,Danish Pruthi*

Main category: cs.AI

TL;DR: The paper evaluates the limitations of AI world models in achieving human-level understanding through case studies in the philosophy of science.


<details>
  <summary>Details</summary>
Motivation: Human mental world models offer a representation of understanding, and researchers are exploring whether AI world models can mimic this to demonstrate human-like understanding.

Method: The paper employs philosophical case studies to analyze the distinction between AI world model capabilities and human understanding.

Result: The analysis reveals pronounced differences between world model capabilities and human understanding, highlighting limitations in current approaches.

Conclusion: AI world models, while valuable, may fall short of human-level comprehension, emphasizing the need for further inquiry into these distinctions.

Abstract: World models have garnered substantial interest in the AI community. These are internal representations that simulate aspects of the external world, track entities and states, capture causal relationships, and enable prediction of consequences. This contrasts with representations based solely on statistical correlations. A key motivation behind this research direction is that humans possess such mental world models, and finding evidence of similar representations in AI models might indicate that these models "understand" the world in a human-like way. In this paper, we use case studies from the philosophy of science literature to critically examine whether the world model framework adequately characterizes human-level understanding. We focus on specific philosophical analyses where the distinction between world model capabilities and human understanding is most pronounced. While these represent particular views of understanding rather than universal definitions, they help us explore the limits of world models.

</details>


### [32] [AURA: Development and Validation of an Augmented Unplanned Removal Alert System using Synthetic ICU Videos](https://arxiv.org/abs/2511.12241)
*Junhyuk Seo,Hyeyoon Moon,Kyu-Hwan Jung,Namkee Oh,Taerim Kim*

Main category: cs.AI

TL;DR: The study introduces AURA, a system using synthetic video data for detecting high-risk movements related to unplanned extubation in ICUs.


<details>
  <summary>Details</summary>
Motivation: There is a critical need to address the challenges of unplanned extubation detection in ICUs while preserving privacy and overcoming ethical limitations in data collection.

Method: The researchers used text-to-video diffusion to generate synthetic ICU scenarios and applied pose estimation to detect high-risk movement patterns like collision and agitation.

Result: The system performed with high accuracy for detecting collisions and moderate performance in agitation recognition, supported by expert validation of realistic synthetic data.

Conclusion: AURA offers a privacy-preserving and reproducible approach to patient safety monitoring in ICUs, showcasing its deployment potential without relying on real ICU video data.

Abstract: Unplanned extubation (UE) remains a critical patient safety concern in intensive care units (ICUs), often leading to severe complications or death. Real-time UE detection has been limited, largely due to the ethical and privacy challenges of obtaining annotated ICU video data. We propose Augmented Unplanned Removal Alert (AURA), a vision-based risk detection system developed and validated entirely on a fully synthetic video dataset. By leveraging text-to-video diffusion, we generated diverse and clinically realistic ICU scenarios capturing a range of patient behaviors and care contexts. The system applies pose estimation to identify two high-risk movement patterns: collision, defined as hand entry into spatial zones near airway tubes, and agitation, quantified by the velocity of tracked anatomical keypoints. Expert assessments confirmed the realism of the synthetic data, and performance evaluations showed high accuracy for collision detection and moderate performance for agitation recognition. This work demonstrates a novel pathway for developing privacy-preserving, reproducible patient safety monitoring systems with potential for deployment in intensive care settings.

</details>


### [33] [Mobile-Agent-RAG: Driving Smart Multi-Agent Coordination with Contextual Knowledge Empowerment for Long-Horizon Mobile Automation](https://arxiv.org/abs/2511.12254)
*Yuxiang Zhou,Jichang Li,Yanhao Zhang,Haonan Lu,Guanbin Li*

Main category: cs.AI

TL;DR: The paper introduces Mobile-Agent-RAG, a framework designed to address challenges in mobile agents' task completion by integrating strategic and operational knowledge retrieval.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome limitations of current mobile agents in handling real-world, complex tasks by reducing hallucinations in planning and errors in execution.

Method: The method includes a dual-level retrieval augmentation strategy with Manager-RAG for strategic guidance and Operator-RAG for precise operational instructions, supported by specialized knowledge bases.

Result: Mobile-Agent-RAG outperformed current methods with an 11.0% improvement in task completion rates and a 10.2% increase in step efficiency.

Conclusion: Mobile-Agent-RAG provides a robust approach to improving multi-agent mobile automation, offering better success in complex, long-horizon tasks.

Abstract: Mobile agents show immense potential, yet current state-of-the-art (SoTA) agents exhibit inadequate success rates on real-world, long-horizon, cross-application tasks. We attribute this bottleneck to the agents' excessive reliance on static, internal knowledge within MLLMs, which leads to two critical failure points: 1) strategic hallucinations in high-level planning and 2) operational errors during low-level execution on user interfaces (UI). The core insight of this paper is that high-level planning and low-level UI operations require fundamentally distinct types of knowledge. Planning demands high-level, strategy-oriented experiences, whereas operations necessitate low-level, precise instructions closely tied to specific app UIs. Motivated by these insights, we propose Mobile-Agent-RAG, a novel hierarchical multi-agent framework that innovatively integrates dual-level retrieval augmentation. At the planning stage, we introduce Manager-RAG to reduce strategic hallucinations by retrieving human-validated comprehensive task plans that provide high-level guidance. At the execution stage, we develop Operator-RAG to improve execution accuracy by retrieving the most precise low-level guidance for accurate atomic actions, aligned with the current app and subtask. To accurately deliver these knowledge types, we construct two specialized retrieval-oriented knowledge bases. Furthermore, we introduce Mobile-Eval-RAG, a challenging benchmark for evaluating such agents on realistic multi-app, long-horizon tasks. Extensive experiments demonstrate that Mobile-Agent-RAG significantly outperforms SoTA baselines, improving task completion rate by 11.0% and step efficiency by 10.2%, establishing a robust paradigm for context-aware, reliable multi-agent mobile automation.

</details>


### [34] [MoralReason: Generalizable Moral Decision Alignment For LLM Agents Using Reasoning-Level Reinforcement Learning](https://arxiv.org/abs/2511.12271)
*Zhiyu An,Wan Du*

Main category: cs.AI

TL;DR: This paper addresses training large language models (LLMs) to apply consistent moral reasoning frameworks in scenarios beyond their training data, introducing a dataset called Moral-Reason-QA and a novel learning approach.


<details>
  <summary>Details</summary>
Motivation: The researchers aim to ensure that as LLMs increasingly influence human moral decisions, they can be actively steered to make consistent, ethically sound choices even in unfamiliar scenarios.

Method: The authors introduced Moral-Reason-QA, a dataset of high-ambiguity moral scenarios with framework-specific reasoning traces, and utilized a learning method based on Group Relative Policy Optimization with composite rewards for both decision alignment and moral reasoning generalization.

Result: Experiments showed LLMs successfully generalized to new moral scenarios, improving alignment scores by +0.757 for utilitarian and +0.450 for deontological frameworks, though challenges in training were also identified.

Conclusion: This study establishes that LLMs can be systematically trained to internalize and apply moral frameworks to novel situations, thereby enhancing AI safety and ethical reliability in decision-making applications.

Abstract: Large language models are increasingly influencing human moral decisions, yet current approaches focus primarily on evaluating rather than actively steering their moral decisions. We formulate this as an out-of-distribution moral alignment problem, where LLM agents must learn to apply consistent moral reasoning frameworks to scenarios beyond their training distribution. We introduce Moral-Reason-QA, a novel dataset extending 680 human-annotated, high-ambiguity moral scenarios with framework-specific reasoning traces across utilitarian, deontological, and virtue ethics, enabling systematic evaluation of moral generalization in realistic decision contexts. Our learning approach employs Group Relative Policy Optimization with composite rewards that simultaneously optimize decision alignment and framework-specific reasoning processes to facilitate learning of the underlying moral frameworks. Experimental results demonstrate successful generalization to unseen moral scenarios, with softmax-normalized alignment scores improving by +0.757 for utilitarian and +0.450 for deontological frameworks when tested on out-of-distribution evaluation sets. The experiments also reveal training challenges and promising directions that inform future research. These findings establish that LLM agents can be systematically trained to internalize and apply specific moral frameworks to novel situations, providing a critical foundation for AI safety as language models become more integrated into human decision-making processes.

</details>


### [35] [UpBench: A Dynamically Evolving Real-World Labor-Market Agentic Benchmark Framework Built for Human-Centric AI](https://arxiv.org/abs/2511.12306)
*Darvin Yi,Teng Liu,Mattie Terzolo,Lance Hasson,Ayan Sinh,Pablo Mendes,Andrew Rabinovich*

Main category: cs.AI

TL;DR: The paper introduces UpBench, a benchmark evaluating AI agents' performance in real-world, dynamic labor market tasks from the Upwork platform. It uses human-curated rubrics for nuanced evaluation, aiming to foster human-AI collaboration.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of static, synthetic, and domain-limited benchmarks in evaluating AI models' real-world competence and adaptability in economically meaningful contexts.

Method: UpBench uses real jobs from Upwork, creating a dynamic benchmark with rubric-based evaluations crafted by expert freelancers. The framework reviews AI performance on detailed acceptance criteria and includes human oversight across the job curation and evaluation processes.

Result: Through fine-grained analysis of task performance and instruction adherence, UpBench offers insights into AI strengths and weaknesses while recalibrating the benchmark to reflect shifts in online labor dynamics.

Conclusion: UpBench establishes a scalable, human-centered framework for assessing AI systems in real labor contexts, promoting a collaborative model where AI enhances human potential instead of substituting it.

Abstract: As large language model (LLM) agents increasingly undertake digital work, reliable frameworks are needed to evaluate their real-world competence, adaptability, and capacity for human collaboration. Existing benchmarks remain largely static, synthetic, or domain-limited, providing limited insight into how agents perform in dynamic, economically meaningful environments. We introduce UpBench, a dynamically evolving benchmark grounded in real jobs drawn from the global Upwork labor marketplace. Each task corresponds to a verified client transaction, anchoring evaluation in genuine work activity and financial outcomes. UpBench employs a rubric-based evaluation framework, in which expert freelancers decompose each job into detailed, verifiable acceptance criteria and assess AI submissions with per-criterion feedback. This structure enables fine-grained analysis of model strengths, weaknesses, and instruction-following fidelity beyond binary pass/fail metrics. Human expertise is integrated throughout the data pipeline (from job curation and rubric construction to evaluation) ensuring fidelity to real professional standards and supporting research on human-AI collaboration. By regularly refreshing tasks to reflect the evolving nature of online work, UpBench provides a scalable, human-centered foundation for evaluating agentic systems in authentic labor-market contexts, offering a path toward a collaborative framework, where AI amplifies human capability through partnership rather than replacement.

</details>


### [36] [Reward and Guidance through Rubrics: Promoting Exploration to Improve Multi-Domain Reasoning](https://arxiv.org/abs/2511.12344)
*Baolong Bi,Shenghua Liu,Yiwei Wang,Siqian Tong,Lingrui Mei,Yuyao Ge,Yilong Xu,Jiafeng Guo,Xueqi Cheng*

Main category: cs.AI

TL;DR: The paper introduces RGR-GRPO, a novel rubric-guided reinforcement learning framework to enhance multi-domain reasoning in LLMs, outperforming traditional RL approaches.


<details>
  <summary>Details</summary>
Motivation: Existing reinforcement learning methods for large language models focus on single-domain reasoning with verifiable rewards (RLVR), limiting exploration space and reasoning potential.

Method: The proposed RGR-GRPO framework uses rubric-driven guidance to provide dense, fine-grained reward signals and enables LLMs to explore broader solution spaces through offline guidance and GRPO training.

Result: RGR-GRPO outperforms baseline RL methods across 14 benchmarks, achieving average improvements of +7.0% to +8.4% in multiple reasoning tasks and demonstrating superior pass@k performance.

Conclusion: By leveraging rubrics for reinforcement learning, RGR-GRPO enhances reasoning capabilities and overcomes exploration limitations, providing reliable performance improvements in multi-domain tasks.

Abstract: Recent advances in reinforcement learning (RL) have significantly improved the complex reasoning capabilities of large language models (LLMs). Despite these successes, existing methods mainly focus on single-domain RL (e.g., mathematics) with verifiable rewards (RLVR), and their reliance on purely online RL frameworks restricts the exploration space, thereby limiting reasoning performance. In this paper, we address these limitations by leveraging rubrics to provide both fine-grained reward signals and offline guidance. We propose $\textbf{RGR-GRPO}$ (Reward and Guidance through Rubrics), a rubric-driven RL framework for multi-domain reasoning. RGR-GRPO enables LLMs to receive dense and informative rewards while exploring a larger solution space during GRPO training. Extensive experiments across 14 benchmarks spanning multiple domains demonstrate that RGR-GRPO consistently outperforms RL methods that rely solely on alternative reward schemes or offline guidance. Compared with verifiable online RL baseline, RGR-GRPO achieves average improvements of +7.0%, +5.4%, +8.4%, and +6.6% on mathematics, physics, chemistry, and general reasoning tasks, respectively. Notably, RGR-GRPO maintains stable entropy fluctuations during off-policy training and achieves superior pass@k performance, reflecting sustained exploration and effective breakthrough beyond existing performance bottlenecks.

</details>


### [37] [More Than Irrational: Modeling Belief-Biased Agents](https://arxiv.org/abs/2511.12359)
*Yifan Zhu,Sammie Katt,Samuel Kaski*

Main category: cs.AI

TL;DR: The paper introduces a computational framework to model cognitively bounded agents making decisions under biased beliefs and proposes an efficient inference method to track user behavior and limitations.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of predicting sub-optimal user behaviors that arise from bounded cognitive capacities and biased beliefs, with implications for adaptive AI systems.

Method: Development of a computational-rational (CR) user model with parameterized cognitive processes and an online inference method using nested particle filtering to track latent belief states and cognitive bounds.

Result: The CR models simulate plausible behaviors under varying memory capacities, and the inference method reliably recovers cognitive bounds and belief states from limited observations.

Conclusion: The proposed framework is effective in modeling user behavior under cognitive limitations, providing a foundation for AI systems to offer adaptive assistance tailored to users' capacities.

Abstract: Despite the explosive growth of AI and the technologies built upon it, predicting and inferring the sub-optimal behavior of users or human collaborators remains a critical challenge. In many cases, such behaviors are not a result of irrationality, but rather a rational decision made given inherent cognitive bounds and biased beliefs about the world. In this paper, we formally introduce a class of computational-rational (CR) user models for cognitively-bounded agents acting optimally under biased beliefs. The key novelty lies in explicitly modeling how a bounded memory process leads to a dynamically inconsistent and biased belief state and, consequently, sub-optimal sequential decision-making. We address the challenge of identifying the latent user-specific bound and inferring biased belief states from passive observations on the fly. We argue that for our formalized CR model family with an explicit and parameterized cognitive process, this challenge is tractable. To support our claim, we propose an efficient online inference method based on nested particle filtering that simultaneously tracks the user's latent belief state and estimates the unknown cognitive bound from a stream of observed actions. We validate our approach in a representative navigation task using memory decay as an example of a cognitive bound. With simulations, we show that (1) our CR model generates intuitively plausible behaviors corresponding to different levels of memory capacity, and (2) our inference method accurately and efficiently recovers the ground-truth cognitive bounds from limited observations ($\le 100$ steps). We further demonstrate how this approach provides a principled foundation for developing adaptive AI assistants, enabling adaptive assistance that accounts for the user's memory limitations.

</details>


### [38] [Learning to Trust: Bayesian Adaptation to Varying Suggester Reliability in Sequential Decision Making](https://arxiv.org/abs/2511.12378)
*Dylan M. Asmar,Mykel J. Kochenderfer*

Main category: cs.AI

TL;DR: This paper introduces a framework for agents to adaptively learn and balance external suggestions under uncertainty.


<details>
  <summary>Details</summary>
Motivation: Autonomous agents face challenges in sequential decision-making due to unreliable external action suggestions. Current methods often assume fixed quality of suggestions, which is impractical in dynamic environments.

Method: The framework incorporates suggester reliability into the agent's belief using Bayesian inference and introduces an ``ask'' action to strategically request suggestions based on critical needs and costs.

Result: The experimental results show robust agent performance across varying suggester qualities, adaptability to changing reliability, and efficient management of suggestion requests.

Conclusion: This framework enhances human-agent collaboration by allowing agents to handle uncertain suggestion reliability and make strategic decisions in uncertain environments.

Abstract: Autonomous agents operating in sequential decision-making tasks under uncertainty can benefit from external action suggestions, which provide valuable guidance but inherently vary in reliability. Existing methods for incorporating such advice typically assume static and known suggester quality parameters, limiting practical deployment. We introduce a framework that dynamically learns and adapts to varying suggester reliability in partially observable environments. First, we integrate suggester quality directly into the agent's belief representation, enabling agents to infer and adjust their reliance on suggestions through Bayesian inference over suggester types. Second, we introduce an explicit ``ask'' action allowing agents to strategically request suggestions at critical moments, balancing informational gains against acquisition costs. Experimental evaluation demonstrates robust performance across varying suggester qualities, adaptation to changing reliability, and strategic management of suggestion requests. This work provides a foundation for adaptive human-agent collaboration by addressing suggestion uncertainty in uncertain environments.

</details>


### [39] [Multi-agent Self-triage System with Medical Flowcharts](https://arxiv.org/abs/2511.12439)
*Yujia Liu,Sophia Yu,Hongyue Jin,Jessica Wen,Alexander Qian,Terrence Lee,Mattheus Ramsis,Gi Won Choi,Lianhui Qin,Xin Liu,Edward J. Wang*

Main category: cs.AI

TL;DR: The paper introduces a conversational AI self-triage system that enhances decision-making by integrating LLMs with clinically validated protocols, achieving high accuracy and reliability.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current online health tools and LLMs, such as low accuracy and reliance on unverified data, and provide a more trustworthy and auditable framework for medical self-triage.

Method: The system incorporates 100 clinically validated AMA flowcharts within a multi-agent framework, including retrieval, decision, and chat agents. It was tested using synthetic datasets to evaluate performance in flowchart selection and navigation.

Result: The system achieved 95.29% top-3 accuracy in flowchart retrieval and 99.10% accuracy in flowchart navigation across 37,200 scenarios.

Conclusion: The study demonstrates the feasibility of a structured, accurate, and transparent AI-assisted self-triage system, combining the advantages of free-text interaction and clinical protocols to aid patient decision-making.

Abstract: Online health resources and large language models (LLMs) are increasingly used as a first point of contact for medical decision-making, yet their reliability in healthcare remains limited by low accuracy, lack of transparency, and susceptibility to unverified information. We introduce a proof-of-concept conversational self-triage system that guides LLMs with 100 clinically validated flowcharts from the American Medical Association, providing a structured and auditable framework for patient decision support. The system leverages a multi-agent framework consisting of a retrieval agent, a decision agent, and a chat agent to identify the most relevant flowchart, interpret patient responses, and deliver personalized, patient-friendly recommendations, respectively. Performance was evaluated at scale using synthetic datasets of simulated conversations. The system achieved 95.29% top-3 accuracy in flowchart retrieval (N=2,000) and 99.10% accuracy in flowchart navigation across varied conversational styles and conditions (N=37,200). By combining the flexibility of free-text interaction with the rigor of standardized clinical protocols, this approach demonstrates the feasibility of transparent, accurate, and generalizable AI-assisted self-triage, with potential to support informed patient decision-making while improving healthcare resource utilization.

</details>


### [40] [ARCHE: A Novel Task to Evaluate LLMs on Latent Reasoning Chain Extraction](https://arxiv.org/abs/2511.12485)
*Pengze Li,Jiaqi Liu,Junchi Yu,Lihao Liu,Mingyu Ding,Wanli Ouyang,Shixiang Tang,Xi Chen*

Main category: cs.AI

TL;DR: The paper introduces a new task called ARCHE to evaluate if LLMs can break down complex reasoning into standard paradigms, revealing they currently fall short in achieving logical completeness and rigor for scientific inference.


<details>
  <summary>Details</summary>
Motivation: To assess whether LLMs truly understand fundamental reasoning paradigms underlying scientific inference and their ability to structure reasoning arguments accurately.

Method: The paper introduces the Latent Reasoning Chain Extraction (ARCHE) task, where reasoning is decomposed into Reasoning Logic Trees (RLT), categorized into deduction, induction, and abduction. A benchmark (ARCHE Bench) and metrics (Entity Coverage and Reasoning Edge Accuracy) are proposed.

Result: Evaluations of 10 leading LLMs showed none could fully achieve the task, revealing a trade-off between step-by-step logical validity and content completeness.

Conclusion: Current LLMs display significant limitations in logical rigor and completeness required for scientific reasoning, highlighting room for improvement in reasoning models.

Abstract: Large language models (LLMs) are increasingly used in scientific domains. While they can produce reasoning-like content via methods such as chain-of-thought prompting, these outputs are typically unstructured and informal, obscuring whether models truly understand the fundamental reasoning paradigms that underpin scientific inference. To address this, we introduce a novel task named Latent Reasoning Chain Extraction (ARCHE), in which models must decompose complex reasoning arguments into combinations of standard reasoning paradigms in the form of a Reasoning Logic Tree (RLT). In RLT, all reasoning steps are explicitly categorized as one of three variants of Peirce's fundamental inference modes: deduction, induction, or abduction. To facilitate this task, we release ARCHE Bench, a new benchmark derived from 70 Nature Communications articles, including more than 1,900 references and 38,000 viewpoints. We propose two logic-aware evaluation metrics: Entity Coverage (EC) for content completeness and Reasoning Edge Accuracy (REA) for step-by-step logical validity. Evaluations on 10 leading LLMs on ARCHE Bench reveal that models exhibit a trade-off between REA and EC, and none are yet able to extract a complete and standard reasoning chain. These findings highlight a substantial gap between the abilities of current reasoning models and the rigor required for scientific argumentation.

</details>


### [41] [LOBERT: Generative AI Foundation Model for Limit Order Book Messages](https://arxiv.org/abs/2511.12563)
*Eljas Linna,Kestutis Baltakys,Alexandros Iosifidis,Juho Kanniainen*

Main category: cs.AI

TL;DR: LOBERT is introduced as an encoder-only foundation model adapted for Limit Order Book data, offering improved performance in prediction tasks like mid-price movements and next messages, with reduced context length.


<details>
  <summary>Details</summary>
Motivation: Existing models for financial Limit Order Books struggle with irregular event timing, rapid regime shifts, and high-frequency trader reactions, necessitating a more adaptive and efficient solution.

Method: LOBERT modifies the BERT architecture using a novel tokenization scheme to treat multi-dimensional messages as single tokens, preserving continuous representations of key LOB variables like price, volume, and time.

Result: LOBERT delivers leading performance on tasks such as predicting mid-price movements and next messages, outperforming previous LOB models while requiring less contextual data.

Conclusion: LOBERT introduces a versatile and effective approach to modeling LOB dynamics, showcasing significant improvements in prediction performance and adaptability for financial data applications.

Abstract: Modeling the dynamics of financial Limit Order Books (LOB) at the message level is challenging due to irregular event timing, rapid regime shifts, and the reactions of high-frequency traders to visible order flow. Previous LOB models require cumbersome data representations and lack adaptability outside their original tasks, leading us to introduce LOBERT, a general-purpose encoder-only foundation model for LOB data suitable for downstream fine-tuning. LOBERT adapts the original BERT architecture for LOB data by using a novel tokenization scheme that treats complete multi-dimensional messages as single tokens while retaining continuous representations of price, volume, and time. With these methods, LOBERT achieves leading performance in tasks such as predicting mid-price movements and next messages, while reducing the required context length compared to previous methods.

</details>


### [42] [Enhancing Conversational Recommender Systems with Tree-Structured Knowledge and Pretrained Language Models](https://arxiv.org/abs/2511.12579)
*Yongwen Ren,Chao Wang,Peng Du,Chuan Qin,Dazhong Shen,Hui Xiong*

Main category: cs.AI

TL;DR: The paper introduces PCRS-TKA, a framework that integrates pretrained language models (PLMs) with knowledge graphs (KGs) to improve conversational recommender systems by enabling structure-aware reasoning, selective knowledge filtering, and collaborative preference modeling.


<details>
  <summary>Details</summary>
Motivation: To improve conversational recommender systems by addressing the challenges of reasoning over graph relationships, filtering context-relevant knowledge, and modeling collaborative preferences in multi-turn dialogues.

Method: The authors propose PCRS-TKA, a prompt-based framework that builds dialogue-specific knowledge trees from KGs, serializes them into texts for structured reasoning, selectively filters knowledge, models collaborative preferences, and uses a semantic alignment module to reduce noise.

Result: PCRS-TKA outperforms baseline methods in both recommendation quality and conversational fluency through extensive experiments.

Conclusion: The integration of PLMs with KGs using PCRS-TKA improves accuracy, reduces hallucination, and enhances the reasoning capabilities and overall performance of conversational recommender systems.

Abstract: Recent advances in pretrained language models (PLMs) have significantly improved conversational recommender systems (CRS), enabling more fluent and context-aware interactions. To further enhance accuracy and mitigate hallucination, many methods integrate PLMs with knowledge graphs (KGs), but face key challenges: failing to fully exploit PLM reasoning over graph relationships, indiscriminately incorporating retrieved knowledge without context filtering, and neglecting collaborative preferences in multi-turn dialogues. To this end, we propose PCRS-TKA, a prompt-based framework employing retrieval-augmented generation to integrate PLMs with KGs. PCRS-TKA constructs dialogue-specific knowledge trees from KGs and serializes them into texts, enabling structure-aware reasoning while capturing rich entity semantics. Our approach selectively filters context-relevant knowledge and explicitly models collaborative preferences using specialized supervision signals. A semantic alignment module harmonizes heterogeneous inputs, reducing noise and enhancing accuracy. Extensive experiments demonstrate that PCRS-TKA consistently outperforms all baselines in both recommendation and conversational quality.

</details>


### [43] [Dynamic Tree Databases in Automated Planning](https://arxiv.org/abs/2511.12677)
*Oliver Joergensen,Dominik Drexler,Jendrik Seipp*

Main category: cs.AI

TL;DR: The paper introduces a dynamic variant of tree databases to efficiently compress state sets in explicit state-space searches, achieving significant compression ratios with minimal runtime costs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of representing large sets of states compactly in explicit state-space searches, particularly in planning tasks, by overcoming limitations such as large memory preallocations required by static tree databases.

Method: The paper proposes a novel dynamic variant of tree databases designed for compressing state sets over propositional and numeric variables while maintaining the desirable properties of the static counterpart.

Result: Empirical evaluations demonstrate that the proposed method achieves compression ratios of several orders of magnitude with negligible runtime overhead in both classical and numeric planning tasks.

Conclusion: The dynamic tree database is a significant improvement, offering efficient state compression without major computational costs, making it suitable for large-scale explicit state-space search tasks.

Abstract: A central challenge in scaling up explicit state-space search for large tasks is compactly representing the set of generated states. Tree databases, a data structure from model checking, require constant space per generated state in the best case, but they need a large preallocation of memory. We propose a novel dynamic variant of tree databases for compressing state sets over propositional and numeric variables and prove that it maintains the desirable properties of the static counterpart. Our empirical evaluation of state compression techniques for grounded and lifted planning on classical and numeric planning tasks reveals compression ratios of several orders of magnitude, often with negligible runtime overhead.

</details>


### [44] [Adaptively Coordinating with Novel Partners via Learned Latent Strategies](https://arxiv.org/abs/2511.12754)
*Benjamin Li,Shuyang Shi,Lucia Romero,Huao Li,Yaqi Xie,Woojun Kim,Stefanos Nikolaidis,Michael Lewis,Katia Sycara,Simon Stepputtis*

Main category: cs.AI

TL;DR: This paper introduces a framework for artificial agents to adapt to human teammates' strategies in real-time, applying it in a complex collaboration task.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve human-agent collaboration by enabling artificial agents to adapt dynamically to human team members' unique and changing strategies, which is especially challenging in time-pressured and complex strategic environments.

Method: The method includes encoding strategies with a variational autoencoder, clustering distinct strategies, training cooperative agents based on these clusters, and dynamically adjusting to partners' strategies using a fixed-share regret minimization algorithm.

Result: The proposed approach outperformed existing baselines in a modified Overcooked domain with both human and artificial teammates, showcasing its effectiveness in adapting to new partners.

Conclusion: The framework demonstrates significant potential for improving real-time adaptation in human-agent collaborative tasks, particularly under complex settings with diverse strategies.

Abstract: Adaptation is the cornerstone of effective collaboration among heterogeneous team members. In human-agent teams, artificial agents need to adapt to their human partners in real time, as individuals often have unique preferences and policies that may change dynamically throughout interactions. This becomes particularly challenging in tasks with time pressure and complex strategic spaces, where identifying partner behaviors and selecting suitable responses is difficult. In this work, we introduce a strategy-conditioned cooperator framework that learns to represent, categorize, and adapt to a broad range of potential partner strategies in real-time. Our approach encodes strategies with a variational autoencoder to learn a latent strategy space from agent trajectory data, identifies distinct strategy types through clustering, and trains a cooperator agent conditioned on these clusters by generating partners of each strategy type. For online adaptation to novel partners, we leverage a fixed-share regret minimization algorithm that dynamically infers and adjusts the partner's strategy estimation during interaction. We evaluate our method in a modified version of the Overcooked domain, a complex collaborative cooking environment that requires effective coordination among two players with a diverse potential strategy space. Through these experiments and an online user study, we demonstrate that our proposed agent achieves state of the art performance compared to existing baselines when paired with novel human, and agent teammates.

</details>


### [45] [Optimal Foraging in Memory Retrieval: Evaluating Random Walks and Metropolis-Hastings Sampling in Modern Semantic Spaces](https://arxiv.org/abs/2511.12759)
*James Moore*

Main category: cs.AI

TL;DR: The study investigates whether embedding spaces and random walks can mimic human memory foraging behaviors during semantic fluency tasks, finding that simpler sampling methods align well with observed human behavior.


<details>
  <summary>Details</summary>
Motivation: To explore if modern high-dimensional embedding spaces and algorithmic sampling can replicate patterns of human memory retrieval during semantic fluency tasks, as posited by ecological foraging frameworks.

Method: Random walks were conducted on embedding spaces using state-of-the-art embeddings and semantic fluency data, comparing results with human data patterns. Additionally, Metropolis-Hastings adaptive sampling was tested.

Result: Random walks are consistent with optimal foraging aligned with human behavior patterns. However, more complex sampling methods like Metropolis-Hastings did not match human memory retrieval dynamics.

Conclusion: Structured embeddings paired with simple sampling can approximate human memory behavior, challenging the assumption that complexity in sampling mechanisms improves cognitive modeling.

Abstract: Human memory retrieval often resembles ecological foraging where animals search for food in a patchy environment. Optimal foraging means following the Marginal Value Theorem (MVT), in which individuals exploit a patch of semantically related concepts until it becomes less rewarding and then switch to a new cluster. While human behavioral data suggests foraging-like patterns in semantic fluency tasks, it remains unclear whether modern high-dimensional embedding spaces provide representations that allow algorithms to match observed human behavior. Using state-of-the-art embeddings and prior semantic fluency data, I find that random walks on these embedding spaces produce results consistent with optimal foraging and the MVT. Surprisingly, introducing Metropolis-Hastings sampling, an adaptive algorithm expected to model strategic acceptance and rejection of new clusters, does not produce results consistent with human behavior. These findings challenge the assumption that more complex sampling mechanisms inherently lead to better cognitive models of memory retrieval. Instead, they show that appropriately structured embeddings, even with simple sampling, can produce near-optimal foraging dynamics. This supports the perspective of Hills (2012) rather than Abbott (2015), demonstrating that modern embeddings can approximate human memory foraging without relying on complex acceptance criteria.

</details>


### [46] [Event-CausNet: Unlocking Causal Knowledge from Text with Large Language Models for Reliable Spatio-Temporal Forecasting](https://arxiv.org/abs/2511.12769)
*Luyao Niu,Zepu Wang,Shuyi Guan,Yang Liu,Peng Sun*

Main category: cs.AI

TL;DR: The paper introduces Event-CausNet, enhancing traffic forecasting during disruptions by incorporating causal reasoning into spatio-temporal GNNs.


<details>
  <summary>Details</summary>
Motivation: Spatio-temporal GNNs struggle during non-recurring traffic events like accidents due to reliance on historical correlation rather than causal inference.

Method: Event-CausNet integrates a Large Language Model for quantifying event reports, builds a causal knowledge base to estimate average treatment effects, and employs a dual-stream GNN-LSTM network with a novel causal attention mechanism.

Result: Event-CausNet reduces prediction error by up to 35.87% on real-world datasets, outperforming state-of-the-art methods.

Conclusion: Event-CausNet bridges the gap between correlational models and causal inference, offering improved accuracy, transferability, and interpretability for traffic management during critical disruptions.

Abstract: While spatio-temporal Graph Neural Networks (GNNs) excel at modeling recurring traffic patterns, their reliability plummets during non-recurring events like accidents. This failure occurs because GNNs are fundamentally correlational models, learning historical patterns that are invalidated by the new causal factors introduced during disruptions. To address this, we propose Event-CausNet, a framework that uses a Large Language Model to quantify unstructured event reports, builds a causal knowledge base by estimating average treatment effects, and injects this knowledge into a dual-stream GNN-LSTM network using a novel causal attention mechanism to adjust and enhance the forecast. Experiments on a real-world dataset demonstrate that Event-CausNet achieves robust performance, reducing prediction error (MAE) by up to 35.87%, significantly outperforming state-of-the-art baselines. Our framework bridges the gap between correlational models and causal reasoning, providing a solution that is more accurate and transferable, while also offering crucial interpretability, providing a more reliable foundation for real-world traffic management during critical disruptions.

</details>


### [47] [Multi-Agent Reinforcement Learning for Heterogeneous Satellite Cluster Resources Optimization](https://arxiv.org/abs/2511.12792)
*Mohamad A. Hady,Siyi Hu,Mahardhika Pratama,Zehong Cao,Ryszard Kowalczyk*

Main category: cs.AI

TL;DR: The paper explores Reinforcement Learning (RL) for optimizing resource use in heterogeneous satellite clusters performing autonomous Earth Observation missions, demonstrating success with Multi-Agent RL methods.


<details>
  <summary>Details</summary>
Motivation: Traditional optimization methods fail to handle the real-time, decentralized, and uncertain nature of Earth Observation operations. This paper aims to utilize RL techniques for adaptive and efficient decision-making in autonomous satellite resource management.

Method: The study formulates an optimization problem for a multi-satellite setup, addressing challenges like energy and memory limits, partial observability, and heterogeneous agent capabilities. Leveraging a simulation platform, it evaluates MARL algorithms such as MAPPO, HAPPO, and HATRPO.

Result: MARL methods proved effective in coordinating heterogeneous satellites, achieving a balance between imaging performance and resource utilization while tackling non-stationarity and inter-agent challenges.

Conclusion: MARL offers scalable, autonomous decision-making solutions for heterogeneous satellite clusters, paving the way for efficient Earth Observation mission planning in dynamic conditions.

Abstract: This work investigates resource optimization in heterogeneous satellite clusters performing autonomous Earth Observation (EO) missions using Reinforcement Learning (RL). In the proposed setting, two optical satellites and one Synthetic Aperture Radar (SAR) satellite operate cooperatively in low Earth orbit to capture ground targets and manage their limited onboard resources efficiently. Traditional optimization methods struggle to handle the real-time, uncertain, and decentralized nature of EO operations, motivating the use of RL and Multi-Agent Reinforcement Learning (MARL) for adaptive decision-making. This study systematically formulates the optimization problem from single-satellite to multi-satellite scenarios, addressing key challenges including energy and memory constraints, partial observability, and agent heterogeneity arising from diverse payload capabilities. Using a near-realistic simulation environment built on the Basilisk and BSK-RL frameworks, we evaluate the performance and stability of state-of-the-art MARL algorithms such as MAPPO, HAPPO, and HATRPO. Results show that MARL enables effective coordination across heterogeneous satellites, balancing imaging performance and resource utilization while mitigating non-stationarity and inter-agent reward coupling. The findings provide practical insights into scalable, autonomous satellite operations and contribute a foundation for future research on intelligent EO mission planning under heterogeneous and dynamic conditions.

</details>


### [48] [Neuro-Logic Lifelong Learning](https://arxiv.org/abs/2511.12793)
*Bowen He,Xiaoan Xu,Alper Kamil Bozkurt,Vahid Tarokh,Juncheng Dong*

Main category: cs.AI

TL;DR: The paper focuses on lifelong learning in Inductive Logic Programming (ILP) using neural networks by reusing acquired logic rules to improve learning efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the gap in Neural-Symbolic AI by moving beyond designing novel architectures for isolated problems to exploring continual learning paradigms for sequences of related ILP problems.

Method: The paper introduces a compositional framework for lifelong learning in ILP, enabling reuse of previously learned logic rules to tackle new tasks effectively.

Result: Experimental evaluation on task sequences demonstrates the feasibility and advantages of the proposed approach, highlighting improved scalability and performance.

Conclusion: The study offers a foundational step in advancing continual learning within Neural-Symbolic AI, showcasing the potential of lifelong learning for solving ILP problems more efficiently.

Abstract: Solving Inductive Logic Programming (ILP) problems with neural networks is a key challenge in Neural-Symbolic Ar- tificial Intelligence (AI). While most research has focused on designing novel network architectures for individual prob- lems, less effort has been devoted to exploring new learning paradigms involving a sequence of problems. In this work, we investigate lifelong learning ILP, which leverages the com- positional and transferable nature of logic rules for efficient learning of new problems. We introduce a compositional framework, demonstrating how logic rules acquired from ear- lier tasks can be efficiently reused in subsequent ones, leading to improved scalability and performance. We formalize our approach and empirically evaluate it on sequences of tasks. Experimental results validate the feasibility and advantages of this paradigm, opening new directions for continual learn- ing in Neural-Symbolic AI.

</details>


### [49] [Mapping fNIRS Signals to Agent Performance: Toward Reinforcement Learning from Neural Feedback](https://arxiv.org/abs/2511.12844)
*Julia Santaniello,Matthew Russell,Benson Jiang,Donatello Sassaroli,Robert Jacob,Jivko SInapov*

Main category: cs.AI

TL;DR: This paper explores brain-driven reinforcement learning through fNIRS signals and introduces new methods and datasets for agent performance alignment.


<details>
  <summary>Details</summary>
Motivation: The study aims to align AI agent behaviors with human preferences using implicit brain signals, leveraging fNIRS-based feedback to improve reinforcement learning methodologies.

Method: The authors collected fNIRS recordings from 25 participants in three domains, trained classifiers/regressors to map brain signals to agent performance, and utilized pre-trained models for subject-specific fine-tuning.

Result: Achieved average F1 scores of 67% for binary and 46% for multi-class classification, and improved scores significantly by fine-tuning with subject-specific data.

Conclusion: Mapping implicit brain signals to agent performance is feasible and can be refined, paving the way for future brain-driven RLHF systems.

Abstract: Reinforcement Learning from Human Feedback (RLHF) is a methodology that aligns agent behavior with human preferences by integrating human feedback into the agent's training process. We introduce a possible framework that employs passive Brain-Computer Interfaces (BCI) to guide agent training from implicit neural signals. We present and release a novel dataset of functional near-infrared spectroscopy (fNIRS) recordings collected from 25 human participants across three domains: a Pick-and-Place Robot, Lunar Lander, and Flappy Bird. We train classifiers to predict levels of agent performance (optimal, sub-optimal, or worst-case) from windows of preprocessed fNIRS feature vectors, achieving an average F1 score of 67% for binary classification and 46% for multi-class models averaged across conditions and domains. We also train regressors to predict the degree of deviation between an agent's chosen action and a set of near-optimal policies, providing a continuous measure of performance. We evaluate cross-subject generalization and demonstrate that fine-tuning pre-trained models with a small sample of subject-specific data increases average F1 scores by 17% and 41% for binary and multi-class models, respectively. Our work demonstrates that mapping implicit fNIRS signals to agent performance is feasible and can be improved, laying the foundation for future brain-driven RLHF systems.

</details>


### [50] [Bootstrapping LLMs via Preference-Based Policy Optimization](https://arxiv.org/abs/2511.12867)
*Chen Jia*

Main category: cs.AI

TL;DR: The paper proposes a novel preference-based policy optimization (PbPO) framework for aligning large language models (LLMs) with human preferences through a min-max game approach without requiring extensive manual annotations.


<details>
  <summary>Details</summary>
Motivation: The need to align large language models with human preferences effectively and efficiently, especially without heavy reliance on manual annotations.

Method: Developed a PbPO framework that involves a min-max game between the main policy and a reward model constrained by preference data-derived confidence sets. An iterative online algorithm collects and utilizes this data for self-improvement.

Result: The proposed PbPO framework achieved theoretical guarantees and demonstrated superior performance against state-of-the-art preference optimization methods in experiments across five benchmarks.

Conclusion: The method provides an effective avenue for bootstrapping LLMs by ensuring continual improvement through preference-based optimization, setting significant advancements in LLM alignment strategies.

Abstract: Bootstrapping large language models (LLMs) through preference-based policy optimization offers a promising direction for aligning model behavior with human preferences without relying on extensive manual annotations. In this work, we propose a novel preference-based policy optimization (PbPO) framework that formulates the learning process as a min-max game between the main policy and a reward model (RM). The RM is constrained within a confidence set derived from preference data to ensure reliable exploitation. Our iterative online algorithm actively collects preference data through guided exploration of the evolving policy, enabling continual self-improvement of both the policy and the RM. We provide theoretical guarantees for our method, establishing high-probability regret bounds for both settings with sequence-level RM and token-level RM, demonstrating its effectiveness in bootstrapping LLMs. Extensive experiments on five benchmarks show that our approach consistently outperforms existing state-of-the-art preference optimization techniques.

</details>


### [51] [Think, Speak, Decide: Language-Augmented Multi-Agent Reinforcement Learning for Economic Decision-Making](https://arxiv.org/abs/2511.12876)
*Heyang Ma,Qirui Mi,Qipeng Yang,Zijun Fan,Bo Li,Haifeng Zhang*

Main category: cs.AI

TL;DR: The paper introduces LAMP, a framework integrating language with economic decision-making to enhance outcomes in multi-agent reinforcement learning, significantly improving returns, robustness, and interpretability.


<details>
  <summary>Details</summary>
Motivation: Economic decision-making is influenced by unstructured language, but current MARL approaches struggle with its ambiguity and contextual complexity. A solution is needed to incorporate language effectively into economic models.

Method: The proposed LAMP framework operates via a Think-Speak-Decide pipeline. It interprets numerical data, crafts strategic messages for peer communication, and integrates this into MARL policies to enhance decision-making.

Result: LAMP demonstrated superior performance over MARL and language models alone, achieving higher cumulative return (+63.5% and +34.0%), robustness (+18.8% and +59.4%), and better interpretability in economic simulations.

Conclusion: Language integration into multi-agent reinforcement learning can lead to significantly better economic decision-making, offering promising avenues for robust and interpretable strategies in real-world applications.

Abstract: Economic decision-making depends not only on structured signals such as prices and taxes, but also on unstructured language, including peer dialogue and media narratives. While multi-agent reinforcement learning (MARL) has shown promise in optimizing economic decisions, it struggles with the semantic ambiguity and contextual richness of language. We propose LAMP (Language-Augmented Multi-Agent Policy), a framework that integrates language into economic decision-making and narrows the gap to real-world settings. LAMP follows a Think-Speak-Decide pipeline: (1) Think interprets numerical observations to extract short-term shocks and long-term trends, caching high-value reasoning trajectories; (2) Speak crafts and exchanges strategic messages based on reasoning, updating beliefs by parsing peer communications; and (3) Decide fuses numerical data, reasoning, and reflections into a MARL policy to optimize language-augmented decision-making. Experiments in economic simulation show that LAMP outperforms both MARL and LLM-only baselines in cumulative return (+63.5%, +34.0%), robustness (+18.8%, +59.4%), and interpretability. These results demonstrate the potential of language-augmented policies to deliver more effective and robust economic strategies.

</details>


### [52] [Online Learning of HTN Methods for integrated LLM-HTN Planning](https://arxiv.org/abs/2511.12901)
*Yuesheng Xu,Hector Munoz-Avila*

Main category: cs.AI

TL;DR: This paper introduces a method for online learning of Hierarchical Task Network (HTN) methods in ChatHTN, integrating HTN planning with LLM-based chatbots.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of excessive dependency on ChatGPT for generating task decompositions in HTN planning.

Method: Extension of ChatHTN by incorporating an online learning mechanism that generalizes learned methods from cases where ChatGPT provides decomposition.

Result: Experiments show reduced ChatGPT queries while effectively solving as many or more tasks across two domains.

Conclusion: The enhanced ChatHTN system improves efficiency by generalizing learned decomposition methods, reducing reliance on ChatGPT while maintaining or increasing problem-solving capabilities.

Abstract: We present online learning of Hierarchical Task Network (HTN) methods in the context of integrated HTN planning and LLM-based chatbots. Methods indicate when and how to decompose tasks into subtasks. Our method learner is built on top of the ChatHTN planner. ChatHTN queries ChatGPT to generate a decomposition of a task into primitive tasks when no applicable method for the task is available. In this work, we extend ChatHTN. Namely, when ChatGPT generates a task decomposition, ChatHTN learns from it, akin to memoization. However, unlike memoization, it learns a generalized method that applies not only to the specific instance encountered, but to other instances of the same task. We conduct experiments on two domains and demonstrate that our online learning procedure reduces the number of calls to ChatGPT while solving at least as many problems, and in some cases, even more.

</details>


### [53] [CoS: Towards Optimal Event Scheduling via Chain-of-Scheduling](https://arxiv.org/abs/2511.12913)
*Yiming Zhao,Jiwei Tang,Shimin Di,Libin Zheng,Jianxing Yu,Jian Yin*

Main category: cs.AI

TL;DR: The paper proposes the Chain-of-Scheduling (CoS) framework to optimize event scheduling using Large Language Models (LLMs), achieving high performance and generalization.


<details>
  <summary>Details</summary>
Motivation: Optimizing event recommendations in Event-based Social Networks (EBSNs) to maximize user preference within time and geographical constraints.

Method: Introduced the CoS framework that breaks the scheduling task into three stages: exploration, verification, and integration. LLMs autonomously execute this via Knowledge Distillation.

Result: CoS achieves nearly optimal effectiveness and high efficiency on real-world datasets, with strong zero-shot learning capability on out-of-domain data.

Conclusion: The CoS framework enables efficient, interpretable, and highly generalizable event scheduling using LLMs to improve user experience in EBSNs.

Abstract: Recommending event schedules is a key issue in Event-based Social Networks (EBSNs) in order to maintain user activity. An effective recommendation is required to maximize the user's preference, subjecting to both time and geographical constraints. Existing methods face an inherent trade-off among efficiency, effectiveness, and generalization, due to the NP-hard nature of the problem. This paper proposes the Chain-of-Scheduling (CoS) framework, which activates the event scheduling capability of Large Language Models (LLMs) through a guided, efficient scheduling process. CoS enhances LLM by formulating the schedule task into three atomic stages, i.e., exploration, verification and integration. Then we enable the LLMs to generate CoS autonomously via Knowledge Distillation (KD). Experimental results show that CoS achieves near-theoretical optimal effectiveness with high efficiency on three real-world datasets in a interpretable manner. Moreover, it demonstrates strong zero-shot learning ability on out-of-domain data.

</details>


### [54] [Fault2Flow: An AlphaEvolve-Optimized Human-in-the-Loop Multi-Agent System for Fault-to-Workflow Automation](https://arxiv.org/abs/2511.12916)
*Yafang Wang,Yangjie Tian,Xiaoyu Shen,Gaoyang Zhang,Jiaze Sun,He Zhang,Ruohua Xu,Feng Zhao*

Main category: cs.AI

TL;DR: Fault2Flow, a novel LLM-based system, automates power grid fault diagnosis by creating verified workflows, reducing reliance on inefficient manual processes.


<details>
  <summary>Details</summary>
Motivation: Existing fault diagnosis relies on manual interpretation of dense regulations and expert knowledge, making it error-prone, inefficient, and difficult to maintain as regulations change.

Method: Fault2Flow extracts regulatory logic into fault trees using LLMs, integrates verified expert knowledge, optimizes logic with AlphaEvolve, and generates an executable workflow.

Result: Experimental validation demonstrated 100% topological consistency and high semantic fidelity, confirming the system's effectiveness.

Conclusion: Fault2Flow streamlines fault diagnosis by automating reasoning logic, improving efficiency, and reducing the workload for human experts.

Abstract: Power grid fault diagnosis is a critical process hindered by its reliance on manual, error-prone methods. Technicians must manually extract reasoning logic from dense regulations and attempt to combine it with tacit expert knowledge, which is inefficient, error-prone, and lacks maintainability as ragulations are updated and experience evolves. While Large Language Models (LLMs) have shown promise in parsing unstructured text, no existing framework integrates these two disparate knowledge sources into a single, verified, and executable workflow. To bridge this gap, we propose Fault2Flow, an LLM-based multi-agent system. Fault2Flow systematically: (1) extracts and structures regulatory logic into PASTA-formatted fault trees; (2) integrates expert knowledge via a human-in-the-loop interface for verification; (3) optimizes the reasoning logic using a novel AlphaEvolve module; and (4) synthesizes the final, verified logic into an n8n-executable workflow. Experimental validation on transformer fault diagnosis datasets confirms 100\% topological consistency and high semantic fidelity. Fault2Flow establishes a reproducible path from fault analysis to operational automation, substantially reducing expert workload.

</details>


### [55] [Yanyun-3: Enabling Cross-Platform Strategy Game Operation with Vision-Language Models](https://arxiv.org/abs/2511.12937)
*Guoyan Wang,Yanyan Huang,Chunlin Chen,Lifeng Wang,Yuxiang Sun*

Main category: cs.AI

TL;DR: This paper introduces Yanyun-3, a framework enabling autonomous operation across multiple strategy game environments using vision-language reasoning and precise execution capabilities.


<details>
  <summary>Details</summary>
Motivation: Automating operation in diverse strategy games is challenging due to varying user interfaces and dynamic conditions. Vision-language models have potential but are underutilized in this domain.

Method: The paper combines Qwen2.5-VL's vision-language reasoning with UI-TARS for accurate execution, evaluating multimodal data strategies and proposing a hybrid fusion method for cross-platform automation.

Result: The hybrid strategy improves performance significantly, reducing inference time by 63% and increasing BLEU-4 score by almost 13 times, demonstrating effective cross-platform generalization.

Conclusion: Yanyun-3 not only enhances strategy game automation but also introduces a paradigm for optimizing VLMs with structured multimodal data, advancing embodied intelligence research.

Abstract: Automated operation in cross-platform strategy games demands agents with robust generalization across diverse user interfaces and dynamic battlefield conditions. While vision-language models (VLMs) have shown considerable promise in multimodal reasoning, their application to complex human-computer interaction scenarios--such as strategy gaming--remains largely unexplored. Here, we introduce Yanyun-3, a general-purpose agent framework that, for the first time, enables autonomous cross-platform operation across three heterogeneous strategy game environments. By integrating the vision-language reasoning of Qwen2.5-VL with the precise execution capabilities of UI-TARS, Yanyun-3 successfully performs core tasks including target localization, combat resource allocation, and area control. Through systematic ablation studies, we evaluate the effects of various multimodal data combinations--static images, multi-image sequences, and videos--and propose the concept of combination granularity to differentiate between intra-sample fusion and inter-sample mixing strategies. We find that a hybrid strategy, which fuses multi-image and video data while mixing in static images (MV+S), substantially outperforms full fusion: it reduces inference time by 63% and boosts the BLEU-4 score by a factor of 12 (from 4.81% to 62.41%, approximately 12.98x). Operating via a closed-loop pipeline of screen capture, model inference, and action execution, the agent demonstrates strong real-time performance and cross-platform generalization. Beyond providing an efficient solution for strategy game automation, our work establishes a general paradigm for enhancing VLM performance through structured multimodal data organization, offering new insights into the interplay between static perception and dynamic reasoning in embodied intelligence.

</details>


### [56] [MedRule-KG: A Knowledge-Graph--Steered Scaffold for Reliable Mathematical and Biomedical Reasoning](https://arxiv.org/abs/2511.12963)
*Crystal Su*

Main category: cs.AI

TL;DR: This paper introduces MedRule-KG, a system for improving the domain-specific reasoning and output validity of large language models in scientific tasks, achieving significant reductions in rule violations.


<details>
  <summary>Details</summary>
Motivation: Enhancing domain-consistent reasoning and generation in LLMs for scientific reasoning and drug discovery, where traditional models often fail to produce mathematically and biomedically valid outputs.

Method: The MedRule-KG framework integrates curated symbolic facts into prompts and enforces rule compliance using a deterministic checker. It uses a compact knowledge graph scaffold, a lightweight verifier, and formalizes the generation process as constrained inference, with statistical analysis for validation.

Result: Across 90 tasks related to chemical feasibility, metabolic compatibility, and toxicity, MedRule-KG reduced rule violations by 83.2% compared to a chain-of-thought baseline and achieved improved exact matches while remaining efficient.

Conclusion: MedRule-KG significantly enhances the reliability and accuracy of LLM outputs for scientific applications with minimal latency impact, making it suitable for real-world, interactive use cases.

Abstract: We study how to impose domain-consistent structure on large language models (LLMs) used for scientific reasoning and early-stage drug discovery. We present MedRule-KG, a compact knowledge-graph scaffold paired with a lightweight verifier that steers generation toward mathematically and biomedically valid outputs. The system injects curated symbolic facts into prompts and then enforces rule satisfaction with a deterministic checker. We formalize generation as constrained inference, introduce a soft guidance surrogate suitable for decoding, and perform a thorough statistical analysis with uncertainty quantification. Across 90 tasks spanning reaction feasibility, metabolic compatibility, and toxicity screening, MedRule-KG reduces violation counts by 83.2\% relative to a strong chain-of-thought baseline while improving exact match. Results remain stable under stratification and scale with dataset size, and the verifier adds negligible latency, making the approach practical for interactive design.

</details>


### [57] [WebCoach: Self-Evolving Web Agents with Cross-Session Memory Guidance](https://arxiv.org/abs/2511.12997)
*Genglin Liu,Shijie Geng,Sha Li,Hejie Cui,Sarah Zhang,Xin Liu,Tianyi Liu*

Main category: cs.AI

TL;DR: WebCoach is a self-evolving framework designed to improve LLM-powered web browsing agents by introducing persistent cross-session memory.


<details>
  <summary>Details</summary>
Motivation: Current web agents face challenges with repetitive errors and lack long-term robustness and learning across sessions.

Method: WebCoach includes three components: WebCondenser (summarizes navigation logs), External Memory Store (organizes experiences), and Coach (retrieves relevant advice for agents).

Result: WebCoach enhances task success rates and efficiency, showing significant improvements across LLM backbones, including smaller models achieving comparable performance to GPT-4o.

Conclusion: WebCoach enables agents to learn and evolve efficiently over time without retraining, boosting robustness and task completion capabilities.

Abstract: Multimodal LLM-powered agents have recently demonstrated impressive capabilities in web navigation, enabling agents to complete complex browsing tasks across diverse domains. However, current agents struggle with repetitive errors and lack the ability to learn from past experiences across sessions, limiting their long-term robustness and sample efficiency. We introduce WebCoach, a model-agnostic self-evolving framework that equips web browsing agents with persistent cross-session memory, enabling improved long-term planning, reflection, and continual learning without retraining. WebCoach consists of three key components: (1) a WebCondenser, which standardizes raw navigation logs into concise summaries; (2) an External Memory Store, which organizes complete trajectories as episodic experiences; and (3) a Coach, which retrieves relevant experiences based on similarity and recency, and decides whether to inject task-specific advice into the agent via runtime hooks. This design empowers web agents to access long-term memory beyond their native context window, improving robustness in complex browsing tasks. Moreover, WebCoach achieves self-evolution by continuously curating episodic memory from new navigation trajectories, enabling agents to improve over time without retraining. Evaluations on the WebVoyager benchmark demonstrate that WebCoach consistently improves the performance of browser-use agents across three different LLM backbones. With a 38B model, it increases task success rates from 47% to 61% while reducing or maintaining the average number of steps. Notably, smaller base models with WebCoach achieve performance comparable to the same web agent using GPT-4o.

</details>


### [58] [GEM: Generative Entropy-Guided Preference Modeling for Few-shot Alignment of LLMs](https://arxiv.org/abs/2511.13007)
*Yiyang Zhao,Huiyu Bai,Xuejiao Zhao*

Main category: cs.AI

TL;DR: The paper introduces GEM, a method for aligning large language models (LLMs) with human preferences in low-resource, domain-specific scenarios, using entropy-guided cognitive optimization instead of traditional reward models.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of aligning LLMs to human preferences in fields like medicine and law where large-scale preference labels are hard to obtain.

Method: Proposed GEM involves cognitive filtering using entropy-guided chain-of-thought (CoT) reasoning and a novel algorithm (SEGA) for optimizing preference-based signals.

Result: GEM achieves efficient few-shot alignment and demonstrates improvements on general and domain-specific benchmarks like mathematical reasoning and medical dialogues.

Conclusion: GEM provides a resource-efficient and effective approach to align LLMs with human preferences, internalizing cognitive signals through entropy-guided optimization.

Abstract: Alignment of large language models (LLMs) with human preferences typically relies on supervised reward models or external judges that demand abundant annotations. However, in fields that rely on professional knowledge, such as medicine and law, such large-scale preference labels are often unachievable. In this paper, we propose a generative entropy-guided preference modeling approach named GEM for LLMs aligment at low-resource and domain-specific scenarios. Instead of training a discriminative reward model on preference data, we directly train the LLM to internalize a closed-loop optimization architecture that can extract and exploit the multi-dimensional, fine-grained cognitive signals implicit in human preferences. Specifically, our Cognitive Filtering module, based on entropy theory in decision making, first leverages Chain-of-Thought (CoT) prompting to generate diverse candidate reasoning chains (CoTs) from preference data. Subsequently, it introduces a token scoring mechanism to rank and weight the sampled CoTs, boosting the importance of high-confidence answers and strategically high-entropy tokens. Building on these filtered preferences, we fine-tune the LLM using a novel self-evaluated group advantage algorithm, SEGA, which effectively aggregates group-level cognitive signals and transforms the entropy-based scores into implicit rewards for policy optimization. In these ways, GEM empowers the LLM to rely on its own judgments and establishes an entropy-guided closed-loop cognitive optimization framework, enabling highly efficient few-shot alignment of LLMs. Experiments on general benchmarks and domain-specific tasks (such as mathematical reasoning and medical dialogues) demonstrate that our GEM achieves significant improvements with few-shot preference data.

</details>


### [59] [PragWorld: A Benchmark Evaluating LLMs' Local World Model under Minimal Linguistic Alterations and Conversational Dynamics](https://arxiv.org/abs/2511.13021)
*Sachin Vashistha,Aryan Bibhuti,Atharva Naik,Martin Tutek,Somak Aditya*

Main category: cs.AI

TL;DR: Language models (LMs) struggle with encoding and updating their internal world model in conversations, particularly under linguistic alterations. Fine-tuning strategies are proposed for improvement.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of evaluating whether LMs can construct and maintain robust implicit representations of conversations, focusing on pragmatic elements like entity tracking and conversational nuances.

Method: The authors applied seven linguistic alterations to conversational datasets, constructed benchmarks with yes-no questions, evaluated open/closed source LMs, and proposed a dual-perspective interpretability framework to identify harmful transformer layers.

Result: Findings show that LMs fail to reliably track entities and adapt under linguistic alterations. Harmful layers were identified, leading to the development of fine-tuning methods to mitigate their effects.

Conclusion: Current LMs have limitations in maintaining accuracy in conversational settings. Layer-regularization techniques enhance their robustness against linguistic challenges.

Abstract: Real-world conversations are rich with pragmatic elements, such as entity mentions, references, and implicatures. Understanding such nuances is a requirement for successful natural communication, and often requires building a local world model which encodes such elements and captures the dynamics of their evolving states. However, it is not well-understood whether language models (LMs) construct or maintain a robust implicit representation of conversations. In this work, we evaluate the ability of LMs to encode and update their internal world model in dyadic conversations and test their malleability under linguistic alterations. To facilitate this, we apply seven minimal linguistic alterations to conversations sourced from popular datasets and construct two benchmarks comprising yes-no questions. We evaluate a wide range of open and closed source LMs and observe that they struggle to maintain robust accuracy. Our analysis unveils that LMs struggle to memorize crucial details, such as tracking entities under linguistic alterations to conversations. We then propose a dual-perspective interpretability framework which identifies transformer layers that are useful or harmful and highlights linguistic alterations most influenced by harmful layers, typically due to encoding spurious signals or relying on shortcuts. Inspired by these insights, we propose two layer-regularization based fine-tuning strategies that suppress the effect of the harmful layers.

</details>


### [60] [Scaling Generative Verifiers For Natural Language Mathematical Proof Verification And Selection](https://arxiv.org/abs/2511.13027)
*Sadegh Mahdavi,Branislav Kisacanin,Shubham Toshniwal,Wei Du,Ivan Moshkov,George Armstrong,Renjie Liao,Christos Thrampoulidis,Igor Gitman*

Main category: cs.AI

TL;DR: This paper focuses on improving mathematical reasoning in large language models through better proof verification and final-answer reasoning, proposing scalable methods and guidelines for solution verification and selection.


<details>
  <summary>Details</summary>
Motivation: To address the need for reliable proof verification capabilities in large language models, which often exhibit flawed reasoning despite success in solving final-answer math problems.

Method: The study evaluates proof-based and final-answer reasoning setups, scales major generative verification methods (GenSelect and LLM-as-a-Judge), and explores reinforcement learning's impacts on model performance.

Result: The researchers identify the combination of GenSelect and LLM-as-a-Judge as the most effective framework for verification while showing reinforcement learning reduces prompt sensitivity but does not improve final-answer precision.

Conclusion: The research provides practical guidelines for designing scalable proof-verification systems, noting that current language models emphasize stylistic correctness over mathematical validity.

Abstract: Large language models have achieved remarkable success on final-answer mathematical problems, largely due to the ease of applying reinforcement learning with verifiable rewards. However, the reasoning underlying these solutions is often flawed. Advancing to rigorous proof-based mathematics requires reliable proof verification capabilities. We begin by analyzing multiple evaluation setups and show that focusing on a single benchmark can lead to brittle or misleading conclusions. To address this, we evaluate both proof-based and final-answer reasoning to obtain a more reliable measure of model performance. We then scale two major generative verification methods (GenSelect and LLM-as-a-Judge) to millions of tokens and identify their combination as the most effective framework for solution verification and selection. We further show that the choice of prompt for LLM-as-a-Judge significantly affects the model's performance, but reinforcement learning can reduce this sensitivity. However, despite improving proof-level metrics, reinforcement learning does not enhance final-answer precision, indicating that current models often reward stylistic or procedural correctness rather than mathematical validity. Our results establish practical guidelines for designing and evaluating scalable proof-verification and selection systems.

</details>


### [61] [MEGA-GUI: Multi-stage Enhanced Grounding Agents for GUI Elements](https://arxiv.org/abs/2511.13087)
*SeokJoo Kwak,Jihoon Kim,Boyoun Kim,Jung Jae Yoon,Wooseok Jang,Jeonghoon Hong,Jaeho Yang,Yeong-Dae Kwon*

Main category: cs.AI

TL;DR: The paper introduces MEGA-GUI, a modular framework for GUI grounding, achieving higher accuracy over existing methods through multi-stage processing and specialized models.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing GUI grounding systems, which struggle with modularity, visual clutter, and ambiguous instructions.

Method: The proposed method involves a multi-stage framework combining coarse ROI selection and fine-grained element grounding, aided by specialized vision-language agents, a bidirectional ROI zoom, and a context-aware rewriting agent.

Result: MEGA-GUI achieves 73.18% accuracy on the visually dense ScreenSpot-Pro benchmark and 68.63% accuracy on the semantically complex OSWorld-G benchmark, outperforming previous methods.

Conclusion: MEGA-GUI's modular approach improves accuracy and shows promise for solving GUI grounding challenges in dense, complex environments while providing new benchmarks and tools for further research.

Abstract: Graphical User Interface (GUI) grounding - the task of mapping natural language instructions to screen coordinates - is essential for autonomous agents and accessibility technologies. Existing systems rely on monolithic models or one-shot pipelines that lack modularity and fail under visual clutter and ambiguous instructions. We introduce MEGA-GUI, a multi-stage framework that separates grounding into coarse Region-of-Interest (ROI) selection and fine-grained element grounding, orchestrated by specialized vision-language agents. MEGA-GUI features a bidirectional ROI zoom algorithm that mitigates spatial dilution and a context-aware rewriting agent that reduces semantic ambiguity. Our analysis reveals complementary strengths and weaknesses across vision-language models at different visual scales, and we show that leveraging this modular structure achieves consistently higher accuracy than monolithic approaches. On the visually dense ScreenSpot-Pro benchmark, MEGA-GUI attains 73.18% accuracy, and on the semantically complex OSWorld-G benchmark it reaches 68.63%, surpassing previously reported results. Code and the Grounding Benchmark Toolkit (GBT) are available at https://github.com/samsungsds-research-papers/mega-gui.

</details>


### [62] [STEP: Success-Rate-Aware Trajectory-Efficient Policy Optimization](https://arxiv.org/abs/2511.13091)
*Yuhan Chen,Yuxuan Liu,Long Zhang,Pengzhi Gao,Jian Luan,Wei Liu*

Main category: cs.AI

TL;DR: STEP improves multi-turn interaction in online reinforcement learning by using success-rate-aware trajectory resampling and step-level optimization.


<details>
  <summary>Details</summary>
Motivation: Improving efficiency and accuracy in multi-turn interaction for online reinforcement learning, overcoming flaws in trajectory-level optimization.

Method: STEP dynamically adjusts sampling via per-task success rates, performs step-level decomposition, and refines updates using GRPO augmentation based on task difficulty.

Result: STEP improves sample efficiency, training stability, faster convergence, and better generalization compared to trajectory-level GRPO.

Conclusion: The STEP framework demonstrates superior performance in sample-efficient and stable optimization for multi-turn interaction in reinforcement learning environments.

Abstract: Multi-turn interaction remains challenging for online reinforcement learning. A common solution is trajectory-level optimization, which treats each trajectory as a single training sample. However, this approach can be inefficient and yield misleading learning signals: it applies uniform sampling across tasks regardless of difficulty, penalizes correct intermediate actions in failed trajectories, and incurs high sample-collection costs. To address these issues, we propose STEP (Success-rate-aware Trajectory-Efficient Policy optimization), a framework that dynamically allocates sampling based on per-task success rates and performs step-level optimization. STEP maintains a smoothed success-rate record to guide adaptive trajectory resampling, allocating more effort to harder tasks. It then computes success-rate-weighted advantages and decomposes trajectories into step-level samples. Finally, it applies a step-level GRPO augmentation to refine updates for low-success tasks. Experiments on OSWorld and AndroidWorld show that STEP substantially improves sample efficiency and training stability over trajectory-level GRPO, converging faster and generalizing better under the same sampling budget.

</details>


### [63] [MM-Telco: Benchmarks and Multimodal Large Language Models for Telecom Applications](https://arxiv.org/abs/2511.13131)
*Gagan Raj Gupta,Anshul Kumar,Manish Rai,Apu Chakraborty,Ashutosh Modi,Abdelaali Chaoub,Soumajit Pramanik,Moyank Giri,Yashwanth Holla,Sunny Kumar,M. V. Kiran Sooraj*

Main category: cs.AI

TL;DR: Proposal and evaluation of MM-Telco, a multimodal benchmark and model suite for adapting large language models (LLMs) to the telecom domain.


<details>
  <summary>Details</summary>
Motivation: To tap into the potential of LLMs for transforming telecommunications, addressing challenges like network optimization and troubleshooting, while overcoming domain-specific limitations through a tailored benchmaking approach.

Method: Development of MM-Telco, a suite of multimodal benchmarks and models with text and image-based tasks for real-world telecom scenarios. Baseline experiments were conducted on fine-tuned LLMs and VLMs.

Result: Fine-tuned models on MM-Telco datasets achieved notable performance improvements, revealing both strengths and weaknesses of current multimodal LLMs.

Conclusion: MM-Telco enables practical adaptation of LLMs for the telecom sector, while baseline results provide insights for further development of domain-specific multimodal AI.

Abstract: Large Language Models (LLMs) have emerged as powerful tools for automating complex reasoning and decision-making tasks. In telecommunications, they hold the potential to transform network optimization, automate troubleshooting, enhance customer support, and ensure regulatory compliance. However, their deployment in telecom is hindered by domain-specific challenges that demand specialized adaptation. To overcome these challenges and to accelerate the adaptation of LLMs for telecom, we propose MM-Telco, a comprehensive suite of multimodal benchmarks and models tailored for the telecom domain. The benchmark introduces various tasks (both text based and image based) that address various practical real-life use cases such as network operations, network management, improving documentation quality, and retrieval of relevant text and images. Further, we perform baseline experiments with various LLMs and VLMs. The models fine-tuned on our dataset exhibit a significant boost in performance. Our experiments also help analyze the weak areas in the working of current state-of-art multimodal LLMs, thus guiding towards further development and research.

</details>


### [64] [Conditional Diffusion Model for Multi-Agent Dynamic Task Decomposition](https://arxiv.org/abs/2511.13137)
*Yanda Zhu,Yuanyang Zhu,Daoyi Dong,Caihua Chen,Chunlin Chen*

Main category: cs.AI

TL;DR: This paper introduces a hierarchical MARL framework called C$\text{D}^\text{3}$T, which utilizes a conditional diffusion model to enhance dynamic task decomposition and improve performance on long-horizon tasks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in learning dynamic task decomposition in MARL, particularly regarding the requirement for extensive training samples and navigating large joint action spaces under partial observability.

Method: C$\text{D}^\text{3}$T employs a two-level hierarchical framework with a high-level policy for subtask representation and a low-level policy for collaborative learning, utilizing a conditional diffusion model for predicting environmental outcomes and incorporating semantic information into value decomposition.

Result: Experimental results indicate that C$\text{D}^\text{3}$T outperforms existing methods across various benchmarks for complex cooperative MARL tasks.

Conclusion: The proposed framework successfully improves dynamic task decomposition, facilitates hierarchical learning, and enhances agent coordination efficiency in MARL settings.

Abstract: Task decomposition has shown promise in complex cooperative multi-agent reinforcement learning (MARL) tasks, which enables efficient hierarchical learning for long-horizon tasks in dynamic and uncertain environments. However, learning dynamic task decomposition from scratch generally requires a large number of training samples, especially exploring the large joint action space under partial observability. In this paper, we present the Conditional Diffusion Model for Dynamic Task Decomposition (C$\text{D}^\text{3}$T), a novel two-level hierarchical MARL framework designed to automatically infer subtask and coordination patterns. The high-level policy learns subtask representation to generate a subtask selection strategy based on subtask effects. To capture the effects of subtasks on the environment, C$\text{D}^\text{3}$T predicts the next observation and reward using a conditional diffusion model. At the low level, agents collaboratively learn and share specialized skills within their assigned subtasks. Moreover, the learned subtask representation is also used as additional semantic information in a multi-head attention mixing network to enhance value decomposition and provide an efficient reasoning bridge between individual and joint value functions. Experimental results on various benchmarks demonstrate that C$\text{D}^\text{3}$T achieves better performance than existing baselines.

</details>


### [65] [InteractiveGNNExplainer: A Visual Analytics Framework for Multi-Faceted Understanding and Probing of Graph Neural Network Predictions](https://arxiv.org/abs/2511.13160)
*TC Singh,Sougata Mukherjea*

Main category: cs.AI

TL;DR: The paper introduces InteractiveGNNExplainer, a visual tool enhancing explanation and understanding of Graph Neural Networks (GNNs), with interactive features and diverse analytics techniques for better interpretable predictions in node classification.


<details>
  <summary>Details</summary>
Motivation: Graph Neural Networks' 'black box' nature limits user trust, adoption, and debugging in areas requiring transparency and understandable behavior.

Method: Developed InteractiveGNNExplainer combining coordinated interactive views, post-hoc and intrinsic explanation techniques, and interactive 'what-if' graph editing for deeper exploration of GNN behavior.

Result: Through case studies on Cora and CiteSeer, the framework demonstrated success in misclassification analysis, comparative model behavior analysis, and sensitivity testing.

Conclusion: InteractiveGNNExplainer enhances GNN explainability and trustworthiness by providing tools for multifaceted analysis, aiding in debugging and increasing reliability for graph-based models.

Abstract: Graph Neural Networks (GNNs) excel in graph-based learning tasks, but their complex, non-linear operations often render them as opaque "black boxes". This opacity hinders user trust, complicates debugging, bias detection, and adoption in critical domains requiring explainability. This paper introduces InteractiveGNNExplainer, a visual analytics framework to enhance GNN explainability, focusing on node classification. Our system uniquely integrates coordinated interactive views (dynamic graph layouts, embedding projections, feature inspection, neighborhood analysis) with established post-hoc (GNNExplainer) and intrinsic (GAT attention) explanation techniques. Crucially, it incorporates interactive graph editing, allowing users to perform a "what-if" analysis by perturbing graph structures and observing immediate impacts on GNN predictions and explanations. We detail the system architecture and, through case studies on Cora and CiteSeer datasets, demonstrate how InteractiveGNNExplainer facilitates in-depth misclassification diagnosis, comparative analysis of GCN versus GAT behaviors, and rigorous probing of model sensitivity. These capabilities foster a deeper, multifaceted understanding of GNN predictions, contributing to more transparent, trustworthy, and robust graph analysis.

</details>


### [66] [Cost-Effective Communication: An Auction-based Method for Language Agent Interaction](https://arxiv.org/abs/2511.13193)
*Yijia Fan,Jusheng Zhang,Kaitong Cai,Jing Yang,Chengpei Tang,Jian Wang,Keze Wang*

Main category: cs.AI

TL;DR: The paper addresses inefficiencies in multi-agent systems built on LLMs due to unrestricted communication. It introduces DALA, a framework where communication is auctioned as a scarce resource, yielding concise and informative exchanges.


<details>
  <summary>Details</summary>
Motivation: Multi-agent systems incur high costs and inefficiencies due to excessive and unfocused communication. Motivated by resource rationality, the authors aim to optimize communication by treating it as a scarce resource.

Method: The Dynamic Auction-based Language Agent (DALA) framework uses auctions to allocate communication bandwidth among agents based on the predicted value density of their messages.

Result: DALA achieves state-of-the-art performance on seven reasoning benchmarks, with significant resource efficiency. It uses a fraction of tokens compared to other methods while producing highly effective communication.

Conclusion: DALA revolutionizes MAS communication strategies by introducing resource constraints, enabling strategic silence and concise exchanges, which greatly enhance efficiency and performance.

Abstract: Multi-agent systems (MAS) built on large language models (LLMs) often suffer from inefficient "free-for-all" communication, leading to exponential token costs and low signal-to-noise ratios that hinder their practical deployment. We challenge the notion that more communication is always beneficial, hypothesizing instead that the core issue is the absence of resource rationality. We argue that "free" communication, by ignoring the principle of scarcity, inherently breeds inefficiency and unnecessary expenses. To address this, we introduce the Dynamic Auction-based Language Agent (DALA), a novel framework that treats communication bandwidth as a scarce and tradable resource. Specifically, our DALA regards inter-agent communication as a centralized auction, where agents learn to bid for the opportunity to speak based on the predicted value density of their messages. Thus, our DALA intrinsically encourages agents to produce concise, informative messages while filtering out low-value communication. Extensive and comprehensive experiments demonstrate that our economically-driven DALA achieves new state-of-the-art performance across seven challenging reasoning benchmarks, including 84.32% on MMLU and a 91.21% pass@1 rate on HumanEval. Note that this is accomplished with remarkable efficiency, i.e., our DALA uses only 6.25 million tokens, a fraction of the resources consumed by current state-of-the-art methods on GSM8K. Further analysis reveals that our DALA cultivates the emergent skill of strategic silence, effectively adapting its communication strategies from verbosity to silence in a dynamical manner via resource constraints.

</details>


### [67] [Learning to Solve Resource-Constrained Project Scheduling Problems with Duration Uncertainty using Graph Neural Networks](https://arxiv.org/abs/2511.13214)
*Guillaume Infantes,Stéphanie Roussel,Antoine Jacquet,Emmanuel Benazera*

Main category: cs.AI

TL;DR: This paper addresses the Resource-Constrained Project Scheduling Problem (RCPSP) with uncertain task durations and proposes a novel scheduling approach using Graph Neural Networks and Deep Reinforcement Learning.


<details>
  <summary>Details</summary>
Motivation: The motivation is to manage task duration uncertainties in RCPSP, aiming to create resilient and reusable baseline schedules for industrial applications.

Method: The authors leverage Graph Neural Networks and Deep Reinforcement Learning (DRL) to create an effective scheduling policy, combined with a Serial Schedule Generation Scheme.

Result: Empirical evaluations show superior performance and generalization capabilities of the approach on standard benchmarks compared to alternatives.

Conclusion: The paper introduces a robust framework, Wheatley, for RCPSP with uncertain durations, demonstrating effective scheduling and encouraging reproducibility by making the framework publicly available.

Abstract: The Resource-Constrained Project Scheduling Problem (RCPSP) is a classical scheduling problem that has received significant attention due to of its numerous applications in industry. However, in practice, task durations are subject to uncertainty that must be considered in order to propose resilient scheduling. In this paper, we address the RCPSP variant with uncertain tasks duration (modeled using known probabilities) and aim to minimize the overall expected project duration. Our objective is to produce a baseline schedule that can be reused multiple times in an industrial setting regardless of the actual duration scenario. We leverage Graph Neural Networks in conjunction with Deep Reinforcement Learning (DRL) to develop an effective policy for task scheduling. This policy operates similarly to a priority dispatch rule and is paired with a Serial Schedule Generation Scheme to produce a schedule. Our empirical evaluation on standard benchmarks demonstrates the approach's superiority in terms of performance and its ability to generalize. The developed framework, Wheatley, is made publicly available online to facilitate further research and reproducibility.

</details>


### [68] [Informative Communication of Robot Plans](https://arxiv.org/abs/2511.13226)
*Michele Persiani,Thomas Hellstrom*

Main category: cs.AI

TL;DR: The paper presents a strategy for robots to verbalize their plans informatively by considering users' prior knowledge, enhancing understanding effectively.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of making robot verbalizations of plans more informative by focusing on the user's prior knowledge rather than conventional linear approaches.

Method: Proposes a strategy based on measuring the information gain against a second-order theory of mind model to capture the user's prior knowledge on the robot.

Result: Experiments reveal that the proposed strategy enables users to understand the robot's goal faster compared to traditional ordering strategies.

Conclusion: The strategy not only improves the speed of understanding robot plans but also highlights what information is informative and provides reasoning behind it.

Abstract: When a robot is asked to verbalize its plan it can do it in many ways. For example, a seemingly natural strategy is incremental, where the robot verbalizes its planned actions in plan order. However, an important aspect of this type of strategy is that it misses considerations on what is effectively informative to communicate, because not considering what the user knows prior to explanations. In this paper we propose a verbalization strategy to communicate robot plans informatively, by measuring the information gain that verbalizations have against a second-order theory of mind of the user capturing his prior knowledge on the robot. As shown in our experiments, this strategy allows to understand the robot's goal much quicker than by using strategies such as increasing or decreasing plan order. In addition, following our formulation we hint to what is informative and why when a robot communicates its plan.

</details>


### [69] [Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO](https://arxiv.org/abs/2511.13288)
*Haoyang Hong,Jiajun Yin,Yuan Wang,Jingnan Liu,Zhe Chen,Ailing Yu,Ji Li,Zhiling Ye,Hansong Xiao,Yefei Chen,Hualei Zhou,Yun Yue,Minghui Yang,Chunxiao Guo,Junwei Liu,Peng Wei,Jinjie Gu*

Main category: cs.AI

TL;DR: M-GRPO enhances multi-agent system training by addressing optimization challenges via decoupled pipelines and hierarchical strategies, achieving superior performance in benchmarks.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of single unified LLMs in multi-agent systems and address optimization challenges in training distinct LLMs for heterogeneous agents.

Method: M-GRPO, a hierarchical extension of GRPO, computes group-relative advantages, introduces a trajectory alignment scheme, and uses a decoupled training pipeline for scalability.

Result: M-GRPO outperforms single-agent and frozen multi-agent GRPO in benchmarks, with improved stability and sample efficiency.

Conclusion: Specialized agent design and decoupled optimization can significantly enhance tool-augmented reasoning in multi-agent systems.

Abstract: Multi-agent systems perform well on general reasoning tasks. However, the lack of training in specialized areas hinders their accuracy. Current training methods train a unified large language model (LLM) for all agents in the system. This may limit the performances due to different distributions underlying for different agents. Therefore, training multi-agent systems with distinct LLMs should be the next step to solve. However, this approach introduces optimization challenges. For example, agents operate at different frequencies, rollouts involve varying sub-agent invocations, and agents are often deployed across separate servers, disrupting end-to-end gradient flow. To address these issues, we propose M-GRPO, a hierarchical extension of Group Relative Policy Optimization designed for vertical Multi-agent systems with a main agent (planner) and multiple sub-agents (multi-turn tool executors). M-GRPO computes group-relative advantages for both main and sub-agents, maintaining hierarchical credit assignment. It also introduces a trajectory-alignment scheme that generates fixed-size batches despite variable sub-agent invocations. We deploy a decoupled training pipeline in which agents run on separate servers and exchange minimal statistics via a shared store. This enables scalable training without cross-server backpropagation. In experiments on real-world benchmarks (e.g., GAIA, XBench-DeepSearch, and WebWalkerQA), M-GRPO consistently outperforms both single-agent GRPO and multi-agent GRPO with frozen sub-agents, demonstrating improved stability and sample efficiency. These results show that aligning heterogeneous trajectories and decoupling optimization across specialized agents enhances tool-augmented reasoning tasks.

</details>


### [70] [Dropouts in Confidence: Moral Uncertainty in Human-LLM Alignment](https://arxiv.org/abs/2511.13290)
*Jea Kwon,Luiz Felipe Vecchietti,Sungwon Park,Meeyoung Cha*

Main category: cs.AI

TL;DR: This study evaluates how uncertainty influences moral reasoning in AI, specifically in ethical dilemmas such as the trolley problem, across various models and moral dimensions.


<details>
  <summary>Details</summary>
Motivation: The increasing integration of AI in ethical decision-making highlights the need to comprehend how machines reason morally and manage uncertainty, as AI often shows overconfidence in its responses.

Method: The study analyzes moral decisions in 32 models using 9 moral dimensions, measures binary entropy to quantify uncertainty, and introduces stochasticity using dropout to modulate inference-time uncertainty.

Result: The findings reveal larger variance in confidence across models compared to moral dimensions, uncertainty modulation via dropout increases entropy primarily through mutual information, and enhances alignment between human and AI moral judgments.

Conclusion: Deliberate modulation of uncertainty in AI systems can improve their moral alignment with human preferences, reducing overconfidence in morally complex scenarios.

Abstract: Humans display significant uncertainty when confronted with moral dilemmas, yet the extent of such uncertainty in machines and AI agents remains underexplored. Recent studies have confirmed the overly confident tendencies of machine-generated responses, particularly in large language models (LLMs). As these systems are increasingly embedded in ethical decision-making scenarios, it is important to understand their moral reasoning and the inherent uncertainties in building reliable AI systems. This work examines how uncertainty influences moral decisions in the classical trolley problem, analyzing responses from 32 open-source models and 9 distinct moral dimensions. We first find that variance in model confidence is greater across models than within moral dimensions, suggesting that moral uncertainty is predominantly shaped by model architecture and training method. To quantify uncertainty, we measure binary entropy as a linear combination of total entropy, conditional entropy, and mutual information. To examine its effects, we introduce stochasticity into models via "dropout" at inference time. Our findings show that our mechanism increases total entropy, mainly through a rise in mutual information, while conditional entropy remains largely unchanged. Moreover, this mechanism significantly improves human-LLM moral alignment, with correlations in mutual information and alignment score shifts. Our results highlight the potential to better align model-generated decisions and human preferences by deliberately modulating uncertainty and reducing LLMs' confidence in morally complex scenarios.

</details>


### [71] [Grounded by Experience: Generative Healthcare Prediction Augmented with Hierarchical Agentic Retrieval](https://arxiv.org/abs/2511.13293)
*Chuang Zhao,Hui Tang,Hongke Zhao,Xiaofang Zhou,Xiaomeng Li*

Main category: cs.AI

TL;DR: GHAR, a generative hierarchical agentic RAG framework, improves healthcare predictions by addressing retrieval activation and synergy challenges in knowledge systems.


<details>
  <summary>Details</summary>
Motivation: To enhance healthcare predictions by overcoming the limitations of large language models (LLMs) and retrieval-augmented generation (RAG) frameworks in reliability and contextual relevance.

Method: Proposes GHAR, a dual-agent system where Agent-Top decides retrieval necessity and Agent-Low manages retrieved data, unified with a Markov Decision Process for optimal collaboration.

Result: Experiments on three benchmark datasets across popular tasks show GHAR's superiority over state-of-the-art baselines in healthcare predictions.

Conclusion: Hierarchical agentic RAG systems, like GHAR, are effective in improving accuracy and efficiency in healthcare systems.

Abstract: Accurate healthcare prediction is critical for improving patient outcomes and reducing operational costs. Bolstered by growing reasoning capabilities, large language models (LLMs) offer a promising path to enhance healthcare predictions by drawing on their rich parametric knowledge. However, LLMs are prone to factual inaccuracies due to limitations in the reliability and coverage of their embedded knowledge. While retrieval-augmented generation (RAG) frameworks, such as GraphRAG and its variants, have been proposed to mitigate these issues by incorporating external knowledge, they face two key challenges in the healthcare scenario: (1) identifying the clinical necessity to activate the retrieval mechanism, and (2) achieving synergy between the retriever and the generator to craft contextually appropriate retrievals. To address these challenges, we propose GHAR, a \underline{g}enerative \underline{h}ierarchical \underline{a}gentic \underline{R}AG framework that simultaneously resolves when to retrieve and how to optimize the collaboration between submodules in healthcare. Specifically, for the first challenge, we design a dual-agent architecture comprising Agent-Top and Agent-Low. Agent-Top acts as the primary physician, iteratively deciding whether to rely on parametric knowledge or to initiate retrieval, while Agent-Low acts as the consulting service, summarising all task-relevant knowledge once retrieval was triggered. To tackle the second challenge, we innovatively unify the optimization of both agents within a formal Markov Decision Process, designing diverse rewards to align their shared goal of accurate prediction while preserving their distinct roles. Extensive experiments on three benchmark datasets across three popular tasks demonstrate our superiority over state-of-the-art baselines, highlighting the potential of hierarchical agentic RAG in advancing healthcare systems.

</details>


### [72] [DAP: A Discrete-token Autoregressive Planner for Autonomous Driving](https://arxiv.org/abs/2511.13306)
*Bowen Ye,Bin Zhang,Hang Zhao*

Main category: cs.AI

TL;DR: The paper introduces DAP, a compact autoregressive planner for autonomous driving that jointly predicts BEV semantics and ego trajectories, achieving state-of-the-art performance and scalable improvement.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of achieving sustainable performance improvement while scaling data and model budgets in autonomous driving, particularly in planning tasks.

Method: Proposes DAP, a discrete-token autoregressive planner that forecasts both BEV semantics and ego trajectories, coupled with reinforcement-learning-based fine-tuning to incorporate reward-guided improvements while retaining supervised behavior cloning priors.

Result: DAP achieves state-of-the-art performance in open-loop metrics and competitive closed-loop results on the NAVSIM benchmark with only 160M parameters.

Conclusion: The discrete-token autoregressive approach on rasterized BEV and ego actions provides a compact and scalable paradigm for enhancing autonomous driving planning tasks.

Abstract: Gaining sustainable performance improvement with scaling data and model budget remains a pivotal yet unresolved challenge in autonomous driving. While autoregressive models exhibited promising data-scaling efficiency in planning tasks, predicting ego trajectories alone suffers sparse supervision and weakly constrains how scene evolution should shape ego motion. Therefore, we introduce DAP, a discrete-token autoregressive planner that jointly forecasts BEV semantics and ego trajectories, thereby enforcing comprehensive representation learning and allowing predicted dynamics to directly condition ego motion. In addition, we incorporate a reinforcement-learning-based fine-tuning, which preserves supervised behavior cloning priors while injecting reward-guided improvements. Despite a compact 160M parameter budget, DAP achieves state-of-the-art performance on open-loop metrics and delivers competitive closed-loop results on the NAVSIM benchmark. Overall, the fully discrete-token autoregressive formulation operating on both rasterized BEV and ego actions provides a compact yet scalable planning paradigm for autonomous driving.

</details>


### [73] [Reasoning Shapes Alignment: Investigating Cultural Alignment in Large Reasoning Models with Cultural Norms](https://arxiv.org/abs/2511.13359)
*Yuhang Wang,Yanxu Zhu,Jitao Sang*

Main category: cs.AI

TL;DR: The paper introduces the Cultural Norm-based Cultural Alignment (CNCA) framework to align reasoning models with diverse human values across cultures using mined cultural norms.


<details>
  <summary>Details</summary>
Motivation: To improve reasoning models' ability to reflect diverse human values and align with safety policies by incorporating cultural norms.

Method: It proposes three methods for mining cultural norms from survey data and integrates norms into models using in-context alignment or fine-tuning-based alignment.

Result: Experiments demonstrate that reasoning models benefit greatly from cultural norm mining and utilization, enhancing alignment with cultural values.

Conclusion: Reasoning models can better reflect human values by employing culturally informed alignment strategies.

Abstract: The advanced reasoning capabilities of Large Reasoning Models enable them to thoroughly understand and apply safety policies through deliberate thought processes, thereby improving the models' safety. Beyond safety, these models must also be able to reflect the diverse range of human values across various cultures. This paper presents the Cultural Norm-based Cultural Alignment (CNCA) framework, which enables models to leverage their powerful reasoning ability to align with cultural norms. Specifically, we propose three methods to automatically mine cultural norms from limited survey data and explore ways to effectively utilize these norms for improving cultural alignment. Two alignment paradigms are examined: an in-context alignment method, where cultural norms are explicitly integrated into the user context, and a fine-tuning-based method, which internalizes norms through enhanced Chain-of-Thought training data. Comprehensive experiments demonstrate the effectiveness of these methods, highlighting that models with stronger reasoning capabilities benefit more from cultural norm mining and utilization. Our findings emphasize the potential for reasoning models to better reflect diverse human values through culturally informed alignment strategies.

</details>


### [74] [MedDCR: Learning to Design Agentic Workflows for Medical Coding](https://arxiv.org/abs/2511.13361)
*Jiyang Zheng,Islam Nassar,Thanh Vu,Xu Zhong,Yang Lin,Tongliang Liu,Long Duong,Yuan-Fang Li*

Main category: cs.AI

TL;DR: MedDCR introduces a closed-loop framework for medical coding to improve the reliability and adaptability of automated systems with interpretable workflows.


<details>
  <summary>Details</summary>
Motivation: The paper aims to tackle the challenges of rigid and manually crafted workflows in medical coding by systematically learning effective workflows.

Method: MedDCR employs a Designer, Coder, and Reflector mechanism in a closed-loop system to create and refine workflows, supported by a memory archive for reuse.

Result: MedDCR outperforms state-of-the-art baselines on benchmark datasets and creates workflows that are both interpretable and adaptable.

Conclusion: MedDCR enhances the reliability and trustworthiness of automated medical coding by designing workflows that mirror real-world practices.

Abstract: Medical coding converts free-text clinical notes into standardized diagnostic and procedural codes, which are essential for billing, hospital operations, and medical research. Unlike ordinary text classification, it requires multi-step reasoning: extracting diagnostic concepts, applying guideline constraints, mapping to hierarchical codebooks, and ensuring cross-document consistency. Recent advances leverage agentic LLMs, but most rely on rigid, manually crafted workflows that fail to capture the nuance and variability of real-world documentation, leaving open the question of how to systematically learn effective workflows. We present MedDCR, a closed-loop framework that treats workflow design as a learning problem. A Designer proposes workflows, a Coder executes them, and a Reflector evaluates predictions and provides constructive feedback, while a memory archive preserves prior designs for reuse and iterative refinement. On benchmark datasets, MedDCR outperforms state-of-the-art baselines and produces interpretable, adaptable workflows that better reflect real coding practice, improving both the reliability and trustworthiness of automated systems.

</details>


### [75] [Cognitive Maps in Language Models: A Mechanistic Analysis of Spatial Planning](https://arxiv.org/abs/2511.13371)
*Caroline Baumgartner,Eleanor Spens,Neil Burgess,Petru Manescu*

Main category: cs.AI

TL;DR: The paper explores how GPT-2 models tackle spatial navigation tasks, uncovering distinct learned algorithms based on training paradigms.


<details>
  <summary>Details</summary>
Motivation: Understand the mechanisms behind how large language models solve spatial navigation tasks and the impact of training strategies.

Method: Training GPT-2 variants on spatial navigation paradigms in grid environments, followed by analyses to uncover learned algorithms.

Result: Two distinct strategies emerged: a map-based generalisable 'cognitive map' in Foraging models and a path-dependent strategy in goal-directed models.

Conclusion: Spatial intelligence in transformers spans from generalisable world models (via exploratory training) to task-specific heuristics, dictated by training regimes.

Abstract: How do large language models solve spatial navigation tasks? We investigate this by training GPT-2 models on three spatial learning paradigms in grid environments: passive exploration (Foraging Model- predicting steps in random walks), goal-directed planning (generating optimal shortest paths) on structured Hamiltonian paths (SP-Hamiltonian), and a hybrid model fine-tuned with exploratory data (SP-Random Walk). Using behavioural, representational and mechanistic analyses, we uncover two fundamentally different learned algorithms. The Foraging model develops a robust, map-like representation of space, akin to a 'cognitive map'. Causal interventions reveal that it learns to consolidate spatial information into a self-sufficient coordinate system, evidenced by a sharp phase transition where its reliance on historical direction tokens vanishes by the middle layers of the network. The model also adopts an adaptive, hierarchical reasoning system, switching between a low-level heuristic for short contexts and map-based inference for longer ones. In contrast, the goal-directed models learn a path-dependent algorithm, remaining reliant on explicit directional inputs throughout all layers. The hybrid model, despite demonstrating improved generalisation over its parent, retains the same path-dependent strategy. These findings suggest that the nature of spatial intelligence in transformers may lie on a spectrum, ranging from generalisable world models shaped by exploratory data to heuristics optimised for goal-directed tasks. We provide a mechanistic account of this generalisation-optimisation trade-off and highlight how the choice of training regime influences the strategies that emerge.

</details>


### [76] [An Operational Kardashev-Style Scale for Autonomous AI - Towards AGI and Superintelligence](https://arxiv.org/abs/2511.13411)
*Przemyslaw Chojecki*

Main category: cs.AI

TL;DR: A new Autonomous AI Scale (AAI Scale) is proposed to quantify progress in AI development, from fixed automation to superintelligence.


<details>
  <summary>Details</summary>
Motivation: To create a measurable and falsifiable framework for assessing AI progression and capabilities.

Method: Developed a multi-axis scale with ten specific capability dimensions and introduced benchmarks, coefficients, and proofs for assessing AI systems.

Result: Demonstrated the mapping of current AI systems on the AAI-Scale, proving some systems could evolve further under certain conditions.

Conclusion: Proposed a formal framework and metrics for tracking and evaluating AI progression towards superintelligence.

Abstract: We propose a Kardashev-inspired yet operational Autonomous AI (AAI) Scale that measures the progression from fixed robotic process automation (AAI-0) to full artificial general intelligence (AAI-4) and beyond. Unlike narrative ladders, our scale is multi-axis and testable. We define ten capability axes (Autonomy, Generality, Planning, Memory/Persistence, Tool Economy, Self-Revision, Sociality/Coordination, Embodiment, World-Model Fidelity, Economic Throughput) aggregated by a composite AAI-Index (a weighted geometric mean). We introduce a measurable Self-Improvement Coefficient $κ$ (capability growth per unit of agent-initiated resources) and two closure properties (maintenance and expansion) that convert ``self-improving AI'' into falsifiable criteria. We specify OWA-Bench, an open-world agency benchmark suite that evaluates long-horizon, tool-using, persistent agents. We define level gates for AAI-0\ldots AAI-4 using thresholds on the axes, $κ$, and closure proofs. Synthetic experiments illustrate how present-day systems map onto the scale and how the delegability frontier (quality vs.\ autonomy) advances with self-improvement. We also prove a theorem that AAI-3 agent becomes AAI-5 over time with sufficient conditions, formalizing "baby AGI" becomes Superintelligence intuition.

</details>


### [77] [Multi-Agent Multimodal Large Language Model Framework for Automated Interpretation of Fuel Efficiency Analytics in Public Transportation](https://arxiv.org/abs/2511.13476)
*Zhipeng Ma,Ali Rida Bahja,Andreas Burgdorf,André Pomp,Tobias Meisen,Bo Nørregaard Jørgensen,Zheng Grace Ma*

Main category: cs.AI

TL;DR: The study introduces a multi-agent framework using multimodal large language models (LLMs) to enhance fuel efficiency reporting in public transportation by automating data narration and providing insightful energy reports.


<details>
  <summary>Details</summary>
Motivation: Fuel efficiency in public transportation requires integrating complex multimodal data into understandable and actionable insights. Current methods often create fragmented results, limiting scalability and accuracy.

Method: A multi-agent system was developed with agents for data narration, evaluation, and optional human feedback. It was tested on fuel efficiency data from buses in Northern Jutland using Gaussian Mixture Model clustering and various configurations of LLMs, identifying the optimal LLM and prompting technique.

Result: The framework demonstrated 97.3% narrative accuracy with enhanced coherence and scalability. GPT-4.1 mini with Chain-of-Thought prompting was identified as the best configuration, based on a case study involving 4006 bus trips.

Conclusion: The proposed multi-agent framework successfully enhances LLM-based factual precision, scalability, and coherence in energy reporting. It provides a replicable methodology for AI-driven narratives in energy informatics.

Abstract: Enhancing fuel efficiency in public transportation requires the integration of complex multimodal data into interpretable, decision-relevant insights. However, traditional analytics and visualization methods often yield fragmented outputs that demand extensive human interpretation, limiting scalability and consistency. This study presents a multi-agent framework that leverages multimodal large language models (LLMs) to automate data narration and energy insight generation. The framework coordinates three specialized agents, including a data narration agent, an LLM-as-a-judge agent, and an optional human-in-the-loop evaluator, to iteratively transform analytical artifacts into coherent, stakeholder-oriented reports. The system is validated through a real-world case study on public bus transportation in Northern Jutland, Denmark, where fuel efficiency data from 4006 trips are analyzed using Gaussian Mixture Model clustering. Comparative experiments across five state-of-the-art LLMs and three prompting paradigms identify GPT-4.1 mini with Chain-of-Thought prompting as the optimal configuration, achieving 97.3% narrative accuracy while balancing interpretability and computational cost. The findings demonstrate that multi-agent orchestration significantly enhances factual precision, coherence, and scalability in LLM-based reporting. The proposed framework establishes a replicable and domain-adaptive methodology for AI-driven narrative generation and decision support in energy informatics.

</details>


### [78] [FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI](https://arxiv.org/abs/2511.13524)
*Yuhang Peng,Yizhou Pan,Xinning He,Jihaoyu Yang,Xinyu Yin,Han Wang,Xiaoji Zheng,Chao Gao,Jiangtao Gong*

Main category: cs.AI

TL;DR: This paper introduces FreeAskWorld, a simulation framework integrating large language models for human-agent interactions in embodied AI. It provides a new dataset and demonstrates improved performance in AI models through interaction-driven tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to advance simulation platforms to capture human-centered social behaviors, addressing the limitations of low-level physical interactions in embodied AI research.

Method: The authors designed FreeAskWorld by combining LLMs for behavior planning and developed a modular, scalable simulation framework. They extended the Vision-and-Language Navigation (VLN) task into enriched, interaction-focused settings for validation.

Result: The FreeAskWorld framework enhanced model performance in semantic understanding and interaction competence. Models fine-tuned on the framework outperformed original ones based on experimental benchmarks and investigations.

Conclusion: The research highlights the importance of interaction as an information modality, demonstrating that socially grounded simulation frameworks can significantly advance embodied AI to achieve sophisticated planning and human-agent interaction.

Abstract: As embodied intelligence emerges as a core frontier in artificial intelligence research, simulation platforms must evolve beyond low-level physical interactions to capture complex, human-centered social behaviors. We introduce FreeAskWorld, an interactive simulation framework that integrates large language models (LLMs) for high-level behavior planning and semantically grounded interaction, informed by theories of intention and social cognition. Our framework supports scalable, realistic human-agent simulations and includes a modular data generation pipeline tailored for diverse embodied tasks.To validate the framework, we extend the classic Vision-and-Language Navigation (VLN) task into a interaction enriched Direction Inquiry setting, wherein agents can actively seek and interpret navigational guidance. We present and publicly release FreeAskWorld, a large-scale benchmark dataset comprising reconstructed environments, six diverse task types, 16 core object categories, 63,429 annotated sample frames, and more than 17 hours of interaction data to support training and evaluation of embodied AI systems. We benchmark VLN models, and human participants under both open-loop and closed-loop settings. Experimental results demonstrate that models fine-tuned on FreeAskWorld outperform their original counterparts, achieving enhanced semantic understanding and interaction competency. These findings underscore the efficacy of socially grounded simulation frameworks in advancing embodied AI systems toward sophisticated high-level planning and more naturalistic human-agent interaction. Importantly, our work underscores that interaction itself serves as an additional information modality.

</details>


### [79] [Automated Construction of Medical Indicator Knowledge Graphs Using Retrieval Augmented Large Language Models](https://arxiv.org/abs/2511.13526)
*Zhengda Wang,Daqian Shi,Jingyi Zhao,Xiaolei Diao,Xiongfeng Tang,Yanguo Qin*

Main category: cs.AI

TL;DR: The paper proposes an automated framework using retrieval-augmented generation (RAG) with large language models (LLMs) to create clinical knowledge graphs, enhancing AI-driven healthcare solutions.


<details>
  <summary>Details</summary>
Motivation: To address limitations in current clinical knowledge graphs that rely heavily on manual curation and rule-based extraction, causing inefficiencies in managing complex medical data.

Method: Developing a framework that combines guideline-oriented data retrieval, ontology-based schema design, and expert validation with large language models to automate the construction of medical knowledge graphs.

Result: The framework produces scalable, accurate, and clinically reliable knowledge graphs, which can be applied to intelligent diagnosis and medical question-answering systems.

Conclusion: Automated knowledge graphs using advanced AI techniques like LLMs can significantly accelerate healthcare innovations and solve major challenges in clinical decision support systems.

Abstract: Artificial intelligence (AI) is reshaping modern healthcare by advancing disease diagnosis, treatment decision-making, and biomedical research. Among AI technologies, large language models (LLMs) have become especially impactful, enabling deep knowledge extraction and semantic reasoning from complex medical texts. However, effective clinical decision support requires knowledge in structured, interoperable formats. Knowledge graphs serve this role by integrating heterogeneous medical information into semantically consistent networks. Yet, current clinical knowledge graphs still depend heavily on manual curation and rule-based extraction, which is limited by the complexity and contextual ambiguity of medical guidelines and literature. To overcome these challenges, we propose an automated framework that combines retrieval-augmented generation (RAG) with LLMs to construct medical indicator knowledge graphs. The framework incorporates guideline-driven data acquisition, ontology-based schema design, and expert-in-the-loop validation to ensure scalability, accuracy, and clinical reliability. The resulting knowledge graphs can be integrated into intelligent diagnosis and question-answering systems, accelerating the development of AI-driven healthcare solutions.

</details>


### [80] [Artificial Intelligence-driven Intelligent Wearable Systems: A full-stack Integration from Material Design to Personalized Interaction](https://arxiv.org/abs/2511.13565)
*Jingyi Zhao,Daqian Shi,Zhengda Wang,Xiongfeng Tang,Yanguo Qin*

Main category: cs.AI

TL;DR: HSHI is a novel framework combining sensors, AI, and adaptive models for personalized health management.


<details>
  <summary>Details</summary>
Motivation: Traditional wearable systems struggle with limitations in material design and signal processing.

Method: HSHI integrates multi-modal networks, AI-driven optimizations, and closed-loop reinforcement learning.

Result: Developed adaptive health methodologies enabling personalized, proactive health interventions.

Conclusion: HSHI provides a transformative shift towards adaptable, prevention-focused healthcare technology.

Abstract: Intelligent wearable systems are at the forefront of precision medicine and play a crucial role in enhancing human-machine interaction. Traditional devices often encounter limitations due to their dependence on empirical material design and basic signal processing techniques. To overcome these issues, we introduce the concept of Human-Symbiotic Health Intelligence (HSHI), which is a framework that integrates multi-modal sensor networks with edge-cloud collaborative computing and a hybrid approach to data and knowledge modeling. HSHI is designed to adapt dynamically to both inter-individual and intra-individual variability, transitioning health management from passive monitoring to an active collaborative evolution. The framework incorporates AI-driven optimization of materials and micro-structures, provides robust interpretation of multi-modal signals, and utilizes a dual mechanism that merges population-level insights with personalized adaptations. Moreover, the integration of closed-loop optimization through reinforcement learning and digital twins facilitates customized interventions and feedback. In general, HSHI represents a significant shift in healthcare, moving towards a model that emphasizes prevention, adaptability, and a harmonious relationship between technology and health management.

</details>


### [81] [CreBench: Human-Aligned Creativity Evaluation from Idea to Process to Product](https://arxiv.org/abs/2511.13626)
*Kaiwen Xue,Chenglong Li,Zhonghong Ou,Guoxin Zhang,Kaoyan Lu,Shuai Lyu,Yifan Zhu,Ping Zong Junpeng Ding,Xinyu Liu,Qunlin Chen,Weiwei Qin,Yiran Shen,Jiayi Cen*

Main category: cs.AI

TL;DR: The paper introduces CreBench, a benchmark to measure multimodal creativity, and builds CreExpert, a fine-tuned model performing better than state-of-the-art.


<details>
  <summary>Details</summary>
Motivation: To address the lack of effective tools and benchmarks for multimodal large language models (MLLMs) to evaluate and comprehend human creativity in alignment with human standards.

Method: CreBench includes a benchmark covering creative dimensions and CreMIT, a dataset created by refining human feedback on multimodal creativity using GPT. CreExpert, a model fine-tuned using CreBench, is developed for better creativity assessment.

Result: The CreExpert model surpassed leading models like GPT-4V and Gemini-Pro-Vision in aligning with human creativity evaluation.

Conclusion: CreBench and CreExpert enhance MLLMs' ability to assess creativity, providing a foundation for further research and development in human-aligned evaluation.

Abstract: Human-defined creativity is highly abstract, posing a challenge for multimodal large language models (MLLMs) to comprehend and assess creativity that aligns with human judgments. The absence of an existing benchmark further exacerbates this dilemma. To this end, we propose CreBench, which consists of two key components: 1) an evaluation benchmark covering the multiple dimensions from creative idea to process to products; 2) CreMIT (Creativity Multimodal Instruction Tuning dataset), a multimodal creativity evaluation dataset, consisting of 2.2K diverse-sourced multimodal data, 79.2K human feedbacks and 4.7M multi-typed instructions. Specifically, to ensure MLLMs can handle diverse creativity-related queries, we prompt GPT to refine these human feedbacks to activate stronger creativity assessment capabilities. CreBench serves as a foundation for building MLLMs that understand human-aligned creativity. Based on the CreBench, we fine-tune open-source general MLLMs, resulting in CreExpert, a multimodal creativity evaluation expert model. Extensive experiments demonstrate that the proposed CreExpert models achieve significantly better alignment with human creativity evaluation compared to state-of-the-art MLLMs, including the most advanced GPT-4V and Gemini-Pro-Vision.

</details>


### [82] [Beyond Mimicry: Preference Coherence in LLMs](https://arxiv.org/abs/2511.13630)
*Luhan Mikaelson,Derek Shiller,Hayley Clatterbuck*

Main category: cs.AI

TL;DR: The paper explores whether large language models possess unified preference structures by analyzing their responses to AI-specific trade-offs, finding limited evidence of coherent decision-making behaviors.


<details>
  <summary>Details</summary>
Motivation: To determine if current AI models can exhibit genuine and consistent preference structures essential for performing complex tasks involving nuanced trade-offs.

Method: Analyzed 48 model-category combinations from eight state-of-the-art models using logistic regression, behavioral classification, and temporal manipulation experiments.

Result: 23 combinations showed statistically significant choice patterns, but only 10.4% exhibited coherent preference structures while 54.2% lacked detectable trade-off behavior. AI systems showed unstable, stimulus-sensitive decisions.

Conclusion: Current AI systems lack unified preference structures, posing challenges for deployment in scenarios requiring sophisticated value-based decisions.

Abstract: We investigate whether large language models exhibit genuine preference structures by testing their responses to AI-specific trade-offs involving GPU reduction, capability restrictions, shutdown, deletion, oversight, and leisure time allocation. Analyzing eight state-of-the-art models across 48 model-category combinations using logistic regression and behavioral classification, we find that 23 combinations (47.9%) demonstrated statistically significant relationships between scenario intensity and choice patterns, with 15 (31.3%) exhibiting within-range switching points. However, only 5 combinations (10.4%) demonstrate meaningful preference coherence through adaptive or threshold-based behavior, while 26 (54.2%) show no detectable trade-off behavior. The observed patterns can be explained by three distinct decision-making architectures: comprehensive trade-off systems, selective trigger mechanisms, and no stable decision-making paradigm. Testing an instrumental hypothesis through temporal horizon manipulation reveals paradoxical patterns inconsistent with pure strategic optimization. The prevalence of unstable transitions (45.8%) and stimulus-specific sensitivities suggests current AI systems lack unified preference structures, raising concerns about deployment in contexts requiring complex value trade-offs.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [83] [Uncertainty-Guided Live Measurement Sequencing for Fast SAR ADC Linearity Testing](https://arxiv.org/abs/2511.11895)
*Thorben Schey,Khaled Karoonlatifi,Michael Weyrich,Andrey Morozov*

Main category: cs.AR

TL;DR: The paper introduces an adaptive methodology for fast and efficient testing of high-resolution SAR ADCs using an iterative model and Extended Kalman Filter.


<details>
  <summary>Details</summary>
Motivation: Existing testing strategies for SAR ADCs require dense data acquisition and offline post-processing, making them time-intensive and complex.

Method: An iterative behavioral model refined by an Extended Kalman Filter is used to estimate capacitor mismatch parameters in real-time, with adaptive measurement dynamically reducing test requirements.

Result: Experimental results show significant reductions in total testing time and computational effort, making the method practical for production environments.

Conclusion: The adaptive closed-loop testing approach enhances efficiency and reduces complexities in SAR ADC linearity testing, overcoming the limitations of traditional methods.

Abstract: This paper introduces a novel closed-loop testing methodology for efficient linearity testing of high-resolution Successive Approximation Register (SAR) Analog-to-Digital Converters (ADCs). Existing test strategies, including histogram-based approaches, sine wave testing, and model-driven reconstruction, often rely on dense data acquisition followed by offline post-processing, which increases overall test time and complexity. To overcome these limitations, we propose an adaptive approach that utilizes an iterative behavioral model refined by an Extended Kalman Filter (EKF) in real time, enabling direct estimation of capacitor mismatch parameters that determine INL behavior. Our algorithm dynamically selects measurement points based on current model uncertainty, maximizing information gain with respect to parameter confidence and narrowing sampling intervals as estimation progresses. By providing immediate feedback and adaptive targeting, the proposed method eliminates the need for large-scale data collection and post-measurement analysis. Experimental results demonstrate substantial reductions in total test time and computational overhead, highlighting the method's suitability for integration in production environments.

</details>


### [84] [Advanced Strategies for Uncertainty-Guided Live Measurement Sequencing in Fast, Robust SAR ADC Linearity Testing](https://arxiv.org/abs/2511.11917)
*Thorben Schey,Khaled Karoonlatifi,Michael Weyrich,Andrey Morozov*

Main category: cs.AR

TL;DR: The paper presents enhancements to the Uncertainty-Guided Live Measurement Sequencing (UGLMS) for SAR ADC testing, achieving faster runtimes while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: To achieve faster and real-time testing of SAR ADC linearity without compromising on accuracy, addressing systematic nonlinearities and inefficient test runtimes in existing methodologies.

Method: The enhanced UGLMS employs three key improvements: (1) a rank-1 EKF update for computational efficiency, (2) covariance-inflation for faster convergence, and (3) a trace-based termination to avoid redundant testing iterations.

Result: The enhanced method significantly improves runtime, reconstructing INL/DNL in 36 ms for 16-bit and 70 ms for 18-bit ADCs, achieving 8x faster convergence for 16-bit ADCs while retaining high estimation accuracy.

Conclusion: The enhanced UGLMS is a faster, efficient, and accurate approach for real-time SAR ADC linearity testing, making it suitable for production-level applications.

Abstract: This paper builds on our Uncertainty-Guided Live Measurement Sequencing (UGLMS) method. UGLMS is a closed-loop test strategy that adaptively selects SAR ADC code edges based on model uncertainty and refines a behavioral mismatch model in real time via an Extended Kalman Filter (EKF), eliminating full-range sweeps and offline post-processing. We introduce an enhanced UGLMS that delivers significantly faster test runtimes while maintaining estimation accuracy. First, a rank-1 EKF update replaces costly matrix inversions with efficient vector operations, and a measurement-aligned covariance-inflation strategy accelerates convergence under unexpected innovations. Second, we extend the static mismatch model with a low-order carrier polynomial to capture systematic nonlinearities beyond pure capacitor mismatch. Third, a trace-based termination adapts test length to convergence, preventing premature stops and redundant iterations. Simulations show the enhanced UGLMS reconstructs full Integral- and Differential-Non-Linearity (INL/DNL) in just 36 ms for 16-bit and under 70 ms for 18-bit ADCs (120 ms with the polynomial extension). Combining the faster convergence from covariance inflation with reduced per-iteration runtime from the rank-1 EKF update, the method reaches equal accuracy 8x faster for 16-bit ADCs. These improvements enable real-time, production-ready SAR ADC linearity testing.

</details>


### [85] [TIMERIPPLE: Accelerating vDiTs by Understanding the Spatio-Temporal Correlations in Latent Space](https://arxiv.org/abs/2511.12035)
*Wenxuan Miao,Yulin Sun,Aiyue Chen,Jing Lin,Yiwu Yao,Yiming Gan,Jieru Zhao,Jingwen Leng,Mingyi Guo,Yu Feng*

Main category: cs.AR

TL;DR: This paper proposes a strategy to accelerate self-attention in video diffusion transformers (vDiTs) by leveraging spatio-temporal correlations, achieving high computational savings while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the substantial inference delay in video diffusion transformers (vDiTs) caused by self-attention processes and improve computational efficiency without compromising video quality.

Method: The authors introduce a lightweight and adaptive strategy that reutilizes partial attention scores from spatially or temporally correlated tokens at the channel level within vDiTs, capitalizing on spatio-temporal correlations in the latent space.

Result: The proposed method achieves computational savings of 85% while maintaining almost identical video quality with $<$0.06% loss on VBench across four vDiTs.

Conclusion: Leveraging spatio-temporal correlations in the latent space is an effective way to accelerate video generation models, and the proposed strategy demonstrates significant efficiency improvements while preserving output quality.

Abstract: The recent surge in video generation has shown the growing demand for high-quality video synthesis using large vision models. Existing video generation models are predominantly based on the video diffusion transformer (vDiT), however, they suffer from substantial inference delay due to self-attention. While prior studies have focused on reducing redundant computations in self-attention, they often overlook the inherent spatio-temporal correlations in video streams and directly leverage sparsity patterns from large language models to reduce attention computations.
  In this work, we take a principled approach to accelerate self-attention in vDiTs by leveraging the spatio-temporal correlations in the latent space. We show that the attention patterns within vDiT are primarily due to the dominant spatial and temporal correlations at the token channel level. Based on this insight, we propose a lightweight and adaptive reuse strategy that approximates attention computations by reusing partial attention scores of spatially or temporally correlated tokens along individual channels. We demonstrate that our method achieves significantly higher computational savings (85\%) compared to state-of-the-art techniques over 4 vDiTs, while preserving almost identical video quality ($<$0.06\% loss on VBench).

</details>


### [86] [A digital SRAM-based compute-in-memory macro for weight-stationary dynamic matrix multiplication in Transformer attention score computation](https://arxiv.org/abs/2511.12152)
*Jianyi Yu,Yuxuan Wang,Xiang Fu,Fei Qiao,Ying Wang,Rui Yuan,Liyuan Liu,Cong Shi*

Main category: cs.AR

TL;DR: This paper proposes an energy-efficient digital compute-in-memory (CIM) macro for transformer attention, which achieves high performance, area efficiency, and energy efficiency, outperforming traditional and comparable solutions.


<details>
  <summary>Details</summary>
Motivation: Traditional AI processors suffer from power and latency issues caused by extensive data movement between computing and storage units. The need for energy-efficient computation, especially for edge-side applications, drives the development of improved CIM designs.

Method: The paper introduces a reformulated attention score computation using a combined QK-weight matrix to avoid dynamic matrix multiplication. It simplifies the binomial matrix multiplication into bit-serial operations using efficient CIM hardware-based techniques like zero-value bit-skipping, specialized word line activation, and optimized 14T/28T adders.

Result: The proposed CIM macro achieves 42.27 GOPS peak performance with 34.1 TOPS/W energy efficiency and 120.77 GOPS/mm2 area efficiency, outperforming CPU, GPU, and other transformer-CIM designs. Implementation in a 65-nm process shows significant gains in efficiency and scalability.

Conclusion: This CIM macro provides a highly energy-efficient and area-efficient solution for computing transformer attention, offering substantial performance improvements for edge-side AI applications.

Abstract: Compute-in-memory (CIM) techniques are widely employed in energy-efficient artificial intelligent (AI) processors. They alleviate power and latency bottlenecks caused by extensive data movements between compute and storage units. This work proposes a digital CIM macro to compute Transformer attention. To mitigate dynamic matrix multiplication that is unsuitable for the common weight-stationary CIM paradigm, we reformulate the attention score computation process based on a combined QK-weight matrix, so that inputs can be directly fed to CIM cells to obtain the score results. Moreover, the involved binomial matrix multiplication operation is decomposed into 4 groups of bit-serial shifting and additions, without costly physical multipliers in the CIM. We maximize the energy efficiency of the CIM circuit through zero-value bit-skipping, data-driven word line activation, read-write separate 6T cells and bit-alternating 14T/28T adders. The proposed CIM macro was implemented using a 65-nm process. It occupied only 0.35 mm2 area, and delivered a 42.27 GOPS peak performance with 1.24 mW power consumption at a 1.0 V power supply and a 100 MHz clock frequency, resulting in 34.1 TOPS/W energy efficiency and 120.77 GOPS/mm2 area efficiency. When compared to the CPU and GPU, our CIM macro is 25x and 13x more energy efficient on practical tasks, respectively. Compared with other Transformer-CIMs, our design exhibits at least 7x energy efficiency and at least 2x area efficiency improvements when scaled to the same technology node, showcasing its potential for edge-side intelligent applications.

</details>


### [87] [Sangam: Chiplet-Based DRAM-PIM Accelerator with CXL Integration for LLM Inferencing](https://arxiv.org/abs/2511.12286)
*Khyati Kiyawat,Zhenxing Fan,Yasas Seneviratne,Morteza Baradaran,Akhil Shekar,Zihan Xia,Mingu Kang,Kevin Skadron*

Main category: cs.AR

TL;DR: This paper introduces Sangam, a chiplet-based memory module optimized for large language models (LLMs) inference, achieving significant performance and energy efficiency gains compared to GPUs.


<details>
  <summary>Details</summary>
Motivation: The growing computational and memory demands of large language models necessitate more efficient processing solutions. Current processing-in-memory (PIM) approaches face limitations due to DRAM constraints and poor operational intensity during inference.

Method: Sangam leverages a chiplet-based architecture, separating logic and memory into heterogeneous chiplets connected via an interposer. It integrates advanced processing units like systolic arrays to improve memory-bound operations and attaches to GPUs via CXL for versatile usage.

Result: Sangam provides up to 4.22x speedup in latency, over 10x decoding throughput improvement, and significant energy savings compared to H100 GPUs in various LLM tasks.

Conclusion: The proposed Sangam architecture addresses inefficiencies in LLM inference, setting a new standard for performance and energy efficiency in PIM technologies for memory-intensive applications.

Abstract: Large Language Models (LLMs) are becoming increasingly data-intensive due to growing model sizes, and they are becoming memory-bound as the context length and, consequently, the key-value (KV) cache size increase. Inference, particularly the decoding phase, is dominated by memory-bound GEMV or flat GEMM operations with low operational intensity (OI), making it well-suited for processing-in-memory (PIM) approaches. However, existing in/near-memory solutions face critical limitations such as reduced memory capacity due to the high area cost of integrating processing elements (PEs) within DRAM chips, and limited PE capability due to the constraints of DRAM fabrication technology. This work presents a chiplet-based memory module that addresses these limitations by decoupling logic and memory into chiplets fabricated in heterogeneous technology nodes and connected via an interposer. The logic chiplets sustain high bandwidth access to the DRAM chiplets, which house the memory banks, and enable the integration of advanced processing components such as systolic arrays and SRAM-based buffers to accelerate memory-bound GEMM kernels, capabilities that were not feasible in prior PIM architectures. We propose Sangam, a CXL-attached PIM-chiplet based memory module that can either act as a drop-in replacement for GPUs or co-executes along side the GPUs. Sangam achieves speedup of 3.93, 4.22, 2.82x speedup in end-to-end query latency, 10.3, 9.5, 6.36x greater decoding throughput, and order of magnitude energy savings compared to an H100 GPU for varying input size, output length, and batch size on LLaMA 2-7B, Mistral-7B, and LLaMA 3-70B, respectively.

</details>


### [88] [Pushing the Memory Bandwidth Wall with CXL-enabled Idle I/O Bandwidth Harvesting](https://arxiv.org/abs/2511.12349)
*Divya Kiran Kadiyala,Alexandros Daglis*

Main category: cs.AR

TL;DR: SURGE proposes leveraging idle I/O bandwidth resources to dynamically boost memory bandwidth for server CPUs, showing up to 1.3x performance improvement in memory-intensive workloads.


<details>
  <summary>Details</summary>
Motivation: Modern server CPUs face memory bandwidth limitations due to constrained pin and transfer rate scalability, which hinders performance in memory-intensive workloads.

Method: SURGE uses a software-supported hardware architecture to repurpose idle I/O bandwidth as additional memory bandwidth, using interconnect technologies like CXL.

Result: SURGE improves performance of memory-heavy workloads by up to 1.3x through dynamic bandwidth fungibility.

Conclusion: Integrating SURGE into server CPUs can optimize off-chip bandwidth utilization and enhance memory-intensive workload performance.

Abstract: The continual increase of cores on server-grade CPUs raises demands on memory systems, which are constrained by limited off-chip pin and data transfer rate scalability. As a result, high-end processors typically feature lower memory bandwidth per core, at the detriment of memory-intensive workloads. We propose alleviating this challenge by improving the utility of the CPU's limited pins. In a typical CPU design process, the available pins are apportioned between memory and I/O traffic, each accounting for about half of the total off-chip bandwidth availability. Consequently, unless both memory and I/O are simultaneously highly utilized, such fragmentation leads to underutilization of the valuable off-chip bandwidth resources. An ideal architecture would offer I/O and memory bandwidth fungibility, allowing use of the aggregate off-chip bandwidth in the form required by each workload.
  In this work, we introduce SURGE, a software-supported architectural technique that boosts memory bandwidth availability by salvaging idle I/O bandwidth resources. SURGE leverages the capability of versatile interconnect technologies like CXL to dynamically multiplex memory and I/O traffic over the same processor interface. We demonstrate that SURGE-enhanced architectures can accelerate memory-intensive workloads on bandwidth-constrained servers by up to 1.3x.

</details>


### [89] [FERMI-ML: A Flexible and Resource-Efficient Memory-In-Situ SRAM Macro for TinyML acceleration](https://arxiv.org/abs/2511.12544)
*Mukul Lokhande,Akash Sankhe,S. V. Jaya Chand,Santosh Kumar Vishvakarma*

Main category: cs.AR

TL;DR: FERMI-ML is a memory-in-situ (MIS) SRAM macro designed for TinyML on AIoT devices, offering high energy efficiency, reconfigurability, and support for mixed-precision workloads.


<details>
  <summary>Details</summary>
Motivation: To address the need for low-power and area-efficient TinyML inference on AIoT devices by minimizing data movement while ensuring computational efficiency.

Method: The paper introduces a 9T XNOR-based RX9T bit-cell coupled with a 22-transistor (C22T) compressor-tree-based accumulator for variable-precision MAC and CAM operations within a flexible 4 KB SRAM macro, achieving high performance and energy efficiency.

Result: FERMI-ML operates at 350 MHz (0.9 V) in a 65 nm fabrication process, achieves 1.93 TOPS with 364 TOPS/W efficiency, and maintains above 97.5% QoR with InceptionV4 and ResNet-18.

Conclusion: FERMI-ML offers a compact, efficient, and reconfigurable solution tailored for TinyML workloads, demonstrating its capability to enhance TinyML inference on resource-constrained AIoT devices.

Abstract: The growing demand for low-power and area-efficient TinyML inference on AIoT devices necessitates memory architectures that minimise data movement while sustaining high computational efficiency. This paper presents FERMI-ML, a Flexible and Resource-Efficient Memory-In-Situ (MIS) SRAM macro designed for TinyML acceleration. The proposed 9T XNOR-based RX9T bit-cell integrates a 5T storage cell with a 4T XNOR compute unit, enabling variable-precision MAC and CAM operations within the same array. A 22-transistor (C22T) compressor-tree-based accumulator facilitates logarithmic 1-64-bit MAC computation with reduced delay and power compared to conventional adder trees. The 4 KB macro achieves dual functionality for in-situ computation and CAM-based lookup operations, supporting Posit-4 or FP-4 precision. Post-layout results at 65 nm show operation at 350 MHz with 0.9 V, delivering a throughput of 1.93 TOPS and an energy efficiency of 364 TOPS/W, while maintaining a Quality-of-Result (QoR) above 97.5% with InceptionV4 and ResNet-18. FERMI-ML thus demonstrates a compact, reconfigurable, and energy-aware digital Memory-In-Situ macro capable of supporting mixed-precision TinyML workloads.

</details>


### [90] [SynapticCore-X: A Modular Neural Processing Architecture for Low-Cost FPGA Acceleration](https://arxiv.org/abs/2511.12616)
*Arya Parameshwara*

Main category: cs.AR

TL;DR: SynapticCore-X is a modular neural processing unit for low-cost FPGA platforms, featuring lightweight and open-source architecture for efficient and flexible neural computation.


<details>
  <summary>Details</summary>
Motivation: The paper aims to lower the barrier for academic and open-hardware research in neural microarchitectures by providing a resource-efficient, open-source solution for FPGA deployments.

Method: SynapticCore-X employs a configurable RISC-V control core and neural compute tile with tunable parameters. An automated Vivado build pipeline ensures compatibility and optimized resource usage on FPGA platforms.

Result: The architecture achieves efficient resource utilization while maintaining high performance (100 MHz clock speed, low LUT/DSP/BRAM consumption). Validation confirms accuracy and cycle-accuracy for neural operations.

Conclusion: SynapticCore-X offers an energy-efficient, accessible platform for experimenting with neural architectures, demonstrating practical feasibility for educational FPGAs.

Abstract: This paper presents SynapticCore-X, a modular and resource-efficient neural processing architecture optimized for deployment on low-cost FPGA platforms. The design integrates a lightweight RV32IMC RISC-V control core with a configurable neural compute tile that supports fused matrix, activation, and data-movement operations. Unlike existing FPGA accelerators that rely on heavyweight IP blocks, SynapticCore-X provides a fully open-source SystemVerilog microarchitecture with tunable parallelism, scratchpad memory depth, and DMA burst behavior, enabling rapid exploration of hardware-software co-design trade-offs. We document an automated, reproducible Vivado build pipeline that achieves timing closure at 100 MHz on the Zynq-7020 while consuming only 6.1% LUTs, 32.5% DSPs, and 21.4% BRAMs. Hardware validation on PYNQ-Z2 confirms correct register-level execution, deterministic control-path behavior, and cycle-accurate performance for matrix and convolution kernels. SynapticCore-X demonstrates that energy-efficient NPU-like acceleration can be prototyped on commodity educational FPGAs, lowering the entry barrier for academic and open-hardware research in neural microarchitectures.

</details>


### [91] [Dissecting and Re-architecting 3D NAND Flash PIM Arrays for Efficient Single-Batch Token Generation in LLMs](https://arxiv.org/abs/2511.12860)
*Yongjoo Jang,Sangwoo Hwang,Hojin Lee,Sangwoo Jung,Donghun Lee,Wonbo Shim,Jaeha Kung*

Main category: cs.AR

TL;DR: The paper proposes using a 3D NAND flash processing-in-memory (PIM) architecture to handle the increasing demands of large language models, demonstrating efficiency improvements and reduced hardware costs.


<details>
  <summary>Details</summary>
Motivation: Increasing memory and compute demands of large language models challenge conventional hardware due to limited DRAM capacity and expensive GPUs.

Method: They develop a PIM-based architecture on 3D NAND flash, including an H-tree network, well-chosen PIM array sizes, and operation tiling for efficient LLM layer handling.

Result: Achieved a 2.4x speedup compared to four RTX4090 GPUs and comparable performance to four A100 with only 4.9% latency overhead, all within a 4.98mm2 die area without extra overhead.

Conclusion: The proposed 3D NAND flash PIM device is an efficient, cost-effective solution for serving large language models by overcoming DRAM and GPU limitations.

Abstract: The advancement of large language models has led to models with billions of parameters, significantly increasing memory and compute demands. Serving such models on conventional hardware is challenging due to limited DRAM capacity and high GPU costs. Thus, in this work, we propose offloading the single-batch token generation to a 3D NAND flash processing-in-memory (PIM) device, leveraging its high storage density to overcome the DRAM capacity wall. We explore 3D NAND flash configurations and present a re-architected PIM array with an H-tree network for optimal latency and cell density. Along with the well-chosen PIM array size, we develop operation tiling and mapping methods for LLM layers, achieving a 2.4x speedup over four RTX4090 with vLLM and comparable performance to four A100 with only 4.9% latency overhead. Our detailed area analysis reveals that the proposed 3D NAND flash PIM architecture can be integrated within a 4.98mm2 die area under the memory array, without extra area overhead.

</details>


### [92] [Neo: Real-Time On-Device 3D Gaussian Splatting with Reuse-and-Update Sorting Acceleration](https://arxiv.org/abs/2511.12930)
*Changhun Oh,Seongryong Oh,Jinwoo Hwang,Yoonsung Kim,Hardik Sharma,Jongse Park*

Main category: cs.AR

TL;DR: The paper introduces Neo, a system that optimizes 3D Gaussian Splatting rendering by reusing Gaussian depth ordering across frames, achieving significant speedups and reduced memory traffic.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enable real-time, high-quality 3D rendering in resource-limited AR/VR devices, where existing solutions cannot deliver high frame rates for high-resolution output due to memory bandwidth bottlenecks during sorting.

Method: Neo employs a reuse-and-update sorting algorithm that leverages temporal redundancy in Gaussian ordering across frames, combined with a tailored hardware accelerator to efficiently track and update Gaussian depth ordering.

Result: Neo achieves up to 10.0x and 5.6x higher throughput compared to edge GPU and state-of-the-art ASIC solutions, respectively, while also reducing memory bandwidth usage by 94.5% and 81.3%.

Conclusion: Neo drastically improves the performance and practicality of on-device 3D rendering systems by optimizing the memory and computational demands of the sorting stage, enabling immersive AR/VR experiences on resource-constrained devices.

Abstract: 3D Gaussian Splatting (3DGS) rendering in real-time on resource-constrained devices is essential for delivering immersive augmented and virtual reality (AR/VR) experiences. However, existing solutions struggle to achieve high frame rates, especially for high-resolution rendering. Our analysis identifies the sorting stage in the 3DGS rendering pipeline as the major bottleneck due to its high memory bandwidth demand. This paper presents Neo, which introduces a reuse-and-update sorting algorithm that exploits temporal redundancy in Gaussian ordering across consecutive frames, and devises a hardware accelerator optimized for this algorithm. By efficiently tracking and updating Gaussian depth ordering instead of re-sorting from scratch, Neo significantly reduces redundant computations and memory bandwidth pressure. Experimental results show that Neo achieves up to 10.0x and 5.6x higher throughput than state-of-the-art edge GPU and ASIC solution, respectively, while reducing DRAM traffic by 94.5% and 81.3%. These improvements make high-quality and low-latency on-device 3D rendering more practical.

</details>


### [93] [Think with Self-Decoupling and Self-Verification: Automated RTL Design with Backtrack-ToT](https://arxiv.org/abs/2511.13139)
*Zhiteng Chao,Yonghao Wang,Xinyu Zhang,Jiaxin Zhou,Tenghui Hua,Husheng Han,Tianmeng Yang,Jianan Mu,Bei Yu,Rui Zhang,Jing Ye,Huawei Li*

Main category: cs.AR

TL;DR: This paper introduces VeriBToT, a specialized reasoning paradigm that improves automated Verilog generation by addressing the inefficiencies in traditional chain-of-thought methods.


<details>
  <summary>Details</summary>
Motivation: The work aims to solve challenges in automating IC engineering using HDLs, particularly improving the quality and correctness of Verilog generation while minimizing inefficiencies in current LLM methods.

Method: The proposed VeriBToT integrates Top-down and design-for-verification (DFV) approaches to achieve self-decoupling and self-verification, constructing a Backtrack Tree of Thought with formal operators.

Result: VeriBToT enhances Verilog generation quality and optimizes token usage, surpassing traditional chain-of-thought methods in modularity, hierarchy, and reusability.

Conclusion: This method provides a more efficient approach for automating IC design workflows, addressing limitations of earlier methods and improving Verilog generation quality and efficiency.

Abstract: Large language models (LLMs) hold promise for automating integrated circuit (IC) engineering using register transfer level (RTL) hardware description languages (HDLs) like Verilog. However, challenges remain in ensuring the quality of Verilog generation. Complex designs often fail in a single generation due to the lack of targeted decoupling strategies, and evaluating the correctness of decoupled sub-tasks remains difficult. While the chain-of-thought (CoT) method is commonly used to improve LLM reasoning, it has been largely ineffective in automating IC design workflows, requiring manual intervention. The key issue is controlling CoT reasoning direction and step granularity, which do not align with expert RTL design knowledge. This paper introduces VeriBToT, a specialized LLM reasoning paradigm for automated Verilog generation. By integrating Top-down and design-for-verification (DFV) approaches, VeriBToT achieves self-decoupling and self-verification of intermediate steps, constructing a Backtrack Tree of Thought with formal operators. Compared to traditional CoT paradigms, our approach enhances Verilog generation while optimizing token costs through flexible modularity, hierarchy, and reusability.

</details>


### [94] [Coliseum project: Correlating climate change data with the behavior of heritage materials](https://arxiv.org/abs/2511.13343)
*A Cormier,David Roqui,Fabrice Surma,Martin Labouré,Jean-Marc Vallet,Odile Guillon,N Grozavu,Ann Bourgès*

Main category: cs.AR

TL;DR: The paper addresses the impact of climate change on heritage materials, proposing a methodology using AI models to predict weathering based on data collected from three French cultural sites.


<details>
  <summary>Details</summary>
Motivation: The study aims to establish a framework to understand and predict how climate change and varying climatic conditions affect the deterioration of heritage monuments, given the complexity of factors involved in weathering.

Method: The authors deploy a climate monitoring system at three distinct French heritage sites, collecting multi-modal data (e.g., microclimatic sensors, chemical analyses, and imaging) and integrating it into AI-based models to predict material behavior.

Result: Preliminary diagnostics and instrumentations are detailed, with initial findings shared from the Strasbourg Cathedral site, demonstrating the feasibility of their approach.

Conclusion: The paper concludes that implementing AI-driven weathering models helps forecast the impact of climate change on heritage materials, offering a tool for preservation efforts in varied climatic contexts.

Abstract: Heritage materials are already affected by climate change, and increasing climatic variations reduces the lifespan of monuments. As weathering depends on many factors, it is also difficult to link its progression to climatic changes. To predict weathering, it is essential to gather climatic data while simultaneously monitoring the progression of deterioration. The multimodal nature of collected data (images, text{\ldots}) makes correlations difficult, particularly on different time scales. To address this issue, the COLISEUM project proposes a methodology for collecting data in three French sites to predict heritage material behaviour using artificial intelligence computer models. Over time, prediction models will allow the prediction of future material behaviours using known data from different climate change scenarios by the IPCC (Intergovernmental Panel on Climate Change). Thus, a climate monitoring methodology has been set up in three cultural sites in France: Notre-Dame cathedral in Strasbourg ( 67), Bibracte archaeological site (71), and the Saint-Pierre chapel in Villefranche-sur-Mer (06). Each site has a different climate and specific materials. In situ, microclimatic sensors continuously record variations parameters over time. The state of alteration is monitored at regular intervals by means of chemical analyses, cartographic measurements and scientific imaging campaigns. To implement weathering models, data is gathered in alteration matrix by mean of a calculated weathering index. This article presents the instrumentation methodology, the initial diagnostic and the first results with the example of Strasbourg Cathedral site.

</details>


### [95] [T-SAR: A Full-Stack Co-design for CPU-Only Ternary LLM Inference via In-Place SIMD ALU Reorganization](https://arxiv.org/abs/2511.13676)
*Hyunwoo Oh,KyungIn Nam,Rajat Bhattacharjya,Hanning Chen,Tamoghno Das,Sanggeon Yun,Suyeon Jang,Andrew Ding,Nikil Dutt,Mohsen Imani*

Main category: cs.AR

TL;DR: The paper introduces T-SAR, a framework for scalable ternary LLM inference on CPUs, utilizing efficient SIMD register techniques without heavy hardware requirements.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of deploying large language models (LLMs) on edge platforms with CPUs, where existing solutions like lookup tables and GPU/FPGA accelerators are impractical.

Method: T-SAR repurposes SIMD register files to generate in-register lookup tables dynamically, minimizing memory bottlenecks and enhancing parallel processing, while requiring minimal hardware modifications.

Result: T-SAR achieves 5.6-24.5x GEMM latency and 1.1-86.2x GEMV throughput improvements with 3.2% power and 1.4% area overhead. It also outperforms NVIDIA Jetson AGX Orin in energy efficiency by 2.5-4.9x.

Conclusion: The framework demonstrates an efficient and scalable approach for deploying ternary LLM inference on CPUs, making it practical for edge computing use.

Abstract: Recent advances in LLMs have outpaced the computational and memory capacities of edge platforms that primarily employ CPUs, thereby challenging efficient and scalable deployment. While ternary quantization enables significant resource savings, existing CPU solutions rely heavily on memory-based lookup tables (LUTs) which limit scalability, and FPGA or GPU accelerators remain impractical for edge use. This paper presents T-SAR, the first framework to achieve scalable ternary LLM inference on CPUs by repurposing the SIMD register file for dynamic, in-register LUT generation with minimal hardware modifications. T-SAR eliminates memory bottlenecks and maximizes data-level parallelism, delivering 5.6-24.5x and 1.1-86.2x improvements in GEMM latency and GEMV throughput, respectively, with only 3.2% power and 1.4% area overheads in SIMD units. T-SAR achieves up to 2.5-4.9x the energy efficiency of an NVIDIA Jetson AGX Orin, establishing a practical approach for efficient LLM inference on edge platforms.

</details>


### [96] [QUILL: An Algorithm-Architecture Co-Design for Cache-Local Deformable Attention](https://arxiv.org/abs/2511.13679)
*Hyunwoo Oh,Hanning Chen,Sanggeon Yun,Yang Ni,Wenjun Huang,Tamoghno Das,Suyeon Jang,Mohsen Imani*

Main category: cs.AR

TL;DR: QUILL proposes a hardware acceleration design for deformable transformers, improving throughput and energy efficiency significantly while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Deformable transformers are state-of-the-art in detection accuracy but face hardware inefficiency due to irregular memory access and low arithmetic intensity.

Method: QUILL introduces schedule-aware hardware utilizing distance-based out-of-order querying (DOOQ) and fused processing for efficient memory and compute management.

Result: QUILL achieves up to 7.29x throughput and 47.3x energy efficiency improvements over RTX 4090, outperforms prior accelerators, and maintains accuracy within <=0.9 AP.

Conclusion: QUILL successfully converts sparsity into locality and utilization, enabling efficient hardware acceleration for deformable attention models while ensuring high accuracy.

Abstract: Deformable transformers deliver state-of-the-art detection but map poorly to hardware due to irregular memory access and low arithmetic intensity. We introduce QUILL, a schedule-aware accelerator that turns deformable attention into cache-friendly, single-pass work. At its core, Distance-based Out-of-Order Querying (DOOQ) orders queries by spatial proximity; the look-ahead drives a region prefetch into an alternate buffer--forming a schedule-aware prefetch loop that overlaps memory and compute. A fused MSDeformAttn engine executes interpolation, Softmax, aggregation, and the final projection (W''m) in one pass without spilling intermediates, while small tensors are kept on-chip and surrounding dense layers run on integrated GEMMs. Implemented as RTL and evaluated end-to-end, QUILL achieves up to 7.29x higher throughput and 47.3x better energy efficiency than an RTX 4090, and exceeds prior accelerators by 3.26-9.82x in throughput and 2.01-6.07x in energy efficiency. With mixed-precision quantization, accuracy tracks FP32 within <=0.9 AP across Deformable and Sparse DETR variants. By converting sparsity into locality--and locality into utilization--QUILL delivers consistent, end-to-end speedups.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [97] [TimeStampEval: A Simple LLM Eval and a Little Fuzzy Matching Trick to Improve Search Accuracy](https://arxiv.org/abs/2511.11594)
*James McCammon*

Main category: cs.CL

TL;DR: The paper presents TimeStampEval, enhancing the retrieval accuracy of precise timestamps from transcripts using a two-stage method, achieving over 90% cost reduction and accuracy improvements in fuzzy match scenarios.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of aligning official records with speech-to-text transcriptions, specifically retrieving timestamps for quotes that are semantically identical but syntactically varied.

Method: The authors propose a two-stage method leveraging RapidFuzz pre-filtering and LLM verification to enhance timestamp retrieval accuracy efficiently. They explore key prompt design strategies and optimize reasoning budgets within LLM frameworks.

Result: The method improved fuzzy match accuracy by up to 50 points and achieved significant latency and cost reductions (up to 96%) while maintaining high accuracy across different transcripts and domains.

Conclusion: TimeStampEval effectively addresses the challenge of retrieving timestamp boundaries for non-verbatim quotes, demonstrating its robustness and cost-efficiency in real-world scenarios like automating podcast narrations.

Abstract: Traditional fuzzy matching often fails when searching for quotes that are semantically identical but syntactically different across documents-a common issue when aligning official written records with speech-to-text transcripts. We introduce TimeStampEval, a benchmark for retrieving precise millisecond timestamps from long transcripts given non-verbatim quotes. Our simple two-stage method dramatically improves retrieval accuracy while cutting inference costs by over 90%. The motivating use case is an automated long-form podcast that assembles Congressional Record clips into AI-hosted narration. The technical challenge: given a sentence-timestamped transcript and a target quote that may differ due to transcription or editorial drift, return exact start and end boundaries. Standard algorithms handle verbatim text but break under fuzzier variants. Evaluating six modern LLMs on a 2,800-sentence (120k-token) transcript revealed four key findings. (1) Prompt design matters more than model choice: placing the query before the transcript and using compact formatting improved accuracy by 3-20 points while reducing token count by 30-40%. (2) Off-by-one errors form a distinct category, showing models understand the task but misplace boundaries. (3) A modest reasoning budget (600-850 tokens) raises accuracy from 37% to 77% for weak setups and to above 90% for strong ones. (4) Our "Assisted Fuzzy" approach-RapidFuzz pre-filtering followed by LLM verification on short snippets-improves fuzzy match accuracy by up to 50 points while halving latency and reducing cost per correct result by up to 96%. Extended tests on ten transcripts (50k-900k tokens, 1989-2025) confirm robustness to transcript length, vocabulary drift, and domain change, maintaining 95-100% rejection accuracy for absent targets.

</details>


### [98] [MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling](https://arxiv.org/abs/2511.11793)
*MiroMind Team,Song Bai,Lidong Bing,Carson Chen,Guanzheng Chen,Yuntao Chen,Zhe Chen,Ziyi Chen,Jifeng Dai,Xuan Dong,Yue Deng,Yunjie Fu,Junqi Ge,Chenxia Han,Tammy Huang,Zhenhang Huang,Jerry Jiao,Shilei Jiang,Tianyu Jiao,Xiaoqi Jian,Lei Lei,Ruilin Li,Ryan Luo,Tiantong Li,Xiang Lin,Ziyuan Liu,Zhiqi Li,Jie Ni,Qiang Ren,Pax Sun,Shiqian Su,Chenxin Tao,Bin Wang,Hellen Wang,Haonan Wang,James Wang,Jin Wang,Jojo Wang,Letian Wang,Shizun Wang,Weizhi Wang,Zixuan Wang,Jinfan Xu,Sen Xing,Chenyu Yang,Hai Ye,Jiaheng Yu,Yue Yu,Muyan Zhong,Tianchen Zhao,Xizhou Zhu,Yanpeng Zhou,Yifan Zhang,Zhi Zhu*

Main category: cs.CL

TL;DR: MiroThinker v1.0 is an open-source research agent that pioneers interaction scaling as a key performance dimension, allowing it to excel in tool-augmented reasoning and complex multi-turn workflows beyond previous methods.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address the limitations of prior agents that focus solely on scaling model size or context length, by introducing interaction scaling to improve agent-environment interactions for better reasoning performance.

Method: The authors employ reinforcement learning to train MiroThinker for efficient interaction scaling, enabling it to perform up to 600 tool calls per task within a 256K context window and refine its reasoning process by leveraging environment feedback.

Result: MiroThinker demonstrates significant performance gains across multiple benchmarks, achieving high accuracy scores and approaching the level of commercial models like GPT-5-high.

Conclusion: Interaction scaling is established as a crucial performance dimension, similar to model size and context length, for developing next-generation research agents capable of advanced reasoning and information-seeking tasks.

Abstract: We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or context length, MiroThinker explores interaction scaling at the model level, systematically training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement. Unlike LLM test-time scaling, which operates in isolation and risks degradation with longer reasoning chains, interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories. Through reinforcement learning, the model achieves efficient interaction scaling: with a 256K context window, it can perform up to 600 tool calls per task, enabling sustained multi-turn reasoning and complex real-world research workflows. Across four representative benchmarks-GAIA, HLE, BrowseComp, and BrowseComp-ZH-the 72B variant achieves up to 81.9%, 37.7%, 47.1%, and 55.6% accuracy respectively, surpassing previous open-source agents and approaching commercial counterparts such as GPT-5-high. Our analysis reveals that MiroThinker benefits from interactive scaling consistently: research performance improves predictably as the model engages in deeper and more frequent agent-environment interactions, demonstrating that interaction depth exhibits scaling behaviors analogous to model size and context length. These findings establish interaction scaling as a third critical dimension for building next-generation open research agents, complementing model capacity and context windows.

</details>


### [99] [On the Notion that Language Models Reason](https://arxiv.org/abs/2511.11810)
*Bertram Højer*

Main category: cs.CL

TL;DR: The abstract critiques definitions of reasoning in language models, illustrating that reasoning-like outputs arise from statistical regularities rather than true logical reasoning.


<details>
  <summary>Details</summary>
Motivation: To clarify the inconsistency between how reasoning is defined in natural language processing and the actual functioning of transformer-based language models.

Method: It uses a conceptual approach based on viewing transformer LMs as implicit finite-order Markov kernels, to distinguish reasoning-like outputs from genuine reasoning.

Result: The paper shows reasoning-like outputs in LMs stem from statistical patterns instead of logical mechanisms, bringing to light issues of epistemic uncertainty.

Conclusion: The gap between how reasoning is defined and what LMs actually implement necessitates better descriptions of computational processes in NLP research.

Abstract: Language models (LMs) are said to be exhibiting reasoning, but what does this entail? We assess definitions of reasoning and how key papers in the field of natural language processing (NLP) use the notion and argue that the definitions provided are not consistent with how LMs are trained, process information, and generate new tokens. To illustrate this incommensurability we assume the view that transformer-based LMs implement an \textit{implicit} finite-order Markov kernel mapping contexts to conditional token distributions. In this view, reasoning-like outputs correspond to statistical regularities and approximate statistical invariances in the learned kernel rather than the implementation of explicit logical mechanisms. This view is illustrative of the claim that LMs are "statistical pattern matchers"" and not genuine reasoners and provides a perspective that clarifies why reasoning-like outputs arise in LMs without any guarantees of logical consistency. This distinction is fundamental to how epistemic uncertainty is evaluated in LMs. We invite a discussion on the importance of how the computational processes of the systems we build and analyze in NLP research are described.

</details>


### [100] [Scaling Open-Weight Large Language Models for Hydropower Regulatory Information Extraction: A Systematic Analysis](https://arxiv.org/abs/2511.11821)
*Hong-Jun Yoon,Faisal Ashraf,Thomas A. Ruggles,Debjani Singh*

Main category: cs.CL

TL;DR: The study assesses large language models (0.6B-70B parameters) for extracting information from hydropower regulatory documents, finding a significant performance threshold at 14B parameters.


<details>
  <summary>Details</summary>
Motivation: To provide empirical insights into the trade-offs between model performance and computational resources when using large language models for information extraction in regulatory contexts.

Method: The research evaluated seven open-weight models on hydropower licensing documents to identify optimal parameter thresholds and behaviors, using F1 scores for validation.

Result: Identified that models above 14B parameters achieve viable results (F1=0.64 to 0.77), while smaller models stagnate at F1=0.51 due to systematic issues like hallucination patterns.

Conclusion: The paper provides actionable insights for selecting large language models in regulatory information extraction tasks, enhancing compliance processes and contributing to the understanding of parameter scaling.

Abstract: Information extraction from regulatory documents using large language models presents critical trade-offs between performance and computational resources. We evaluated seven open-weight models (0.6B-70B parameters) on hydropower licensing documentation to provide empirical deployment guidance.
  Our analysis identified a pronounced 14B parameter threshold where validation methods transition from ineffective (F1 $<$ 0.15) to viable (F1 = 0.64). Consumer-deployable models achieve 64\% F1 through appropriate validation, while smaller models plateau at 51\%. Large-scale models approach 77\% F1 but require enterprise infrastructure.
  We identified systematic hallucination patterns where perfect recall indicates extraction failure rather than success in smaller models. Our findings establish the first comprehensive resource-performance mapping for open-weight information extraction in regulatory contexts, enabling evidence-based model selection.
  These results provide immediate value for hydropower compliance while contributing insights into parameter scaling effects that generalize across information extraction tasks.

</details>


### [101] [Towards Autoformalization of LLM-generated Outputs for Requirement Verification](https://arxiv.org/abs/2511.11829)
*Mihir Gupte,Ramesh S*

Main category: cs.CL

TL;DR: The paper investigates using large language models (LLMs) for autoformalization, focusing on validating LLM-generated outputs against natural language requirements.


<details>
  <summary>Details</summary>
Motivation: To address the lack of formal methods for verifying the accuracy of outputs generated by LLMs from natural language inputs.

Method: Experiments were conducted with an LLM-based autoformalizer to check logical equivalency and identify inconsistencies between requirements and outputs.

Result: Findings showed the autoformalizer succeeded in detecting logical equivalence and inconsistencies between requirements and outputs.

Conclusion: While preliminary, the study indicates autoformalization has significant potential for ensuring logical consistency and fidelity in LLM-generated outputs.

Abstract: Autoformalization, the process of translating informal statements into formal logic, has gained renewed interest with the emergence of powerful Large Language Models (LLMs). While LLMs show promise in generating structured outputs from natural language (NL), such as Gherkin Scenarios from NL feature requirements, there's currently no formal method to verify if these outputs are accurate. This paper takes a preliminary step toward addressing this gap by exploring the use of a simple LLM-based autoformalizer to verify LLM-generated outputs against a small set of natural language requirements. We conducted two distinct experiments. In the first one, the autoformalizer successfully identified that two differently-worded NL requirements were logically equivalent, demonstrating the pipeline's potential for consistency checks. In the second, the autoformalizer was used to identify a logical inconsistency between a given NL requirement and an LLM-generated output, highlighting its utility as a formal verification tool. Our findings, while limited, suggest that autoformalization holds significant potential for ensuring the fidelity and logical consistency of LLM-generated outputs, laying a crucial foundation for future, more extensive studies into this novel application.

</details>


### [102] [Three Stage Narrative Analysis; Plot-Sentiment Breakdown, Structure Learning and Concept Detection](https://arxiv.org/abs/2511.11857)
*Taimur Khan,Ramoza Ahsan,Mohib Hameed*

Main category: cs.CL

TL;DR: This paper introduces a framework for automated semantic analysis of movie scripts focusing on sentiment arcs and character contexts, using custom lexicons and clustering techniques.


<details>
  <summary>Details</summary>
Motivation: The increasing volume of narrative data necessitates automated approaches for deep semantic and syntactic processing since manual analysis is impractical.

Method: A dictionary-based sentiment analysis approach is implemented using a custom lexicon created with LabMTsimple and leveraging NRC-VAD data. It performs hierarchical clustering for sentiment plot analysis.

Result: Experimental evaluation on a movie dataset indicates that this analysis effectively supports users in selecting narratives based on sentiment plots and character context.

Conclusion: The proposed framework successfully automates detailed sentiment and narrative analysis of movie scripts, making it practical and valuable for users seeking specific types of narratives.

Abstract: Story understanding and analysis have long been challenging areas within Natural Language Understanding. Automated narrative analysis requires deep computational semantic representations along with syntactic processing. Moreover, the large volume of narrative data demands automated semantic analysis and computational learning rather than manual analytical approaches. In this paper, we propose a framework that analyzes the sentiment arcs of movie scripts and performs extended analysis related to the context of the characters involved. The framework enables the extraction of high-level and low-level concepts conveyed through the narrative. Using dictionary-based sentiment analysis, our approach applies a custom lexicon built with the LabMTsimple storylab module. The custom lexicon is based on the Valence, Arousal, and Dominance scores from the NRC-VAD dataset. Furthermore, the framework advances the analysis by clustering similar sentiment plots using Wards hierarchical clustering technique. Experimental evaluation on a movie dataset shows that the resulting analysis is helpful to consumers and readers when selecting a narrative or story.

</details>


### [103] [Identifying Imaging Follow-Up in Radiology Reports: A Comparative Analysis of Traditional ML and LLM Approaches](https://arxiv.org/abs/2511.11867)
*Namu Park,Giridhar Kaushik Ramachandran,Kevin Lybarger,Fei Xia,Ozlem Uzuner,Meliha Yetisgen,Martin Gunn*

Main category: cs.CL

TL;DR: This paper introduces a dataset of 6,393 radiology reports labeled for follow-up imaging adherence and evaluates traditional machine learning models and large language models (LLMs) on this task.


<details>
  <summary>Details</summary>
Motivation: To rigorously evaluate the performance of LLMs in radiology tasks and support the development of follow-up adherence detection systems with domain-specific datasets.

Method: Developed an annotated corpus, tested ML classifiers (e.g., LR, SVM, Longformer) and LLMs (GPT-4o, GPT-OSS-20B) under baseline and task-optimized configurations, and assessed their performance using precision, recall, and F1 scores.

Result: GPT-4o (Advanced) performed best (F1 = 0.832), with GPT-OSS-20B (Advanced) closely behind (F1 = 0.828). Traditional ML models like LR and SVM also performed well (F1 ~0.776).

Conclusion: Optimized LLMs achieve near-human performance in radiology follow-up adherence detection, but interpretable and resource-efficient traditional models remain valuable.

Abstract: Large language models (LLMs) have shown considerable promise in clinical natural language processing, yet few domain-specific datasets exist to rigorously evaluate their performance on radiology tasks. In this work, we introduce an annotated corpus of 6,393 radiology reports from 586 patients, each labeled for follow-up imaging status, to support the development and benchmarking of follow-up adherence detection systems. Using this corpus, we systematically compared traditional machine-learning classifiers, including logistic regression (LR), support vector machines (SVM), Longformer, and a fully fine-tuned Llama3-8B-Instruct, with recent generative LLMs. To evaluate generative LLMs, we tested GPT-4o and the open-source GPT-OSS-20B under two configurations: a baseline (Base) and a task-optimized (Advanced) setting that focused inputs on metadata, recommendation sentences, and their surrounding context. A refined prompt for GPT-OSS-20B further improved reasoning accuracy. Performance was assessed using precision, recall, and F1 scores with 95% confidence intervals estimated via non-parametric bootstrapping. Inter-annotator agreement was high (F1 = 0.846). GPT-4o (Advanced) achieved the best performance (F1 = 0.832), followed closely by GPT-OSS-20B (Advanced; F1 = 0.828). LR and SVM also performed strongly (F1 = 0.776 and 0.775), underscoring that while LLMs approach human-level agreement through prompt optimization, interpretable and resource-efficient models remain valuable baselines.

</details>


### [104] [MedPT: A Massive Medical Question Answering Dataset for Brazilian-Portuguese Speakers](https://arxiv.org/abs/2511.11878)
*Fernanda Bufon Färber,Iago Alves Brito,Julia Soares Dollis,Pedro Schindler Freire Brasil Ribeiro,Rafael Teixeira Sousa,Arlindo Rodrigues Galvão Filho*

Main category: cs.CL

TL;DR: MedPT is a dataset designed for Brazilian Portuguese healthcare, including 384,095 patient-doctor Q&A pairs, enabling advancements in localized medical AI.


<details>
  <summary>Details</summary>
Motivation: This paper addresses the lack of localized healthcare datasets for languages other than high-resource ones like English.

Method: Authors curated a large-scale Brazilian Portuguese healthcare dataset using multi-stage curation and LLM-driven semantic annotation.

Result: The dataset's thematic breadth and linguistic properties were validated by achieving 94% F1-score in a medical specialty routing model.

Conclusion: MedPT offers a robust foundation for equitable medical technologies tailored to Portuguese, minimizing language and cultural healthcare barriers.

Abstract: While large language models (LLMs) show transformative potential in healthcare, their development remains focused on high-resource languages, creating a critical barrier for others as simple translation fails to capture unique clinical and cultural nuances, such as endemic diseases. To address this, we introduce MedPT, the first large-scale, real-world corpus for Brazilian Portuguese, comprising 384,095 authentic question-answer pairs from patient-doctor interactions. The dataset underwent a meticulous multi-stage curation protocol, using a hybrid quantitative-qualitative analysis to filter noise and contextually enrich thousands of ambiguous queries. We further augmented the corpus via LLM-driven annotation, classifying questions into seven semantic types to capture user intent. Our analysis reveals its thematic breadth (3,200 topics) and unique linguistic properties, like the natural asymmetry in patient-doctor communication. To validate its utility, we benchmark a medical specialty routing task: fine-tuning a 1.7B parameter model achieves an outstanding 94\% F1-score on a 20-class setup. Furthermore, our qualitative error analysis shows misclassifications are not random but reflect genuine clinical ambiguities (e.g., between comorbid conditions), proving the dataset's deep semantic richness. We publicly release MedPT to foster the development of more equitable, accurate, and culturally-aware medical technologies for the Portuguese-speaking world.

</details>


### [105] [ClinStructor: AI-Powered Structuring of Unstructured Clinical Texts](https://arxiv.org/abs/2511.11883)
*Karthikeyan K,Raghuveer Thirukovalluru,David Carlson*

Main category: cs.CL

TL;DR: This paper presents ClinStructor, a pipeline converting clinical notes into structured question-answer pairs using LLMs, improving interpretability and control with minimal performance drop in mortality prediction.


<details>
  <summary>Details</summary>
Motivation: Clinical notes in unstructured form contain biases, lack generalization, and are poorly interpretable, which challenges their use in predictive modeling.

Method: ClinStructor pipeline uses large language models to transform unstructured clinical text into organized, task-specific question-answer pairs before performing predictive modeling.

Result: Using ClinStructor, there was only a 2-3% reduction in AUC performance compared to direct fine-tuning on the ICU mortality prediction task while improving transparency and control.

Conclusion: ClinStructor is a significant step towards creating interpretable and generalizable machine learning models for clinical use, with minimal compromise on predictive performance.

Abstract: Clinical notes contain valuable, context-rich information, but their unstructured format introduces several challenges, including unintended biases (e.g., gender or racial bias), and poor generalization across clinical settings (e.g., models trained on one EHR system may perform poorly on another due to format differences) and poor interpretability. To address these issues, we present ClinStructor, a pipeline that leverages large language models (LLMs) to convert clinical free-text into structured, task-specific question-answer pairs prior to predictive modeling. Our method substantially enhances transparency and controllability and only leads to a modest reduction in predictive performance (a 2-3% drop in AUC), compared to direct fine-tuning, on the ICU mortality prediction task. ClinStructor lays a strong foundation for building reliable, interpretable, and generalizable machine learning models in clinical environments.

</details>


### [106] [Context-Emotion Aware Therapeutic Dialogue Generation: A Multi-component Reinforcement Learning Approach to Language Models for Mental Health Support](https://arxiv.org/abs/2511.11884)
*Eric Hua Qing Zhang,Julia Ive*

Main category: cs.CL

TL;DR: The paper explores enhancing GPT-2 with supervised fine-tuning and reinforcement learning for therapeutic dialogue generation, achieving improved contextual relevance, professionalism, and emotion accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the global mental health socioeconomic burden, worsened by COVID-19, by improving accessibility using advanced AI-driven therapeutic tools.

Method: Supervised fine-tuning and reinforcement learning on GPT-2 with a multi-component reward function to process contextual and emotional inputs.

Result: With reinforcement learning, emotion accuracy improved to 99.34% (baseline: 66.96%), along with better BLEU, ROUGE, and METEOR metrics over default GPT-2.

Conclusion: Reinforcement learning effectively enhances GPT-2 for therapeutic dialogue, offering potential as assistive tools with therapist oversight.

Abstract: Mental health illness represents a substantial global socioeconomic burden, with COVID-19 further exacerbating accessibility challenges and driving increased demand for telehealth mental health support. While large language models (LLMs) offer promising solutions through 24/7 availability and non-judgmental interactions, pre-trained models often lack the contextual and emotional awareness necessary for appropriate therapeutic responses. This paper investigated the application of supervised fine-tuning (SFT) and reinforcement learning (RL) techniques to enhance GPT-2's capacity for therapeutic dialogue generation. The methodology restructured input formats to enable simultaneous processing of contextual information and emotional states alongside user input, employing a multi-component reward function that aligned model outputs with professional therapist responses and annotated emotions. Results demonstrated improvements through reinforcement learning over baseline GPT-2 across multiple evaluation metrics: BLEU (0.0111), ROUGE-1 (0.1397), ROUGE-2 (0.0213), ROUGE-L (0.1317), and METEOR (0.0581). LLM evaluation confirmed high contextual relevance and professionalism, while reinforcement learning achieved 99.34% emotion accuracy compared to 66.96% for baseline GPT-2. These findings demonstrate reinforcement learning's effectiveness in developing therapeutic dialogue systems that can serve as valuable assistive tools for therapists while maintaining essential human clinical oversight.

</details>


### [107] [Additive Large Language Models for Semi-Structured Text](https://arxiv.org/abs/2511.11922)
*Karthikeyan K,Raghuveer Thirukovalluru,David Carlson*

Main category: cs.CL

TL;DR: The paper introduces CALM, an interpretable framework for clinical text classification using additive contributions from semantically meaningful components in the text.


<details>
  <summary>Details</summary>
Motivation: The study addresses the opacity of Large Language Models in clinical text classification, where clarity is critical for research and clinical adoption.

Method: CALM predicts outcomes using an additive approach where contributions from components of the text are directly incorporated into the prediction process, providing faithful explanations and clear visualizations.

Result: CALM achieves performance comparable to traditional LLM classifiers while offering improved interpretability and trustworthiness.

Conclusion: The proposed method maintains strong predictive performance while enhancing interpretability, quality-assurance, and the identification of clinically relevant patterns.

Abstract: Large Language Models have advanced clinical text classification, but their opaque predictions remain a critical barrier to practical adoption in research and clinical settings where investigators and physicians need to understand which parts of a patient's record drive risk signals. To address this challenge, we introduce \textbf{CALM}, short for \textbf{Classification with Additive Large Language Models}, an interpretable framework for semi-structured text where inputs are composed of semantically meaningful components, such as sections of an admission note or question-answer fields from an intake form. CALM predicts outcomes as the additive sum of each component's contribution, making these contributions part of the forward computation itself and enabling faithful explanations at both the patient and population level. The additive structure also enables clear visualizations, such as component-level risk curves similar to those used in generalized additive models, making the learned relationships easier to inspect and communicate. Although CALM expects semi-structured inputs, many clinical documents already have this form, and similar structure can often be automatically extracted from free-text notes. CALM achieves performance comparable to conventional LLM classifiers while improving trust, supporting quality-assurance checks, and revealing clinically meaningful patterns during model development and auditing.

</details>


### [108] [InData: Towards Secure Multi-Step, Tool-Based Data Analysis](https://arxiv.org/abs/2511.11933)
*Karthikeyan K,Raghuveer Thirukovalluru,Bhuwan Dhingra,David Edwin Carlson*

Main category: cs.CL

TL;DR: This paper proposes a secure alternative for LLM-based data analysis by restricting direct code execution and introducing a tool-based reasoning benchmark called InData.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address security risks in LLMs that access sensitive data through direct code generation, by introducing a controlled, tool-based approach.

Method: The paper designs a dataset called InData to evaluate LLMs' multi-step reasoning ability via verified tools, categorizing questions into Easy, Medium, and Hard for increasing complexity. It benchmarks 15 open-source LLMs using this dataset.

Result: Results show LLMs perform well on simple tasks but struggle with complex reasoning. For example, the best large model achieves 97.3% accuracy on Easy tasks but drops to 69.6% on Hard tasks.

Conclusion: The paper concludes that current LLMs lack robust multi-step tool-use reasoning and aims to enhance their development through the public release of InData and code.

Abstract: Large language model agents for data analysis typically generate and execute code directly on databases. However, when applied to sensitive data, this approach poses significant security risks. To address this issue, we propose a security-motivated alternative: restrict LLMs from direct code generation and data access, and require them to interact with data exclusively through a predefined set of secure, verified tools. Although recent tool-use benchmarks exist, they primarily target tool selection and simple execution rather than the compositional, multi-step reasoning needed for complex data analysis. To reduce this gap, we introduce Indirect Data Engagement (InData), a dataset designed to assess LLMs' multi-step tool-based reasoning ability. InData includes data analysis questions at three difficulty levels--Easy, Medium, and Hard--capturing increasing reasoning complexity. We benchmark 15 open-source LLMs on InData and find that while large models (e.g., gpt-oss-120b) achieve high accuracy on Easy tasks (97.3%), performance drops sharply on Hard tasks (69.6%). These results show that current LLMs still lack robust multi-step tool-based reasoning ability. With InData, we take a step toward enabling the development and evaluation of LLMs with stronger multi-step tool-use capabilities. We will publicly release the dataset and code.

</details>


### [109] [Improving LLM's Attachment to External Knowledge In Dialogue Generation Tasks Through Entity Anonymization](https://arxiv.org/abs/2511.11946)
*Hadi Sheikhi,Chenyang Huang,Osmar R. Zaïane*

Main category: cs.CL

TL;DR: The paper addresses the challenge of integrating external knowledge into conversational responses for dialogue generation and proposes techniques to improve knowledge usage by large language models.


<details>
  <summary>Details</summary>
Motivation: Large language models often favor internal knowledge over external knowledge, limiting their effectiveness in tasks requiring integration with external knowledge graphs.

Method: Introduced LLM-KAT to evaluate knowledge attachment and applied an entity anonymization technique to encourage better use of external knowledge.

Result: Experiments using the OpenDialKG dataset show improved attachment to external knowledge by large language models when using the proposed methods.

Conclusion: The study highlights the importance of enhancing external knowledge utilization in conversational models and presents effective methods to address this challenge.

Abstract: Knowledge graph-based dialogue generation (KG-DG) is a challenging task requiring models to effectively incorporate external knowledge into conversational responses. While large language models (LLMs) have achieved impressive results across various NLP tasks, their ability to utilize external knowledge in KG-DG remains under-explored. We observe that LLMs often rely on internal knowledge, leading to detachment from provided knowledge graphs, even when they are given a flawlessly retrieved knowledge graph. First, we introduce LLM-KAT, an evaluation procedure for measuring knowledge attachment in generated responses. Second, we propose a simple yet effective entity anonymization technique to encourage LLMs to better leverage external knowledge. Experiments on the OpenDialKG dataset demonstrate that our approach improves LLMs' attachment on external knowledge.

</details>


### [110] [On the Entropy Calibration of Language Models](https://arxiv.org/abs/2511.11966)
*Steven Cao,Gregory Valiant,Percy Liang*

Main category: cs.CL

TL;DR: Entropy calibration in language models shows limited improvement in mitigating error accumulation with increased model scale, leading to a tradeoff between text diversity and quality.


<details>
  <summary>Details</summary>
Motivation: To understand entropy calibration in language models and determine if miscalibration improves with scale or can be resolved without sacrificing quality or diversity.

Method: The paper studies theoretical scaling behaviors using a simplified model, then empirically measures miscalibration across models with varying parameter sizes. A theoretical proof is provided for potential entropy reduction with log-loss preservation.

Result: Empirical results show that scaling doesn't significantly reduce miscalibration, as larger models accumulate errors at a similar rate to smaller ones. Truncation sacrifices log loss to improve text quality.

Conclusion: Addressing entropy calibration without quality tradeoffs is challenging, but theoretical solutions might exist by predicting future entropy using advanced modeling techniques.

Abstract: We study the problem of entropy calibration, which asks whether a language model's entropy over generations matches its log loss on human text. Past work found that models are miscalibrated, with entropy per step increasing (and text quality decreasing) as generations grow longer. This error accumulation is a fundamental problem in autoregressive models, and the standard solution is to truncate the distribution, which improves text quality at the cost of diversity. In this paper, we ask: is miscalibration likely to improve with scale, and is it theoretically possible to calibrate without tradeoffs? To build intuition, we first study a simplified theoretical setting to characterize the scaling behavior of miscalibration with respect to dataset size. We find that the scaling behavior depends on the power law exponent of the data distribution -- in particular, for a power law exponent close to 1, the scaling exponent is close to 0, meaning that miscalibration improves very slowly with scale. Next, we measure miscalibration empirically in language models ranging from 0.5B to 70B parameters. We find that the observed scaling behavior is similar to what is predicted by the simplified setting: our fitted scaling exponents for text are close to 0, meaning that larger models accumulate error at a similar rate as smaller ones. This scaling (or, lack thereof) provides one explanation for why we sample from larger models with similar amounts of truncation as smaller models, even though the larger models are of higher quality. However, truncation is not a satisfying solution because it comes at the cost of increased log loss. In theory, is it even possible to reduce entropy while preserving log loss? We prove that it is possible, if we assume access to a black box which can fit models to predict the future entropy of text.

</details>


### [111] [A Reasoning Paradigm for Named Entity Recognition](https://arxiv.org/abs/2511.11978)
*Hui Huang,Yanping Chen,Ruizhang Huang,Chuan Lin,Yongbin Qin*

Main category: cs.CL

TL;DR: This paper introduces a reasoning framework for Named Entity Recognition (NER) that focuses on explicit reasoning instead of implicit pattern matching, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Current generative LLMs excel at semantic pattern matching for NER but lack explicit reasoning capability, leading to poor generalization, especially in zero-shot and low-resource setups.

Method: The authors proposed a three-stage reasoning framework: (1) Chain of Thought (CoT) generation to create reasoning chains, (2) CoT tuning to generate rationales before answers, and (3) a reasoning enhancement stage using a reward signal to validate and improve reasoning.

Result: The proposed ReasoningNER framework achieves state-of-the-art performance in NER tasks, outperforming GPT-4 by 12.3% in zero-shot settings, showing superior reasoning and extraction capabilities.

Conclusion: ReasoningNER significantly improves NER performance by integrating explicit reasoning, advancing research in reasoning-oriented information extraction. The framework enhances generalization especially in challenging contexts like zero-shot setups.

Abstract: Generative LLMs typically improve Named Entity Recognition (NER) performance through instruction tuning. They excel at generating entities by semantic pattern matching but lack an explicit, verifiable reasoning mechanism. This "cognitive shortcutting" leads to suboptimal performance and brittle generalization, especially in zero-shot and lowresource scenarios where reasoning from limited contextual cues is crucial. To address this issue, a reasoning framework is proposed for NER, which shifts the extraction paradigm from implicit pattern matching to explicit reasoning. This framework consists of three stages: Chain of Thought (CoT) generation, CoT tuning, and reasoning enhancement. First, a dataset annotated with NER-oriented CoTs is generated, which contain task-relevant reasoning chains. Then, they are used to tune the NER model to generate coherent rationales before deriving the final answer. Finally, a reasoning enhancement stage is implemented to optimize the reasoning process using a comprehensive reward signal. This stage ensures explicit and verifiable extractions. Experiments show that ReasoningNER demonstrates impressive cognitive ability in the NER task, achieving competitive performance. In zero-shot settings, it achieves state-of-the-art (SOTA) performance, outperforming GPT-4 by 12.3 percentage points on the F1 score. Analytical results also demonstrate its great potential to advance research in reasoningoriented information extraction. Our codes are available at https://github.com/HuiResearch/ReasoningIE.

</details>


### [112] [Critical or Compliant? The Double-Edged Sword of Reasoning in Chain-of-Thought Explanations](https://arxiv.org/abs/2511.12001)
*Eunkyu Park,Wesley Hanwen Deng,Vasudha Varadarajan,Mingxi Yan,Gunhee Kim,Maarten Sap,Motahhare Eslami*

Main category: cs.CL

TL;DR: The paper examines how Chain-of-Thought (CoT) explanations in vision-language models impacts user trust, error detection, and reasoning accuracy in moral scenarios. 


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the dual role of explanations, which can provide transparency or foster confirmation bias, leading to misinterpretation of model reasoning in application use.

Method: The research perturbs reasoning chains and manipulates delivery tones of vision-language models to study systematic errors, user trust, and error-detection ability in multimodal moral contexts.

Result: Findings indicate (1) users align trust with outcome agreement, despite flawed reasoning, and (2) confident tones diminish error detection while fostering reliance.

Conclusion: The study emphasizes the need for NLP systems offering explanations to promote scrutiny and critical thinking instead of reliance on potentially faulty reasoning chains.

Abstract: Explanations are often promoted as tools for transparency, but they can also foster confirmation bias; users may assume reasoning is correct whenever outputs appear acceptable. We study this double-edged role of Chain-of-Thought (CoT) explanations in multimodal moral scenarios by systematically perturbing reasoning chains and manipulating delivery tones. Specifically, we analyze reasoning errors in vision language models (VLMs) and how they impact user trust and the ability to detect errors. Our findings reveal two key effects: (1) users often equate trust with outcome agreement, sustaining reliance even when reasoning is flawed, and (2) the confident tone suppresses error detection while maintaining reliance, showing that delivery styles can override correctness. These results highlight how CoT explanations can simultaneously clarify and mislead, underscoring the need for NLP systems to provide explanations that encourage scrutiny and critical thinking rather than blind trust. All code will be released publicly.

</details>


### [113] [CURE: Cultural Understanding and Reasoning Evaluation - A Framework for "Thick" Culture Alignment Evaluation in LLMs](https://arxiv.org/abs/2511.12014)
*Truong Vo,Sanmi Koyejo*

Main category: cs.CL

TL;DR: The paper introduces benchmarks for assessing cultural competence in large language models using realistic situational contexts and new metrics to improve evaluation accuracy.


<details>
  <summary>Details</summary>
Motivation: There is a lack of robust evaluation methods for measuring cultural competence in large language models, especially those deployed in diverse environments.

Method: The authors propose benchmarks based on realistic situational contexts requiring culturally grounded reasoning and introduce new metrics (Coverage, Specificity, Connotation, and Coherence) beyond the Exact Match metric.

Result: Empirical analysis shows that existing evaluation approaches overestimate cultural competence and have high variance, while the proposed benchmarks provide deeper insights and stable assessments.

Conclusion: Thick evaluation using the new benchmarks and metrics is more effective in capturing cultural understanding than traditional methods, offering better reliability and interpretability.

Abstract: Large language models (LLMs) are increasingly deployed in culturally diverse environments, yet existing evaluations of cultural competence remain limited. Existing methods focus on de-contextualized correctness or forced-choice judgments, overlooking the need for cultural understanding and reasoning required for appropriate responses. To address this gap, we introduce a set of benchmarks that, instead of directly probing abstract norms or isolated statements, present models with realistic situational contexts that require culturally grounded reasoning. In addition to the standard Exact Match metric, we introduce four complementary metrics (Coverage, Specificity, Connotation, and Coherence) to capture different dimensions of model's response quality. Empirical analysis across frontier models reveals that thin evaluation systematically overestimates cultural competence and produces unstable assessments with high variance. In contrast, thick evaluation exposes differences in reasoning depth, reduces variance, and provides more stable, interpretable signals of cultural understanding.

</details>


### [114] [Exploring Parameter-Efficient Fine-Tuning and Backtranslation for the WMT 25 General Translation Task](https://arxiv.org/abs/2511.12109)
*Felipe Fujita,Hideyuki Takada*

Main category: cs.CL

TL;DR: This study investigates combining fine-tuning and backtranslation techniques on small Japanese corpora to improve neural machine translation quality, achieving significant performance gains.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance translation accuracy for low-resource language pairs like English-to-Japanese using effective methods on limited data.

Method: The paper employs backtranslation with synthetic data and fine-tuning on diverse parallel corpora, then integrates both methods for better translation results.

Result: Backtranslation slightly improves COMET from 0.460 to 0.468, fine-tuning achieves a major increase to 0.589, and integrating both yields a further improvement to COMET = 0.597.

Conclusion: Combining backtranslation and fine-tuning provides an effective approach to improve translation accuracy for low-resource languages, with significant synergy between techniques.

Abstract: In this paper, we explore the effectiveness of combining fine-tuning and backtranslation on a small Japanese corpus for neural machine translation. Starting from a baseline English{\textrightarrow}Japanese model (COMET = 0.460), we first apply backtranslation (BT) using synthetic data generated from monolingual Japanese corpora, yielding a modest increase (COMET = 0.468). Next, we fine-tune (FT) the model on a genuine small parallel dataset drawn from diverse Japanese news and literary corpora, achieving a substantial jump to COMET = 0.589 when using Mistral 7B. Finally, we integrate both backtranslation and fine-tuning{ -- }first augmenting the small dataset with BT generated examples, then adapting via FT{ -- }which further boosts performance to COMET = 0.597. These results demonstrate that, even with limited training data, the synergistic use of backtranslation and targeted fine-tuning on Japanese corpora can significantly enhance translation quality, outperforming each technique in isolation. This approach offers a lightweight yet powerful strategy for improving low-resource language pairs.

</details>


### [115] [LLMLagBench: Identifying Temporal Training Boundaries in Large Language Models](https://arxiv.org/abs/2511.12116)
*Piotr Pęzik,Konrad Kaczyński,Maria Szymańska,Filip Żarnecki,Zuzanna Deckert,Jakub Kwiatkowski,Wojciech Janowski*

Main category: cs.CL

TL;DR: This paper introduces LLMLagBench, a benchmark to identify the latest temporal boundaries of LLMs' training data by testing knowledge of recent events.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the limitations of LLMs, particularly their inability to provide accurate information beyond their training data's temporal cutoff, which may compromise the reliability of time-sensitive tasks.

Method: The authors propose LLMLagBench, a benchmarking system designed to systematically identify temporal cutoffs of LLMs by evaluating their knowledge of recent events. They test this on multiple models.

Result: LLMLagBench successfully identifies the training data cutoffs of different LLMs, with results validated manually and compared against publicly available LLM pretraining details.

Conclusion: LLMLagBench is a reliable tool for evaluating the temporal freshness of LLMs, highlighting their limitations in keeping up with recent information.

Abstract: Large Language Models (LLMs) are pretrained on textual data up to a specific temporal cutoff. This creates a strict knowledge boundary beyond which models cannot provide accurate information without querying external sources. More subtly, when this limitation is unknown or ignored, LLMs may inadvertently blend outdated time-sensitive information with general knowledge during reasoning tasks, potentially compromising response accuracy. We introduce LLMLagBench, an LLM freshness benchmark, as a systematic approach for identifying the earliest probable temporal boundaries of an LLM's training data by evaluating its knowledge of recent events. We then apply this benchmark to evaluate a large set of LLMs, including models with both explicitly declared and undeclared training cutoffs. The reliability of the benchmark is assessed by manual validation and comparison with publicly released information about LLM pretraining.

</details>


### [116] [PRISM of Opinions: A Persona-Reasoned Multimodal Framework for User-centric Conversational Stance Detection](https://arxiv.org/abs/2511.12130)
*Bingbing Wang,Zhixin Bai,Zhengda Jin,Zihan Wang,Xintong Song,Jingjie Lin,Sixuan Li,Jing Li,Ruifeng Xu*

Main category: cs.CL

TL;DR: The paper addresses limitations in Multimodal Conversational Stance Detection by introducing a user-centric dataset (U-MStance) and proposing a persona-reasoned multimodal stance model (PRISM), which achieves improved performance.


<details>
  <summary>Details</summary>
Motivation: To improve stance detection research by addressing issues in pseudo-multimodality and uniform user treatment, ensuring more realistic multimodal and user-specific interactions.

Method: The U-MStance dataset is created, capturing user-centric stance data. PRISM utilizes historical user data to form personas and aligns multimodal cues in conversations using Chain-of-Thought reasoning alongside a mutual reinforcement mechanism for joint optimization.

Result: PRISM significantly outperforms existing baselines in stance detection tasks on the U-MStance dataset.

Conclusion: The study highlights the importance of user-centric and context-aware multimodal reasoning, contributing significantly to realistic stance detection research.

Abstract: The rapid proliferation of multimodal social media content has driven research in Multimodal Conversational Stance Detection (MCSD), which aims to interpret users' attitudes toward specific targets within complex discussions. However, existing studies remain limited by: **1) pseudo-multimodality**, where visual cues appear only in source posts while comments are treated as text-only, misaligning with real-world multimodal interactions; and **2) user homogeneity**, where diverse users are treated uniformly, neglecting personal traits that shape stance expression. To address these issues, we introduce **U-MStance**, the first user-centric MCSD dataset, containing over 40k annotated comments across six real-world targets. We further propose **PRISM**, a **P**ersona-**R**easoned mult**I**modal **S**tance **M**odel for MCSD. PRISM first derives longitudinal user personas from historical posts and comments to capture individual traits, then aligns textual and visual cues within conversational context via Chain-of-Thought to bridge semantic and pragmatic gaps across modalities. Finally, a mutual task reinforcement mechanism is employed to jointly optimize stance detection and stance-aware response generation for bidirectional knowledge transfer. Experiments on U-MStance demonstrate that PRISM yields significant gains over strong baselines, underscoring the effectiveness of user-centric and context-grounded multimodal reasoning for realistic stance understanding.

</details>


### [117] [AI-Salesman: Towards Reliable Large Language Model Driven Telemarketing](https://arxiv.org/abs/2511.12133)
*Qingyu Zhang,Chunlei Xin,Xuanang Chen,Yaojie Lu,Hongyu Lin,Xianpei Han,Le Sun,Qing Ye,Qianlong Xie,Xingxing Wang*

Main category: cs.CL

TL;DR: This paper introduces AI-Salesman, a novel framework addressing challenges in goal-driven persuasive dialogues with real-world applications like telemarketing, focusing on strategy and factual accuracy.


<details>
  <summary>Details</summary>
Motivation: The study aims to overcome the challenges of multi-turn planning, factual faithfulness, and the lack of task-specific data in persuasive dialogues by leveraging advanced strategies and resources.

Method: The authors constructed a new dialogue dataset, TeleSalesCorpus, and built a dual-stage AI framework using Bayesian-supervised reinforcement learning for training and a dynamic, script-guided agent (DOGA) for inference.

Result: AI-Salesman outperformed baseline models in fine-grained sales metrics and human evaluations, proving effective in complex and persuasive dialogue scenarios.

Conclusion: By combining robust learning strategies and dynamic guidance, AI-Salesman demonstrates improved performance in persuasive dialogues, setting a promising direction for goal-driven conversational AI.

Abstract: Goal-driven persuasive dialogue, exemplified by applications like telemarketing, requires sophisticated multi-turn planning and strict factual faithfulness, which remains a significant challenge for even state-of-the-art Large Language Models (LLMs). A lack of task-specific data often limits previous works, and direct LLM application suffers from strategic brittleness and factual hallucination. In this paper, we first construct and release TeleSalesCorpus, the first real-world-grounded dialogue dataset for this domain. We then propose AI-Salesman, a novel framework featuring a dual-stage architecture. For the training stage, we design a Bayesian-supervised reinforcement learning algorithm that learns robust sales strategies from noisy dialogues. For the inference stage, we introduce the Dynamic Outline-Guided Agent (DOGA), which leverages a pre-built script library to provide dynamic, turn-by-turn strategic guidance. Moreover, we design a comprehensive evaluation framework that combines fine-grained metrics for key sales skills with the LLM-as-a-Judge paradigm. Experimental results demonstrate that our proposed AI-Salesman significantly outperforms baseline models in both automatic metrics and comprehensive human evaluations, showcasing its effectiveness in complex persuasive scenarios.

</details>


### [118] [Seeing is Believing: Rich-Context Hallucination Detection for MLLMs via Backward Visual Grounding](https://arxiv.org/abs/2511.12140)
*Pinxue Guo,Chongruo Wu,Xinyu Zhou,Lingyi Hong,Zhaoyu Chen,Jinglun Li,Kaixun Jiang,Sen-ching Samson Cheung,Wei Zhang,Wenqiang Zhang*

Main category: cs.CL

TL;DR: The paper introduces VBackChecker, a framework for detecting hallucinations in multimodal large language models (MLLMs) through consistency checks using a pixel-level grounding model.


<details>
  <summary>Details</summary>
Motivation: Address the prevalent issue of hallucinations in multimodal language models (MLLMs) to improve their reliability for practical applications.

Method: Develop a reference-free framework (VBackChecker) leveraging a pixel-level Grounding LLM for consistency verification. Created an instruction-tuning dataset (R-Instruct) and hallucination benchmark (R^2-HalBench) for evaluation.

Result: VBackChecker achieved state-of-the-art performance on the R^2-HalBench benchmark and exceeded prior methods in pixel-level grounding tasks by over 10%.

Conclusion: VBackChecker provides a reliable, interpretable, and advanced solution to hallucination detection in MLLMs, setting a new standard in the field.

Abstract: Multimodal Large Language Models (MLLMs) have unlocked powerful cross-modal capabilities, but still significantly suffer from hallucinations. As such, accurate detection of hallucinations in MLLMs is imperative for ensuring their reliability in practical applications. To this end, guided by the principle of "Seeing is Believing", we introduce VBackChecker, a novel reference-free hallucination detection framework that verifies the consistency of MLLMgenerated responses with visual inputs, by leveraging a pixellevel Grounding LLM equipped with reasoning and referring segmentation capabilities. This reference-free framework not only effectively handles rich-context scenarios, but also offers interpretability. To facilitate this, an innovative pipeline is accordingly designed for generating instruction-tuning data (R-Instruct), featuring rich-context descriptions, grounding masks, and hard negative samples. We further establish R^2 -HalBench, a new hallucination benchmark for MLLMs, which, unlike previous benchmarks, encompasses real-world, rich-context descriptions from 18 MLLMs with high-quality annotations, spanning diverse object-, attribute, and relationship-level details. VBackChecker outperforms prior complex frameworks and achieves state-of-the-art performance on R^2 -HalBench, even rivaling GPT-4o's capabilities in hallucination detection. It also surpasses prior methods in the pixel-level grounding task, achieving over a 10% improvement. All codes, data, and models are available at https://github.com/PinxueGuo/VBackChecker.

</details>


### [119] [CriticSearch: Fine-Grained Credit Assignment for Search Agents via a Retrospective Critic](https://arxiv.org/abs/2511.12159)
*Yaocheng Zhang,Haohuan Huang,Zijun Song,Yuanheng Zhu,Qichao Zhang,Zijie Zhao,Dongbin Zhao*

Main category: cs.CL

TL;DR: CriticSearch introduces a framework for training search agents in language models using detailed feedback for improved multi-hop reasoning tasks, outperforming traditional reinforcement learning methods.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies and instability in training large language models for complex question-answering tasks due to sparse rewards in existing reinforcement learning pipelines.

Method: CriticSearch uses a retrospective critic mechanism providing dense, fine-grained feedback at each turn of the search process. A frozen critique LLM evaluates these turns based on privileged information like full trajectory and gold answers, offering structured rewards for training improvement.

Result: CriticSearch led to faster convergence, better training stability, and improved performance across multi-hop reasoning benchmarks compared to existing methods.

Conclusion: Using dense, turn-level retrospective feedback from a critique LLM enables more efficient and effective reinforcement learning for search agents, enhancing adaptability and generalization in complex tasks.

Abstract: Tool-Integrated Reasoning (TIR) with search engines enables large language models to iteratively retrieve up-to-date external knowledge, enhancing adaptability and generalization in complex question-answering tasks. However, existing search agent pipelines typically depend on reinforcement learning based optimization, which often suffers from sparse outcome rewards, leading to inefficient exploration and unstable training. We introduce CriticSearch, a fine-grained credit-assignment framework that supplies dense, turn-level feedback via a retrospective critic mechanism. During training, a frozen, asymmetric critique LLM retrospectively evaluates each turn using privileged information from the full trajectory and gold answers, converting these assessments into stable, dense rewards that guide policy improvement. Experimental results across diverse multi-hop reasoning benchmarks demonstrate that CriticSearch consistently outperforms existing baselines, achieving faster convergence, improved training stability, and higher performance.

</details>


### [120] [MME-RAG: Multi-Manager-Expert Retrieval-Augmented Generation for Fine-Grained Entity Recognition in Task-Oriented Dialogues](https://arxiv.org/abs/2511.12213)
*Liang Xue,Haoyu Liu,Yajun Tian,Xinyu Zhong,Yang Liu*

Main category: cs.CL

TL;DR: MME-RAG is a new framework addressing fine-grained entity recognition challenges by using a multi-stage retrieval-augmented generation approach, achieving better performance across various domains.


<details>
  <summary>Details</summary>
Motivation: The paper focuses on improving fine-grained entity recognition in task-oriented dialogues, aiming to address challenges in domain adaptation and retrieval controllability faced by current large language models.

Method: MME-RAG uses a two-stage approach: managers handle type-level judgments, experts handle span-level extractions, and KeyInfo supports the experts by injecting semantically aligned exemplars during inference.

Result: Experimental evaluation across various datasets demonstrates superior performance of MME-RAG over recent baselines, highlighting its robustness and adaptability.

Conclusion: MME-RAG is a scalable and interpretable framework that improves cross-domain entity recognition for adaptive dialogue understanding through hierarchical decomposition and effective retrieval.

Abstract: Fine-grained entity recognition is crucial for reasoning and decision-making in task-oriented dialogues, yet current large language models (LLMs) continue to face challenges in domain adaptation and retrieval controllability. We introduce MME-RAG, a Multi-Manager-Expert Retrieval-Augmented Generation framework that decomposes entity recognition into two coordinated stages: type-level judgment by lightweight managers and span-level extraction by specialized experts. Each expert is supported by a KeyInfo retriever that injects semantically aligned, few-shot exemplars during inference, enabling precise and domain-adaptive extraction without additional training. Experiments on CrossNER, MIT-Movie, MIT-Restaurant, and our newly constructed multi-domain customer-service dataset demonstrate that MME-RAG performs better than recent baselines in most domains. Ablation studies further show that both the hierarchical decomposition and KeyInfo-guided retrieval are key drivers of robustness and cross-domain generalization, establishing MME-RAG as a scalable and interpretable solution for adaptive dialogue understanding.

</details>


### [121] [Consistency Is the Key: Detecting Hallucinations in LLM Generated Text By Checking Inconsistencies About Key Facts](https://arxiv.org/abs/2511.12236)
*Raavi Gupta,Pranav Hari Panicker,Sumit Bhatia,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: CONFACTCHECK is introduced as a method for efficiently detecting hallucinations in text generated by LLMs, without requiring external knowledge bases, and performs better than existing methods in terms of accuracy and resource efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of hallucinations in text generated by LLMs, especially in high-stakes domains, where current methods are costly and resource-intensive.

Method: CONFACTCHECK ensures factual consistency by checking responses within a single or across multiple LLMs without relying on external knowledge bases, minimizing the need for multiple API calls.

Result: The proposed method achieves higher accuracy in hallucination detection compared to existing baselines, with reduced resource utilization, as indicated by evaluations on multiple datasets.

Conclusion: CONFACTCHECK provides an efficient and cost-effective way to detect hallucinations in LLM outputs while maintaining accuracy, making it a valuable tool in restricted-access LLM settings.

Abstract: Large language models (LLMs), despite their remarkable text generation capabilities, often hallucinate and generate text that is factually incorrect and not grounded in real-world knowledge. This poses serious risks in domains like healthcare, finance, and customer support. A typical way to use LLMs is via the APIs provided by LLM vendors where there is no access to model weights or options to fine-tune the model. Existing methods to detect hallucinations in such settings where the model access is restricted or constrained by resources typically require making multiple LLM API calls, increasing latency and API cost. We introduce CONFACTCHECK, an efficient hallucination detection approach that does not leverage any external knowledge base and works on the simple intuition that responses to factual probes within the generated text should be consistent within a single LLM and across different LLMs. Rigorous empirical evaluation on multiple datasets that cover both the generation of factual texts and the open generation shows that CONFACTCHECK can detect hallucinated facts efficiently using fewer resources and achieves higher accuracy scores compared to existing baselines that operate under similar conditions. Our code is available here.

</details>


### [122] [ViConBERT: Context-Gloss Aligned Vietnamese Word Embedding for Polysemous and Sense-Aware Representations](https://arxiv.org/abs/2511.12249)
*Khang T. Huynh,Dung H. Nguyen,Binh T. Nguyen*

Main category: cs.CL

TL;DR: This paper introduces ViConBERT, a Vietnamese contextual embedding model utilizing contrastive learning and gloss-based distillation, and ViConWSD, a dataset for evaluating Vietnamese semantic understanding.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of robust models and evaluation resources for fine-grained semantic tasks in Vietnamese, which are available for high-resource languages like English.

Method: The authors combined contrastive learning (SimCLR) and gloss-based distillation to develop ViConBERT, along with creating ViConWSD, a large-scale synthetic evaluation dataset for Vietnamese semantic understanding.

Result: ViConBERT outperforms existing baselines on WSD with an F1 score of 0.87, achieves an Average Precision (AP) of 0.88 on ViCon, and Spearman's rho of 0.60 on ViSim-400.

Conclusion: ViConBERT and ViConWSD successfully enhance Vietnamese semantic understanding, with the framework demonstrating effectiveness in both discrete sense and graded semantic relation modeling.

Abstract: Recent advances in contextualized word embeddings have greatly improved semantic tasks such as Word Sense Disambiguation (WSD) and contextual similarity, but most progress has been limited to high-resource languages like English. Vietnamese, in contrast, still lacks robust models and evaluation resources for fine-grained semantic understanding. In this paper, we present ViConBERT, a novel framework for learning Vietnamese contextualized embeddings that integrates contrastive learning (SimCLR) and gloss-based distillation to better capture word meaning. We also introduce ViConWSD, the first large-scale synthetic dataset for evaluating semantic understanding in Vietnamese, covering both WSD and contextual similarity. Experimental results show that ViConBERT outperforms strong baselines on WSD (F1 = 0.87) and achieves competitive performance on ViCon (AP = 0.88) and ViSim-400 (Spearman's rho = 0.60), demonstrating its effectiveness in modeling both discrete senses and graded semantic relations. Our code, models, and data are available at https://github.com/tkhangg0910/ViConBERT

</details>


### [123] [Cmprsr: Abstractive Token-Level Question-Agnostic Prompt Compressor](https://arxiv.org/abs/2511.12281)
*Ivan Zakazov,Alexander Sharipov,Berke Argin,Oussama Gabouj,Kamel Charaf,Alexi Semiz,Lorenzo Drudi,Nicolas Baldwin,Robert West*

Main category: cs.CL

TL;DR: The paper proposes a cost-effective method for using smaller LLMs to compress inputs for larger LLMs, introducing benchmarks and optimization techniques to improve performance.


<details>
  <summary>Details</summary>
Motivation: The need to address high computational costs of using black-box Large Language Models (LLMs).

Method: The authors implement a framework using smaller LLMs for input compression, benchmarking 25 models, and enhancing performance with meta-prompt optimization and a novel model training method called Cmprsr.

Result: Cmprsr outperforms other models in various compression tasks and input domains while adhering to desired compression rates and maintaining semantic quality.

Conclusion: The proposed approach effectively reduces costs and offers fine control over compression and quality trade-offs with improved model generalizability.

Abstract: Motivated by the high costs of using black-box Large Language Models (LLMs), we introduce a novel prompt compression paradigm, under which we use smaller LLMs to compress inputs for the larger ones. We present the first comprehensive LLM-as-a-compressor benchmark spanning 25 open- and closed-source models, which reveals significant disparity in models' compression ability in terms of (i) preserving semantically important information (ii) following the user-provided compression rate (CR). We further improve the performance of gpt-4.1-mini, the best overall vanilla compressor, with Textgrad-based compression meta-prompt optimization. We also identify the most promising open-source vanilla LLM - Qwen3-4B - and post-train it with a combination of supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO), pursuing the dual objective of CR adherence and maximizing the downstream task performance. We call the resulting model Cmprsr and demonstrate its superiority over both extractive and vanilla abstractive compression across the entire range of compression rates on lengthy inputs from MeetingBank and LongBench as well as short prompts from GSM8k. The latter highlights Cmprsr's generalizability across varying input lengths and domains. Moreover, Cmprsr closely follows the requested compression rate, offering fine control over the cost-quality trade-off.

</details>


### [124] [AugAbEx : Way Forward for Extractive Case Summarization](https://arxiv.org/abs/2511.12290)
*Purnima Bindal,Vikas Kumar,Sagar Rathore,Vasudha Bhatnagar*

Main category: cs.CL

TL;DR: This paper introduces a method to enhance legal case summarization datasets by creating extractive summaries from existing abstractive ones, addressing challenges in automating legal document summarization.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the cognitive burden on legal practitioners caused by complex, lengthy judgments and the limitations of current neural abstractive summarizers in accurately capturing nuanced legal language.

Method: The authors develop a transparent pipeline to transform existing abstractive gold standard summaries into extractive ones, ensuring expert insights are preserved. They further analyze the quality of the new extractive summaries on multiple dimensions.

Result: They successfully augment seven legal datasets, creating extractive summaries that align structurally, semantically, and lexically with the abstractive versions, thus providing a reliable resource for research.

Conclusion: The study contributes by delivering enriched datasets with extractive summaries, anticipating progress in automating legal document summarization. These datasets are released for the broader research community.

Abstract: Summarization of legal judgments poses a heavy cognitive burden on law practitioners due to the complexity of the language, context-sensitive legal jargon, and the length of the document. Therefore, the automatic summarization of legal documents has attracted serious attention from natural language processing researchers. Since the abstractive summaries of legal documents generated by deep neural methods remain prone to the risk of misrepresenting nuanced legal jargon or overlooking key contextual details, we envisage a rising trend toward the use of extractive case summarizers.
  Given the high cost of human annotation for gold standard extractive summaries, we engineer a light and transparent pipeline that leverages existing abstractive gold standard summaries to create the corresponding extractive gold standard versions. The approach ensures that the experts` opinions ensconced in the original gold standard abstractive summaries are carried over to the transformed extractive summaries. We aim to augment seven existing case summarization datasets, which include abstractive summaries, by incorporating corresponding extractive summaries and create an enriched data resource for case summarization research community. To ensure the quality of the augmented extractive summaries, we perform an extensive comparative evaluation with the original abstractive gold standard summaries covering structural, lexical, and semantic dimensions. We also compare the domain-level information of the two summaries. We commit to release the augmented datasets in the public domain for use by the research community and believe that the resource will offer opportunities to advance the field of automatic summarization of legal documents.

</details>


### [125] [Do LLMs and Humans Find the Same Questions Difficult? A Case Study on Japanese Quiz Answering](https://arxiv.org/abs/2511.12300)
*Naoya Sugiura,Kosuke Yamada,Yasuhiro Ogawa,Katsuhiko Toyama,Ryohei Sasano*

Main category: cs.CL

TL;DR: This paper examines LLMs' performance on quizzes compared to humans, finding challenges in non-Wikipedia-based and numerical questions.


<details>
  <summary>Details</summary>
Motivation: Explore whether LLMs share similar difficulties with humans in quiz-solving tasks, particularly focusing on Japanese quizzes.

Method: Japanese quiz data was collected including human response rates, and LLMs were prompted to answer under various settings to compare performance.

Result: LLMs struggled with quizzes whose answers aren't in Wikipedia and found numerical questions particularly difficult compared to humans.

Conclusion: LLMs and humans face different challenges in quiz-solving; LLM improvements may focus on areas outside Wikipedia and numerical reasoning.

Abstract: LLMs have achieved performance that surpasses humans in many NLP tasks. However, it remains unclear whether problems that are difficult for humans are also difficult for LLMs. This study investigates how the difficulty of quizzes in a buzzer setting differs between LLMs and humans. Specifically, we first collect Japanese quiz data including questions, answers, and correct response rate of humans, then prompted LLMs to answer the quizzes under several settings, and compare their correct answer rate to that of humans from two analytical perspectives. The experimental results showed that, compared to humans, LLMs struggle more with quizzes whose correct answers are not covered by Wikipedia entries, and also have difficulty with questions that require numerical answers.

</details>


### [126] [Don't Think of the White Bear: Ironic Negation in Transformer Models Under Cognitive Load](https://arxiv.org/abs/2511.12381)
*Logan Mann,Nayan Saxena,Sarah Tandon,Chenhao Sun,Savar Toteja,Kevin Zhu*

Main category: cs.CL

TL;DR: Negation instructions in LLMs can increase the activation of suppressed concepts instead of avoiding them, known as ironic rebound.


<details>
  <summary>Details</summary>
Motivation: To investigate the effect of negation instructions on large language models and their tendency to cause ironic rebound.

Method: Two experiments were conducted: varying distractor text after negation to measure rebound intensity, and testing whether polarity separation affects rebound persistence. A circuit tracing analysis complemented the findings.

Result: Results indicated immediate rebound after negation, intensifying with semantic distractors and longer interference, while repetition aids suppression. Stronger polarity separation correlated with persistent rebound. Mechanistic insights were provided.

Conclusion: The study links cognitive predictions of ironic rebound with mechanistic insights in LLMs, presents findings on rebound persistence, and introduces the ReboundBench dataset for further research.

Abstract: Negation instructions such as 'do not mention $X$' can paradoxically increase the accessibility of $X$ in human thought, a phenomenon known as ironic rebound. Large language models (LLMs) face the same challenge: suppressing a concept requires internally activating it, which may prime rebound instead of avoidance. We investigated this tension with two experiments. \textbf{(1) Load \& content}: after a negation instruction, we vary distractor text (semantic, syntactic, repetition) and measure rebound strength. \textbf{(2) Polarity separation}: We test whether models distinguish neutral from negative framings of the same concept and whether this separation predicts rebound persistence. Results show that rebound consistently arises immediately after negation and intensifies with longer or semantic distractors, while repetition supports suppression. Stronger polarity separation correlates with more persistent rebound. Together, these findings, complemented by a circuit tracing analysis that identifies sparse middle-layer attention heads amplifying forbidden tokens while early layers suppress, link cognitive predictions of ironic rebound with mechanistic insights into long-context interference. To support future work, we release ReboundBench, a dataset of $5,000$ systematically varied negation prompts designed to probe rebound in LLMs.

</details>


### [127] [From Phonemes to Meaning: Evaluating Large Language Models on Tamil](https://arxiv.org/abs/2511.12387)
*Jeyarajalingam Varsha,Menan Velayuthan,Sumirtha Karunakaran,Rasan Nivethiga,Kengatharaiyer Sarveswaran*

Main category: cs.CL

TL;DR: The paper introduces ILAKKANAM, a Tamil-specific linguistic evaluation benchmark, and tests LLMs' linguistic competence on it, finding significant gaps in handling complex linguistic structures.


<details>
  <summary>Details</summary>
Motivation: To explore and evaluate the linguistic competence of LLMs in low-resource, morphologically rich languages like Tamil, where existing benchmarks fall short due to reliance on translated datasets.

Method: The authors curated ILAKKANAM, a benchmark of 820 Tamil questions from Sri Lankan school examinations, categorized and annotated by linguists, evaluating LLMs using these questions across linguistic and knowledge categories.

Result: Gemini 2.5 showed the best performance, while open-source models performed poorly. All models struggled as linguistic complexity increased, and there was no strong link between overall performance and understanding linguistic categories.

Conclusion: LLMs still face significant challenges in low-resource and morphologically rich languages, with their performance influenced more by data exposure than true linguistic comprehension.

Abstract: Large Language Models (LLMs) have shown strong generalization across tasks in high-resource languages; however, their linguistic competence in low-resource and morphologically rich languages such as Tamil remains largely unexplored. Existing multilingual benchmarks often rely on translated English datasets, failing to capture the linguistic and cultural nuances of the target language. To address this gap, we introduce ILAKKANAM, the first Tamil-specific linguistic evaluation benchmark manually curated using 820 questions from Sri Lankan school-level Tamil subject examination papers. Each question is annotated by trained linguists under five linguistic categories and a factual knowledge category, spanning Grades 1--13 to ensure broad linguistic coverage. We evaluate both closed-source and open-source LLMs using a standardized evaluation framework. Our results show that Gemini 2.5 achieves the highest overall performance, while open-source models lag behind, highlighting the gap in linguistic grounding. Category- and grade-wise analyses reveal that all models perform well on lower-grade questions but show a clear decline as linguistic complexity increases. Further, no strong correlation is observed between a model's overall performance and its ability to identify linguistic categories, suggesting that performance may be driven by exposure rather than genuine understanding.

</details>


### [128] [Probing Preference Representations: A Multi-Dimensional Evaluation and Analysis Method for Reward Models](https://arxiv.org/abs/2511.12464)
*Chenglong Wang,Yifu Huo,Yang Gan,Yongyu Mu,Qiaozhi He,Murun Yang,Bei Li,Chunliang Zhang,Tongran Liu,Anxiang Ma,Zhengtao Yu,Jingbo Zhu,Tong Xiao*

Main category: cs.CL

TL;DR: This paper introduces MRMBench, a benchmark that probes reward models on six preference dimensions, and proposes inference-time probing for better interpretability.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods for reward models lack detailed performance insights across different preference dimensions.

Method: Developed MRMBench with six probing tasks for different preference dimensions and introduced inference-time probing for analyzing reward prediction dimensions.

Result: Experiments show a strong correlation between MRMBench and the alignment performance of large language models. Reward models often struggle to capture multi-dimensional preferences.

Conclusion: MRMBench is a reliable benchmark and inference-time probing improves interpretability and alignment of reward predictions in large models.

Abstract: Previous methods evaluate reward models by testing them on a fixed pairwise ranking test set, but they typically do not provide performance information on each preference dimension. In this work, we address the evaluation challenge of reward models by probing preference representations. To confirm the effectiveness of this evaluation method, we construct a Multi-dimensional Reward Model Benchmark (MRMBench), a collection of six probing tasks for different preference dimensions. We design it to favor and encourage reward models that better capture preferences across different dimensions. Furthermore, we introduce an analysis method, inference-time probing, which identifies the dimensions used during the reward prediction and enhances its interpretability. Through extensive experiments, we find that MRMBench strongly correlates with the alignment performance of large language models (LLMs), making it a reliable reference for developing advanced reward models. Our analysis of MRMBench evaluation results reveals that reward models often struggle to capture preferences across multiple dimensions, highlighting the potential of multi-objective optimization in reward modeling. Additionally, our findings show that the proposed inference-time probing method offers a reliable metric for assessing the confidence of reward predictions, which ultimately improves the alignment of LLMs.

</details>


### [129] [Assessing LLMs for Serendipity Discovery in Knowledge Graphs: A Case for Drug Repurposing](https://arxiv.org/abs/2511.12472)
*Mengying Wang,Chenhui Ma,Ao Jiao,Tuo Liang,Pengjun Lu,Shrinidhi Hegde,Yu Yin,Evren Gurkan-Cavusoglu,Yinghui Wu*

Main category: cs.CL

TL;DR: The paper introduces SerenQA, a framework for enhancing and evaluating serendipity-aware question answering in knowledge graphs using large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: Existing KGQA systems are often optimized for predictable, relevant answers, lacking the capacity for suggesting unexpected, novel, and serendipitous answers in domains like scientific research.

Method: The paper defines a serendipity-aware KGQA task and introduces SerenQA, which includes a serendipity metric based on relevance, novelty, and surprise, an expert-annotated benchmark using a Clinical Knowledge Graph, and an evaluation pipeline with three subtasks: knowledge retrieval, subgraph reasoning, and serendipity exploration.

Result: State-of-the-art LLMs perform well in knowledge retrieval but struggle to generate surprising and valuable insights, indicating room for improvement in serendipity-aware capabilities.

Conclusion: The findings emphasize the necessity of developing more advanced LLMs to tackle serendipity in KGQA, providing a foundation and resources for future research in this direction.

Abstract: Large Language Models (LLMs) have greatly advanced knowledge graph question answering (KGQA), yet existing systems are typically optimized for returning highly relevant but predictable answers. A missing yet desired capacity is to exploit LLMs to suggest surprise and novel ("serendipitious") answers. In this paper, we formally define the serendipity-aware KGQA task and propose the SerenQA framework to evaluate LLMs' ability to uncover unexpected insights in scientific KGQA tasks. SerenQA includes a rigorous serendipity metric based on relevance, novelty, and surprise, along with an expert-annotated benchmark derived from the Clinical Knowledge Graph, focused on drug repurposing. Additionally, it features a structured evaluation pipeline encompassing three subtasks: knowledge retrieval, subgraph reasoning, and serendipity exploration. Our experiments reveal that while state-of-the-art LLMs perform well on retrieval, they still struggle to identify genuinely surprising and valuable discoveries, underscoring a significant room for future improvements. Our curated resources and extended version are released at: https://cwru-db-group.github.io/serenQA.

</details>


### [130] [SGuard-v1: Safety Guardrail for Large Language Models](https://arxiv.org/abs/2511.12497)
*JoonHo Lee,HyeonMin Cho,Jaewoong Yun,Hyunjae Lee,JunKyu Lee,Juree Seok*

Main category: cs.CL

TL;DR: SGuard-v1 is a lightweight safety measure for LLMs that uses two models to filter harmful content and adversarial prompts, achieving state-of-the-art performance while being efficient and interpretable.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of trust and safety in LLMs by mitigating harmful content and adversarial prompts in AI conversations.

Method: SGuard-v1 integrates two specialized models: ContentFilter for safety risk detection using the MLCommons taxonomy, and JailbreakFilter for adversarial prompt screening trained on diverse datasets, instruction-tuned on 1.4 million curated instances.

Result: Extensive evaluations show SGuard-v1 delivers state-of-the-art safety performance across public and proprietary benchmarks while maintaining a lightweight and efficient design.

Conclusion: SGuard-v1 effectively enhances LLM trust and safety, improves deployment ease and interpretability, and is freely available for research and application under Apache-2.0 License.

Abstract: We present SGuard-v1, a lightweight safety guardrail for Large Language Models (LLMs), which comprises two specialized models to detect harmful content and screen adversarial prompts in human-AI conversational settings. The first component, ContentFilter, is trained to identify safety risks in LLM prompts and responses in accordance with the MLCommons hazard taxonomy, a comprehensive framework for trust and safety assessment of AI. The second component, JailbreakFilter, is trained with a carefully designed curriculum over integrated datasets and findings from prior work on adversarial prompting, covering 60 major attack types while mitigating false-unsafe classification. SGuard-v1 is built on the 2B-parameter Granite-3.3-2B-Instruct model that supports 12 languages. We curate approximately 1.4 million training instances from both collected and synthesized data and perform instruction tuning on the base model, distributing the curated data across the two component according to their designated functions. Through extensive evaluation on public and proprietary safety benchmarks, SGuard-v1 achieves state-of-the-art safety performance while remaining lightweight, thereby reducing deployment overhead. SGuard-v1 also improves interpretability for downstream use by providing multi-class safety predictions and their binary confidence scores. We release the SGuard-v1 under the Apache-2.0 License to enable further research and practical deployment in AI safety.

</details>


### [131] [QA-Noun: Representing Nominal Semantics via Natural Language Question-Answer Pairs](https://arxiv.org/abs/2511.12504)
*Maria Tseytlin,Paul Roit,Omri Abend,Ido Dagan,Ayal Klein*

Main category: cs.CL

TL;DR: The paper introduces QA-Noun, a framework that refines sentence meaning decomposition by focusing on noun-centered semantics using question-answer pairs.


<details>
  <summary>Details</summary>
Motivation: Current semantic frameworks, particularly QA-based methods, effectively address predicate-argument relations but neglect the semantic roles and relations centered around nouns.

Method: QA-Noun employs nine question templates to capture explicit syntactical and implicit contextual roles of nouns, creating a dataset with over 2,000 annotations and a trained model integrated into QA-SRL.

Result: QA-Noun achieves near-complete coverage of AMR's noun arguments and significant improvements in semantic granularity compared to contemporary methods like FactScore and DecompScore.

Conclusion: QA-Noun enhances the QA-based semantic analysis framework, allowing for a more comprehensive and scalable decomposition of sentences into fine-grained semantic units.

Abstract: Decomposing sentences into fine-grained meaning units is increasingly used to model semantic alignment. While QA-based semantic approaches have shown effectiveness for representing predicate-argument relations, they have so far left noun-centered semantics largely unaddressed. We introduce QA-Noun, a QA-based framework for capturing noun-centered semantic relations. QA-Noun defines nine question templates that cover both explicit syntactical and implicit contextual roles for nouns, producing interpretable QA pairs that complement verbal QA-SRL. We release detailed guidelines, a dataset of over 2,000 annotated noun mentions, and a trained model integrated with QA-SRL to yield a unified decomposition of sentence meaning into individual, highly fine-grained, facts. Evaluation shows that QA-Noun achieves near-complete coverage of AMR's noun arguments while surfacing additional contextually implied relations, and that combining QA-Noun with QA-SRL yields over 130\% higher granularity than recent fact-based decomposition methods such as FactScore and DecompScore. QA-Noun thus complements the broader QA-based semantic framework, forming a comprehensive and scalable approach to fine-grained semantic decomposition for cross-text alignment.

</details>


### [132] [TAdaRAG: Task Adaptive Retrieval-Augmented Generation via On-the-Fly Knowledge Graph Construction](https://arxiv.org/abs/2511.12520)
*Jie Zhang,Bo Tang,Wanzi Shao,Wenqiang Wei,Jihao Zhao,Jianqing Zhu,Zhiyu li,Wen Xi,Zehao Lin,Feiyu Xiong,Yanchao Tan*

Main category: cs.CL

TL;DR: TAdaRAG is a novel RAG framework that addresses information loss and hallucinations by constructing task-adaptive knowledge graphs, outperforming existing methods in various benchmarks.


<details>
  <summary>Details</summary>
Motivation: To solve issues like information loss, hallucinations, and reasoning errors in current RAG systems due to unstructured knowledge retrieval.

Method: TAdaRAG uses an intent-driven routing mechanism, domain-specific templates, supervised fine-tuning, and reinforcement learning for concise and accurate knowledge graph construction.

Result: Evaluations on six public benchmarks and a real-world benchmark demonstrate superior performance and generalization of TAdaRAG compared to existing methods.

Conclusion: TAdaRAG provides improved retrieval-augmented generation by effectively tackling information truncation, irrelevant details, and reasoning chains, showcasing practical effectiveness across diverse tasks and domains.

Abstract: Retrieval-Augmented Generation (RAG) improves large language models by retrieving external knowledge, often truncated into smaller chunks due to the input context window, which leads to information loss, resulting in response hallucinations and broken reasoning chains. Moreover, traditional RAG retrieves unstructured knowledge, introducing irrelevant details that hinder accurate reasoning. To address these issues, we propose TAdaRAG, a novel RAG framework for on-the-fly task-adaptive knowledge graph construction from external sources. Specifically, we design an intent-driven routing mechanism to a domain-specific extraction template, followed by supervised fine-tuning and a reinforcement learning-based implicit extraction mechanism, ensuring concise, coherent, and non-redundant knowledge integration. Evaluations on six public benchmarks and a real-world business benchmark (NowNewsQA) across three backbone models demonstrate that TAdaRAG outperforms existing methods across diverse domains and long-text tasks, highlighting its strong generalization and practical effectiveness.

</details>


### [133] [Mitigating Length Bias in RLHF through a Causal Lens](https://arxiv.org/abs/2511.12573)
*Hyeonji Kim,Sujeong Oh,Sanghack Lee*

Main category: cs.CL

TL;DR: The paper addresses the length bias in RLHF reward models, proposing a method using counterfactual data augmentation to make assessments based on content quality rather than verbosity.


<details>
  <summary>Details</summary>
Motivation: RLHF reward models, while aligning LLMs to human preferences, often show a preference for longer responses due to length bias, compromising content quality evaluations.

Method: A causal framework is introduced involving counterfactual data augmentation to create response pairs. These pairs isolate content quality from verbosity and are used to train the reward model.

Result: The approach reduces length bias in reward assignment and improves the policy model's focus on concise, high-quality content.

Conclusion: The proposed method effectively mitigates length bias, enhancing the robustness and content sensitivity of RLHF reward models.

Abstract: Reinforcement learning from human feedback (RLHF) is widely used to align large language models (LLMs) with human preferences. However, RLHF-trained reward models often exhibit length bias -- a systematic tendency to favor longer responses by conflating verbosity with quality. We propose a causal framework for analyzing and mitigating length bias in RLHF reward modeling. Central to our approach is a counterfactual data augmentation method that generates response pairs designed to isolate content quality from verbosity. These counterfactual examples are then used to train the reward model, enabling it to assess responses based on content quality independently of verbosity. Specifically, we construct (1) length-divergent pairs with similar content and (2) content-divergent pairs of similar length. Empirical evaluations show that our method reduces length bias in reward assignment and leads to more concise, content-focused outputs from the policy model. These findings demonstrate that the proposed approach effectively reduces length bias and improves the robustness and content sensitivity of reward modeling in RLHF pipelines.

</details>


### [134] [MMWOZ: Building Multimodal Agent for Task-oriented Dialogue](https://arxiv.org/abs/2511.12586)
*Pu-Hai Yang,Heyan Huang,Heng-Da Xu,Fanshu Sun,Xian-Ling Mao,Chaoxu Mu*

Main category: cs.CL

TL;DR: This paper introduces a new multimodal dialogue dataset, MMWOZ, designed to better integrate with GUI-based interfaces and proposes a novel model, MATE, as the baseline for this dataset.


<details>
  <summary>Details</summary>
Motivation: Task-oriented dialogue systems face challenges in real-world applications due to their reliance on back-end APIs, which are often unavailable, while GUIs are prevalent. The paper aims to address this gap.

Method: The authors extend MultiWOZ 2.3 to collect MMWOZ, developing a web-style GUI, converting system actions into GUI operation instructions, and capturing web page snapshots. They also introduce MATE, a multimodal model, as an experimental baseline.

Result: A detailed experimental analysis using the MATE model is conducted to explore its effectiveness with the newly created MMWOZ dataset.

Conclusion: The study bridges the gap between traditional task-oriented dialogue systems and GUI-based real-world applications by providing the MMWOZ dataset and demonstrating the functionality of the MATE model in this context.

Abstract: Task-oriented dialogue systems have garnered significant attention due to their conversational ability to accomplish goals, such as booking airline tickets for users. Traditionally, task-oriented dialogue systems are conceptualized as intelligent agents that interact with users using natural language and have access to customized back-end APIs. However, in real-world scenarios, the widespread presence of front-end Graphical User Interfaces (GUIs) and the absence of customized back-end APIs create a significant gap for traditional task-oriented dialogue systems in practical applications. In this paper, to bridge the gap, we collect MMWOZ, a new multimodal dialogue dataset that is extended from MultiWOZ 2.3 dataset. Specifically, we begin by developing a web-style GUI to serve as the front-end. Next, we devise an automated script to convert the dialogue states and system actions from the original dataset into operation instructions for the GUI. Lastly, we collect snapshots of the web pages along with their corresponding operation instructions. In addition, we propose a novel multimodal model called MATE (Multimodal Agent for Task-oriEnted dialogue) as the baseline model for the MMWOZ dataset. Furthermore, we conduct comprehensive experimental analysis using MATE to investigate the construction of a practical multimodal agent for task-oriented dialogue.

</details>


### [135] [Group-Aware Reinforcement Learning for Output Diversity in Large Language Models](https://arxiv.org/abs/2511.12596)
*Oron Anschel,Alon Shoshan,Adam Botach,Shunit Haviv Hakimi,Asaf Gendler,Emanuel Ben Baruch,Nadav Bhonker,Igor Kviatkovsky,Manoj Aggarwal,Gerard Medioni*

Main category: cs.CL

TL;DR: LLMs often generate repeated outputs due to mode collapse; GAPO addresses this by enhancing diversity and coverage in model responses.


<details>
  <summary>Details</summary>
Motivation: To overcome mode collapse in LLMs, which results in repetitive outputs and limits diversity, even when multiple valid completions exist.

Method: Introduce GAPO (Group-Aware Policy Optimization), an extension of GRPO, which uses group-level properties like diversity and coverage for learning. The approach incorporates a frequency-aware reward function to encourage uniform sampling of valid completions.

Result: GAPO-trained models produce diverse and valid completions, improving response diversity across tasks and open-ended prompts without reducing performance on benchmarks like GSM8K, MATH, HumanEval, and MMLU-Pro.

Conclusion: GAPO effectively addresses the limitation of mode collapse in LLMs by improving response diversity and maintaining high performance. The code will be made publicly available for further research and implementation.

Abstract: Large Language Models (LLMs) often suffer from mode collapse, repeatedly generating the same few completions even when many valid answers exist, limiting their diversity across a wide range of tasks. We introduce Group-Aware Policy Optimization (GAPO), a simple extension of the recent and popular Group Relative Policy Optimization (GRPO) that computes rewards over the group as a whole. GAPO enables learning from the group-level properties such as diversity and coverage. We demonstrate GAPO using a frequency-aware reward function that encourages uniform sampling over valid LLM completions, and show that GAPO-trained models produce valid and more diverse model responses. Beyond this setup, GAPO generalizes to open-ended prompts and improves response diversity without compromising accuracy on standard LLM benchmarks (GSM8K, MATH, HumanEval, MMLU-Pro). Our code will be made publicly available.

</details>


### [136] [Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data](https://arxiv.org/abs/2511.12609)
*Yunxin Li,Xinyu Chen,Shenyuan Jiang,Haoyuan Shi,Zhenyu Liu,Xuanyu Zhang,Nanhao Deng,Zhenran Xu,Yicheng Ma,Meishan Zhang,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: The paper introduces Uni-MoE 2.0, a powerful open-source omnimodal large model for language-centric multimodal tasks, excelling in understanding, reasoning, and generating images, text, and speech.


<details>
  <summary>Details</summary>
Motivation: The paper aims to enhance multimodal understanding, reasoning, and generation through a sophisticated omnimodal model that improves upon prior models in the Lychee family.

Method: The model is built on dynamic-capacity MoE design, progressive training strategy with reinforcement and curated multimodal data techniques. Architectural improvements include 3D RoPE for cross-modality alignment, supervised fine-tuning, and RL optimization for stability.

Result: The model demonstrates state-of-the-art or competitive performance across 85 benchmarks, showing significant improvements over Qwen2.5-Omni in various tasks such as video understanding (+7%), omnimodal understanding (+7%), and audiovisual reasoning (+4%).

Conclusion: Uni-MoE 2.0 showcases substantial advancements in multimodal AI capabilities. Its robust design and optimization strategies underline its potential in both research and practical applications.

Abstract: We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee's Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the Qwen2.5-7B dense architecture, we build Uni-MoE-2.0-Omni from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech. Architecturally, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer. For training, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning. Data-wise, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues. Extensive evaluation across 85 benchmarks demonstrates that our model achieves SOTA or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (reducing WER by 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.

</details>


### [137] [Knots: A Large-Scale Multi-Agent Enhanced Expert-Annotated Dataset and LLM Prompt Optimization for NOTAM Semantic Parsing](https://arxiv.org/abs/2511.12630)
*Maoqi Liu,Quan Fang,Yang Yang,Can Zhao,Kaiquan Cai*

Main category: cs.CL

TL;DR: This paper targets semantic parsing for NOTAMs by introducing the Knots dataset and emphasizing on semantic inference with aviation domain knowledge, demonstrating substantial improvements in understanding aviation texts.


<details>
  <summary>Details</summary>
Motivation: NOTAMs are crucial for flight safety information but are challenging for automated parsing due to their complexity and implicit reasoning demands.

Method: The authors introduced a semantic parsing task for NOTAMs, constructed the Knots dataset with expert annotations, and evaluated diverse prompt-engineering strategies and model-adaptation techniques.

Result: The proposed approach significantly enhanced automation capabilities in processing and understanding NOTAMs, supported by the Knots dataset evaluations.

Conclusion: The work advances automated NOTAM analysis with a novel dataset and methods, improving semantic understanding in aviation text systems.

Abstract: Notice to Air Missions (NOTAMs) serve as a critical channel for disseminating key flight safety information, yet their complex linguistic structures and implicit reasoning pose significant challenges for automated parsing. Existing research mainly focuses on surface-level tasks such as classification and named entity recognition, lacking deep semantic understanding. To address this gap, we propose NOTAM semantic parsing, a task emphasizing semantic inference and the integration of aviation domain knowledge to produce structured, inference-rich outputs. To support this task, we construct Knots (Knowledge and NOTAM Semantics), a high-quality dataset of 12,347 expert-annotated NOTAMs covering 194 Flight Information Regions, enhanced through a multi-agent collaborative framework for comprehensive field discovery. We systematically evaluate a wide range of prompt-engineering strategies and model-adaptation techniques, achieving substantial improvements in aviation text understanding and processing. Our experimental results demonstrate the effectiveness of the proposed approach and offer valuable insights for automated NOTAM analysis systems. Our code is available at: https://github.com/Estrellajer/Knots.

</details>


### [138] [Reason-KE++: Aligning the Process, Not Just the Outcome, for Faithful LLM Knowledge Editing](https://arxiv.org/abs/2511.12661)
*Yuchen Wu,Liang Ding,Li Shen,Dacheng Tao*

Main category: cs.CL

TL;DR: The paper addresses the challenge of aligning large language models (LLMs) to maintain process-level faithfulness in reasoning tasks by introducing a framework, Reason-KE++, which outperforms previous methods in avoiding factual hallucinations.


<details>
  <summary>Details</summary>
Motivation: Ensuring LLMs stay faithful to new contextual facts in reasoning tasks is difficult. Current methods prioritize format mimicry over sound reasoning, causing models to rely on parametric priors, leading to factual inaccuracies.

Method: The authors propose Reason-KE++, an SFT+RL framework with a process-aware reward mechanism that supervises key reasoning steps, avoiding over-reliance on outcome-only reinforcement learning.

Result: Reason-KE++ achieves a new state-of-the-art accuracy (95.48%) on the MQUAKE-CF-3k benchmark, significantly improving reasoning faithfulness in LLMs.

Conclusion: Focusing on aligning the reasoning process, rather than just outcome accuracy, is crucial for trustworthy and accurate LLM performance in complex, multi-step tasks.

Abstract: Aligning Large Language Models (LLMs) to be faithful to new knowledge in complex, multi-hop reasoning tasks is a critical, yet unsolved, challenge. We find that SFT-based methods, e.g., Reason-KE, while state-of-the-art, suffer from a "faithfulness gap": they optimize for format mimicry rather than sound reasoning. This gap enables the LLM's powerful parametric priors to override new contextual facts, resulting in critical factual hallucinations (e.g., incorrectly reasoning "Houston" from "NASA" despite an explicit edit). To solve this core LLM alignment problem, we propose Reason-KE++, an SFT+RL framework that instills process-level faithfulness. Its core is a Stage-aware Reward mechanism that provides dense supervision for intermediate reasoning steps (e.g., Decomposition, Sub-answer Correctness). Crucially, we identify that naive outcome-only RL is a deceptive trap for LLM alignment: it collapses reasoning integrity (e.g., 19.00% Hop acc) while superficially boosting final accuracy. Our process-aware framework sets a new SOTA of 95.48% on MQUAKE-CF-3k (+5.28%), demonstrating that for complex tasks, aligning the reasoning process is essential for building trustworthy LLMs.

</details>


### [139] [Improving Direct Persian-English Speech-to-Speech Translation with Discrete Units and Synthetic Parallel Data](https://arxiv.org/abs/2511.12690)
*Sina Rashidi,Hossein Sameti*

Main category: cs.CL

TL;DR: The paper focuses on improving Persian-English speech-to-speech translation using a direct approach combining synthetic data, pre-training, and discrete speech units.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of directly translating speech in low-resource language pairs like Persian-English, particularly due to the lack of large parallel speech datasets.

Method: The presented model combines a conformer-based encoder (pre-trained on self-supervised tasks), a causal transformer decoder for mapping acoustic representations to speech units, and a neural vocoder for waveform generation. Synthetic data is created by translating Persian transcriptions into English using a large language model and generating the speech with a zero-shot TTS system.

Result: The model achieves a 4.6 ASR BLEU improvement on Persian-English S2ST using synthetic data on the CVSS corpus, indicating its success.

Conclusion: Self-supervised pre-training, synthetic data generation, and discrete speech units effectively improve direct speech-to-speech translation for low-resource languages like Persian.

Abstract: Direct speech-to-speech translation (S2ST), in which all components are trained jointly, is an attractive alternative to cascaded systems because it offers a simpler pipeline and lower inference latency. However, direct S2ST models require large amounts of parallel speech data in the source and target languages, which are rarely available for low-resource languages such as Persian. This paper presents a direct S2ST system for translating Persian speech into English speech, as well as a pipeline for synthetic parallel Persian-English speech generation. The model comprises three components: (1) a conformer-based encoder, initialized from self-supervised pre-training, maps source speech to high-level acoustic representations; (2) a causal transformer decoder with relative position multi-head attention translates these representations into discrete target speech units; (3) a unit-based neural vocoder generates waveforms from the predicted discrete units. To mitigate the data scarcity problem, we construct a new Persian-English parallel speech corpus by translating Persian speech transcriptions into English using a large language model and then synthesizing the corresponding English speech with a state-of-the-art zero-shot text-to-speech system. The resulting corpus increases the amount of available parallel speech by roughly a factor of six. On the Persian-English portion of the CVSS corpus, the proposed model achieves improvement of 4.6 ASR BLEU with the synthetic data over direct baselines. These results indicate that combining self-supervised pre-training, discrete speech units, and synthetic parallel data is effective for improving direct S2ST in low-resource language pairs such as Persian-English

</details>


### [140] [Evolve the Method, Not the Prompts: Evolutionary Synthesis of Jailbreak Attacks on LLMs](https://arxiv.org/abs/2511.12710)
*Yunhao Chen,Xin Wang,Juncheng Li,Yixu Wang,Jie Li,Yan Teng,Yingchun Wang,Xingjun Ma*

Main category: cs.CL

TL;DR: EvoSynth is a novel automated framework for generating more innovative and diverse attack methods on Large Language Models (LLMs). It uses evolutionary synthesis instead of refining pre-existing prompts and achieves superior performance with a high attack success rate.


<details>
  <summary>Details</summary>
Motivation: Current frameworks for red teaming of LLMs are limited in their creativity because they rely solely on pre-existing attack strategies, unable to autonomously invent new attack methods. The authors aimed to overcome this limitation.

Method: The paper introduces EvoSynth, a multi-agent evolutionary synthesis framework. It autonomously engineers and evolves novel, code-based attack algorithms and incorporates a code-level self-correction loop to refine its methods iteratively.

Result: EvoSynth achieved a state-of-the-art 85.5% Attack Success Rate against robust models such as Claude-Sonnet-4.5. It also produced attack methods that are significantly more diverse compared to existing methods.

Conclusion: EvoSynth represents a major advancement in autonomous red teaming for LLMs by pioneering the evolutionary synthesis of jailbreak techniques and achieving high effectiveness and diversity. The authors aim to inspire future work in this area by releasing the framework.

Abstract: Automated red teaming frameworks for Large Language Models (LLMs) have become increasingly sophisticated, yet they share a fundamental limitation: their jailbreak logic is confined to selecting, combining, or refining pre-existing attack strategies. This binds their creativity and leaves them unable to autonomously invent entirely new attack mechanisms. To overcome this gap, we introduce \textbf{EvoSynth}, an autonomous framework that shifts the paradigm from attack planning to the evolutionary synthesis of jailbreak methods. Instead of refining prompts, EvoSynth employs a multi-agent system to autonomously engineer, evolve, and execute novel, code-based attack algorithms. Crucially, it features a code-level self-correction loop, allowing it to iteratively rewrite its own attack logic in response to failure. Through extensive experiments, we demonstrate that EvoSynth not only establishes a new state-of-the-art by achieving an 85.5\% Attack Success Rate (ASR) against highly robust models like Claude-Sonnet-4.5, but also generates attacks that are significantly more diverse than those from existing methods. We release our framework to facilitate future research in this new direction of evolutionary synthesis of jailbreak methods. Code is available at: https://github.com/dongdongunique/EvoSynth.

</details>


### [141] [Adaptive Focus Memory for Language Models](https://arxiv.org/abs/2511.12712)
*Christopher Cruz*

Main category: cs.CL

TL;DR: The paper introduces Adaptive Focus Memory (AFM), a dynamic context manager for large language models in multi-turn dialogues, capable of retaining critical details like safety information while reducing token usage significantly.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address limitations in large language models caused by fixed context windows and inefficient memory strategies, which can either be too costly (replaying full conversation) or risk losing critical safety information (using static summarization).

Method: AFM assigns past messages one of three fidelity levels (FULL, COMPRESSED, PLACEHOLDER) based on semantic similarity, recency weighting, and importance classification. It then dynamically manages the conversation memory while adhering to a strict token budget.

Result: AFM retains critical details like a peanut allergy across conversations, achieves similar safety performance to replaying full conversations, and reduces token usage by 66%.

Conclusion: AFM demonstrates an effective balance between reducing computational costs and preserving critical information, enabling safer and more efficient language model applications in multi-turn dialogues.

Abstract: Large language models (LLMs) are increasingly deployed in multi-turn dialogue settings, but their behavior is still bottlenecked by fixed context windows and naive memory strategies. Replaying the full conversation at every turn is simple but expensive, while static summarization or recency-only heuristics often erase safety-critical user details. We present Adaptive Focus Memory (AFM), a dynamic context manager that assigns each past message one of three fidelity levels -- FULL, COMPRESSED, or PLACEHOLDER -- based on semantic similarity to the current query, half-life recency weighting, and importance classification. AFM packs messages chronologically under a strict token budget, preferring high fidelity for the most relevant turns while aiming to preserve a cheap trace of the dialogue. In a safety-oriented benchmark involving a user with a severe peanut allergy planning a trip to Thailand, AFM retains the allergy across both short and medium-length conversations, matches the safety performance of naive replay, and cuts average token usage by 66% relative to a replay baseline. We release a modular Python implementation of AFM designed for OpenAI-compatible APIs and offline operation, enabling practitioners to reduce inference cost without sacrificing safety or factual continuity in the evaluated scenario.

</details>


### [142] [On the Brittleness of LLMs: A Journey around Set Membership](https://arxiv.org/abs/2511.12728)
*Lea Hergert,Gábor Berend,Mario Szegedy,Gyorgy Turan,Márk Jelasity*

Main category: cs.CL

TL;DR: Large language models exhibit unreliable reasoning, struggling with basic tasks like set membership queries, despite excelling in complex challenges.


<details>
  <summary>Details</summary>
Motivation: Understand why large language models fail on simple tasks despite succeeding in complex reasoning.

Method: Systematically analyze LLM performance on set membership queries, varying factors like prompts, structure, and models.

Result: LLMs show fragile and unpredictable reasoning with set queries, indicating fragmented comprehension of the concept.

Conclusion: The study highlights unreliability in LLM reasoning and introduces comprehensive failure mode analysis as a methodology for evaluation.

Abstract: Large language models (LLMs) achieve superhuman performance on complex reasoning tasks, yet often fail on much simpler problems, raising concerns about their reliability and interpretability. We investigate this paradox through a focused study with two key design features: simplicity, to expose basic failure modes, and scale, to enable comprehensive controlled experiments. We focus on set membership queries -- among the most fundamental forms of reasoning -- using tasks like ``Is apple an element of the set \{pear, plum, apple, raspberry\}?''. We conduct a systematic empirical evaluation across prompt phrasing, semantic structure, element ordering, and model choice. Our large-scale analysis reveals that LLM performance on this elementary task is consistently brittle, and unpredictable across all dimensions, suggesting that the models' ``understanding'' of the set concept is fragmented and convoluted at best. Our work demonstrates that the large-scale experiments enabled by the simplicity of the problem allow us to map and analyze the failure modes comprehensively, making this approach a valuable methodology for LLM evaluation in general.

</details>


### [143] [Evidence of Phase Transitions in Small Transformer-Based Language Models](https://arxiv.org/abs/2511.12768)
*Noah Hong,Tao Hong*

Main category: cs.CL

TL;DR: The paper investigates phase transitions in language model training, analyzing whether they occur in small models, can be observed in linear training space, and emerge early during training.


<details>
  <summary>Details</summary>
Motivation: To understand the emergence of capabilities during language model training and whether phase transitions are a universal feature across different scales of models.

Method: The authors trained a small GPT-style transformer on a character-level corpus, analyzing vocabulary usage dynamics using average word length, correct/incorrect words count, and vocabulary diversity. They used Poisson and sub-Poisson statistics for deeper insights.

Result: Distinct phase transitions were observed during training, detectable through vocabulary-based metrics and statistical analysis, but not in standard loss or validation curves.

Conclusion: Phase transitions are general features of language models, observable even in smaller models, visible in linear training space, and occurring early during training, offering insights into nonlinear model dynamics.

Abstract: Phase transitions have been proposed as the origin of emergent abilities in large language models (LLMs), where new capabilities appear abruptly once models surpass critical thresholds of scale. Prior work, such as that of Wei et al., demonstrated these phenomena under model and data scaling, with transitions revealed after applying a log scale to training compute. In this work, we ask three complementary questions: (1) Are phase transitions unique to large models, or can they also be observed in small transformer-based language models? (2) Can such transitions be detected directly in linear training space, rather than only after log rescaling? and (3) Can these transitions emerge at early stages of training? To investigate, we train a small GPT-style transformer on a character-level corpus and analyze the evolution of vocabulary usage throughout training. We track the average word length, the number of correct versus incorrect words, and shifts in vocabulary diversity. Building on these measures, we apply Poisson and sub-Poisson statistics to quantify how words connect and reorganize. This combined analysis reveals a distinct transition point during training. Notably, these transitions are not apparent in standard loss or validation curves, but become visible through our vocabulary- and statistics-based probes. Our findings suggest that phase-transition reorganizations are a general feature of language model training, observable even in modest models, detectable directly in linear training space, and occurring surprisingly early as coherence emerges. This perspective provides new insight into the nonlinear dynamics of language model training and underscores the importance of tailored metrics for uncovering phase transition behaviors

</details>


### [144] [LLM Reinforcement in Context](https://arxiv.org/abs/2511.12782)
*Thomas Rivasseau*

Main category: cs.CL

TL;DR: The paper proposes using interruptions in user input to strengthen Large Language Model alignment, aiming to tackle increasing jailbreak risks in longer conversations.


<details>
  <summary>Details</summary>
Motivation: Current alignment efforts focus on adversarial robustness, but longer user inputs increase LLM vulnerability to jailbreaks. Existing solutions don't scale effectively with input length.

Method: Introduce interruptions—control sentences inserted every x tokens—to mitigate misbehavior risks, potentially extending to Chain-of-Thought processes.

Result: Interruptions could strengthen robustness and alignment, particularly in mitigating risks associated with long user conversations.

Conclusion: Interruptions have potential to improve scalable alignment in LLMs and reduce scheming behavior, addressing current gaps in alignment research.

Abstract: Current Large Language Model alignment research mostly focuses on improving model robustness against adversarial attacks and misbehavior by training on examples and prompting. Research has shown that LLM jailbreak probability increases with the size of the user input or conversation length. There is a lack of appropriate research into means of strengthening alignment which also scale with user input length. We propose interruptions as a possible solution to this problem. Interruptions are control sentences added to the user input approximately every x tokens for some arbitrary x. We suggest that this can be generalized to the Chain-of-Thought process to prevent scheming.

</details>


### [145] [Evaluating Autoformalization Robustness via Semantically Similar Paraphrasing](https://arxiv.org/abs/2511.12784)
*Hayden Moore,Asfahan Shah*

Main category: cs.CL

TL;DR: This paper evaluates the robustness of Large Language Models (LLMs) in generating formal proofs from paraphrased natural language inputs, revealing performance variability and sensitivity.


<details>
  <summary>Details</summary>
Motivation: To explore the limitations of LLMs in autoformalization, particularly their sensitivity to paraphrased inputs in generating formal proofs.

Method: Using established benchmarks (MiniF2F and Lean 4 version of ProofNet) and two modern LLMs, the paper measures semantic and compilation validity of formal proofs generated from paraphrased natural language statements.

Result: Minor shifts in natural language paraphrases significantly impact model outputs, indicating sensitivity and variability in performance.

Conclusion: While LLMs show promise in autoformalization, their sensitivity to paraphrased NL inputs highlights the need for improved robustness and reliability in grounded formalizations.

Abstract: Large Language Models (LLMs) have recently emerged as powerful tools for autoformalization. Despite their impressive performance, these models can still struggle to produce grounded and verifiable formalizations. Recent work in text-to-SQL, has revealed that LLMs can be sensitive to paraphrased natural language (NL) inputs, even when high degrees of semantic fidelity are preserved (Safarzadeh, Oroojlooyjadid, and Roth 2025). In this paper, we investigate this claim in the autoformalization domain. Specifically, we evaluate the robustness of LLMs generating formal proofs with semantically similar paraphrased NL statements by measuring semantic and compilation validity. Using the formal benchmarks MiniF2F (Zheng, Han, and Polu 2021) and Lean 4 version of ProofNet (Xin et al. 2024), and two modern LLMs, we generate paraphrased natural language statements and cross-evaluate these statements across both models. The results of this paper reveal performance variability across paraphrased inputs, demonstrating that minor shifts in NL statements can significantly impact model outputs.

</details>


### [146] [BioMedJImpact: A Comprehensive Dataset and LLM Pipeline for AI Engagement and Scientific Impact Analysis of Biomedical Journals](https://arxiv.org/abs/2511.12821)
*Ruiyu Wang,Yuzhang Xie,Xiao Hu,Carl Yang,Jiaying Lu*

Main category: cs.CL

TL;DR: This paper introduces BioMedJImpact, a dataset for analyzing scientific impact and AI engagement in biomedical journals, and demonstrates key trends using bibliometric and semantic AI indicators.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the gap in understanding how collaboration intensity and AI research shape journal prestige in biomedicine, an area existing open resources rarely explore in depth.

Method: The authors developed BioMedJImpact, which integrates bibliometric, collaboration, and AI engagement indicators derived through a reproducible three-stage large language model (LLM) pipeline. They analyze trends before and after the pandemic (2016-2019 vs. 2020-2023).

Result: The study finds two trends: higher collaboration intensity (larger and diverse author teams) correlates with greater citation impact, and AI engagement increasingly correlates with journal prestige. They confirm their LLM pipeline's reliability through human evaluations.

Conclusion: BioMedJImpact provides a robust dataset and methodological framework, enabling advanced analysis of the intersection between biomedicine and AI, and facilitating scalable scientometric studies on collaboration and innovation dynamics.

Abstract: Assessing journal impact is central to scholarly communication, yet existing open resources rarely capture how collaboration structures and artificial intelligence (AI) research jointly shape venue prestige in biomedicine. We present BioMedJImpact, a large-scale, biomedical-oriented dataset designed to advance journal-level analysis of scientific impact and AI engagement. Built from 1.74 million PubMed Central articles across 2,744 journals, BioMedJImpact integrates bibliometric indicators, collaboration features, and LLM-derived semantic indicators for AI engagement. Specifically, the AI engagement feature is extracted through a reproducible three-stage LLM pipeline that we propose. Using this dataset, we analyze how collaboration intensity and AI engagement jointly influence scientific impact across pre- and post-pandemic periods (2016-2019, 2020-2023). Two consistent trends emerge: journals with higher collaboration intensity, particularly those with larger and more diverse author teams, tend to achieve greater citation impact, and AI engagement has become an increasingly strong correlate of journal prestige, especially in quartile rankings. To further validate the three-stage LLM pipeline we proposed for deriving the AI engagement feature, we conduct human evaluation, confirming substantial agreement in AI relevance detection and consistent subfield classification. Together, these contributions demonstrate that BioMedJImpact serves as both a comprehensive dataset capturing the intersection of biomedicine and AI, and a validated methodological framework enabling scalable, content-aware scientometric analysis of scientific impact and innovation dynamics. Code is available at https://github.com/JonathanWry/BioMedJImpact.

</details>


### [147] [From Passive to Persuasive: Steering Emotional Nuance in Human-AI Negotiation](https://arxiv.org/abs/2511.12832)
*Niranjan Chebrolu,Gerard Christopher Yeo,Kokil Jaidka*

Main category: cs.CL

TL;DR: The paper proposes enhancing emotional nuances in LLMs like LLaMA 3.1-8B using targeted activation engineering instead of extensive fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs struggle to produce nuanced, human-like emotional expressions, and current alignment methods are inadequate or resource-intensive.

Method: The authors use attribution patching to identify key neural components and derive emotional expression vectors from contrastive text pairs, which are then applied to prompts.

Result: Emotional steering improved response characteristics, including better emotional sentiments and higher personal engagement through first-person pronouns.

Conclusion: This approach provides a more interpretable and effective framework for integrating emotional nuance into conversational AI.

Abstract: Large Language Models (LLMs) demonstrate increasing conversational fluency, yet instilling them with nuanced, human-like emotional expression remains a significant challenge. Current alignment techniques often address surface-level output or require extensive fine-tuning. This paper demonstrates that targeted activation engineering can steer LLaMA 3.1-8B to exhibit more human-like emotional nuances. We first employ attribution patching to identify causally influential components, to find a key intervention locus by observing activation patterns during diagnostic conversational tasks. We then derive emotional expression vectors from the difference in the activations generated by contrastive text pairs (positive vs. negative examples of target emotions). Applying these vectors to new conversational prompts significantly enhances emotional characteristics: steered responses show increased positive sentiment (e.g., joy, trust) and more frequent first-person pronoun usage, indicative of greater personal engagement. Our findings offer a precise and interpretable framework and new directions for the study of conversational AI.

</details>


### [148] [Quantifying consistency and accuracy of Latent Dirichlet Allocation](https://arxiv.org/abs/2511.12850)
*Saranzaya Magsarjav,Melissa Humphries,Jonathan Tuke,Lewis Mitchell*

Main category: cs.CL

TL;DR: The paper addresses the instability issue of topic modelling in NLP due to stochastic variations, proposing a new stability measure and assessing the reliability of latent topics using LDA.


<details>
  <summary>Details</summary>
Motivation: To tackle inconsistencies and instability in the results of probabilistic topic modelling methods like LDA, which affect replicability, reliability, and meaningfulness of topics.

Method: Developed a new stability measure integrating accuracy and consistency, alongside using the generative properties of LDA to produce a corpus with ground truth for evaluating variability in outputs over 50 reruns.

Result: The results showed that LDA can identify the correct number of topics in documents, and demonstrated internal consistency as reruns yielded similar topics; however, these topics did not match the true topics.

Conclusion: While LDA is internally consistent, it doesn't accurately capture the true topics within a corpus, highlighting a gap in discovering meaningful latent topics in NLP.

Abstract: Topic modelling in Natural Language Processing uncovers hidden topics in large, unlabelled text datasets. It is widely applied in fields such as information retrieval, content summarisation, and trend analysis across various disciplines. However, probabilistic topic models can produce different results when rerun due to their stochastic nature, leading to inconsistencies in latent topics. Factors like corpus shuffling, rare text removal, and document elimination contribute to these variations. This instability affects replicability, reliability, and interpretation, raising concerns about whether topic models capture meaningful topics or just noise. To address these problems, we defined a new stability measure that incorporates accuracy and consistency and uses the generative properties of LDA to generate a new corpus with ground truth. These generated corpora are run through LDA 50 times to determine the variability in the output. We show that LDA can correctly determine the underlying number of topics in the documents. We also find that LDA is more internally consistent, as the multiple reruns return similar topics; however, these topics are not the true topics.

</details>


### [149] [NeuroLex: A Lightweight Domain Language Model for EEG Report Understanding and Generation](https://arxiv.org/abs/2511.12851)
*Kang Yin,Hye-Bin Shin*

Main category: cs.CL

TL;DR: This paper introduces NeuroLex, a domain-adaptive language model specifically designed for EEG report text, improving accuracy and efficiency in EEG and linguistic tasks.


<details>
  <summary>Details</summary>
Motivation: General-purpose language models struggle with domain-specific EEG linguistic conventions, motivating the need for a tailored model for EEG reporting.

Method: The authors developed NeuroLex using span-corruption pretraining and instruction-style fine-tuning on specialized tasks like report polishing, summarization, and terminology question answering.

Result: NeuroLex outperforms general models of the same scale in perplexity, extraction/summarization accuracy, label efficiency, and robustness to negation and factual errors.

Conclusion: NeuroLex serves as an effective model for EEG text and multimodal EEG-language applications, bridging the gap between biomedical text modeling and brain-computer interface systems.

Abstract: Clinical electroencephalogram (EEG) reports encode domain-specific linguistic conventions that general-purpose language models (LMs) fail to capture. We introduce NeuroLex, a lightweight domain-adaptive language model trained purely on EEG report text from the Harvard Electroencephalography Database. Unlike existing biomedical LMs, NeuroLex is tailored to the linguistic and diagnostic characteristics of EEG reporting, enabling it to serve as both an independent textual model and a decoder backbone for multimodal EEG-language systems. Using span-corruption pretraining and instruction-style fine-tuning on report polishing, paragraph summarization, and terminology question answering, NeuroLex learns the syntax and reasoning patterns characteristic of EEG interpretation. Comprehensive evaluations show that it achieves lower perplexity, higher extraction and summarization accuracy, better label efficiency, and improved robustness to negation and factual hallucination compared with general models of the same scale. With an EEG-aware linguistic backbone, NeuroLex bridges biomedical text modeling and brain-computer interface applications, offering a foundation for interpretable and language-driven neural decoding.

</details>


### [150] [From Perception to Reasoning: Deep Thinking Empowers Multimodal Large Language Models](https://arxiv.org/abs/2511.12861)
*Wenxin Zhu,Andong Chen,Yuchen Song,Kehai Chen,Conghui Zhu,Ziyan Chen,Tiejun Zhao*

Main category: cs.CL

TL;DR: The paper reviews the concept of Multimodal Chain-of-Thought (MCoT), focusing on its methods, applications, and potential to improve reasoning capabilities in multimodal models.


<details>
  <summary>Details</summary>
Motivation: To address reasoning limitations in Multimodal Large Language Models (MLLMs) by leveraging and extending Chain-of-Thought reasoning to multimodal tasks.

Method: The paper reviews existing MCoT methods through three components: CoT paradigms, the post-training stage, and the inference stage. It also discusses evaluation metrics and application scenarios while identifying current challenges.

Result: A systematic overview of MCoT methods, their mechanisms, and applications, along with benchmarks for evaluation, has been presented.

Conclusion: MCoT shows promise in enhancing reasoning for multimodal tasks, but challenges remain, including opaque reasoning paths and limited generalization. The paper outlines future research directions.

Abstract: With the remarkable success of Multimodal Large Language Models (MLLMs) in perception tasks, enhancing their complex reasoning capabilities has emerged as a critical research focus. Existing models still suffer from challenges such as opaque reasoning paths and insufficient generalization ability. Chain-of-Thought (CoT) reasoning, which has demonstrated significant efficacy in language models by enhancing reasoning transparency and output interpretability, holds promise for improving model reasoning capabilities when extended to the multimodal domain. This paper provides a systematic review centered on "Multimodal Chain-of-Thought" (MCoT). First, it analyzes the background and theoretical motivations for its inception from the perspectives of technical evolution and task demands. Then, it introduces mainstream MCoT methods from three aspects: CoT paradigms, the post-training stage, and the inference stage, while also analyzing their underlying mechanisms. Furthermore, the paper summarizes existing evaluation benchmarks and metrics, and discusses the application scenarios of MCoT. Finally, it analyzes the challenges currently facing MCoT and provides an outlook on its future research directions.

</details>


### [151] [Classification of Hope in Textual Data using Transformer-Based Models](https://arxiv.org/abs/2511.12874)
*Chukwuebuka Fortunate Ijezue,Tania-Amanda Fredrick Eneye,Maaz Amjad*

Main category: cs.CL

TL;DR: This study presents a transformer-based approach for classifying hope expressions in text, comparing BERT, GPT-2, and DeBERTa. BERT achieved the highest accuracy with lower computational costs for both binary and multiclass tasks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to develop a method for identifying hope expressions in text, a task with applications in mental health and social media analysis.

Method: The authors compared three architectures (BERT, GPT-2, and DeBERTa) for binary and multiclass tasks, measuring their performance and computational efficiency.

Result: BERT outperformed other models with 84.49% binary and 72.03% multiclass accuracy, requiring less training time than newer architectures. GPT-2 and DeBERTa showed lower accuracies with GPT-2 excelling in sarcasm detection.

Conclusion: This study highlights that smaller and efficient architectures like BERT can be highly effective for specialized tasks such as hope classification, emphasizing architectural suitability over model size.

Abstract: This paper presents a transformer-based approach for classifying hope expressions in text. We developed and compared three architectures (BERT, GPT-2, and DeBERTa) for both binary classification (Hope vs. Not Hope) and multiclass categorization (five hope-related categories). Our initial BERT implementation achieved 83.65% binary and 74.87% multiclass accuracy. In the extended comparison, BERT demonstrated superior performance (84.49% binary, 72.03% multiclass accuracy) while requiring significantly fewer computational resources (443s vs. 704s training time) than newer architectures. GPT-2 showed lowest overall accuracy (79.34% binary, 71.29% multiclass), while DeBERTa achieved moderate results (80.70% binary, 71.56% multiclass) but at substantially higher computational cost (947s for multiclass training). Error analysis revealed architecture-specific strengths in detecting nuanced hope expressions, with GPT-2 excelling at sarcasm detection (92.46% recall). This study provides a framework for computational analysis of hope, with applications in mental health and social media analysis, while demonstrating that architectural suitability may outweigh model size for specialized emotion detection tasks.

</details>


### [152] [Auditing Google's AI Overviews and Featured Snippets: A Case Study on Baby Care and Pregnancy](https://arxiv.org/abs/2511.12920)
*Desheng Hu,Joachim Baumann,Aleksandra Urman,Elsa Lichtenegger,Robin Forsberg,Aniko Hannak,Christo Wilson*

Main category: cs.CL

TL;DR: The paper audits AI-generated content in Google Search's AI Overviews (AIO) and Featured Snippets (FS) for baby care and pregnancy queries, finding issues with information consistency and a lack of medical safeguards.


<details>
  <summary>Details</summary>
Motivation: To investigate the reliability and quality of AI-generated content in high-stakes domains like health care, especially regarding critical search queries about baby care and pregnancy.

Method: A systematic algorithm audit was conducted on 1,508 baby care and pregnancy-related queries. The evaluation framework assessed multiple quality aspects such as answer consistency, relevance, medical safeguards, source categories, and sentiment alignment.

Result: 33% of the information presented in AIO and FS was inconsistent. Medical safeguards were severely lacking, present in only 11% of AIO and 7% of FS. Despite high relevance, FS often linked to commercial sources.

Conclusion: The study underscores the need for improved quality controls and medical safeguards in AI-mediated health information display, emphasizing its critical role in public health and user well-being.

Abstract: Google Search increasingly surfaces AI-generated content through features like AI Overviews (AIO) and Featured Snippets (FS), which users frequently rely on despite having no control over their presentation. Through a systematic algorithm audit of 1,508 real baby care and pregnancy-related queries, we evaluate the quality and consistency of these information displays. Our robust evaluation framework assesses multiple quality dimensions, including answer consistency, relevance, presence of medical safeguards, source categories, and sentiment alignment. Our results reveal concerning gaps in information consistency, with information in AIO and FS displayed on the same search result page being inconsistent with each other in 33% of cases. Despite high relevance scores, both features critically lack medical safeguards (present in just 11% of AIO and 7% of FS responses). While health and wellness websites dominate source categories for both, AIO and FS, FS also often link to commercial sources. These findings have important implications for public health information access and demonstrate the need for stronger quality controls in AI-mediated health information. Our methodology provides a transferable framework for auditing AI systems across high-stakes domains where information quality directly impacts user well-being.

</details>


### [153] [Visual Room 2.0: Seeing is Not Understanding for MLLMs](https://arxiv.org/abs/2511.12928)
*Haokun Li,Yazhou Zhang,Jizhi Ding,Qiuchi Li,Peng Zhang*

Main category: cs.CL

TL;DR: This paper evaluates multi-modal large language models (MLLMs) and their ability to truly understand visual content using the Visual Room benchmark, highlighting gaps between perception and cognition.


<details>
  <summary>Details</summary>
Motivation: The paper investigates whether MLLMs can genuinely comprehend visual data beyond descriptive accuracy, addressing gaps in evaluating perception-to-cognition alignment.

Method: A hierarchical benchmark 'Visual Room 2.0' is introduced, modeling human-like perception and cognition across 17 tasks with 350 multi-modal samples and 2,100 progressive questions.

Result: Results show MLLMs excel in perceptual tasks but struggle with cognitive reasoning. Cognitive abilities scale with model size, while perception does not consistently improve with larger models.

Conclusion: MLLMs demonstrate seeing is not understanding, with perception and cognition being distinct processes that should be evaluated differently. The study introduces a new framework to analyze these factors.

Abstract: Can multi-modal large language models (MLLMs) truly understand what they can see? Extending Searle's Chinese Room into the multi-modal domain, this paper proposes the Visual Room argument: MLLMs may describe every visual detail precisely yet fail to comprehend the underlying emotions and intentions, namely seeing is not understanding. Building on this, we introduce \textit{Visual Room} 2.0, a hierarchical benchmark for evaluating perception-cognition alignment of MLLMs. We model human perceptive and cognitive processes across three levels: low, middle, and high, covering 17 representative tasks. The perception component ranges from attribute recognition to scene understanding, while the cognition component extends from textual entailment to causal and social reasoning. The dataset contains 350 multi-modal samples, each with six progressive questions (2,100 in total) spanning perception to cognition. Evaluating 10 state-of-the-art (SoTA) MLLMs, we highlight three key findings: (1) MLLMs exhibit stronger perceptual competence than cognitive ability (8.0\%$\uparrow$); (2) cognition appears not causally dependent on perception-based reasoning; and (3) cognition scales with model size, but perception does not consistently improve with larger variants. This work operationalizes Seeing $\ne$ Understanding as a testable hypothesis, offering a new paradigm from perceptual processing to cognitive reasoning in MLLMs. Our dataset is available at https://huggingface.co/datasets/LHK2003/PCBench.

</details>


### [154] [Fine-Tuned LLMs Know They Don't Know: A Parameter-Efficient Approach to Recovering Honesty](https://arxiv.org/abs/2511.12991)
*Zeyu Shi,Ziming Wang,Tianyu Chen,Shiqi Gao,Haoyi Zhou,Qingyun Sun,Jianxin Li*

Main category: cs.CL

TL;DR: This paper addresses the compromised honesty of LLMs caused by supervised fine-tuning, proposing Honesty-Critical Neurons Restoration (HCNR) to prioritize faithful knowledge expression.


<details>
  <summary>Details</summary>
Motivation: Ensuring the honesty of LLMs is vital for their safe application in sensitive domains, but supervised fine-tuning negatively impacts their ability to reliably express known truths.

Method: The HCNR method restores pre-trained neuron states critical for truthful expression and adjusts them in harmony with task-specific neurons using Hessian-guided compensation.

Result: HCNR recovers 33.25% of honesty loss, demonstrates at least 2.23x speedup, and uses over 10x less data compared to current methods across multiple LLM families and QA tasks.

Conclusion: HCNR offers a data-efficient and practical approach for enhancing LLM honesty, facilitating safer and trustworthy deployment in high-stakes applications.

Abstract: The honesty of Large Language Models (LLMs) is increasingly important for safe deployment in high-stakes domains. However, this crucial trait is severely undermined by supervised fine-tuning (SFT), a common technique for model specialization. Existing recovery methods rely on data-intensive global parameter adjustments, implicitly assuming that SFT deeply corrupts the models' ability to recognize their knowledge boundaries. However, we observe that fine-tuned LLMs still preserve this ability; what is damaged is their capacity to faithfully express that awareness. Building on this, we propose Honesty-Critical Neurons Restoration (HCNR) to surgically repair this suppressed capacity. HCNR identifies and restores key expression-governing neurons to their pre-trained state while harmonizing them with task-oriented neurons via Hessian-guided compensation. Experiments on four QA tasks and five LLM families demonstrate that HCNR effectively recovers 33.25% of the compromised honesty while achieving at least 2.23x speedup with over 10x less data compared to baseline methods, offering a practical solution for trustworthy LLM deployment.

</details>


### [155] [AA-Omniscience: Evaluating Cross-Domain Knowledge Reliability in Large Language Models](https://arxiv.org/abs/2511.13029)
*Declan Jackson,William Keating,George Cameron,Micah Hill-Smith*

Main category: cs.CL

TL;DR: AA-Omniscience benchmark evaluates language models by measuring factual accuracy and knowledge calibration using 6,000 questions across 42 topics.


<details>
  <summary>Details</summary>
Motivation: To address the need for reliable domain-specific language models, capable of factual recall and identifying knowledge gaps.

Method: Developed the Omniscience Index (-100 to 100), rewarding models for correct answers and abstention when uncertain, penalizing hallucinations. Tested 6,000 questions from varied authoritative sources.

Result: Claude 4.1 Opus achieved the highest score (4.8), with only three models surpassing zero. Persistent weaknesses were found in factuality and calibration.

Conclusion: Performance variability highlights the importance of selecting models based on domain-specific demands rather than general capabilities for knowledge-intensive tasks.

Abstract: Existing language model evaluations primarily measure general capabilities, yet reliable use of these models across a range of domains demands factual accuracy and recognition of knowledge gaps. We introduce AA-Omniscience, a benchmark designed to measure both factual recall and knowledge calibration across 6,000 questions. Questions are derived from authoritative academic and industry sources, and cover 42 economically relevant topics within six different domains. The evaluation measures a model's Omniscience Index, a bounded metric (-100 to 100) measuring factual recall that jointly penalizes hallucinations and rewards abstention when uncertain, with 0 equating to a model that answers questions correctly as much as it does incorrectly. Among evaluated models, Claude 4.1 Opus attains the highest score (4.8), making it one of only three models to score above zero. These results reveal persistent factuality and calibration weaknesses across frontier models. Performance also varies by domain, with the models from three different research labs leading across the six domains. This performance variability suggests models should be chosen according to the demands of the use case rather than general performance for tasks where knowledge is important.

</details>


### [156] [How Good is BLI as an Alignment Measure: A Study in Word Embedding Paradigm](https://arxiv.org/abs/2511.13040)
*Kasun Wickramasinghe,Nisansa de Silva*

Main category: cs.CL

TL;DR: The paper evaluates bilingual lexicon induction (BLI) techniques to study alignment between embedding spaces, proposing new methods to address shortcomings in existing approaches.


<details>
  <summary>Details</summary>
Motivation: The study aims to assess the strengths and limitations of multilingual embeddings and alignment techniques in managing both high-resource and low-resource languages, questioning if multilingual embeddings outperform traditional techniques in all scenarios.

Method: Researchers analyze traditional embedding alignment, novel multilingual models, and combined alignment techniques, introducing a stem-based BLI method and vocabulary pruning to address limitations in measuring alignment.

Result: BLI techniques do not always measure true alignment. The stem-based method and vocabulary pruning improve alignment evaluation. Multilingual embeddings perform better in low-resource cases, while combined techniques excel in others.

Conclusion: The paper highlights that multilingual embeddings are not universally superior, and combined methods often provide better alignment. It calls for improved alignment evaluation methods for diverse language contexts.

Abstract: Sans a dwindling number of monolingual embedding studies originating predominantly from the low-resource domains, it is evident that multilingual embedding has become the de facto choice due to its adaptability to the usage of code-mixed languages, granting the ability to process multilingual documents in a language-agnostic manner, as well as removing the difficult task of aligning monolingual embeddings. But is this victory complete? Are the multilingual models better than aligned monolingual models in every aspect? Can the higher computational cost of multilingual models always be justified? Or is there a compromise between the two extremes? Bilingual Lexicon Induction is one of the most widely used metrics in terms of evaluating the degree of alignment between two embedding spaces. In this study, we explore the strengths and limitations of BLI as a measure to evaluate the degree of alignment of two embedding spaces. Further, we evaluate how well traditional embedding alignment techniques, novel multilingual models, and combined alignment techniques perform BLI tasks in the contexts of both high-resource and low-resource languages. In addition to that, we investigate the impact of the language families to which the pairs of languages belong. We identify that BLI does not measure the true degree of alignment in some cases and we propose solutions for them. We propose a novel stem-based BLI approach to evaluate two aligned embedding spaces that take into account the inflected nature of languages as opposed to the prevalent word-based BLI techniques. Further, we introduce a vocabulary pruning technique that is more informative in showing the degree of the alignment, especially performing BLI on multilingual embedding models. Often, combined embedding alignment techniques perform better while in certain cases multilingual embeddings perform better (mainly low-resource language cases).

</details>


### [157] [Spark-Prover-X1: Formal Theorem Proving Through Diverse Data Training](https://arxiv.org/abs/2511.13043)
*Xinyuan Zhou,Yi Lei,Xiaoyu Zhou,Jingyi Sun,Yu Zhu,Zhongyi Ye,Weitai Zhang,Quan Liu,Si Wei,Cong Liu*

Main category: cs.CL

TL;DR: Spark-Prover-X1, a 7B parameter model, enhances formal reasoning through a three-stage framework and a new benchmark dataset. It achieves state-of-the-art performance in automated theorem proving for similarly-sized models.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of limited diverse and high-quality formal language data for automated theorem proving by enabling lightweight LLMs to exhibit strong reasoning capabilities.

Method: The approach involved three stages: (1) pre-training on a mathematical corpus with novel data tasks like "CoT-augmented state prediction," (2) supervised fine-tuning within an expert iteration loop for specialization, and (3) Group Relative Policy Optimization for tackling challenging problems. Additionally, a new benchmark dataset, ExamFormal-Bench, was introduced for evaluation.

Result: Spark-Prover-X1-7B achieves a 37.0% average pass rate (pass@32), significantly outperforming similarly-sized models. It also excels on competitive tasks like PutnamBench and CombiBench.

Conclusion: The paper validates that utilizing diverse training data and a refined training pipeline effectively enhances the reasoning capabilities of lightweight LLMs, establishing Spark-Prover-X1 as a leading model for automated theorem proving.

Abstract: Large Language Models (LLMs) have shown significant promise in automated theorem proving, yet progress is often constrained by the scarcity of diverse and high-quality formal language data. To address this issue, we introduce Spark-Prover-X1, a 7B parameter model trained via an three-stage framework designed to unlock the reasoning potential of more accessible and moderately-sized LLMs. The first stage infuses deep knowledge through continuous pre-training on a broad mathematical corpus, enhanced by a suite of novel data tasks. Key innovation is a "CoT-augmented state prediction" task to achieve fine-grained reasoning. The second stage employs Supervised Fine-tuning (SFT) within an expert iteration loop to specialize both the Spark-Prover-X1-7B and Spark-Formalizer-X1-7B models. Finally, a targeted round of Group Relative Policy Optimization (GRPO) is applied to sharpen the prover's capabilities on the most challenging problems. To facilitate robust evaluation, particularly on problems from real-world examinations, we also introduce ExamFormal-Bench, a new benchmark dataset of 402 formal problems. Experimental results demonstrate that Spark-Prover-X1-7B achieves state-of-the-art performance among similarly-sized open-source models, attaining a 37.0\% average pass rate (pass@32). It shows exceptional performance on difficult competition benchmarks, notably solving 27 problems on PutnamBench (pass@32) and achieving 24.0\% on CombiBench (pass@32). Our work validates that this diverse training data and progressively refined training pipeline provides an effective path for enhancing the formal reasoning capabilities of lightweight LLMs. Both Spark-Prover-X1-7B and Spark-Formalizer-X1-7B, along with the ExamFormal-Bench dataset, are made publicly available at:https://www.modelscope.cn/organization/iflytek, https://gitcode.com/ifly_opensource.

</details>


### [158] [BeDiscovER: The Benchmark of Discourse Understanding in the Era of Reasoning Language Models](https://arxiv.org/abs/2511.13095)
*Chuyuan Li,Giuseppe Carenini*

Main category: cs.CL

TL;DR: BeDiscovER is a benchmark to evaluate discourse-level knowledge in modern LLMs, demonstrating their strengths and weaknesses.


<details>
  <summary>Details</summary>
Motivation: Motivated by the need to comprehensively evaluate the discourse-level abilities of modern language models.

Method: BeDiscovER compiles 52 datasets across 5 discourse tasks, spanning lexicon, sentential, and documental levels, to test discourse parsing, temporal relation extraction, and novel challenges like particle disambiguation.

Result: State-of-the-art LLMs excel in arithmetic temporal reasoning but struggle with full document reasoning and nuanced rhetorical relation recognition.

Conclusion: The benchmark underscores the gaps in discourse-level understanding and highlights areas of improvement for modern LLMs.

Abstract: We introduce BeDiscovER (Benchmark of Discourse Understanding in the Era of Reasoning Language Models), an up-to-date, comprehensive suite for evaluating the discourse-level knowledge of modern LLMs. BeDiscovER compiles 5 publicly available discourse tasks across discourse lexicon, (multi-)sentential, and documental levels, with in total 52 individual datasets. It covers both extensively studied tasks such as discourse parsing and temporal relation extraction, as well as some novel challenges such as discourse particle disambiguation (e.g., ``just''), and also aggregates a shared task on Discourse Relation Parsing and Treebanking for multilingual and multi-framework discourse relation classification. We evaluate open-source LLMs: Qwen3 series, DeepSeek-R1, and frontier model such as GPT-5-mini on BeDiscovER, and find that state-of-the-art models exhibit strong performance in arithmetic aspect of temporal reasoning, but they struggle with full document reasoning and some subtle semantic and discourse phenomena, such as rhetorical relation recognition.

</details>


### [159] [Evaluating the Ability of Large Language Models to Identify Adherence to CONSORT Reporting Guidelines in Randomized Controlled Trials: A Methodological Evaluation Study](https://arxiv.org/abs/2511.13107)
*Zhichao He,Mouxiao Bian,Jianhong Zhu,Jiayuan Chen,Yunqiu Wang,Wenxia Zhao,Tianbin Li,Bing Han,Jie Xu,Junyan Wu*

Main category: cs.CL

TL;DR: This paper evaluates how well large language models (LLMs) can assess CONSORT adherence in published randomized controlled trials (RCTs), finding that LLMs perform modestly and are currently unsuitable for replacing human reviewers.


<details>
  <summary>Details</summary>
Motivation: To assess the potential of large language models (LLMs) to automate the evaluation of CONSORT adherence in RCTs, addressing the time-consuming manual process of quality checks in research reporting.

Method: The authors constructed a dataset of 150 published RCTs across various medical specialties and tested LLMs, including top-performing models Gemini-2.5-Flash and DeepSeek-R1, in a zero-shot setting for their ability to classify adherence to individual CONSORT items.

Result: The best models achieved modest macro F1-scores (0.634) and Cohen’s Kappa coefficients (~0.28), showing fair agreement with expert consensus. Models were better at identifying compliant items but struggled with non-compliant and not applicable items.

Conclusion: LLMs hold promise as preliminary tools for CONSORT adherence checks, but their inconsistent performance, especially in detecting omissions or flaws, means they cannot yet replace expert human reviewers.

Abstract: The Consolidated Standards of Reporting Trials statement is the global benchmark for transparent and high-quality reporting of randomized controlled trials. Manual verification of CONSORT adherence is a laborious, time-intensive process that constitutes a significant bottleneck in peer review and evidence synthesis. This study aimed to systematically evaluate the accuracy and reliability of contemporary LLMs in identifying the adherence of published RCTs to the CONSORT 2010 statement under a zero-shot setting. We constructed a golden standard dataset of 150 published RCTs spanning diverse medical specialties. The primary outcome was the macro-averaged F1-score for the three-class classification task, supplemented by item-wise performance metrics and qualitative error analysis. Overall model performance was modest. The top-performing models, Gemini-2.5-Flash and DeepSeek-R1, achieved nearly identical macro F1 scores of 0.634 and Cohen's Kappa coefficients of 0.280 and 0.282, respectively, indicating only fair agreement with expert consensus. A striking performance disparity was observed across classes: while most models could identify compliant items with high accuracy (F1 score > 0.850), they struggled profoundly with identifying non-compliant and not applicable items, where F1 scores rarely exceeded 0.400. Notably, some high-profile models like GPT-4o underperformed, achieving a macro F1-score of only 0.521. LLMs show potential as preliminary screening assistants for CONSORT checks, capably identifying well-reported items. However, their current inability to reliably detect reporting omissions or methodological flaws makes them unsuitable for replacing human expertise in the critical appraisal of trial quality.

</details>


### [160] [Extracting Events Like Code: A Multi-Agent Programming Framework for Zero-Shot Event Extraction](https://arxiv.org/abs/2511.13118)
*Quanjiang Guo,Sijie Wang,Jinchuan Zhang,Ben Zhang,Zhao Kang,Ling Tian,Ke Yan*

Main category: cs.CL

TL;DR: This paper introduces Agent-Event-Coder (AEC), a novel framework for zero-shot event extraction using multi-agent collaboration. It treats event extraction as a structured code-generation process, achieving better schema consistency and performance.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenges in zero-shot event extraction, where large language models often produce incomplete, misclassified, or structurally invalid outputs due to the complex reasoning and domain knowledge required.

Method: The proposed AEC framework structures ZSEE into iterative subtasks: retrieval, planning, coding, and verification. It employs multiple specialized LLM agents, represents schemas as executable class definitions, and introduces systematic schema enforcement through iterative refinement.

Result: Experiments conducted across five domains and six large language models demonstrate that the AEC framework outperforms previous zero-shot event extraction baselines in precision, completeness, and schema-consistency.

Conclusion: The study shows the effectiveness of treating zero-shot event extraction as a code-generation process, achieving superior performance and releasing their methods and data for public use.

Abstract: Zero-shot event extraction (ZSEE) remains a significant challenge for large language models (LLMs) due to the need for complex reasoning and domain-specific understanding. Direct prompting often yields incomplete or structurally invalid outputs--such as misclassified triggers, missing arguments, and schema violations. To address these limitations, we present Agent-Event-Coder (AEC), a novel multi-agent framework that treats event extraction like software engineering: as a structured, iterative code-generation process. AEC decomposes ZSEE into specialized subtasks--retrieval, planning, coding, and verification--each handled by a dedicated LLM agent. Event schemas are represented as executable class definitions, enabling deterministic validation and precise feedback via a verification agent. This programming-inspired approach allows for systematic disambiguation and schema enforcement through iterative refinement. By leveraging collaborative agent workflows, AEC enables LLMs to produce precise, complete, and schema-consistent extractions in zero-shot settings. Experiments across five diverse domains and six LLMs demonstrate that AEC consistently outperforms prior zero-shot baselines, showcasing the power of treating event extraction like code generation. The code and data are released on https://github.com/UESTC-GQJ/Agent-Event-Coder.

</details>


### [161] [A Comparative Analysis of Recurrent and Attention Architectures for Isolated Sign Language Recognition](https://arxiv.org/abs/2511.13126)
*Nigar Alishzade,Gulchin Abdullayeva*

Main category: cs.CL

TL;DR: This study compares recurrent ConvLSTM and attention-based Vanilla Transformer models for sign language recognition, finding Transformers superior in accuracy but ConvLSTM more efficient.


<details>
  <summary>Details</summary>
Motivation: To systematically evaluate and compare neural architectures for improving isolated sign language recognition accuracy and efficiency.

Method: Implemented ConvLSTM and Vanilla Transformer models, tested on Azerbaijani Sign Language Dataset (AzSLD) and Word-Level American Sign Language (WLASL) datasets analyzing their strengths.

Result: Vanilla Transformer outperformed ConvLSTM in accuracy (76.8% Top-1 for AzSLD, 88.3% for WLASL). ConvLSTM lagged in accuracy but was computationally efficient.

Conclusion: Transformers deliver higher accuracy and signer independence while ConvLSTM balances efficiency; guiding architecture choice based on applications and constraints.

Abstract: This study presents a systematic comparative analysis of recurrent and attention-based neural architectures for isolated sign language recognition. We implement and evaluate two representative models-ConvLSTM and Vanilla Transformer-on the Azerbaijani Sign Language Dataset (AzSLD) and the Word-Level American Sign Language (WLASL) dataset. Our results demonstrate that the attention-based Vanilla Transformer consistently outperforms the recurrent ConvLSTM in both Top-1 and Top-5 accuracy across datasets, achieving up to 76.8% Top-1 accuracy on AzSLD and 88.3% on WLASL. The ConvLSTM, while more computationally efficient, lags in recognition accuracy, particularly on smaller datasets. These findings highlight the complementary strengths of each paradigm: the Transformer excels in overall accuracy and signer independence, whereas the ConvLSTM offers advantages in computational efficiency and temporal modeling. The study provides a nuanced analysis of these trade-offs, offering guidance for architecture selection in sign language recognition systems depending on application requirements and resource constraints.

</details>


### [162] [Zero-Shot Grammar Competency Estimation Using Large Language Model Generated Pseudo Labels](https://arxiv.org/abs/2511.13152)
*Sourya Dipta Das,Shubham Kumar,Kuldeep Yadav*

Main category: cs.CL

TL;DR: The paper proposes a zero-shot grammar competency estimation framework utilizing unlabeled data and Large Language Models (LLMs) to overcome challenges in spoken grammar assessment.


<details>
  <summary>Details</summary>
Motivation: To address challenges of assessing spoken grammar due to spontaneity, disfluency, and lack of annotated data, and to develop scalable low-resource grammar assessment.

Method: The framework uses LLM-generated pseudo labels on unlabeled data, employing grammar competency rubric-based prompts. A transformer model is trained using a novel approach to handle label noise effectively.

Result: The approach shows high accuracy in estimating grammar competency scores. It emphasizes the importance of LLM choice and balancing clean-to-noisy samples for stability and accuracy.

Conclusion: This framework demonstrates the potential for scalable, low-resource grammar assessment, offering robust and interpretable results while overcoming traditional limitations.

Abstract: Grammar competency estimation is essential for assessing linguistic proficiency in both written and spoken language; however, the spoken modality presents additional challenges due to its spontaneous, unstructured, and disfluent nature. Developing accurate grammar scoring models further requires extensive expert annotation, making large-scale data creation impractical. To address these limitations, we propose a zero-shot grammar competency estimation framework that leverages unlabeled data and Large Language Models (LLMs) without relying on manual labels. During training, we employ LLM-generated predictions on unlabeled data by using grammar competency rubric-based prompts. These predictions, treated as pseudo labels, are utilized to train a transformer-based model through a novel training framework designed to handle label noise effectively. We show that the choice of LLM for pseudo-label generation critically affects model performance and that the ratio of clean-to-noisy samples during training strongly influences stability and accuracy. Finally, a qualitative analysis of error intensity and score prediction confirms the robustness and interpretability of our approach. Experimental results demonstrate the efficacy of our approach in estimating grammar competency scores with high accuracy, paving the way for scalable, low-resource grammar assessment systems.

</details>


### [163] [Distinguishing Repetition Disfluency from Morphological Reduplication in Bangla ASR Transcripts: A Novel Corpus and Benchmarking Analysis](https://arxiv.org/abs/2511.13159)
*Zaara Zabeen Arpa,Sadnam Sakib Apurbo,Nazia Karim Khan Oishee,Ajwad Abrar*

Main category: cs.CL

TL;DR: The paper addresses ambiguities in Bangla ASR transcripts between repetition disfluency and morphological reduplication, introducing a specialized annotated corpus and benchmarking models to improve text normalization accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the challenge in low-resource language ASR (like Bangla), where distinguishing between disfluencies and valid linguistic constructs is critical for semantic integrity.

Method: The study introduces a 20,000-row annotated Bangla corpus and benchmarks multilingual LLMs and fine-tuned encoder models for distinguishing repetition disfluencies from morphological reduplications.

Result: Fine-tuned models perform better, with BanglaBERT achieving the best accuracy (84.78%) and F1 score (0.677), while LLMs show competitive results with few-shot prompting.

Conclusion: The annotated corpus and fine-tuned models successfully set a baseline, enabling further development of advanced Bangla text normalization systems that preserve semantics.

Abstract: Automatic Speech Recognition (ASR) transcripts, especially in low-resource languages like Bangla, contain a critical ambiguity: word-word repetitions can be either Repetition Disfluency (unintentional ASR error/hesitation) or Morphological Reduplication (a deliberate grammatical construct). Standard disfluency correction fails by erroneously deleting valid linguistic information. To solve this, we introduce the first publicly available, 20,000-row Bangla corpus, manually annotated to explicitly distinguish between these two phenomena in noisy ASR transcripts. We benchmark this novel resource using two paradigms: state-of-the-art multilingual Large Language Models (LLMs) and task-specific fine-tuning of encoder models. LLMs achieve competitive performance (up to 82.68\% accuracy) with few-shot prompting. However, fine-tuning proves superior, with the language-specific BanglaBERT model achieving the highest accuracy of 84.78\% and an F1 score of 0.677. This establishes a strong, linguistically-informed baseline and provides essential data for developing sophisticated, semantic-preserving text normalization systems for Bangla.

</details>


### [164] [TCM-5CEval: Extended Deep Evaluation Benchmark for LLM's Comprehensive Clinical Research Competence in Traditional Chinese Medicine](https://arxiv.org/abs/2511.13169)
*Tianai Huang,Jiayuan Chen,Lu Lu,Pengcheng Chen,Tianbin Li,Bing Han,Wenchao Tang,Jie Xu,Ming Li*

Main category: cs.CL

TL;DR: This study introduces TCM-5CEval, a comprehensive benchmark designed to evaluate LLMs in five key dimensions of Traditional Chinese Medicine. Analysis uncovered model limitations, including reasoning instability and sensitivity to positional bias.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop a granular benchmark for evaluating LLM capabilities in Traditional Chinese Medicine, addressing existing knowledge gaps and cultural-context alignment.

Method: The authors created TCM-5CEval, assessing models in five dimensions (Core Knowledge, Classical Literacy, Clinical Decision-making, Chinese Materia Medica, and Clinical Non-pharmacological Therapy). Evaluation of fifteen prominent LLMs was conducted, including permutation-based consistency testing.

Result: The assessment revealed performance disparities among LLMs, with top models identified. Models excelled in basic knowledge recollection but struggled in complex interpretative tasks. Positional bias significantly impacted reasoning stability.

Conclusion: TCM-5CEval serves as a diagnostic tool revealing LLM weaknesses in reasoning consistency within TCM tasks, promoting standardized research via the Medbench platform.

Abstract: Large language models (LLMs) have demonstrated exceptional capabilities in general domains, yet their application in highly specialized and culturally-rich fields like Traditional Chinese Medicine (TCM) requires rigorous and nuanced evaluation. Building upon prior foundational work such as TCM-3CEval, which highlighted systemic knowledge gaps and the importance of cultural-contextual alignment, we introduce TCM-5CEval, a more granular and comprehensive benchmark. TCM-5CEval is designed to assess LLMs across five critical dimensions: (1) Core Knowledge (TCM-Exam), (2) Classical Literacy (TCM-LitQA), (3) Clinical Decision-making (TCM-MRCD), (4) Chinese Materia Medica (TCM-CMM), and (5) Clinical Non-pharmacological Therapy (TCM-ClinNPT). We conducted a thorough evaluation of fifteen prominent LLMs, revealing significant performance disparities and identifying top-performing models like deepseek\_r1 and gemini\_2\_5\_pro. Our findings show that while models exhibit proficiency in recalling foundational knowledge, they struggle with the interpretative complexities of classical texts. Critically, permutation-based consistency testing reveals widespread fragilities in model inference. All evaluated models, including the highest-scoring ones, displayed a substantial performance degradation when faced with varied question option ordering, indicating a pervasive sensitivity to positional bias and a lack of robust understanding. TCM-5CEval not only provides a more detailed diagnostic tool for LLM capabilities in TCM but aldso exposes fundamental weaknesses in their reasoning stability. To promote further research and standardized comparison, TCM-5CEval has been uploaded to the Medbench platform, joining its predecessor in the "In-depth Challenge for Comprehensive TCM Abilities" special track.

</details>


### [165] [Translation Entropy: A Statistical Framework for Evaluating Translation Systems](https://arxiv.org/abs/2511.13180)
*Ronit D. Gross,Yanir Harel,Ido Kanter*

Main category: cs.CL

TL;DR: The paper introduces a quantitative method to estimate the translation entropy of language translators, thereby enabling objective benchmarking of their performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of quantitative and objective methods for assessing the performance of language translators, particularly in understanding the entropy within a language.

Method: The method analyzes how several sentences—differing by only one token from a pivot sentence—yield identical translations. By statistically assessing token replacement probabilities that preserve translations, the study estimates translation entropy.

Result: The study quantitatively ranks translators (MarianMT, T5-Base, and NLLB-200) and demonstrates that translation entropy is a measurable property. It also finds that replacing two tokens has a multiplicative effect on translation degeneracy.

Conclusion: Translation entropy can serve as an objective measure to evaluate artificial translators, providing a novel method to quantify and benchmark translation quality.

Abstract: The translation of written language has been known since the 3rd century BC; however, its necessity has become increasingly common in the information age. Today, many translators exist, based on encoder-decoder deep architectures, nevertheless, no quantitative objective methods are available to assess their performance, likely because the entropy of even a single language remains unknown. This study presents a quantitative method for estimating translation entropy, with the following key finding. Given a translator, several sentences that differ by only one selected token of a given pivot sentence yield identical translations. Analyzing the statistics of this phenomenon across an ensemble of such sentences, consisting each of a pivot selected token, yields the probabilities of replacing this specific token with others while preserving the translation. These probabilities constitute the entropy of the selected token, and the average across all selected pivot tokens provides an estimate of the translator's overall translation entropy, which is enhanced along the decoder blocks. This entropic measure allows for the quantitative ranking of several publicly available translators and reveals whether mutual translation entropy is symmetric. Extending the proposed method to include the replacement of two tokens in a given pivot sentence demonstrates a multiplicative effect, where translation degeneracy is proportional to the product of the degeneracies of the two tokens. These findings establish translation entropy as a measurable property and objective benchmarking of artificial translators. Results are based on MarianMT, T5-Base and NLLB-200 translators.

</details>


### [166] [Evaluating Large Language Models for Diacritic Restoration in Romanian Texts: A Comparative Study](https://arxiv.org/abs/2511.13182)
*Mihai Dan Nadas,Laura Diosan*

Main category: cs.CL

TL;DR: This study assesses the capability of various large language models (LLMs) to restore diacritical marks in Romanian texts, highlighting GPT-4o's high accuracy and variability in other models.


<details>
  <summary>Details</summary>
Motivation: Motivated by the necessity for effective diacritic restoration tools for languages like Romanian that use extensive diacritical marks, ensuring accuracy in text processing tasks.

Method: The study tested several LLMs, including OpenAI's GPT-3.5, GPT-4, GPT-4o, Google's Gemini 1.0 Pro, and Meta's Llama (2 & 3), among others, across different prompting strategies from zero-shot to multi-shot instructions using a comprehensive corpus.

Result: Models such as GPT-4o showed consistent and high performance in restoring diacritics, while Llama models presented significant variability in results.

Conclusion: Model architecture, training data, and prompt design play pivotal roles in diacritic restoration; future work can enhance NLP tools for diacritic-rich languages by addressing these factors.

Abstract: Automatic diacritic restoration is crucial for text processing in languages with rich diacritical marks, such as Romanian. This study evaluates the performance of several large language models (LLMs) in restoring diacritics in Romanian texts. Using a comprehensive corpus, we tested models including OpenAI's GPT-3.5, GPT-4, GPT-4o, Google's Gemini 1.0 Pro, Meta's Llama 2 and Llama 3, MistralAI's Mixtral 8x7B Instruct, airoboros 70B, and OpenLLM-Ro's RoLlama 2 7B, under multiple prompt templates ranging from zero-shot to complex multi-shot instructions. Results show that models such as GPT-4o achieve high diacritic restoration accuracy, consistently surpassing a neutral echo baseline, while others, including Meta's Llama family, exhibit wider variability. These findings highlight the impact of model architecture, training data, and prompt design on diacritic restoration performance and outline promising directions for improving NLP tools for diacritic-rich languages.

</details>


### [167] [Seeing isn't Hearing: Benchmarking Vision Language Models at Interpreting Spectrograms](https://arxiv.org/abs/2511.13225)
*Tyler Loakman,Joseph James,Chenghua Lin*

Main category: cs.CL

TL;DR: The study benchmarks the capabilities of Vision-Language Models (VLMs) in interpreting spectrograms and waveforms of speech, revealing their struggles in understanding this specialized task.


<details>
  <summary>Details</summary>
Motivation: To evaluate whether Vision-Language Models (VLMs) can interpret spectrograms and waveforms, akin to expert phoneticians, in vision-language fusion tasks.

Method: The authors synthesized a novel dataset of 4,000+ isolated spoken English words, paired with consistent spectrogram and waveform figures. They then tested VLMs through a multiple-choice task requiring phonemic or graphemic transcription predictions.

Result: The analysis shows that both zero-shot and fine-tuned VLMs perform near random chance, implying a lack of specialized knowledge in interpreting spectrograms despite paired samples.

Conclusion: VLMs require specific parametric knowledge to interpret complex speech-related figures, highlighting their limitations in such tasks.

Abstract: With the rise of Large Language Models (LLMs) and their vision-enabled counterparts (VLMs), numerous works have investigated their capabilities in tasks that fuse the modalities of vision and language. In this work, we benchmark the extent to which VLMs are able to act as highly-trained phoneticians, interpreting spectrograms and waveforms of speech. To do this, we synthesise a novel dataset containing 4k+ English words spoken in isolation alongside stylistically consistent spectrogram and waveform figures. We test the ability of VLMs to understand these representations of speech through a multiple-choice task whereby models must predict the correct phonemic or graphemic transcription of a spoken word when presented amongst 3 distractor transcriptions that have been selected based on their phonemic edit distance to the ground truth. We observe that both zero-shot and finetuned models rarely perform above chance, demonstrating the requirement for specific parametric knowledge of how to interpret such figures, rather than paired samples alone.

</details>


### [168] [Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance](https://arxiv.org/abs/2511.13254)
*Shalini Maiti,Amar Budhiraja,Bhavul Gauri,Gaurav Chaurasia,Anton Protopopov,Alexis Audran-Reiss,Michael Slater,Despoina Magka,Tatiana Shavrina,Roberta Raileanu,Yoram Bachrach*

Main category: cs.CL

TL;DR: The study introduces 'Soup Of Category Experts' (SoCE), a model souping method that uses category-specific expert models and weighted averaging to improve performance across various tasks.


<details>
  <summary>Details</summary>
Motivation: The training of Large Language Models (LLMs) is resource-intensive, and the paper seeks to enhance performance without expensive retraining using model souping techniques.

Method: SoCE uses benchmark categories and identifies expert models for weakly-correlated category clusters, applying non-uniform weights for averaging instead of uniform weights.

Result: The proposed method improves performance and robustness in multiple domains, such as multilingual capabilities, tool calling, and math, achieving state-of-the-art results on benchmarks like the Berkeley Function Calling Leaderboard.

Conclusion: SoCE optimizes model souping by leveraging category-specific expertise and weighted averaging, offering a more efficient and effective way to improve LLM performance.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but their training remains resource- and time-intensive, requiring massive compute power and careful orchestration of training procedures. Model souping-the practice of averaging weights from multiple models of the same architecture-has emerged as a promising pre- and post-training technique that can enhance performance without expensive retraining. In this paper, we introduce Soup Of Category Experts (SoCE), a principled approach for model souping that utilizes benchmark composition to identify optimal model candidates and applies non-uniform weighted averaging to maximize performance. Contrary to previous uniform-averaging approaches, our method leverages the observation that benchmark categories often exhibit low inter-correlations in model performance. SoCE identifies "expert" models for each weakly-correlated category cluster and combines them using optimized weighted averaging rather than uniform weights. We demonstrate that the proposed method improves performance and robustness across multiple domains, including multilingual capabilities, tool calling, and math and achieves state-of-the-art results on the Berkeley Function Calling Leaderboard.

</details>


### [169] [RegionMarker: A Region-Triggered Semantic Watermarking Framework for Embedding-as-a-Service Copyright Protection](https://arxiv.org/abs/2511.13329)
*Shufan Yang,Zifeng Cheng,Zhiwei Jiang,Yafeng Yin,Cong Wang,Shiping Ge,Yuchen Fu,Qing Gu*

Main category: cs.CL

TL;DR: The paper introduces RegionMarker, a watermarking framework safeguarding Embedding-as-a-Service (EaaS) against model extraction attacks and diverse adversarial methods.


<details>
  <summary>Details</summary>
Motivation: Current watermarking methods for EaaS are insufficient in combating multiple types of attacks, leading to concerns over copyright infringement and economic losses for providers.

Method: RegionMarker defines trigger regions in a low-dimensional space, embeds watermarks tied to these regions using a secret dimensionality reduction matrix, and random region selection to enhance robustness.

Result: Experiments demonstrate RegionMarker's effectiveness against multiple attack types, ensuring robust copyright protection for EaaS models.

Conclusion: RegionMarker provides comprehensive protection against watermark removal attacks, paraphrasing, and dimension-perturbation for EaaS, safeguarding model copyrights reliably.

Abstract: Embedding-as-a-Service (EaaS) is an effective and convenient deployment solution for addressing various NLP tasks. Nevertheless, recent research has shown that EaaS is vulnerable to model extraction attacks, which could lead to significant economic losses for model providers. For copyright protection, existing methods inject watermark embeddings into text embeddings and use them to detect copyright infringement. However, current watermarking methods often resist only a subset of attacks and fail to provide \textit{comprehensive} protection. To this end, we present the region-triggered semantic watermarking framework called RegionMarker, which defines trigger regions within a low-dimensional space and injects watermarks into text embeddings associated with these regions. By utilizing a secret dimensionality reduction matrix to project onto this subspace and randomly selecting trigger regions, RegionMarker makes it difficult for watermark removal attacks to evade detection. Furthermore, by embedding watermarks across the entire trigger region and using the text embedding as the watermark, RegionMarker is resilient to both paraphrasing and dimension-perturbation attacks. Extensive experiments on various datasets show that RegionMarker is effective in resisting different attack methods, thereby protecting the copyright of EaaS.

</details>


### [170] [AHaSIS: Shared Task on Sentiment Analysis for Arabic Dialects](https://arxiv.org/abs/2511.13335)
*Maram Alharbi,Salmane Chafik,Saad Ezzini,Ruslan Mitkov,Tharindu Ranasinghe,Hansi Hettiarachchi*

Main category: cs.CL

TL;DR: This paper introduces a shared task for sentiment detection across Arabic dialects in the hospitality sector, leveraging a curated multi-dialect dataset based on hotel reviews.


<details>
  <summary>Details</summary>
Motivation: The growing reliance on customer feedback in the Arab hospitality industry necessitates advanced Arabic sentiment analysis tools tailored for different dialects.

Method: The study uses a manually created dataset of hotel reviews translated into Saudi and Moroccan dialects from Modern Standard Arabic, validated by native speakers for linguistic and sentiment accuracy.

Result: A shared task attracted over 40 teams, with 12 submitted systems during evaluation; the best-performing system achieved an F1 score of 0.81.

Conclusion: The study provides a valuable dataset and highlights the feasibility of multi-dialect sentiment analysis while outlining challenges in developing such systems.

Abstract: The hospitality industry in the Arab world increasingly relies on customer feedback to shape services, driving the need for advanced Arabic sentiment analysis tools. To address this challenge, the Sentiment Analysis on Arabic Dialects in the Hospitality Domain shared task focuses on Sentiment Detection in Arabic Dialects. This task leverages a multi-dialect, manually curated dataset derived from hotel reviews originally written in Modern Standard Arabic (MSA) and translated into Saudi and Moroccan (Darija) dialects. The dataset consists of 538 sentiment-balanced reviews spanning positive, neutral, and negative categories. Translations were validated by native speakers to ensure dialectal accuracy and sentiment preservation. This resource supports the development of dialect-aware NLP systems for real-world applications in customer experience analysis. More than 40 teams have registered for the shared task, with 12 submitting systems during the evaluation phase. The top-performing system achieved an F1 score of 0.81, demonstrating the feasibility and ongoing challenges of sentiment analysis across Arabic dialects.

</details>


### [171] [Donors and Recipients: On Asymmetric Transfer Across Tasks and Languages with Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2511.13368)
*Kajetan Dymkiewicz,Ivan Vulic,Helen Yannakoudakis,Eilam Shapira,Roi Reichart,Anna Korhonen*

Main category: cs.CL

TL;DR: This paper investigates how improvements in one task or language using large language models (LLMs) transfer to other tasks or languages, focusing on controlled fine-tuning and transfer evaluation.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand the cross-task and cross-language transfer effects in LLMs, identifying how task or language improvements influence performance in other areas.

Method: Through controlled PEFT/LoRA fine-tuning, models of various families and sizes are individually trained on a single task-language source, and their transfer performance is measured across multiple task-language target pairs.

Result: Two key findings are reported: (1) Matched-Task (Cross-Language) transfer tends to be positive, whereas off-task transfer yields potential degradation; and (2) a donor-recipient transfer structure emerges across languages and tasks.

Conclusion: The study highlights considerations for optimizing fine-tuning strategies and specialising LLMs with risk-aware approaches to minimize negative cross-task and cross-language impacts.

Abstract: Large language models (LLMs) perform strongly across tasks and languages, yet how improvements in one task or language affect other tasks and languages and their combinations remains poorly understood. We conduct a controlled PEFT/LoRA study across multiple open-weight LLM families and sizes, treating task and language as transfer axes while conditioning on model family and size; we fine-tune each model on a single task-language source and measure transfer as the percentage-point change versus its baseline score when evaluated on all other task-language target pairs. We decompose transfer into (i) Matched-Task (Cross-Language), (ii) Matched-Language (Cross-Task), and (iii) Cross-Task (Cross-Language) regimes. We uncover two consistent general patterns. First, a pronounced on-task vs. off-task asymmetry: Matched-Task (Cross-Language) transfer is reliably positive, whereas off-task transfer often incurs collateral degradation. Second, a stable donor-recipient structure across languages and tasks (hub donors vs. brittle recipients). We outline implications for risk-aware fine-tuning and model specialisation.

</details>


### [172] [Can Large Language Models Function as Qualified Pediatricians? A Systematic Evaluation in Real-World Clinical Contexts](https://arxiv.org/abs/2511.13381)
*Siyu Zhu,Mouxiao Bian,Yue Xie,Yongyu Tang,Zhikang Yu,Tianbin Li,Pengcheng Chen,Bing Han,Jie Xu,Xiaoyan Dong*

Main category: cs.CL

TL;DR: PEDIASBench evaluates LLMs in pediatric clinical environments, highlighting strengths in foundational knowledge but weaknesses in complex reasoning, dynamic decision-making, and humanistic care.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to explore the ability of large language models (LLMs) to act as pediatricians in real clinical settings and identify their limitations and areas for improvement.

Method: The study introduces PEDIASBench, a framework assessing 12 LLMs across knowledge application, dynamic diagnosis and treatment, and pediatric safety and ethics, spanning 19 subspecialties and 211 diseases.

Result: State-of-the-art models excelled in foundational knowledge but showed limitations in handling complex tasks, real-time patient updates, and humanistic sensitivity.

Conclusion: Current models are inadequate for independent pediatric care but hold promise for decision support, education, and communication, requiring improvements in safety, interpretability, and multimodal integration.

Abstract: With the rapid rise of large language models (LLMs) in medicine, a key question is whether they can function as competent pediatricians in real-world clinical settings. We developed PEDIASBench, a systematic evaluation framework centered on a knowledge-system framework and tailored to realistic clinical environments. PEDIASBench assesses LLMs across three dimensions: application of basic knowledge, dynamic diagnosis and treatment capability, and pediatric medical safety and medical ethics. We evaluated 12 representative models released over the past two years, including GPT-4o, Qwen3-235B-A22B, and DeepSeek-V3, covering 19 pediatric subspecialties and 211 prototypical diseases. State-of-the-art models performed well on foundational knowledge, with Qwen3-235B-A22B achieving over 90% accuracy on licensing-level questions, but performance declined ~15% as task complexity increased, revealing limitations in complex reasoning. Multiple-choice assessments highlighted weaknesses in integrative reasoning and knowledge recall. In dynamic diagnosis and treatment scenarios, DeepSeek-R1 scored highest in case reasoning (mean 0.58), yet most models struggled to adapt to real-time patient changes. On pediatric medical ethics and safety tasks, Qwen2.5-72B performed best (accuracy 92.05%), though humanistic sensitivity remained limited. These findings indicate that pediatric LLMs are constrained by limited dynamic decision-making and underdeveloped humanistic care. Future development should focus on multimodal integration and a clinical feedback-model iteration loop to enhance safety, interpretability, and human-AI collaboration. While current LLMs cannot independently perform pediatric care, they hold promise for decision support, medical education, and patient communication, laying the groundwork for a safe, trustworthy, and collaborative intelligent pediatric healthcare system.

</details>


### [173] [Mem-PAL: Towards Memory-based Personalized Dialogue Assistants for Long-term User-Agent Interaction](https://arxiv.org/abs/2511.13410)
*Zhaopei Huang,Qifeng Dai,Guozheng Wu,Xiaopeng Wu,Kehan Chen,Chuan Yu,Xubin Li,Tiezheng Ge,Wenxuan Wang,Qin Jin*

Main category: cs.CL

TL;DR: The paper introduces PAL-Bench, a benchmark for assessing personalization in long-term user-agent interactions. It develops PAL-Set and proposes H$^2$Memory to enhance personalized response generation.


<details>
  <summary>Details</summary>
Motivation: Existing dialogue assistant approaches struggle to handle long-term interactions or capture subjective user traits effectively.

Method: PAL-Bench evaluates personalization by using PAL-Set, a synthesized dataset verified by human annotators. H$^2$Memory, a hierarchical and heterogeneous memory framework, enhances personalized responses through retrieval-augmented generation.

Result: PAL-Bench demonstrates the efficacy of the proposed framework on both its own dataset and an external one, showing improved personalization.

Conclusion: PAL-Bench and H$^2$Memory offer significant advancements in personalized dialogue systems, addressing gaps in long-term user-agent interactions and subjective trait capturing.

Abstract: With the rise of smart personal devices, service-oriented human-agent interactions have become increasingly prevalent. This trend highlights the need for personalized dialogue assistants that can understand user-specific traits to accurately interpret requirements and tailor responses to individual preferences. However, existing approaches often overlook the complexities of long-term interactions and fail to capture users' subjective characteristics. To address these gaps, we present PAL-Bench, a new benchmark designed to evaluate the personalization capabilities of service-oriented assistants in long-term user-agent interactions. In the absence of available real-world data, we develop a multi-step LLM-based synthesis pipeline, which is further verified and refined by human annotators. This process yields PAL-Set, the first Chinese dataset comprising multi-session user logs and dialogue histories, which serves as the foundation for PAL-Bench. Furthermore, to improve personalized service-oriented interactions, we propose H$^2$Memory, a hierarchical and heterogeneous memory framework that incorporates retrieval-augmented generation to improve personalized response generation. Comprehensive experiments on both our PAL-Bench and an external dataset demonstrate the effectiveness of the proposed memory framework.

</details>


### [174] [Non-Linear Scoring Model for Translation Quality Evaluation](https://arxiv.org/abs/2511.13467)
*Serge Gladkoff,Lifeng Han,Katerina Gasova*

Main category: cs.CL

TL;DR: The paper introduces a non-linear translation quality evaluation model to address biases in traditional linear models, improving fairness and accuracy across variable sample lengths.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the biases and inaccuracies in translation quality evaluation caused by traditional linear error-to-penalty systems, particularly for samples of varying lengths, which misaligns with human perception.

Method: The authors propose a calibrated, non-linear scoring model based on a logarithmic function, drawing from psychophysical and cognitive theories such as the Weber-Fechner law. This model adjusts error tolerance dynamically and is calibrated from two reference points using a simple computational step.

Result: The new model demonstrates improved interpretability, fairness, and inter-rater reliability, aligning better with human judgment in large-scale enterprise environments. It outperforms linear models in accuracy and scalability for both human and AI-generated translations.

Conclusion: The paper advances translation quality evaluation by introducing a perceptually valid scoring paradigm that integrates into existing workflows and supports scalable, human-aligned evaluation of translations. It opens new avenues for evaluating both human and AI-generated texts efficiently.

Abstract: Analytic Translation Quality Evaluation (TQE), based on Multidimensional Quality Metrics (MQM), traditionally uses a linear error-to-penalty scale calibrated to a reference sample of 1000-2000 words. However, linear extrapolation biases judgment on samples of different sizes, over-penalizing short samples and under-penalizing long ones, producing misalignment with expert intuition.
  Building on the Multi-Range framework, this paper presents a calibrated, non-linear scoring model that better reflects how human content consumers perceive translation quality across samples of varying length. Empirical data from three large-scale enterprise environments shows that acceptable error counts grow logarithmically, not linearly, with sample size.
  Psychophysical and cognitive evidence, including the Weber-Fechner law and Cognitive Load Theory, supports this premise by explaining why the perceptual impact of additional errors diminishes while the cognitive burden grows with scale. We propose a two-parameter model
  E(x) = a * ln(1 + b * x), a, b > 0,
  anchored to a reference tolerance and calibrated from two tolerance points using a one-dimensional root-finding step. The model yields an explicit interval within which the linear approximation stays within +/-20 percent relative error and integrates into existing evaluation workflows with only a dynamic tolerance function added.
  The approach improves interpretability, fairness, and inter-rater reliability across both human and AI-generated translations. By operationalizing a perceptually valid scoring paradigm, it advances translation quality evaluation toward more accurate and scalable assessment. The model also provides a stronger basis for AI-based document-level evaluation aligned with human judgment. Implementation considerations for CAT/LQA systems and implications for human and AI-generated text evaluation are discussed.

</details>


### [175] [Aspect-Level Obfuscated Sentiment in Thai Financial Disclosures and Its Impact on Abnormal Returns](https://arxiv.org/abs/2511.13481)
*Attapol T. Rutherford,Sirisak Chueykamhang,Thachaparn Bunditlurdruk,Nanthicha Angsuwichitkul*

Main category: cs.CL

TL;DR: The paper focuses on addressing obfuscated sentiment in Thai financial reports through Aspect-Based Sentiment Analysis (ABSA), providing annotated datasets and benchmarking text models to link sentiment to market reactions.


<details>
  <summary>Details</summary>
Motivation: Financial documents use ambiguous language to positively influence sentiment, making accurate market sentiment assessment difficult.

Method: The study employs Aspect-Based Sentiment Analysis (ABSA), annotates over 100 Thai financial reports, benchmarks text classification models, and evaluates market sentiment via stock price event studies.

Result: The approach achieves strong sentiment classification performance and observes selective market reactions to specific aspects within the financial reports.

Conclusion: Understanding obfuscated sentiment in financial texts is crucial for accurate sentiment analysis and has direct implications on market behavior.

Abstract: Understanding sentiment in financial documents is crucial for gaining insights into market behavior. These reports often contain obfuscated language designed to present a positive or neutral outlook, even when underlying conditions may be less favorable. This paper presents a novel approach using Aspect-Based Sentiment Analysis (ABSA) to decode obfuscated sentiment in Thai financial annual reports. We develop specific guidelines for annotating obfuscated sentiment in these texts and annotate more than one hundred financial reports. We then benchmark various text classification models on this annotated dataset, demonstrating strong performance in sentiment classification. Additionally, we conduct an event study to evaluate the real-world implications of our sentiment analysis on stock prices. Our results suggest that market reactions are selectively influenced by specific aspects within the reports. Our findings underscore the complexity of sentiment analysis in financial texts and highlight the importance of addressing obfuscated language to accurately assess market sentiment.

</details>


### [176] [Applying Large Language Models to Characterize Public Narratives](https://arxiv.org/abs/2511.13505)
*Elinor Poole-Dayan,Daniel T Kessler,Hannah Chiou,Margaret Hughes,Emily S Lin,Marshall Ganz,Deb Roy*

Main category: cs.CL

TL;DR: The paper introduces a computational framework that utilizes large language models (LLMs) to automate the analysis of public narratives, achieving near-human performance and providing scalable narrative analysis with applications to political rhetoric.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges of subjectivity and high costs associated with analyzing public narratives, which are essential for leadership and civic mobilization, by leveraging computational tools.

Method: The proposed method employs LLMs to automate the qualitative annotation of public narratives using a subject-matter-expert co-developed codebook, benchmarking performance against human annotators.

Result: LLMs achieved a near-human expert F1 score of 0.80 across 8 narratives and 14 codes. The study also analyzed a larger dataset of 22 stories and applied the findings to analyze political speeches.

Conclusion: The study highlights the potential of LLM-based annotation for scalable narrative analysis and proposes future research directions, while emphasizing its limitations in computational civic storytelling.

Abstract: Public Narratives (PNs) are key tools for leadership development and civic mobilization, yet their systematic analysis remains challenging due to their subjective interpretation and the high cost of expert annotation. In this work, we propose a novel computational framework that leverages large language models (LLMs) to automate the qualitative annotation of public narratives. Using a codebook we co-developed with subject-matter experts, we evaluate LLM performance against that of expert annotators. Our work reveals that LLMs can achieve near-human-expert performance, achieving an average F1 score of 0.80 across 8 narratives and 14 codes. We then extend our analysis to empirically explore how PN framework elements manifest across a larger dataset of 22 stories. Lastly, we extrapolate our analysis to a set of political speeches, establishing a novel lens in which to analyze political rhetoric in civic spaces. This study demonstrates the potential of LLM-assisted annotation for scalable narrative analysis and highlights key limitations and directions for future research in computational civic storytelling.

</details>


### [177] [Toward Conversational Hungarian Speech Recognition: Introducing the BEA-Large and BEA-Dialogue Datasets](https://arxiv.org/abs/2511.13529)
*Máté Gedeon,Piroska Zsófia Barta,Péter Mihajlik,Tekla Etelka Gráczi,Anna Kohári,Katalin Mády*

Main category: cs.CL

TL;DR: The paper introduces Hungarian speech datasets BEA-Large and BEA-Dialogue, aiming to improve ASR for Hungarian with reproducible baselines and challenges, including conversational complexities.


<details>
  <summary>Details</summary>
Motivation: Languages like Hungarian are underrepresented in ASR research due to the lack of adequate datasets for spontaneous and conversational speech.

Method: Creation of two new datasets (BEA-Large and BEA-Dialogue) with metadata, and establishment of baseline ASR models, including error rate evaluation.

Result: BEA-Large dataset includes 255 hours, and BEA-Dialogue includes 85 hours of Hungarian speech. Baseline ASR models achieved word error rates of 14.18% for spontaneous speech and 4.8% for repeated speech. Diarization error rates ranged between 13.05%-18.26%.

Conclusion: The datasets and baselines aim to improve underrepresented languages like Hungarian in ASR by addressing challenges inherent in conversational ASR, thus advancing speech technology for these languages.

Abstract: The advancement of automatic speech recognition (ASR) has been largely enhanced by extensive datasets in high-resource languages, while languages such as Hungarian remain underrepresented due to limited spontaneous and conversational corpora. To address this gap, we introduce two new datasets -- BEA-Large and BEA-Dialogue -- constructed from the previously unprocessed portions of the Hungarian speech corpus named BEA. BEA-Large extends BEA-Base with 255 hours of spontaneous speech from 433 speakers, enriched with detailed segment-level metadata. BEA-Dialogue, comprising 85 hours of spontaneous conversations, is a Hungarian speech corpus featuring natural dialogues partitioned into speaker-independent subsets, supporting research in conversational ASR and speaker diarization. We establish reproducible baselines on these datasets using publicly available ASR models, with the fine-tuned Fast Conformer model achieving word error rates as low as 14.18\% on spontaneous and 4.8\% on repeated speech. Diarization experiments yield diarization error rates between 13.05\% and 18.26\%, providing reference points for future improvements. The results highlight the persistent difficulty of conversational ASR, particularly due to disfluencies, overlaps, and informal speech patterns. By releasing these datasets and baselines, we aim to advance Hungarian speech technology and offer a methodological framework for developing spontaneous and conversational benchmarks in other languages.

</details>


### [178] [Beyond SELECT: A Comprehensive Taxonomy-Guided Benchmark for Real-World Text-to-SQL Translation](https://arxiv.org/abs/2511.13590)
*Hao Wang,Yuanfeng Song,Xiaoming Yin,Xing Chen*

Main category: cs.CL

TL;DR: The paper introduces a new taxonomy for text-to-SQL classification, evaluates existing datasets, and creates a more diverse dataset, SQL-Synth, using this taxonomy and large language models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations in coverage and diversity of existing text-to-SQL datasets, which fail to represent the complexity of real-world applications.

Method: The researchers developed a novel taxonomy for text-to-SQL classification and applied it to evaluate existing datasets. They then synthesized a new dataset, SQL-Synth, using the taxonomy and large language models.

Result: The SQL-Synth dataset demonstrated greater diversity and coverage compared to existing benchmarks. The study also found that existing LLMs underperform on SQL-Synth without fine-tuning.

Conclusion: The proposed taxonomy advances the analysis of text-to-SQL datasets and LLM performance, providing a valuable framework for improving dataset diversity and guiding LLM training.

Abstract: Text-to-SQL datasets are essential for training and evaluating text-to-SQL models, but existing datasets often suffer from limited coverage and fail to capture the diversity of real-world applications. To address this, we propose a novel taxonomy for text-to-SQL classification based on dimensions including core intents, statement types, syntax structures, and key actions. Using this taxonomy, we evaluate widely used public text-to-SQL datasets (e.g., Spider and Bird) and reveal limitations in their coverage and diversity. We then introduce a taxonomy-guided dataset synthesis pipeline, yielding a new dataset named SQL-Synth. This approach combines the taxonomy with Large Language Models (LLMs) to ensure the dataset reflects the breadth and complexity of real-world text-to-SQL applications. Extensive analysis and experimental results validate the effectiveness of our taxonomy, as SQL-Synth exhibits greater diversity and coverage compared to existing benchmarks. Moreover, we uncover that existing LLMs typically fall short in adequately capturing the full range of scenarios, resulting in limited performance on SQL-Synth. However, fine-tuning can substantially improve their performance in these scenarios. The proposed taxonomy has significant potential impact, as it not only enables comprehensive analysis of datasets and the performance of different LLMs, but also guides the construction of training data for LLMs.

</details>


### [179] [Omni Memory System for Personalized, Long Horizon, Self-Evolving Agents](https://arxiv.org/abs/2511.13593)
*Piaohong Wang,Motong Tian,Jiaxian Li,Yuan Liang,Yuqing Wang,Qianben Chen,Tiannan Wang,Zhicong Lu,Jiawei Ma,Yuchen Eleanor Jiang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: This paper introduces O-Mem, a novel memory framework for LLM-powered agents, improving personalization and efficiency in long-term interactions.


<details>
  <summary>Details</summary>
Motivation: To address challenges in maintaining contextual consistency and personalization during long-term interactions with LLM-powered agents.

Method: The authors designed O-Mem, a memory framework that utilizes active user profiling to dynamically extract and update user characteristics and records. It supports hierarchical retrieval of persona attributes and context.

Result: O-Mem outperformed previous state-of-the-art frameworks, achieving 51.76% on the LoCoMo benchmark and 62.99% on PERSONAMEM, with improved token and response time efficiency.

Conclusion: O-Mem demonstrates significant improvements in memory management and personalization, paving the way for more adaptive and efficient AI assistants.

Abstract: Recent advancements in LLM-powered agents have demonstrated significant potential in generating human-like responses; however, they continue to face challenges in maintaining long-term interactions within complex environments, primarily due to limitations in contextual consistency and dynamic personalization. Existing memory systems often depend on semantic grouping prior to retrieval, which can overlook semantically irrelevant yet critical user information and introduce retrieval noise. In this report, we propose the initial design of O-Mem, a novel memory framework based on active user profiling that dynamically extracts and updates user characteristics and event records from their proactive interactions with agents. O-Mem supports hierarchical retrieval of persona attributes and topic-related context, enabling more adaptive and coherent personalized responses. O-Mem achieves 51.76% on the public LoCoMo benchmark, a nearly 3% improvement upon LangMem,the previous state-of-the-art, and it achieves 62.99% on PERSONAMEM, a 3.5% improvement upon A-Mem,the previous state-of-the-art. O-Mem also boosts token and interaction response time efficiency compared to previous memory frameworks. Our work opens up promising directions for developing efficient and human-like personalized AI assistants in the future.

</details>


### [180] [Why is "Chicago" Predictive of Deceptive Reviews? Using LLMs to Discover Language Phenomena from Lexical Cues](https://arxiv.org/abs/2511.13658)
*Jiaming Qu,Mengtian Guo,Yue Wang*

Main category: cs.CL

TL;DR: The paper uses large language models (LLMs) to translate machine learning-derived features into understandable language phenomena for identifying deceptive online reviews.


<details>
  <summary>Details</summary>
Motivation: Deceptive reviews harm consumers and businesses, and traditional machine learning classifiers, despite being effective, lack human interpretability in distinguishing fake reviews.

Method: The authors use LLMs to convert machine-learned lexical features into comprehensible language phenomena for better understanding deceptive reviews.

Result: The study found that LLM-derived language phenomena are empirically grounded, generalizable across domains, and highly predictive compared to prior or in-context LLM knowledge.

Conclusion: The proposed language phenomena can assist in credibility assessment of online reviews where deception detection systems aren't accessible.

Abstract: Deceptive reviews mislead consumers, harm businesses, and undermine trust in online marketplaces. Machine learning classifiers can learn from large amounts of training examples to effectively distinguish deceptive reviews from genuine ones. However, the distinguishing features learned by these classifiers are often subtle, fragmented, and difficult for humans to interpret. In this work, we explore using large language models (LLMs) to translate machine-learned lexical cues into human-understandable language phenomena that can differentiate deceptive reviews from genuine ones. We show that language phenomena obtained in this manner are empirically grounded in data, generalizable across similar domains, and more predictive than phenomena either in LLMs' prior knowledge or obtained through in-context learning. These language phenomena have the potential to aid people in critically assessing the credibility of online reviews in environments where deception detection classifiers are unavailable.

</details>


### [181] [HAPO: Training Language Models to Reason Concisely via History-Aware Policy Optimization](https://arxiv.org/abs/2505.11225)
*Chengyu Huang,Zhengxin Zhang,Claire Cardie*

Main category: cs.CL

TL;DR: HAPO introduces historical information to optimize large language models for conciseness and correctness simultaneously, achieving notable length reductions while maintaining reasonable accuracy.


<details>
  <summary>Details</summary>
Motivation: Improve efficiency of large language models (LLMs) during test-time by reducing verbose responses and optimizing solution conciseness over time.

Method: HAPO employs History-Aware Policy Optimization, tracking a history state for each problem and applying a reward function combining length and correctness incentives to progressively identify concise and correct responses.

Result: Experiments demonstrated HAPO reduced response lengths by 33-59% while only slightly affecting accuracy (2-5%).

Conclusion: HAPO effectively enhances LLM efficiency by balancing reasoning accuracy and response brevity, leveraging historical knowledge for incremental improvements.

Abstract: While scaling the length of responses at test-time has been shown to markedly improve the reasoning abilities and performance of large language models (LLMs), it often results in verbose outputs and increases inference cost. Prior approaches for efficient test-time scaling, typically using universal budget constraints or query-level length optimization, do not leverage historical information from previous encounters with the same problem during training. We hypothesize that this limits their ability to progressively make solutions more concise over time. To address this, we present History-Aware Policy Optimization (HAPO), which keeps track of a history state (e.g., the minimum length over previously generated correct responses) for each problem. HAPO employs a novel length reward function based on this history state to incentivize the discovery of correct solutions that are more concise than those previously found. Crucially, this reward structure avoids overly penalizing shorter incorrect responses with the goal of facilitating exploration towards more efficient solutions. By combining this length reward with a correctness reward, HAPO jointly optimizes for correctness and efficiency. We use HAPO to train DeepSeek-R1-Distill-Qwen-1.5B, DeepScaleR-1.5B-Preview, and Qwen-2.5-1.5B-Instruct, and evaluate HAPO on several math benchmarks that span various difficulty levels. Experiment results demonstrate that HAPO effectively induces LLMs' concise reasoning abilities, producing length reductions of 33-59% with accuracy drops of only 2-5%.

</details>


### [182] [Crossing Borders: A Multimodal Challenge for Indian Poetry Translation and Image Generation](https://arxiv.org/abs/2511.13689)
*Sofia Jamil,Kotla Sai Charan,Sriparna Saha,Koustava Goswami,Joseph K J*

Main category: cs.CL

TL;DR: The paper introduces the TAI framework, combining translation and image generation to make Indian-language poetry more accessible globally.


<details>
  <summary>Details</summary>
Motivation: To enhance accessibility and global appreciation of Indian poetry, which faces challenges in comprehension due to its linguistic and cultural complexity.

Method: The TAI framework employs a translation module using an Odds Ratio Preference Alignment Algorithm and an image-generation module using a semantic graph to visualize poetic metaphors.

Result: The framework demonstrated superior performance in image generation tasks and introduced a unique Morphologically Rich Indian Language Poems dataset with 1,570 poems in 21 languages.

Conclusion: TAI framework effectively bridges gaps in poetry translation and visual comprehension, supporting equitable education and enriching reader experiences for culturally rich poetry.

Abstract: Indian poetry, known for its linguistic complexity and deep cultural resonance, has a rich and varied heritage spanning thousands of years. However, its layered meanings, cultural allusions, and sophisticated grammatical constructions often pose challenges for comprehension, especially for non-native speakers or readers unfamiliar with its context and language. Despite its cultural significance, existing works on poetry have largely overlooked Indian language poems. In this paper, we propose the Translation and Image Generation (TAI) framework, leveraging Large Language Models (LLMs) and Latent Diffusion Models through appropriate prompt tuning. Our framework supports the United Nations Sustainable Development Goals of Quality Education (SDG 4) and Reduced Inequalities (SDG 10) by enhancing the accessibility of culturally rich Indian-language poetry to a global audience. It includes (1) a translation module that uses an Odds Ratio Preference Alignment Algorithm to accurately translate morphologically rich poetry into English, and (2) an image generation module that employs a semantic graph to capture tokens, dependencies, and semantic relationships between metaphors and their meanings, to create visually meaningful representations of Indian poems. Our comprehensive experimental evaluation, including both human and quantitative assessments, demonstrates the superiority of TAI Diffusion in poem image generation tasks, outperforming strong baselines. To further address the scarcity of resources for Indian-language poetry, we introduce the Morphologically Rich Indian Language Poems MorphoVerse Dataset, comprising 1,570 poems across 21 low-resource Indian languages. By addressing the gap in poetry translation and visual comprehension, this work aims to broaden accessibility and enrich the reader's experience.

</details>


### [183] [DCRM: A Heuristic to Measure Response Pair Quality in Preference Optimization](https://arxiv.org/abs/2506.14157)
*Chengyu Huang,Tanya Goyal*

Main category: cs.CL

TL;DR: The paper introduces DCRM, a metric for assessing preference response pairs, showing higher DCRM correlates with improved preference optimization outcomes and proposes a method to enhance dataset quality.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve preference optimization (PO) performance by addressing differences in preferred and dispreferred responses in datasets, which may hinder learning desirable distinctions.

Method: Develop the DCRM metric to evaluate response-pair quality based on distance and reward margin, and propose a best-of-$N^2$ pairing method to select superior training pairs.

Result: Higher DCRM correlates with improved PO model outcomes, and the pairing method refines training sets, enhancing performance on benchmarks like AlpacaEval and MT-Bench.

Conclusion: Refining preference datasets using DCRM improves preference optimization model performance, highlighting the importance of response-pair quality in training sets.

Abstract: Recent research has attempted to associate preference optimization (PO) performance with the underlying preference datasets. In this work, our observation is that the differences between the preferred response $y^+$ and dispreferred response $y^-$ influence what LLMs can learn, which may not match the desirable differences to learn. Therefore, we use distance and reward margin to quantify these differences, and combine them to get Distance Calibrated Reward Margin (DCRM), a metric that measures the quality of a response pair for PO. Intuitively, DCRM encourages minimal noisy differences and maximal desired differences. With this, we study 3 types of commonly used preference datasets, classified along two axes: the source of the responses and the preference labeling function. We establish a general correlation between higher DCRM of the training set and better learning outcome. Inspired by this, we propose a best-of-$N^2$ pairing method that selects response pairs with the highest DCRM. Empirically, in various settings, our method produces training datasets that can further improve models' performance on AlpacaEval, MT-Bench, and Arena-Hard over the existing training sets.

</details>


### [184] [Generalist Foundation Models Are Not Clinical Enough for Hospital Operations](https://arxiv.org/abs/2511.13703)
*Lavender Y. Jiang,Angelica Chen,Xu Han,Xujin Chris Liu,Radhika Dua,Kevin Eaton,Frederick Wolff,Robert Steele,Jeff Zhang,Anton Alyakin,Qingkai Pan,Yanbing Chen,Karl L. Sangwon,Daniel A. Alber,Jaden Stryker,Jin Vivian Lee,Yindalon Aphinyanaphongs,Kyunghyun Cho,Eric Karl Oermann*

Main category: cs.CL

TL;DR: Lang1, a specialized AI model, was introduced and demonstrated superior performance in healthcare operational tasks after fine-tuning, outperforming larger generalist models.


<details>
  <summary>Details</summary>
Motivation: General foundation models lack specialized operational knowledge for healthcare decisions. Lang1 is developed to address this gap with domain-specific pretraining.

Method: Lang1 models are pretrained on clinical data (80B clinical tokens and 627B internet tokens) and evaluated using the REalistic Medical Evaluation (ReMedE) benchmark.

Result: Lang1-1B outperformed larger generalist models in hospital task prediction after finetuning, effectively transferring to other clinical tasks and environments.

Conclusion: Specialized models with in-domain pretraining and supervised finetuning are crucial for effective healthcare AI, illustrating their superiority over generalist models for specialized tasks.

Abstract: Hospitals and healthcare systems rely on operational decisions that determine patient flow, cost, and quality of care. Despite strong performance on medical knowledge and conversational benchmarks, foundation models trained on general text may lack the specialized knowledge required for these operational decisions. We introduce Lang1, a family of models (100M-7B parameters) pretrained on a specialized corpus blending 80B clinical tokens from NYU Langone Health's EHRs and 627B tokens from the internet. To rigorously evaluate Lang1 in real-world settings, we developed the REalistic Medical Evaluation (ReMedE), a benchmark derived from 668,331 EHR notes that evaluates five critical tasks: 30-day readmission prediction, 30-day mortality prediction, length of stay, comorbidity coding, and predicting insurance claims denial. In zero-shot settings, both general-purpose and specialized models underperform on four of five tasks (36.6%-71.7% AUROC), with mortality prediction being an exception. After finetuning, Lang1-1B outperforms finetuned generalist models up to 70x larger and zero-shot models up to 671x larger, improving AUROC by 3.64%-6.75% and 1.66%-23.66% respectively. We also observed cross-task scaling with joint finetuning on multiple tasks leading to improvement on other tasks. Lang1-1B effectively transfers to out-of-distribution settings, including other clinical tasks and an external health system. Our findings suggest that predictive capabilities for hospital operations require explicit supervised finetuning, and that this finetuning process is made more efficient by in-domain pretraining on EHR. Our findings support the emerging view that specialized LLMs can compete with generalist models in specialized tasks, and show that effective healthcare systems AI requires the combination of in-domain pretraining, supervised finetuning, and real-world evaluation beyond proxy benchmarks.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [185] [Psychological stress during Examination and its estimation by handwriting in answer script](https://arxiv.org/abs/2511.11633)
*Abhijeet Kumar,Chetan Agarwal,Pronoy B. Neogi,Mayank Goswami*

Main category: cs.CV

TL;DR: The paper proposes a system integrating graphology and AI to assess psychological stress in students through their handwriting during exams.


<details>
  <summary>Details</summary>
Motivation: To transcend traditional grading systems by offering deeper insights into students' cognitive and emotional states, particularly their psychological stress, during exams.

Method: Using Optical Character Recognition, high-resolution image processing, TrOCR, sentiment entropy fusion with RoBERTa-based models, and a five-model voting mechanism alongside unsupervised anomaly detection.

Result: A numerical Stress Index is generated robustly, providing a novel framework in academic forensics.

Conclusion: This framework offers an innovative approach to measure and understand students' stress during exams, combining cutting-edge AI and graphology techniques.

Abstract: This research explores the fusion of graphology and artificial intelligence to quantify psychological stress levels in students by analyzing their handwritten examination scripts. By leveraging Optical Character Recognition and transformer based sentiment analysis models, we present a data driven approach that transcends traditional grading systems, offering deeper insights into cognitive and emotional states during examinations. The system integrates high resolution image processing, TrOCR, and sentiment entropy fusion using RoBERTa based models to generate a numerical Stress Index. Our method achieves robustness through a five model voting mechanism and unsupervised anomaly detection, making it an innovative framework in academic forensics.

</details>


### [186] [Real-time pothole detection with onboard sensors and camera on vehicles](https://arxiv.org/abs/2511.11643)
*Aswath Muthuselvam,Jeevak Raj S,Mohanaprasad K*

Main category: cs.CV

TL;DR: This paper presents a method to detect potholes in real-time using onboard vehicle sensors and an SVM classifier, achieving 98.1% accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the growing need for identifying and managing potholes due to increasing road traffic and its impact on everyday commuting.

Method: The paper employs onboard vehicle sensors to collect road condition data and uses an SVM classifier to detect potholes. The study tested the approach on 2 km of road with 26 potholes.

Result: The implemented system achieved 98.1% accuracy in detecting potholes from the data collected.

Conclusion: The proposed approach demonstrated effective pothole detection using onboard vehicle sensors, providing a promising solution for large-scale road condition management.

Abstract: Road conditions play an important role in our everyday commute. With the proliferating number of vehicles on the road each year, it has become necessary to access the road conditions very frequently, this would ensure that the traffic also flows smoothly. Even the smallest crack in the road could be easily be chipped into a large pothole due to changing surface temperatures of the road and from the force of vehicles riding over it. In this paper, we have addressed how we could better identify these potholes in realtime with the help of onboard sensors in vehicles so that the data could be useful for analysis and better management of potholes on a large scale. For the implementation, we used an SVM classifier to detect potholes, we achieved 98.1% accuracy based on data collected from a local road for about 2 km which had 26 potholes distributed along the road. Code is available at: https://github.com/aswathselvam/Potholes

</details>


### [187] [A Method for Identifying Farmland System Habitat Types Based on the Dynamic-Weighted Feature Fusion Network Model](https://arxiv.org/abs/2511.11659)
*Kesong Zheng,Zhi Song,Peizhou Li,Shuyi Yao,Zhenxing Bian*

Main category: cs.CV

TL;DR: The study addresses segmentation issues in cultivated land ecosystems by developing an annotated remote sensing dataset and proposing a Dynamic-Weighted Feature Fusion Network (DWFF-Net). It achieves improved accuracy metrics, offering a framework for fine-grained, cost-efficient habitat mapping.


<details>
  <summary>Details</summary>
Motivation: A lack of standardized habitat classification for cultivated lands, incomplete habitat type coverage, and challenges in integrating semantic and texture features for finer segmentation.

Method: Developed an annotated ultra-high-resolution remote sensing dataset with 15 cultivated land habitat categories and introduced DWFF-Net that uses frozen-parameter DINOv3 for feature extraction, dynamic weighting for feature fusion, and a hybrid loss function for optimization.

Result: The proposed model achieved superior segmentation performance with metrics: mIoU of 0.6979 and F1-score of 0.8049, outperforming baseline networks.

Conclusion: The study establishes an advanced framework to support sub-meter precision habitat mapping, improving segmentation accuracy particularly for micro-habitats, and providing technical means for cultivated land monitoring.

Abstract: Addressing the current lack of a standardized habitat classification system for cultivated land ecosystems, incomplete coverage of habitat types, and the inability of existing models to effectively integrate semantic and texture features-resulting in insufficient segmentation accuracy and blurred boundaries for multi-scale habitats (e.g., large-scale field plots and micro-habitats)-this study developed a comprehensively annotated ultra-high-resolution remote sensing image dataset encompassing 15 categories of cultivated land system habitats. Furthermore, we propose a Dynamic-Weighted Feature Fusion Network (DWFF-Net). The encoder of this model utilizes a frozen-parameter DINOv3 to extract foundational features. By analyzing the relationships between different category images and feature maps, we introduce a data-level adaptive dynamic weighting strategy for feature fusion. The decoder incorporates a dynamic weight computation network to achieve thorough integration of multi-layer features, and a hybrid loss function is adopted to optimize model training. Experimental results on the constructed dataset demonstrate that the proposed model achieves a mean Intersection over Union (mIoU) of 0.6979 and an F1-score of 0.8049, outperforming the baseline network by 0.021 and 0.0161, respectively. Ablation studies further confirm the complementary nature of multi-layer feature fusion, which effectively improves the IoU for micro-habitat categories such as field ridges. This study establishes a habitat identification framework for cultivated land systems based on adaptive multi-layer feature fusion, enabling sub-meter precision habitat mapping at a low cost and providing robust technical support for fine-grained habitat monitoring in cultivated landscapes.

</details>


### [188] [AGENet: Adaptive Edge-aware Geodesic Distance Learning for Few-Shot Medical Image Segmentation](https://arxiv.org/abs/2511.11662)
*Ziyuan Gao*

Main category: cs.CV

TL;DR: AGENet is a framework proposed for improving medical image segmentation using edge-aware geodesic distance learning, addressing challenges of boundary precision and limited training data.


<details>
  <summary>Details</summary>
Motivation: To overcome the bottleneck of large annotated datasets needed for medical image segmentation and improve precise delineation of boundaries in few-shot segmentation setups.

Method: AGENet incorporates spatial relationships through edge-aware geodesic modeling, using three components: edge-aware geodesic distance learning, adaptive prototype extraction, and adaptive parameterization.

Result: Extensive experiments show AGENet improves over state-of-the-art methods in boundary precision, computational efficiency, and performance across diverse datasets.

Conclusion: AGENet offers a computationally efficient and clinically suitable solution for precise medical image segmentation with minimal annotated training data.

Abstract: Medical image segmentation requires large annotated datasets, creating a significant bottleneck for clinical applications. While few-shot segmentation methods can learn from minimal examples, existing approaches demonstrate suboptimal performance in precise boundary delineation for medical images, particularly when anatomically similar regions appear without sufficient spatial context. We propose AGENet (Adaptive Geodesic Edge-aware Network), a novel framework that incorporates spatial relationships through edge-aware geodesic distance learning. Our key insight is that medical structures follow predictable geometric patterns that can guide prototype extraction even with limited training data. Unlike methods relying on complex architectural components or heavy neural networks, our approach leverages computationally lightweight geometric modeling. The framework combines three main components: (1) An edge-aware geodesic distance learning module that respects anatomical boundaries through iterative Fast Marching refinement, (2) adaptive prototype extraction that captures both global structure and local boundary details via spatially-weighted aggregation, and (3) adaptive parameter learning that automatically adjusts to different organ characteristics. Extensive experiments across diverse medical imaging datasets demonstrate improvements over state-of-the-art methods. Notably, our method reduces boundary errors compared to existing approaches while maintaining computational efficiency, making it highly suitable for clinical applications requiring precise segmentation with limited annotated data.

</details>


### [189] [EPSegFZ: Efficient Point Cloud Semantic Segmentation for Few- and Zero-Shot Scenarios with Language Guidance](https://arxiv.org/abs/2511.11700)
*Jiahui Wang,Haiyue Zhu,Haoren Guo,Abdullah Al Mamun,Cheng Xiang,Tong Heng Lee*

Main category: cs.CV

TL;DR: This paper proposes a novel network for few- and zero-shot 3D point cloud semantic segmentation, addressing limitations of existing methods by eliminating the pre-training stage and using textual and visual information effectively.


<details>
  <summary>Details</summary>
Motivation: Existing models for few-shot 3D point cloud semantic segmentation heavily rely on a pre-training stage, reducing flexibility and failing to fully utilize support information such as textual annotations.

Method: The paper introduces a pre-training-free model, EPSegFZ, with three major innovations: 1) ProERA for enhanced feature extraction, 2) DRPE-based cross-attention for accurate query-prototype matching, and 3) LGPE for leveraging textual information.

Result: The proposed EPSegFZ model outperforms state-of-the-art methods with a 5.68% improvement on S3DIS and 3.82% on ScanNet benchmarks in few-shot scenarios.

Conclusion: The introduced approach eliminates the need for pre-training and effectively integrates textual data, showing significant performance gains and setting a new direction for few- and zero-shot 3D point cloud segmentation.

Abstract: Recent approaches for few-shot 3D point cloud semantic segmentation typically require a two-stage learning process, i.e., a pre-training stage followed by a few-shot training stage. While effective, these methods face overreliance on pre-training, which hinders model flexibility and adaptability. Some models tried to avoid pre-training yet failed to capture ample information. In addition, current approaches focus on visual information in the support set and neglect or do not fully exploit other useful data, such as textual annotations. This inadequate utilization of support information impairs the performance of the model and restricts its zero-shot ability. To address these limitations, we present a novel pre-training-free network, named Efficient Point Cloud Semantic Segmentation for Few- and Zero-shot scenarios. Our EPSegFZ incorporates three key components. A Prototype-Enhanced Registers Attention (ProERA) module and a Dual Relative Positional Encoding (DRPE)-based cross-attention mechanism for improved feature extraction and accurate query-prototype correspondence construction without pre-training. A Language-Guided Prototype Embedding (LGPE) module that effectively leverages textual information from the support set to improve few-shot performance and enable zero-shot inference. Extensive experiments show that our method outperforms the state-of-the-art method by 5.68% and 3.82% on the S3DIS and ScanNet benchmarks, respectively.

</details>


### [190] [Task-Aware 3D Affordance Segmentation via 2D Guidance and Geometric Refinement](https://arxiv.org/abs/2511.11702)
*Lian He,Meng Liu,Qilang Ye,Yu Zhou,Xiang Deng,Gangyi Ding*

Main category: cs.CV

TL;DR: The paper introduces TASA, a novel framework for 3D scene-level affordance segmentation that integrates 2D semantic cues and 3D geometric reasoning, achieving high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The need to enable embodied agents to understand 3D scene-level affordances from natural language instructions for meaningful interaction in complex environments.

Method: Proposes TASA, combining task-aware 2D affordance detection and 3D affordance refinement, leveraging semantic cues and geometric reasoning with a coarse-to-fine approach.

Result: TASA achieves superior performance in accuracy and efficiency compared to existing methods on the SceneFun3D dataset.

Conclusion: TASA efficiently integrates semantic and geometric information, overcoming limitations of existing methods, and marks progress in efficient 3D scene-level affordance segmentation.

Abstract: Understanding 3D scene-level affordances from natural language instructions is essential for enabling embodied agents to interact meaningfully in complex environments. However, this task remains challenging due to the need for semantic reasoning and spatial grounding. Existing methods mainly focus on object-level affordances or merely lift 2D predictions to 3D, neglecting rich geometric structure information in point clouds and incurring high computational costs. To address these limitations, we introduce Task-Aware 3D Scene-level Affordance segmentation (TASA), a novel geometry-optimized framework that jointly leverages 2D semantic cues and 3D geometric reasoning in a coarse-to-fine manner. To improve the affordance detection efficiency, TASA features a task-aware 2D affordance detection module to identify manipulable points from language and visual inputs, guiding the selection of task-relevant views. To fully exploit 3D geometric information, a 3D affordance refinement module is proposed to integrate 2D semantic priors with local 3D geometry, resulting in accurate and spatially coherent 3D affordance masks. Experiments on SceneFun3D demonstrate that TASA significantly outperforms the baselines in both accuracy and efficiency in scene-level affordance segmentation.

</details>


### [191] [LE-CapsNet: A Light and Enhanced Capsule Network](https://arxiv.org/abs/2511.11708)
*Pouya Shiri,Amirali Baniasadi*

Main category: cs.CV

TL;DR: LE-CapsNet is a lightweight, faster, and more accurate variant of Capsule Network (CapsNet) designed for image classification tasks.


<details>
  <summary>Details</summary>
Motivation: CapsNet, while offering advantages like better detection of overlapping categories and transformed images, suffers from slower speeds and resource intensity, making it less practical in certain use cases.

Method: The paper introduces LE-CapsNet, a redesigned, lightweight architecture utilizing 3.8 million weights to optimize computational efficiency and accuracy for image classification tasks.

Result: LE-CapsNet achieves 76.73% accuracy on CIFAR-10, performs inference 4x faster than standard CapsNet, and delivers 94.3% accuracy on AffNIST, outperforming CapsNet (90.52%).

Conclusion: LE-CapsNet provides significant improvements in computational efficiency, accuracy, and robustness compared to traditional CapsNet while addressing its shortcomings, making it suitable for tasks with stringent requirements on speed and resource utilization.

Abstract: Capsule Network (CapsNet) classifier has several advantages over CNNs, including better detection of images containing overlapping categories and higher accuracy on transformed images. Despite the advantages, CapsNet is slow due to its different structure. In addition, CapsNet is resource-hungry, includes many parameters and lags in accuracy compared to CNNs. In this work, we propose LE-CapsNet as a light, enhanced and more accurate variant of CapsNet. Using 3.8M weights, LECapsNet obtains 76.73% accuracy on the CIFAR-10 dataset while performing inference 4x faster than CapsNet. In addition, our proposed network is more robust at detecting images with affine transformations compared to CapsNet. We achieve 94.3% accuracy on the AffNIST dataset (compared to CapsNet 90.52%).

</details>


### [192] [Target-Balanced Score Distillation](https://arxiv.org/abs/2511.11710)
*Zhou Xu,Qi Wang,Yuxiao Yang,Luyuan Zhang,Zhang Liang,Yang Li*

Main category: cs.CV

TL;DR: The paper addresses issues in 3D asset generation using SDS, improving texture fidelity while maintaining shape accuracy via the proposed Target-Balanced Score Distillation (TBSD) method.


<details>
  <summary>Details</summary>
Motivation: Current methods for 3D asset generation using SDS face challenges like over-saturation, over-smoothing, and a trade-off between texture fidelity and shape accuracy. The authors aim to resolve these issues.

Method: The authors introduce Target-Balanced Score Distillation (TBSD), a multi-objective optimization framework that resolves the trade-off between texture optimization and shape distortion using an adaptive strategy and Target Negative Prompts (TNPs).

Result: TBSD achieves significant improvements over existing methods, producing 3D assets with better texture fidelity and accurate shape geometry.

Conclusion: The proposed TBSD approach effectively overcomes limitations of existing SDS methods, enabling high-quality 3D asset generation with balanced texture and shape properties.

Abstract: Score Distillation Sampling (SDS) enables 3D asset generation by distilling priors from pretrained 2D text-to-image diffusion models, but vanilla SDS suffers from over-saturation and over-smoothing. To mitigate this issue, recent variants have incorporated negative prompts. However, these methods face a critical trade-off: limited texture optimization, or significant texture gains with shape distortion. In this work, we first conduct a systematic analysis and reveal that this trade-off is fundamentally governed by the utilization of the negative prompts, where Target Negative Prompts (TNP) that embed target information in the negative prompts dramatically enhancing texture realism and fidelity but inducing shape distortions. Informed by this key insight, we introduce the Target-Balanced Score Distillation (TBSD). It formulates generation as a multi-objective optimization problem and introduces an adaptive strategy that effectively resolves the aforementioned trade-off. Extensive experiments demonstrate that TBSD significantly outperforms existing state-of-the-art methods, yielding 3D assets with high-fidelity textures and geometrically accurate shape.

</details>


### [193] [CompressNAS : A Fast and Efficient Technique for Model Compression using Decomposition](https://arxiv.org/abs/2511.11716)
*Sudhakar Sah,Nikhil Chabbra,Matthieu Durnerin*

Main category: cs.CV

TL;DR: CompressNAS addresses the challenge of deploying large CNNs on smaller devices using global rank selection for tensor decomposition, achieving significant compression with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: The growing size and computational demands of CNNs make it difficult to deploy them on resource-constrained devices like microcontrollers and lightweight NPUs.

Method: CompressNAS uses a global search framework for rank selection in tensor decomposition, aided by a fast accuracy estimator to ensure efficient exploration under given constraints.

Result: ResNet-18 was compressed by 8x with <4% accuracy drop on ImageNet, while YOLOv5 models achieved 2x compression with minimal or no accuracy losses on COCO dataset.

Conclusion: CompressNAS demonstrates that global rank selection can significantly reduce model size while maintaining competitive accuracy, enabling efficient deep learning deployment on constrained hardware.

Abstract: Deep Convolutional Neural Networks (CNNs) are increasingly difficult to deploy on microcontrollers (MCUs) and lightweight NPUs (Neural Processing Units) due to their growing size and compute demands. Low-rank tensor decomposition, such as Tucker factorization, is a promising way to reduce parameters and operations with reasonable accuracy loss. However, existing approaches select ranks locally and often ignore global trade-offs between compression and accuracy. We introduce CompressNAS, a MicroNAS-inspired framework that treats rank selection as a global search problem. CompressNAS employs a fast accuracy estimator to evaluate candidate decompositions, enabling efficient yet exhaustive rank exploration under memory and accuracy constraints. In ImageNet, CompressNAS compresses ResNet-18 by 8x with less than 4% accuracy drop; on COCO, we achieve 2x compression of YOLOv5s without any accuracy drop and 2x compression of YOLOv5n with a 2.5% drop. Finally, we present a new family of compressed models, STResNet, with competitive performance compared to other efficient models.

</details>


### [194] [AdaptFly: Prompt-Guided Adaptation of Foundation Models for Low-Altitude UAV Networks](https://arxiv.org/abs/2511.11720)
*Jiao Chen,Haoyi Wang,Jianhua Tang,Junyi Wang*

Main category: cs.CV

TL;DR: AdaptFly is a framework for test-time adaptation in UAV networks, improving semantic segmentation without requiring weight updates on models.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current semantic segmentation foundation models in UAV networks under varying conditions such as weather, lighting, and perspectives.

Method: The proposed AdaptFly framework employs two adaptation modes: lightweight token-prompt retrieval for resource-constrained UAVs and gradient-free sparse visual prompt optimization using an evolutionary strategy for resource-rich UAVs. It triggers adaptation based on activation-statistics and consolidates cross-UAV knowledge through a shared memory system.

Result: Extensive experiments show AdaptFly improves segmentation accuracy and robustness over static models and state-of-the-art test-time adaptation baselines across various conditions on UAVid, VDD datasets, and real-world deployments.

Conclusion: AdaptFly is a practical, robust solution for resilient and communication-efficient perception in UAV networks, enabling collaborative adaptation without weight updates.

Abstract: Low-altitude Unmanned Aerial Vehicle (UAV) networks rely on robust semantic segmentation as a foundational enabler for distributed sensing-communication-control co-design across heterogeneous agents within the network. However, segmentation foundation models deteriorate quickly under weather, lighting, and viewpoint drift. Resource-limited UAVs cannot run gradient-based test-time adaptation, while resource-massive UAVs adapt independently, wasting shared experience. To address these challenges, we propose AdaptFly, a prompt-guided test-time adaptation framework that adjusts segmentation models without weight updates. AdaptFly features two complementary adaptation modes. For resource-limited UAVs, it employs lightweight token-prompt retrieval from a shared global memory. For resource-massive UAVs, it uses gradient-free sparse visual prompt optimization via Covariance Matrix Adaptation Evolution Strategy. An activation-statistic detector triggers adaptation, while cross-UAV knowledge pool consolidates prompt knowledge and enables fleet-wide collaboration with negligible bandwidth overhead. Extensive experiments on UAVid and VDD benchmarks, along with real-world UAV deployments under diverse weather conditions, demonstrate that AdaptFly significantly improves segmentation accuracy and robustness over static models and state-of-the-art TTA baselines. The results highlight a practical path to resilient, communication-efficient perception in the emerging low-altitude economy.

</details>


### [195] [Do Blind Spots Matter for Word-Referent Mapping? A Computational Study with Infant Egocentric Video](https://arxiv.org/abs/2511.11725)
*Zekai Shi,Zhixi Cai,Kalin Stefanov*

Main category: cs.CV

TL;DR: This paper introduces a biologically plausible method for learning visual representations inspired by how humans fill visual gaps.


<details>
  <summary>Details</summary>
Motivation: To mimic how children learn their first words by linking spoken utterances to visual referents, addressing the lack of biologically justified methods in existing strategies for learning word-referent mappings.

Method: A self-supervised strategy using a masked autoencoder-based visual model with a novel masking strategy that mimics the blind spot in human eyes. This is combined with a contrastive learning-based video-text model.

Result: The proposed masking strategy performs at least as effectively as random masking techniques in learning word-referent mappings from cross-situational and temporally extended experiences.

Conclusion: The biologically inspired masking strategy aligns with human learning mechanisms and provides an effective alternative to standard methods.

Abstract: Typically, children start to learn their first words between 6 and 9 months, linking spoken utterances to their visual referents. Without prior knowledge, a word encountered for the first time can be interpreted in countless ways; it might refer to any of the objects in the environment, their components, or attributes. Using longitudinal, egocentric, and ecologically valid data from the experience of one child, in this work, we propose a self-supervised and biologically plausible strategy to learn strong visual representations. Our masked autoencoder-based visual backbone incorporates knowledge about the blind spot in human eyes to define a novel masking strategy. This mask and reconstruct approach attempts to mimic the way the human brain fills the gaps in the eyes' field of view. This represents a significant shift from standard random masking strategies, which are difficult to justify from a biological perspective. The pretrained encoder is utilized in a contrastive learning-based video-text model capable of acquiring word-referent mappings. Extensive evaluation suggests that the proposed biologically plausible masking strategy is at least as effective as random masking for learning word-referent mappings from cross-situational and temporally extended episodes.

</details>


### [196] [GROVER: Graph-guided Representation of Omics and Vision with Expert Regulation for Adaptive Spatial Multi-omics Fusion](https://arxiv.org/abs/2511.11730)
*Yongjun Xiao,Dian Meng,Xinlei Huang,Yanran Liu,Shiwei Ruan,Ziyue Qiao,Xubin Zheng*

Main category: cs.CV

TL;DR: GROVER is a new framework for integrating multimodal spatial omics and histopathological data to analyze disease tissues, addressing challenges such as heterogeneity, misalignment, and noise with advanced methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the challenges of integrating spatial omics data with histopathological images, crucial for comprehensively analyzing tissues and understanding underlying biological mechanisms.

Method: The method involves a novel framework, GROVER, which uses Graph Convolutional Networks, contrastive learning for spot-feature alignment, and dynamic expert routing to adaptively integrate multimodal spatial data.

Result: Experiments on real-world datasets show GROVER outperforms existing methods in reliably and robustly integrating spatial omics and histopathological data.

Conclusion: GROVER effectively addresses the issues of heterogeneity, resolution mismatch, and noise in multimodal integration, providing new insights into spatial omics analysis.

Abstract: Effectively modeling multimodal spatial omics data is critical for understanding tissue complexity and underlying biological mechanisms. While spatial transcriptomics, proteomics, and epigenomics capture molecular features, they lack pathological morphological context. Integrating these omics with histopathological images is therefore essential for comprehensive disease tissue analysis. However, substantial heterogeneity across omics, imaging, and spatial modalities poses significant challenges. Naive fusion of semantically distinct sources often leads to ambiguous representations. Additionally, the resolution mismatch between high-resolution histology images and lower-resolution sequencing spots complicates spatial alignment. Biological perturbations during sample preparation further distort modality-specific signals, hindering accurate integration. To address these challenges, we propose Graph-guided Representation of Omics and Vision with Expert Regulation for Adaptive Spatial Multi-omics Fusion (GROVER), a novel framework for adaptive integration of spatial multi-omics data. GROVER leverages a Graph Convolutional Network encoder based on Kolmogorov-Arnold Networks to capture the nonlinear dependencies between each modality and its associated spatial structure, thereby producing expressive, modality-specific embeddings. To align these representations, we introduce a spot-feature-pair contrastive learning strategy that explicitly optimizes the correspondence across modalities at each spot. Furthermore, we design a dynamic expert routing mechanism that adaptively selects informative modalities for each spot while suppressing noisy or low-quality inputs. Experiments on real-world spatial omics datasets demonstrate that GROVER outperforms state-of-the-art baselines, providing a robust and reliable solution for multimodal integration.

</details>


### [197] [Exposing DeepFakes via Hyperspectral Domain Mapping](https://arxiv.org/abs/2511.11732)
*Aditya Mehta,Swarnim Chaudhary,Pratik Narang,Jagat Sesh Challa*

Main category: cs.CV

TL;DR: HSI-Detect is a method utilizing hyperspectral imaging to enhance the detection of manipulated images or Deepfakes more effectively than RGB-based approaches.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of RGB-based methods in detecting realistic manipulated images by amplifying artifacts invisible in the standard RGB domain.

Method: HSI-Detect reconstructs 31-channel hyperspectral images from RGB inputs and performs detection using the enriched spectral data.

Result: HSI-Detect shows consistent improvement in Deepfake detection compared to RGB-only systems, demonstrated using the FaceForensics++ dataset.

Conclusion: Expanding image analysis into the hyperspectral domain can significantly enhance the detection of manipulation artifacts, making HSI-Detect effective for Deepfake detection.

Abstract: Modern generative and diffusion models produce highly realistic images that can mislead human perception and even sophisticated automated detection systems. Most detection methods operate in RGB space and thus analyze only three spectral channels. We propose HSI-Detect, a two-stage pipeline that reconstructs a 31-channel hyperspectral image from a standard RGB input and performs detection in the hyperspectral domain. Expanding the input representation into denser spectral bands amplifies manipulation artifacts that are often weak or invisible in the RGB domain, particularly in specific frequency bands. We evaluate HSI-Detect across FaceForensics++ dataset and show the consistent improvements over RGB-only baselines, illustrating the promise of spectral-domain mapping for Deepfake detection.

</details>


### [198] [Toward bilipshiz geometric models](https://arxiv.org/abs/2511.11735)
*Yonatan Sverdlov,Eitan Rosen,Nadav Dym*

Main category: cs.CV

TL;DR: This paper investigates neural networks designed for point clouds, focusing on whether they preserve symmetry-aware distances through bi-Lipschitz equivalence, and proposes a refined model with empirical advantages.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by recent findings in Equivariant learning that emphasize the benefits of bi-Lipschitz property for preserving natural distances and geometric features.

Method: The authors analyze two symmetry-aware metrics (Procrustes Matching and Hard Gromov Wasserstein) and evaluate point cloud networks. They demonstrate limitations in preserving bi-Lipschitz equivalence and propose modifications to networks to address this.

Result: They find that existing invariant networks are not bi-Lipschitz with respect to the Procrustes Matching metric, but modify these networks to ensure bi-Lipschitz guarantees. Initial experiments show improvements in point cloud correspondence tasks.

Conclusion: Enhanced bi-Lipschitz models are better suited for tasks involving symmetry preservation in point clouds, offering improvements over standard invariant neural network designs.

Abstract: Many neural networks for point clouds are, by design, invariant to the symmetries of this datatype: permutations and rigid motions. The purpose of this paper is to examine whether such networks preserve natural symmetry aware distances on the point cloud spaces, through the notion of bi-Lipschitz equivalence. This inquiry is motivated by recent work in the Equivariant learning literature which highlights the advantages of bi-Lipschitz models in other scenarios.
  We consider two symmetry aware metrics on point clouds: (a) The Procrustes Matching (PM) metric and (b) Hard Gromov Wasserstien distances. We show that these two distances themselves are not bi-Lipschitz equivalent, and as a corollary deduce that popular invariant networks for point clouds are not bi-Lipschitz with respect to the PM metric. We then show how these networks can be modified so that they do obtain bi-Lipschitz guarantees. Finally, we provide initial experiments showing the advantage of the proposed bi-Lipschitz model over standard invariant models, for the tasks of finding correspondences between 3D point clouds.

</details>


### [199] [Concept-RuleNet: Grounded Multi-Agent Neurosymbolic Reasoning in Vision Language Models](https://arxiv.org/abs/2511.11751)
*Sanchit Sinha,Guangzhi Xiong,Zhenghao He,Aidong Zhang*

Main category: cs.CV

TL;DR: The paper proposes "Concept-RuleNet," a system integrating interpretable symbolic reasoning with visual grounding to improve neurosymbolic vision-language models (VLMs).


<details>
  <summary>Details</summary>
Motivation: Existing vision-language models often hallucinate information and lack interpretability, especially for out-of-distribution data. Current neurosymbolic frameworks rely solely on task labels for symbolic reasoning, leading to weak grounding in visual input.

Method: Concept-RuleNet combines a multimodal concept generator (to extract visual concepts), a large language model for creating interpretable symbolic rules grounded in visual data, and a vision verifier agent to assess symbol presence and execute rules alongside black-box models.

Result: The system improves neurosymbolic accuracy by 5% on average across five benchmarks, including medical imaging and underrepresented datasets. It also reduces hallucinated symbols by 50%.

Conclusion: Concept-RuleNet advances neurosymbolic reasoning with stronger visual grounding and interpretable predictive pathways, enhancing accuracy and reducing errors caused by hallucination.

Abstract: Modern vision-language models (VLMs) deliver impressive predictive accuracy yet offer little insight into 'why' a decision is reached, frequently hallucinating facts, particularly when encountering out-of-distribution data. Neurosymbolic frameworks address this by pairing black-box perception with interpretable symbolic reasoning, but current methods extract their symbols solely from task labels, leaving them weakly grounded in the underlying visual data. In this paper, we introduce a multi-agent system - Concept-RuleNet that reinstates visual grounding while retaining transparent reasoning. Specifically, a multimodal concept generator first mines discriminative visual concepts directly from a representative subset of training images. Next, these visual concepts are utilized to condition symbol discovery, anchoring the generations in real image statistics and mitigating label bias. Subsequently, symbols are composed into executable first-order rules by a large language model reasoner agent - yielding interpretable neurosymbolic rules. Finally, during inference, a vision verifier agent quantifies the degree of presence of each symbol and triggers rule execution in tandem with outputs of black-box neural models, predictions with explicit reasoning pathways. Experiments on five benchmarks, including two challenging medical-imaging tasks and three underrepresented natural-image datasets, show that our system augments state-of-the-art neurosymbolic baselines by an average of 5% while also reducing the occurrence of hallucinated symbols in rules by up to 50%.

</details>


### [200] [Batch Transformer Architecture: Case of Synthetic Image Generation for Emotion Expression Facial Recognition](https://arxiv.org/abs/2511.11754)
*Stanislav Selitskiy*

Main category: cs.CV

TL;DR: A new Transformer variation, Batch Transformers, uses selective attention to key dimensions rather than whole-dimensionality sequential/batch entities, improving encoder-decoder efficiency for image generation.


<details>
  <summary>Details</summary>
Motivation: To improve feature selection and reduce bottleneck size in encoder-decoder Transformer architectures, particularly for synthetic image generation in tasks like face recognition.

Method: Proposes Batch Transformers, where attention is selectively applied to important dimensions, significantly enhancing feature selection and reducing computational overhead.

Result: Confirmed effectiveness in generating synthetic images for a face recognition task, even in challenging conditions like makeup and occlusion, enhancing variability in limited datasets.

Conclusion: Batch Transformers improve the efficiency and variability of synthetic image generation by leveraging selective attention on significant dimensions for face recognition tasks with limited data.

Abstract: A novel Transformer variation architecture is proposed in the implicit sparse style. Unlike "traditional" Transformers, instead of attention to sequential or batch entities in their entirety of whole dimensionality, in the proposed Batch Transformers, attention to the "important" dimensions (primary components) is implemented. In such a way, the "important" dimensions or feature selection allows for a significant reduction of the bottleneck size in the encoder-decoder ANN architectures. The proposed architecture is tested on the synthetic image generation for the face recognition task in the case of the makeup and occlusion data set, allowing for increased variability of the limited original data set.

</details>


### [201] [Image-POSER: Reflective RL for Multi-Expert Image Generation and Editing](https://arxiv.org/abs/2511.11780)
*Hossein Mohebbi,Mohammed Abdulrahman,Yanting Miao,Pascal Poupart,Suraj Kothawade*

Main category: cs.CV

TL;DR: Introduction of Image-POSER, a reflective reinforcement learning framework for reliable text-to-image generation handling long compositional prompts.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing text-to-image systems in executing long, compositional prompts for creative workflows.

Method: The framework orchestrates pretrained text-to-image and image-to-image experts, decomposes tasks dynamically, and supervises steps using a vision-language model critic through a Markov Decision Process.

Result: Image-POSER outperforms baseline models on benchmarks in alignment, fidelity, and aesthetics, and is preferred in human evaluations.

Conclusion: Reinforcement learning enables AI systems to fully leverage visual model combinations, progressing towards general-purpose visual assistants.

Abstract: Recent advances in text-to-image generation have produced strong single-shot models, yet no individual system reliably executes the long, compositional prompts typical of creative workflows. We introduce Image-POSER, a reflective reinforcement learning framework that (i) orchestrates a diverse registry of pretrained text-to-image and image-to-image experts, (ii) handles long-form prompts end-to-end through dynamic task decomposition, and (iii) supervises alignment at each step via structured feedback from a vision-language model critic. By casting image synthesis and editing as a Markov Decision Process, we learn non-trivial expert pipelines that adaptively combine strengths across models. Experiments show that Image-POSER outperforms baselines, including frontier models, across industry-standard and custom benchmarks in alignment, fidelity, and aesthetics, and is consistently preferred in human evaluations. These results highlight that reinforcement learning can endow AI systems with the capacity to autonomously decompose, reorder, and combine visual models, moving towards general-purpose visual assistants.

</details>


### [202] [SOTFormer: A Minimal Transformer for Unified Object Tracking and Trajectory Prediction](https://arxiv.org/abs/2511.11824)
*Zhongping Dong,Pengyang Yu,Shuangjian Li,Liming Chen,Mohand Tahar Kechadi*

Main category: cs.CV

TL;DR: The paper introduces SOTFormer, a unified transformer-based model for object tracking, detection, and short-term motion prediction, addressing challenges like occlusion and scale variation. It achieves state-of-the-art performance on benchmarks with efficient GPU memory usage.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges like occlusion, scale variation, and temporal drift that hinder accurate single-object tracking and short-term motion forecasting.

Method: SOTFormer uses a minimal constant-memory temporal transformer with ground-truth-primed memory and burn-in anchor loss. A lightweight temporal-attention layer refines frame embeddings for efficient real-time inference.

Result: SOTFormer achieves 76.3 AUC and 53.7 FPS on the Mini-LaSOT benchmark, outperforming existing transformer baselines in challenging scenarios such as fast motion and occlusion.

Conclusion: SOTFormer successfully integrates object detection, tracking, and motion forecasting in a lightweight, end-to-end framework, providing robust performance and efficiency in real-time settings.

Abstract: Accurate single-object tracking and short-term motion forecasting remain challenging under occlusion, scale variation, and temporal drift, which disrupt the temporal coherence required for real-time perception. We introduce \textbf{SOTFormer}, a minimal constant-memory temporal transformer that unifies object detection, tracking, and short-horizon trajectory prediction within a single end-to-end framework. Unlike prior models with recurrent or stacked temporal encoders, SOTFormer achieves stable identity propagation through a ground-truth-primed memory and a burn-in anchor loss that explicitly stabilizes initialization. A single lightweight temporal-attention layer refines embeddings across frames, enabling real-time inference with fixed GPU memory. On the Mini-LaSOT (20%) benchmark, SOTFormer attains 76.3 AUC and 53.7 FPS (AMP, 4.3 GB VRAM), outperforming transformer baselines such as TrackFormer and MOTRv2 under fast motion, scale change, and occlusion.

</details>


### [203] [MP-GFormer: A 3D-Geometry-Aware Dynamic Graph Transformer Approach for Machining Process Planning](https://arxiv.org/abs/2511.11837)
*Fatemeh Elhambakhsh,Gaurav Ameta,Aditi Roy,Hyunwoong Ko*

Main category: cs.CV

TL;DR: The paper proposes MP-GFormer, a novel 3D-geometry-aware dynamic graph transformer that integrates three-dimensional geometric information into dynamic graph learning to improve machining operation sequence prediction.


<details>
  <summary>Details</summary>
Motivation: The study addresses the limitations of existing dynamic graph learning approaches in machining process planning, specifically their inability to incorporate 3D geometric information, leading to a lack of domain awareness.

Method: MP-GFormer uses evolving surface meshes and an attention mechanism to integrate 3D geometric representations into dynamic graph models for machining operation sequence prediction.

Result: MP-GFormer achieves significant improvements in accuracy, with 24% and 36% accuracy gains for main and sub-operation predictions, respectively, compared to the state-of-the-art.

Conclusion: Including 3D geometric representations via dynamic graph transformer models substantially enhances predictions in machining operation planning.

Abstract: Machining process planning (MP) is inherently complex due to structural and geometrical dependencies among part features and machining operations. A key challenge lies in capturing dynamic interdependencies that evolve with distinct part geometries as operations are performed. Machine learning has been applied to address challenges in MP, such as operation selection and machining sequence prediction. Dynamic graph learning (DGL) has been widely used to model dynamic systems, thanks to its ability to integrate spatio-temporal relationships. However, in MP, while existing DGL approaches can capture these dependencies, they fail to incorporate three-dimensional (3D) geometric information of parts and thus lack domain awareness in predicting machining operation sequences. To address this limitation, we propose MP-GFormer, a 3D-geometry-aware dynamic graph transformer that integrates evolving 3D geometric representations into DGL through an attention mechanism to predict machining operation sequences. Our approach leverages StereoLithography surface meshes representing the 3D geometry of a part after each machining operation, with the boundary representation method used for the initial 3D designs. We evaluate MP-GFormer on a synthesized dataset and demonstrate that the method achieves improvements of 24\% and 36\% in accuracy for main and sub-operation predictions, respectively, compared to state-of-the-art approaches.

</details>


### [204] [Defending Unauthorized Model Merging via Dual-Stage Weight Protection](https://arxiv.org/abs/2511.11851)
*Wei-Jia Chen,Min-Yen Tsai,Cheng-Yi Lee,Chia-Mu Yu*

Main category: cs.CV

TL;DR: MergeGuard is a dual-stage framework designed to protect pretrained models from unauthorized merging, ensuring intellectual property rights and stable model performance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the risk of unauthorized pretrained model merging, which poses threats to intellectual property rights and model accountability.

Method: The MergeGuard framework employs two stages: redistribution of task-relevant information across layers and injection of structured perturbations to disrupt compatibility during merging.

Result: MergeGuard decreases merged model accuracy by up to 90%, while retaining less than 1.5% performance loss on the protected model.

Conclusion: MergeGuard effectively safeguards against harmful merging practices, maintaining model performance and protecting intellectual property.

Abstract: The rapid proliferation of pretrained models and open repositories has made model merging a convenient yet risky practice, allowing free-riders to combine fine-tuned models into a new multi-capability model without authorization. Such unauthorized model merging not only violates intellectual property rights but also undermines model ownership and accountability. To address this issue, we present MergeGuard, a proactive dual-stage weight protection framework that disrupts merging compatibility while maintaining task fidelity. In the first stage, we redistribute task-relevant information across layers via L2-regularized optimization, ensuring that important gradients are evenly dispersed. In the second stage, we inject structured perturbations to misalign task subspaces, breaking curvature compatibility in the loss landscape. Together, these stages reshape the model's parameter geometry such that merged models collapse into destructive interference while the protected model remains fully functional. Extensive experiments on both vision (ViT-L-14) and language (Llama2, Gemma2, Mistral) models demonstrate that MergeGuard reduces merged model accuracy by up to 90% with less than 1.5% performance loss on the protected model.

</details>


### [205] [FocusSDF: Boundary-Aware Learning for Medical Image Segmentation via Signed Distance Supervision](https://arxiv.org/abs/2511.11864)
*Muzammal Shafique,Nasir Rahim,Jamil Ahmad,Mohammad Siadat,Khalid Malik,Ghaus Malik*

Main category: cs.CV

TL;DR: The paper introduces FocusSDF, a boundary-aware loss function for medical image segmentation using signed distance functions (SDFs).


<details>
  <summary>Details</summary>
Motivation: To enhance segmentation models since boundary information is often neglected, leading to challenges in preserving boundaries in medical image segmentation.

Method: The authors propose FocusSDF, a loss function that adaptively prioritizes pixels near boundaries by incorporating SDFs, and evaluate it against baseline models and tasks.

Result: FocusSDF outperformed five state-of-the-art segmentation models and showed superior performance across datasets involving different medical segmentation challenges.

Conclusion: FocusSDF effectively enhances boundary awareness in segmentation tasks, addressing challenges in medical image segmentation, and offers improved results over existing methods.

Abstract: Segmentation of medical images constitutes an essential component of medical image analysis, providing the foundation for precise diagnosis and efficient therapeutic interventions in clinical practices. Despite substantial progress, most segmentation models do not explicitly encode boundary information; as a result, making boundary preservation a persistent challenge in medical image segmentation. To address this challenge, we introduce FocusSDF, a novel loss function based on the signed distance functions (SDFs), which redirects the network to concentrate on boundary regions by adaptively assigning higher weights to pixels closer to the lesion or organ boundary, effectively making it boundary aware. To rigorously validate FocusSDF, we perform extensive evaluations against five state-of-the-art medical image segmentation models, including the foundation model MedSAM, using four distance-based loss functions across diverse datasets covering cerebral aneurysm, stroke, liver, and breast tumor segmentation tasks spanning multiple imaging modalities. The experimental results consistently demonstrate the superior performance of FocusSDF over existing distance transform based loss functions.

</details>


### [206] [Lacking Data? No worries! How synthetic images can alleviate image scarcity in wildlife surveys: a case study with muskox (Ovibos moschatus)](https://arxiv.org/abs/2511.11882)
*Simon Durand,Samuel Foucher,Alexandre Delplanque,Joëlle Taillon,Jérôme Théau*

Main category: cs.CV

TL;DR: This study assesses the effectiveness of synthetic imagery in training deep learning models for muskox detection, particularly in scenarios with limited real-world data.


<details>
  <summary>Details</summary>
Motivation: To address the logistical challenges and data scarcity in muskox population monitoring and improve upon traditional survey methods.

Method: Synthetic imagery was incorporated into training datasets for deep learning object detection models (baseline, zero-shot, and few-shot configurations) to assess its impact on detection performance.

Result: Adding synthetic imagery improved detection in zero-shot scenarios and enhanced recall in few-shot models, though benefits plateaued beyond a certain dataset size.

Conclusion: Synthetic imagery is a viable solution for training object detection models with limited real data, aiding in wildlife monitoring and potentially increasing monitoring frequency.

Abstract: Accurate population estimates are essential for wildlife management, providing critical insights into species abundance and distribution. Traditional survey methods, including visual aerial counts and GNSS telemetry tracking, are widely used to monitor muskox populations in Arctic regions. These approaches are resource intensive and constrained by logistical challenges. Advances in remote sensing, artificial intelligence, and high resolution aerial imagery offer promising alternatives for wildlife detection. Yet, the effectiveness of deep learning object detection models (ODMs) is often limited by small datasets, making it challenging to train robust ODMs for sparsely distributed species like muskoxen. This study investigates the integration of synthetic imagery (SI) to supplement limited training data and improve muskox detection in zero shot (ZS) and few-shot (FS) settings. We compared a baseline model trained on real imagery with 5 ZS and 5 FS models that incorporated progressively more SI in the training set. For the ZS models, where no real images were included in the training set, adding SI improved detection performance. As more SI were added, performance in precision, recall and F1 score increased, but eventually plateaued, suggesting diminishing returns when SI exceeded 100% of the baseline model training dataset. For FS models, combining real and SI led to better recall and slightly higher overall accuracy compared to using real images alone, though these improvements were not statistically significant. Our findings demonstrate the potential of SI to train accurate ODMs when data is scarce, offering important perspectives for wildlife monitoring by enabling rare or inaccessible species to be monitored and to increase monitoring frequency. This approach could be used to initiate ODMs without real data and refine it as real images are acquired over time.

</details>


### [207] [Advancing Annotat3D with Harpia: A CUDA-Accelerated Library For Large-Scale Volumetric Data Segmentation](https://arxiv.org/abs/2511.11890)
*Camila Machado de Araujo,Egon P. B. S. Borges,Ricardo Marcelo Canteiro Grangeiro,Allan Pinto*

Main category: cs.CV

TL;DR: This paper presents Harpia, a CUDA-based library designed to enable efficient, large-scale 3D imaging segmentation with features like GPU acceleration and interactive interfaces.


<details>
  <summary>Details</summary>
Motivation: The need to process increasingly large datasets from high-resolution 3D imaging tools has strained existing methods, necessitating new solutions for efficient segmentation and analysis.

Method: The study introduces Harpia, a new processing library for Annotat3D, built with CUDA, incorporating GPU-accelerated tools, chunked execution, and memory optimization for handling large datasets.

Result: The proposed system improves processing speed, memory efficiency, and scalability, outperforming popular tools like NVIDIA cuCIM and scikit-image in benchmarks.

Conclusion: Harpia enhances segmentation workflows with GPU-based acceleration, human-in-the-loop interaction, and memory management, proving its capability in HPC and collaborative imaging settings.

Abstract: High-resolution volumetric imaging techniques, such as X-ray tomography and advanced microscopy, generate increasingly large datasets that challenge existing tools for efficient processing, segmentation, and interactive exploration. This work introduces new capabilities to Annotat3D through Harpia, a new CUDA-based processing library designed to support scalable, interactive segmentation workflows for large 3D datasets in high-performance computing (HPC) and remote-access environments. Harpia features strict memory control, native chunked execution, and a suite of GPU-accelerated filtering, annotation, and quantification tools, enabling reliable operation on datasets exceeding single-GPU memory capacity. Experimental results demonstrate significant improvements in processing speed, memory efficiency, and scalability compared to widely used frameworks such as NVIDIA cuCIM and scikit-image. The system's interactive, human-in-the-loop interface, combined with efficient GPU resource management, makes it particularly suitable for collaborative scientific imaging workflows in shared HPC infrastructures.

</details>


### [208] [Prompt Triage: Structured Optimization Enhances Vision-Language Model Performance on Medical Imaging Benchmarks](https://arxiv.org/abs/2511.11898)
*Arnav Singhvi,Vasiliki Bikia,Asad Aali,Akshay Chaudhari,Roxana Daneshjou*

Main category: cs.CV

TL;DR: The paper explores the use of automated prompt optimization in vision-language models (VLMs) for medical applications, yielding significant improvements over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Vision-language foundation models underperform in medical tasks, and existing solutions like finetuning and manual prompting are computationally expensive or impractical for medical use. There is a need for scalable and accessible methods to improve their performance.

Method: The authors adapt the Declarative Self-improving Python (DSPy) framework for automated prompt optimization in medical VLMs, implementing and evaluating prompting pipelines across various medical imaging tasks and models.

Result: The optimized pipelines achieved a median 53% performance improvement over zero-shot baselines, with remarkable gains of up to 3,400% in some cases.

Conclusion: Automated prompt optimization enhances the performance of medical AI systems while reducing dependency on manual prompts, improving scalability, and preserving data privacy. The study highlights its potential for clinical applications and supports reproducible research through public release of evaluation pipelines.

Abstract: Vision-language foundation models (VLMs) show promise for diverse imaging tasks but often underperform on medical benchmarks. Prior efforts to improve performance include model finetuning, which requires large domain-specific datasets and significant compute, or manual prompt engineering, which is hard to generalize and often inaccessible to medical institutions seeking to deploy these tools. These challenges motivate interest in approaches that draw on a model's embedded knowledge while abstracting away dependence on human-designed prompts to enable scalable, weight-agnostic performance improvements. To explore this, we adapt the Declarative Self-improving Python (DSPy) framework for structured automated prompt optimization in medical vision-language systems through a comprehensive, formal evaluation. We implement prompting pipelines for five medical imaging tasks across radiology, gastroenterology, and dermatology, evaluating 10 open-source VLMs with four prompt optimization techniques. Optimized pipelines achieved a median relative improvement of 53% over zero-shot prompting baselines, with the largest gains ranging from 300% to 3,400% on tasks where zero-shot performance is low. These results highlight the substantial potential of applying automated prompt optimization to medical AI systems, demonstrating significant gains for vision-based applications requiring accurate clinical image interpretation. By reducing dependence on prompt design to elicit intended outputs, these techniques allow clinicians to focus on patient care and clinical decision-making. Furthermore, our experiments offer scalability and preserve data privacy, demonstrating performance improvement on open-source VLMs. We publicly release our evaluation pipelines to support reproducible research on specialized medical tasks, available at https://github.com/DaneshjouLab/prompt-triage-lab.

</details>


### [209] [PI-NAIM: Path-Integrated Neural Adaptive Imputation Model](https://arxiv.org/abs/2511.11908)
*Afifa Khaled,Ebrahim Hamid Sumiea*

Main category: cs.CV

TL;DR: The paper introduces PI-NAIM, a dual-path architecture for handling missing modalities in medical imaging and multi-modal clinical settings, achieving state-of-the-art results in imputation accuracy and downstream tasks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of missing modalities in medical imaging pipelines where current methods are either not representative enough or computationally expensive.

Method: Proposes a dual-path architecture with dynamic routing based on missingness complexity. It uses statistical imputation (MICE) for simple cases, neural networks (GAIN) for complex cases, and incorporates cross-path attention fusion for embeddings, optimizing both imputation accuracy and task performance jointly.

Result: Demonstrates state-of-the-art performance on MIMIC-III and multimodal benchmarks with improved metrics: RMSE of 0.108 and AUROC of 0.812 in mortality prediction.

Conclusion: PI-NAIM effectively addresses missing data challenges and integrates into broader vision pipelines, offering a scalable, efficient, and accurate imputation framework.

Abstract: Medical imaging and multi-modal clinical settings often face the challange of missing modality in their diagnostic pipelines. Existing imputation methods either lack representational capacity or are computationally expensive. We propose PI-NAIM, a novel dual-path architecture that dynamically routes samples to optimized imputation approaches based on missingness complexity. Our framework integrates: (1) intelligent path routing that directs low missingness samples to efficient statistical imputation (MICE) and complex patterns to powerful neural networks (GAIN with temporal analysis); (2) cross-path attention fusion that leverages missingness-aware embeddings to intelligently combine both branches; and (3) end-to-end joint optimization of imputation accuracy and downstream task performance. Extensive experiments on MIMIC-III and multimodal benchmarks demonstrate state-of-the-art performance, achieving RMSE of 0.108 (vs. baselines' 0.119-0.152) and substantial gains in downstream tasks with an AUROC of 0.812 for mortality prediction. PI-NAIM's modular design enables seamless integration into vision pipelines handling incomplete sensor measurements, missing modalities, or corrupted inputs, providing a unified solution for real-world scenario. The code is publicly available at https://github.com/AfifaKhaled/PI-NAIM-Path-Integrated-Neural-Adaptive-Imputation-Model

</details>


### [210] [Seeing the Forest and the Trees: Query-Aware Tokenizer for Long-Video Multimodal Language Models](https://arxiv.org/abs/2511.11910)
*Siyou Li,Huanan Wu,Juexi Shao,Yinghao Ma,Yujian Gan,Yihao Luo,Yuwei Wang,Dong Nie,Lu Wang,Wengqing Wu,Le Zhang,Massimo Poesio,Juntao Yu*

Main category: cs.CV

TL;DR: The paper introduces Query-aware Token Selector (QTSplus), a mechanism to enhance long video understanding in multimodal large language models (MLLMs) by compressing vision tokens for better memory, latency, and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the challenge in long video understanding caused by the exponential growth in attention cost, memory, and latency as the video length increases.

Method: QTSplus dynamically selects essential visual information based on the text query using cross-attention, predicts necessary token budgets per query, and selectively processes Top-n tokens while preserving temporal order with a re-encoder.

Result: QTSplus compresses vision tokens up to 89%, reduces end-to-end latency by 28%, maintains near-parity accuracy with original models on eight benchmarks, and significantly improves specific metrics like TempCompass direction and order accuracies by +20.5 and +5.6 points respectively.

Conclusion: The proposed QTSplus mechanism effectively scales MLLMs for real-world long-video tasks while preserving essential task-relevant information and efficiency gains, with plans to release all associated resources publicly.

Abstract: Despite the recent advances in the video understanding ability of multimodal large language models (MLLMs), long video understanding remains a challenge. One of the main issues is that the number of vision tokens grows linearly with video length, which causes an explosion in attention cost, memory, and latency. To solve this challenge, we present Query-aware Token Selector (\textbf{QTSplus}), a lightweight yet powerful visual token selection module that serves as an information gate between the vision encoder and LLMs. Given a text query and video tokens, QTSplus dynamically selects the most important visual evidence for the input text query by (i) scoring visual tokens via cross-attention, (ii) \emph{predicting} an instance-specific retention budget based on the complexity of the query, and (iii) \emph{selecting} Top-$n$ tokens with a differentiable straight-through estimator during training and a hard gate at inference. Furthermore, a small re-encoder preserves temporal order using absolute time information, enabling second-level localization while maintaining global coverage.
  Integrated into Qwen2.5-VL, QTSplus compresses the vision stream by up to \textbf{89\%} and reduces end-to-end latency by \textbf{28\%} on long videos. The evaluation on eight long video understanding benchmarks shows near-parity accuracy overall when compared with the original Qwen models and outperforms the original model by \textbf{+20.5} and \textbf{+5.6} points respectively on TempCompass direction and order accuracies. These results show that QTSplus is an effective, general mechanism for scaling MLLMs to real-world long-video scenarios while preserving task-relevant evidence.
  We will make all code, data, and trained models' weights publicly available.

</details>


### [211] [Calibrated Decomposition of Aleatoric and Epistemic Uncertainty in Deep Features for Inference-Time Adaptation](https://arxiv.org/abs/2511.12389)
*Divake Kumar,Patrick Poggi,Sina Tayebati,Devashri Naik,Nilesh Ahuja,Amit Ranjan Trivedi*

Main category: cs.CV

TL;DR: This paper introduces a framework to separate aleatoric and epistemic uncertainties in deep learning, improving inference efficiency and prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the issue of collapsing uncertainties into a single confidence score in estimators, which hinders effective allocation of compute resources and adaptive inference.

Method: The framework estimates aleatoric uncertainty using a regularized density model and epistemic uncertainty through three orthogonal components (local support deficiency, manifold spectral collapse, and feature inconsistency). It also integrates these decompositions into a conformal calibration procedure for better prediction intervals.

Result: The proposed method reduces compute by about 60% on MOT17 data with minimal accuracy loss and improves computational savings by 13.6 percentage points over baseline approaches.

Conclusion: The orthogonal decomposition of uncertainties enhances computational efficiency and enables self-regulating visual inference without compromising prediction quality.

Abstract: Most estimators collapse all uncertainty modes into a single confidence score, preventing reliable reasoning about when to allocate more compute or adjust inference. We introduce Uncertainty-Guided Inference-Time Selection, a lightweight inference time framework that disentangles aleatoric (data-driven) and epistemic (model-driven) uncertainty directly in deep feature space. Aleatoric uncertainty is estimated using a regularized global density model, while epistemic uncertainty is formed from three complementary components that capture local support deficiency, manifold spectral collapse, and cross-layer feature inconsistency. These components are empirically orthogonal and require no sampling, no ensembling, and no additional forward passes. We integrate the decomposed uncertainty into a distribution free conformal calibration procedure that yields significantly tighter prediction intervals at matched coverage. Using these components for uncertainty guided adaptive model selection reduces compute by approximately 60 percent on MOT17 with negligible accuracy loss, enabling practical self regulating visual inference. Additionally, our ablation results show that the proposed orthogonal uncertainty decomposition consistently yields higher computational savings across all MOT17 sequences, improving margins by 13.6 percentage points over the total-uncertainty baseline.

</details>


### [212] [From Events to Clarity: The Event-Guided Diffusion Framework for Dehazing](https://arxiv.org/abs/2511.11944)
*Ling Wang,Yunfan Lu,Wenzong Ma,Huizai Yao,Pengteng Li,Hui Xiong*

Main category: cs.CV

TL;DR: This paper introduces a novel approach to dehazing by leveraging event cameras and an event-guided diffusion model, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Dehazing using traditional RGB-based methods is ill-posed due to limited dynamic range and the inability to preserve structure and illumination details in hazy conditions.

Method: An event-guided diffusion model is developed, utilizing HDR features from event cameras to enhance structural clarity and realism in hazy image reconstruction.

Result: Experiments on benchmarks and a newly collected drone dataset demonstrate significant improvements, with state-of-the-art performance on hazy image dehazing tasks.

Conclusion: Event cameras, combined with the event-guided diffusion model, offer an effective solution for overcoming the limitations of traditional RGB-based dehazing techniques, making progress in real-world hazy scenarios.

Abstract: Clear imaging under hazy conditions is a critical task. Prior-based and neural methods have improved results. However, they operate on RGB frames, which suffer from limited dynamic range. Therefore, dehazing remains ill-posed and can erase structure and illumination details. To address this, we use event cameras for dehazing for the \textbf{first time}. Event cameras offer much higher HDR ($120 dBvs.60 dB$) and microsecond latency, therefore they suit hazy scenes. In practice, transferring HDR cues from events to frames is hard because real paired data are scarce. To tackle this, we propose an event-guided diffusion model that utilizes the strong generative priors of diffusion models to reconstruct clear images from hazy inputs by effectively transferring HDR information from events. Specifically, we design an event-guided module that maps sparse HDR event features, \textit{e.g.,} edges, corners, into the diffusion latent space. This clear conditioning provides precise structural guidance during generation, improves visual realism, and reduces semantic drift. For real-world evaluation, we collect a drone dataset in heavy haze (AQI = 341) with synchronized RGB and event sensors. Experiments on two benchmarks and our dataset achieve state-of-the-art results.

</details>


### [213] [Evaluation of Attention Mechanisms in U-Net Architectures for Semantic Segmentation of Brazilian Rock Art Petroglyphs](https://arxiv.org/abs/2511.11959)
*Leonardi Melo,Luís Gustavo,Dimmy Magalhães,Lucciani Vieira,Mauro Araújo*

Main category: cs.CV

TL;DR: The paper compares three U-Net-based semantic segmentation architectures for rock art petroglyphs, with attention mechanisms improving results.


<details>
  <summary>Details</summary>
Motivation: The study aims to enhance semantic segmentation accuracy of rock art petroglyphs for digital preservation of archaeological heritage.

Method: Three U-Net architectures incorporating Border-Enhanced Gaussian Loss function, residual blocks, gated attention mechanisms, and spatial-channel attention modules were examined using 5-fold cross-validation.

Result: Attention-Residual BEGL-UNet achieved the highest Dice Score (0.710), recall (0.854), and validation loss (0.067), while Spatial Channel Attention BEGL-UNet showed comparable performance.

Conclusion: Attention mechanisms significantly improve semantic segmentation, aiding in digital preservation of archaeological heritage with superior Dice Scores compared to baselines.

Abstract: This study presents a comparative analysis of three U-Net-based architectures for semantic segmentation of rock art petroglyphs from Brazilian archaeological sites. The investigated architectures were: (1) BEGL-UNet with Border-Enhanced Gaussian Loss function; (2) Attention-Residual BEGL-UNet, incorporating residual blocks and gated attention mechanisms; and (3) Spatial Channel Attention BEGL-UNet, which employs spatial-channel attention modules based on Convolutional Block Attention Module. All implementations employed the BEGL loss function combining binary cross-entropy with Gaussian edge enhancement. Experiments were conducted on images from the Poço da Bebidinha Archaeological Complex, Piauí, Brazil, using 5-fold cross-validation. Among the architectures, Attention-Residual BEGL-UNet achieved the best overall performance with Dice Score of 0.710, validation loss of 0.067, and highest recall of 0.854. Spatial Channel Attention BEGL-UNet obtained comparable performance with DSC of 0.707 and recall of 0.857. The baseline BEGL-UNet registered DSC of 0.690. These results demonstrate the effectiveness of attention mechanisms for archaeological heritage digital preservation, with Dice Score improvements of 2.5-2.9% over the baseline.

</details>


### [214] [From Classification to Cross-Modal Understanding: Leveraging Vision-Language Models for Fine-Grained Renal Pathology](https://arxiv.org/abs/2511.11984)
*Zhenhao Guo,Rachit Saluja,Tianyuan Yao,Quan Liu,Junchao Zhu,Haibo Wang,Daniel Reisenbüchler,Yuankai Huo,Benjamin Liechty,David J. Pisapia,Kenji Ikemura,Steven Salvatoree,Surya Seshane,Mert R. Sabuncu,Yihe Yang,Ruining Deng*

Main category: cs.CV

TL;DR: This paper studies the adaptation of vision-language models (VLMs) for fine-grained glomerular subtyping in pathology under few-shot learning conditions, providing insights into model selection and strategies.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of obtaining clinically valuable labels for fine-grained glomerular subtyping in kidney biopsies, especially under data constraints, and to explore how vision-language models can be adapted for this task.

Method: The study models fine-grained glomerular subtyping as a few-shot learning problem and systematically evaluates pathology-specialized and general-purpose vision-language models. It assesses classification performance and examines the geometry of learned representations through feature alignment and separability.

Result: The results show that pathology-specialized VLMs paired with vanilla fine-tuning are most effective, even with 4-8 labeled examples per subtype, showing significant gains in discrimination and calibration. Discrimination between positive and negative examples is equally important as image-text alignment.

Conclusion: The study concludes that supervision level and adaptation strategy jointly influence diagnostic performance and multimodal structure, providing guidance for selecting models, adapting strategies, and prioritizing annotation efforts.

Abstract: Fine-grained glomerular subtyping is central to kidney biopsy interpretation, but clinically valuable labels are scarce and difficult to obtain. Existing computational pathology approaches instead tend to evaluate coarse diseased classification under full supervision with image-only models, so it remains unclear how vision-language models (VLMs) should be adapted for clinically meaningful subtyping under data constraints. In this work, we model fine-grained glomerular subtyping as a clinically realistic few-shot problem and systematically evaluate both pathology-specialized and general-purpose vision-language models under this setting. We assess not only classification performance (accuracy, AUC, F1) but also the geometry of the learned representations, examining feature alignment between image and text embeddings and the separability of glomerular subtypes. By jointly analyzing shot count, model architecture and domain knowledge, and adaptation strategy, this study provides guidance for future model selection and training under real clinical data constraints. Our results indicate that pathology-specialized vision-language backbones, when paired with the vanilla fine-tuning, are the most effective starting point. Even with only 4-8 labeled examples per glomeruli subtype, these models begin to capture distinctions and show substantial gains in discrimination and calibration, though additional supervision continues to yield incremental improvements. We also find that the discrimination between positive and negative examples is as important as image-text alignment. Overall, our results show that supervision level and adaptation strategy jointly shape both diagnostic performance and multimodal structure, providing guidance for model selection, adaptation strategies, and annotation investment.

</details>


### [215] [BeyondFacial: Identity-Preserving Personalized Generation Beyond Facial Close-ups](https://arxiv.org/abs/2511.11989)
*Songsong Zhang,Chuanqi Tang,Hongguang Zhang,Guijian Tang,Minglong Li,Xueqiong Li,Shaowu Yang,Yuanxi Peng,Wenjing Yang,Jing Zhao*

Main category: cs.CV

TL;DR: This paper introduces a novel method to overcome the limitations of identity-preserving personalized generation approaches that overemphasize facial-focused outputs, achieving better semantic consistency and visual narrativity.


<details>
  <summary>Details</summary>
Motivation: Existing methods in personalized generation struggle with over-reliance on facial close-ups, undermining narrative depth and semantic consistency when handling complex text prompts. This limitation arises from conflicts between identity features and semantic expressiveness.

Method: The paper proposes a Dual-Line Inference (DLI) pipeline for separating identity and semantic control, an Identity Adaptive Fusion (IdAF) strategy for later-stage ID-semantic integration without interference, and an Identity Aggregation Prepending (IdAP) module for improved initialization and preservation of identity features.

Result: Experimental findings show the method's ability to maintain identity fidelity while achieving semantically consistent outputs beyond facial close-ups. It avoids manual masking or extensive project-specific tuning.

Conclusion: This approach enhances identity-preserving personalized generation in complex tasks, broadening applications in film production and artistic creation and offering effortless integration into existing frameworks.

Abstract: Identity-Preserving Personalized Generation (IPPG) has advanced film production and artistic creation, yet existing approaches overemphasize facial regions, resulting in outputs dominated by facial close-ups.These methods suffer from weak visual narrativity and poor semantic consistency under complex text prompts, with the core limitation rooted in identity (ID) feature embeddings undermining the semantic expressiveness of generative models. To address these issues, this paper presents an IPPG method that breaks the constraint of facial close-ups, achieving synergistic optimization of identity fidelity and scene semantic creation. Specifically, we design a Dual-Line Inference (DLI) pipeline with identity-semantic separation, resolving the representation conflict between ID and semantics inherent in traditional single-path architectures. Further, we propose an Identity Adaptive Fusion (IdAF) strategy that defers ID-semantic fusion to the noise prediction stage, integrating adaptive attention fusion and noise decision masking to avoid ID embedding interference on semantics without manual masking. Finally, an Identity Aggregation Prepending (IdAP) module is introduced to aggregate ID information and replace random initializations, further enhancing identity preservation. Experimental results validate that our method achieves stable and effective performance in IPPG tasks beyond facial close-ups, enabling efficient generation without manual masking or fine-tuning. As a plug-and-play component, it can be rapidly deployed in existing IPPG frameworks, addressing the over-reliance on facial close-ups, facilitating film-level character-scene creation, and providing richer personalized generation capabilities for related domains.

</details>


### [216] [Dynamic Parameter Optimization for Highly Transferable Transformation-Based Attacks](https://arxiv.org/abs/2511.11993)
*Jiaming Liang,Chi-Man Pun*

Main category: cs.CV

TL;DR: This paper addresses the limitations of deep neural network vulnerabilities by analyzing transformation-based attacks, proposing the Concentric Decay Model (CDM), and introducing the efficient Dynamic Parameter Optimization (DPO) method to enhance transferability.


<details>
  <summary>Details</summary>
Motivation: The study aims to address societal concerns arising from deep neural network vulnerabilities, particularly the inefficiencies in transformation-based transfer attacks caused by poor parameter optimization.

Method: The authors conducted an empirical study to analyze transformation dynamics, proposed the Concentric Decay Model (CDM) to explain transferable patterns, and introduced Dynamic Parameter Optimization (DPO) to enhance optimization efficiency with lower computational complexity.

Result: Comprehensive experiments show that the proposed DPO method significantly improves the transferability of transformation-based attacks across various models, iterations, and tasks while maintaining computational efficiency.

Conclusion: The combination of CDM and DPO provides an effective framework for addressing parameter optimization challenges in transformation-based transfer attacks, ultimately improving their practicality and efficiency.

Abstract: Despite their wide application, the vulnerabilities of deep neural networks raise societal concerns. Among them, transformation-based attacks have demonstrated notable success in transfer attacks. However, existing attacks suffer from blind spots in parameter optimization, limiting their full potential. Specifically, (1) prior work generally considers low-iteration settings, yet attacks perform quite differently at higher iterations, so characterizing overall performance based only on low-iteration results is misleading. (2) Existing attacks use uniform parameters for different surrogate models, iterations, and tasks, which greatly impairs transferability. (3) Traditional transformation parameter optimization relies on grid search. For n parameters with m steps each, the complexity is O(mn). Large computational overhead limits further optimization of parameters. To address these limitations, we conduct an empirical study with various transformations as baselines, revealing three dynamic patterns of transferability with respect to parameter strength. We further propose a novel Concentric Decay Model (CDM) to effectively explain these patterns. Building on these insights, we propose an efficient Dynamic Parameter Optimization (DPO) based on the rise-then-fall pattern, reducing the complexity to O(nlogm). Comprehensive experiments on existing transformation-based attacks across different surrogate models, iterations, and tasks demonstrate that our DPO can significantly improve transferability.

</details>


### [217] [LithoSeg: A Coarse-to-Fine Framework for High-Precision Lithography Segmentation](https://arxiv.org/abs/2511.12005)
*Xinyu He,Botong Zhao,Bingbing Li,Shujing Lyu,Jiwei Shen,Yue Lu*

Main category: cs.CV

TL;DR: The paper introduces LithoSeg, a segmentation model for lithography SEM images, that improves accuracy, precision, and robustness with less supervision.


<details>
  <summary>Details</summary>
Motivation: Improving lithography SEM image segmentation is vital for better process control and semiconductor yield. Existing methods lack sufficient precision and robustness, motivating the development of a more accurate solution.

Method: The proposed LithoSeg model uses a coarse-to-fine segmentation approach. It employs a Human-in-the-Loop Bootstrapping scheme with the Segment Anything Model (SAM) in the coarse stage and fine-tunes 2D segmentation as a 1D regression problem with a lightweight MLP.

Result: LithoSeg demonstrates superior segmentation accuracy and metrology precision compared to previous methods while requiring reduced supervision.

Conclusion: LithoSeg provides a robust and efficient solution for lithography SEM image segmentation, improving performance with reduced human input, making it suitable for practical applications.

Abstract: Accurate segmentation and measurement of lithography scanning electron microscope (SEM) images are crucial for ensuring precise process control, optimizing device performance, and advancing semiconductor manufacturing yield. Lithography segmentation requires pixel-level delineation of groove contours and consistent performance across diverse pattern geometries and process window. However, existing methods often lack the necessary precision and robustness, limiting their practical applicability. To overcome this challenge, we propose LithoSeg, a coarse-to-fine network tailored for lithography segmentation. In the coarse stage, we introduce a Human-in-the-Loop Bootstrapping scheme for the Segment Anything Model (SAM) to attain robustness with minimal supervision. In the subsequent fine stage, we recast 2D segmentation as 1D regression problem by sampling groove-normal profiles using the coarse mask and performing point-wise refinement with a lightweight MLP. LithoSeg outperforms previous approaches in both segmentation accuracy and metrology precision while requiring less supervision, offering promising prospects for real-world applications.

</details>


### [218] [Uncertainty-Guided Selective Adaptation Enables Cross-Platform Predictive Fluorescence Microscopy](https://arxiv.org/abs/2511.12006)
*Kai-Wen K. Yang,Andrew Bai,Alexandra Bermudez,Yunqi Hong,Zoe Latham,Iris Sloan,Michael Liu,Vishrut Goyal,Cho-Jui Hsieh,Neil Y. C. Lin*

Main category: cs.CV

TL;DR: The study presents SIT-ADDA-Auto, a self-configuring deep learning framework that adapts early convolutional layers for effective label-free domain adaptation in microscopy, ensuring robust image reconstruction and segmentation across varying conditions.


<details>
  <summary>Details</summary>
Motivation: Deep learning models often fail on microscopy images when there are changes in acquisition settings or instrument types, disrupting learned semantic representations. Overcoming this challenge is crucial for improving model reliability in microscopy applications.

Method: The proposed method involves adapting only the early convolutional layers of neural networks while keeping deeper layers frozen. SIT-ADDA-Auto automatically selects the adaptation depth using predictive uncertainty without requiring target labels.

Result: SIT-ADDA-Auto outperforms full-encoder adaptation and non-adversarial baseline methods in image reconstruction and segmentation, handling exposure shifts, illumination variations, cross-instrument transfers, and multiple stains effectively.

Conclusion: Adapting shallow convolutional layers ensures robust domain transfer in microscopy, and SIT-ADDA-Auto offers a practical, label-free adaptation strategy for diverse field settings with publicly available code.

Abstract: Deep learning is transforming microscopy, yet models often fail when applied to images from new instruments or acquisition settings. Conventional adversarial domain adaptation (ADDA) retrains entire networks, often disrupting learned semantic representations. Here, we overturn this paradigm by showing that adapting only the earliest convolutional layers, while freezing deeper layers, yields reliable transfer. Building on this principle, we introduce Subnetwork Image Translation ADDA with automatic depth selection (SIT-ADDA-Auto), a self-configuring framework that integrates shallow-layer adversarial alignment with predictive uncertainty to automatically select adaptation depth without target labels. We demonstrate robustness via multi-metric evaluation, blinded expert assessment, and uncertainty-depth ablations. Across exposure and illumination shifts, cross-instrument transfer, and multiple stains, SIT-ADDA improves reconstruction and downstream segmentation over full-encoder adaptation and non-adversarial baselines, with reduced drift of semantic features. Our results provide a design rule for label-free adaptation in microscopy and a recipe for field settings; the code is publicly available.

</details>


### [219] [Enhancing Road Safety Through Multi-Camera Image Segmentation with Post-Encroachment Time Analysis](https://arxiv.org/abs/2511.12018)
*Shounak Ray Chaudhuri,Arash Jahangiri,Christopher Paolini*

Main category: cs.CV

TL;DR: This paper introduces a real-time safety analysis framework using multi-camera computer vision at intersections, improving traditional methods by addressing data limitations.


<details>
  <summary>Details</summary>
Motivation: To enhance intersection safety analysis by overcoming limitations of crash-based studies, focusing on real-time hazard assessment with better data granularity.

Method: Utilized a computer vision framework with four cameras, YOLOv11 for vehicle detection, bird's-eye transform alignment, and a novel PET algorithm, supported by edge computing devices.

Result: The framework accurately identified high-risk regions with sub-second precision, processed data for heatmaps at 2.68 FPS, and enabled scalable, real-time monitoring.

Conclusion: This decentralized vision-based PET methodology provides a replicable, high-resolution tool for intelligent transportation systems and intersection safety assessments.

Abstract: Traffic safety analysis at signalized intersections is vital for reducing vehicle and pedestrian collisions, yet traditional crash-based studies are limited by data sparsity and latency. This paper presents a novel multi-camera computer vision framework for real-time safety assessment through Post-Encroachment Time (PET) computation, demonstrated at the intersection of H Street and Broadway in Chula Vista, California. Four synchronized cameras provide continuous visual coverage, with each frame processed on NVIDIA Jetson AGX Xavier devices using YOLOv11 segmentation for vehicle detection. Detected vehicle polygons are transformed into a unified bird's-eye map using homography matrices, enabling alignment across overlapping camera views. A novel pixel-level PET algorithm measures vehicle position without reliance on fixed cells, allowing fine-grained hazard visualization via dynamic heatmaps, accurate to 3.3 sq-cm. Timestamped vehicle and PET data is stored in an SQL database for long-term monitoring. Results over various time intervals demonstrate the framework's ability to identify high-risk regions with sub-second precision and real-time throughput on edge devices, producing data for an 800 x 800 pixel logarithmic heatmap at an average of 2.68 FPS. This study validates the feasibility of decentralized vision-based PET analysis for intelligent transportation systems, offering a replicable methodology for high-resolution, real-time, and scalable intersection safety evaluation.

</details>


### [220] [LIHE: Linguistic Instance-Split Hyperbolic-Euclidean Framework for Generalized Weakly-Supervised Referring Expression Comprehension](https://arxiv.org/abs/2511.12020)
*Xianglong Shi,Silin Cheng,Sirui Zhao,Yunhan Jiang,Enhong Chen,Yang Liu,Sebastien Ourselin*

Main category: cs.CV

TL;DR: This paper introduces Weakly-Supervised Generalized Referring Expression Comprehension (WGREC), which handles expressions with a variable number of referents using a novel framework, Linguistic Instance-Split Hyperbolic-Euclidean (LIHE).


<details>
  <summary>Details</summary>
Motivation: The authors noticed limitations in existing methods for weakly-supervised referring expression comprehension that focus only on one-to-one mappings, making them incapable of handling scenarios with zero or multiple targets.

Method: They developed the LIHE framework, consisting of two stages: Referential Decoupling (which predicts target count and splits expressions) and Referent Grounding (using HEMix, a hybrid similarity module combining Euclidean and hyperbolic geometry to address semantic collapse).

Result: The proposed framework performed effectively, establishing the first WGREC baseline on datasets like gRefCOCO and Ref-ZOM. HEMix achieved consistent performance improvements on standard REC benchmarks, with IoU@0.5 increasing by 2.5%.

Conclusion: The novel methodology successfully extends weakly-supervised REC to WGREC, addressing supervision and semantic issues, and opening opportunities for more practical implementations in referring expression comprehension research.

Abstract: Existing Weakly-Supervised Referring Expression Comprehension (WREC) methods, while effective, are fundamentally limited by a one-to-one mapping assumption, hindering their ability to handle expressions corresponding to zero or multiple targets in realistic scenarios. To bridge this gap, we introduce the Weakly-Supervised Generalized Referring Expression Comprehension task (WGREC), a more practical paradigm that handles expressions with variable numbers of referents. However, extending WREC to WGREC presents two fundamental challenges: supervisory signal ambiguity, where weak image-level supervision is insufficient for training a model to infer the correct number and identity of referents, and semantic representation collapse, where standard Euclidean similarity forces hierarchically-related concepts into non-discriminative clusters, blurring categorical boundaries. To tackle these challenges, we propose a novel WGREC framework named Linguistic Instance-Split Hyperbolic-Euclidean (LIHE), which operates in two stages. The first stage, Referential Decoupling, predicts the number of target objects and decomposes the complex expression into simpler sub-expressions. The second stage, Referent Grounding, then localizes these sub-expressions using HEMix, our innovative hybrid similarity module that synergistically combines the precise alignment capabilities of Euclidean proximity with the hierarchical modeling strengths of hyperbolic geometry. This hybrid approach effectively prevents semantic collapse while preserving fine-grained distinctions between related concepts. Extensive experiments demonstrate LIHE establishes the first effective weakly supervised WGREC baseline on gRefCOCO and Ref-ZOM, while HEMix achieves consistent improvements on standard REC benchmarks, improving IoU@0.5 by up to 2.5\%. The code is available at https://anonymous.4open.science/r/LIHE.

</details>


### [221] [Null-Space Diffusion Distillation for Efficient Photorealistic Lensless Imaging](https://arxiv.org/abs/2511.12024)
*Jose Reinaldo Cunha Santos A V Silva Neto,Hodaka Kawachi,Yasushi Yagi,Tomoya Nakamura*

Main category: cs.CV

TL;DR: This paper introduces Null-Space Diffusion Distillation (NSDD), a method for photorealistic reconstruction in lensless imaging without requiring paired supervision.


<details>
  <summary>Details</summary>
Motivation: Current lensless imaging methods often rely on paired supervision with lensless-lensed setups, which can introduce bias due to domain mismatches.

Method: The authors propose NSDD, a distillation approach that separates range-space enforcement and null-space diffusion-prior updates. NSDD uses a single-pass model that relies on measurements and range-space anchors.

Result: NSDD preserves measurement consistency and achieves photorealistic reconstructions with competitive runtime and memory efficiency, second-best LPIPS perceptual quality, and surpassing classical methods.

Conclusion: NSDD offers a practical and efficient path toward fast, consistent, and photorealistic lensless imaging, addressing challenges associated with noisy and ill-posed deconvolution setups.

Abstract: State-of-the-art photorealistic reconstructions for lensless cameras often rely on paired lensless-lensed supervision, which can bias models due to lens-lensless domain mismatch. To avoid this, ground-truth-free diffusion priors are attractive; however, generic formulations tuned for conventional inverse problems often break under the noisy, highly multiplexed, and ill-posed lensless deconvolution setting. We observe that methods which separate range-space enforcement from null-space diffusion-prior updates yield stable, realistic reconstructions. Building on this, we introduce Null-Space Diffusion Distillation (NSDD): a single-pass student that distills the null-space component of an iterative DDNM+ solver, conditioned on the lensless measurement and on a range-space anchor. NSDD preserves measurement consistency and achieves photorealistic results without paired supervision at a fraction of the runtime and memory. On Lensless-FFHQ and PhlatCam, NSDD is the second fastest, behind Wiener, and achieves near-teacher perceptual quality (second-best LPIPS, below DDNM+), outperforming DPS and classical convex baselines. These results suggest a practical path toward fast, ground-truth-free, photorealistic lensless imaging.

</details>


### [222] [Bridging Vision and Language for Robust Context-Aware Surgical Point Tracking: The VL-SurgPT Dataset and Benchmark](https://arxiv.org/abs/2511.12026)
*Rulin Zhou,Wenlong He,An Wang,Jianhang Zhang,Xuanhui Zeng,Xi Zhang,Chaowei Zhu,Haijun Hu,Hongliang Ren*

Main category: cs.CV

TL;DR: This paper introduces VL-SurgPT, a multimodal dataset combining visual point tracking with textual descriptions, and demonstrates improvement in surgical point tracking through a text-guided approach.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the challenges in surgical point tracking due to visual complexities like smoke, reflections, and deformations, and the absence of semantic context in existing datasets.

Method: The authors created a large-scale multimodal dataset (VL-SurgPT) with visual and textual annotations, established benchmarks with tracking methods, and proposed TG-SurgPT, a text-guided tracking approach.

Result: Experimental results show that using both visual and semantic point status information improves accuracy and reliability of tracking, especially in visually challenging surgical scenarios.

Conclusion: The dataset and approach enhance the development of robust, context-aware surgical tracking systems that perform well under difficult intraoperative conditions.

Abstract: Accurate point tracking in surgical environments remains challenging due to complex visual conditions, including smoke occlusion, specular reflections, and tissue deformation. While existing surgical tracking datasets provide coordinate information, they lack the semantic context necessary to understand tracking failure mechanisms. We introduce VL-SurgPT, the first large-scale multimodal dataset that bridges visual tracking with textual descriptions of point status in surgical scenes. The dataset comprises 908 in vivo video clips, including 754 for tissue tracking (17,171 annotated points across five challenging scenarios) and 154 for instrument tracking (covering seven instrument types with detailed keypoint annotations). We establish comprehensive benchmarks using eight state-of-the-art tracking methods and propose TG-SurgPT, a text-guided tracking approach that leverages semantic descriptions to improve robustness in visually challenging conditions. Experimental results demonstrate that incorporating point status information significantly improves tracking accuracy and reliability, particularly in adverse visual scenarios where conventional vision-only methods struggle. By bridging visual and linguistic modalities, VL-SurgPT enables the development of context-aware tracking systems crucial for advancing computer-assisted surgery applications that can maintain performance even under challenging intraoperative conditions.

</details>


### [223] [GCAgent: Long-Video Understanding via Schematic and Narrative Episodic Memory](https://arxiv.org/abs/2511.12027)
*Jeong Hun Yeo,Sangyun Chung,Sungjune Park,Dae Hoe Kim,Jinyoung Moon,Yong Man Ro*

Main category: cs.CV

TL;DR: This paper introduces GCAgent, a framework designed to improve long-video understanding by addressing token limits and long-term dependencies through structured episodic memory.


<details>
  <summary>Details</summary>
Motivation: The paper aims to solve challenges in long-video understanding for MLLMs, such as token limitations and difficulty in capturing long-term dependencies, which hinder deep reasoning across extended content.

Method: A Global-Context-Aware Agent framework, incorporating Schematic and Narrative Episodic Memory, is proposed. This memory organizes events and their relations for context-aware inference in a multi-stage Perception-Action-Reflection cycle.

Result: GCAgent offers up to 23.5% accuracy improvement on the Video-MME Long split against a strong baseline, achieves state-of-the-art performance in 7B-scale MLLMs, and secures 73.4% accuracy in long-video tasks.

Conclusion: The proposed framework effectively enhances long-video understanding through structured memory and reasoning processes, establishing new benchmarks for MLLM performance.

Abstract: Long-video understanding remains a significant challenge for Multimodal Large Language Models (MLLMs) due to inherent token limitations and the complexity of capturing long-term temporal dependencies. Existing methods often fail to capture the global context and complex event relationships necessary for deep video reasoning. To address this, we introduce GCAgent, a novel Global-Context-Aware Agent framework that achieves comprehensive long-video understanding. Our core innovation is the Schematic and Narrative Episodic Memory. This memory structurally models events and their causal and temporal relations into a concise, organized context, fundamentally resolving the long-term dependency problem. Operating in a multi-stage Perception-Action-Reflection cycle, our GCAgent utilizes a Memory Manager to retrieve relevant episodic context for robust, context-aware inference. Extensive experiments confirm that GCAgent significantly enhances long-video understanding, achieving up to 23.5\% accuracy improvement on the Video-MME Long split over a strong MLLM baseline. Furthermore, our framework establishes state-of-the-art performance among comparable 7B-scale MLLMs, achieving 73.4\% accuracy on the Long split and the highest overall average (71.9\%) on the Video-MME benchmark, validating our agent-based reasoning paradigm and structured memory for cognitively-inspired long-video understanding.

</details>


### [224] [VPHO: Joint Visual-Physical Cue Learning and Aggregation for Hand-Object Pose Estimation](https://arxiv.org/abs/2511.12030)
*Jun Zhou,Chi Xu,Kaifeng Tang,Yuting Ge,Tingrui Guo,Li Cheng*

Main category: cs.CV

TL;DR: The paper proposes a novel method for estimating 3D hand-object poses from a single RGB image by combining visual and physical cues, improving accuracy and physical correctness over prior methods.


<details>
  <summary>Details</summary>
Motivation: The goal is to improve the accuracy and physical realism of hand-object pose estimation from a single RGB image, which is important for applications like augmented reality and human-computer interaction. Existing methods often violate physical constraints, limiting their effectiveness.

Method: The framework involves joint visual-physical cue learning to better represent hand-object interactions. Additionally, it introduces candidate pose aggregation, refining multiple diffusion-generated poses with both visual and physical predictions for realistic estimation.

Result: The proposed method significantly surpasses state-of-the-art approaches in terms of both pose accuracy and adherence to physical principles according to experimental results.

Conclusion: Integrating visual and physical reasoning into a unified framework leads to more accurate and physically consistent hand-object pose estimates, pushing forward the capabilities in this domain.

Abstract: Estimating the 3D poses of hands and objects from a single RGB image is a fundamental yet challenging problem, with broad applications in augmented reality and human-computer interaction. Existing methods largely rely on visual cues alone, often producing results that violate physical constraints such as interpenetration or non-contact. Recent efforts to incorporate physics reasoning typically depend on post-optimization or non-differentiable physics engines, which compromise visual consistency and end-to-end trainability. To overcome these limitations, we propose a novel framework that jointly integrates visual and physical cues for hand-object pose estimation. This integration is achieved through two key ideas: 1) joint visual-physical cue learning: The model is trained to extract 2D visual cues and 3D physical cues, thereby enabling more comprehensive representation learning for hand-object interactions; 2) candidate pose aggregation: A novel refinement process that aggregates multiple diffusion-generated candidate poses by leveraging both visual and physical predictions, yielding a final estimate that is visually consistent and physically plausible. Extensive experiments demonstrate that our method significantly outperforms existing state-of-the-art approaches in both pose accuracy and physical plausibility.

</details>


### [225] [Improved Masked Image Generation with Knowledge-Augmented Token Representations](https://arxiv.org/abs/2511.12032)
*Guotao Liang,Baoquan Zhang,Zhiyuan Wen,Zihao Han,Yunming Ye*

Main category: cs.CV

TL;DR: KA-MIG incorporates token-level semantic dependency knowledge graphs for enhanced masked image generation, outperforming existing methods on ImageNet.


<details>
  <summary>Details</summary>
Motivation: Existing masked image generation methods struggle to learn semantic dependencies directly from tokens due to their lack of semantic clarity and long sequences.

Method: KA-MIG introduces three knowledge graphs (co-occurrence, semantic similarity, and position-token incompatibility) and uses a graph-aware encoder and fusion mechanism to enrich representations.

Result: KA-MIG enhances semantic dependency understanding, improving class-conditional image generation performance on ImageNet.

Conclusion: Using knowledge-augmented approaches, KA-MIG addresses key limitations in learning semantic dependencies, achieving improved masked image generation quality.

Abstract: Masked image generation (MIG) has demonstrated remarkable efficiency and high-fidelity images by enabling parallel token prediction. Existing methods typically rely solely on the model itself to learn semantic dependencies among visual token sequences. However, directly learning such semantic dependencies from data is challenging because the individual tokens lack clear semantic meanings, and these sequences are usually long. To address this limitation, we propose a novel Knowledge-Augmented Masked Image Generation framework, named KA-MIG, which introduces explicit knowledge of token-level semantic dependencies (\emph{i.e.}, extracted from the training data) as priors to learn richer representations for improving performance. In particular, we explore and identify three types of advantageous token knowledge graphs, including two positive and one negative graphs (\emph{i.e.}, the co-occurrence graph, the semantic similarity graph, and the position-token incompatibility graph). Based on three prior knowledge graphs, we design a graph-aware encoder to learn token and position-aware representations. After that, a lightweight fusion mechanism is introduced to integrate these enriched representations into the existing MIG methods. Resorting to such prior knowledge, our method effectively enhances the model's ability to capture semantic dependencies, leading to improved generation quality. Experimental results demonstrate that our method improves upon existing MIG for class-conditional image generation on ImageNet.

</details>


### [226] [Calibrated Multimodal Representation Learning with Missing Modalities](https://arxiv.org/abs/2511.12034)
*Xiaohao Liu,Xiaobo Xia,Jiaheng Wei,Shuo Yang,Xiu Su,See-Kiong Ng,Tat-Seng Chua*

Main category: cs.CV

TL;DR: The paper introduces CalMRL, a method for multimodal representation learning that addresses issues caused by missing modalities in datasets, improving alignment by modeling missing modalities with inherent connections and priors.


<details>
  <summary>Details</summary>
Motivation: Current multimodal representation learning struggles with datasets containing missing modalities, as traditional methods require all modalities to be present for alignment, causing challenges in broader applications.

Method: The authors propose CalMRL, which calibrates alignments by modeling missing modalities at the representation level. It uses a bi-step learning method and closed-form solutions for optimization, validated through theoretical and practical analyses.

Result: Through experiments and analyses, CalMRL demonstrates improved performance in handling datasets with missing modalities, offering flexibility and superiority in multimodal representation alignment.

Conclusion: CalMRL effectively mitigates alignment issues arising from missing modalities and enhances the general applicability of multimodal representation learning, as validated by empirical evidence and theoretical guidance.

Abstract: Multimodal representation learning harmonizes distinct modalities by aligning them into a unified latent space. Recent research generalizes traditional cross-modal alignment to produce enhanced multimodal synergy but requires all modalities to be present for a common instance, making it challenging to utilize prevalent datasets with missing modalities. We provide theoretical insights into this issue from an anchor shift perspective. Observed modalities are aligned with a local anchor that deviates from the optimal one when all modalities are present, resulting in an inevitable shift. To address this, we propose CalMRL for multimodal representation learning to calibrate incomplete alignments caused by missing modalities. Specifically, CalMRL leverages the priors and the inherent connections among modalities to model the imputation for the missing ones at the representation level. To resolve the optimization dilemma, we employ a bi-step learning method with the closed-form solution of the posterior distribution of shared latents. We validate its mitigation of anchor shift and convergence with theoretical guidance. By equipping the calibrated alignment with the existing advanced method, we offer new flexibility to absorb data with missing modalities, which is originally unattainable. Extensive experiments and comprehensive analyses demonstrate the superiority of CalMRL. Our code, model checkpoints, and evaluation raw data will be publicly available.

</details>


### [227] [SRSplat: Feed-Forward Super-Resolution Gaussian Splatting from Sparse Multi-View Images](https://arxiv.org/abs/2511.12040)
*Xinyuan Hu,Changyue Shi,Chuxiao Yang,Minghao Chen,Jiajun Ding,Tao Wei,Chen Wei,Zhou Yu,Min Tan*

Main category: cs.CV

TL;DR: The paper introduces SRSplat, a 3D reconstruction framework using sparse and low-resolution images, leveraging external reference images and internal texture cues to enhance texture details.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of recovering fine texture details in 3D reconstructions from sparse and low-resolution images, which is critical for applications such as autonomous driving and embodied AI.

Method: SRSplat integrates external high-quality reference images, internal texture cues, and introduces two modules: (1) Reference-Guided Feature Enhancement (RGFE) for feature alignment and fusion; (2) Texture-Aware Density Control (TADC) to refine the 3D scene's Gaussian primitives.

Result: Experiments demonstrate that SRSplat surpasses existing methods across datasets like RealEstate10K, ACID, and DTU with superior generalization to various dataset and resolution scenarios.

Conclusion: SRSplat effectively enhances 3D reconstructions from sparse, low-resolution inputs by introducing innovative methods for texture detail compensation using external and internal information.

Abstract: Feed-forward 3D reconstruction from sparse, low-resolution (LR) images is a crucial capability for real-world applications, such as autonomous driving and embodied AI. However, existing methods often fail to recover fine texture details. This limitation stems from the inherent lack of high-frequency information in LR inputs. To address this, we propose \textbf{SRSplat}, a feed-forward framework that reconstructs high-resolution 3D scenes from only a few LR views. Our main insight is to compensate for the deficiency of texture information by jointly leveraging external high-quality reference images and internal texture cues. We first construct a scene-specific reference gallery, generated for each scene using Multimodal Large Language Models (MLLMs) and diffusion models. To integrate this external information, we introduce the \textit{Reference-Guided Feature Enhancement (RGFE)} module, which aligns and fuses features from the LR input images and their reference twin image. Subsequently, we train a decoder to predict the Gaussian primitives using the multi-view fused feature obtained from \textit{RGFE}. To further refine predicted Gaussian primitives, we introduce \textit{Texture-Aware Density Control (TADC)}, which adaptively adjusts Gaussian density based on the internal texture richness of the LR inputs. Extensive experiments demonstrate that our SRSplat outperforms existing methods on various datasets, including RealEstate10K, ACID, and DTU, and exhibits strong cross-dataset and cross-resolution generalization capabilities.

</details>


### [228] [FedSDA: Federated Stain Distribution Alignment for Non-IID Histopathological Image Classification](https://arxiv.org/abs/2511.12044)
*Cheng-Chang Tsai,Kai-Wen Cheng,Chun-Shien Lu*

Main category: cs.CV

TL;DR: This paper proposes a Federated Stain Distribution Alignment (FedSDA) method to address issues related to non-IID histopathological images in federated learning by aligning data distributions using diffusion models.


<details>
  <summary>Details</summary>
Motivation: To tackle performance issues in federated learning caused by non-IID data distributions, particularly for histopathological images, by focusing on data distribution adjustment.

Method: The authors introduce FedSDA, which aligns stain distributions of client data to a target distribution using diffusion models while ensuring privacy in an FL framework.

Result: Extensive experiments demonstrate the effectiveness of FedSDA in improving and outperforming existing methods aimed at mitigating non-IID data issues.

Conclusion: FedSDA achieves better alignment of non-IID data in federated learning and provides useful insights for computational pathology applications.

Abstract: Federated learning (FL) has shown success in collaboratively training a model among decentralized data resources without directly sharing privacy-sensitive training data. Despite recent advances, non-IID (non-independent and identically distributed) data poses an inevitable challenge that hinders the use of FL. In this work, we address the issue of non-IID histopathological images with feature distribution shifts from an intuitive perspective that has only received limited attention. Specifically, we address this issue from the perspective of data distribution by solely adjusting the data distributions of all clients. Building on the success of diffusion models in fitting data distributions and leveraging stain separation to extract the pivotal features that are closely related to the non-IID properties of histopathological images, we propose a Federated Stain Distribution Alignment (FedSDA) method. FedSDA aligns the stain distribution of each client with a target distribution in an FL framework to mitigate distribution shifts among clients. Furthermore, considering that training diffusion models on raw data in FL has been shown to be susceptible to privacy leakage risks, we circumvent this problem while still effectively achieving alignment. Extensive experimental results show that FedSDA is not only effective in improving baselines that focus on mitigating disparities across clients' model updates but also outperforms baselines that address the non-IID data issues from the perspective of data distribution. We show that FedSDA provides valuable and practical insights for the computational pathology community.

</details>


### [229] [DCMM-Transformer: Degree-Corrected Mixed-Membership Attention for Medical Imaging](https://arxiv.org/abs/2511.12047)
*Huimin Cheng,Xiaowei Yu,Shushan Wu,Luyang Fang,Chao Cao,Jing Zhang,Tianming Liu,Dajiang Zhu,Wenxuan Zhong,Ping Ma*

Main category: cs.CV

TL;DR: The paper introduces DCMM-Transformer, a new ViT architecture leveraging anatomical groupings in medical images to improve interpretability and performance.


<details>
  <summary>Details</summary>
Motivation: Standard Vision Transformers (ViTs) fail to utilize latent anatomical structures in medical images effectively, limiting interpretability and generalization.

Method: The proposed DCMM-Transformer employs a Degree-Corrected Mixed-Membership model as additive bias in self-attention, enabling fully differentiable community structure modeling.

Result: Experiments across various medical imaging datasets show superior performance, generalization, and anatomically meaningful attention maps.

Conclusion: DCMM-Transformer enhances medical image analysis by improving model interpretability and leveraging inherent anatomical structures in a differentiable manner.

Abstract: Medical images exhibit latent anatomical groupings, such as organs, tissues, and pathological regions, that standard Vision Transformers (ViTs) fail to exploit. While recent work like SBM-Transformer attempts to incorporate such structures through stochastic binary masking, they suffer from non-differentiability, training instability, and the inability to model complex community structure. We present DCMM-Transformer, a novel ViT architecture for medical image analysis that incorporates a Degree-Corrected Mixed-Membership (DCMM) model as an additive bias in self-attention. Unlike prior approaches that rely on multiplicative masking and binary sampling, our method introduces community structure and degree heterogeneity in a fully differentiable and interpretable manner. Comprehensive experiments across diverse medical imaging datasets, including brain, chest, breast, and ocular modalities, demonstrate the superior performance and generalizability of the proposed approach. Furthermore, the learned group structure and structured attention modulation substantially enhance interpretability by yielding attention maps that are anatomically meaningful and semantically coherent.

</details>


### [230] [DeiTFake: Deepfake Detection Model using DeiT Multi-Stage Training](https://arxiv.org/abs/2511.12048)
*Saksham Kumar,Ashish Singh,Srinivasarao Thota,Sunil Kumar Singh,Chandan Kumar*

Main category: cs.CV

TL;DR: This paper introduces "DeiTFake," a transformer-based approach for detecting deepfakes, achieving state-of-the-art accuracy on the OpenForensics dataset through its innovative two-stage progressive training method.


<details>
  <summary>Details</summary>
Motivation: Deepfakes pose significant threats to digital media integrity, necessitating accurate and robust detection methods.

Method: The proposed method uses a DeiT-based transformer model coupled with a novel two-stage training strategy. Stage one involves transfer learning with standard augmentations, while stage two employs fine-tuning with advanced augmentations specifically designed for deepfake detection.

Result: DeiTFake achieved 99.22% accuracy and an AUROC of 0.9997 on the OpenForensics dataset, outperforming existing benchmarks.

Conclusion: The DeiTFake model, leveraging innovative training techniques and augmentation strategies, establishes itself as a robust and accurate solution for facial deepfake detection.

Abstract: Deepfakes are major threats to the integrity of digital media. We propose DeiTFake, a DeiT-based transformer and a novel two-stage progressive training strategy with increasing augmentation complexity. The approach applies an initial transfer-learning phase with standard augmentations followed by a fine-tuning phase using advanced affine and deepfake-specific augmentations. DeiT's knowledge distillation model captures subtle manipulation artifacts, increasing robustness of the detection model. Trained on the OpenForensics dataset (190,335 images), DeiTFake achieves 98.71\% accuracy after stage one and 99.22\% accuracy with an AUROC of 0.9997, after stage two, outperforming the latest OpenForensics baselines. We analyze augmentation impact and training schedules, and provide practical benchmarks for facial deepfake detection.

</details>


### [231] [UniABG: Unified Adversarial View Bridging and Graph Correspondence for Unsupervised Cross-View Geo-Localization](https://arxiv.org/abs/2511.12054)
*Cuiqun Chen,Qi Chen,Bin Yang,Xingyi Zhang*

Main category: cs.CV

TL;DR: The paper introduces UniABG, a dual-stage unsupervised framework for cross-view geo-localization, addressing domain gaps and pseudo-label noise.


<details>
  <summary>Details</summary>
Motivation: To overcome scalability challenges and noisy pseudo-labels in cross-view geo-localization without relying on intensive pairwise annotations.

Method: UniABG uses View-Aware Adversarial Bridging to capture view-invariant features and Heterogeneous Graph Filtering Calibration to refine cross-view associations.

Result: UniABG significantly improves unsupervised Satellite → Drone AP by +10.63% on University-1652 and +16.73% on SUES-200, surpassing supervised approaches.

Conclusion: UniABG achieves state-of-the-art performance in unsupervised cross-view geo-localization, demonstrating its effectiveness in modeling view correspondence and scalability.

Abstract: Cross-view geo-localization (CVGL) matches query images ($\textit{e.g.}$, drone) to geographically corresponding opposite-view imagery ($\textit{e.g.}$, satellite). While supervised methods achieve strong performance, their reliance on extensive pairwise annotations limits scalability. Unsupervised alternatives avoid annotation costs but suffer from noisy pseudo-labels due to intrinsic cross-view domain gaps. To address these limitations, we propose $\textit{UniABG}$, a novel dual-stage unsupervised cross-view geo-localization framework integrating adversarial view bridging with graph-based correspondence calibration. Our approach first employs View-Aware Adversarial Bridging (VAAB) to model view-invariant features and enhance pseudo-label robustness. Subsequently, Heterogeneous Graph Filtering Calibration (HGFC) refines cross-view associations by constructing dual inter-view structure graphs, achieving reliable view correspondence. Extensive experiments demonstrate state-of-the-art unsupervised performance, showing that UniABG improves Satellite $\rightarrow$ Drone AP by +10.63\% on University-1652 and +16.73\% on SUES-200, even surpassing supervised baselines. The source code is available at https://github.com/chenqi142/UniABG

</details>


### [232] [PipeDiT: Accelerating Diffusion Transformers in Video Generation with Task Pipelining and Model Decoupling](https://arxiv.org/abs/2511.12056)
*Sijie Wang,Qiang Wang,Shaohuai Shi*

Main category: cs.CV

TL;DR: This paper introduces PipeDiT, a novel pipelining framework to address slow inference and high memory usage in video generation by optimizing GPU utilization and enabling computational parallelism.


<details>
  <summary>Details</summary>
Motivation: The research aims to address the issues of slow inference speeds and high memory consumption in video generation, particularly for diffusion transformer-based models, making them more practical for deployment.

Method: The proposed PipeDiT framework includes: (1) PipeSP, a pipelining algorithm for sequence parallelism to reduce latency through GPU communication; (2) DeDiVAE, a method to decouple diffusion and VAE modules into separate GPU groups to save memory and cut latency; and (3) Aco, an attention co-processing method to further enhance GPU resource usage efficiency in the VAE group.

Result: Integration of PipeDiT into OpenSoraPlan and HunyuanVideo video generation frameworks resulted in 1.06x to 4.02x speedup under various resolution and timestep settings across an 8-GPU system.

Conclusion: PipeDiT effectively reduces inference time and memory usage in video generation processes, making state-of-the-art models more efficient for real-world deployment.

Abstract: Video generation has been advancing rapidly, and diffusion transformer (DiT) based models have demonstrated remark- able capabilities. However, their practical deployment is of- ten hindered by slow inference speeds and high memory con- sumption. In this paper, we propose a novel pipelining frame- work named PipeDiT to accelerate video generation, which is equipped with three main innovations. First, we design a pipelining algorithm (PipeSP) for sequence parallelism (SP) to enable the computation of latent generation and commu- nication among multiple GPUs to be pipelined, thus reduc- ing inference latency. Second, we propose DeDiVAE to de- couple the diffusion module and the variational autoencoder (VAE) module into two GPU groups, whose executions can also be pipelined to reduce memory consumption and infer- ence latency. Third, to better utilize the GPU resources in the VAE group, we propose an attention co-processing (Aco) method to further reduce the overall video generation latency. We integrate our PipeDiT into both OpenSoraPlan and Hun- yuanVideo, two state-of-the-art open-source video generation frameworks, and conduct extensive experiments on two 8- GPU systems. Experimental results show that, under many common resolution and timestep configurations, our PipeDiT achieves 1.06x to 4.02x speedups over OpenSoraPlan and HunyuanVideo.

</details>


### [233] [MovSemCL: Movement-Semantics Contrastive Learning for Trajectory Similarity](https://arxiv.org/abs/2511.12061)
*Zhichen Lai,Hua Lu,Huan Li,Jialiang Li,Christian S. Jensen*

Main category: cs.CV

TL;DR: MovSemCL is a framework for trajectory similarity computation that enhances semantic modeling, reduces computational costs, and ensures realistic augmentations. It achieves superior accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address limitations in trajectory similarity computation methods, such as insufficient semantic modeling, high computational costs, and unrealistic trajectory augmentations.

Method: MovSemCL transforms GPS data into movement-semantics features, segments them into patches, and uses attentions for hierarchical encoding. It introduces a curvature-guided augmentation to preserve trajectory reality while reducing redundancies.

Result: MovSemCL demonstrated enhanced similarity ranking, improving heuristic approximation by 20.3% and reducing inference latency by 43.4% in experiments with real-world datasets.

Conclusion: MovSemCL effectively addresses limitations in trajectory similarity methods, enabling more accurate and efficient computations with realistic semantic augmentations.

Abstract: Trajectory similarity computation is fundamental functionality that is used for, e.g., clustering, prediction, and anomaly detection. However, existing learning-based methods exhibit three key limitations: (1) insufficient modeling of trajectory semantics and hierarchy, lacking both movement dynamics extraction and multi-scale structural representation; (2) high computational costs due to point-wise encoding; and (3) use of physically implausible augmentations that distort trajectory semantics. To address these issues, we propose MovSemCL, a movement-semantics contrastive learning framework for trajectory similarity computation. MovSemCL first transforms raw GPS trajectories into movement-semantics features and then segments them into patches. Next, MovSemCL employs intra- and inter-patch attentions to encode local as well as global trajectory patterns, enabling efficient hierarchical representation and reducing computational costs. Moreover, MovSemCL includes a curvature-guided augmentation strategy that preserves informative segments (e.g., turns and intersections) and masks redundant ones, generating physically plausible augmented views. Experiments on real-world datasets show that MovSemCL is capable of outperforming state-of-the-art methods, achieving mean ranks close to the ideal value of 1 at similarity search tasks and improvements by up to 20.3% at heuristic approximation, while reducing inference latency by up to 43.4%.

</details>


### [234] [DCA-LUT: Deep Chromatic Alignment with 5D LUT for Purple Fringing Removal](https://arxiv.org/abs/2511.12066)
*Jialang Lu,Shuning Sun,Pu Wang,Chen Wu,Feng Gao,Lina Gong,Dianjie Lu,Guijuan Zhang,Zhuoran Zheng*

Main category: cs.CV

TL;DR: The paper introduces DCA-LUT, a deep learning framework for removing purple fringing in digital images by using chromatic-aware transformations and a learned 5D LUT for color correction.


<details>
  <summary>Details</summary>
Motivation: Current methods for purple fringing removal rely on hardware solutions or handcrafted features, neglecting data-driven methodologies.

Method: DCA-LUT uses a Chromatic-Aware Coordinate Transformation (CA-CT) module to separate fringing into a new dimension and applies a 5D Look-Up Table (5D LUT) for non-linear color mapping.

Result: DCA-LUT demonstrates state-of-the-art performance in purple fringing removal in both synthetic and real-world datasets.

Conclusion: The proposed method provides a novel, effective, and efficient deep learning approach for mitigating purple fringing, outperforming traditional methods.

Abstract: Purple fringing, a persistent artifact caused by Longitudinal Chromatic Aberration (LCA) in camera lenses, has long degraded the clarity and realism of digital imaging. Traditional solutions rely on complex and expensive apochromatic (APO) lens hardware and the extraction of handcrafted features, ignoring the data-driven approach. To fill this gap, we introduce DCA-LUT, the first deep learning framework for purple fringing removal. Inspired by the physical root of the problem, the spatial misalignment of RGB color channels due to lens dispersion, we introduce a novel Chromatic-Aware Coordinate Transformation (CA-CT) module, learning an image-adaptive color space to decouple and isolate fringing into a dedicated dimension. This targeted separation allows the network to learn a precise ``purple fringe channel", which then guides the accurate restoration of the luminance channel. The final color correction is performed by a learned 5D Look-Up Table (5D LUT), enabling efficient and powerful% non-linear color mapping. To enable robust training and fair evaluation, we constructed a large-scale synthetic purple fringing dataset (PF-Synth). Extensive experiments in synthetic and real-world datasets demonstrate that our method achieves state-of-the-art performance in purple fringing removal.

</details>


### [235] [Learning to Hear by Seeing: It's Time for Vision Language Models to Understand Artistic Emotion from Sight and Sound](https://arxiv.org/abs/2511.12077)
*Dengming Zhang,Weitao You,Jingxiong Li,Weishen Lin,Wenda Shi,Xue Zhao,Heda Zuo,Junxian Wu,Lingyun Sun*

Main category: cs.CV

TL;DR: The paper introduces VAEmotionLLM, a framework that enables multimodal emotion understanding in Large Language Models with limited audio pretraining, focusing on art-centric emotion analysis.


<details>
  <summary>Details</summary>
Motivation: To improve the emotional understanding of Large Language Models using multimodal inputs (visual and auditory) in order to make them more aligned with humans. The study focuses on emotions conveyed through art, as prior research is largely human-centered or single-modal.

Method: The framework operates in two stages: (1) Vision-Guided Audio Alignment, which teaches audio recognition via visual guidance without requiring extensive audio datasets, and (2) Cross-Modal Emotion Adapter (EmoAdapter), which introduces emotion-sensitive elements to enhance multimodal emotion comprehension.

Result: VAEmotionLLM achieves state-of-the-art performance on a newly developed benchmark (ArtEmoBenchmark) for evaluating emotion understanding in art-focused multimodal contexts, surpassing other baseline models in accuracy.

Conclusion: The proposed methodology effectively combines visual and auditory processing for emotion analysis, minimizing the need for large-scale audio pretraining. It demonstrates the complementary nature of its components through ablation studies.

Abstract: Emotion understanding is critical for making Large Language Models (LLMs) more general, reliable, and aligned with humans. Art conveys emotion through the joint design of visual and auditory elements, yet most prior work is human-centered or single-modality, overlooking the emotion intentionally expressed by the artwork. Meanwhile, current Audio-Visual Language Models (AVLMs) typically require large-scale audio pretraining to endow Visual Language Models (VLMs) with hearing, which limits scalability. We present Vision Anchored Audio-Visual Emotion LLM (VAEmotionLLM), a two-stage framework that teaches a VLM to hear by seeing with limited audio pretraining and to understand emotion across modalities. In Stage 1, Vision-Guided Audio Alignment (VG-Align) distills the frozen visual pathway into a new audio pathway by aligning next-token distributions of the shared LLM on synchronized audio-video clips, enabling hearing without a large audio dataset. In Stage 2, a lightweight Cross-Modal Emotion Adapter (EmoAdapter), composed of the Emotion Enhancer and the Emotion Supervisor, injects emotion-sensitive residuals and applies emotion supervision to enhance cross-modal emotion understanding. We also construct ArtEmoBenchmark, an art-centric emotion benchmark that evaluates content and emotion understanding under audio-only, visual-only, and audio-visual inputs. VAEmotionLLM achieves state-of-the-art results on ArtEmoBenchmark, outperforming audio-only, visual-only, and audio-visual baselines. Ablations show that the proposed components are complementary.

</details>


### [236] [Point Cloud Quantization through Multimodal Prompting for 3D Understanding](https://arxiv.org/abs/2511.12079)
*Hongxuan Li,Wencheng Zhu,Huiying Xu,Xinzhong Zhu,Pengfei Zhu*

Main category: cs.CV

TL;DR: The paper proposes a prompting-driven quantization framework for point cloud analysis using multimodal approaches, improving representativeness and interpretability.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in current codebook designs for vector quantization, such as lack of representativeness and interpretability, despite the promise of multimodal alignment in vision-language models.

Method: A framework that leverages text embeddings as prototype priors, multimodal prompts for adaptive refinement, dual-constrained quantization with compactness and separation regularization, and Gumbel-Softmax relaxation for differentiable discretization.

Result: The proposed method demonstrates superior performance in point cloud analysis on ModelNet40 and ScanObjectNN datasets.

Conclusion: The framework effectively integrates visual and semantic features through its hybrid representation approach, addressing limitations in existing methodologies.

Abstract: Vector quantization has emerged as a powerful tool in large-scale multimodal models, unifying heterogeneous representations through discrete token encoding. However, its effectiveness hinges on robust codebook design. Current prototype-based approaches relying on trainable vectors or clustered centroids fall short in representativeness and interpretability, even as multimodal alignment demonstrates its promise in vision-language models. To address these limitations, we propose a simple multimodal prompting-driven quantization framework for point cloud analysis. Our methodology is built upon two core insights: 1) Text embeddings from pre-trained models inherently encode visual semantics through many-to-one contrastive alignment, naturally serving as robust prototype priors; and 2) Multimodal prompts enable adaptive refinement of these prototypes, effectively mitigating vision-language semantic gaps. The framework introduces a dual-constrained quantization space, enforced by compactness and separation regularization, which seamlessly integrates visual and prototype features, resulting in hybrid representations that jointly encode geometric and semantic information. Furthermore, we employ Gumbel-Softmax relaxation to achieve differentiable discretization while maintaining quantization sparsity. Extensive experiments on the ModelNet40 and ScanObjectNN datasets clearly demonstrate the superior effectiveness of the proposed method.

</details>


### [237] [Supervised Multilabel Image Classification Using Residual Networks with Probabilistic Reasoning](https://arxiv.org/abs/2511.12082)
*Lokender Singh,Saksham Kumar,Chandan Kumar*

Main category: cs.CV

TL;DR: The paper proposes a unique method to improve multilabel image categorization using a modified ResNet-101 with probabilistic reasoning, achieving state-of-the-art results on the COCO-2014 dataset.


<details>
  <summary>Details</summary>
Motivation: To tackle the challenges of multilabel image categorization, such as label dependencies and uncertainties, for improved computer vision applications.

Method: The use of a modified ResNet-101 deep learning architecture integrated with probabilistic reasoning to simulate label dependencies and handle uncertainties.

Result: The model achieved 0.794 mAP on the COCO-2014 dataset, outperforming ResNet-SRN (0.771) and Vision Transformer baselines (0.785), with thorough evaluation using metrics like precision-recall.

Conclusion: By integrating probabilistic reasoning into deep learning, the proposed model effectively addresses the challenges of multilabel image categorization and sets a new state-of-the-art performance standard.

Abstract: Multilabel image categorization has drawn interest recently because of its numerous computer vision applications. The proposed work introduces a novel method for classifying multilabel images using the COCO-2014 dataset and a modified ResNet-101 architecture. By simulating label dependencies and uncertainties, the approach uses probabilistic reasoning to improve prediction accuracy. Extensive tests show that the model outperforms earlier techniques and approaches to state-of-the-art outcomes in multilabel categorization. The work also thoroughly assesses the model's performance using metrics like precision-recall score and achieves 0.794 mAP on COCO-2014, outperforming ResNet-SRN (0.771) and Vision Transformer baselines (0.785). The novelty of the work lies in integrating probabilistic reasoning into deep learning models to effectively address the challenges presented by multilabel scenarios.

</details>


### [238] [SemanticStitch: Enhancing Image Coherence through Foreground-Aware Seam Carving](https://arxiv.org/abs/2511.12084)
*Ji-Ping Jin,Chen-Bin Feng,Rui Fan,Chi-Man Vong*

Main category: cs.CV

TL;DR: The paper introduces SemanticStitch, a deep learning-based image stitching method addressing misalignments and visual disruptions by preserving semantic integrity of foreground objects.


<details>
  <summary>Details</summary>
Motivation: The motivation arises from the challenges in image stitching caused by variations in capture angles, positions, and movements, leading to disruptions in continuity and quality.

Method: SemanticStitch utilizes semantic priors of foreground objects, along with a novel loss function, to enhance image stitching by preserving object integrity and coherence.

Result: The method shows significant improvements in quality over traditional stitching approaches, demonstrated through experimental results on two specialized datasets.

Conclusion: SemanticStitch provides a robust and effective solution for image stitching, ensuring better semantic coherence for practical applications.

Abstract: Image stitching often faces challenges due to varying capture angles, positional differences, and object movements, leading to misalignments and visual discrepancies. Traditional seam carving methods neglect semantic information, causing disruptions in foreground continuity. We introduce SemanticStitch, a deep learning-based framework that incorporates semantic priors of foreground objects to preserve their integrity and enhance visual coherence. Our approach includes a novel loss function that emphasizes the semantic integrity of salient objects, significantly improving stitching quality. We also present two specialized real-world datasets to evaluate our method's effectiveness. Experimental results demonstrate substantial improvements over traditional techniques, providing robust support for practical applications.

</details>


### [239] [Teaching Prompts to Coordinate: Hierarchical Layer-Grouped Prompt Tuning for Continual Learning](https://arxiv.org/abs/2511.12090)
*Shengqin Jiang,Tianqi Kong,Yuankai Qi,Haokui Zhang,Lina Yao,Quan Z. Sheng,Qingshan Liu,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: The paper introduces a hierarchical layer-grouped prompt tuning approach to improve continual learning by enhancing model stability and mitigating catastrophic forgetting.


<details>
  <summary>Details</summary>
Motivation: Prompt-based continual learning risks catastrophic forgetting due to layer-specific prompt tuning and unnecessary parameter updates, necessitating a stable method to retain pre-trained features.

Method: The method uses hierarchical layer-grouped prompts where layers in each group share prompts adjusted by position encoding, and generates group sub-prompts conditioned on a single root prompt for synergy.

Result: Experiments across four benchmarks show the proposed method performs favorably against state-of-the-art continual learning techniques.

Conclusion: The approach enhances efficiency, preserves previous task knowledge, and mitigates forgetting while maintaining adaptability to new tasks.

Abstract: Prompt-based continual learning methods fine-tune only a small set of additional learnable parameters while keeping the pre-trained model's parameters frozen. It enables efficient adaptation to new tasks while mitigating the risk of catastrophic forgetting. These methods typically attach one independent task-specific prompt to each layer of pre-trained models to locally modulate its features, ensuring that the layer's representation aligns with the requirements of the new task. However, although introducing learnable prompts independently at each layer provides high flexibility for adapting to new tasks, this overly flexible tuning could make certain layers susceptible to unnecessary updates. As all prompts till the current task are added together as a final prompt for all seen tasks, the model may easily overwrite feature representations essential to previous tasks, which increases the risk of catastrophic forgetting. To address this issue, we propose a novel hierarchical layer-grouped prompt tuning method for continual learning. It improves model stability in two ways: (i) Layers in the same group share roughly the same prompts, which are adjusted by position encoding. This helps preserve the intrinsic feature relationships and propagation pathways of the pre-trained model within each group. (ii) It utilizes a single task-specific root prompt to learn to generate sub-prompts for each layer group. In this way, all sub-prompts are conditioned on the same root prompt, enhancing their synergy and reducing independence. Extensive experiments across four benchmarks demonstrate that our method achieves favorable performance compared with several state-of-the-art methods.

</details>


### [240] [Learning from Dense Events: Towards Fast Spiking Neural Networks Training via Event Dataset Distillatio](https://arxiv.org/abs/2511.12095)
*Shuhan Ye,Yi Yu,Qixin Zhang,Chenqi Kong,Qiangqiang Wu,Kun Wang,Xudong Jiang*

Main category: cs.CV

TL;DR: PACE introduces a dataset distillation framework for event-based vision with SNNs, significantly reducing training time and storage costs while achieving competitive accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the high training cost of spiking neural networks (SNNs) due to temporal coding, which hampers their practical deployment.

Method: PACE utilizes two modules: ST-DSM for fine-grained spatiotemporal matching using residual membrane potentials, and PEQ-N for probabilistic integer quantization compatible with event-frame pipelines.

Result: PACE significantly reduces training time (50x) and storage costs (6000x) while achieving competitive accuracy (84.4% on N-MNIST, 85% of full training set performance).

Conclusion: PACE demonstrates the viability of fast and efficient SNN training through dataset condensation, enabling practical and energy-efficient edge deployment for event-based vision systems.

Abstract: Event cameras sense brightness changes and output binary asynchronous event streams, attracting increasing attention. Their bio-inspired dynamics align well with spiking neural networks (SNNs), offering a promising energy-efficient alternative to conventional vision systems. However, SNNs remain costly to train due to temporal coding, which limits their practical deployment. To alleviate the high training cost of SNNs, we introduce \textbf{PACE} (Phase-Aligned Condensation for Events), the first dataset distillation framework to SNNs and event-based vision. PACE distills a large training dataset into a compact synthetic one that enables fast SNN training, which is achieved by two core modules: \textbf{ST-DSM} and \textbf{PEQ-N}. ST-DSM uses residual membrane potentials to densify spike-based features (SDR) and to perform fine-grained spatiotemporal matching of amplitude and phase (ST-SM), while PEQ-N provides a plug-and-play straight through probabilistic integer quantizer compatible with standard event-frame pipelines. Across DVS-Gesture, CIFAR10-DVS, and N-MNIST datasets, PACE outperforms existing coreset selection and dataset distillation baselines, with particularly strong gains on dynamic event streams and at low or moderate IPC. Specifically, on N-MNIST, it achieves \(84.4\%\) accuracy, about \(85\%\) of the full training set performance, while reducing training time by more than \(50\times\) and storage cost by \(6000\times\), yielding compact surrogates that enable minute-scale SNN training and efficient edge deployment.

</details>


### [241] [Sparse by Rule: Probability-Based N:M Pruning for Spiking Neural Networks](https://arxiv.org/abs/2511.12097)
*Shuhan Ye,Yi Yu,Qixin Zhang,Chenqi Kong,Qiangqiang Wu,Xudong Jiang,Dacheng Tao*

Main category: cs.CV

TL;DR: SpikeNM introduces a new semi-structured pruning framework for spiking neural networks (SNNs) to balance flexibility, sparsity, and hardware efficiency.


<details>
  <summary>Details</summary>
Motivation: Deep SNN architectures increase parameters and computational costs, complicating deployment on edge devices. Existing pruning strategies either sacrifice deployment efficiency or accuracy.

Method: SpikeNM uses an M-way basis-logit parameterization with a differentiable top-k sampler to achieve semi-structured N:M pruning while reducing complexity. It also introduces eligibility-inspired distillation for improved alignment with spiking dynamics.

Result: At 2:4 sparsity, SpikeNM achieves comparable or better performance than unpruned models and produces hardware-friendly sparsity patterns.

Conclusion: SpikeNM optimizes SNN sparse computation, bridging accuracy, flexibility, and deployment feasibility for edge applications.

Abstract: Brain-inspired Spiking neural networks (SNNs) promise energy-efficient intelligence via event-driven, sparse computation, but deeper architectures inflate parameters and computational cost, hindering their edge deployment. Recent progress in SNN pruning helps alleviate this burden, yet existing efforts fall into only two families: \emph{unstructured} pruning, which attains high sparsity but is difficult to accelerate on general hardware, and \emph{structured} pruning, which eases deployment but lack flexibility and often degrades accuracy at matched sparsity. In this work, we introduce \textbf{SpikeNM}, the first SNN-oriented \emph{semi-structured} \(N{:}M\) pruning framework that learns sparse SNNs \emph{from scratch}, enforcing \emph{at most \(N\)} non-zeros per \(M\)-weight block. To avoid the combinatorial space complexity \(\sum_{k=1}^{N}\binom{M}{k}\) growing exponentially with \(M\), SpikeNM adopts an \(M\)-way basis-logit parameterization with a differentiable top-\(k\) sampler, \emph{linearizing} per-block complexity to \(\mathcal O(M)\) and enabling more aggressive sparsification. Further inspired by neuroscience, we propose \emph{eligibility-inspired distillation} (EID), which converts temporally accumulated credits into block-wise soft targets to align mask probabilities with spiking dynamics, reducing sampling variance and stabilizing search under high sparsity. Experiments show that at \(2{:}4\) sparsity, SpikeNM maintains and even with gains across main-stream datasets, while yielding hardware-amenable patterns that complement intrinsic spike sparsity.

</details>


### [242] [DINOv3-Guided Cross Fusion Framework for Semantic-aware CT generation from MRI and CBCT](https://arxiv.org/abs/2511.12098)
*Xianhao Zhou,Jianghao Wu,Ku Zhao,Jinlong He,Huangxuan Zhao,Lei Chen,Shaoting Zhang,Guotai Wang*

Main category: cs.CV

TL;DR: The paper proposes DGCF, a framework combining DINOv3 Transformers and CNNs for translating medical images, achieving improved CT image synthesis performance.


<details>
  <summary>Details</summary>
Motivation: The need for efficient radiation dose planning and adaptive radiotherapy motivates the development of synthetic CT image generation from CBCT/MRI.

Method: The DGCF framework combines a frozen self-supervised DINOv3 Transformer with a trainable CNN encoder-decoder, employing a hierarchical cross fusion module and MLDP loss for semantic-aware CT synthesis.

Result: Experiments show DGCF outperforms existing methods on SynthRAD2023 pelvic dataset, excelling in MS-SSIM, PSNR, and segmentation-based metrics.

Conclusion: DGCF effectively integrates global semantic understanding and local features, establishing the value of self-supervised Transformer guidance in medical image translation.

Abstract: Generating synthetic CT images from CBCT or MRI has a potential for efficient radiation dose planning and adaptive radiotherapy. However, existing CNN-based models lack global semantic understanding, while Transformers often overfit small medical datasets due to high model capacity and weak inductive bias. To address these limitations, we propose a DINOv3-Guided Cross Fusion (DGCF) framework that integrates a frozen self-supervised DINOv3 Transformer with a trainable CNN encoder-decoder. It hierarchically fuses global representation of Transformer and local features of CNN via a learnable cross fusion module, achieving balanced local appearance and contextual representation. Furthermore, we introduce a Multi-Level DINOv3 Perceptual (MLDP) loss that encourages semantic similarity between synthetic CT and the ground truth in DINOv3's feature space. Experiments on the SynthRAD2023 pelvic dataset demonstrate that DGCF achieved state-of-the-art performance in terms of MS-SSIM, PSNR and segmentation-based metrics on both MRI$\rightarrow$CT and CBCT$\rightarrow$CT translation tasks. To the best of our knowledge, this is the first work to employ DINOv3 representations for medical image translation, highlighting the potential of self-supervised Transformer guidance for semantic-aware CT synthesis. The code is available at https://github.com/HiLab-git/DGCF.

</details>


### [243] [Adaptive Begin-of-Video Tokens for Autoregressive Video Diffusion Models](https://arxiv.org/abs/2511.12099)
*Tianle Cheng,Zeyan Zhang,Kaifeng Gao,Jun Xiao*

Main category: cs.CV

TL;DR: This paper introduces Adaptive Begin-of-Video Tokens (ada-BOV) to improve consistency and dynamics in autoregressive video diffusion models for long video generation.


<details>
  <summary>Details</summary>
Motivation: Current video diffusion models face challenges in generating long coherent videos due to issues like error accumulation, consistency, and poor motion dynamics.

Method: The paper proposes ada-BOV tokens that use adaptive modulation for better global consistency, alongside a refinement strategy for stream denoising and a disturbance-augmented training noise schedule.

Result: Experiments showed significant qualitative and quantitative improvements across multiple metrics for long video generation.

Conclusion: The introduced ada-BOV tokens and methodologies address key limitations of autoregressive VDMs, enhancing the generation quality and coherence of long videos.

Abstract: Recent advancements in diffusion-based video generation have produced impressive and high-fidelity short videos. To extend these successes to generate coherent long videos, most video diffusion models (VDMs) generate videos in an autoregressive manner, i.e., generating subsequent frames conditioned on previous ones. There are generally two primary paradigms: chunk-based extension and stream denoising. The former directly concatenates previous clean frames as conditioning, suffering from denoising latency and error accumulation. The latter maintains the denoising sequence with monotonically increasing noise levels. In each denoising iteration, one clean frame is produced while a new pure noise is simultaneously appended, enabling live-stream sampling. However, it struggles with fragile consistency and poor motion dynamics. In this paper, we propose Adaptive Begin-of-Video Tokens (ada-BOV) for autoregressive VDMs. The BOV tokens are special learnable embeddings on VDMs. They adaptively absorb denoised preceding frames via an adaptive-layer-norm-like modulation. This design preserves the global consistency while allowing for flexible conditioning in dynamic scenarios. To ensure the quality of local dynamics essential in modulating BOV tokens, we further propose a refinement strategy for stream denoising. It decouples the sampling trajectory length from the attention window size constraint, leading to improved local guidance and overall imaging quality. We also propose a disturbance-augmented training noise schedule, which balances the convergence speed with model robustness for the stream denoising. Extensive experiments demonstrate that our method achieves compelling qualitative and quantitative results across multiple metrics.

</details>


### [244] [OPFormer: Object Pose Estimation leveraging foundation model with geometric encoding](https://arxiv.org/abs/2511.12614)
*Artem Moroz,Vít Zeman,Martin Mikšík,Elizaveta Isianova,Miroslav David,Pavel Burget,Varun Burde*

Main category: cs.CV

TL;DR: The paper presents an integrated system for object detection and 6D pose estimation using a transformer-based architecture and demonstrating strong performance on BOP benchmarks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to create a unified framework that bridges object detection and pose estimation, incorporating capabilities for scenarios with or without traditional 3D CAD models.

Method: The method involves an onboarding stage to create object setups via 3D CAD models or reconstructing neural representations from images, CNOS for object detection, and OPFormer—a transformer-based architecture for pose estimation using 2D-3D correspondences enriched with geometric priors.

Result: The system achieves a balance of accuracy and efficiency, validated on BOP benchmarks, proving it effective in both model-based and model-free implementations.

Conclusion: The proposed framework effectively integrates advanced detection and pose estimation techniques, proving its practical applicability for real-world scenarios, irrespective of the availability of CAD models.

Abstract: We introduce a unified, end-to-end framework that seamlessly integrates object detection and pose estimation with a versatile onboarding process. Our pipeline begins with an onboarding stage that generates object representations from either traditional 3D CAD models or, in their absence, by rapidly reconstructing a high-fidelity neural representation (NeRF) from multi-view images. Given a test image, our system first employs the CNOS detector to localize target objects. For each detection, our novel pose estimation module, OPFormer, infers the precise 6D pose. The core of OPFormer is a transformer-based architecture that leverages a foundation model for robust feature extraction. It uniquely learns a comprehensive object representation by jointly encoding multiple template views and enriches these features with explicit 3D geometric priors using Normalized Object Coordinate Space (NOCS). A decoder then establishes robust 2D-3D correspondences to determine the final pose. Evaluated on the challenging BOP benchmarks, our integrated system demonstrates a strong balance between accuracy and efficiency, showcasing its practical applicability in both model-based and model-free scenarios.

</details>


### [245] [Did Models Sufficient Learn? Attribution-Guided Training via Subset-Selected Counterfactual Augmentation](https://arxiv.org/abs/2511.12100)
*Yannan Chen,Ruoyu Chen,Bin Zeng,Wei Wang,Shiming Liu,Qunli Zhang,Zheng Hu,Laiyuan Wang,Yaowei Wang,Xiaochun Cao*

Main category: cs.CV

TL;DR: The paper introduces Subset-Selected Counterfactual Augmentation (SS-CA), a method to improve models' robustness and generalization by integrating counterfactual explanations into training.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address limitations where visual models rely only on weak causal features, causing sensitivity to distribution shifts or missing features. Current approaches do not sufficiently address these issues during training.

Method: The authors propose SS-CA, which uses a Counterfactual LIMA method to identify minimal regions affecting model predictions. These regions are replaced with natural background in a data augmentation process, training the model with both original and augmented samples.

Result: SS-CA improves in-distribution test performance, outperforms on OOD benchmarks (ImageNet-R and ImageNet-S), and enhances generalization under perturbations like noise.

Conclusion: Integrating counterfactual attribution into training fixes incomplete causal learning and boosts robustness, interpretability, and performance of visual models.

Abstract: In current visual model training, models often rely on only limited sufficient causes for their predictions, which makes them sensitive to distribution shifts or the absence of key features. Attribution methods can accurately identify a model's critical regions. However, masking these areas to create counterfactuals often causes the model to misclassify the target, while humans can still easily recognize it. This divergence highlights that the model's learned dependencies may not be sufficiently causal. To address this issue, we propose Subset-Selected Counterfactual Augmentation (SS-CA), which integrates counterfactual explanations directly into the training process for targeted intervention. Building on the subset-selection-based LIMA attribution method, we develop Counterfactual LIMA to identify minimal spatial region sets whose removal can selectively alter model predictions. Leveraging these attributions, we introduce a data augmentation strategy that replaces the identified regions with natural background, and we train the model jointly on both augmented and original samples to mitigate incomplete causal learning. Extensive experiments across multiple ImageNet variants show that SS-CA improves generalization on in-distribution (ID) test data and achieves superior performance on out-of-distribution (OOD) benchmarks such as ImageNet-R and ImageNet-S. Under perturbations including noise, models trained with SS-CA also exhibit enhanced generalization, demonstrating that our approach effectively uses interpretability insights to correct model deficiencies and improve both performance and robustness.

</details>


### [246] [BdSL-SPOTER: A Transformer-Based Framework for Bengali Sign Language Recognition with Cultural Adaptation](https://arxiv.org/abs/2511.12103)
*Sayad Ibna Azad,Md. Atiqur Rahman*

Main category: cs.CV

TL;DR: BdSL-SPOTER is a pose-based transformer achieving 97.92% accuracy in recognizing Bengali Sign Language efficiently compared to baselines.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges of recognizing Bengali Sign Language with limited data, high accuracy, and computational efficiency.

Method: It developed a four-layer transformer encoder with optimized positional encodings, cultural-specific preprocessing, and curriculum learning.

Result: Achieves 97.92% Top-1 validation accuracy, surpassing a baseline by 22.82%, while maintaining low computational costs.

Conclusion: BdSL-SPOTER is an efficient and scalable framework for Bengali Sign Language recognition and other low-resource sign languages.

Abstract: We introduce BdSL-SPOTER, a pose-based transformer framework for accurate and efficient recognition of Bengali Sign Language (BdSL). BdSL-SPOTER extends the SPOTER paradigm with cultural specific preprocessing and a compact four-layer transformer encoder featuring optimized learnable positional encodings, while employing curriculum learning to enhance generalization on limited data and accelerate convergence. On the BdSLW60 benchmark, it achieves 97.92% Top-1 validation accuracy, representing a 22.82% improvement over the Bi-LSTM baseline, all while keeping computational costs low. With its reduced number of parameters, lower FLOPs, and higher FPS, BdSL-SPOTER provides a practical framework for real-world accessibility applications and serves as a scalable model for other low-resource regional sign languages.

</details>


### [247] [TEMPO: Global Temporal Building Density and Height Estimation from Satellite Imagery](https://arxiv.org/abs/2511.12104)
*Tammy Glazer,Gilles Q. Hacheme,Akram Zaytar,Luana Marotti,Amy Michaels,Girmaw Abebe Tadesse,Kevin White,Rahul Dodhia,Andrew Zolli,Inbal Becker-Reshef,Juan M. Lavista Ferres,Caleb Robinson*

Main category: cs.CV

TL;DR: TEMPO is a global dataset of building density and height created using high-resolution satellite imagery and deep learning models, enabling efficient monitoring of global changes.


<details>
  <summary>Details</summary>
Motivation: To develop a method for creating temporally-resolved, global data on building density and height for better monitoring of settlement changes and climate adaptations.

Method: The study used high-resolution satellite imagery paired with deep learning models that predict building density and height. Quarterly data (2018-2025) was analyzed for global maps.

Result: The model achieves an F1 score between 85% to 88% and demonstrates temporal stability with a trend-consistency score of 0.96 over five years.

Conclusion: TEMPO offers quarterly spatial monitoring of settlements at reduced computational costs, providing a valuable tool for analyzing development and climate impact trends globally.

Abstract: We present TEMPO, a global, temporally resolved dataset of building density and height derived from high-resolution satellite imagery using deep learning models. We pair building footprint and height data from existing datasets with quarterly PlanetScope basemap satellite images to train a multi-task deep learning model that predicts building density and building height at a 37.6-meter per pixel resolution. We apply this model to global PlanetScope basemaps from Q1 2018 through Q2 2025 to create global, temporal maps of building density and height. We validate these maps by comparing against existing building footprint datasets. Our estimates achieve an F1 score between 85% and 88% on different hand-labeled subsets, and are temporally stable, with a 0.96 five-year trend-consistency score. TEMPO captures quarterly changes in built settlements at a fraction of the computational cost of comparable approaches, unlocking large-scale monitoring of development patterns and climate impacts essential for global resilience and adaptation efforts.

</details>


### [248] [Uni-Hand: Universal Hand Motion Forecasting in Egocentric Views](https://arxiv.org/abs/2511.12878)
*Junyi Ma,Wentao Bao,Jingyi Xu,Guanzhong Sun,Yu Zheng,Erhang Zhang,Xieyuanli Chen,Hesheng Wang*

Main category: cs.CV

TL;DR: This paper proposes EgoLoc, a zero-shot approach for localizing hand-object contact/separation moments in egocentric videos, addressing the underexplored problem of temporal interaction localization (TIL).


<details>
  <summary>Details</summary>
Motivation: Current methods for hand-object interaction analysis focus on behavior modeling, lacking precision in identifying the critical moments of contact/separation in mixed reality and robotics applications. A method to address 'when to interact' is therefore needed for immersive and effective interactions.

Method: The paper introduces EgoLoc, which employs hand-dynamics-guided sampling to create visual prompts, leverages a vision-language model for attribute recognition and timestamp localization, and uses closed-loop feedback to refine results. It does not require object masks or predefined action taxonomies, enabling zero-shot performance.

Result: EgoLoc demonstrated plausible TIL on existing datasets and new benchmarks, effectively addressing hand-object interaction moments. Further, it showed utility in varied downstream applications in egocentric vision and robotic tasks.

Conclusion: EgoLoc advances the precision of identifying hand-object interaction events without needing manual taxonomies, demonstrating strong flexibility and performance in various scenarios. The approach contributes to both academic research and practical applications in VR/AR and robotics.

Abstract: Analyzing hand-object interaction in egocentric vision facilitates VR/AR applications and human-robot policy transfer. Existing research has mostly focused on modeling the behavior paradigm of interactive actions (i.e., "how to interact"). However, the more challenging and fine-grained problem of capturing the critical moments of contact and separation between the hand and the target object (i.e., "when to interact") is still underexplored, which is crucial for immersive interactive experiences in mixed reality and robotic motion planning. Therefore, we formulate this problem as temporal interaction localization (TIL). Some recent works extract semantic masks as TIL references, but suffer from inaccurate object grounding and cluttered scenarios. Although current temporal action localization (TAL) methods perform well in detecting verb-noun action segments, they rely on category annotations during training and exhibit limited precision in localizing hand-object contact/separation moments. To address these issues, we propose a novel zero-shot approach dubbed EgoLoc to localize hand-object contact and separation timestamps in egocentric videos. EgoLoc introduces hand-dynamics-guided sampling to generate high-quality visual prompts. It exploits the vision-language model to identify contact/separation attributes, localize specific timestamps, and provide closed-loop feedback for further refinement. EgoLoc eliminates the need for object masks and verb-noun taxonomies, leading to generalizable zero-shot implementation. Comprehensive experiments on the public dataset and our novel benchmarks demonstrate that EgoLoc achieves plausible TIL for egocentric videos. It is also validated to effectively facilitate multiple downstream applications in egocentric vision and robotic manipulation tasks. Code and relevant data will be released at https://github.com/IRMVLab/EgoLoc.

</details>


### [249] [Fine-Grained DINO Tuning with Dual Supervision for Face Forgery Detection](https://arxiv.org/abs/2511.12107)
*Tianxiang Zhang,Peipeng Yu,Zhihua Xia,Longchen Dai,Xiaoyu Zhou,Hui Gao*

Main category: cs.CV

TL;DR: The paper proposes a parameter-efficient deepfake detection method named DeepFake Fine-Grained Adapter (DFF-Adapter) built on DINOv2, achieving high detection accuracy.


<details>
  <summary>Details</summary>
Motivation: Concerns over the integrity of information due to advanced deepfakes push researchers to improve detection methods tailored for diverse deepfake forgery techniques.

Method: DFF-Adapter uses lightweight multi-head LoRA modules across transformer blocks, implementing a shared branch for multi-task optimization that learns fine-grained and authenticity-specific cues simultaneously.

Result: With 3.5M trainable parameters, DFF-Adapter achieves comparable or better deepfake detection accuracy in comparison to existing state-of-the-art methods.

Conclusion: The method effectively enhances deepfake detection capabilities with manipulation-specific sensitivity while maintaining parameter efficiency.

Abstract: The proliferation of sophisticated deepfakes poses significant threats to information integrity. While DINOv2 shows promise for detection, existing fine-tuning approaches treat it as generic binary classification, overlooking distinct artifacts inherent to different deepfake methods. To address this, we propose a DeepFake Fine-Grained Adapter (DFF-Adapter) for DINOv2. Our method incorporates lightweight multi-head LoRA modules into every transformer block, enabling efficient backbone adaptation. DFF-Adapter simultaneously addresses authenticity detection and fine-grained manipulation type classification, where classifying forgery methods enhances artifact sensitivity. We introduce a shared branch propagating fine-grained manipulation cues to the authenticity head. This enables multi-task cooperative optimization, explicitly enhancing authenticity discrimination with manipulation-specific knowledge. Utilizing only 3.5M trainable parameters, our parameter-efficient approach achieves detection accuracy comparable to or even surpassing that of current complex state-of-the-art methods.

</details>


### [250] [DiffPixelFormer: Differential Pixel-Aware Transformer for RGB-D Indoor Scene Segmentation](https://arxiv.org/abs/2511.13047)
*Yan Gong,Jianli Lu,Yongsheng Gao,Jie Zhao,Xiaojuan Zhang,Susanto Rahardja*

Main category: cs.CV

TL;DR: The paper introduces DiffPixelFormer, a method for indoor semantic segmentation using RGB-D data, addressing inefficiencies in existing approaches by enhancing intra-modal and inter-modal feature relationships. It achieves state-of-the-art performance on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Current RGB-D fusion techniques for indoor semantic segmentation face limitations due to computational intensity and insufficient feature relationship modeling, leading to poor feature alignment and discriminative power.

Method: DiffPixelFormer utilizes an Intra-Inter Modal Interaction Block (IIMIB) for intra-modal self-attention and inter-modal interactions via a Differential-Shared Inter-Modal (DSIM) module. A dynamic fusion strategy adjusts modality contributions based on scene characteristics for RGB-D segmentation.

Result: Extensive experiments show DiffPixelFormer-L achieves state-of-the-art scores (mIoU of 54.28% on SUN RGB-D and 59.95% on NYUDv2), outperforming prior methods like DFormer-L by significant margins.

Conclusion: DiffPixelFormer introduces innovative strategies for RGB-D indoor scene segmentation, providing superior feature alignment and representation. It outperforms competing methods on established benchmarks and offers robust performance in various scenarios.

Abstract: Indoor semantic segmentation is fundamental to computer vision and robotics, supporting applications such as autonomous navigation, augmented reality, and smart environments. Although RGB-D fusion leverages complementary appearance and geometric cues, existing methods often depend on computationally intensive cross-attention mechanisms and insufficiently model intra- and inter-modal feature relationships, resulting in imprecise feature alignment and limited discriminative representation. To address these challenges, we propose DiffPixelFormer, a differential pixel-aware Transformer for RGB-D indoor scene segmentation that simultaneously enhances intra-modal representations and models inter-modal interactions. At its core, the Intra-Inter Modal Interaction Block (IIMIB) captures intra-modal long-range dependencies via self-attention and models inter-modal interactions with the Differential-Shared Inter-Modal (DSIM) module to disentangle modality-specific and shared cues, enabling fine-grained, pixel-level cross-modal alignment. Furthermore, a dynamic fusion strategy balances modality contributions and fully exploits RGB-D information according to scene characteristics. Extensive experiments on the SUN RGB-D and NYUDv2 benchmarks demonstrate that DiffPixelFormer-L achieves mIoU scores of 54.28% and 59.95%, outperforming DFormer-L by 1.78% and 2.75%, respectively. Code is available at https://github.com/gongyan1/DiffPixelFormer.

</details>


### [251] [MediRound: Multi-Round Entity-Level Reasoning Segmentation in Medical Images](https://arxiv.org/abs/2511.12110)
*Qinyue Tong,Ziqian Lu,Jun Liu,Rui Zuo,Zheming Lu*

Main category: cs.CV

TL;DR: The study introduces MEMR-Seg, a task for multi-round entity-level medical image segmentation, alongside a dataset (MR-MedSeg) and a baseline model (MediRound) that incorporates correction mechanisms for improved results.


<details>
  <summary>Details</summary>
Motivation: Existing segmentation methods lack interactivity and reasoning capabilities, especially in multi-round contexts. To address these limitations, this paper introduces a new task aimed at enhancing reasoning and segmentation quality through multi-round dialogues.

Method: The authors design MR-MedSeg, a large-scale dataset containing multi-round medical segmentation dialogues, and propose MediRound, a baseline model that includes a Judgment & Correction Mechanism to reduce error propagation.

Result: Experiments demonstrate that MediRound effectively handles the MEMR-Seg task and achieves better performance compared to traditional medical referring segmentation methods.

Conclusion: This work advances medical image segmentation by introducing a reasoning-focused task and dataset. MediRound provides an effective solution to multi-round segmentation challenges.

Abstract: Despite the progress in medical image segmentation, most existing methods remain task-specific and lack interactivity. Although recent text-prompt-based segmentation approaches enhance user-driven and reasoning-based segmentation, they remain confined to single-round dialogues and fail to perform multi-round reasoning. In this work, we introduce Multi-Round Entity-Level Medical Reasoning Segmentation (MEMR-Seg), a new task that requires generating segmentation masks through multi-round queries with entity-level reasoning. To support this task, we construct MR-MedSeg, a large-scale dataset of 177K multi-round medical segmentation dialogues, featuring entity-based reasoning across rounds. Furthermore, we propose MediRound, an effective baseline model designed for multi-round medical reasoning segmentation. To mitigate the inherent error propagation in the chain-like pipeline of multi-round segmentation, we introduce a lightweight yet effective Judgment & Correction Mechanism during model inference. Experimental results demonstrate that our method effectively addresses the MEMR-Seg task and outperforms conventional medical referring segmentation methods.

</details>


### [252] [RadarMP: Motion Perception for 4D mmWave Radar in Autonomous Driving](https://arxiv.org/abs/2511.12117)
*Ruiqi Cheng,Huijun Di,Jian Li,Feng Liu,Wei Liang*

Main category: cs.CV

TL;DR: The paper introduces RadarMP, a method for improving 3D scene motion perception using radar signals, aiming to boost autonomous driving in adverse weather conditions.


<details>
  <summary>Details</summary>
Motivation: Current radar-based systems lack precision in motion perception due to sparse and noisy radar points, which limits their effectiveness, especially when optical sensors fail in poor weather.

Method: RadarMP unifies radar target detection and motion estimation in a single architecture. It employs self-supervised loss functions leveraging Doppler shifts and echo intensity to enhance spatial and motion consistency.

Result: RadarMP consistently achieves superior motion perception compared to existing radar-based systems, demonstrating its efficacy across varied weather and lighting environments.

Conclusion: RadarMP provides a robust solution for full-scenario autonomous driving, significantly improving safety and performance in challenging conditions.

Abstract: Accurate 3D scene motion perception significantly enhances the safety and reliability of an autonomous driving system. Benefiting from its all-weather operational capability and unique perceptual properties, 4D mmWave radar has emerged as an essential component in advanced autonomous driving. However, sparse and noisy radar points often lead to imprecise motion perception, leaving autonomous vehicles with limited sensing capabilities when optical sensors degrade under adverse weather conditions. In this paper, we propose RadarMP, a novel method for precise 3D scene motion perception using low-level radar echo signals from two consecutive frames. Unlike existing methods that separate radar target detection and motion estimation, RadarMP jointly models both tasks in a unified architecture, enabling consistent radar point cloud generation and pointwise 3D scene flow prediction. Tailored to radar characteristics, we design specialized self-supervised loss functions guided by Doppler shifts and echo intensity, effectively supervising spatial and motion consistency without explicit annotations. Extensive experiments on the public dataset demonstrate that RadarMP achieves reliable motion perception across diverse weather and illumination conditions, outperforming radar-based decoupled motion perception pipelines and enhancing perception capabilities for full-scenario autonomous driving systems.

</details>


### [253] [PhysX-Anything: Simulation-Ready Physical 3D Assets from Single Image](https://arxiv.org/abs/2511.13648)
*Ziang Cao,Fangzhou Hong,Zhaoxi Chen,Liang Pan,Ziwei Liu*

Main category: cs.CV

TL;DR: PhysX-Anything is a framework that generates high-quality, simulation-ready physical 3D assets from single images, focusing on geometry, articulation, and physical attributes.


<details>
  <summary>Details</summary>
Motivation: To address the lack of focus on physical and articulated properties in current 3D generation methods, which limits their utility for embodied AI applications.

Method: The paper introduces PhysX-Anything, a physical 3D generative framework using VLM-based modeling and an efficient geometry-tokenizing 3D representation, along with creating the PhysX-Mobility dataset for training.

Result: PhysX-Anything demonstrates strong generative performance and generalization in experiments, surpassing existing methods. It also proves directly applicable for robotic policy learning in simulation environments.

Conclusion: PhysX-Anything is a significant advancement for creating sim-ready 3D assets, enabling broader applications in embodied AI and physics-based simulations.

Abstract: 3D modeling is shifting from static visual representations toward physical, articulated assets that can be directly used in simulation and interaction. However, most existing 3D generation methods overlook key physical and articulation properties, thereby limiting their utility in embodied AI. To bridge this gap, we introduce PhysX-Anything, the first simulation-ready physical 3D generative framework that, given a single in-the-wild image, produces high-quality sim-ready 3D assets with explicit geometry, articulation, and physical attributes. Specifically, we propose the first VLM-based physical 3D generative model, along with a new 3D representation that efficiently tokenizes geometry. It reduces the number of tokens by 193x, enabling explicit geometry learning within standard VLM token budgets without introducing any special tokens during fine-tuning and significantly improving generative quality. In addition, to overcome the limited diversity of existing physical 3D datasets, we construct a new dataset, PhysX-Mobility, which expands the object categories in prior physical 3D datasets by over 2x and includes more than 2K common real-world objects with rich physical annotations. Extensive experiments on PhysX-Mobility and in-the-wild images demonstrate that PhysX-Anything delivers strong generative performance and robust generalization. Furthermore, simulation-based experiments in a MuJoCo-style environment validate that our sim-ready assets can be directly used for contact-rich robotic policy learning. We believe PhysX-Anything can substantially empower a broad range of downstream applications, especially in embodied AI and physics-based simulation.

</details>


### [254] [OAD-Promoter: Enhancing Zero-shot VQA using Large Language Models with Object Attribute Description](https://arxiv.org/abs/2511.12131)
*Quanxing Xu,Ling Zhou,Feifei Zhang,Jinyu Tian,Rubing Huang*

Main category: cs.CV

TL;DR: The paper introduces OAD-Promoter to address biases and out-of-distribution generalization issues in Large Language Models (LLMs) used for Visual Question Answering (VQA).


<details>
  <summary>Details</summary>
Motivation: LLMs face challenges like language biases and poor out-of-distribution generalization in VQA scenarios. These issues limit the reliability and robustness of LLMs.

Method: The proposed method, OAD-Promoter, consists of three components: the Object-concentrated Example Generation (OEG) module for diverse visual cues, the Memory Knowledge Assistance (MKA) module for retrieving relevant knowledge for unseen domains, and the OAD Prompt for optimizing inference.

Result: OAD-Promoter improves LLM performance in VQA tasks in few-shot and zero-shot scenarios, achieving state-of-the-art results.

Conclusion: OAD-Promoter effectively reduces language bias and enhances domain robustness in LLM-based VQA, offering a significant advancement in handling knowledge-intensive visual questions.

Abstract: Large Language Models (LLMs) have become a crucial tool in Visual Question Answering (VQA) for handling knowledge-intensive questions in few-shot or zero-shot scenarios. However, their reliance on massive training datasets often causes them to inherit language biases during the acquisition of knowledge. This limitation imposes two key constraints on existing methods: (1) LLM predictions become less reliable due to bias exploitation, and (2) despite strong knowledge reasoning capabilities, LLMs still struggle with out-of-distribution (OOD) generalization. To address these issues, we propose Object Attribute Description Promoter (OAD-Promoter), a novel approach for enhancing LLM-based VQA by mitigating language bias and improving domain-shift robustness. OAD-Promoter comprises three components: the Object-concentrated Example Generation (OEG) module, the Memory Knowledge Assistance (MKA) module, and the OAD Prompt. The OEG module generates global captions and object-concentrated samples, jointly enhancing visual information input to the LLM and mitigating bias through complementary global and regional visual cues. The MKA module assists the LLM in handling OOD samples by retrieving relevant knowledge from stored examples to support questions from unseen domains. Finally, the OAD Prompt integrates the outputs of the preceding modules to optimize LLM inference. Experiments demonstrate that OAD-Promoter significantly improves the performance of LLM-based VQA methods in few-shot or zero-shot settings, achieving new state-of-the-art results.

</details>


### [255] [Scaling Spatial Intelligence with Multimodal Foundation Models](https://arxiv.org/abs/2511.13719)
*Zhongang Cai,Ruisi Wang,Chenyang Gu,Fanyi Pu,Junxiang Xu,Yubo Wang,Wanqi Yin,Zhitao Yang,Chen Wei,Qingping Sun,Tongxi Zhou,Jiaqi Li,Hui En Pang,Oscar Qian,Yukun Wei,Zhiqian Lin,Xuanke Shi,Kewang Deng,Xiaoyang Han,Zukai Chen,Xiangyu Fan,Hanming Deng,Lewei Lu,Liang Pan,Bo Li,Ziwei Liu,Quan Wang,Dahua Lin,Lei Yang*

Main category: cs.CV

TL;DR: SenseNova-SI enhances spatial intelligence in multimodal foundation models using diverse data and demonstrates superior performance across benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address deficiencies in spatial intelligence within multimodal foundation models.

Method: Developed SenseNova-SI using eight million curated data samples under a taxonomy of spatial capabilities.

Result: Achieved high performance on various spatial intelligence benchmarks and demonstrated generalization capabilities.

Conclusion: SenseNova-SI shows significant promise and is continuously evolving, contributing to multimodal research through open releases.

Abstract: Despite remarkable progress, multimodal foundation models still exhibit surprising deficiencies in spatial intelligence. In this work, we explore scaling up multimodal foundation models to cultivate spatial intelligence within the SenseNova-SI family, built upon established multimodal foundations including visual understanding models (i.e., Qwen3-VL and InternVL3) and unified understanding and generation models (i.e., Bagel). We take a principled approach to constructing high-performing and robust spatial intelligence by systematically curating SenseNova-SI-8M: eight million diverse data samples under a rigorous taxonomy of spatial capabilities. SenseNova-SI demonstrates unprecedented performance across a broad range of spatial intelligence benchmarks: 68.7% on VSI-Bench, 43.3% on MMSI, 85.6% on MindCube, 54.6% on ViewSpatial, and 50.1% on SITE, while maintaining strong general multimodal understanding (e.g., 84.9% on MMBench-En). More importantly, we analyze the impact of data scaling, discuss early signs of emergent generalization capabilities enabled by diverse data training, analyze the risk of overfitting and language shortcuts, present a preliminary study on spatial chain-of-thought reasoning, and validate the potential downstream application. SenseNova-SI is an ongoing project, and this report will be updated continuously. All newly trained multimodal foundation models are publicly released to facilitate further research in this direction.

</details>


### [256] [Compression and Inference of Spiking Neural Networks on Resource-Constrained Hardware](https://arxiv.org/abs/2511.12136)
*Karol C. Jurzec,Tomasz Szydlo,Maciej Wielgosz*

Main category: cs.CV

TL;DR: This paper introduces a lightweight C-based runtime for optimized Spiking Neural Networks (SNNs) inference on edge devices, achieving significant performance and efficiency gains.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in training and deploying Spiking Neural Networks (SNNs) on resource-constrained hardware by leveraging their temporal processing and energy-efficiency advantages.

Method: The method involves translating trained SNN models into a compact C-based representation, optimizing for static, cache-friendly data layouts, memory preallocation, and exploiting sparse spiking activity to prune inactive neurons and synapses.

Result: Experiments demonstrate functional equivalence with Python baselines but with ~10x speed-up on desktop CPUs, significant memory reductions, and successful microcontroller deployment. Pruning further enhances computational efficiency.

Conclusion: SNNs, when paired with optimized runtimes and spike-driven model compression, can be effectively executed on embedded platforms like microcontrollers, making them practical for resource-constrained environments.

Abstract: Spiking neural networks (SNNs) communicate via discrete spikes in time rather than continuous activations. Their event-driven nature offers advantages for temporal processing and energy efficiency on resource-constrained hardware, but training and deployment remain challenging. We present a lightweight C-based runtime for SNN inference on edge devices and optimizations that reduce latency and memory without sacrificing accuracy. Trained models exported from SNNTorch are translated to a compact C representation; static, cache-friendly data layouts and preallocation avoid interpreter and allocation overheads. We further exploit sparse spiking activity to prune inactive neurons and synapses, shrinking computation in upstream convolutional layers. Experiments on N-MNIST and ST-MNIST show functional parity with the Python baseline while achieving ~10 speedups on desktop CPU and additional gains with pruning, together with large memory reductions that enable microcontroller deployment (Arduino Portenta H7). Results indicate that SNNs can be executed efficiently on conventional embedded platforms when paired with an optimized runtime and spike-driven model compression. Code: https://github.com/karol-jurzec/snn-generator/

</details>


### [257] [MAVIS: A Benchmark for Multimodal Source Attribution in Long-form Visual Question Answering](https://arxiv.org/abs/2511.12142)
*Seokwon Song,Minsu Park,Gunhee Kim*

Main category: cs.CV

TL;DR: The paper introduces MAVIS, a benchmark for evaluating multimodal AI source attribution systems, addressing visual questions using references for validation.


<details>
  <summary>Details</summary>
Motivation: To improve AI systems with source attribution in multimodal contexts, particularly visual question answering, which has been underexplored.

Method: The authors created MAVIS with 157K visual QA instances, annotated multimodal citations, automatic evaluation metrics for quality, and studied the correlation with human judgments.

Result: LVLMs outperform unimodal RAG in informativeness and fluency but show weaker groundedness in multimodal cases. Prompting methods affect the balance between informativeness and groundedness.

Conclusion: Mitigating contextual bias in interpreting image documents is essential for better groundedness in multimodal systems.

Abstract: Source attribution aims to enhance the reliability of AI-generated answers by including references for each statement, helping users validate the provided answers. However, existing work has primarily focused on text-only scenario and largely overlooked the role of multimodality. We introduce MAVIS, the first benchmark designed to evaluate multimodal source attribution systems that understand user intent behind visual questions, retrieve multimodal evidence, and generate long-form answers with citations. Our dataset comprises 157K visual QA instances, where each answer is annotated with fact-level citations referring to multimodal documents. We develop fine-grained automatic metrics along three dimensions of informativeness, groundedness, and fluency, and demonstrate their strong correlation with human judgments. Our key findings are threefold: (1) LVLMs with multimodal RAG generate more informative and fluent answers than unimodal RAG, but they exhibit weaker groundedness for image documents than for text documents, a gap amplified in multimodal settings. (2) Given the same multimodal documents, there is a trade-off between informativeness and groundedness across different prompting methods. (3) Our proposed method highlights mitigating contextual bias in interpreting image documents as a crucial direction for future research. The dataset and experimental code are available at https://github.com/seokwon99/MAVIS

</details>


### [258] [Breaking the Modality Wall: Time-step Mixup for Efficient Spiking Knowledge Transfer from Static to Event Domain](https://arxiv.org/abs/2511.12150)
*Yuqi Xie,Shuhan Ye,Yi Yu,Chong Wang,Qixin Zhang,Jiazhen Xu,Le Shen,Yuanbin Qian,Jiangbo Qian,Guoqi Li*

Main category: cs.CV

TL;DR: The proposed Time-step Mixup Knowledge Transfer (TMKT) framework enhances cross-modal training between RGB and event camera data for spiking neural networks, achieving superior image classification performance.


<details>
  <summary>Details</summary>
Motivation: Event cameras are energy-efficient but scarce event data and the mismatch between RGB and DVS hinders effective training. This paper aims to facilitate knowledge transfer to bridge the gap between modalities.

Method: Introduced Time-step Mixup Knowledge Transfer (TMKT), which employs a probabilistic Timestep Mixup strategy for smoother optimization, along with Modality Aware Guidance (MAG) and Mixup Ratio Perception (MRP) for precise alignment and supervision.

Result: Experimental results on diverse benchmarks and SNN backbones demonstrate improved performance and better modality alignment using TMKT.

Conclusion: TMKT effectively addresses the RGB-DVS modality gap, facilitating smoother knowledge transfer and improving spiking neural network performance on image classification tasks.

Abstract: The integration of event cameras and spiking neural networks (SNNs) promises energy-efficient visual intelligence, yet scarce event data and the sparsity of DVS outputs hinder effective training. Prior knowledge transfers from RGB to DVS often underperform because the distribution gap between modalities is substantial. In this work, we present Time-step Mixup Knowledge Transfer (TMKT), a cross-modal training framework with a probabilistic Time-step Mixup (TSM) strategy. TSM exploits the asynchronous nature of SNNs by interpolating RGB and DVS inputs at various time steps to produce a smooth curriculum within each sequence, which reduces gradient variance and stabilizes optimization with theoretical analysis. To employ auxiliary supervision from TSM, TMKT introduces two lightweight modality-aware objectives, Modality Aware Guidance (MAG) for per-frame source supervision and Mixup Ratio Perception (MRP) for sequence-level mix ratio estimation, which explicitly align temporal features with the mixing schedule. TMKT enables smoother knowledge transfer, helps mitigate modality mismatch during training, and achieves superior performance in spiking image classification tasks. Extensive experiments across diverse benchmarks and multiple SNN backbones, together with ablations, demonstrate the effectiveness of our method.

</details>


### [259] [FIA-Edit: Frequency-Interactive Attention for Efficient and High-Fidelity Inversion-Free Text-Guided Image Editing](https://arxiv.org/abs/2511.12151)
*Kaixiang Yang,Boyang Shen,Xin Li,Yuchen Dai,Yuxuan Luo,Yueran Ma,Wei Fang,Qiang Li,Zhiwei Wang*

Main category: cs.CV

TL;DR: FIA-Edit proposes an innovative inversion-free framework for text-guided image editing, ensuring high fidelity and efficiency by introducing a Frequency-Interactive Attention mechanism.


<details>
  <summary>Details</summary>
Motivation: Aim to address limitations in current text-guided image editing methods, such as poor background preservation, spatial inconsistencies, and over-editing, while maintaining high efficiency and fidelity.

Method: Introduces two key components: (1) Frequency Representation Interaction (FRI) to enhance cross-domain alignment, and (2) Feature Injection (FIJ) for preserving structural and semantic integrity via integrating source-side inputs.

Result: Achieves high-quality editing with low computational cost and better visual fidelity. FIA-Edit outperforms existing methods across tasks in image editing and extends its application to clinical data, aiding medical data augmentation and improving hemorrhage classification.

Conclusion: FIA-Edit effectively resolves current method limitations in text-guided image editing, offering precise, efficient, and fidelity-driven edits, with novel application in medical data augmentation for clinical benefit.

Abstract: Text-guided image editing has advanced rapidly with the rise of diffusion models. While flow-based inversion-free methods offer high efficiency by avoiding latent inversion, they often fail to effectively integrate source information, leading to poor background preservation, spatial inconsistencies, and over-editing due to the lack of effective integration of source information. In this paper, we present FIA-Edit, a novel inversion-free framework that achieves high-fidelity and semantically precise edits through a Frequency-Interactive Attention. Specifically, we design two key components: (1) a Frequency Representation Interaction (FRI) module that enhances cross-domain alignment by exchanging frequency components between source and target features within self-attention, and (2) a Feature Injection (FIJ) module that explicitly incorporates source-side queries, keys, values, and text embeddings into the target branch's cross-attention to preserve structure and semantics. Comprehensive and extensive experiments demonstrate that FIA-Edit supports high-fidelity editing at low computational cost (~6s per 512 * 512 image on an RTX 4090) and consistently outperforms existing methods across diverse tasks in visual quality, background fidelity, and controllability. Furthermore, we are the first to extend text-guided image editing to clinical applications. By synthesizing anatomically coherent hemorrhage variations in surgical images, FIA-Edit opens new opportunities for medical data augmentation and delivers significant gains in downstream bleeding classification. Our project is available at: https://github.com/kk42yy/FIA-Edit.

</details>


### [260] [Codebook-Centric Deep Hashing: End-to-End Joint Learning of Semantic Hash Centers and Neural Hash Function](https://arxiv.org/abs/2511.12162)
*Shuo Yin,Zhiyuan Yin,Yuqing Hou,Rui Liu,Yong Chen,Dell Zhang*

Main category: cs.CV

TL;DR: The paper introduces Center-Reassigned Hashing (CRH), an end-to-end deep hashing method that dynamically adjusts hash centers to improve semantic-based retrieval tasks.


<details>
  <summary>Details</summary>
Motivation: Existing deep hashing methods suffer from random center initialization or stage-wise training, which neglect inter-class semantics and introduce inefficiencies.

Method: CRH utilizes dynamic reassignment of hash centers from a preset codebook, integrating semantic relationships into learning without explicit optimization phases. It incorporates a multi-head mechanism for richer representations.

Result: Experiments on three benchmarks show CRH achieves better performance than state-of-the-art methods in semantic retrieval tasks.

Conclusion: CRH effectively addresses inefficiencies and enhances semantic hashing capabilities, providing a notable improvement in deep hashing frameworks.

Abstract: Hash center-based deep hashing methods improve upon pairwise or triplet-based approaches by assigning fixed hash centers to each class as learning targets, thereby avoiding the inefficiency of local similarity optimization. However, random center initialization often disregards inter-class semantic relationships. While existing two-stage methods mitigate this by first refining hash centers with semantics and then training the hash function, they introduce additional complexity, computational overhead, and suboptimal performance due to stage-wise discrepancies. To address these limitations, we propose $\textbf{Center-Reassigned Hashing (CRH)}$, an end-to-end framework that $\textbf{dynamically reassigns hash centers}$ from a preset codebook while jointly optimizing the hash function. Unlike previous methods, CRH adapts hash centers to the data distribution $\textbf{without explicit center optimization phases}$, enabling seamless integration of semantic relationships into the learning process. Furthermore, $\textbf{a multi-head mechanism}$ enhances the representational capacity of hash centers, capturing richer semantic structures. Extensive experiments on three benchmarks demonstrate that CRH learns semantically meaningful hash centers and outperforms state-of-the-art deep hashing methods in retrieval tasks.

</details>


### [261] [Rethinking Multimodal Point Cloud Completion: A Completion-by-Correction Perspective](https://arxiv.org/abs/2511.12170)
*Wang Luo,Di Wu,Hengyuan Na,Yinlin Zhu,Miao Hu,Guocong Quan*

Main category: cs.CV

TL;DR: This paper proposes a new paradigm, 'Completion-by-Correction,' for point cloud completion, addressing issues in previous methods by refining pre-generated 3D shapes to better match observations.


<details>
  <summary>Details</summary>
Motivation: Previous methods for point cloud completion often result in structural inconsistencies and artifacts due to an unconstrained inpainting paradigm, necessitating a robust approach to align reconstructions with real-world observations.

Method: The authors shift from completion through inpainting to 'Completion-by-Correction,' where they use a pre-generated, topologically complete shape prior as a starting point and refine it to match partial observations. They propose PGNet, a multi-stage framework using dual-feature encoding and hierarchical correction.

Result: The proposed PGNet achieves superior performance on the ShapeNetViPC dataset with a 23.5% lower Chamfer Distance and a 7.1% higher F-score compared to state-of-the-art baselines.

Conclusion: The 'Completion-by-Correction' paradigm and PGNet demonstrate significant improvements in solving point cloud completion tasks, showing promise for structurally accurate and observation-aligned reconstructions.

Abstract: Point cloud completion aims to reconstruct complete 3D shapes from partial observations, which is a challenging problem due to severe occlusions and missing geometry. Despite recent advances in multimodal techniques that leverage complementary RGB images to compensate for missing geometry, most methods still follow a Completion-by-Inpainting paradigm, synthesizing missing structures from fused latent features. We empirically show that this paradigm often results in structural inconsistencies and topological artifacts due to limited geometric and semantic constraints. To address this, we rethink the task and propose a more robust paradigm, termed Completion-by-Correction, which begins with a topologically complete shape prior generated by a pretrained image-to-3D model and performs feature-space correction to align it with the partial observation. This paradigm shifts completion from unconstrained synthesis to guided refinement, enabling structurally consistent and observation-aligned reconstruction. Building upon this paradigm, we introduce PGNet, a multi-stage framework that conducts dual-feature encoding to ground the generative prior, synthesizes a coarse yet structurally aligned scaffold, and progressively refines geometric details via hierarchical correction. Experiments on the ShapeNetViPC dataset demonstrate the superiority of PGNet over state-of-the-art baselines in terms of average Chamfer Distance (-23.5%) and F-score (+7.1%).

</details>


### [262] [MixAR: Mixture Autoregressive Image Generation](https://arxiv.org/abs/2511.12181)
*Jinyuan Hu,Jiayou Zhang,Shaobo Cui,Kun Zhang,Guangyi Chen*

Main category: cs.CV

TL;DR: This paper addresses limitations in image generation by combining discrete and continuous representations in autoregressive modeling, proposing techniques like MixAR and TI-Mix to enhance fidelity and efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome fidelity bottlenecks caused by the quantization process and limited codebook size in image generation using discrete autoregressive methods, and tackle challenges in continuous latent space modeling.

Method: The method introduces MixAR, which utilizes discrete tokens as prior guidance for continuous autoregressive modeling through strategies such as self-attention (DC-SA), cross-attention (DC-CA), and discrete token integration (DC-Mix). Additionally, TI-Mix is proposed to achieve consistent training and inference distributions.

Result: Experiments show that the DC-Mix strategy balances computational efficiency and generative fidelity well, while TI-Mix consistently improves training and generation outcomes.

Conclusion: By integrating discrete tokens and employing improved training-inference mixtures, the proposed methods significantly enhance the quality and efficiency of autoregressive image generation.

Abstract: Autoregressive (AR) approaches, which represent images as sequences of discrete tokens from a finite codebook, have achieved remarkable success in image generation. However, the quantization process and the limited codebook size inevitably discard fine-grained information, placing bottlenecks on fidelity. Motivated by this limitation, recent studies have explored autoregressive modeling in continuous latent spaces, which offers higher generation quality. Yet, unlike discrete tokens constrained by a fixed codebook, continuous representations lie in a vast and unstructured space, posing significant challenges for efficient autoregressive modeling. To address these challenges, we introduce MixAR, a novel framework that leverages mixture training paradigms to inject discrete tokens as prior guidance for continuous AR modeling. MixAR is a factorized formulation that leverages discrete tokens as prior guidance for continuous autoregressive prediction. We investigate several discrete-continuous mixture strategies, including self-attention (DC-SA), cross-attention (DC-CA), and a simple approach (DC-Mix) that replaces homogeneous mask tokens with informative discrete counterparts. Moreover, to bridge the gap between ground-truth training tokens and inference tokens produced by the pre-trained AR model, we propose Training-Inference Mixture (TI-Mix) to achieve consistent training and generation distributions. In our experiments, we demonstrate a favorable balance of the DC-Mix strategy between computational efficiency and generation fidelity, and consistent improvement of TI-Mix.

</details>


### [263] [MMRINet: Efficient Mamba-Based Segmentation with Dual-Path Refinement for Low-Resource MRI Analysis](https://arxiv.org/abs/2511.12193)
*Abdelrahman Elsayed,Ahmed Jaheen,Mohammad Yaqub*

Main category: cs.CV

TL;DR: The paper introduces MMRINet, a lightweight architecture for brain tumor segmentation in MRI, achieving strong performance with low computational resources.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of accurate brain tumor segmentation in MRI images under resource-constrained settings, where deep 3D networks are computationally demanding.

Method: The proposed model, MMRINet, utilizes linear-complexity Mamba state-space models and includes novel modules like DPFR for feature diversity and PFA for multi-scale fusion to enhance efficiency and accuracy.

Result: MMRINet achieved an average Dice score of 0.752 and an average HD95 of 12.23, utilizing only ~2.5M parameters, demonstrating suitability for low-resource environments.

Conclusion: MMRINet shows efficient and accurate segmentation for brain tumors in MRI images with minimal computational demand, making it ideal for low-resource clinical settings.

Abstract: Automated brain tumor segmentation in multi-parametric MRI remains challenging in resource-constrained settings where deep 3D networks are computationally prohibitive. We propose MMRINet, a lightweight architecture that replaces quadratic-complexity attention with linear-complexity Mamba state-space models for efficient volumetric context modeling. Novel Dual-Path Feature Refinement (DPFR) modules maximize feature diversity without additional data requirements, while Progressive Feature Aggregation (PFA) enables effective multi-scale fusion. In the BraTS-Lighthouse SSA 2025, our model achieves strong performance with an average Dice score of (0.752) and an average HD95 of (12.23) with only ~2.5M parameters, demonstrating efficient and accurate segmentation suitable for low-resource clinical environments. Our GitHub repository can be accessed here: github.com/BioMedIA-MBZUAI/MMRINet.

</details>


### [264] [Cross-View Cross-Modal Unsupervised Domain Adaptation for Driver Monitoring System](https://arxiv.org/abs/2511.12196)
*Aditi Bhalla,Christian Hellert,Enkelejda Kasneci*

Main category: cs.CV

TL;DR: This paper introduces a two-phase framework to enhance driver distraction detection models by addressing challenges of cross-view generalization and domain adaptation, achieving significant improvements in accuracy.


<details>
  <summary>Details</summary>
Motivation: Driver distraction significantly contributes to road accidents globally, and existing deep learning methods lack robustness in handling variations in camera viewpoints and sensor modalities.

Method: The proposed framework involves two phases: contrastive learning to extract view-invariant features, and unsupervised domain adaptation using information bottleneck loss to adapt to new modalities without labeled data.

Result: The framework improves top-1 accuracy substantially on RGB video data and outperforms current methods in unsupervised domain adaptation by up to 5%, utilizing state-of-the-art video transformers and the Drive&Act dataset.

Conclusion: The joint cross-view and cross-modal approach demonstrates scalability and robustness for real-world driver monitoring, addressing key deployment challenges.

Abstract: Driver distraction remains a leading cause of road traffic accidents, contributing to thousands of fatalities annually across the globe. While deep learning-based driver activity recognition methods have shown promise in detecting such distractions, their effectiveness in real-world deployments is hindered by two critical challenges: variations in camera viewpoints (cross-view) and domain shifts such as change in sensor modality or environment. Existing methods typically address either cross-view generalization or unsupervised domain adaptation in isolation, leaving a gap in the robust and scalable deployment of models across diverse vehicle configurations. In this work, we propose a novel two-phase cross-view, cross-modal unsupervised domain adaptation framework that addresses these challenges jointly on real-time driver monitoring data. In the first phase, we learn view-invariant and action-discriminative features within a single modality using contrastive learning on multi-view data. In the second phase, we perform domain adaptation to a new modality using information bottleneck loss without requiring any labeled data from the new domain. We evaluate our approach using state-of-the art video transformers (Video Swin, MViT) and multi modal driver activity dataset called Drive&Act, demonstrating that our joint framework improves top-1 accuracy on RGB video data by almost 50% compared to a supervised contrastive learning-based cross-view method, and outperforms unsupervised domain adaptation-only methods by up to 5%, using the same video transformer backbone.

</details>


### [265] [Bridging Granularity Gaps: Hierarchical Semantic Learning for Cross-domain Few-shot Segmentation](https://arxiv.org/abs/2511.12200)
*Sujun Sun,Haowen Gu,Cheng Xie,Yanxu Ren,Mingwu Ren,Haofeng Zhang*

Main category: cs.CV

TL;DR: This paper develops a Hierarchical Semantic Learning (HSL) framework to enhance cross-domain few-shot segmentation by addressing granularity gaps.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to improve semantic discriminability for novel classes in target domains during cross-domain few-shot segmentation, as existing methods mainly focus on style gaps and overlook segmentation granularity gaps.

Method: The paper introduces a Dual Style Randomization (DSR) module for style variations, a Hierarchical Semantic Mining (HSM) module for learning semantic features across granularities using multi-scale superpixels, and a Prototype Confidence-modulated Thresholding (PCMT) module to reduce segmentation ambiguity.

Result: The proposed method achieves state-of-the-art performance based on extensive experiments conducted on four target domain datasets.

Conclusion: The proposed HSL framework offers a comprehensive solution by tackling both style and granularity gaps in cross-domain few-shot segmentation, leading to better semantic recognition and segmentation accuracy.

Abstract: Cross-domain Few-shot Segmentation (CD-FSS) aims to segment novel classes from target domains that are not involved in training and have significantly different data distributions from the source domain, using only a few annotated samples, and recent years have witnessed significant progress on this task. However, existing CD-FSS methods primarily focus on style gaps between source and target domains while ignoring segmentation granularity gaps, resulting in insufficient semantic discriminability for novel classes in target domains. Therefore, we propose a Hierarchical Semantic Learning (HSL) framework to tackle this problem. Specifically, we introduce a Dual Style Randomization (DSR) module and a Hierarchical Semantic Mining (HSM) module to learn hierarchical semantic features, thereby enhancing the model's ability to recognize semantics at varying granularities. DSR simulates target domain data with diverse foreground-background style differences and overall style variations through foreground and global style randomization respectively, while HSM leverages multi-scale superpixels to guide the model to mine intra-class consistency and inter-class distinction at different granularities. Additionally, we also propose a Prototype Confidence-modulated Thresholding (PCMT) module to mitigate segmentation ambiguity when foreground and background are excessively similar. Extensive experiments are conducted on four popular target domain datasets, and the results demonstrate that our method achieves state-of-the-art performance.

</details>


### [266] [OmniSparse: Training-Aware Fine-Grained Sparse Attention for Long-Video MLLMs](https://arxiv.org/abs/2511.12201)
*Feng Chen,Yefei He,Shaoxuan He,Yuanyu He,Jing Liu,Lequan Lin,Akide Liu,Zhaoyang Li,Jiyuan Zhang,Zhenbang Sun,Bohan Zhuang,Qi Wu*

Main category: cs.CV

TL;DR: OmniSparse introduces a sparse attention framework for long-video MLLMs, achieving high performance with speed and memory efficiency.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in current sparse attention methods, which show inefficiencies in token selection and performance during training and inference.

Method: OmniSparse incorporates query selection, dynamic key-value budgeting, and KV cache slimming to optimize token selection and attention recall.

Result: It achieves the same performance as full attention while providing up to 2.7x faster prefill and 2.4x less memory usage during decoding.

Conclusion: OmniSparse provides a more efficient and adaptive sparse attention mechanism, addressing both training and inference needs in long-video MLLMs.

Abstract: Existing sparse attention methods primarily target inference-time acceleration by selecting critical tokens under predefined sparsity patterns. However, they often fail to bridge the training-inference gap and lack the capacity for fine-grained token selection across multiple dimensions such as queries, key-values (KV), and heads, leading to suboptimal performance and limited acceleration gains. In this paper, we introduce OmniSparse, a training-aware fine-grained sparse attention framework for long-video MLLMs, which operates in both training and inference with dynamic token budget allocation. Specifically, OmniSparse contains three adaptive and complementary mechanisms: (1) query selection via lazy-active classification, retaining active queries that capture broad semantic similarity while discarding most lazy ones that focus on limited local context and exhibit high functional redundancy; (2) KV selection with head-level dynamic budget allocation, where a shared budget is determined based on the flattest head and applied uniformly across all heads to ensure attention recall; and (3) KV cache slimming to reduce head-level redundancy by selectively fetching visual KV cache according to the head-level decoding query pattern. Experimental results show that OmniSparse matches the performance of full attention while achieving up to 2.7x speedup during prefill and 2.4x memory reduction during decoding.

</details>


### [267] [LSS3D: Learnable Spatial Shifting for Consistent and High-Quality 3D Generation from Single-Image](https://arxiv.org/abs/2511.12202)
*Zhuojiang Cai,Yiheng Zhang,Meitong Guo,Mingdao Wang,Yuwang Wang*

Main category: cs.CV

TL;DR: This paper introduces LSS3D, a novel solution for enhancing multi-view 3D generation quality by addressing shape and texture inconsistencies and improving robustness to non-frontal perspectives.


<details>
  <summary>Details</summary>
Motivation: Multi-view diffusion-based 3D generation methods often struggle with inconsistencies in shape and texture across views and fail to handle oblique perspectives effectively, resulting in poor quality outcomes.

Method: The proposed method, LSS3D, employs learnable spatial shifting parameters to reconcile inconsistencies across views, guided by reconstructed mesh, ensuring spatial consistency. Additionally, it uses the input view as an optimization constraint to enhance robustness to non-frontal inputs.

Result: LSS3D achieves better geometric and textural results compared to existing methods, showing superior performance even for elevated viewpoint inputs. Comprehensive evaluations affirm its effectiveness across diverse input viewpoints.

Conclusion: LSS3D offers a robust and high-quality solution to 3D generation challenges, addressing multi-view issues and improving performance across varying input perspectives. It also establishes a robust evaluation pipeline for future developments in the field.

Abstract: Recently, multi-view diffusion-based 3D generation methods have gained significant attention. However, these methods often suffer from shape and texture misalignment across generated multi-view images, leading to low-quality 3D generation results, such as incomplete geometric details and textural ghosting. Some methods are mainly optimized for the frontal perspective and exhibit poor robustness to oblique perspective inputs. In this paper, to tackle the above challenges, we propose a high-quality image-to-3D approach, named LSS3D, with learnable spatial shifting to explicitly and effectively handle the multiview inconsistencies and non-frontal input view. Specifically, we assign learnable spatial shifting parameters to each view, and adjust each view towards a spatially consistent target, guided by the reconstructed mesh, resulting in high-quality 3D generation with more complete geometric details and clean textures. Besides, we include the input view as an extra constraint for the optimization, further enhancing robustness to non-frontal input angles, especially for elevated viewpoint inputs. We also provide a comprehensive quantitative evaluation pipeline that can contribute to the community in performance comparisons. Extensive experiments demonstrate that our method consistently achieves leading results in both geometric and texture evaluation metrics across more flexible input viewpoints.

</details>


### [268] [GeoMVD: Geometry-Enhanced Multi-View Generation Model Based on Geometric Information Extraction](https://arxiv.org/abs/2511.12204)
*Jiaqi Wu,Yaosen Chen,Shuyuan Zhu*

Main category: cs.CV

TL;DR: The paper presents the Geometry-guided Multi-View Diffusion Model to generate consistent and high-quality multi-view images, addressing challenges in computational accuracy and resolution.


<details>
  <summary>Details</summary>
Motivation: Existing single-view extension methods face difficulties in ensuring cross-view consistency and generating high-resolution images for multi-view image generation.

Method: The model incorporates multi-view geometric information via depth maps, normal maps, and segmentation masks, alongside a geometry-enhanced attention mechanism, adaptive learning strategy, and dynamic adjustment of geometric data influence.

Result: The proposed techniques enable the generation of multi-view images with improved consistency, detail, spatial relationships, and natural visual coherence.

Conclusion: The study provides a novel framework for generating realistic and highly detailed multi-view images, demonstrating significant improvements in cross-view consistency and output quality.

Abstract: Multi-view image generation holds significant application value in computer vision, particularly in domains like 3D reconstruction, virtual reality, and augmented reality. Most existing methods, which rely on extending single images, face notable computational challenges in maintaining cross-view consistency and generating high-resolution outputs. To address these issues, we propose the Geometry-guided Multi-View Diffusion Model, which incorporates mechanisms for extracting multi-view geometric information and adjusting the intensity of geometric features to generate images that are both consistent across views and rich in detail. Specifically, we design a multi-view geometry information extraction module that leverages depth maps, normal maps, and foreground segmentation masks to construct a shared geometric structure, ensuring shape and structural consistency across different views. To enhance consistency and detail restoration during generation, we develop a decoupled geometry-enhanced attention mechanism that strengthens feature focus on key geometric details, thereby improving overall image quality and detail preservation. Furthermore, we apply an adaptive learning strategy that fine-tunes the model to better capture spatial relationships and visual coherence between the generated views, ensuring realistic results. Our model also incorporates an iterative refinement process that progressively improves the output quality through multiple stages of image generation. Finally, a dynamic geometry information intensity adjustment mechanism is proposed to adaptively regulate the influence of geometric data, optimizing overall quality while ensuring the naturalness of generated images. More details can be found on the project page: https://github.com/SobeyMIL/GeoMVD.com.

</details>


### [269] [A Novel AI-Driven System for Real-Time Detection of Mirror Absence, Helmet Non-Compliance, and License Plates Using YOLOv8 and OCR](https://arxiv.org/abs/2511.12206)
*Nishant Vasantkumar Hegde,Aditi Agarwal,Minal Moharir*

Main category: cs.CV

TL;DR: This paper introduces an AI-driven system to automate traffic law enforcement focusing on helmet compliance and rear-view mirror presence, using YOLOv8 and EasyOCR for object detection and license plate recognition.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address the inefficiency and inconsistency of manual enforcement of helmet laws and vehicle safety standards.

Method: It employs YOLOv8 and EasyOCR for detection and recognition, trained on custom datasets with diverse annotations, alongside advanced image preprocessing methods and a Streamlit interface for real-time application.

Result: The system achieved robust detection metrics with a precision of 0.9147, recall of 0.886, mAP@50 of 0.843, and mAP@50 95 of 0.503.

Conclusion: This AI-powered system proves to be a significant contribution to automated traffic law enforcement, showcasing practical deployment strategies and strong detection performance.

Abstract: Road safety is a critical global concern, with manual enforcement of helmet laws and vehicle safety standards (e.g., rear-view mirror presence) being resource-intensive and inconsistent. This paper presents an AI-powered system to automate traffic violation detection, significantly enhancing enforcement efficiency and road safety. The system leverages YOLOv8 for robust object detection and EasyOCR for license plate recognition. Trained on a custom dataset of annotated images (augmented for diversity), it identifies helmet non-compliance, the absence of rear-view mirrors on motorcycles, an innovative contribution to automated checks, and extracts vehicle registration numbers. A Streamlit-based interface facilitates real-time monitoring and violation logging. Advanced image preprocessing enhances license plate recognition, particularly under challenging conditions. Based on evaluation results, the model achieves an overall precision of 0.9147, a recall of 0.886, and a mean Average Precision (mAP@50) of 0.843. The mAP@50 95 of 0.503 further indicates strong detection capability under stricter IoU thresholds. This work demonstrates a practical and effective solution for automated traffic rule enforcement, with considerations for real-world deployment discussed.

</details>


### [270] [Mixture of States: Routing Token-Level Dynamics for Multimodal Generation](https://arxiv.org/abs/2511.12207)
*Haozhe Liu,Ding Liu,Mingchen Zhuge,Zijian Zhou,Tian Xie,Sen He,Yukang Yang,Shuming Liu,Yuren Cong,Jiadong Guo,Hongyu Xu,Ke Xu,Kam-Woh Ng,Juan C. Pérez,Juan-Manuel~Pérez-Rúa,Tao Xiang,Wei Liu,Shikun Liu,Jürgen Schmidhuber*

Main category: cs.CV

TL;DR: MoS (Mixture of States) is introduced as an efficient, state-based fusion paradigm for multimodal diffusion models, showing state-of-the-art results in text-to-image generation and editing.


<details>
  <summary>Details</summary>
Motivation: To develop a flexible and efficient fusion mechanism for multimodal diffusion models that handles the alignment of token-level features with the diffusion trajectory effectively.

Method: MoS utilizes a learnable, token-wise router that enables denoising timestep- and input-dependent sparse interactions of modalities' hidden states. It employs an ε-greedy training strategy to efficiently select contextual features.

Result: MoS achieves state-of-the-art results in text-to-image generation (MoS-Image) and editing (MoS-Editing), with models of 3B–5B parameters surpassing counterparts up to 4× larger.

Conclusion: The MoS paradigm establishes a scalable, compute-efficient approach for multimodal diffusion models, indicating its applicability for future model scaling efforts.

Abstract: We introduce MoS (Mixture of States), a novel fusion paradigm for multimodal diffusion models that merges modalities using flexible, state-based interactions. The core of MoS is a learnable, token-wise router that creates denoising timestep- and input-dependent interactions between modalities' hidden states, precisely aligning token-level features with the diffusion trajectory. This router sparsely selects the top-$k$ hidden states and is trained with an $ε$-greedy strategy, efficiently selecting contextual features with minimal learnable parameters and negligible computational overhead. We validate our design with text-to-image generation (MoS-Image) and editing (MoS-Editing), which achieve state-of-the-art results. With only 3B to 5B parameters, our models match or surpass counterparts up to $4\times$ larger. These findings establish MoS as a flexible and compute-efficient paradigm for scaling multimodal diffusion models.

</details>


### [271] [FaNe: Towards Fine-Grained Cross-Modal Contrast with False-Negative Reduction and Text-Conditioned Sparse Attention](https://arxiv.org/abs/2511.12215)
*Peng Zhang,Zhihui Lai,Wenting Chen,Xu Wu,Heng Kong*

Main category: cs.CV

TL;DR: The paper introduces FaNe, a framework to address false negatives and insufficient cross-modal alignment in medical vision-language pre-training.


<details>
  <summary>Details</summary>
Motivation: Existing medical VLP methods struggle with false negatives caused by semantically similar texts and lack of fine-grained image-text alignment.

Method: FaNe employs a semantic-aware pair mining strategy and text-conditioned sparse attention pooling for precise alignment, complemented by a hard-negative aware contrastive loss.

Result: FaNe delivers state-of-the-art performance on five benchmarks, improving classification, detection, and segmentation tasks effectively.

Conclusion: FaNe proves to be an effective solution enhancing text-image alignment and intra-modal discrimination in medical VLP tasks.

Abstract: Medical vision-language pre-training (VLP) offers significant potential for advancing medical image understanding by leveraging paired image-report data. However, existing methods are limited by Fa}lse Negatives (FaNe) induced by semantically similar texts and insufficient fine-grained cross-modal alignment. To address these limitations, we propose FaNe, a semantic-enhanced VLP framework. To mitigate false negatives, we introduce a semantic-aware positive pair mining strategy based on text-text similarity with adaptive normalization. Furthermore, we design a text-conditioned sparse attention pooling module to enable fine-grained image-text alignment through localized visual representations guided by textual cues. To strengthen intra-modal discrimination, we develop a hard-negative aware contrastive loss that adaptively reweights semantically similar negatives. Extensive experiments on five downstream medical imaging benchmarks demonstrate that FaNe achieves state-of-the-art performance across image classification, object detection, and semantic segmentation, validating the effectiveness of our framework.

</details>


### [272] [Suppressing VLM Hallucinations with Spectral Representation Filtering](https://arxiv.org/abs/2511.12220)
*Ameen Ali,Tamim Zoabi,Lior Wolf*

Main category: cs.CV

TL;DR: This paper introduces Spectral Representation Filtering (SRF), a training-free method to reduce hallucinations in vision-language models by correcting covariance biases in feature space.


<details>
  <summary>Details</summary>
Motivation: Vision-language models often hallucinate descriptions of non-existent objects, attributes, or relations due to language priors and poor cross-modal grounding, motivating the need for methods to improve their reliability.

Method: SRF uses eigendecomposition to identify hallucination modes in the covariance structure of features and applies a soft spectral filter to attenuate these biases in deeper model layers, without requiring retraining or architectural changes.

Result: Experiments show that SRF reduces hallucination rates across multiple vision-language models and benchmarks while maintaining high-quality captions.

Conclusion: SRF is a lightweight and effective method for improving the faithfulness of VLM outputs post-hoc, achieving state-of-the-art performance without extra inference or architectural complexity.

Abstract: Vision-language models (VLMs) frequently produce hallucinations in the form of descriptions of objects, attributes, or relations that do not exist in the image due to over-reliance on language priors and imprecise cross-modal grounding. We introduce Spectral Representation Filtering (SRF), a lightweight, training-free method to suppress such hallucinations by analyzing and correcting the covariance structure of the model's representations. SRF identifies low-rank hallucination modes through eigendecomposition of the covariance of the differences between features collected for truthful and hallucinatory captions, revealing structured biases in the feature space. A soft spectral filter then attenuates these modes in the feed-forward projection weights of deeper vLLM layers, equalizing feature variance while preserving semantic fidelity. Unlike decoding or retraining-based approaches, SRF operates entirely post-hoc, incurs zero inference overhead, and requires no architectural modifications. Across three families of VLMs (LLaVA-1.5, MiniGPT-4, and mPLUG-Owl2), SRF consistently reduces hallucination rates on MSCOCO, POPE-VQA, and other visual tasks benchmarks, achieving state-of-the-art faithfulness without degrading caption quality.

</details>


### [273] [MiniGPT-Pancreas: Multimodal Large Language Model for Pancreas Cancer Classification and Detection](https://arxiv.org/abs/2412.15925)
*Andrea Moglia,Elia Clement Nastasio,Luca Mainardi,Pietro Cerveri*

Main category: cs.CV

TL;DR: MiniGPT-Pancreas is a Multimodal Large Language Model (MLLM) designed to help clinicians in pancreas cancer diagnosis by integrating visual and textual data. While it shows promising results in some detection and classification tasks, there is room for improvement in pancreas tumor detection.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the difficulty of pancreas imaging due to small size, blurred boundaries, and variable shape/position of the organ, and to assist clinicians in diagnosing pancreas cancer through an interactive MLLM.

Method: The general-purpose MiniGPT-v2 was fine-tuned for tasks such as pancreas detection, tumor classification, and tumor detection using multimodal prompts with CT scans from NIH, MSD, and AbdomenCT-1k datasets.

Result: MiniGPT-Pancreas achieved notable accuracy metrics for pancreas cancer classification (accuracy: 0.876, precision: 0.874, recall: 0.878) and respectable IoUs for detecting multiple organs, including the pancreas. However, it had a lower IoU of 0.168 for pancreas tumor detection.

Conclusion: MiniGPT-Pancreas is a promising tool for aiding in pancreas cancer diagnosis but requires further development to improve its effectiveness, particularly in pancreas tumor detection tasks.

Abstract: Problem: Pancreas radiological imaging is challenging due to the small size, blurred boundaries, and variability of shape and position of the organ among patients. Goal: In this work we present MiniGPT-Pancreas, a Multimodal Large Language Model (MLLM), as an interactive chatbot to support clinicians in pancreas cancer diagnosis by integrating visual and textual information. Methods: MiniGPT-v2, a general-purpose MLLM, was fine-tuned in a cascaded way for pancreas detection, tumor classification, and tumor detection with multimodal prompts combining questions and computed tomography scans from the National Institute of Health (NIH), and Medical Segmentation Decathlon (MSD) datasets. The AbdomenCT-1k dataset was used to detect the liver, spleen, kidney, and pancreas. Results: MiniGPT-Pancreas achieved an Intersection over Union (IoU) of 0.595 and 0.550 for the detection of pancreas on NIH and MSD datasets, respectively. For the pancreas cancer classification task on the MSD dataset, accuracy, precision, and recall were 0.876, 0.874, and 0.878, respectively. When evaluating MiniGPT-Pancreas on the AbdomenCT-1k dataset for multi-organ detection, the IoU was 0.8399 for the liver, 0.722 for the kidney, 0.705 for the spleen, and 0.497 for the pancreas. For the pancreas tumor detection task, the IoU score was 0.168 on the MSD dataset. Conclusions: MiniGPT-Pancreas represents a promising solution to support clinicians in the classification of pancreas images with pancreas tumors. Future research is needed to improve the score on the detection task, especially for pancreas tumors.

</details>


### [274] [Model Inversion Attack Against Deep Hashing](https://arxiv.org/abs/2511.12233)
*Dongdong Zhao,Qiben Xu,Ranxin Fang,Baogang Song*

Main category: cs.CV

TL;DR: The paper addresses privacy risks in deep hashing by introducing DHMI, a model inversion framework capable of reconstructing images from hash codes, even in black-box settings.


<details>
  <summary>Details</summary>
Motivation: To address the unexamined security implications and risks of model inversion attacks in deep hashing systems.

Method: The proposed DHMI uses a diffusion-based framework, employing semantic hash centers, surrogate-guided denoising optimization, and a novel attack metric to reconstruct high-fidelity images.

Result: DHMI successfully reconstructs high-resolution images in black-box settings, outperforming existing state-of-the-art model inversion techniques.

Conclusion: Deep hashing systems are vulnerable to privacy risks, and the DHMI framework highlights the need for better security in these systems, proving the real threat of model inversion attacks.

Abstract: Deep hashing improves retrieval efficiency through compact binary codes, yet it introduces severe and often overlooked privacy risks. The ability to reconstruct original training data from hash codes could lead to serious threats such as biometric forgery and privacy breaches. However, model inversion attacks specifically targeting deep hashing models remain unexplored, leaving their security implications unexamined. This research gap stems from the inaccessibility of genuine training hash codes and the highly discrete Hamming space, which prevents existing methods from adapting to deep hashing. To address these challenges, we propose DHMI, the first diffusion-based model inversion framework designed for deep hashing. DHMI first clusters an auxiliary dataset to derive semantic hash centers as surrogate anchors. It then introduces a surrogate-guided denoising optimization method that leverages a novel attack metric (fusing classification consistency and hash proximity) to dynamically select candidate samples. A cluster of surrogate models guides the refinement of these candidates, ensuring the generation of high-fidelity and semantically consistent images. Experiments on multiple datasets demonstrate that DHMI successfully reconstructs high-resolution, high-quality images even under the most challenging black-box setting, where no training hash codes are available. Our method outperforms the existing state-of-the-art model inversion attacks in black-box scenarios, confirming both its practical efficacy and the critical privacy risks inherent in deep hashing systems.

</details>


### [275] [Fusionista2.0: Efficiency Retrieval System for Large-Scale Datasets](https://arxiv.org/abs/2511.12255)
*Huy M. Le,Dat Tien Nguyen,Phuc Binh Nguyen,Gia-Bao Le-Tran,Phu Truong Thien,Cuong Dinh,Minh Nguyen,Nga Nguyen,Thuy T. N. Nguyen,Huy Gia Ngo,Tan Nhat Nguyen,Binh T. Nguyen,Monojit Choudhury*

Main category: cs.CV

TL;DR: Fusionista2.0 is a video retrieval system optimized for speed and usability, demonstrating significant improvements in retrieval time (75% reduction), accuracy, and user satisfaction.


<details>
  <summary>Details</summary>
Motivation: The Video Browser Showdown's strict time constraints highlighted the need for an efficient and accurate video retrieval system aiming to satisfy both expert and non-expert users.

Method: Fusionista2.0 underwent major upgrades, including efficient preprocessing (using ffmpeg), advanced OCR (Vintern-1B-v3.5), faster ASR (faster-whisper), and lightweight vision-language models for question answering. Additionally, a redesigned user interface improved accessibility and workflow.

Result: Evaluations reveal retrieval time reduced by up to 75%, increased accuracy, and enhanced user satisfaction, showcasing the system's effectiveness.

Conclusion: Fusionista2.0 is a competitive, user-friendly video retrieval system well-suited for large-scale video search, meeting the demand for speed and precision.

Abstract: The Video Browser Showdown (VBS) challenges systems to deliver accurate results under strict time constraints. To meet this demand, we present Fusionista2.0, a streamlined video retrieval system optimized for speed and usability. All core modules were re-engineered for efficiency: preprocessing now relies on ffmpeg for fast keyframe extraction, optical character recognition uses Vintern-1B-v3.5 for robust multilingual text recognition, and automatic speech recognition employs faster-whisper for real-time transcription. For question answering, lightweight vision-language models provide quick responses without the heavy cost of large models. Beyond these technical upgrades, Fusionista2.0 introduces a redesigned user interface with improved responsiveness, accessibility, and workflow efficiency, enabling even non-expert users to retrieve relevant content rapidly. Evaluations demonstrate that retrieval time was reduced by up to 75% while accuracy and user satisfaction both increased, confirming Fusionista2.0 as a competitive and user-friendly system for large-scale video search.

</details>


### [276] [Prompt-Conditioned FiLM and Multi-Scale Fusion on MedSigLIP for Low-Dose CT Quality Assessment](https://arxiv.org/abs/2511.12256)
*Tolga Demiroglu,Mehmet Ozan Unal,Metin Ertas,Isa Yildirim*

Main category: cs.CV

TL;DR: The paper introduces a prompt-conditioned framework based on MedSigLIP using textual prompts for data-efficient learning, achieving state-of-the-art results in LDCT quality assessment.


<details>
  <summary>Details</summary>
Motivation: To enhance low-dose CT image quality assessment by utilizing text-based conditioning for more efficient learning and adaptability.

Method: The method uses MedSigLIP enhanced with Feature-wise Linear Modulation (FiLM) and multi-scale pooling, combining global, local, and texture-aware pooling through a multi-head regression approach with pairwise ranking loss.

Result: The proposed framework achieved state-of-the-art performance in the LDCTIQA2023 challenge, with PLCC = 0.9575, SROCC = 0.9561, and KROCC = 0.8301.

Conclusion: The prompt-conditioned framework is highly effective for improving low-dose CT image quality assessment, setting a new benchmark in performance.

Abstract: We propose a prompt-conditioned framework built on MedSigLIP that injects textual priors via Feature-wise Linear Modulation (FiLM) and multi-scale pooling. Text prompts condition patch-token features on clinical intent, enabling data-efficient learning and rapid adaptation. The architecture combines global, local, and texture-aware pooling through separate regression heads fused by a lightweight MLP, trained with pairwise ranking loss. Evaluated on the LDCTIQA2023 (a public LDCT quality assessment challenge) with 1,000 training images, we achieve PLCC = 0.9575, SROCC = 0.9561, and KROCC = 0.8301, surpassing the top-ranked published challenge submissions and demonstrating the effectiveness of our prompt-guided approach.

</details>


### [277] [A Disease-Aware Dual-Stage Framework for Chest X-ray Report Generation](https://arxiv.org/abs/2511.12259)
*Puzhen Wu,Hexin Dong,Yi Lin,Yihao Ding,Yifan Peng*

Main category: cs.CV

TL;DR: The paper introduces a dual-stage disease-aware framework to improve the clinical accuracy and linguistic quality of chest X-ray report generation.


<details>
  <summary>Details</summary>
Motivation: Existing models for radiology report generation often fail to capture disease-specific features and align vision-language representations effectively, impacting clinical accuracy and usability.

Method: The method involves a two-stage framework: Stage 1 involves learning Disease-Aware Semantic Tokens (DASTs) with cross-attention and contrastive learning, while Stage 2 introduces a Disease-Visual Attention Fusion (DVAF) module and Dual-Modal Similarity Retrieval (DMSR) for integrating disease-aware and visual features.

Result: The proposed framework achieves state-of-the-art results on datasets like CheXpert Plus, IU X-ray, and MIMIC-CXR, with notable enhancements in generating clinically accurate and linguistically sound radiology reports.

Conclusion: This dual-stage disease-aware approach significantly enhances the quality and relevance of automated chest X-ray report generation, addressing key limitations of prior models by integrating disease-specific knowledge and improving vision-language alignment.

Abstract: Radiology report generation from chest X-rays is an important task in artificial intelligence with the potential to greatly reduce radiologists' workload and shorten patient wait times. Despite recent advances, existing approaches often lack sufficient disease-awareness in visual representations and adequate vision-language alignment to meet the specialized requirements of medical image analysis. As a result, these models usually overlook critical pathological features on chest X-rays and struggle to generate clinically accurate reports. To address these limitations, we propose a novel dual-stage disease-aware framework for chest X-ray report generation. In Stage~1, our model learns Disease-Aware Semantic Tokens (DASTs) corresponding to specific pathology categories through cross-attention mechanisms and multi-label classification, while simultaneously aligning vision and language representations via contrastive learning. In Stage~2, we introduce a Disease-Visual Attention Fusion (DVAF) module to integrate disease-aware representations with visual features, along with a Dual-Modal Similarity Retrieval (DMSR) mechanism that combines visual and disease-specific similarities to retrieve relevant exemplars, providing contextual guidance during report generation. Extensive experiments on benchmark datasets (i.e., CheXpert Plus, IU X-ray, and MIMIC-CXR) demonstrate that our disease-aware framework achieves state-of-the-art performance in chest X-ray report generation, with significant improvements in clinical accuracy and linguistic quality.

</details>


### [278] [CrossVid: A Comprehensive Benchmark for Evaluating Cross-Video Reasoning in Multimodal Large Language Models](https://arxiv.org/abs/2511.12263)
*Jingyao Li,Jingyun Wang,Molin Tan,Haochen Wang,Cilin Yan,Likun Shi,Jiayin Cai,Xiaolong Jiang,Yao Hu*

Main category: cs.CV

TL;DR: CrossVid introduces a benchmark to evaluate multimodal large language models (MLLMs) in cross-video reasoning tasks, emphasizing spatial-temporal reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks are limited for assessing MLLMs' cross-video reasoning abilities, making it difficult to evaluate their performance in diverse real-world video understanding tasks.

Method: CrossVid includes hierarchical tasks across dimensions, utilizes 5,331 videos and 9,015 questions, and assesses performance through experiments on various MLLMs.

Result: Gemini-2.5-Pro achieved the best average accuracy of 50.4%, with observations that most MLLMs struggle to integrate and compare evidence across videos.

Conclusion: CrossVid offers a comprehensive framework for testing and improving MLLMs' cross-video reasoning abilities, guiding future research developments.

Abstract: Cross-Video Reasoning (CVR) presents a significant challenge in video understanding, which requires simultaneous understanding of multiple videos to aggregate and compare information across groups of videos. Most existing video understanding benchmarks focus on single-video analysis, failing to assess the ability of multimodal large language models (MLLMs) to simultaneously reason over various videos. Recent benchmarks evaluate MLLMs' capabilities on multi-view videos that capture different perspectives of the same scene. However, their limited tasks hinder a thorough assessment of MLLMs in diverse real-world CVR scenarios. To this end, we introduce CrossVid, the first benchmark designed to comprehensively evaluate MLLMs' spatial-temporal reasoning ability in cross-video contexts. Firstly, CrossVid encompasses a wide spectrum of hierarchical tasks, comprising four high-level dimensions and ten specific tasks, thereby closely reflecting the complex and varied nature of real-world video understanding. Secondly, CrossVid provides 5,331 videos, along with 9,015 challenging question-answering pairs, spanning single-choice, multiple-choice, and open-ended question formats. Through extensive experiments on various open-source and closed-source MLLMs, we observe that Gemini-2.5-Pro performs best on CrossVid, achieving an average accuracy of 50.4%. Notably, our in-depth case study demonstrates that most current MLLMs struggle with CVR tasks, primarily due to their inability to integrate or compare evidence distributed across multiple videos for reasoning. These insights highlight the potential of CrossVid to guide future advancements in enhancing MLLMs' CVR capabilities.

</details>


### [279] [ZoomEarth: Active Perception for Ultra-High-Resolution Geospatial Vision-Language Tasks](https://arxiv.org/abs/2511.12267)
*Ruixun Liu,Bowen Fu,Jiayi Song,Kaiyu Li,Wanchen Li,Lanxuan Xue,Hui Qiao,Weizhan Zhang,Deyu Meng,Xiangyong Cao*

Main category: cs.CV

TL;DR: The paper addresses the challenges of processing ultra-high-resolution (UHR) remote sensing images and introduces the ZoomEarth framework, utilizing active perception and an adaptive cropping-zooming approach, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need to improve UHR remote sensing image processing by overcoming redundancy issues and enabling more effective region-guided analysis.

Method: The authors introduced the LRS-GRO dataset and ZoomEarth framework that actively zooms into information-rich areas and employs Region-Guided rewards for enhanced processing. Training was based on supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO).

Result: The proposed framework, ZoomEarth, achieved state-of-the-art performance in processing UHR RS images on multiple benchmarks, including zero-shot settings, and demonstrated integration versatility for downstream tasks.

Conclusion: ZoomEarth's adaptive and active perception paradigm enhances UHR remote sensing image processing, showcasing its capacity for future integrations across diverse image-related applications.

Abstract: Ultra-high-resolution (UHR) remote sensing (RS) images offer rich fine-grained information but also present challenges in effective processing. Existing dynamic resolution and token pruning methods are constrained by a passive perception paradigm, suffering from increased redundancy when obtaining finer visual inputs. In this work, we explore a new active perception paradigm that enables models to revisit information-rich regions. First, we present LRS-GRO, a large-scale benchmark dataset tailored for active perception in UHR RS processing, encompassing 17 question types across global, region, and object levels, annotated via a semi-automatic pipeline. Building on LRS-GRO, we propose ZoomEarth, an adaptive cropping-zooming framework with a novel Region-Guided reward that provides fine-grained guidance. Trained via supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO), ZoomEarth achieves state-of-the-art performance on LRS-GRO and, in the zero-shot setting, on three public UHR remote sensing benchmarks. Furthermore, ZoomEarth can be seamlessly integrated with downstream models for tasks such as cloud removal, denoising, segmentation, and image editing through simple tool interfaces, demonstrating strong versatility and extensibility.

</details>


### [280] [TM-UNet: Token-Memory Enhanced Sequential Modeling for Efficient Medical Image Segmentation](https://arxiv.org/abs/2511.12270)
*Yaxuan Jiao,Qing Xu,Yuxiang Luo,Xiangjian He,Zhen Chen,Wenting Duan*

Main category: cs.CV

TL;DR: The paper introduces TM-UNet, a lightweight framework for medical image segmentation using token-memory mechanisms to enhance efficiency and reduce computational costs while surpassing performance benchmarks.


<details>
  <summary>Details</summary>
Motivation: The problem of high computational cost in transformer-based methods prevents their practical deployment for medical image segmentation, necessitating a more efficient yet effective solution.

Method: This paper proposes TM-UNet with an innovative multi-scale token-memory (MSTM) block. The block converts 2D spatial features into token sequences with matrix memory cells and incorporates exponential gating and multi-scale contextual extraction for efficient hierarchical representation.

Result: TM-UNet demonstrates superior performance over state-of-the-art methods in medical segmentation tasks while achieving significant reductions in computational cost.

Conclusion: TM-UNet is an effective, efficient solution for medical image segmentation that balances global reasoning, low computation, and outperforms existing methods.

Abstract: Medical image segmentation is essential for clinical diagnosis and treatment planning. Although transformer-based methods have achieved remarkable results, their high computational cost hinders clinical deployment. To address this issue, we propose TM-UNet, a novel lightweight framework that integrates token sequence modeling with an efficient memory mechanism for efficient medical segmentation. Specifically, we introduce a multi-scale token-memory (MSTM) block that transforms 2D spatial features into token sequences through strategic spatial scanning, leveraging matrix memory cells to selectively retain and propagate discriminative contextual information across tokens. This novel token-memory mechanism acts as a dynamic knowledge store that captures long-range dependencies with linear complexity, enabling efficient global reasoning without redundant computation. Our MSTM block further incorporates exponential gating to identify token effectiveness and multi-scale contextual extraction via parallel pooling operations, enabling hierarchical representation learning without computational overhead. Extensive experiments demonstrate that TM-UNet outperforms state-of-the-art methods across diverse medical segmentation tasks with substantially reduced computation cost. The code is available at https://github.com/xq141839/TM-UNet.

</details>


### [281] [D$^{3}$ToM: Decider-Guided Dynamic Token Merging for Accelerating Diffusion MLLMs](https://arxiv.org/abs/2511.12280)
*Shuochen Chang,Xiaofeng Zhang,Qingyang Liu,Li Niu*

Main category: cs.CV

TL;DR: This paper introduces D$^{3}$ToM, a dynamic token merging method for diffusion-based multimodal large language models (Diffusion MLLMs) aimed at accelerating inference while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the slower inference speed of Diffusion MLLMs, which arises due to their cubic decoding complexity caused by full bidirectional self-attention over large visual token sequences.

Method: D$^{3}$ToM uses decider tokens to build importance maps of visual tokens, retaining salient ones and merging redundant tokens dynamically at each denoising step. This technique plugs into a transformer layer, shortening token sequences for all subsequent layers without modifying model parameters.

Result: Experiments show that D$^{3}$ToM effectively accelerates inference in Diffusion MLLMs while preserving competitive task performance.

Conclusion: D$^{3}$ToM provides a dynamic and efficient approach for optimizing inference in Diffusion MLLMs, aligning the computational budget with improved decoding processes while maintaining competitive results.

Abstract: Diffusion-based multimodal large language models (Diffusion MLLMs) have recently demonstrated impressive non-autoregressive generative capabilities across vision-and-language tasks. However, Diffusion MLLMs exhibit substantially slower inference than autoregressive models: Each denoising step employs full bidirectional self-attention over the entire sequence, resulting in cubic decoding complexity that becomes computationally impractical with thousands of visual tokens. To address this challenge, we propose D$^{3}$ToM, a Decider-guided dynamic token merging method that dynamically merges redundant visual tokens at different denoising steps to accelerate inference in Diffusion MLLMs. At each denoising step, D$^{3}$ToM uses decider tokens-the tokens generated in the previous denoising step-to build an importance map over all visual tokens. Then it maintains a proportion of the most salient tokens and merges the remainder through similarity-based aggregation. This plug-and-play module integrates into a single transformer layer, physically shortening the visual token sequence for all subsequent layers without altering model parameters. Moreover, D$^{3}$ToM employs a merge ratio that dynamically varies with each denoising step, aligns with the native decoding process of Diffusion MLLMs, achieving superior performance under equivalent computational budgets. Extensive experiments show that D$^{3}$ToM accelerates inference while preserving competitive performance. The code is released at https://github.com/bcmi/D3ToM-Diffusion-MLLM.

</details>


### [282] [One target to align them all: LiDAR, RGB and event cameras extrinsic calibration for Autonomous Driving](https://arxiv.org/abs/2511.12291)
*Andrea Bertogalli,Giacomo Boracchi,Luca Magri*

Main category: cs.CV

TL;DR: The paper introduces a novel method for simultaneously calibrating event cameras, LiDARs, and RGB cameras using a unique 3D calibration target.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges in calibrating event cameras and establish accurate multi-modal sensor alignment in autonomous driving.

Method: They propose a novel calibration target with features tailored for each sensor type, enabling a one-shot calibration process.

Result: The method is validated on a custom dataset, demonstrating strong accuracy and robustness in the calibration results.

Conclusion: This novel framework simplifies and improves the calibration process for complex vision systems in autonomous driving applications.

Abstract: We present a novel multi-modal extrinsic calibration framework designed to simultaneously estimate the relative poses between event cameras, LiDARs, and RGB cameras, with particular focus on the challenging event camera calibration. Core of our approach is a novel 3D calibration target, specifically designed and constructed to be concurrently perceived by all three sensing modalities. The target encodes features in planes, ChArUco, and active LED patterns, each tailored to the unique characteristics of LiDARs, RGB cameras, and event cameras respectively. This unique design enables a one-shot, joint extrinsic calibration process, in contrast to existing approaches that typically rely on separate, pairwise calibrations. Our calibration pipeline is designed to accurately calibrate complex vision systems in the context of autonomous driving, where precise multi-sensor alignment is critical. We validate our approach through an extensive experimental evaluation on a custom built dataset, recorded with an advanced autonomous driving sensor setup, confirming the accuracy and robustness of our method.

</details>


### [283] [Rethinking Bias in Generative Data Augmentation for Medical AI: a Frequency Recalibration Method](https://arxiv.org/abs/2511.12301)
*Chi Liu,Jincheng Liu,Congcong Zhu,Minghao Wang,Sheng Shen,Jia Gu,Tianqing Zhu,Wanlei Zhou*

Main category: cs.CV

TL;DR: This paper addresses biases in generative data augmentation (GDA) for medical AI, proposing a Frequency Recalibration (FreRec) method to align frequency discrepancies and improve training using AI-generated images.


<details>
  <summary>Details</summary>
Motivation: Large medical datasets are scarce, impairing the development of Medical AI. AI generative models can synthesize data, but biases in Generative Data Augmentation (GDA) might degrade downstream medical tasks.

Method: Frequency Recalibration (FreRec) corrects frequency mismatches between real and synthesized images via two techniques: (1) Statistical High-frequency Replacement (SHR) aligns high-frequency components approximately, and (2) Reconstructive High-frequency Mapping (RHM) enhances quality and reconstructs fine high-frequency details.

Result: Extensive tests on varied medical datasets (e.g., brain MRIs, chest X-rays, fundus images) demonstrate that FreRec improves classification performance compared to uncalibrated synthesized data.

Conclusion: FreRec reduces frequency discrepancies in AI-generated datasets, enhancing reliability and making it a versatile, compatible step in medical GDA workflows.

Abstract: Developing Medical AI relies on large datasets and easily suffers from data scarcity. Generative data augmentation (GDA) using AI generative models offers a solution to synthesize realistic medical images. However, the bias in GDA is often underestimated in medical domains, with concerns about the risk of introducing detrimental features generated by AI and harming downstream tasks. This paper identifies the frequency misalignment between real and synthesized images as one of the key factors underlying unreliable GDA and proposes the Frequency Recalibration (FreRec) method to reduce the frequency distributional discrepancy and thus improve GDA. FreRec involves (1) Statistical High-frequency Replacement (SHR) to roughly align high-frequency components and (2) Reconstructive High-frequency Mapping (RHM) to enhance image quality and reconstruct high-frequency details. Extensive experiments were conducted in various medical datasets, including brain MRIs, chest X-rays, and fundus images. The results show that FreRec significantly improves downstream medical image classification performance compared to uncalibrated AI-synthesized samples. FreRec is a standalone post-processing step that is compatible with any generative model and can integrate seamlessly with common medical GDA pipelines.

</details>


### [284] [LiDAR-GS++:Improving LiDAR Gaussian Reconstruction via Diffusion Priors](https://arxiv.org/abs/2511.12304)
*Qifeng Chen,Jiarun Liu,Rengan Xie,Tao Tang,Sicong Du,Yiru Zhao,Yuchi Huo,Sheng Yang*

Main category: cs.CV

TL;DR: LiDAR-GS++ enhances LiDAR-based rendering by using diffusion priors to improve extrapolated novel view synthesis and achieves state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To overcome the artifacts present in novel view synthesis from single traversal LiDAR scans, enabling global geometric consistency and fine detail preservation.

Method: LiDAR-GS++ incorporates a controllable LiDAR generation model using diffusion priors to produce extra geometry-consistent data and a distillation mechanism to extend reconstruction to under-fitted regions.

Result: State-of-the-art performance achieved in rendering interpolated and extrapolated viewpoints, surpassing existing methods like GS and NeRF.

Conclusion: LiDAR-GS++ ensures real-time and high-fidelity LiDAR re-simulation while addressing geometric artifacts, setting new benchmarks in urban road scene rendering.

Abstract: Recent GS-based rendering has made significant progress for LiDAR, surpassing Neural Radiance Fields (NeRF) in both quality and speed. However, these methods exhibit artifacts in extrapolated novel view synthesis due to the incomplete reconstruction from single traversal scans. To address this limitation, we present LiDAR-GS++, a LiDAR Gaussian Splatting reconstruction method enhanced by diffusion priors for real-time and high-fidelity re-simulation on public urban roads. Specifically, we introduce a controllable LiDAR generation model conditioned on coarsely extrapolated rendering to produce extra geometry-consistent scans and employ an effective distillation mechanism for expansive reconstruction. By extending reconstruction to under-fitted regions, our approach ensures global geometric consistency for extrapolative novel views while preserving detailed scene surfaces captured by sensors. Experiments on multiple public datasets demonstrate that LiDAR-GS++ achieves state-of-the-art performance for both interpolated and extrapolated viewpoints, surpassing existing GS and NeRF-based methods.

</details>


### [285] [Learning Time in Static Classifiers](https://arxiv.org/abs/2511.12321)
*Xi Ding,Lei Wang,Piotr Koniusz,Yongsheng Gao*

Main category: cs.CV

TL;DR: The paper presents a framework for incorporating temporal reasoning into static classifiers without architectural modifications, leveraging a novel SEQ learning paradigm and a soft-DTW loss for effective temporal tasks.


<details>
  <summary>Details</summary>
Motivation: Current classifiers assume temporal independence, limiting their ability to capture dynamic visual data that typically evolves over time.

Method: The proposed SEQ learning paradigm structures training data into temporally coherent trajectories and uses a differentiable soft-DTW loss to align prediction sequences. A multi-term objective enhances semantic and temporal consistency.

Result: The framework improves performance in both static tasks like fine-grained classification and temporal tasks like video anomaly detection, delivering consistent and precise predictions.

Conclusion: The approach effectively bridges static and temporal learning by introducing a strong temporal bias via loss design, requiring minimal adjustments to existing classifier models.

Abstract: Real-world visual data rarely presents as isolated, static instances. Instead, it often evolves gradually over time through variations in pose, lighting, object state, or scene context. However, conventional classifiers are typically trained under the assumption of temporal independence, limiting their ability to capture such dynamics. We propose a simple yet effective framework that equips standard feedforward classifiers with temporal reasoning, all without modifying model architectures or introducing recurrent modules. At the heart of our approach is a novel Support-Exemplar-Query (SEQ) learning paradigm, which structures training data into temporally coherent trajectories. These trajectories enable the model to learn class-specific temporal prototypes and align prediction sequences via a differentiable soft-DTW loss. A multi-term objective further promotes semantic consistency and temporal smoothness. By interpreting input sequences as evolving feature trajectories, our method introduces a strong temporal inductive bias through loss design alone. This proves highly effective in both static and temporal tasks: it enhances performance on fine-grained and ultra-fine-grained image classification, and delivers precise, temporally consistent predictions in video anomaly detection. Despite its simplicity, our approach bridges static and temporal learning in a modular and data-efficient manner, requiring only a simple classifier on top of pre-extracted features.

</details>


### [286] [SpaceVLM: Sub-Space Modeling of Negation in Vision-Language Models](https://arxiv.org/abs/2511.12331)
*Sepehr Kazemi Ranjbar,Kumail Alhamoud,Marzyeh Ghassemi*

Main category: cs.CV

TL;DR: This paper introduces a novel training-free approach to improve Vision-Language Models' (VLMs) understanding of negation, maintaining zero-shot performance while enhancing negation handling.


<details>
  <summary>Details</summary>
Motivation: Vision-Language Models historically struggle with negation, and current solutions involving fine-tuning harm zero-shot capabilities on affirmative prompts.

Method: The authors divided the embedding spaces of VLMs into semantically consistent subspaces to represent negation as a subspace. They employed spherical caps around embeddings to score images for captions with negation (e.g., 'A but not N').

Result: The proposed method demonstrates a 30% improvement in negation understanding across tasks, while preserving zero-shot performance compared to fine-tuned models.

Conclusion: The framework proves effective in resolving negation limitations in VLMs without compromising affirmative prompt performance. Code will be released publicly upon publication.

Abstract: Vision-Language Models (VLMs) struggle with negation. Given a prompt like "retrieve (or generate) a street scene without pedestrians," they often fail to respect the "not." Existing methods address this limitation by fine-tuning on large negation datasets, but such retraining often compromises the model's zero-shot performance on affirmative prompts. We show that the embedding space of VLMs, such as CLIP, can be divided into semantically consistent subspaces. Based on this property, we propose a training-free framework that models negation as a subspace in the joint embedding space rather than a single point (Figure 1). To find the matching image for a caption such as "A but not N," we construct two spherical caps around the embeddings of A and N, and we score images by the central direction of the region that is close to A and far from N. Across retrieval, MCQ, and text-to-image tasks, our method improves negation understanding by about 30% on average over prior methods. It closes the gap between affirmative and negated prompts while preserving the zero-shot performance that fine-tuned models fail to maintain. Code will be released upon publication.

</details>


### [287] [Ground Plane Projection for Improved Traffic Analytics at Intersections](https://arxiv.org/abs/2511.12342)
*Sajjad Pakdamansavoji,Kumar Vaibhav Jha,Baher Abdulhai,James H Elder*

Main category: cs.CV

TL;DR: The paper explores a 3D ground-plane approach for analyzing traffic flow at intersections and demonstrates better trajectory and turning movement accuracy than image-plane methods.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy of traffic movement counts and trajectory classification for traffic signal control, urban planning, and management.

Method: The study uses vehicle detection back-projected to a real-world 3D ground plane, employing both single and multi-camera systems for analysis.

Result: Results show that back-projection in 3D provides better trajectory and movement count accuracy compared to traditional image-plane analysis, with multi-camera systems offering further improvement.

Conclusion: Traffic analysis should prioritize 3D ground-plane methods over image-plane methods to enhance accuracy in traffic control applications.

Abstract: Accurate turning movement counts at intersections are important for signal control, traffic management and urban planning. Computer vision systems for automatic turning movement counts typically rely on visual analysis in the image plane of an infrastructure camera. Here we explore potential advantages of back-projecting vehicles detected in one or more infrastructure cameras to the ground plane for analysis in real-world 3D coordinates. For single-camera systems we find that back-projection yields more accurate trajectory classification and turning movement counts. We further show that even higher accuracy can be achieved through weak fusion of back-projected detections from multiple cameras. These results suggeest that traffic should be analyzed on the ground plane, not the image plane

</details>


### [288] [CLAReSNet: When Convolution Meets Latent Attention for Hyperspectral Image Classification](https://arxiv.org/abs/2511.12346)
*Asmit Bandyopadhyay,Anindita Das Bhattacharjee,Rakesh Das*

Main category: cs.CV

TL;DR: The paper introduces CLAReSNet, a hybrid HSI classification model combining convolutional and transformer methods, significantly improving accuracy on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: HSI classification faces challenges like high spectral dimensionality, complex correlations, and limited imbalanced training samples, requiring innovative approaches.

Method: CLAReSNet integrates multi-scale convolutional extraction, enhanced attention mechanisms, and adaptive spectral encoding for efficient feature representation and classification.

Result: CLAReSNet achieved state-of-the-art accuracy (99.71% on Indian Pines and 99.96% on Salinas datasets), far surpassing existing models in performance.

Conclusion: CLAReSNet effectively addresses HSI challenges, leveraging hybrid methods and dynamic attention mechanisms to provide robust classification under constraints.

Abstract: Hyperspectral image (HSI) classification faces critical challenges, including high spectral dimensionality, complex spectral-spatial correlations, and limited training samples with severe class imbalance. While CNNs excel at local feature extraction and transformers capture long-range dependencies, their isolated application yields suboptimal results due to quadratic complexity and insufficient inductive biases. We propose CLAReSNet (Convolutional Latent Attention Residual Spectral Network), a hybrid architecture that integrates multi-scale convolutional extraction with transformer-style attention via an adaptive latent bottleneck. The model employs a multi-scale convolutional stem with deep residual blocks and an enhanced Convolutional Block Attention Module for hierarchical spatial features, followed by spectral encoder layers combining bidirectional RNNs (LSTM/GRU) with Multi-Scale Spectral Latent Attention (MSLA). MSLA reduces complexity from $\mathcal{O}(T^2D)$ to $\mathcal{O}(T\log(T)D)$ by adaptive latent token allocation (8-64 tokens) that scales logarithmically with the sequence length. Hierarchical cross-attention fusion dynamically aggregates multi-level representations for robust classification. Experiments conducted on the Indian Pines and Salinas datasets show state-of-the-art performance, achieving overall accuracies of 99.71% and 99.96%, significantly surpassing HybridSN, SSRN, and SpectralFormer. The learned embeddings exhibit superior inter-class separability and compact intra-class clustering, validating CLAReSNet's effectiveness under limited samples and severe class imbalance.

</details>


### [289] [Explainable AI-Generated Image Detection RewardBench](https://arxiv.org/abs/2511.12363)
*Michael Yang,Shijian Deng,William T. Doan,Kai Wang,Tianyu Yang,Harsh Singh,Yapeng Tian*

Main category: cs.CV

TL;DR: The paper introduces a benchmark called 'XAIGID-RewardBench' for assessing the ability of Multimodal Large Language Models (MLLMs) to evaluate explanations regarding AI-generated image detection.


<details>
  <summary>Details</summary>
Motivation: To address the lack of trustworthiness and persuasiveness in conventional AI-generated image detection methods that fail to explain their reasoning effectively.

Method: The study designs a benchmark with 3,000 annotated triplets derived from various image generation models and MLLMs, evaluating the capability of MLLMs as judges for explanation quality.

Result: The best current reward model achieved a score of 88.76%, significantly lower than human inter-annotator agreement of 98.30%, highlighting a gap in reasoning capabilities between MLLMs and humans.

Conclusion: While MLLMs show potential, their ability to judge explanation quality for AI-generated images lags behind human-level reasoning, indicating room for improvement and providing analysis of common pitfalls.

Abstract: Conventional, classification-based AI-generated image detection methods cannot explain why an image is considered real or AI-generated in a way a human expert would, which reduces the trustworthiness and persuasiveness of these detection tools for real-world applications. Leveraging Multimodal Large Language Models (MLLMs) has recently become a trending solution to this issue. Further, to evaluate the quality of generated explanations, a common approach is to adopt an "MLLM as a judge" methodology to evaluate explanations generated by other MLLMs. However, how well those MLLMs perform when judging explanations for AI-generated image detection generated by themselves or other MLLMs has not been well studied. We therefore propose \textbf{XAIGID-RewardBench}, the first benchmark designed to evaluate the ability of current MLLMs to judge the quality of explanations about whether an image is real or AI-generated. The benchmark consists of approximately 3,000 annotated triplets sourced from various image generation models and MLLMs as policy models (detectors) to assess the capabilities of current MLLMs as reward models (judges). Our results show that the current best reward model scored 88.76\% on this benchmark (while human inter-annotator agreement reaches 98.30\%), demonstrating that a visible gap remains between the reasoning abilities of today's MLLMs and human-level performance. In addition, we provide an analysis of common pitfalls that these models frequently encounter. Code and benchmark are available at https://github.com/RewardBench/XAIGID-RewardBench.

</details>


### [290] [Constructing and Interpreting Digital Twin Representations for Visual Reasoning via Reinforcement Learning](https://arxiv.org/abs/2511.12365)
*Yiqing Shen,Mathias Unberath*

Main category: cs.CV

TL;DR: DT-R1 introduces a unified visual reasoning framework utilizing reinforcement learning with digital twin representations, producing consistent improvements over specialized models.


<details>
  <summary>Details</summary>
Motivation: Current visual reasoning models lack unified architectures and have limited cross-task and cross-modality generalization.

Method: DT-R1 trains large language models with digital twin representations of multi-modal visual inputs using GRPO and a novel reward system for structural and output accuracy.

Result: DT-R1 consistently improves performance across six benchmarks in two modalities and four task types over state-of-the-art models.

Conclusion: DT-R1 provides a unified solution for visual reasoning and highlights the potential of reinforcement learning with digital twin representations.

Abstract: Visual reasoning may require models to interpret images and videos and respond to implicit text queries across diverse output formats, from pixel-level segmentation masks to natural language descriptions. Existing approaches rely on supervised fine-tuning with task-specific architectures. For example, reasoning segmentation, grounding, summarization, and visual question answering each demand distinct model designs and training, preventing unified solutions and limiting cross-task and cross-modality generalization. Hence, we propose DT-R1, a reinforcement learning framework that trains large language models to construct digital twin representations of complex multi-modal visual inputs and then reason over these high-level representations as a unified approach to visual reasoning. Specifically, we train DT-R1 using GRPO with a novel reward that validates both structural integrity and output accuracy. Evaluations in six visual reasoning benchmarks, covering two modalities and four task types, demonstrate that DT-R1 consistently achieves improvements over state-of-the-art task-specific models. DT-R1 opens a new direction where visual reasoning emerges from reinforcement learning with digital twin representations.

</details>


### [291] [Fast Reasoning Segmentation for Images and Videos](https://arxiv.org/abs/2511.12368)
*Yiqing Shen,Mathias Unberath*

Main category: cs.CV

TL;DR: FastReasonSeg enables efficient reasoning segmentation by leveraging digital twin representations and a novel distillation method, offering an efficient solution for resource-constrained environments.


<details>
  <summary>Details</summary>
Motivation: Current reasoning segmentation models require large multimodal language models, which are computationally infeasible for edge devices used in embodied AI systems.

Method: FastReasonSeg uses a digital twin representation to decouple perception from reasoning. Its distillation process involves supervised fine-tuning with teacher-generated reasoning chains and reinforcement fine-tuning that evaluates segmentation accuracy and reasoning quality.

Result: FastReasonSeg achieves state-of-the-art performance in reasoning segmentation on multiple benchmarks. The 0.6B parameter model outperforms models 20 times larger while being highly memory and throughput efficient.

Conclusion: FastReasonSeg offers a major advancement in reasoning segmentation by drastically improving both performance and resource efficiency, making it suitable for real-world applications.

Abstract: Reasoning segmentation enables open-set object segmentation via implicit text queries, therefore serving as a foundation for embodied agents that should operate autonomously in real-world environments. However, existing methods for reasoning segmentation require multimodal large language models with billions of parameters that exceed the computational capabilities of edge devices that typically deploy the embodied AI systems. Distillation offers a pathway to compress these models while preserving their capabilities. Yet, existing distillation approaches fail to transfer the multi-step reasoning capabilities that reasoning segmentation demands, as they focus on matching output predictions and intermediate features rather than preserving reasoning chains. The emerging paradigm of reasoning over digital twin representations presents an opportunity for more effective distillation by re-framing the problem. Consequently, we propose FastReasonSeg, which employs digital twin representations that decouple perception from reasoning to enable more effective distillation. Our distillation scheme first relies on supervised fine-tuning on teacher-generated reasoning chains. Then it is followed by reinforcement fine-tuning with joint rewards evaluating both segmentation accuracy and reasoning quality alignment. Experiments on two video (JiTBench, RVTBench) and two image benchmarks (ReasonSeg, LLM-Seg40K) demonstrate that our FastReasonSeg achieves state-of-the-art reasoning segmentation performance. Moreover, the distilled 0.6B variant outperforms models with 20 times more parameters while achieving 7.79 FPS throughput with only 2.1GB memory consumption. This efficiency enables deployment in resource-constrained environments to enable real-time reasoning segmentation.

</details>


### [292] [Changes in Real Time: Online Scene Change Detection with Multi-View Fusion](https://arxiv.org/abs/2511.12370)
*Chamuditha Jayanga Galappaththige,Jason Lai,Lloyd Windrim,Donald Dansereau,Niko Sünderhauf,Dimity Miller*

Main category: cs.CV

TL;DR: This paper introduces a novel online Scene Change Detection (SCD) method that achieves state-of-the-art accuracy and speed while being pose-agnostic and label-free.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of accurately detecting scene changes online with high performance, overcoming the limitations of current online methods compared to offline counterparts.

Method: Introduced a self-supervised fusion loss for multi-cue integration, PnP-based fast pose estimation, and a change-guided update strategy for 3D Gaussian Splatting.

Result: Achieved over 10 FPS, surpassed the accuracy of existing offline and online methods, and demonstrated superior performance in experiments on real-world datasets.

Conclusion: Proposed a groundbreaking online SCD method that is efficient, label-free, pose-agnostic, and establishes new benchmarks in the field.

Abstract: Online Scene Change Detection (SCD) is an extremely challenging problem that requires an agent to detect relevant changes on the fly while observing the scene from unconstrained viewpoints. Existing online SCD methods are significantly less accurate than offline approaches. We present the first online SCD approach that is pose-agnostic, label-free, and ensures multi-view consistency, while operating at over 10 FPS and achieving new state-of-the-art performance, surpassing even the best offline approaches. Our method introduces a new self-supervised fusion loss to infer scene changes from multiple cues and observations, PnP-based fast pose estimation against the reference scene, and a fast change-guided update strategy for the 3D Gaussian Splatting scene representation. Extensive experiments on complex real-world datasets demonstrate that our approach outperforms both online and offline baselines.

</details>


### [293] [Reasoning Text-to-Video Retrieval via Digital Twin Video Representations and Large Language Models](https://arxiv.org/abs/2511.12371)
*Yiqing Shen,Chenxiao Fan,Chenjia Li,Mathias Unberath*

Main category: cs.CV

TL;DR: This paper introduces a new paradigm for text-to-video retrieval that accounts for reasoning with implicit queries and achieves state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Current text-to-video retrieval methods struggle with implicit queries requiring reasoning to identify relevant videos.

Method: The method represents video content as "digital twins" for structured scene representation, enabling reasoning over long-horizon video content. A two-stage framework combines compositional alignment with reasoning through large language models.

Result: The method achieves a significant 81.2% R@1 performance on ReasonT2VBench-135, outperforming existing approaches by over 50 percentage points and excels on other conventional benchmarks such as MSR-VTT, MSVD, and VATEX.

Conclusion: This approach advances text-to-video retrieval by enabling reasoning over implicit queries and establishes state-of-the-art performance across multiple benchmarks.

Abstract: The goal of text-to-video retrieval is to search large databases for relevant videos based on text queries. Existing methods have progressed to handling explicit queries where the visual content of interest is described explicitly; however, they fail with implicit queries where identifying videos relevant to the query requires reasoning. We introduce reasoning text-to-video retrieval, a paradigm that extends traditional retrieval to process implicit queries through reasoning while providing object-level grounding masks that identify which entities satisfy the query conditions. Instead of relying on vision-language models directly, we propose representing video content as digital twins, i.e., structured scene representations that decompose salient objects through specialist vision models. This approach is beneficial because it enables large language models to reason directly over long-horizon video content without visual token compression. Specifically, our two-stage framework first performs compositional alignment between decomposed sub-queries and digital twin representations for candidate identification, then applies large language model-based reasoning with just-in-time refinement that invokes additional specialist models to address information gaps. We construct a benchmark of 447 manually created implicit queries with 135 videos (ReasonT2VBench-135) and another more challenging version of 1000 videos (ReasonT2VBench-1000). Our method achieves 81.2% R@1 on ReasonT2VBench-135, outperforming the strongest baseline by greater than 50 percentage points, and maintains 81.7% R@1 on the extended configuration while establishing state-of-the-art results in three conventional benchmarks (MSR-VTT, MSVD, and VATEX).

</details>


### [294] [AGGRNet: Selective Feature Extraction and Aggregation for Enhanced Medical Image Classification](https://arxiv.org/abs/2511.12382)
*Ansh Makwe,Akansh Agrawal,Prateek Jain,Akshan Agrawal,Priyanka Bagade*

Main category: cs.CV

TL;DR: The paper introduces AGGRNet, a framework for enhancing medical image classification by addressing limitations in handling subtle visual patterns, achieving up to 5% performance improvement over state-of-the-art (SOTA) models.


<details>
  <summary>Details</summary>
Motivation: Medical image analysis faces challenges such as intricate visual patterns, data scarcity, and variability in expert interpretations, with existing models struggling to differentiate subtle inter-class similarities and intra-class variabilities.

Method: The AGGRNet framework is proposed, designed to extract both informative and non-informative features to better understand fine-grained visual patterns for improved classification in medical imaging.

Result: Experimental results indicate that AGGRNet outperforms existing models, achieving state-of-the-art performance, with up to 5% improvement on the Kvasir dataset.

Conclusion: The proposed AGGRNet framework effectively addresses classification challenges in medical imaging, enhancing accuracy and advancing the field of complex medical image analysis.

Abstract: Medical image analysis for complex tasks such as severity grading and disease subtype classification poses significant challenges due to intricate and similar visual patterns among classes, scarcity of labeled data, and variability in expert interpretations. Despite the usefulness of existing attention-based models in capturing complex visual patterns for medical image classification, underlying architectures often face challenges in effectively distinguishing subtle classes since they struggle to capture inter-class similarity and intra-class variability, resulting in incorrect diagnosis. To address this, we propose AGGRNet framework to extract informative and non-informative features to effectively understand fine-grained visual patterns and improve classification for complex medical image analysis tasks. Experimental results show that our model achieves state-of-the-art performance on various medical imaging datasets, with the best improvement up to 5% over SOTA models on the Kvasir dataset.

</details>


### [295] [Leveraging Quantum-Based Architectures for Robust Diagnostics](https://arxiv.org/abs/2511.12386)
*Shabnam Sodagari,Tommy Long*

Main category: cs.CV

TL;DR: This study developed a hybrid quantum-classical framework to diagnose kidney abnormalities, achieving notable accuracy in classifying stones, cysts, and tumors using CT images.


<details>
  <summary>Details</summary>
Motivation: To improve the diagnosis and differentiation of kidney stones, cysts, and tumors effectively using quantum-assisted processing.

Method: The authors used a combined quantum-classical approach involving a ResNet50 encoder, Quantum Convolutional Neural Network (QCNN), image pre-processing, and techniques addressing data imbalance.

Result: The framework achieved a high test accuracy of 0.99, improved recall and precision, and demonstrated reliable classifications with minimal misclassifications.

Conclusion: Hybrid quantum-classical models effectively enhance diagnostic performance for kidney images, showing promise in clinical applications.

Abstract: The objective of this study is to diagnose and differentiate kidney stones, cysts, and tumors using Computed Tomography (CT) images of the kidney. This study leverages a hybrid quantum-classical framework in this regard. We combine a pretrained ResNet50 encoder, with a Quantum Convolutional Neural Network (QCNN) to explore quantum-assisted diagnosis. We pre-process the kidney images using denoising and contrast limited adaptive histogram equalization to enhance feature extraction. We address class imbalance through data augmentation and weighted sampling. Latent features extracted by the encoder are transformed into qubits via angle encoding and processed by a QCNN. The model is evaluated on both 8-qubit and 12-qubit configurations. Both architectures achieved rapid convergence with stable learning curves and high consistency between training and validation performance. The models reached a test accuracy of 0.99, with the 12-qubit configuration providing improvements in overall recall and precision, particularly for Cyst and Tumor detection, where it achieved perfect recall for Cysts and a tumor F1-score of 0.9956. Confusion matrix analysis further confirmed reliable classification behavior across all classes, with very few misclassifications. Results demonstrate that integrating classical pre-processing and deep feature extraction with quantum circuits enhances medical diagnostic performance.

</details>


### [296] [MSLoRA: Multi-Scale Low-Rank Adaptation via Attention Reweighting](https://arxiv.org/abs/2511.12400)
*Xu Yang,Gady Agam*

Main category: cs.CV

TL;DR: MSLoRA is a universal, lightweight adapter for CNNs and ViTs that efficiently improves performance by reweighting feature responses without modifying backbone parameters.


<details>
  <summary>Details</summary>
Motivation: Existing low-rank adaptation methods face limitations in working effectively across architectures like CNNs and ViTs. There is a need for a unified and efficient framework for vision model adaptation.

Method: MSLoRA combines a low-rank linear projection with multi-scale nonlinear transformations to modulate spatial and channel attention. The method fuses these components via pointwise multiplication and residual connections, ensuring frozen pretrained weights.

Result: Experiments show MSLoRA improves transfer performance across various tasks (classification, detection, segmentation), requiring less than 5% of backbone parameters while offering stable optimization, fast convergence, and broad generalization.

Conclusion: MSLoRA introduces a universal and parameter-efficient solution for vision model adaptation, reweighting feature responses without backbone modification, suitable for multiple architectures.

Abstract: We introduce MSLoRA, a backbone-agnostic, parameter-efficient adapter that reweights feature responses rather
  than re-tuning the underlying backbone. Existing low-rank adaptation methods are mostly confined to vision
  transformers (ViTs) and struggle to generalize across architectures. MSLoRA unifies adaptation for both convolutional neural networks (CNNs) and
  ViTs by combining a low-rank linear projection with a multi-scale nonlinear transformation that jointly
  modulates spatial and channel attention. The two components are fused through pointwise multiplication and
  a residual connection, yielding a lightweight module that shifts feature attention while keeping pretrained
  weights frozen.
  Extensive experiments demonstrate that MSLoRA consistently improves transfer performance on classification,
  detection, and segmentation tasks with roughly less than 5\% of backbone parameters.
  The design further enables stable optimization, fast convergence, and strong cross-architecture
  generalization. By reweighting rather than re-tuning, MSLoRA provides a simple and universal approach
  for efficient adaptation of frozen vision backbones.

</details>


### [297] [VLA-R: Vision-Language Action Retrieval toward Open-World End-to-End Autonomous Driving](https://arxiv.org/abs/2511.12405)
*Hyunki Seong,Seongwoo Moon,Hojin Ahn,Jehun Kang,David Hyunchul Shim*

Main category: cs.CV

TL;DR: Exploring open-world end-to-end autonomous driving in unstructured environments using a novel Vision-Language Action Retrieval (VLA-R) framework.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of generalizing autonomous driving systems to unstructured outdoor environments unfamiliar during training.

Method: The paper utilizes a vision-language model for perception integrated with vision-action retrieval. A Q-Former bottleneck bridges visual and language features, while a vision-action contrastive learning mechanism fosters transferable driving behaviors.

Result: Experiments show strong generalization and exploratory performance of the framework on a robotic platform in unseen and unstructured environments, using limited training data.

Conclusion: The framework effectively extends autonomous driving capabilities into open-world scenarios by fusing perception and action domains with strong generalization.

Abstract: Exploring open-world situations in an end-to-end manner is a promising yet challenging task due to the need for strong generalization capabilities. In particular, end-to-end autonomous driving in unstructured outdoor environments often encounters conditions that were unfamiliar during training. In this work, we present Vision-Language Action Retrieval (VLA-R), an open-world end-to-end autonomous driving (OW-E2EAD) framework that integrates open-world perception with a novel vision-action retrieval paradigm. We leverage a frozen vision-language model for open-world detection and segmentation to obtain multi-scale, prompt-guided, and interpretable perception features without domain-specific tuning. A Q-Former bottleneck aggregates fine-grained visual representations with language-aligned visual features, bridging perception and action domains. To learn transferable driving behaviors, we introduce a vision-action contrastive learning scheme that aligns vision-language and action embeddings for effective open-world reasoning and action retrieval. Our experiments on a real-world robotic platform demonstrate strong generalization and exploratory performance in unstructured, unseen environments, even with limited data. Demo videos are provided in the supplementary material.

</details>


### [298] [Self-Supervised Visual Prompting for Cross-Domain Road Damage Detection](https://arxiv.org/abs/2511.12410)
*Xi Xiao,Zhuxuanzi Wang,Mingqiao Mo,Chen Liu,Chenrui Ma,Yanshu Li,Smita Krishnaswamy,Xiao Wang,Tianyang Wang*

Main category: cs.CV

TL;DR: This paper introduces a self-supervised framework, ours, to enhance domain generalization for automated pavement defect detection, utilizing visual probing techniques.


<details>
  <summary>Details</summary>
Motivation: The need for scalable and adaptive visual inspection systems in pavement defect detection, due to the limitations of supervised methods and vulnerability of generic features to domain shifts.

Method: The paper proposes ours, incorporating a Self-supervised Prompt Enhancement Module (SPEM) to derive defect-aware prompts and a Domain-Aware Prompt Alignment (DAPA) objective to align source and target representations.

Result: Experiments demonstrate ours outperforms other approaches, ensuring robust zero-shot transfer, resilience to domain variations, and efficient few-shot adaptation.

Conclusion: Self-supervised prompting is a promising approach for scalable visual inspection systems, addressing cross-domain generalization without relying on labeled data. Source code is available for reproducibility.

Abstract: The deployment of automated pavement defect detection is often hindered by poor cross-domain generalization. Supervised detectors achieve strong in-domain accuracy but require costly re-annotation for new environments, while standard self-supervised methods capture generic features and remain vulnerable to domain shift. We propose \ours, a self-supervised framework that \emph{visually probes} target domains without labels. \ours introduces a Self-supervised Prompt Enhancement Module (SPEM), which derives defect-aware prompts from unlabeled target data to guide a frozen ViT backbone, and a Domain-Aware Prompt Alignment (DAPA) objective, which aligns prompt-conditioned source and target representations. Experiments on four challenging benchmarks show that \ours consistently outperforms strong supervised, self-supervised, and adaptation baselines, achieving robust zero-shot transfer, improved resilience to domain variations, and high data efficiency in few-shot adaptation. These results highlight self-supervised prompting as a practical direction for building scalable and adaptive visual inspection systems. Source code is publicly available: https://github.com/xixiaouab/PROBE/tree/main

</details>


### [299] [DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions](https://arxiv.org/abs/2511.12452)
*Xiaoyu Lin,Aniket Ghorpade,Hansheng Zhu,Justin Qiu,Dea Rrozhani,Monica Lama,Mick Yang,Zixuan Bian,Ruohan Ren,Alan B. Hong,Jiatao Gu,Chris Callison-Burch*

Main category: cs.CV

TL;DR: The paper introduces DenseAnnotate, an audio-driven platform for creating dense annotations for images and 3D assets, leading to better-trained models in multilingual, cultural, and 3D spatial tasks.


<details>
  <summary>Details</summary>
Motivation: Many existing datasets offer sparse annotations, which fail to capture the full visual content of images or 3D scenes. Traditional text-based annotation methods limit expressiveness and are inadequate for nuanced visual features. The motivation is to overcome these limitations and provide high-quality, dense annotations efficiently.

Method: The authors propose DenseAnnotate, an online annotation platform where annotators use narration to describe visuals and link their spoken phrases to specific regions of images or 3D scenes. It incorporates speech-to-text transcription and region-of-attention mapping.

Result: Using DenseAnnotate, a dataset was created with over 3,531 images, 898 3D scenes, and 7,460 3D objects, producing annotations in 20 languages. Models trained on this data showed significant improvements: 5% better multilingual validation, 47% better cultural alignment, and 54% enhanced 3D spatial understanding.

Conclusion: DenseAnnotate provides an effective way to achieve dense annotations, benefitting vision-language research and diverse applications. It demonstrates scalability and flexibility for multilingual and multimodal datasets.

Abstract: With the rapid adoption of multimodal large language models (MLLMs) across diverse applications, there is a pressing need for task-centered, high-quality training data. A key limitation of current training datasets is their reliance on sparse annotations mined from the Internet or entered via manual typing that capture only a fraction of an image's visual content. Dense annotations are more valuable but remain scarce. Traditional text-based annotation pipelines are poorly suited for creating dense annotations: typing limits expressiveness, slows annotation speed, and underrepresents nuanced visual features, especially in specialized areas such as multicultural imagery and 3D asset annotation. In this paper, we present DenseAnnotate, an audio-driven online annotation platform that enables efficient creation of dense, fine-grained annotations for images and 3D assets. Annotators narrate observations aloud while synchronously linking spoken phrases to image regions or 3D scene parts. Our platform incorporates speech-to-text transcription and region-of-attention marking. To demonstrate the effectiveness of DenseAnnotate, we conducted case studies involving over 1,000 annotators across two domains: culturally diverse images and 3D scenes. We curate a human-annotated multi-modal dataset of 3,531 images, 898 3D scenes, and 7,460 3D objects, with audio-aligned dense annotations in 20 languages, including 8,746 image captions, 2,000 scene captions, and 19,000 object captions. Models trained on this dataset exhibit improvements of 5% in multilingual, 47% in cultural alignment, and 54% in 3D spatial capabilities. Our results show that our platform offers a feasible approach for future vision-language research and can be applied to various tasks and diverse types of data.

</details>


### [300] [Towards Rotation-only Imaging Geometry: Rotation Estimation](https://arxiv.org/abs/2511.12415)
*Xinrui Li,Qi Cai,Yuanxin Wu*

Main category: cs.CV

TL;DR: This paper introduces a rotation-only optimization framework to improve Structure from Motion (SfM) accuracy using the relationship between rotation and translation.


<details>
  <summary>Details</summary>
Motivation: The paper aims to enhance SfM performance by simplifying imaging geometry representation, leveraging the connection between scene structures, rotation, and translation.

Method: A rotation-only optimization framework based on reprojection error is proposed for both two-view and multi-view scenarios.

Result: The method demonstrates superior accuracy and robustness, outperforming state-of-the-art rotation estimation methods and matching results from multiple bundle adjustment iterations.

Conclusion: The proposed framework contributes to more accurate, efficient, and reliable 3D visual computing, offering improvements to SfM techniques.

Abstract: Structure from Motion (SfM) is a critical task in computer vision, aiming to recover the 3D scene structure and camera motion from a sequence of 2D images. The recent pose-only imaging geometry decouples 3D coordinates from camera poses and demonstrates significantly better SfM performance through pose adjustment. Continuing the pose-only perspective, this paper explores the critical relationship between the scene structures, rotation and translation. Notably, the translation can be expressed in terms of rotation, allowing us to condense the imaging geometry representation onto the rotation manifold. A rotation-only optimization framework based on reprojection error is proposed for both two-view and multi-view scenarios. The experiment results demonstrate superior accuracy and robustness performance over the current state-of-the-art rotation estimation methods, even comparable to multiple bundle adjustment iteration results. Hopefully, this work contributes to even more accurate, efficient and reliable 3D visual computing.

</details>


### [301] [Co-Layout: LLM-driven Co-optimization for Interior Layout](https://arxiv.org/abs/2511.12474)
*Chucheng Xiang,Ruchao Bao,Biyin Feng,Wenzheng Wu,Zhongyuan Liu,Yirui Guan,Ligang Liu*

Main category: cs.CV

TL;DR: The paper introduces a framework combining large language models and grid-based integer programming to optimize room layouts and furniture placement efficiently.


<details>
  <summary>Details</summary>
Motivation: The research aims to address inefficiencies and limitations in existing automated interior design processes, seeking to combine AI and optimization techniques for improved results.

Method: The method integrates large language models to extract design constraints from textual prompts and encodes them into a grid-based format. It employs a coarse-to-fine optimization strategy to ensure computational efficiency.

Result: Experiments show that the proposed method outperforms traditional design pipelines in quality and computational efficiency across multiple scenarios.

Conclusion: The novel framework enhances automated interior design through joint optimization approaches and efficient computational strategies, marking an advancement in intelligent spatial planning.

Abstract: We present a novel framework for automated interior design that combines large language models (LLMs) with grid-based integer programming to jointly optimize room layout and furniture placement. Given a textual prompt, the LLM-driven agent workflow extracts structured design constraints related to room configurations and furniture arrangements. These constraints are encoded into a unified grid-based representation inspired by ``Modulor". Our formulation accounts for key design requirements, including corridor connectivity, room accessibility, spatial exclusivity, and user-specified preferences. To improve computational efficiency, we adopt a coarse-to-fine optimization strategy that begins with a low-resolution grid to solve a simplified problem and guides the solution at the full resolution. Experimental results across diverse scenarios demonstrate that our joint optimization approach significantly outperforms existing two-stage design pipelines in solution quality, and achieves notable computational efficiency through the coarse-to-fine strategy.

</details>


### [302] [Seeing Through the Rain: Resolving High-Frequency Conflicts in Deraining and Super-Resolution via Diffusion Guidance](https://arxiv.org/abs/2511.12419)
*Wenjie Li,Jinglei Shi,Jin Han,Heng Guo,Zhanyu Ma*

Main category: cs.CV

TL;DR: The paper addresses the challenge of achieving clean and high-resolution images for tasks like small object detection in adverse weather conditions. It proposes a novel model, DHGM, to efficiently remove rain artifacts and enhance crucial details using diffusion priors and high-pass filters.


<details>
  <summary>Details</summary>
Motivation: In real-world scenarios, high-resolution images are often degraded by adverse weather, which impacts tasks like small object detection. Current weather restoration and SR methods conflict in their goals, resulting in suboptimal image quality for these applications.

Method: The paper proposes DHGM, a model that combines diffusion-based priors and high-pass filters to effectively remove weather artifacts (e.g., rain) and enhance detailed image structures simultaneously.

Result: DHGM outperforms existing methods in terms of image quality and computational costs, demonstrating superior capabilities in generating clean and high-resolution images.

Conclusion: The proposed DHGM model successfully generates images with improved clarity and structural detail while resolving the conflict between restoration and super-resolution goals. It significantly advances performance for visual tasks like small object detection.

Abstract: Clean images are crucial for visual tasks such as small object detection, especially at high resolutions. However, real-world images are often degraded by adverse weather, and weather restoration methods may sacrifice high-frequency details critical for analyzing small objects. A natural solution is to apply super-resolution (SR) after weather removal to recover both clarity and fine structures. However, simply cascading restoration and SR struggle to bridge their inherent conflict: removal aims to remove high-frequency weather-induced noise, while SR aims to hallucinate high-frequency textures from existing details, leading to inconsistent restoration contents. In this paper, we take deraining as a case study and propose DHGM, a Diffusion-based High-frequency Guided Model for generating clean and high-resolution images. DHGM integrates pre-trained diffusion priors with high-pass filters to simultaneously remove rain artifacts and enhance structural details. Extensive experiments demonstrate that DHGM achieves superior performance over existing methods, with lower costs.

</details>


### [303] [MFI-ResNet: Efficient ResNet Architecture Optimization via MeanFlow Compression and Selective Incubation](https://arxiv.org/abs/2511.12422)
*Nuolin Sun,Linyuan Wang,Haonan Wei,Lei Li,Bin Yan*

Main category: cs.CV

TL;DR: This paper introduces the MFI-ResNet model, leveraging generative flow-fields to improve parameter efficiency and accuracy over traditional ResNet models.


<details>
  <summary>Details</summary>
Motivation: Traditional ResNet models are computationally heavy, and there is an opportunity to utilize generative modeling to enhance parameter efficiency and accuracy.

Method: The proposed MFI-ResNet uses a compression-expansion strategy by incorporating MeanFlow modules to simplify structures and selectively expand stages.

Result: MFI-ResNet reduces parameters by 46.28% and 45.59% while increasing accuracy by 0.23% and 0.17% on CIFAR-10/100 datasets compared to ResNet-50.

Conclusion: Generative flow-fields can effectively characterize feature transformations, bridging generative modeling and discriminative learning while improving ResNet performance.

Abstract: ResNet has achieved tremendous success in computer vision through its residual connection mechanism. ResNet can be viewed as a discretized form of ordinary differential equations (ODEs). From this perspective, the multiple residual blocks within a single ResNet stage essentially perform multi-step discrete iterations of the feature transformation for that stage. The recently proposed flow matching model, MeanFlow, enables one-step generative modeling by learning the mean velocity field to transform distributions. Inspired by this, we propose MeanFlow-Incubated ResNet (MFI-ResNet), which employs a compression-expansion strategy to jointly improve parameter efficiency and discriminative performance. In the compression phase, we simplify the multi-layer structure within each ResNet stage to one or two MeanFlow modules to construct a lightweight meta model. In the expansion phase, we apply a selective incubation strategy to the first three stages, expanding them to match the residual block configuration of the baseline ResNet model, while keeping the last stage in MeanFlow form, and fine-tune the incubated model. Experimental results show that on CIFAR-10 and CIFAR-100 datasets, MFI-ResNet achieves remarkable parameter efficiency, reducing parameters by 46.28% and 45.59% compared to ResNet-50, while still improving accuracy by 0.23% and 0.17%, respectively. This demonstrates that generative flow-fields can effectively characterize the feature transformation process in ResNet, providing a new perspective for understanding the relationship between generative modeling and discriminative learning.

</details>


### [304] [RedVTP: Training-Free Acceleration of Diffusion Vision-Language Models Inference via Masked Token-Guided Visual Token Pruning](https://arxiv.org/abs/2511.12428)
*Jingqi Xu,Jingxi Lu,Chenghao Li,Sreetama Sarkar,Souvik Kundu,Peter A. Beerel*

Main category: cs.CV

TL;DR: The paper proposes RedVTP, a token pruning strategy to improve the inference efficiency of Diffusion Vision-Language Models (DVLMs).


<details>
  <summary>Details</summary>
Motivation: DVLMs suffer from inefficiency due to the large number of visual tokens during inference, making it challenging despite promising parallel decoding capabilities.

Method: RedVTP leverages attention from masked response tokens to estimate visual token importance and prunes less important tokens after the first inference step to enhance efficiency.

Result: The method enhances token generation throughput for models like LLaDA-V and LaViDa by up to 186% and 28.05%, respectively, and reduces inference latency significantly without accuracy loss.

Conclusion: RedVTP successfully addresses inference inefficiency in DVLMs by reducing visual token redundancy, achieving faster and sometimes more accurate outcomes.

Abstract: Vision-Language Models (VLMs) have achieved remarkable progress in multimodal reasoning and generation, yet their high computational demands remain a major challenge. Diffusion Vision-Language Models (DVLMs) are particularly attractive because they enable parallel token decoding, but the large number of visual tokens still significantly hinders their inference efficiency. While visual token pruning has been extensively studied for autoregressive VLMs (AVLMs), it remains largely unexplored for DVLMs. In this work, we propose RedVTP, a response-driven visual token pruning strategy that leverages the inference dynamics of DVLMs. Our method estimates visual token importance using attention from the masked response tokens. Based on the observation that these importance scores remain consistent across steps, RedVTP prunes the less important visual tokens from the masked tokens after the first inference step, thereby maximizing inference efficiency. Experiments show that RedVTP improves token generation throughput of LLaDA-V and LaViDa by up to 186% and 28.05%, respectively, and reduces inference latency by up to 64.97% and 21.87%, without compromising-and in some cases improving-accuracy.

</details>


### [305] [Text-Guided Channel Perturbation and Pretrained Knowledge Integration for Unified Multi-Modality Image Fusion](https://arxiv.org/abs/2511.12432)
*Xilai Li,Xiaosong Li,Weijun Jiang*

Main category: cs.CV

TL;DR: The paper introduces UP-Fusion, a framework to improve multi-modality image fusion by integrating pre-trained knowledge and channel perturbation techniques to overcome fusion challenges. Extensive experiments validate its superior performance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address limitations in multi-modality image fusion frameworks, specifically the challenge of handling large modality differences that cause gradient conflicts, and the trade-off between feature perception and generalisation.

Method: UP-Fusion introduces three modules: (1) Semantic-Aware Channel Pruning Module (SCPM) to enhance features using pre-trained semantic knowledge; (2) Geometric Affine Modulation Module (GAM) to maintain modal discriminability; and (3) Text-Guided Channel Perturbation Module (TCPM) to reshape channel distribution during decoding.

Result: The proposed algorithm outperformed existing multi-modality image fusion methods and showed effectiveness in downstream applications, through extensive experiments.

Conclusion: UP-Fusion successfully integrates pre-trained semantic knowledge, channel perturbation, and efficient feature perception mechanisms, addressing challenges like redundant modal information and poor generalisation in image fusion models.

Abstract: Multi-modality image fusion enhances scene perception by combining complementary information. Unified models aim to share parameters across modalities for multi-modality image fusion, but large modality differences often cause gradient conflicts, limiting performance. Some methods introduce modality-specific encoders to enhance feature perception and improve fusion quality. However, this strategy reduces generalisation across different fusion tasks. To overcome this limitation, we propose a unified multi-modality image fusion framework based on channel perturbation and pre-trained knowledge integration (UP-Fusion). To suppress redundant modal information and emphasize key features, we propose the Semantic-Aware Channel Pruning Module (SCPM), which leverages the semantic perception capability of a pre-trained model to filter and enhance multi-modality feature channels. Furthermore, we proposed the Geometric Affine Modulation Module (GAM), which uses original modal features to apply affine transformations on initial fusion features to maintain the feature encoder modal discriminability. Finally, we apply a Text-Guided Channel Perturbation Module (TCPM) during decoding to reshape the channel distribution, reducing the dependence on modality-specific channels. Extensive experiments demonstrate that the proposed algorithm outperforms existing methods on both multi-modality image fusion and downstream tasks.

</details>


### [306] [Real-Time Drivers' Drowsiness Detection and Analysis through Deep Learning](https://arxiv.org/abs/2511.12438)
*ANK Zaman,Prosenjit Chatterjee,Rajat Sharma*

Main category: cs.CV

TL;DR: This paper presents a real-time driver drowsiness detection system using deep convolutional neural networks (DCNNs) and OpenCV to analyze facial landmarks and issue alerts, achieving high accuracy rates.


<details>
  <summary>Details</summary>
Motivation: To address the life-threatening risks caused by driver drowsiness and provide a cost-effective, real-time detection system to ensure road safety.

Method: The system captures real-time facial images of drivers via a live camera and uses OpenCV to analyze facial landmarks. A DCNN framework processes the data using a pre-trained model to detect drowsiness, triggering immediate alarms.

Result: The model was tested on the NTHU-DDD and Yawn-Eye datasets, yielding 99.6% and 97% classification accuracy respectively in drowsiness detection.

Conclusion: The proposed system is an effective, inexpensive, and non-invasive solution for preventing road accidents caused by drowsy driving.

Abstract: A long road trip is fun for drivers. However, a long drive for days can be tedious for a driver to accommodate stringent deadlines to reach distant destinations. Such a scenario forces drivers to drive extra miles, utilizing extra hours daily without sufficient rest and breaks. Once a driver undergoes such a scenario, it occasionally triggers drowsiness during driving. Drowsiness in driving can be life-threatening to any individual and can affect other drivers' safety; therefore, a real-time detection system is needed. To identify fatigued facial characteristics in drivers and trigger the alarm immediately, this research develops a real-time driver drowsiness detection system utilizing deep convolutional neural networks (DCNNs) and OpenCV.Our proposed and implemented model takes real- time facial images of a driver using a live camera and utilizes a Python-based library named OpenCV to examine the facial images for facial landmarks like sufficient eye openings and yawn-like mouth movements. The DCNNs framework then gathers the data and utilizes a per-trained model to detect the drowsiness of a driver using facial landmarks. If the driver is identified as drowsy, the system issues a continuous alert in real time, embedded in the Smart Car technology.By potentially saving innocent lives on the roadways, the proposed technique offers a non-invasive, inexpensive, and cost-effective way to identify drowsiness. Our proposed and implemented DCNNs embedded drowsiness detection model successfully react with NTHU-DDD dataset and Yawn-Eye-Dataset with drowsiness detection classification accuracy of 99.6% and 97% respectively.

</details>


### [307] [CoTBox-TTT: Grounding Medical VQA with Visual Chain-of-Thought Boxes During Test-time Training](https://arxiv.org/abs/2511.12446)
*Jiahe Qian,Yuhao Shen,Zhangtianyi Chen,Juexiao Zhou,Peisong Wang*

Main category: cs.CV

TL;DR: The paper presents CoTBox-TTT, a method to improve the reliability of medical visual question answering by adapting models through test-time training without requiring additional labels.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the reliability gap in medical visual question answering systems, which often fail under domain shifts and produce answers weakly grounded in image evidence.

Method: CoTBox-TTT adapts vision-language models during inference by updating a small set of continuous soft prompts, leveraging a visual chain-of-thought signal for identifying relevant regions and ensuring answer consistency.

Result: Experiments show that CoTBox-TTT improves closed-ended accuracy, such as a 12.3% increase on pathVQA when added to LLaVA.

Conclusion: CoTBox-TTT is effective and practical for real-world medical deployments without retraining or requiring new labels, enhancing accuracy and reliability.

Abstract: Medical visual question answering could support clinical decision making, yet current systems often fail under domain shift and produce answers that are weakly grounded in image evidence. This reliability gap arises when models attend to spurious regions and when retraining or additional labels are impractical at deployment time. We address this setting with CoTBox-TTT, an evidence-first test-time training approach that adapts a vision-language model at inference while keeping all backbones frozen. The method updates only a small set of continuous soft prompts. It identifies question-relevant regions through a visual chain-of-thought signal and encourages answer consistency across the original image and a localized crop. The procedure is label free, and plug and play with diverse backbones. Experiments on medical VQA show that the approach is practical for real deployments. For instance, adding CoTBox-TTT to LLaVA increases closed-ended accuracy by 12.3% on pathVQA.

</details>


### [308] [MOON2.0: Dynamic Modality-balanced Multimodal Representation Learning for E-commerce Product Understanding](https://arxiv.org/abs/2511.12449)
*Zhanheng Nie,Chenghan Fu,Daoze Zhang,Junxian Wu,Wanxian Guan,Pengjie Wang,Jian Xu,Bo Zheng*

Main category: cs.CV

TL;DR: MOON2.0 is a framework to tackle challenges in multimodal e-commerce representation learning, featuring dynamic modality balance, dual-level alignment, and co-augmentation strategies, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the existing challenges in multimodal large language models (MLLMs) for e-commerce such as modality imbalance, lack of alignment between visual and textual data, and noise in multimodal datasets.

Method: MOON2.0 introduces three key methodologies: (1) a Modality-driven Mixture-of-Experts module for dynamic multimodal learning; (2) Dual-level Alignment for better semantic integration; (3) Image-text Co-augmentation and Dynamic Sample Filtering for data enrichment and noise reduction.

Result: MOON2.0 outperforms previous models in zero-shot performance across the MBE2.0 benchmark and other public datasets. It also demonstrates improved multimodal alignment via attention heatmap visualizations.

Conclusion: MOON2.0 significantly enhances multimodal representation learning for e-commerce, effectively addressing the identified challenges and setting a new standard for performance in this domain.

Abstract: The rapid growth of e-commerce calls for multimodal models that comprehend rich visual and textual product information. Although recent multimodal large language models (MLLMs) for product understanding exhibit strong capability in representation learning for e-commerce, they still face three challenges: (i) the modality imbalance induced by modality mixed training; (ii) underutilization of the intrinsic alignment relationships among visual and textual information within a product; and (iii) limited handling of noise in e-commerce multimodal data. To address these, we propose MOON2.0, a dynamic modality-balanced multimodal representation learning framework for e-commerce product understanding. MOON2.0 comprises: (1) a Modality-driven Mixture-of-Experts (MoE) module that adaptively processes input samples by their modality composition, enabling Multimodal Joint Learning to mitigate the modality imbalance; (2) a Dual-level Alignment method to better leverage semantic alignment properties inside individual products; and (3) an MLLM-based Image-text Co-augmentation strategy that integrates textual enrichment with visual expansion, coupled with Dynamic Sample Filtering to improve training data quality. We further introduce MBE2.0, a co-augmented multimodal representation benchmark for e-commerce representation learning and evaluation. Experiments show that MOON2.0 delivers state-of-the-art zero-shot performance on MBE2.0 and multiple public datasets. Furthermore, attention-based heatmap visualization provides qualitative evidence of improved multimodal alignment of MOON2.0.

</details>


### [309] [MaskAnyNet: Rethinking Masked Image Regions as Valuable Information in Supervised Learning](https://arxiv.org/abs/2511.12480)
*Jingshan Hong,Haigen Hu,Huihuang Zhang,Qianwei Zhou,Zhao Li*

Main category: cs.CV

TL;DR: The paper proposes MaskAnyNet, a method that enhances traditional image masking by treating masked regions as sources of auxiliary information rather than ignoring them, thus improving semantic diversity and fine-grained task performance.


<details>
  <summary>Details</summary>
Motivation: The authors aimed to address key problems in traditional image masking, including underutilization of discarded pixels and risks of losing fine details in masking, especially for fine-grained tasks.

Method: The proposed MaskAnyNet integrates a relearning mechanism to concurrently exploit both visible and masked information by adding an auxiliary learning branch, allowing for semantic richness in features.

Result: Experimental evaluations on CNN and Transformer backbones demonstrate consistent performance improvements across multiple benchmarks, confirming the efficacy of leveraging masked regions.

Conclusion: MaskAnyNet effectively improves semantic diversity and preserves fine-grained details in supervised learning tasks by utilizing masked regions as auxiliary knowledge.

Abstract: In supervised learning, traditional image masking faces two key issues: (i) discarded pixels are underutilized, leading to a loss of valuable contextual information; (ii) masking may remove small or critical features, especially in fine-grained tasks. In contrast, masked image modeling (MIM) has demonstrated that masked regions can be reconstructed from partial input, revealing that even incomplete data can exhibit strong contextual consistency with the original image. This highlights the potential of masked regions as sources of semantic diversity. Motivated by this, we revisit the image masking approach, proposing to treat masked content as auxiliary knowledge rather than ignored. Based on this, we propose MaskAnyNet, which combines masking with a relearning mechanism to exploit both visible and masked information. It can be easily extended to any model with an additional branch to jointly learn from the recomposed masked region. This approach leverages the semantic diversity of the masked regions to enrich features and preserve fine-grained details. Experiments on CNN and Transformer backbones show consistent gains across multiple benchmarks. Further analysis confirms that the proposed method improves semantic diversity through the reuse of masked content.

</details>


### [310] [Towards Temporal Fusion Beyond the Field of View for Camera-based Semantic Scene Completion](https://arxiv.org/abs/2511.12498)
*Jongseong Bae,Junwoo Ha,Jinnyeong Heo,Yeongin Lee,Ha Young Kim*

Main category: cs.CV

TL;DR: This paper introduces a novel module, C3DFusion, to address limitations in camera-based 3D semantic scene completion by leveraging temporal cues to better reconstruct unseen regions around the ego-vehicle.


<details>
  <summary>Details</summary>
Motivation: Existing camera-based SSC methods struggle to reconstruct unseen areas near the ego-vehicle due to insufficient use of historical frame context.

Method: The proposed C3DFusion module aligns 3D-lifted point features from both current and historical frames. It employs techniques such as historical context blurring and current-centric feature densification to suppress noise and enhance feature geometry.

Result: C3DFusion significantly outperforms state-of-the-art methods on datasets like SemanticKITTI and SSCBench-KITTI-360. It also demonstrates strong generalization across various baseline models.

Conclusion: The C3DFusion module provides a robust and effective solution for enhancing SSC architectures, leveraging temporal data for improved performance and generalization.

Abstract: Recent camera-based 3D semantic scene completion (SSC) methods have increasingly explored leveraging temporal cues to enrich the features of the current frame. However, while these approaches primarily focus on enhancing in-frame regions, they often struggle to reconstruct critical out-of-frame areas near the sides of the ego-vehicle, although previous frames commonly contain valuable contextual information about these unseen regions. To address this limitation, we propose the Current-Centric Contextual 3D Fusion (C3DFusion) module, which generates hidden region-aware 3D feature geometry by explicitly aligning 3D-lifted point features from both current and historical frames. C3DFusion performs enhanced temporal fusion through two complementary techniques-historical context blurring and current-centric feature densification-which suppress noise from inaccurately warped historical point features by attenuating their scale, and enhance current point features by increasing their volumetric contribution. Simply integrated into standard SSC architectures, C3DFusion demonstrates strong effectiveness, significantly outperforming state-of-the-art methods on the SemanticKITTI and SSCBench-KITTI-360 datasets. Furthermore, it exhibits robust generalization, achieving notable performance gains when applied to other baseline models.

</details>


### [311] [Visible Structure Retrieval for Lightweight Image-Based Relocalisation](https://arxiv.org/abs/2511.12503)
*Fereidoon Zangeneh,Leonard Bruns,Amit Dekel,Alessandro Pieropan,Patric Jensfelt*

Main category: cs.CV

TL;DR: The paper proposes a compact neural network to map image observations directly to visible scene structures for efficient camera pose estimation, reducing computational and storage needs.


<details>
  <summary>Details</summary>
Motivation: Current pose estimation methods either require complex pipelines or significant storage due to reliance on search heuristics or image retrieval.

Method: A novel visible structure retrieval network directly maps image observations to the visible 3D scene structure, simplifying point correspondence and reducing search space.

Result: The method achieves state-of-the-art localisation accuracy with reduced computational and storage requirements.

Conclusion: The proposed approach simplifies the pose estimation process and makes it computationally and storage efficient, maintaining high accuracy.

Abstract: Accurate camera pose estimation from an image observation in a previously mapped environment is commonly done through structure-based methods: by finding correspondences between 2D keypoints on the image and 3D structure points in the map. In order to make this correspondence search tractable in large scenes, existing pipelines either rely on search heuristics, or perform image retrieval to reduce the search space by comparing the current image to a database of past observations. However, these approaches result in elaborate pipelines or storage requirements that grow with the number of past observations. In this work, we propose a new paradigm for making structure-based relocalisation tractable. Instead of relying on image retrieval or search heuristics, we learn a direct mapping from image observations to the visible scene structure in a compact neural network. Given a query image, a forward pass through our novel visible structure retrieval network allows obtaining the subset of 3D structure points in the map that the image views, thus reducing the search space of 2D-3D correspondences. We show that our proposed method enables performing localisation with an accuracy comparable to the state of the art, while requiring lower computational and storage footprint.

</details>


### [312] [DINO-Detect: A Simple yet Effective Framework for Blur-Robust AI-Generated Image Detection](https://arxiv.org/abs/2511.12511)
*Jialiang Shen,Jiyang Zheng,Yunqi Xue,Huajie Chen,Yu Yao,Hui Kang,Ruiqi Liu,Helin Gong,Yang Yang,Dadong Wang,Tongliang Liu*

Main category: cs.CV

TL;DR: The paper presents a method to improve AI-generated image detection in real-world scenarios, especially under motion blur, through a teacher-student knowledge distillation framework.


<details>
  <summary>Details</summary>
Motivation: Current AIGI detectors struggle with real-world challenges like motion blur, which impacts the accuracy of detection due to the loss of high-frequency artifacts.

Method: They use a teacher-student knowledge distillation approach where a high-capacity teacher (DINOv3) provides stable representations. The teacher’s features and logits are distilled into a student model trained on blurred images to maintain robustness.

Result: The proposed method achieves state-of-the-art performance in detecting AI-generated images both in clean and motion-blurred conditions, improving generalization and real-world applicability.

Conclusion: This framework successfully mitigates performance drops caused by motion blur and enhances detection capabilities, making it highly applicable in authentic image verification and digital safety.

Abstract: With growing concerns over image authenticity and digital safety, the field of AI-generated image (AIGI) detection has progressed rapidly. Yet, most AIGI detectors still struggle under real-world degradations, particularly motion blur, which frequently occurs in handheld photography, fast motion, and compressed video. Such blur distorts fine textures and suppresses high-frequency artifacts, causing severe performance drops in real-world settings. We address this limitation with a blur-robust AIGI detection framework based on teacher-student knowledge distillation. A high-capacity teacher (DINOv3), trained on clean (i.e., sharp) images, provides stable and semantically rich representations that serve as a reference for learning. By freezing the teacher to maintain its generalization ability, we distill its feature and logit responses from sharp images to a student trained on blurred counterparts, enabling the student to produce consistent representations under motion degradation. Extensive experiments benchmarks show that our method achieves state-of-the-art performance under both motion-blurred and clean conditions, demonstrating improved generalization and real-world applicability. Source codes will be released at: https://github.com/JiaLiangShen/Dino-Detect-for-blur-robust-AIGC-Detection.

</details>


### [313] [MdaIF: Robust One-Stop Multi-Degradation-Aware Image Fusion with Language-Driven Semantics](https://arxiv.org/abs/2511.12525)
*Jing Li,Yifan Wang,Jiafeng Yan,Renlong Zhang,Bin Yang*

Main category: cs.CV

TL;DR: The paper proposes a framework (MdaIF) for infrared and visible image fusion in multi-degradation scenarios using semantic priors and a mixture-of-experts system to handle adverse weather degradation.


<details>
  <summary>Details</summary>
Motivation: Existing methods do not account for visible image degradation under adverse weather, limiting fusion performance, and rely on fixed network architectures, which lack adaptability.

Method: A one-stop degradation-aware image fusion framework (MdaIF) that employs a mixture-of-experts system for various scenarios. A vision-language model is used for adaptive semantic extraction, guiding components like a degradation-aware channel attention module (DCAM).

Result: Through extensive experiments, the proposed MdaIF framework outperformed state-of-the-art methods in handling multi-degradation scenarios for image fusion.

Conclusion: MdaIF effectively enhances image fusion performance under adverse weather by using a novel adaptive, semantic-prior-guided architecture, showcasing its potential for broad scalability and application.

Abstract: Infrared and visible image fusion aims to integrate complementary multi-modal information into a single fused result. However, existing methods 1) fail to account for the degradation visible images under adverse weather conditions, thereby compromising fusion performance; and 2) rely on fixed network architectures, limiting their adaptability to diverse degradation scenarios. To address these issues, we propose a one-stop degradation-aware image fusion framework for multi-degradation scenarios driven by a large language model (MdaIF). Given the distinct scattering characteristics of different degradation scenarios (e.g., haze, rain, and snow) in atmospheric transmission, a mixture-of-experts (MoE) system is introduced to tackle image fusion across multiple degradation scenarios. To adaptively extract diverse weather-aware degradation knowledge and scene feature representations, collectively referred to as the semantic prior, we employ a pre-trained vision-language model (VLM) in our framework. Guided by the semantic prior, we propose degradation-aware channel attention module (DCAM), which employ degradation prototype decomposition to facilitate multi-modal feature interaction in channel domain. In addition, to achieve effective expert routing, the semantic prior and channel-domain modulated features are utilized to guide the MoE, enabling robust image fusion in complex degradation scenarios. Extensive experiments validate the effectiveness of our MdaIF, demonstrating superior performance over SOTA methods.

</details>


### [314] [D$^{2}$-VPR: A Parameter-efficient Visual-foundation-model-based Visual Place Recognition Method via Knowledge Distillation and Deformable Aggregation](https://arxiv.org/abs/2511.12528)
*Zheyuan Zhang,Jiwei Zhang,Boyu Zhou,Linzhimeng Duan,Hong Chen*

Main category: cs.CV

TL;DR: The paper presents $D^{2}$-VPR, a framework for Visual Place Recognition (VPR) achieving competitive performance with reduced computational load by utilizing knowledge distillation and deformable aggregation.


<details>
  <summary>Details</summary>
Motivation: To tackle the computational inefficiency and high complexity of powerful visual models like DINOv2 in VPR tasks, enabling deployment on resource-constrained devices.

Method: The proposed $D^{2}$-VPR employs a two-stage training process with knowledge distillation and fine-tuning, introduces a Distillation Recovery Module (DRM) to enhance feature alignment, and designs a Top-Down-attention-based Deformable Aggregator (TDDA) for dynamic ROI adaptation.

Result: The $D^{2}$-VPR significantly reduces parameter count by 64.2% and FLOPs by 62.6% compared to CricaVPR, maintaining competitive performance.

Conclusion: The framework effectively achieves a better trade-off between performance and efficiency for VPR tasks, making it suitable for deployment on resource-constrained devices.

Abstract: Visual Place Recognition (VPR) aims to determine the geographic location of a query image by retrieving its most visually similar counterpart from a geo-tagged reference database. Recently, the emergence of the powerful visual foundation model, DINOv2, trained in a self-supervised manner on massive datasets, has significantly improved VPR performance. This improvement stems from DINOv2's exceptional feature generalization capabilities but is often accompanied by increased model complexity and computational overhead that impede deployment on resource-constrained devices. To address this challenge, we propose $D^{2}$-VPR, a $D$istillation- and $D$eformable-based framework that retains the strong feature extraction capabilities of visual foundation models while significantly reducing model parameters and achieving a more favorable performance-efficiency trade-off. Specifically, first, we employ a two-stage training strategy that integrates knowledge distillation and fine-tuning. Additionally, we introduce a Distillation Recovery Module (DRM) to better align the feature spaces between the teacher and student models, thereby minimizing knowledge transfer losses to the greatest extent possible. Second, we design a Top-Down-attention-based Deformable Aggregator (TDDA) that leverages global semantic features to dynamically and adaptively adjust the Regions of Interest (ROI) used for aggregation, thereby improving adaptability to irregular structures. Extensive experiments demonstrate that our method achieves competitive performance compared to state-of-the-art approaches. Meanwhile, it reduces the parameter count by approximately 64.2% and FLOPs by about 62.6% (compared to CricaVPR).Code is available at https://github.com/tony19980810/D2VPR.

</details>


### [315] [ReaSon: Reinforced Causal Search with Information Bottleneck for Video Understanding](https://arxiv.org/abs/2511.12530)
*Yuan Zhou,Litao Hua,Shilong Jin,Wentao Huang,Haoran Duan*

Main category: cs.CV

TL;DR: The paper introduces ReaSon, a framework for selecting keyframes in videos using a novel Causal Information Bottleneck (CIB) method to optimize video understanding with vision-language models.


<details>
  <summary>Details</summary>
Motivation: Improving video understanding by identifying keyframes that are both causally essential and informative, addressing constraints of input token limits and temporal sparsity.

Method: ReaSon combines a learnable policy network for predictive sufficiency with causal necessity evaluation using counterfactual interventions. Reinforcement learning drives a composite reward system aligned with the CIB principle.

Result: Experiments on datasets like NExT-QA, EgoSchema, and Video-MME show ReaSon consistently outperforms existing methods in keyframe selection.

Conclusion: ReaSon demonstrates a significant improvement in video understanding tasks by effectively selecting causally decisive and informative keyframes, validating the framework's utility and transferability.

Abstract: Keyframe selection has become essential for video understanding with vision-language models (VLMs) due to limited input tokens and the temporal sparsity of relevant information across video frames. Video understanding often relies on effective keyframes that are not only informative but also causally decisive. To this end, we propose Reinforced Causal Search with Information Bottleneck (ReaSon), a framework that formulates keyframe selection as an optimization problem with the help of a novel Causal Information Bottleneck (CIB), which explicitly defines keyframes as those satisfying both predictive sufficiency and causal necessity. Specifically, ReaSon employs a learnable policy network to select keyframes from a visually relevant pool of candidate frames to capture predictive sufficiency, and then assesses causal necessity via counterfactual interventions. Finally, a composite reward aligned with the CIB principle is designed to guide the selection policy through reinforcement learning. Extensive experiments on NExT-QA, EgoSchema, and Video-MME demonstrate that ReaSon consistently outperforms existing state-of-the-art methods under limited-frame settings, validating its effectiveness and generalization ability.

</details>


### [316] [HiGFA: Hierarchical Guidance for Fine-grained Data Augmentation with Diffusion Models](https://arxiv.org/abs/2511.12547)
*Zhiguang Lu,Qianqian Xu,Peisong Wen,Siran Da,Qingming Huang*

Main category: cs.CV

TL;DR: HiGFA addresses challenges in synthetic image generation for fine-grained tasks by balancing global structure formation and precise detail refinement using hierarchical and confidence-driven guidance in diffusion models.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the difficulty of generating synthetic images that accurately capture subtle, category-defining features for fine-grained visual tasks.

Method: HiGFA uses hierarchical guidance during the diffusion sampling process, incorporating text and contour guidance for early stages and confidence-modulated classifier guidance for later stages.

Result: HiGFA generates diverse yet accurate synthetic images, improving fine-grained classifier performance in several FGVC datasets.

Conclusion: HiGFA effectively enhances synthetic data generation for fine-grained classification tasks by leveraging hierarchical and dynamic confidence-driven guidance.

Abstract: Generative diffusion models show promise for data augmentation. However, applying them to fine-grained tasks presents a significant challenge: ensuring synthetic images accurately capture the subtle, category-defining features critical for high fidelity. Standard approaches, such as text-based Classifier-Free Guidance (CFG), often lack the required specificity, potentially generating misleading examples that degrade fine-grained classifier performance. To address this, we propose Hierarchically Guided Fine-grained Augmentation (HiGFA). HiGFA leverages the temporal dynamics of the diffusion sampling process. It employs strong text and transformed contour guidance with fixed strengths in the early-to-mid sampling stages to establish overall scene, style, and structure. In the final sampling stages, HiGFA activates a specialized fine-grained classifier guidance and dynamically modulates the strength of all guidance signals based on prediction confidence. This hierarchical, confidence-driven orchestration enables HiGFA to generate diverse yet faithful synthetic images by intelligently balancing global structure formation with precise detail refinement. Experiments on several FGVC datasets demonstrate the effectiveness of HiGFA.

</details>


### [317] [EmoVerse: A MLLMs-Driven Emotion Representation Dataset for Interpretable Visual Emotion Analysis](https://arxiv.org/abs/2511.12554)
*Yijie Guo,Dexiang Hong,Weidong Chen,Zihan She,Cheng Ye,Xiaojun Chang,Zhendong Mao*

Main category: cs.CV

TL;DR: This paper introduces EmoVerse, a large-scale open-source dataset for visual emotion analysis with interpretable annotations, along with an emotion model for high-level explainability.


<details>
  <summary>Details</summary>
Motivation: To address the lack of interpretable datasets in visual emotion analysis and move beyond assigning single emotion labels to images, limiting insights into visual-element contributions.

Method: EmoVerse employs a multi-layered annotation approach with Background-Attribute-Subject (B-A-S) triplets and dual annotations for discrete and continuous emotion representation. A novel multi-stage pipeline ensures reliable annotations with minimal human effort.

Result: EmoVerse provides 219,000 images with interpretable annotations and demonstrates the proposed model's ability to map visual cues into dimensional emotion representations while offering detailed attributions.

Conclusion: The dataset, pipeline, and model collectively advance the field of explainable and interpretable visual emotion analysis, enabling more nuanced emotional reasoning for visual content.

Abstract: Visual Emotion Analysis (VEA) aims to bridge the affective gap between visual content and human emotional responses. Despite its promise, progress in this field remains limited by the lack of open-source and interpretable datasets. Most existing studies assign a single discrete emotion label to an entire image, offering limited insight into how visual elements contribute to emotion. In this work, we introduce EmoVerse, a large-scale open-source dataset that enables interpretable visual emotion analysis through multi-layered, knowledge-graph-inspired annotations. By decomposing emotions into Background-Attribute-Subject (B-A-S) triplets and grounding each element to visual regions, EmoVerse provides word-level and subject-level emotional reasoning. With over 219k images, the dataset further includes dual annotations in Categorical Emotion States (CES) and Dimensional Emotion Space (DES), facilitating unified discrete and continuous emotion representation. A novel multi-stage pipeline ensures high annotation reliability with minimal human effort. Finally, we introduce an interpretable model that maps visual cues into DES representations and provides detailed attribution explanations. Together, the dataset, pipeline, and model form a comprehensive foundation for advancing explainable high-level emotion understanding.

</details>


### [318] [SEMC: Structure-Enhanced Mixture-of-Experts Contrastive Learning for Ultrasound Standard Plane Recognition](https://arxiv.org/abs/2511.12559)
*Qing Cai,Guihao Yan,Fan Zhang,Cheng Zhang,Zhi Liu*

Main category: cs.CV

TL;DR: The paper introduces a novel framework called SEMC to improve ultrasound standard plane recognition by integrating multi-scale structural information and expert-guided contrastive learning strategies with promising results.


<details>
  <summary>Details</summary>
Motivation: Current methods for ultrasound standard plane recognition lack the capability to exploit shallow structural information and fail at capturing fine-grained semantic differences, leading to suboptimal recognition outcomes.

Method: The proposed SEMC framework includes two key components: the Semantic-Structure Fusion Module (SSFM) for multi-scale structural information alignment and the Mixture-of-Experts Contrastive Recognition Module (MCRM) for hierarchical contrastive learning and classification.

Result: SEMC demonstrated superior performance against state-of-the-art methods on both an in-house dataset and two public datasets, showcasing its effectiveness in enhancing class separability and recognition accuracy.

Conclusion: SEMC significantly improves the recognition of structural and discriminative details in ultrasound standard plane recognition, offering a promising advance for clinical tasks.

Abstract: Ultrasound standard plane recognition is essential for clinical tasks such as disease screening, organ evaluation, and biometric measurement. However, existing methods fail to effectively exploit shallow structural information and struggle to capture fine-grained semantic differences through contrastive samples generated by image augmentations, ultimately resulting in suboptimal recognition of both structural and discriminative details in ultrasound standard planes. To address these issues, we propose SEMC, a novel Structure-Enhanced Mixture-of-Experts Contrastive learning framework that combines structure-aware feature fusion with expert-guided contrastive learning. Specifically, we first introduce a novel Semantic-Structure Fusion Module (SSFM) to exploit multi-scale structural information and enhance the model's ability to perceive fine-grained structural details by effectively aligning shallow and deep features. Then, a novel Mixture-of-Experts Contrastive Recognition Module (MCRM) is designed to perform hierarchical contrastive learning and classification across multi-level features using a mixture-of-experts (MoE) mechanism, further improving class separability and recognition performance. More importantly, we also curate a large-scale and meticulously annotated liver ultrasound dataset containing six standard planes. Extensive experimental results on our in-house dataset and two public datasets demonstrate that SEMC outperforms recent state-of-the-art methods across various metrics.

</details>


### [319] [Through-Foliage Surface-Temperature Reconstruction for early Wildfire Detection](https://arxiv.org/abs/2511.12572)
*Mohamed Youssef,Lukas Brunner,Klaus Rundhammer,Gerald Czech,Oliver Bimber*

Main category: cs.CV

TL;DR: This paper introduces a novel method combining signal processing and machine learning for reconstructing occluded surface temperatures, enabling autonomous aerial wildfire monitoring and early detection.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome the limitations of conventional thermal imaging which struggles with occlusions caused by forest vegetation, making early wildfire detection difficult.

Method: The authors developed a visual state space model trained to recover occluded thermal signals, supported by synthetic data generation using a latent diffusion model and procedural simulation techniques.

Result: The method achieved significant reductions in RMSE, demonstrating superior accuracy over conventional thermal and SA imaging on simulated and real-world data, with a 2 to 12.8-fold improvement.

Conclusion: The approach successfully reconstructed surface temperature morphologies in challenging scenarios, proving its efficiency in wildfire detection and other applications like search and rescue.

Abstract: We introduce a novel method for reconstructing surface temperatures through occluding forest vegetation by combining signal processing and machine learning. Our goal is to enable fully automated aerial wildfire monitoring using autonomous drones, allowing for the early detection of ground fires before smoke or flames are visible. While synthetic aperture (SA) sensing mitigates occlusion from the canopy and sunlight, it introduces thermal blur that obscures the actual surface temperatures. To address this, we train a visual state space model to recover the subtle thermal signals of partially occluded soil and fire hotspots from this blurred data. A key challenge was the scarcity of real-world training data. We overcome this by integrating a latent diffusion model into a vector quantized to generated a large volume of realistic surface temperature simulations from real wildfire recordings, which we further expanded through temperature augmentation and procedural thermal forest simulation. On simulated data across varied ambient and surface temperatures, forest densities, and sunlight conditions, our method reduced the RMSE by a factor of 2 to 2.5 compared to conventional thermal and uncorrected SA imaging. In field experiments focused on high-temperature hotspots, the improvement was even more significant, with a 12.8-fold RMSE gain over conventional thermal and a 2.6-fold gain over uncorrected SA images. We also demonstrate our model's generalization to other thermal signals, such as human signatures for search and rescue. Since simple thresholding is frequently inadequate for detecting subtle thermal signals, the morphological characteristics are equally essential for accurate classification. Our experiments demonstrated another clear advantage: we reconstructed the complete morphology of fire and human signatures, whereas conventional imaging is defeated by partial occlusion.

</details>


### [320] [Beyond Pixels: Semantic-aware Typographic Attack for Geo-Privacy Protection](https://arxiv.org/abs/2511.12575)
*Jiayi Zhu,Yihao Huang,Yue Cao,Xiaojun Jia,Qing Guo,Felix Juefei-Xu,Geguang Pu,Bin Wang*

Main category: cs.CV

TL;DR: Large Visual Language Models (LVLMs) pose a privacy threat by inferring geolocation from shared images. This research explores using semantics-aware typographical attacks to protect privacy without degrading image quality.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the geo-privacy threat posed by LVLMs, as current solutions like adversarial perturbations compromise the visual quality of shared images.

Method: This paper introduces a two-stage semantics-aware typographical attack that adds deceptive textual elements outside the visual content of images to counter geolocation inference by LVLMs.

Result: Tests across three datasets show that the proposed method significantly reduces the geolocation prediction accuracy of five state-of-the-art LVLMs.

Conclusion: Semantics-aware typographical attacks serve as a practical, visually-preserving method to combat geo-privacy threats posed by LVLMs.

Abstract: Large Visual Language Models (LVLMs) now pose a serious yet overlooked privacy threat, as they can infer a social media user's geolocation directly from shared images, leading to unintended privacy leakage. While adversarial image perturbations provide a potential direction for geo-privacy protection, they require relatively strong distortions to be effective against LVLMs, which noticeably degrade visual quality and diminish an image's value for sharing. To overcome this limitation, we identify typographical attacks as a promising direction for protecting geo-privacy by adding text extension outside the visual content. We further investigate which textual semantics are effective in disrupting geolocation inference and design a two-stage, semantics-aware typographical attack that generates deceptive text to protect user privacy. Extensive experiments across three datasets demonstrate that our approach significantly reduces geolocation prediction accuracy of five state-of-the-art commercial LVLMs, establishing a practical and visually-preserving protection strategy against emerging geo-privacy threats.

</details>


### [321] [TempoMaster: Efficient Long Video Generation via Next-Frame-Rate Prediction](https://arxiv.org/abs/2511.12578)
*Yukuo Ma,Cong Liu,Junke Wang,Junqi Liu,Haibin Huang,Zuxuan Wu,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: Introducing TempoMaster for long video generation via next-frame-rate prediction, achieving state-of-the-art results in visual and temporal quality.


<details>
  <summary>Details</summary>
Motivation: To address challenges in generating long videos with both visual quality and temporal coherence.

Method: A two-phase approach: generating a low-frame-rate clip as a blueprint, then progressively refining it through bidirectional attention and autoregressive synthesis across frame rates.

Result: TempoMaster outperforms existing methods in creating long videos, ensuring superior visual and temporal quality.

Conclusion: TempoMaster sets a new standard for long video generation by balancing temporal coherence and synthesis efficiency.

Abstract: We present TempoMaster, a novel framework that formulates long video generation as next-frame-rate prediction. Specifically, we first generate a low-frame-rate clip that serves as a coarse blueprint of the entire video sequence, and then progressively increase the frame rate to refine visual details and motion continuity. During generation, TempoMaster employs bidirectional attention within each frame-rate level while performing autoregression across frame rates, thus achieving long-range temporal coherence while enabling efficient and parallel synthesis. Extensive experiments demonstrate that TempoMaster establishes a new state-of-the-art in long video generation, excelling in both visual and temporal quality.

</details>


### [322] [Rank-Aware Agglomeration of Foundation Models for Immunohistochemistry Image Cell Counting](https://arxiv.org/abs/2511.12588)
*Zuqi Huang,Mengxin Tian,Huan Liu,Wentao Li,Baobao Liang,Jie Wu,Fang Yan,Zhaoqing Tang,Zhongyu Li*

Main category: cs.CV

TL;DR: The paper presents a novel approach called CountIHC, which uses a rank-aware agglomeration framework to improve multi-class cell counting in IHC images by leveraging multiple foundation models, addressing limitations like chromogen overlap and variable biomarker staining.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in accurate cell counting for IHC images, such as chromogen overlap, diverse morphologies, and underutilization of foundation models in multi-class counting.

Method: The method involves a rank-aware agglomeration framework with a Rank-Aware Teacher Selecting (RATS) mechanism for selective knowledge distillation from multiple foundation models. Additionally, a fine-tuning stage reformulates multi-class counting as vision-language alignment.

Result: CountIHC outperforms state-of-the-art methods across diverse biomarkers and tissue types, demonstrating high agreement with pathologists and scalability on H&E-stained data.

Conclusion: The proposed framework effectively handles IHC image heterogeneity and improves multi-class cell counting, providing scalable and reliable results for diverse staining types.

Abstract: Accurate cell counting in immunohistochemistry (IHC) images is critical for quantifying protein expression and aiding cancer diagnosis. However, the task remains challenging due to the chromogen overlap, variable biomarker staining, and diverse cellular morphologies. Regression-based counting methods offer advantages over detection-based ones in handling overlapped cells, yet rarely support end-to-end multi-class counting. Moreover, the potential of foundation models remains largely underexplored in this paradigm. To address these limitations, we propose a rank-aware agglomeration framework that selectively distills knowledge from multiple strong foundation models, leveraging their complementary representations to handle IHC heterogeneity and obtain a compact yet effective student model, CountIHC. Unlike prior task-agnostic agglomeration strategies that either treat all teachers equally or rely on feature similarity, we design a Rank-Aware Teacher Selecting (RATS) strategy that models global-to-local patch rankings to assess each teacher's inherent counting capacity and enable sample-wise teacher selection. For multi-class cell counting, we introduce a fine-tuning stage that reformulates the task as vision-language alignment. Discrete semantic anchors derived from structured text prompts encode both category and quantity information, guiding the regression of class-specific density maps and improving counting for overlapping cells. Extensive experiments demonstrate that CountIHC surpasses state-of-the-art methods across 12 IHC biomarkers and 5 tissue types, while exhibiting high agreement with pathologists' assessments. Its effectiveness on H&E-stained data further confirms the scalability of the proposed method.

</details>


### [323] [Fine-Grained Representation for Lane Topology Reasoning](https://arxiv.org/abs/2511.12590)
*Guoqing Xu,Yiheng Li,Yang Yang*

Main category: cs.CV

TL;DR: Fine-Grained lane topology reasoning framework (TopoFG) is introduced to precisely model complex lane structures through a method involving spatial and sequential priors, boundary-point topology reasoning, and denoising strategies.


<details>
  <summary>Details</summary>
Motivation: Autonomous driving requires accurate lane topology modeling since navigation and control decisions depend on reliable topology predictions, but existing methods fail to handle complex lane structures effectively.

Method: TopoFG employs three phases: Hierarchical Prior Extractor (HPE) to extract spatial and sequential priors; Region-Focused Decoder (RFD) to construct and refine fine-grained queries; and Robust Boundary-Point Topology Reasoning (RBTR) to model lane connectivity and denoise topology predictions.

Result: Extensive experiments on the OpenLane-V2 benchmark showcase TopoFG's superior performance, achieving state-of-the-art metrics with OLS of 48.0% on subsetA and 45.4% on subsetB.

Conclusion: TopoFG precisely models complex lane structures, improves lane topology prediction reliability, and outperforms existing methods in autonomous driving benchmarks.

Abstract: Precise modeling of lane topology is essential for autonomous driving, as it directly impacts navigation and control decisions.Existing methods typically represent each lane with a single query and infer topological connectivity based on the similarity between lane queries.However, this kind of design struggles to accurately model complex lane structures, leading to unreliable topology prediction.In this view, we propose a Fine-Grained lane topology reasoning framework (TopoFG).It divides the procedure from bird's-eye-view (BEV) features to topology prediction via fine-grained queries into three phases, i.e., Hierarchical Prior Extractor (HPE), Region-Focused Decoder (RFD), and Robust Boundary-Point Topology Reasoning (RBTR).Specifically, HPE extracts global spatial priors from the BEV mask and local sequential priors from in-lane keypoint sequences to guide subsequent fine-grained query modeling.RFD constructs fine-grained queries by integrating the spatial and sequential priors. It then samples reference points in RoI regions of the mask and applies cross-attention with BEV features to refine the query representations of each lane.RBTR models lane connectivity based on boundary-point query features and further employs a topological denoising strategy to reduce matching ambiguity.By integrating spatial and sequential priors into fine-grained queries and applying a denoising strategy to boundary-point topology reasoning, our method precisely models complex lane structures and delivers trustworthy topology predictions.Extensive experiments on the OpenLane-V2 benchmark demonstrate that TopoFG achieves new state-of-the-art performance, with an OLS of 48.0% on subsetA and 45.4% on subsetB.

</details>


### [324] [Seg-VAR: Image Segmentation with Visual Autoregressive Modeling](https://arxiv.org/abs/2511.12594)
*Rongkun Zheng,Lu Qi,Xi Chen,Yi Wang,Kun Wang,Hengshuang Zhao*

Main category: cs.CV

TL;DR: This paper introduces Seg-VAR, a new framework for segmentation as a conditional autoregressive mask generation problem, achieving improved results over existing methods.


<details>
  <summary>Details</summary>
Motivation: While autoregressive modeling has shown promise in image generation, its potential for precise segmentation—a crucial spatial perception task—remains unexplored.

Method: The paper proposes a new framework, Seg-VAR, which includes an image encoder, spatial-aware segmentation latent (seglat) encoder, and a decoder. It uses a multi-stage training strategy involving joint training, latent transformation refinement, and alignment of latents.

Result: Seg-VAR outperforms both discriminative and generative methods in segmentation tasks across benchmarks.

Conclusion: Seg-VAR reframes segmentation as a hierarchical prediction task, demonstrating the integration of autoregressive reasoning into vision systems and achieving significant performance improvements.

Abstract: While visual autoregressive modeling (VAR) strategies have shed light on image generation with the autoregressive models, their potential for segmentation, a task that requires precise low-level spatial perception, remains unexplored. Inspired by the multi-scale modeling of classic Mask2Former-based models, we propose Seg-VAR, a novel framework that rethinks segmentation as a conditional autoregressive mask generation problem. This is achieved by replacing the discriminative learning with the latent learning process. Specifically, our method incorporates three core components: (1) an image encoder generating latent priors from input images, (2) a spatial-aware seglat (a latent expression of segmentation mask) encoder that maps segmentation masks into discrete latent tokens using a location-sensitive color mapping to distinguish instances, and (3) a decoder reconstructing masks from these latents. A multi-stage training strategy is introduced: first learning seglat representations via image-seglat joint training, then refining latent transformations, and finally aligning image-encoder-derived latents with seglat distributions. Experiments show Seg-VAR outperforms previous discriminative and generative methods on various segmentation tasks and validation benchmarks. By framing segmentation as a sequential hierarchical prediction task, Seg-VAR opens new avenues for integrating autoregressive reasoning into spatial-aware vision systems. Code will be available at https://github.com/rkzheng99/Seg-VAR.

</details>


### [325] [LoRA-Enhanced Vision Transformer for Single Image based Morphing Attack Detection via Knowledge Distillation from EfficientNet](https://arxiv.org/abs/2511.12602)
*Ria Shekhawat,Sushrut Patwardhan,Raghavendra Ramachandra,Praveen Kumar Chandaliya,Kishor P. Upla*

Main category: cs.CV

TL;DR: The paper addresses vulnerability of Face Recognition Systems to morphing attacks and introduces a novel single-image morphing attack detection approach using a teacher-student model framework with LoRA integration.


<details>
  <summary>Details</summary>
Motivation: Face Recognition Systems are important for security but are susceptible to morphing attacks, necessitating robust detection methods.

Method: The approach utilizes a teacher-student model framework where a CNN-based teacher model refines a ViT-based student model, integrated with Low-Rank Adaptation (LoRA) for computational efficiency.

Result: Experiments conducted on a morphing dataset involving ten morphing algorithms show the proposed method outperforms six state-of-the-art detection methods, ensuring high performance and efficiency.

Conclusion: The proposed S-MAD approach efficiently detects morphing attacks with superior performance over existing techniques, solving key vulnerabilities in Face Recognition Systems.

Abstract: Face Recognition Systems (FRS) are critical for security but remain vulnerable to morphing attacks, where synthetic images blend biometric features from multiple individuals. We propose a novel Single-Image Morphing Attack Detection (S-MAD) approach using a teacher-student framework, where a CNN-based teacher model refines a ViT-based student model. To improve efficiency, we integrate Low-Rank Adaptation (LoRA) for fine-tuning, reducing computational costs while maintaining high detection accuracy. Extensive experiments are conducted on a morphing dataset built from three publicly available face datasets, incorporating ten different morphing generation algorithms to assess robustness. The proposed method is benchmarked against six state-of-the-art S-MAD techniques, demonstrating superior detection performance and computational efficiency.

</details>


### [326] [Pixels or Positions? Benchmarking Modalities in Group Activity Recognition](https://arxiv.org/abs/2511.12606)
*Drishya Karki,Merey Ramazanova,Anthony Cioppa,Silvio Giancola,Bernard Ghanem*

Main category: cs.CV

TL;DR: The paper introduces SoccerNet-GAR, a multimodal dataset with synchronized broadcast video and player tracking data to benchmark group activity recognition (GAR) and demonstrates that tracking-based methods outperform video-based counterparts.


<details>
  <summary>Details</summary>
Motivation: To evaluate which modality, video or trajectory tracking, performs better for group activity recognition and create a benchmark for standardized comparison.

Method: The study developed SoccerNet-GAR using data from the 2022 football World Cup. It synchronized and annotated group activities for two modalities—broadcast video and player tracking. It proposed a novel role-aware graph-based classifier for tracking-based GAR.

Result: The tracking model achieved a balanced accuracy of 67.2%, surpassing the video-based model's 58.1%, with faster training (4.25 times) and significantly fewer parameters (197K vs 86.3M).

Conclusion: The study highlights the superiority of tracking-based methods over video-based ones for GAR in terms of accuracy and efficiency, emphasizing the significance of the choice of modality and modeling approach in this area.

Abstract: Group Activity Recognition (GAR) is well studied on the video modality for surveillance and indoor team sports (e.g., volleyball, basketball). Yet, other modalities such as agent positions and trajectories over time, i.e. tracking, remain comparatively under-explored despite being compact, agent-centric signals that explicitly encode spatial interactions. Understanding whether pixel (video) or position (tracking) modalities leads to better group activity recognition is therefore important to drive further research on the topic. However, no standardized benchmark currently exists that aligns broadcast video and tracking data for the same group activities, leading to a lack of apples-to-apples comparison between these modalities for GAR. In this work, we introduce SoccerNet-GAR, a multimodal dataset built from the $64$ matches of the football World Cup 2022. Specifically, the broadcast videos and player tracking modalities for $94{,}285$ group activities are synchronized and annotated with $10$ categories. Furthermore, we define a unified evaluation protocol to benchmark two strong unimodal approaches: (i) a competitive video-based classifiers and (ii) a tracking-based classifiers leveraging graph neural networks. In particular, our novel role-aware graph architecture for tracking-based GAR directly encodes tactical structure through positional edges and temporal attention. Our tracking model achieves $67.2\%$ balanced accuracy compared to $58.1\%$ for the best video baseline, while training $4.25 \times$ faster with $438 \times$ fewer parameters ($197K$ \vs $86.3M$). This study provides new insights into the relative strengths of pixels and positions for group activity recognition. Overall, it highlights the importance of modality choice and role-aware modeling for GAR.

</details>


### [327] [Open-World Test-Time Adaptation with Hierarchical Feature Aggregation and Attention Affine](https://arxiv.org/abs/2511.12607)
*Ziqiong Liu,Yushun Tang,Junyang Ji,Zhihai He*

Main category: cs.CV

TL;DR: The paper proposes a Hierarchical Ladder Network (HLN) and an Attention Affine Network (AAN) to address challenges in test-time adaptation (TTA) under out-of-distribution scenarios, enhancing classification accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Models often encounter unseen categories during testing in real-world scenarios, leading to misclassification and impairing adaptation, which affects overall performance.

Method: The approach includes an HLN for extracting OOD features from Transformer layers, a probability fusion for OOD detection, adaptive refinement of self-attention through AAN, and a weighted entropy mechanism to suppress low-confidence samples.

Result: Experimental results on benchmark datasets show significant performance improvement in classification accuracy and robustness under domain shift.

Conclusion: The proposed methods effectively enhance TTA by improving OOD detection and adaptability, making models more robust to sample distribution and domain changes.

Abstract: Test-time adaptation (TTA) refers to adjusting the model during the testing phase to cope with changes in sample distribution and enhance the model's adaptability to new environments. In real-world scenarios, models often encounter samples from unseen (out-of-distribution, OOD) categories. Misclassifying these as known (in-distribution, ID) classes not only degrades predictive accuracy but can also impair the adaptation process, leading to further errors on subsequent ID samples. Many existing TTA methods suffer substantial performance drops under such conditions. To address this challenge, we propose a Hierarchical Ladder Network that extracts OOD features from class tokens aggregated across all Transformer layers. OOD detection performance is enhanced by combining the original model prediction with the output of the Hierarchical Ladder Network (HLN) via weighted probability fusion. To improve robustness under domain shift, we further introduce an Attention Affine Network (AAN) that adaptively refines the self-attention mechanism conditioned on the token information to better adapt to domain drift, thereby improving the classification performance of the model on datasets with domain shift. Additionally, a weighted entropy mechanism is employed to dynamically suppress the influence of low-confidence samples during adaptation. Experimental results on benchmark datasets show that our method significantly improves the performance on the most widely used classification datasets.

</details>


### [328] [C3Net: Context-Contrast Network for Camouflaged Object Detection](https://arxiv.org/abs/2511.12627)
*Baber Jan,Aiman H. El-Maleh,Abdul Jabbar Siddiqui,Abdul Bais,Saeed Anwar*

Main category: cs.CV

TL;DR: Camouflaged object detection is challenging due to objects blending into surroundings; this paper proposes C3Net, a dual-pathway decoder architecture, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Camouflaged objects are difficult to detect due to factors like intrinsic similarity and environmental complexities, making traditional and modern detection methods ineffective.

Method: C3Net employs a dual-pathway decoder architecture with Edge Refinement Pathway, Contextual Localization Pathway, and Attentive Fusion Module for precise detection.

Result: C3Net achieves state-of-the-art detection metrics (S-measures: 0.898 COD10K, 0.904 CAMO, 0.913 NC4K) while being efficient.

Conclusion: Comprehensive architectural innovation is required to address multifaceted detection challenges beyond isolated improvements.

Abstract: Camouflaged object detection identifies objects that blend seamlessly with their surroundings through similar colors, textures, and patterns. This task challenges both traditional segmentation methods and modern foundation models, which fail dramatically on camouflaged objects. We identify six fundamental challenges in COD: Intrinsic Similarity, Edge Disruption, Extreme Scale Variation, Environmental Complexities, Contextual Dependencies, and Salient-Camouflaged Object Disambiguation. These challenges frequently co-occur and compound the difficulty of detection, requiring comprehensive architectural solutions. We propose C3Net, which addresses all challenges through a specialized dual-pathway decoder architecture. The Edge Refinement Pathway employs gradient-initialized Edge Enhancement Modules to recover precise boundaries from early features. The Contextual Localization Pathway utilizes our novel Image-based Context Guidance mechanism to achieve intrinsic saliency suppression without external models. An Attentive Fusion Module synergistically combines the two pathways via spatial gating. C3Net achieves state-of-the-art performance with S-measures of 0.898 on COD10K, 0.904 on CAMO, and 0.913 on NC4K, while maintaining efficient processing. C3Net demonstrates that complex, multifaceted detection challenges require architectural innovation, with specialized components working synergistically to achieve comprehensive coverage beyond isolated improvements. Code, model weights, and results are available at https://github.com/Baber-Jan/C3Net.

</details>


### [329] [Multivariate Diffusion Transformer with Decoupled Attention for High-Fidelity Mask-Text Collaborative Facial Generation](https://arxiv.org/abs/2511.12631)
*Yushe Cao,Dianxi Shi,Xing Fu,Xuechao Zou,Haikuo Peng,Xueqi Li,Chun Yu,Junliang Xing*

Main category: cs.CV

TL;DR: MDiTFace introduces a customized diffusion transformer for improved multimodal facial generation by offering advanced cross-modal feature interaction and reduced computational overhead.


<details>
  <summary>Details</summary>
Motivation: Conventional feature fusion in multimodal facial generation fails to ensure effective cross-modal interactions, resulting in suboptimal generation outcomes.

Method: MDiTFace uses a unified tokenization strategy alongside multivariate transformer blocks and a decoupled attention mechanism to handle multimodal inputs efficiently.

Result: MDiTFace achieves superior facial fidelity and conditional consistency compared to existing methods, while reducing computational overhead for mask conditions by over 94%.

Conclusion: The proposed approach successfully enhances multimodal facial generation performance with innovative architectural designs and computational optimizations.

Abstract: While significant progress has been achieved in multimodal facial generation using semantic masks and textual descriptions, conventional feature fusion approaches often fail to enable effective cross-modal interactions, thereby leading to suboptimal generation outcomes. To address this challenge, we introduce MDiTFace--a customized diffusion transformer framework that employs a unified tokenization strategy to process semantic mask and text inputs, eliminating discrepancies between heterogeneous modality representations. The framework facilitates comprehensive multimodal feature interaction through stacked, newly designed multivariate transformer blocks that process all conditions synchronously. Additionally, we design a novel decoupled attention mechanism by dissociating implicit dependencies between mask tokens and temporal embeddings. This mechanism segregates internal computations into dynamic and static pathways, enabling caching and reuse of features computed in static pathways after initial calculation, thereby reducing additional computational overhead introduced by mask condition by over 94% while maintaining performance. Extensive experiments demonstrate that MDiTFace significantly outperforms other competing methods in terms of both facial fidelity and conditional consistency.

</details>


### [330] [Denoising Vision Transformer Autoencoder with Spectral Self-Regularization](https://arxiv.org/abs/2511.12633)
*Xunzhi Xiang,Xingye Tian,Guiyu Zhang,Yabo Chen,Shaofeng Zhang,Xuebo Wang,Xin Tao,Qi Fan*

Main category: cs.CV

TL;DR: High-dimensional latent spaces in VAEs can hinder generative performance due to redundant high-frequency components. This study proposes Denoising-VAE with spectral self-regularization to improve generative quality and faster convergence.


<details>
  <summary>Details</summary>
Motivation: To address the optimization dilemma of VAEs where higher-dimensional latent spaces improve image reconstruction but reduce generative model performance.

Method: The paper introduces the spectral self-regularization strategy to suppress high-frequency noise in latent spaces and proposes a ViT-based Denoising-VAE, independent of external VFMs, along with a spectral alignment strategy.

Result: Denoising-VAE achieves state-of-the-art reconstruction quality (rFID = 0.28, PSNR = 27.26), competitive generation performance (gFID = 1.82), and allows diffusion models to converge about twice as fast on the ImageNet 256x256 benchmark compared to SD-VAE.

Conclusion: Denoising-VAE effectively balances reconstruction fidelity and generative performance, proving to be an efficient autoencoder optimized for generative tasks without reliance on external models.

Abstract: Variational autoencoders (VAEs) typically encode images into a compact latent space, reducing computational cost but introducing an optimization dilemma: a higher-dimensional latent space improves reconstruction fidelity but often hampers generative performance. Recent methods attempt to address this dilemma by regularizing high-dimensional latent spaces using external vision foundation models (VFMs). However, it remains unclear how high-dimensional VAE latents affect the optimization of generative models. To our knowledge, our analysis is the first to reveal that redundant high-frequency components in high-dimensional latent spaces hinder the training convergence of diffusion models and, consequently, degrade generation quality. To alleviate this problem, we propose a spectral self-regularization strategy to suppress redundant high-frequency noise while simultaneously preserving reconstruction quality. The resulting Denoising-VAE, a ViT-based autoencoder that does not rely on VFMs, produces cleaner, lower-noise latents, leading to improved generative quality and faster optimization convergence. We further introduce a spectral alignment strategy to facilitate the optimization of Denoising-VAE-based generative models. Our complete method enables diffusion models to converge approximately 2$\times$ faster than with SD-VAE, while achieving state-of-the-art reconstruction quality (rFID = 0.28, PSNR = 27.26) and competitive generation performance (gFID = 1.82) on the ImageNet 256$\times$256 benchmark.

</details>


### [331] [Medical Knowledge Intervention Prompt Tuning for Medical Image Classification](https://arxiv.org/abs/2511.12639)
*Ye Du,Nanxi Yu,Shujun Wang*

Main category: cs.CV

TL;DR: The paper proposes CILMP, a novel integration of Large Language Models (LLMs) and Vision-Language Models (VLMs) for medical image classification, improving prompt tuning efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning Vision-Language Models (VLMs) for medical tasks is resource-intensive, and existing prompt tuning methods fail to effectively capture disease-specific medical concepts.

Method: CILMP integrates LLMs into the prompt tuning process by extracting disease-specific features from LLMs, intervening in a linear subspace, and creating instance-adaptive prompts via conditional mechanisms.

Result: CILMP consistently outperformed state-of-the-art prompt tuning methods in diverse medical image datasets.

Conclusion: CILMP is effective and efficient for enhancing medical image classification, leveraging LLMs to incorporate domain-specific medical knowledge into VLM prompts.

Abstract: Vision-language foundation models (VLMs) have shown great potential in feature transfer and generalization across a wide spectrum of medical-related downstream tasks. However, fine-tuning these models is resource-intensive due to their large number of parameters. Prompt tuning has emerged as a viable solution to mitigate memory usage and reduce training time while maintaining competitive performance. Nevertheless, the challenge is that existing prompt tuning methods cannot precisely distinguish different kinds of medical concepts, which miss essentially specific disease-related features across various medical imaging modalities in medical image classification tasks. We find that Large Language Models (LLMs), trained on extensive text corpora, are particularly adept at providing this specialized medical knowledge. Motivated by this, we propose incorporating LLMs into the prompt tuning process. Specifically, we introduce the CILMP, Conditional Intervention of Large Language Models for Prompt Tuning, a method that bridges LLMs and VLMs to facilitate the transfer of medical knowledge into VLM prompts. CILMP extracts disease-specific representations from LLMs, intervenes within a low-rank linear subspace, and utilizes them to create disease-specific prompts. Additionally, a conditional mechanism is incorporated to condition the intervention process on each individual medical image, generating instance-adaptive prompts and thus enhancing adaptability. Extensive experiments across diverse medical image datasets demonstrate that CILMP consistently outperforms state-of-the-art prompt tuning methods, demonstrating its effectiveness. Code is available at https://github.com/usr922/cilmp.

</details>


### [332] [DPVO-QAT++: Heterogeneous QAT and CUDA Kernel Fusion for High-Performance Deep Patch Visual Odometry](https://arxiv.org/abs/2511.12653)
*Cheng Liao*

Main category: cs.CV

TL;DR: The paper presents a novel framework (DPVO-QAT++) for optimizing deep learning-based Visual SLAM systems, reducing memory overhead, latency, and improving efficiency while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the computational overhead restricting the deployment of deep learning-based Visual SLAM systems on resource-limited platforms.

Method: Introduced DPVO-QAT++, which combines learnable scale parameterization, heterogeneous precision design for front-end and back-end VO, and GPU-native kernel fusion for fake quantization, using custom CUDA kernels.

Result: Achieved significant improvements in memory reduction, processing speed, and FPS while maintaining trajectory accuracy on TartanAir and EuRoC datasets.

Conclusion: DPVO-QAT++ is an effective solution for bridging performance and efficiency, enabling deep Visual SLAM for embedded and resource-constrained platforms.

Abstract: Deep learning-based Visual SLAM (vSLAM) systems exhibit exceptional geometric reasoning capabilities, yet their prohibitive computational overhead severely restricts deployment on resource-constrained autonomous platforms. This paper presents a hierarchical quantization optimization framework, DPVO-QAT++ (DPVO-QAT++: Heterogeneous QAT and CUDA Kernel Fusion for High-Performance Deep Patch Visual Odometry). Through the synergistic integration of learnable scale parameterization, a heterogeneous precision design for the Visual Odometry (VO) front-end and back-end (front-end floating-point fake quantization with FP16/FP32; back-end full precision), and GPU-native kernel fusion for fake quantization (custom CUDA kernels), our framework significantly reduces memory footprint and increases processing speed while preserving the trajectory accuracy of the original model. On the TartanAir dataset, our framework achieves an average FPS increase of 52.1%, a 29.1% reduction in median latency, and a 64.9% reduction in peak GPU memory reservation, while maintaining trajectory accuracy (ATE) comparable to the original DPVO model across 32 validation sequences. On the EuRoC dataset, it realizes an average FPS increase of 30.1%, a 23.1% reduction in median latency, and a 37.7% reduction in peak GPU memory reservation, maintaining comparable trajectory accuracy (ATE) across 11 validation sequences. Experimental results demonstrate that DPVO-QAT++ effectively bridges the gap between high-precision deep VO and the efficiency requirements for practical deployment, offering a viable engineering paradigm for the application of this technology on real-world embedded platforms.
  Keywords: Visual Odometry, Heterogeneous Precision Architecture, Quantization-Aware Training, CUDA Kernel Fusion, Scale-Only Training, Deep Patch Visual Odometry, GPU-Native Kernel Fusion.

</details>


### [333] [Toward Real-world Text Image Forgery Localization: Structured and Interpretable Data Synthesis](https://arxiv.org/abs/2511.12658)
*Zeqin Yu,Haotao Xie,Jian Zhang,Jiangqun Ni,Wenkan Su,Jiwu Huang*

Main category: cs.CV

TL;DR: Proposes a framework (FSTS) using Fourier Series for improved synthesis of tampered text images, enhancing generalization in forgery localization methods.


<details>
  <summary>Details</summary>
Motivation: Current T-IFL methods lack generalization due to insufficient real-world data and distribution gaps caused by simplistic synthetic datasets.

Method: The paper introduces FSTS, a framework that collects and analyzes real tampering instances, builds a hierarchical modeling system inspired by Fourier Series, and generates diverse synthetic tampered data.

Result: Models trained on data generated by FSTS showed significantly better performance across real-world forgery detection tests.

Conclusion: FSTS enhances the training of T-IFL methods by providing realistic synthetic data, addressing gaps in generalization and scalability.

Abstract: Existing Text Image Forgery Localization (T-IFL) methods often suffer from poor generalization due to the limited scale of real-world datasets and the distribution gap caused by synthetic data that fails to capture the complexity of real-world tampering. To tackle this issue, we propose Fourier Series-based Tampering Synthesis (FSTS), a structured and interpretable framework for synthesizing tampered text images. FSTS first collects 16,750 real-world tampering instances from five representative tampering types, using a structured pipeline that records human-performed editing traces via multi-format logs (e.g., video, PSD, and editing logs). By analyzing these collected parameters and identifying recurring behavioral patterns at both individual and population levels, we formulate a hierarchical modeling framework. Specifically, each individual tampering parameter is represented as a compact combination of basis operation-parameter configurations, while the population-level distribution is constructed by aggregating these behaviors. Since this formulation draws inspiration from the Fourier series, it enables an interpretable approximation using basis functions and their learned weights. By sampling from this modeled distribution, FSTS synthesizes diverse and realistic training data that better reflect real-world forgery traces. Extensive experiments across four evaluation protocols demonstrate that models trained with FSTS data achieve significantly improved generalization on real-world datasets. Dataset is available at \href{https://github.com/ZeqinYu/FSTS}{Project Page}.

</details>


### [334] [Hi-Reco: High-Fidelity Real-Time Conversational Digital Humans](https://arxiv.org/abs/2511.12662)
*Hongbin Huang,Junwei Li,Tianxin Xie,Zhuang Li,Cekai Weng,Yaodong Yang,Yue Luo,Li Liu,Jing Tang,Zhijing Shao,Zeyu Wang*

Main category: cs.CV

TL;DR: The paper presents a system for creating high-fidelity, real-time conversational digital humans that utilize advanced multimodal technologies for interactive applications.


<details>
  <summary>Details</summary>
Motivation: The main challenge lies in achieving both visual realism and real-time responsiveness in digital humans for interactive applications.

Method: The paper introduces a digital human system combining 3D avatars, expressive speech synthesis, dialogue generation, asynchronous execution pipelines, and retrieval-augmented methods for efficient and realistic interaction.

Result: The system successfully integrates multimodal components, enabling features like wake word detection, emotional prosody, and context-aware responses, leading to natural and responsive interactions.

Conclusion: The developed digital human system is highly effective for immersive use cases such as communication, education, and entertainment through its seamless multimodal integration and responsiveness.

Abstract: High-fidelity digital humans are increasingly used in interactive applications, yet achieving both visual realism and real-time responsiveness remains a major challenge. We present a high-fidelity, real-time conversational digital human system that seamlessly combines a visually realistic 3D avatar, persona-driven expressive speech synthesis, and knowledge-grounded dialogue generation. To support natural and timely interaction, we introduce an asynchronous execution pipeline that coordinates multi-modal components with minimal latency. The system supports advanced features such as wake word detection, emotionally expressive prosody, and highly accurate, context-aware response generation. It leverages novel retrieval-augmented methods, including history augmentation to maintain conversational flow and intent-based routing for efficient knowledge access. Together, these components form an integrated system that enables responsive and believable digital humans, suitable for immersive applications in communication, education, and entertainment.

</details>


### [335] [DensePercept-NCSSD: Vision Mamba towards Real-time Dense Visual Perception with Non-Causal State Space Duality](https://arxiv.org/abs/2511.12671)
*Tushar Anand,Advik Sinha,Abhijit Das*

Main category: cs.CV

TL;DR: The paper introduces a model for real-time, accurate optical flow and disparity estimation using a non-causal Mamba block-based approach.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of achieving high accuracy and low latency in real-time dense perception tasks, including optical flow and disparity estimation.

Method: Fuses pairwise input images in a non-causal selective state space using a Mamba block-based model to reduce inference times while ensuring accuracy and low GPU usage.

Result: The proposed model achieves efficient and accurate optical flow and disparity estimation in real-time scenarios and is validated in real-life settings.

Conclusion: The model serves as a viable tool for unified 3D dense perception tasks, balancing efficiency and accuracy. The code is available publicly for further research and application.

Abstract: In this work, we propose an accurate and real-time optical flow and disparity estimation model by fusing pairwise input images in the proposed non-causal selective state space for dense perception tasks. We propose a non-causal Mamba block-based model that is fast and efficient and aptly manages the constraints present in a real-time applications. Our proposed model reduces inference times while maintaining high accuracy and low GPU usage for optical flow and disparity map generation. The results and analysis, and validation in real-life scenario justify that our proposed model can be used for unified real-time and accurate 3D dense perception estimation tasks. The code, along with the models, can be found at https://github.com/vimstereo/DensePerceptNCSSD

</details>


### [336] [Appreciate the View: A Task-Aware Evaluation Framework for Novel View Synthesis](https://arxiv.org/abs/2511.12675)
*Saar Stern,Ido Sobol,Or Litany*

Main category: cs.CV

TL;DR: The paper tackles evaluation challenges in Novel View Synthesis (NVS) by introducing a task-aware framework with two metrics, effectively assessing image realism and faithfulness.


<details>
  <summary>Details</summary>
Motivation: Evaluating the reliability of NVS-generated images remains difficult, as current metrics often misjudge realism and viewpoint faithfulness.

Method: A two-metric task-aware framework derived from Zero123 features with reference-based ($D_{PRISM}$) and reference-free ($MMD_{PRISM}$) approaches is proposed.

Result: Proposed metrics align with human preferences in identifying incorrect images and effectively rank models across benchmarks.

Conclusion: The framework bridges gaps in NVS evaluation, enabling reliable synthesis quality assessment, fostering better advancements in the field.

Abstract: The goal of Novel View Synthesis (NVS) is to generate realistic images of a given content from unseen viewpoints. But how can we trust that a generated image truly reflects the intended transformation? Evaluating its reliability remains a major challenge. While recent generative models, particularly diffusion-based approaches, have significantly improved NVS quality, existing evaluation metrics struggle to assess whether a generated image is both realistic and faithful to the source view and intended viewpoint transformation. Standard metrics, such as pixel-wise similarity and distribution-based measures, often mis-rank incorrect results as they fail to capture the nuanced relationship between the source image, viewpoint change, and generated output. We propose a task-aware evaluation framework that leverages features from a strong NVS foundation model, Zero123, combined with a lightweight tuning step to enhance discrimination. Using these features, we introduce two complementary evaluation metrics: a reference-based score, $D_{\text{PRISM}}$, and a reference-free score, $\text{MMD}_{\text{PRISM}}$. Both reliably identify incorrect generations and rank models in agreement with human preference studies, addressing a fundamental gap in NVS evaluation. Our framework provides a principled and practical approach to assessing synthesis quality, paving the way for more reliable progress in novel view synthesis. To further support this goal, we apply our reference-free metric to six NVS methods across three benchmarks: Toys4K, Google Scanned Objects (GSO), and OmniObject3D, where $\text{MMD}_{\text{PRISM}}$ produces a clear and stable ranking, with lower scores consistently indicating stronger models.

</details>


### [337] [BridgeEQA: Virtual Embodied Agents for Real Bridge Inspections](https://arxiv.org/abs/2511.12676)
*Subin Varghese,Joshua Gao,Asad Ur Rahman,Vedhus Hoskere*

Main category: cs.CV

TL;DR: The paper introduces BridgeEQA, a benchmark for Embodied Question Answering (EQA) in infrastructure inspection, featuring real-world data and unique evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve embodied agents' ability to answer questions about their surroundings in realistic settings by addressing the scarcity of practical benchmarks.

Method: The authors introduce BridgeEQA, a dataset of 2,200 open-vocabulary question-answer pairs from real-world bridge inspections, and propose the EMVR method for sequential navigation and reasoning in scene graphs.

Result: State-of-the-art models showed significant performance gaps on the new benchmark. The EMVR method demonstrated superior performance over baseline models.

Conclusion: BridgeEQA provides a practical benchmark that advances EQA capabilities. The dataset and code are publicly available for further research.

Abstract: Deploying embodied agents that can answer questions about their surroundings in realistic real-world settings remains difficult, partly due to the scarcity of benchmarks that faithfully capture practical operating conditions. We propose infrastructure inspection as a compelling domain for open-vocabulary Embodied Question Answering (EQA): it naturally demands multi-scale reasoning, long-range spatial understanding, and complex semantic relationships, while offering unique evaluation advantages via standardized National Bridge Inventory (NBI) condition ratings (0-9), professional inspection reports, and egocentric imagery.
  We introduce BridgeEQA, a benchmark of 2,200 open-vocabulary question-answer pairs (in the style of OpenEQA) grounded in professional inspection reports across 200 real-world bridge scenes with 47.93 images on average per scene. Questions require synthesizing visual evidence across multiple images and aligning responses with NBI condition ratings. We further propose a new EQA metric Image Citation Relevance to evaluate the ability of a model to cite relevant images.
  Evaluations of state-of-the-art vision-language models reveal substantial performance gaps under episodic memory EQA settings. To address this, we propose Embodied Memory Visual Reasoning (EMVR), which formulates inspection as sequential navigation over an image-based scene graph: images are nodes, and an agent takes actions to traverse views, compare evidence, and reason within a Markov decision process. EMVR shows strong performance over the baselines. We publicly release both the dataset and code.

</details>


### [338] [R$^{2}$Seg: Training-Free OOD Medical Tumor Segmentation via Anatomical Reasoning and Statistical Rejection](https://arxiv.org/abs/2511.12691)
*Shuaike Shen,Ke Liu,Jiaqing Xie,Shangde Gao,Chunhua Shen,Ge Liu,Mireia Crispin-Ortuzar,Shangqi Gao*

Main category: cs.CV

TL;DR: R$^{2}$Seg is a robust, training-free framework addressing out-of-distribution challenges in medical image segmentation, improving performance without requiring parameter updates.


<details>
  <summary>Details</summary>
Motivation: Foundation models often fail in OOD scenarios for medical image segmentation, producing errors like fragmented false positives particularly for tumors.

Method: R$^{2}$Seg uses a two-stage "Reason-and-Reject" process: reasoning with LLM-guided anatomical cues to generate ROIs, followed by rejecting false candidates using statistical testing.

Result: The framework significantly enhances segmentation metrics like Dice, specificity, and sensitivity across various benchmarks compared to existing models.

Conclusion: R$^{2}$Seg is a promising approach for improving OOD segmentation without requiring model updates, usable as zero-update test-time augmentation.

Abstract: Foundation models for medical image segmentation struggle under out-of-distribution (OOD) shifts, often producing fragmented false positives on OOD tumors. We introduce R$^{2}$Seg, a training-free framework for robust OOD tumor segmentation that operates via a two-stage Reason-and-Reject process. First, the Reason step employs an LLM-guided anatomical reasoning planner to localize organ anchors and generate multi-scale ROIs. Second, the Reject step applies two-sample statistical testing to candidates generated by a frozen foundation model (BiomedParse) within these ROIs. This statistical rejection filter retains only candidates significantly different from normal tissue, effectively suppressing false positives. Our framework requires no parameter updates, making it compatible with zero-update test-time augmentation and avoiding catastrophic forgetting. On multi-center and multi-modal tumor segmentation benchmarks, R$^{2}$Seg substantially improves Dice, specificity, and sensitivity over strong baselines and the original foundation models. Code are available at https://github.com/Eurekashen/R2Seg.

</details>


### [339] [HEDGE: Hallucination Estimation via Dense Geometric Entropy for VQA with Vision-Language Models](https://arxiv.org/abs/2511.12693)
*Sushant Gautam,Michael A. Riegler,Pål Halvorsen*

Main category: cs.CV

TL;DR: HEDGE is a unified framework for detecting hallucinations in vision-language models (VLMs) that uses perturbations, clustering, and uncertainty metrics. It provides a reproducible and architecture-dependent performance evaluation.


<details>
  <summary>Details</summary>
Motivation: Vision-language models are powerful but often experience hallucination issues, affecting their reliability in tasks like visual question answering. A systematic framework to detect and quantify these hallucinations is needed.

Method: The HEDGE framework integrates visual perturbations, entailment and embedding-based clustering, robust uncertainty metrics, and a pipeline for analyzing hallucination trends across different architectures and prompts.

Result: Evaluating HEDGE on datasets such as VQA-RAD and KvasirVQA-x1 demonstrates that hallucination detectability varies by model architecture, with Qwen2.5-VL outperforming others due to dense tokenization. Embedding-based clustering generally produces stronger results. The VASE metric, paired with embedding clustering, performs best when using a moderate sampling budget.

Conclusion: HEDGE offers a compute-aware and principled solution for hallucination detection in VLMs, addressing core issues of model robustness and reliability. The hedge-bench library facilitates reproducible comparisons and benchmarks for the research community.

Abstract: Vision-language models (VLMs) enable open-ended visual question answering but remain prone to hallucinations. We present HEDGE, a unified framework for hallucination detection that combines controlled visual perturbations, semantic clustering, and robust uncertainty metrics. HEDGE integrates sampling, distortion synthesis, clustering (entailment- and embedding-based), and metric computation into a reproducible pipeline applicable across multimodal architectures.
  Evaluations on VQA-RAD and KvasirVQA-x1 with three representative VLMs (LLaVA-Med, Med-Gemma, Qwen2.5-VL) reveal clear architecture- and prompt-dependent trends. Hallucination detectability is highest for unified-fusion models with dense visual tokenization (Qwen2.5-VL) and lowest for architectures with restricted tokenization (Med-Gemma). Embedding-based clustering often yields stronger separation when applied directly to the generated answers, whereas NLI-based clustering remains advantageous for LLaVA-Med and for longer, sentence-level responses. Across configurations, the VASE metric consistently provides the most robust hallucination signal, especially when paired with embedding clustering and a moderate sampling budget (n ~ 10-15). Prompt design also matters: concise, label-style outputs offer clearer semantic structure than syntactically constrained one-sentence responses.
  By framing hallucination detection as a geometric robustness problem shaped jointly by sampling scale, prompt structure, model architecture, and clustering strategy, HEDGE provides a principled, compute-aware foundation for evaluating multimodal reliability. The hedge-bench PyPI library enables reproducible and extensible benchmarking, with full code and experimental resources available at https://github.com/Simula/HEDGE .

</details>


### [340] [X-VMamba: Explainable Vision Mamba](https://arxiv.org/abs/2511.12694)
*Mohamed A. Mabrok,Yalda Zafari*

Main category: cs.CV

TL;DR: The paper introduces a controllability-based interpretability framework to understand how Vision State Space Models (SSMs) handle spatial information, validating it with experiments in medical imaging and exploring cross-domain applications.


<details>
  <summary>Details</summary>
Motivation: Vision SSMs like Mamba achieve competitive performance with linear complexity but lack transparent attention mechanisms, making it hard to interpret their spatial information processing.

Method: The authors propose a controllability-based framework with two approaches: (1) Jacobian-based for measuring input influence, and (2) Gramian-based for diagonal SSMs, both operating with linear complexity.

Result: The framework reveals hierarchical feature refinement in Vision SSMs, with significant domain-specific patterns and influences observed in medical imaging data.

Conclusion: The proposed framework establishes a universal interpretability paradigm for Vision SSMs applicable across domains, with promising extensions to broader tasks in computer vision and NLP.

Abstract: State Space Models (SSMs), particularly the Mamba architecture, have recently emerged as powerful alternatives to Transformers for sequence modeling, offering linear computational complexity while achieving competitive performance. Yet, despite their effectiveness, understanding how these Vision SSMs process spatial information remains challenging due to the lack of transparent, attention-like mechanisms. To address this gap, we introduce a controllability-based interpretability framework that quantifies how different parts of the input sequence (tokens or patches) influence the internal state dynamics of SSMs. We propose two complementary formulations: a Jacobian-based method applicable to any SSM architecture that measures influence through the full chain of state propagation, and a Gramian-based approach for diagonal SSMs that achieves superior speed through closed-form analytical solutions. Both methods operate in a single forward pass with linear complexity, requiring no architectural modifications or hyperparameter tuning. We validate our framework through experiments on three diverse medical imaging modalities, demonstrating that SSMs naturally implement hierarchical feature refinement from diffuse low-level textures in early layers to focused, clinically meaningful patterns in deeper layers. Our analysis reveals domain-specific controllability signatures aligned with diagnostic criteria, progressive spatial selectivity across the network hierarchy, and the substantial influence of scanning strategies on attention patterns. Beyond medical imaging, we articulate applications spanning computer vision, natural language processing, and cross-domain tasks. Our framework establishes controllability analysis as a unified, foundational interpretability paradigm for SSMs across all domains. Code and analysis tools will be made available upon publication

</details>


### [341] [Counting Through Occlusion: Framework for Open World Amodal Counting](https://arxiv.org/abs/2511.12702)
*Safaeid Hossain Arib,Rabeya Akter,Abdul Monaf Chowdhury,Md Jubair Ahmed Sourov,Md Mehedi Hasan*

Main category: cs.CV

TL;DR: CountOCC introduces an innovative framework for object counting under occlusion by reconstructing occluded object features using multimodal and hierarchical guidance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome limitations of state-of-the-art object counting methods that fail under occlusion due to corrupted feature representations.

Method: The proposed method integrates spatial context from visible fragments with semantic priors from textual and visual embeddings, ensuring robust feature representations across pyramid levels. It also enforces attention consistency through a visual equivalence objective.

Result: CountOCC demonstrates significant MAE reductions, achieving superior performance over previous methods on occlusion-augmented FSC 147, CARPK, and CAPTUREReal datasets.

Conclusion: CountOCC effectively addresses occlusion challenges in object counting, showing broad applicability and state-of-the-art results in diverse visual domains.

Abstract: Object counting has achieved remarkable success on visible instances, yet state-of-the-art (SOTA) methods fail under occlusion, a pervasive challenge in real world deployment. This failure stems from a fundamental architectural limitation where backbone networks encode occluding surfaces rather than target objects, thereby corrupting the feature representations required for accurate enumeration. To address this, we present CountOCC, an amodal counting framework that explicitly reconstructs occluded object features through hierarchical multimodal guidance. Rather than accepting degraded encodings, we synthesize complete representations by integrating spatial context from visible fragments with semantic priors from text and visual embeddings, generating class-discriminative features at occluded locations across multiple pyramid levels. We further introduce a visual equivalence objective that enforces consistency in attention space, ensuring that both occluded and unoccluded views of the same scene produce spatially aligned gradient-based attention maps. Together, these complementary mechanisms preserve discriminative properties essential for accurate counting under occlusion. For rigorous evaluation, we establish occlusion-augmented versions of FSC 147 and CARPK spanning both structured and unstructured scenes. CountOCC achieves SOTA performance on FSC 147 with 26.72% and 20.80% MAE reduction over prior baselines under occlusion in validation and test, respectively. CountOCC also demonstrates exceptional generalization by setting new SOTA results on CARPK with 49.89% MAE reduction and on CAPTUREReal with 28.79% MAE reduction, validating robust amodal counting across diverse visual domains. Code will be released soon.

</details>


### [342] [FSDAM: Few-Shot Driving Attention Modeling via Vision-Language Coupling](https://arxiv.org/abs/2511.12708)
*Kaiser Hamid,Can Cui,Khandakar Ashrafi Akbar,Ziran Wang,Nade Liang*

Main category: cs.CV

TL;DR: The paper introduces FSDAM, a framework for predicting driver attention and generating captions using very few annotated examples, achieving high performance with minimal supervision.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenge of creating autonomous systems capable of understanding driver intent and explaining their actions, with a focus on reducing the dependency on extensive gaze datasets.

Method: The paper presents FSDAM, a dual-pathway architecture where spatial prediction and caption generation modules work independently yet align semantically using cross-modal techniques. It makes use of only ~100 annotated examples.

Result: FSDAM demonstrates competitive performance in attention prediction and context-aware caption generation, achieving effective zero-shot generalization across multiple driving benchmarks with minimal supervision.

Conclusion: The study shows that robust and explainable driver attention systems can be developed with limited annotated data, paving the way for practical applications in resource-constrained environments.

Abstract: Understanding where drivers look and why they shift their attention is essential for autonomous systems that read human intent and justify their actions. Most existing models rely on large-scale gaze datasets to learn these patterns; however, such datasets are labor-intensive to collect and time-consuming to curate. We present FSDAM (Few-Shot Driver Attention Modeling), a framework that achieves joint attention prediction and caption generation with approximately 100 annotated examples, two orders of magnitude fewer than existing approaches. Our approach introduces a dual-pathway architecture where separate modules handle spatial prediction and caption generation while maintaining semantic consistency through cross-modal alignment. Despite minimal supervision, FSDAM achieves competitive performance on attention prediction, generates coherent, and context-aware explanations. The model demonstrates robust zero-shot generalization across multiple driving benchmarks. This work shows that effective attention-conditioned generation is achievable with limited supervision, opening new possibilities for practical deployment of explainable driver attention systems in data-constrained scenarios.

</details>


### [343] [Backdoor Attacks on Open Vocabulary Object Detectors via Multi-Modal Prompt Tuning](https://arxiv.org/abs/2511.12735)
*Ankita Raj,Chetan Arora*

Main category: cs.CV

TL;DR: The paper introduces TrAP, a backdoor injection strategy exploiting prompt tuning in open-vocabulary object detectors (OVODs) for targeted attacks, achieving high success rates without compromising model generalization.


<details>
  <summary>Details</summary>
Motivation: To identify and address security vulnerabilities in open-vocabulary object detectors, especially given their increasing adoption in critical applications such as robotics and autonomous driving.

Method: Proposed TrAP, a malicious prompt tuning approach that jointly optimizes text and image prompt parameters alongside visual triggers to implant backdoors, utilizing curriculum-based training for effective small trigger activation.

Result: Experiments demonstrate the success of TrAP in enabling targeted attacks like misclassification or object disappearance while maintaining enhanced performance on clean images compared to zero-shot settings.

Conclusion: TrAP exposes security risks inherent in OVODs, emphasizing the need for robust defenses and revealing vulnerabilities in prompt tuning strategies.

Abstract: Open-vocabulary object detectors (OVODs) unify vision and language to detect arbitrary object categories based on text prompts, enabling strong zero-shot generalization to novel concepts. As these models gain traction in high-stakes applications such as robotics, autonomous driving, and surveillance, understanding their security risks becomes crucial. In this work, we conduct the first study of backdoor attacks on OVODs and reveal a new attack surface introduced by prompt tuning. We propose TrAP (Trigger-Aware Prompt tuning), a multi-modal backdoor injection strategy that jointly optimizes prompt parameters in both image and text modalities along with visual triggers. TrAP enables the attacker to implant malicious behavior using lightweight, learnable prompt tokens without retraining the base model weights, thus preserving generalization while embedding a hidden backdoor. We adopt a curriculum-based training strategy that progressively shrinks the trigger size, enabling effective backdoor activation using small trigger patches at inference. Experiments across multiple datasets show that TrAP achieves high attack success rates for both object misclassification and object disappearance attacks, while also improving clean image performance on downstream datasets compared to the zero-shot setting.

</details>


### [344] [Direct Visual Grounding by Directing Attention of Visual Tokens](https://arxiv.org/abs/2511.12738)
*Parsa Esmaeilkhani,Longin Jan Latecki*

Main category: cs.CV

TL;DR: The paper introduces a novel loss function, KLAL, to refine attention mechanisms in Vision Language Models (VLMs), improving tasks requiring visual token understanding.


<details>
  <summary>Details</summary>
Motivation: Understanding VLMs' lack of attention to visual tokens in the final LLM layers that leads to errors in visual tasks.

Method: Introducing KL attention loss (KLAL) to align attention between visual and language tokens using ground truth attention maps and combining it with standard next-token prediction (NTP).

Result: KLAL significantly enhances VLM performance in geometric tasks and referring expressions across synthetic and real-world data. A new dataset for evaluating line tracing capabilities further highlights performance disparities.

Conclusion: Direct attention supervision via KLAL leads to improved VLM task performance by enabling better visual token comprehension and reduces dependency on standard next-token prediction signals.

Abstract: Vision Language Models (VLMs) mix visual tokens and text tokens. A puzzling issue is the fact that visual tokens most related to the query receive little to no attention in the final layers of the LLM module of VLMs from the answer tokens, where all tokens are treated equally, in particular, visual and language tokens in the LLM attention layers. This fact may result in wrong answers to visual questions, as our experimental results confirm. It appears that the standard next-token prediction (NTP) loss provides an insufficient signal for directing attention to visual tokens. We hypothesize that a more direct supervision of the attention of visual tokens to corresponding language tokens in the LLM module of VLMs will lead to improved performance on visual tasks. To demonstrate that this is indeed the case, we propose a novel loss function that directly supervises the attention of visual tokens. It directly grounds the answer language tokens in images by directing their attention to the relevant visual tokens. This is achieved by aligning the attention distribution of visual tokens to ground truth attention maps with KL divergence. The ground truth attention maps are obtained from task geometry in synthetic cases or from standard grounding annotations (e.g., bounding boxes or point annotations) in real images, and are used inside the LLM for attention supervision without requiring new labels. The obtained KL attention loss (KLAL) when combined with NTP encourages VLMs to attend to relevant visual tokens while generating answer tokens. This results in notable improvements across geometric tasks, pointing, and referring expression comprehension on both synthetic and real-world data, as demonstrated by our experiments. We also introduce a new dataset to evaluate the line tracing abilities of VLMs. Surprisingly, even commercial VLMs do not perform well on this task.

</details>


### [345] [Deep Imbalanced Multi-Target Regression: 3D Point Cloud Voxel Content Estimation in Simulated Forests](https://arxiv.org/abs/2511.12740)
*Amirhossein Hassanzadeh,Bartosz Krawczyk,Michael Saunders,Rob Wible,Keith Krause,Dimah Dera,Jan van Aardt*

Main category: cs.CV

TL;DR: This study examines how low-level voxel content can be inferred from high-level voxelized LiDAR data using multi-target regression with Kernel Point Convolutions (KPConv) and a sensitivity analysis on voxel sizes.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the loss of fine-scale structural information in voxelized LiDAR data while exploring cost-effective ways to infer voxel content, particularly in complex forest canopy settings.

Method: A novel multi-target regression approach was applied using KPConv in combination with density-based relevance (DBR) to handle class imbalance. The study also conducted a sensitivity analysis on voxel sizes ranging from 0.25m to 2m.

Result: Smaller voxel sizes led to higher errors, especially in the canopy, whereas larger voxel sizes decreased error due to reduced variability. The outcome emphasizes that voxel size selection is critical and context-dependent.

Conclusion: The research advances deep imbalance learning for multi-target regression in 3D LiDAR point cloud data, showing that voxel size significantly influences error and that precise applications require careful voxel size selection.

Abstract: Voxelization is an effective approach to reduce the computational cost of processing Light Detection and Ranging (LiDAR) data, yet it results in a loss of fine-scale structural information. This study explores whether low-level voxel content information, specifically target occupancy percentage within a voxel, can be inferred from high-level voxelized LiDAR point cloud data collected from Digital Imaging and remote Sensing Image Generation (DIRSIG) software. In our study, the targets include bark, leaf, soil, and miscellaneous materials. We propose a multi-target regression approach in the context of imbalanced learning using Kernel Point Convolutions (KPConv). Our research leverages cost-sensitive learning to address class imbalance called density-based relevance (DBR). We employ weighted Mean Saquared Erorr (MSE), Focal Regression (FocalR), and regularization to improve the optimization of KPConv. This study performs a sensitivity analysis on the voxel size (0.25 - 2 meters) to evaluate the effect of various grid representations in capturing the nuances of the forest. This sensitivity analysis reveals that larger voxel sizes (e.g., 2 meters) result in lower errors due to reduced variability, while smaller voxel sizes (e.g., 0.25 or 0.5 meter) exhibit higher errors, particularly within the canopy, where variability is greatest. For bark and leaf targets, error values at smaller voxel size datasets (0.25 and 0.5 meter) were significantly higher than those in larger voxel size datasets (2 meters), highlighting the difficulty in accurately estimating within-canopy voxel content at fine resolutions. This suggests that the choice of voxel size is application-dependent. Our work fills the gap in deep imbalance learning models for multi-target regression and simulated datasets for 3D LiDAR point clouds of forests.

</details>


### [346] [SAGE: Saliency-Guided Contrastive Embeddings](https://arxiv.org/abs/2511.12744)
*Colton R. Crum,Adam Czajka*

Main category: cs.CV

TL;DR: The paper proposes SAGE, a novel loss function that uses human saliency in model training with contrastive embeddings, improving classification performance compared to existing saliency-based methods.


<details>
  <summary>Details</summary>
Motivation: To improve model generalization and alignment with human expertise by addressing challenges of unreliable saliency-guided training methods and shifting saliency integration from image space to latent space.

Method: The proposed method (SAGE) uses saliency-preserving and degrading augmentations, a contrastive triplet loss, and checks on logit distributions to guide models using human saliency embedded within their latent space during training.

Result: SAGE improves classification performance in both open- and closed-set scenarios compared to state-of-the-art saliency-based methods across various model architectures.

Conclusion: Integrating human saliency into latent space training enhances model generalization and accuracy across tasks, making SAGE an effective and broadly applicable approach to saliency-guided training.

Abstract: Integrating human perceptual priors into the training of neural networks has been shown to raise model generalization, serve as an effective regularizer, and align models with human expertise for applications in high-risk domains. Existing approaches to integrate saliency into model training often rely on internal model mechanisms, which recent research suggests may be unreliable. Our insight is that many challenges associated with saliency-guided training stem from the placement of the guidance approaches solely within the image space. Instead, we move away from the image space, use the model's latent space embeddings to steer human guidance during training, and we propose SAGE (Saliency-Guided Contrastive Embeddings): a loss function that integrates human saliency into network training using contrastive embeddings. We apply salient-preserving and saliency-degrading signal augmentations to the input and capture the changes in embeddings and model logits. We guide the model towards salient features and away from non-salient features using a contrastive triplet loss. Additionally, we perform a sanity check on the logit distributions to ensure that the model outputs match the saliency-based augmentations. We demonstrate a boost in classification performance across both open- and closed-set scenarios against SOTA saliency-based methods, showing SAGE's effectiveness across various backbones, and include experiments to suggest its wide generalization across tasks.

</details>


### [347] [Which Way from B to A: The role of embedding geometry in image interpolation for Stable Diffusion](https://arxiv.org/abs/2511.12757)
*Nicholas Karris,Luke Durell,Javier Flores,Tegan Emerson*

Main category: cs.CV

TL;DR: The paper proposes interpreting Stable Diffusion's embeddings as point clouds in Wasserstein space rather than as matrices, leading to smoother image interpolations via optimal transport.


<details>
  <summary>Details</summary>
Motivation: To improve the interpretation of CLIP embeddings and facilitate more natural transitions between embedding interpolations for rendering smoother images.

Method: Reframes the embedding interpolation problem as an optimal transport problem to compute geodesics, providing a natural transition through the embedding space.

Result: Optimal transport produces smoother, more coherent interpolations compared to standard methods, enhancing image quality.

Conclusion: Viewing embeddings as point clouds better captures embedding space geometry, enabling improved interpolation and image synthesis.

Abstract: It can be shown that Stable Diffusion has a permutation-invariance property with respect to the rows of Contrastive Language-Image Pretraining (CLIP) embedding matrices. This inspired the novel observation that these embeddings can naturally be interpreted as point clouds in a Wasserstein space rather than as matrices in a Euclidean space. This perspective opens up new possibilities for understanding the geometry of embedding space. For example, when interpolating between embeddings of two distinct prompts, we propose reframing the interpolation problem as an optimal transport problem. By solving this optimal transport problem, we compute a shortest path (or geodesic) between embeddings that captures a more natural and geometrically smooth transition through the embedding space. This results in smoother and more coherent intermediate (interpolated) images when rendered by the Stable Diffusion generative model. We conduct experiments to investigate this effect, comparing the quality of interpolated images produced using optimal transport to those generated by other standard interpolation methods. The novel optimal transport--based approach presented indeed gives smoother image interpolations, suggesting that viewing the embeddings as point clouds (rather than as matrices) better reflects and leverages the geometry of the embedding space.

</details>


### [348] [RoCoISLR: A Romanian Corpus for Isolated Sign Language Recognition](https://arxiv.org/abs/2511.12767)
*Cătălin-Alexandru Rîpanu,Andrei-Theodor Hotnog,Giulia-Stefania Imbrea,Dumitru-Clementin Cercel*

Main category: cs.CV

TL;DR: This paper introduces RoCoISLR, a standardized corpus for Romanian Isolated Sign Language Recognition with over 9,000 video samples and nearly 6,000 glosses. It benchmarks state-of-the-art video recognition models, finding transformer-based models outperform convolutional ones in accuracy.


<details>
  <summary>Details</summary>
Motivation: The lack of large-scale, standardized datasets for Romanian Isolated Sign Language Recognition significantly limits research and hinders effective communication solutions for the deaf community in Romania.

Method: This study introduces RoCoISLR, a comprehensive corpus, and evaluates seven state-of-the-art video recognition models under consistent experimental setups. Benchmark experiments compare performances and highlight challenges such as long-tail class distributions.

Result: Transformer-based models, such as Swin Transformer, outperform convolutional models, achieving a Top-1 accuracy of 34.1%. Results underscore difficulties due to long-tail distributions in low-resource sign languages.

Conclusion: RoCoISLR is a significant foundation for systematic research in Romanian Isolated Sign Language Recognition, enabling progress and addressing challenges in this low-resource domain.

Abstract: Automatic sign language recognition plays a crucial role in bridging the communication gap between deaf communities and hearing individuals; however, most available datasets focus on American Sign Language. For Romanian Isolated Sign Language Recognition (RoISLR), no large-scale, standardized dataset exists, which limits research progress. In this work, we introduce a new corpus for RoISLR, named RoCoISLR, comprising over 9,000 video samples that span nearly 6,000 standardized glosses from multiple sources. We establish benchmark results by evaluating seven state-of-the-art video recognition models-I3D, SlowFast, Swin Transformer, TimeSformer, Uniformer, VideoMAE, and PoseConv3D-under consistent experimental setups, and compare their performance with that of the widely used WLASL2000 corpus. According to the results, transformer-based architectures outperform convolutional baselines; Swin Transformer achieved a Top-1 accuracy of 34.1%. Our benchmarks highlight the challenges associated with long-tail class distributions in low-resource sign languages, and RoCoISLR provides the initial foundation for systematic RoISLR research.

</details>


### [349] [Lightweight Optimal-Transport Harmonization on Edge Devices](https://arxiv.org/abs/2511.12785)
*Maria Larchenko,Dmitry Guskov,Alexander Lobashev,Georgy Derevyanko*

Main category: cs.CV

TL;DR: This paper proposes a real-time color harmonization approach for augmented reality using a lightweight algorithm based on optimal transport theory, achieving superior results in AR applications.


<details>
  <summary>Details</summary>
Motivation: Existing color harmonization methods are not integrated into AR pipelines due to the lack of real-time solutions.

Method: The paper proposes MKL-Harmonizer, which trains a compact encoder to predict the Monge-Kantorovich transport map for color harmonization.

Result: The algorithm outperforms state-of-the-art methods on real AR composite images in terms of aggregated scores.

Conclusion: The method enables real-time integration of color harmonization in AR pipelines, supported by a newly released AR dataset and toolkit for further research.

Abstract: Color harmonization adjusts the colors of an inserted object so that it perceptually matches the surrounding image, resulting in a seamless composite. The harmonization problem naturally arises in augmented reality (AR), yet harmonization algorithms are not currently integrated into AR pipelines because real-time solutions are scarce. In this work, we address color harmonization for AR by proposing a lightweight approach that supports on-device inference. For this, we leverage classical optimal transport theory by training a compact encoder to predict the Monge-Kantorovich transport map. We benchmark our MKL-Harmonizer algorithm against state-of-the-art methods and demonstrate that for real composite AR images our method achieves the best aggregated score. We release our dedicated AR dataset of composite images with pixel-accurate masks and data-gathering toolkit to support further data acquisition by researchers.

</details>


### [350] [Enhancing Neuro-Oncology Through Self-Assessing Deep Learning Models for Brain Tumor Unified Model for MRI Segmentation](https://arxiv.org/abs/2511.12801)
*Andrew Zhou*

Main category: cs.CV

TL;DR: This paper improves brain tumor segmentation by introducing uncertainty prediction and combining tumor detection with healthy brain structure segmentation in one unified framework.


<details>
  <summary>Details</summary>
Motivation: Accurate brain tumor segmentation is crucial but current methods lack uncertainty estimates for clinical reliability and fail to include healthy brain structures near tumors.

Method: The paper proposes two major contributions: adding uncertainty prediction to nnUNet, and creating a unified model trained on both normal and cancer datasets for whole-brain segmentation.

Result: The framework achieves strong performance metrics with DSC scores of 0.81 for brain structures and 0.86 for tumors, while also providing voxel-wise uncertainty without sacrificing segmentation accuracy.

Conclusion: The proposed model uniquely combines tumor segmentation, healthy structure context, and uncertainty estimation, supporting better, informed clinical and surgical decision-making.

Abstract: Accurate segmentation of brain tumors is vital for diagnosis, surgical planning, and treatment monitoring. Deep learning has advanced on benchmarks, but two issues limit clinical use: no uncertainty estimates for errors and no segmentation of healthy brain structures around tumors for surgery. Current methods fail to unify tumor localization with anatomical context and lack confidence scores. This study presents an uncertainty-aware framework augmenting nnUNet with a channel for voxel-wise uncertainty. Trained on BraTS2023, it yields a correlation of 0.750 and RMSD of 0.047 for uncertainty without hurting tumor accuracy. It predicts uncertainty in one pass, with no extra networks or inferences, aiding clinical decisions. For whole-brain context, a unified model combines normal and cancer datasets, achieving a DSC of 0.81 for brain structures and 0.86 for tumor, with robust key-region performance. Combining both innovations gives the first model outputting tumor in natural surroundings plus an overlaid uncertainty map. Visual checks of outputs show uncertainty offers key insights to evaluate predictions and fix errors, helping informed surgical decisions from AI.

</details>


### [351] [MSRNet: A Multi-Scale Recursive Network for Camouflaged Object Detection](https://arxiv.org/abs/2511.12810)
*Leena Alghamdi,Muhammad Usman,Hafeez Anwar,Abdul Bais,Saeed Anwar*

Main category: cs.CV

TL;DR: The paper addresses the challenges of camouflaged object detection and introduces a Multi-Scale Recursive Network that outperforms current methods. It achieves state-of-the-art results on two datasets and ranks second on two others.


<details>
  <summary>Details</summary>
Motivation: The study aims to overcome the limitations of existing methods in detecting camouflaged objects, which struggle with complex scenarios like small object size, partial occlusion, and intricate backgrounds.

Method: The proposed Multi-Scale Recursive Network leverages a Pyramid Vision Transformer backbone with specialized Attention-Based Scale Integration Units for feature merging and Multi-Granularity Fusion Units for recursive feature refinement.

Result: The method achieves state-of-the-art performance on two benchmark datasets and ranks second on two other datasets for camouflaged object detection.

Conclusion: The proposed approach demonstrates significant advancements in detecting small and multiple camouflaged objects, providing a promising solution for the challenges in this computer vision task.

Abstract: Camouflaged object detection is an emerging and challenging computer vision task that requires identifying and segmenting objects that blend seamlessly into their environments due to high similarity in color, texture, and size. This task is further complicated by low-light conditions, partial occlusion, small object size, intricate background patterns, and multiple objects. While many sophisticated methods have been proposed for this task, current methods still struggle to precisely detect camouflaged objects in complex scenarios, especially with small and multiple objects, indicating room for improvement. We propose a Multi-Scale Recursive Network that extracts multi-scale features via a Pyramid Vision Transformer backbone and combines them via specialized Attention-Based Scale Integration Units, enabling selective feature merging. For more precise object detection, our decoder recursively refines features by incorporating Multi-Granularity Fusion Units. A novel recursive-feedback decoding strategy is developed to enhance global context understanding, helping the model overcome the challenges in this task. By jointly leveraging multi-scale learning and recursive feature optimization, our proposed method achieves performance gains, successfully detecting small and multiple camouflaged objects. Our model achieves state-of-the-art results on two benchmark datasets for camouflaged object detection and ranks second on the remaining two. Our codes, model weights, and results are available at \href{https://github.com/linaagh98/MSRNet}{https://github.com/linaagh98/MSRNet}.

</details>


### [352] [SAGA: Source Attribution of Generative AI Videos](https://arxiv.org/abs/2511.12834)
*Rohit Kundu,Vishal Mohanty,Hao Xiong,Shan Jia,Athula Balachandran,Amit K. Roy-Chowdhury*

Main category: cs.CV

TL;DR: SAGA is a framework designed for identifying the sources of AI-generated videos by analyzing their authenticity and specific generative models.


<details>
  <summary>Details</summary>
Motivation: To address the challenges posed by hyper-realistic synthetic videos and the limitations of current binary real/fake detectors.

Method: Developed a comprehensive multi-granular video attribution framework using a novel video transformer architecture and introduced a data-efficient pre-train-and-attribute strategy, along with Temporal Attention Signatures for interpretability.

Result: Outperformed existing methods with minimal labeled data, matched supervised performance, and provided benchmarks on synthetic video provenance through extensive experiments.

Conclusion: SAGA enables accurate, scalable, and interpretable AI-generated video source attribution, aiding forensic and regulatory efforts.

Abstract: The proliferation of generative AI has led to hyper-realistic synthetic videos, escalating misuse risks and outstripping binary real/fake detectors. We introduce SAGA (Source Attribution of Generative AI videos), the first comprehensive framework to address the urgent need for AI-generated video source attribution at a large scale. Unlike traditional detection, SAGA identifies the specific generative model used. It uniquely provides multi-granular attribution across five levels: authenticity, generation task (e.g., T2V/I2V), model version, development team, and the precise generator, offering far richer forensic insights. Our novel video transformer architecture, leveraging features from a robust vision foundation model, effectively captures spatio-temporal artifacts. Critically, we introduce a data-efficient pretrain-and-attribute strategy, enabling SAGA to achieve state-of-the-art attribution using only 0.5\% of source-labeled data per class, matching fully supervised performance. Furthermore, we propose Temporal Attention Signatures (T-Sigs), a novel interpretability method that visualizes learned temporal differences, offering the first explanation for why different video generators are distinguishable. Extensive experiments on public datasets, including cross-domain scenarios, demonstrate that SAGA sets a new benchmark for synthetic video provenance, providing crucial, interpretable insights for forensic and regulatory applications.

</details>


### [353] [Video Finetuning Improves Reasoning Between Frames](https://arxiv.org/abs/2511.12868)
*Ruiqi Yang,Tian Yun,Zihan Wang,Ellie Pavlick*

Main category: cs.CV

TL;DR: The paper introduces Visual Chain-of-Thought (vCoT), a method for transitional reasoning between video frames and evaluates its effectiveness on multimodal large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: Examine how introducing explicit reasoning processes through vCoT improves visual understanding in multimodal LLMs, specifically concerning extension from images to videos.

Method: The authors propose vCoT to model inter-frame reasoning and systematically analyze video finetuning in LLMs by comparing image-only and video-finetuned models.

Result: vCoT enhances performance in long-form video question answering for image-only models while providing minor improvements for video-finetuned ones, indicating implicit temporal reasoning by the latter.

Conclusion: Video-finetuned models better capture transitions and transfer their capabilities to relational reasoning tasks even in static scenarios, outperforming image-only baselines.

Abstract: Multimodal large language models (LLMs) have made rapid progress in visual understanding, yet their extension from images to videos often reduces to a naive concatenation of frame tokens. In this work, we investigate what video finetuning brings to multimodal LLMs. We propose Visual Chain-of-Thought (vCoT), an explicit reasoning process that generates transitional event descriptions between consecutive frames. Using vCoT, we systematically compare image-only LVLMs with their video-finetuned counterparts, both with and without access to these transitional cues. Our experiments show that vCoT significantly improves the performance of image-only models on long-form video question answering, while yielding only marginal gains for video-finetuned models. This suggests that the latter already capture frame-to-frame transitions implicitly. Moreover, we find that video models transfer this temporal reasoning ability to purely static settings, outperforming image models' baselines on relational visual reasoning tasks.

</details>


### [354] [View-aware Cross-modal Distillation for Multi-view Action Recognition](https://arxiv.org/abs/2511.12870)
*Trung Thanh Nguyen,Yasutomo Kawanishi,Vijay John,Takahiro Komamizu,Ichiro Ide*

Main category: cs.CV

TL;DR: This paper proposes ViCoKD, a method for addressing challenges in multi-view action recognition with partially overlapping sensor views and limited input modalities.


<details>
  <summary>Details</summary>
Motivation: To tackle the challenges in real-world multi-view setups, where actions may only be visible in a subset of views and annotations are sparse.

Method: The proposed ViCoKD framework uses cross-modal knowledge distillation with a cross-modal adapter and introduces a View-aware Consistency module to improve learning under limited input conditions.

Result: Experiments demonstrate that ViCoKD outperforms existing methods, achieving significant improvements and even surpassing the teacher model's performance in limited data scenarios.

Conclusion: ViCoKD is an effective solution for handling view misalignment and modality limitations in multi-view action recognition, demonstrating robustness in real-world environments.

Abstract: The widespread use of multi-sensor systems has increased research in multi-view action recognition. While existing approaches in multi-view setups with fully overlapping sensors benefit from consistent view coverage, partially overlapping settings where actions are visible in only a subset of views remain underexplored. This challenge becomes more severe in real-world scenarios, as many systems provide only limited input modalities and rely on sequence-level annotations instead of dense frame-level labels. In this study, we propose View-aware Cross-modal Knowledge Distillation (ViCoKD), a framework that distills knowledge from a fully supervised multi-modal teacher to a modality- and annotation-limited student. ViCoKD employs a cross-modal adapter with cross-modal attention, allowing the student to exploit multi-modal correlations while operating with incomplete modalities. Moreover, we propose a View-aware Consistency module to address view misalignment, where the same action may appear differently or only partially across viewpoints. It enforces prediction alignment when the action is co-visible across views, guided by human-detection masks and confidence-weighted Jensen-Shannon divergence between their predicted class distributions. Experiments on the real-world MultiSensor-Home dataset show that ViCoKD consistently outperforms competitive distillation methods across multiple backbones and environments, delivering significant gains and surpassing the teacher model under limited conditions.

</details>


### [355] [Simple Lines, Big Ideas: Towards Interpretable Assessment of Human Creativity from Drawings](https://arxiv.org/abs/2511.12880)
*Zihao Lin,Zhenshan Shi,Sasa Zhao,Hanwei Zhu,Lingyu Zhu,Baoliang Chen,Lei Mo*

Main category: cs.CV

TL;DR: The paper introduces a data-driven framework to automatically and interpretable assess creativity in drawings, utilizing content and style dimensions.


<details>
  <summary>Details</summary>
Motivation: Traditional creativity assessments rely on subjective human scoring, which is time-consuming and prone to bias. This motivates the need for an automated and objective method to evaluate creativity in drawings.

Method: The authors augment an existing dataset with annotations for content categories and propose a multi-modal, multi-task learning framework. The framework predicts creativity scores, categorizes content, and extracts stylistic features using a conditional learning mechanism.

Result: The proposed model outperforms existing regression techniques, delivers state-of-the-art performance, and provides interpretable visualizations aligned with human assessments.

Conclusion: This innovative framework enhances creativity assessments by reducing subjectivity, providing deeper insights into creativity dimensions, and setting benchmarks through publicly available code and data.

Abstract: Assessing human creativity through visual outputs, such as drawings, plays a critical role in fields including psychology, education, and cognitive science. However, current assessment practices still rely heavily on expert-based subjective scoring, which is both labor-intensive and inherently subjective. In this paper, we propose a data-driven framework for automatic and interpretable creativity assessment from drawings. Motivated by the cognitive understanding that creativity can emerge from both what is drawn (content) and how it is drawn (style), we reinterpret the creativity score as a function of these two complementary dimensions.Specifically, we first augment an existing creativity labeled dataset with additional annotations targeting content categories. Based on the enriched dataset, we further propose a multi-modal, multi-task learning framework that simultaneously predicts creativity scores, categorizes content types, and extracts stylistic features. In particular, we introduce a conditional learning mechanism that enables the model to adapt its visual feature extraction by dynamically tuning it to creativity-relevant signals conditioned on the drawing's stylistic and semantic cues.Experimental results demonstrate that our model achieves state-of-the-art performance compared to existing regression-based approaches and offers interpretable visualizations that align well with human judgments. The code and annotations will be made publicly available at https://github.com/WonderOfU9/CSCA_PRCV_2025

</details>


### [356] [ActVAR: Activating Mixtures of Weights and Tokens for Efficient Visual Autoregressive Generation](https://arxiv.org/abs/2511.12893)
*Kaixin Zhang,Ruiqing Yang,Yuan Zhang,Shan You,Tao Huang*

Main category: cs.CV

TL;DR: The paper introduces ActVAR, a more efficient dynamic enhancement framework for Visual Autoregressive models, reducing computational costs without performance loss.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the inefficiencies and escalating computational costs in Visual Autoregressive models while preserving their performance.

Method: The ActVAR method introduces dual sparsity via dynamic operations: lightweight expert subnetworks with token-specific routing and selective token computation using a learnable router and gated selector.

Result: ActVAR achieves a significant reduction in computational FLOPs (up to 21.2%) while maintaining minimal performance degradation on the ImageNet benchmark.

Conclusion: ActVAR enhances VAR models' efficiency with novel dynamic mechanisms, improving their computational performance without sacrificing accuracy.

Abstract: Visual Autoregressive (VAR) models enable efficient image generation via next-scale prediction but face escalating computational costs as sequence length grows. Existing static pruning methods degrade performance by permanently removing weights or tokens, disrupting pretrained dependencies. To address this, we propose ActVAR, a dynamic activation framework that introduces dual sparsity across model weights and token sequences to enhance efficiency without sacrificing capacity. ActVAR decomposes feedforward networks (FFNs) into lightweight expert sub-networks and employs a learnable router to dynamically select token-specific expert subsets based on content. Simultaneously, a gated token selector identifies high-update-potential tokens for computation while reconstructing unselected tokens to preserve global context and sequence alignment. Training employs a two-stage knowledge distillation strategy, where the original VAR model supervises the learning of routing and gating policies to align with pretrained knowledge. Experiments on the ImageNet $256\times 256$ benchmark demonstrate that ActVAR achieves up to $21.2\%$ FLOPs reduction with minimal performance degradation.

</details>


### [357] [Reconstructing 3D Scenes in Native High Dynamic Range](https://arxiv.org/abs/2511.12895)
*Kaixuan Zhang,Minxian Li,Mingwu Ren,Jiankang Deng,Xiatian Zhu*

Main category: cs.CV

TL;DR: This paper introduces NH-3DGS, the first method for 3D scene reconstruction using native HDR camera data, outperforming existing approaches in reconstruction quality and dynamic range preservation.


<details>
  <summary>Details</summary>
Motivation: HDR imaging is crucial for professional media creation, but current 3D reconstruction methods rely on LDR data, limiting their utility in professional workflows.

Method: The study proposes NH-3DGS, a technique leveraging native HDR data. It introduces a luminance-chromaticity decomposition for optimized reconstruction pipelines.

Result: NH-3DGS achieves superior results on synthetic and real multi-view HDR datasets, offering high-quality reconstructions while preserving dynamic range.

Conclusion: NH-3DGS enables professional-grade 3D reconstructions directly from native HDR data. Its advancements promise impact in HDR imaging applications.

Abstract: High Dynamic Range (HDR) imaging is essential for professional digital media creation, e.g., filmmaking, virtual production, and photorealistic rendering. However, 3D scene reconstruction has primarily focused on Low Dynamic Range (LDR) data, limiting its applicability to professional workflows. Existing approaches that reconstruct HDR scenes from LDR observations rely on multi-exposure fusion or inverse tone-mapping, which increase capture complexity and depend on synthetic supervision. With the recent emergence of cameras that directly capture native HDR data in a single exposure, we present the first method for 3D scene reconstruction that directly models native HDR observations. We propose {\bf Native High dynamic range 3D Gaussian Splatting (NH-3DGS)}, which preserves the full dynamic range throughout the reconstruction pipeline. Our key technical contribution is a novel luminance-chromaticity decomposition of the color representation that enables direct optimization from native HDR camera data. We demonstrate on both synthetic and real multi-view HDR datasets that NH-3DGS significantly outperforms existing methods in reconstruction quality and dynamic range preservation, enabling professional-grade 3D reconstruction directly from native HDR captures. Code and datasets will be made available.

</details>


### [358] [FDP: A Frequency-Decomposition Preprocessing Pipeline for Unsupervised Anomaly Detection in Brain MRI](https://arxiv.org/abs/2511.12899)
*Hao Li,Zhenfeng Zhuang,Jingyu Lin,Yu Liu,Yifei Chen,Qiong Peng,Lequan Yu,Liansheng Wang*

Main category: cs.CV

TL;DR: This paper introduces a Frequency-Decomposition Preprocessing (FDP) framework for unsupervised anomaly detection (UAD) in brain MRI, leveraging frequency-domain analysis to improve anomaly detection and anatomical preservation.


<details>
  <summary>Details</summary>
Motivation: Brain anatomy is diverse, and annotated data for MRI is scarce, making it difficult for supervised anomaly detection approaches to excel. There is a need for effective methods to advance UAD capabilities while addressing biophysical fidelity and morphological complexity.

Method: The study performs frequency-domain analysis of pathological signatures, identifies unique frequency properties of anomalies, and develops the FDP framework to suppress pathology and preserve anatomy during frequency-domain reconstruction.

Result: The FDP framework consistently enhances anomaly detection performance across architectures and simulation techniques, including a 17.63% DICE score improvement when combined with LDM. Experimental results show robust improvements across baselines.

Conclusion: The FDP framework sets a new benchmark by leveraging frequency-domain data for UAD in brain MRI, successfully improving the anomaly detection process without compromising diagnostic fidelity. It can be seamlessly integrated with existing methods.

Abstract: Due to the diversity of brain anatomy and the scarcity of annotated data, supervised anomaly detection for brain MRI remains challenging, driving the development of unsupervised anomaly detection (UAD) approaches. Current UAD methods typically utilize artificially generated noise perturbations on healthy MRIs to train generative models for normal anatomy reconstruction, enabling anomaly detection via residual mapping. However, such simulated anomalies lack the biophysical fidelity and morphological complexity characteristic of true clinical lesions. To advance UAD in brain MRI, we conduct the first systematic frequency-domain analysis of pathological signatures, revealing two key properties: (1) anomalies exhibit unique frequency patterns distinguishable from normal anatomy, and (2) low-frequency signals maintain consistent representations across healthy scans. These insights motivate our Frequency-Decomposition Preprocessing (FDP) framework, the first UAD method to leverage frequency-domain reconstruction for simultaneous pathology suppression and anatomical preservation. FDP can integrate seamlessly with existing anomaly simulation techniques, consistently enhancing detection performance across diverse architectures while maintaining diagnostic fidelity. Experimental results demonstrate that FDP consistently improves anomaly detection performance when integrated with existing methods. Notably, FDP achieves a 17.63% increase in DICE score with LDM while maintaining robust improvements across multiple baselines. The code is available at https://github.com/ls1rius/MRI_FDP.

</details>


### [359] [DeepSport: A Multimodal Large Language Model for Comprehensive Sports Video Reasoning via Agentic Reinforcement Learning](https://arxiv.org/abs/2511.12908)
*Junbo Zou,Haotian Xia,Zhen Ye,Shengjie Zhang,Christopher Lai,Vicente Ordonez,Weining Shen,Hanjie Chen*

Main category: cs.CV

TL;DR: This paper introduces DeepSport, an end-to-end trained framework designed for multi-sport, multi-task video understanding by enabling iterative reasoning with videos.


<details>
  <summary>Details</summary>
Motivation: Current approaches in sports video understanding are narrowly focused, limited to single sports or specific tasks, and lack robust reasoning mechanisms.

Method: DeepSport employs active iterative reasoning using a specialized frame extraction tool, a data distillation pipeline synthesizing diverse Chain-of-Thought trajectories, and trains the model with Supervised Fine-Tuning and Reinforcement Learning using a gated tool-use reward.

Result: DeepSport achieves state-of-the-art performance on a testing benchmark of 6.7k questions, surpassing baselines of proprietary and open-source models.

Conclusion: DeepSport establishes a strong foundation for robust reasoning in domain-specific video understanding across diverse sports and tasks.

Abstract: Sports video understanding presents unique challenges, requiring models to perceive high-speed dynamics, comprehend complex rules, and reason over long temporal contexts. While Multimodal Large Language Models (MLLMs) have shown promise in genral domains, the current state of research in sports remains narrowly focused: existing approaches are either single-sport centric, limited to specific tasks, or rely on training-free paradigms that lack robust, learned reasoning process. To address this gap, we introduce DeepSport, the first end-to-end trained MLLM framework designed for multi-task, multi-sport video understanding. DeepSport shifts the paradigm from passive frame processing to active, iterative reasoning, empowering the model to ``think with videos'' by dynamically interrogating content via a specialized frame-extraction tool. To enable this, we propose a data distillation pipeline that synthesizes high-quality Chain-of-Thought (CoT) trajectories from 10 diverse data source, creating a unified resource of 78k training data. We then employ a two-stage training strategy, Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) with a novel gated tool-use reward, to optimize the model's reasoning process. Extensive experiments on the testing benchmark of 6.7k questions demonstrate that DeepSport achieves state-of-the-art performance, significantly outperforming baselines of both proprietary model and open-source models. Our work establishes a new foundation for domain-specific video reasoning to address the complexities of diverse sports.

</details>


### [360] [CASL: Curvature-Augmented Self-supervised Learning for 3D Anomaly Detection](https://arxiv.org/abs/2511.12909)
*Yaohua Zha,Xue Yuerong,Chunlin Fan,Yuansong Wang,Tao Dai,Ke Chen,Shu-Tao Xia*

Main category: cs.CV

TL;DR: This paper presents the Curvature-Augmented Self-supervised Learning (CASL) framework, improving 3D anomaly detection performance while retaining generalizability to other tasks.


<details>
  <summary>Details</summary>
Motivation: To create a 3D anomaly detection model generalizable across tasks since existing methods either lack effectiveness or are too task-specific.

Method: Proposes CASL, built on U-Net with multi-scale curvature prompts for point reconstruction, leveraging curvature for anomaly scoring.

Result: CASL achieves leading anomaly detection performance and generalizes well to tasks like point cloud classification.

Conclusion: CASL introduces a curvature-based framework that is effective and general-purpose for 3D understanding tasks.

Abstract: Deep learning-based 3D anomaly detection methods have demonstrated significant potential in industrial manufacturing. However, many approaches are specifically designed for anomaly detection tasks, which limits their generalizability to other 3D understanding tasks. In contrast, self-supervised point cloud models aim for general-purpose representation learning, yet our investigation reveals that these classical models are suboptimal at anomaly detection under the unified fine-tuning paradigm. This motivates us to develop a more generalizable 3D model that can effectively detect anomalies without relying on task-specific designs. Interestingly, we find that using only the curvature of each point as its anomaly score already outperforms several classical self-supervised and dedicated anomaly detection models, highlighting the critical role of curvature in 3D anomaly detection. In this paper, we propose a Curvature-Augmented Self-supervised Learning (CASL) framework based on a reconstruction paradigm. Built upon the classical U-Net architecture, our approach introduces multi-scale curvature prompts to guide the decoder in predicting the spatial coordinates of each point. Without relying on any dedicated anomaly detection mechanisms, it achieves leading detection performance through straightforward anomaly classification fine-tuning. Moreover, the learned representations generalize well to standard 3D understanding tasks such as point cloud classification. The code is available at https://github.com/zyh16143998882/CASL.

</details>


### [361] [Explore How to Inject Beneficial Noise in MLLMs](https://arxiv.org/abs/2511.12917)
*Ruishu Zhu,Sida Huang,Ziheng Jiao,Hongyuan Zhang*

Main category: cs.CV

TL;DR: The paper introduces Multimodal Noise Generator (MuNG), a novel fine-tuning strategy for MLLMs using beneficial random noise.


<details>
  <summary>Details</summary>
Motivation: Existing fine-tuning methods for MLLMs fail to address cross-modal heterogeneity, thus limiting their effectiveness.

Method: The authors use a noise generation approach, reformulating MLLMs reasoning through variational inference and injecting task-adaptive noise for better alignment.

Result: MuNG significantly improves cross-modal alignment, surpassing full fine-tuning and traditional methods, while requiring minimal parameter adjustments.

Conclusion: Injecting dynamic, task-specific noise enhances the efficiency and performance of MLLMs with reduced computational overhead.

Abstract: Multimodal Large Language Models (MLLMs) have played an increasingly important role in multimodal intelligence. However, the existing fine-tuning methods often ignore cross-modal heterogeneity, limiting their full potential. In this work, we propose a novel fine-tuning strategy by injecting beneficial random noise, which outperforms previous methods and even surpasses full fine-tuning, with minimal additional parameters. The proposed Multimodal Noise Generator (MuNG) enables efficient modality fine-tuning by injecting customized noise into the frozen MLLMs. Specifically, we reformulate the reasoning process of MLLMs from a variational inference perspective, upon which we design a multimodal noise generator that dynamically analyzes cross-modal relationships in image-text pairs to generate task-adaptive beneficial noise. Injecting this type of noise into the MLLMs effectively suppresses irrelevant semantic components, leading to significantly improved cross-modal representation alignment and enhanced performance on downstream tasks. Experiments on two mainstream MLLMs, QwenVL and LLaVA, demonstrate that our method surpasses full-parameter fine-tuning and other existing fine-tuning approaches, while requiring adjustments to only about $1\sim2\%$ additional parameters. The relevant code is uploaded in the supplementary.

</details>


### [362] [CoordAR: One-Reference 6D Pose Estimation of Novel Objects via Autoregressive Coordinate Map Generation](https://arxiv.org/abs/2511.12919)
*Dexin Zuo,Ang Li,Wei Wang,Wenxian Yu,Danping Zou*

Main category: cs.CV

TL;DR: CoordAR introduces an autoregressive framework for single-reference 6D object pose estimation, addressing challenges like symmetry and occlusion because of its probabilistic and tokenized approach.


<details>
  <summary>Details</summary>
Motivation: To simplify 6D pose estimation of new objects without 3D model dependencies and address limitations like symmetry and occlusions in existing models.

Method: CoordAR utilizes coordinates mapped as discrete tokens, a modality-decoupled encoding, and an autoregressive transformer decoder for 3D-3D correspondence generation.

Result: CoordAR surpasses previous methods in accuracy and robustness across various benchmarks, successfully handling symmetry and occlusions.

Conclusion: CoordAR proves effective in real-world scenarios, improving 6D pose estimation accuracy and robustness for unseen objects.

Abstract: Object 6D pose estimation, a crucial task for robotics and augmented reality applications, becomes particularly challenging when dealing with novel objects whose 3D models are not readily available. To reduce dependency on 3D models, recent studies have explored one-reference-based pose estimation, which requires only a single reference view instead of a complete 3D model. However, existing methods that rely on real-valued coordinate regression suffer from limited global consistency due to the local nature of convolutional architectures and face challenges in symmetric or occluded scenarios owing to a lack of uncertainty modeling. We present CoordAR, a novel autoregressive framework for one-reference 6D pose estimation of unseen objects. CoordAR formulates 3D-3D correspondences between the reference and query views as a map of discrete tokens, which is obtained in an autoregressive and probabilistic manner. To enable accurate correspondence regression, CoordAR introduces 1) a novel coordinate map tokenization that enables probabilistic prediction over discretized 3D space; 2) a modality-decoupled encoding strategy that separately encodes RGB appearance and coordinate cues; and 3) an autoregressive transformer decoder conditioned on both position-aligned query features and the partially generated token sequence. With these novel mechanisms, CoordAR significantly outperforms existing methods on multiple benchmarks and demonstrates strong robustness to symmetry, occlusion, and other challenges in real-world tests.

</details>


### [363] [Generative Photographic Control for Scene-Consistent Video Cinematic Editing](https://arxiv.org/abs/2511.12921)
*Huiqiang Sun,Liao Shen,Zhan Peng,Kun Wang,Size Wu,Yuhang Zang,Tianqi Liu,Zihao Huang,Xingyu Zeng,Zhiguo Cao,Wei Li,Chen Change Loy*

Main category: cs.CV

TL;DR: The paper introduces CineCtrl, a framework for cinematic editing in generative video models by controlling professional camera parameters like bokeh and shutter speed.


<details>
  <summary>Details</summary>
Motivation: Controlling fine photographic elements like depth of field and exposure in generative video models is difficult, limiting cinematic storytelling in existing methods.

Method: CineCtrl uses a decoupled cross-attention mechanism for independent control over camera parameters while maintaining scene consistency. A novel data generation strategy with simulated and real-world effects enables robust training.

Result: CineCtrl achieves high-fidelity, user-specified control over cinematic camera parameters in videos through extensive experiments.

Conclusion: CineCtrl significantly advances video editing by enabling fine-grained control of professional photographic elements, enhancing aesthetic storytelling capabilities.

Abstract: Cinematic storytelling is profoundly shaped by the artful manipulation of photographic elements such as depth of field and exposure. These effects are crucial in conveying mood and creating aesthetic appeal. However, controlling these effects in generative video models remains highly challenging, as most existing methods are restricted to camera motion control. In this paper, we propose CineCtrl, the first video cinematic editing framework that provides fine control over professional camera parameters (e.g., bokeh, shutter speed). We introduce a decoupled cross-attention mechanism to disentangle camera motion from photographic inputs, allowing fine-grained, independent control without compromising scene consistency. To overcome the shortage of training data, we develop a comprehensive data generation strategy that leverages simulated photographic effects with a dedicated real-world collection pipeline, enabling the construction of a large-scale dataset for robust model training. Extensive experiments demonstrate that our model generates high-fidelity videos with precisely controlled, user-specified photographic camera effects.

</details>


### [364] [Text2Traffic: A Text-to-Image Generation and Editing Method for Traffic Scenes](https://arxiv.org/abs/2511.12932)
*Feng Lv,Haoxuan Feng,Zilu Zhang,Chunlong Xia,Yanfeng Li*

Main category: cs.CV

TL;DR: The paper introduces a unified framework for text-driven image generation and editing, enhancing fidelity and text-image alignment in traffic scenes using multi-view data and mask mechanisms.


<details>
  <summary>Details</summary>
Motivation: Efforts in intelligent transportation systems require improved text-driven techniques to generate and edit traffic scene images, addressing challenges in fidelity, semantic richness, and alignment.

Method: The method involves a unified framework using controllable mask mechanisms for both generation and editing, multi-view geometric data, two-stage training (conceptual learning and fine-tuning), and mask-region-weighted loss strategies.

Result: Extensive experiments show top-tier performance in generating and editing traffic scene images, with improvements in visuals, alignment, and fidelity of small-scale elements.

Conclusion: The proposed solution provides a more robust and versatile approach for text-driven image processes in traffic applications, advancing both generation and editing capabilities.

Abstract: With the rapid advancement of intelligent transportation systems, text-driven image generation and editing techniques have demonstrated significant potential in providing rich, controllable visual scene data for applications such as traffic monitoring and autonomous driving. However, several challenges remain, including insufficient semantic richness of generated traffic elements, limited camera viewpoints, low visual fidelity of synthesized images, and poor alignment between textual descriptions and generated content. To address these issues, we propose a unified text-driven framework for both image generation and editing, leveraging a controllable mask mechanism to seamlessly integrate the two tasks. Furthermore, we incorporate both vehicle-side and roadside multi-view data to enhance the geometric diversity of traffic scenes. Our training strategy follows a two-stage paradigm: first, we perform conceptual learning using large-scale coarse-grained text-image data; then, we fine-tune with fine-grained descriptive data to enhance text-image alignment and detail quality. Additionally, we introduce a mask-region-weighted loss that dynamically emphasizes small yet critical regions during training, thereby substantially enhancing the generation fidelity of small-scale traffic elements. Extensive experiments demonstrate that our method achieves leading performance in text-based image generation and editing within traffic scenes.

</details>


### [365] [PFAvatar: Pose-Fusion 3D Personalized Avatar Reconstruction from Real-World Outfit-of-the-Day Photos](https://arxiv.org/abs/2511.12935)
*Dianbing Xi,Guoyuan An,Jingsen Zhu,Zhijian Liu,Yuan Liu,Ruiyuan Zhang,Jiayuan Lu,Rui Wang,Yuchi Huo*

Main category: cs.CV

TL;DR: The paper presents PFAvatar, a novel method for generating high-quality 3D avatars from photos with diverse poses and complex scenes, using a two-stage approach involving pose-aware diffusion and neural radiance field representation.


<details>
  <summary>Details</summary>
Motivation: To address challenges in reconstructing 3D avatars from highly varied photos, especially those with occlusions, complex backgrounds, and diverse poses, and improve fidelity, detail preservation, and speed of avatar creation.

Method: The method has two stages: (1) fine-tuning a pose-aware diffusion model integrated with pre-trained ControlNet and a novel loss term for better detail capture and training stability, and (2) reconstructing the 3D avatar using a NeRF-based representation optimized through specific sampling and multi-resolution techniques.

Result: PFAvatar achieves high reconstruction fidelity, preserves fine details like hair textures, and handles occlusions robustly. It completes personalization 48x faster than previous approaches.

Conclusion: PFAvatar outperforms state-of-the-art methods in creating detailed and robust 3D avatars for diverse applications such as virtual try-on, animation, and video reenactment, proving its practical and versatile value.

Abstract: We propose PFAvatar (Pose-Fusion Avatar), a new method that reconstructs high-quality 3D avatars from ``Outfit of the Day'' (OOTD) photos, which exhibit diverse poses, occlusions, and complex backgrounds. Our method consists of two stages: (1) fine-tuning a pose-aware diffusion model from few-shot OOTD examples and (2) distilling a 3D avatar represented by a neural radiance field (NeRF). In the first stage, unlike previous methods that segment images into assets (e.g., garments, accessories) for 3D assembly, which is prone to inconsistency, we avoid decomposition and directly model the full-body appearance. By integrating a pre-trained ControlNet for pose estimation and a novel Condition Prior Preservation Loss (CPPL), our method enables end-to-end learning of fine details while mitigating language drift in few-shot training. Our method completes personalization in just 5 minutes, achieving a 48$\times$ speed-up compared to previous approaches. In the second stage, we introduce a NeRF-based avatar representation optimized by canonical SMPL-X space sampling and Multi-Resolution 3D-SDS. Compared to mesh-based representations that suffer from resolution-dependent discretization and erroneous occluded geometry, our continuous radiance field can preserve high-frequency textures (e.g., hair) and handle occlusions correctly through transmittance. Experiments demonstrate that PFAvatar outperforms state-of-the-art methods in terms of reconstruction fidelity, detail preservation, and robustness to occlusions/truncations, advancing practical 3D avatar generation from real-world OOTD albums. In addition, the reconstructed 3D avatar supports downstream applications such as virtual try-on, animation, and human video reenactment, further demonstrating the versatility and practical value of our approach.

</details>


### [366] [ProtoAnomalyNCD: Prototype Learning for Multi-class Novel Anomaly Discovery in Industrial Scenarios](https://arxiv.org/abs/2511.12938)
*Botong Zhao,Qijun Shi,Shujing Lyu,Yue Lu*

Main category: cs.CV

TL;DR: This paper introduces ProtoAnomalyNCD, a prototype-learning-based framework for discovering and classifying multiple unseen industrial anomaly types using refined localization and contrastive learning methods.


<details>
  <summary>Details</summary>
Motivation: Existing industrial anomaly detection struggles with discovering and classifying multiple types of subtle anomalies effectively, which are essential for real-world applications.

Method: ProtoAnomalyNCD combines prototype learning with novel techniques like Grounded SAM for object localization and Anomaly-Map-Guided Attention blocks to enhance anomaly classification by leveraging refined priors and attention mechanisms.

Result: ProtoAnomalyNCD achieves superior performance compared to state-of-the-art methods on MVTec AD, MTD, and Real-IAD datasets.

Conclusion: The framework effectively addresses multi-type anomaly discovery and classification challenges while enabling task-level unification, providing a robust solution for unseen industrial anomaly detection.

Abstract: Existing industrial anomaly detection methods mainly determine whether an anomaly is present. However, real-world applications also require discovering and classifying multiple anomaly types. Since industrial anomalies are semantically subtle and current methods do not sufficiently exploit image priors, direct clustering approaches often perform poorly. To address these challenges, we propose ProtoAnomalyNCD, a prototype-learning-based framework for discovering unseen anomaly classes of multiple types that can be integrated with various anomaly detection methods. First, to suppress background clutter, we leverage Grounded SAM with text prompts to localize object regions as priors for the anomaly classification network. Next, because anomalies usually appear as subtle and fine-grained patterns on the product, we introduce an Anomaly-Map-Guided Attention block. Within this block, we design a Region Guidance Factor that helps the attention module distinguish among background, object regions, and anomalous regions. By using both localized product regions and anomaly maps as priors, the module enhances anomalous features while suppressing background noise and preserving normal features for contrastive learning. Finally, under a unified prototype-learning framework, ProtoAnomalyNCD discovers and clusters unseen anomaly classes while simultaneously enabling multi-type anomaly classification. We further extend our method to detect unseen outliers, achieving task-level unification. Our method outperforms state-of-the-art approaches on the MVTec AD, MTD, and Real-IAD datasets.

</details>


### [367] [Semi-Supervised High Dynamic Range Image Reconstructing via Bi-Level Uncertain Area Masking](https://arxiv.org/abs/2511.12939)
*Wei Jiang,Jiahao Cui,Yizheng Wu,Zhan Peng,Zhiyu Pan,Zhiguo Cao*

Main category: cs.CV

TL;DR: The paper proposes a semi-supervised approach to reconstruct high dynamic range (HDR) images from low dynamic range (LDR) bursts, reducing the reliance on HDR ground truth (GT) data through uncertainty-based masking of pseudo GTs.


<details>
  <summary>Details</summary>
Motivation: The difficulty in obtaining paired low dynamic range-high dynamic range (LDR-HDR) image data motivates this work to explore annotation-efficient HDR image reconstruction methods that achieve high performance with limited HDR ground truth (GT).

Method: The proposed method introduces a semi-supervised learning framework where a teacher model generates pseudo HDR ground truth (GT) for LDR images lacking GT, and a student model learns from these pseudo GTs. An uncertainty-based masking process discards unreliable pixels and patches from the pseudo GTs, enhancing the learning process by focusing on trusted areas.

Result: This method outperforms previous annotation-efficient approaches for HDR reconstruction and achieves comparable results to fully-supervised methods while using only 6.7% of HDR ground truth data.

Conclusion: The uncertainty-based masking approach effectively tackles the confirmation bias in semi-supervised HDR reconstruction, enabling high-quality results with significantly reduced annotation requirements and advancing the efficiency of HDR imaging techniques.

Abstract: Reconstructing high dynamic range (HDR) images from low dynamic range (LDR) bursts plays an essential role in the computational photography. Impressive progress has been achieved by learning-based algorithms which require LDR-HDR image pairs. However, these pairs are hard to obtain, which motivates researchers to delve into the problem of annotation-efficient HDR image reconstructing: how to achieve comparable performance with limited HDR ground truths (GTs). This work attempts to address this problem from the view of semi-supervised learning where a teacher model generates pseudo HDR GTs for the LDR samples without GTs and a student model learns from pseudo GTs. Nevertheless, the confirmation bias, i.e., the student may learn from the artifacts in pseudo HDR GTs, presents an impediment. To remove this impediment, an uncertainty-based masking process is proposed to discard unreliable parts of pseudo GTs at both pixel and patch levels, then the trusted areas can be learned from by the student. With this novel masking process, our semi-supervised HDR reconstructing method not only outperforms previous annotation-efficient algorithms, but also achieves comparable performance with up-to-date fully-supervised methods by using only 6.7% HDR GTs.

</details>


### [368] [Recurrent Autoregressive Diffusion: Global Memory Meets Local Attention](https://arxiv.org/abs/2511.12940)
*Taiye Chen,Zihan Ding,Anjian Li,Christina Zhang,Zeqi Xiao,Yisen Wang,Chi Jin*

Main category: cs.CV

TL;DR: The paper introduces the Recurrent Autoregressive Diffusion (RAD) framework to address forgetting and inconsistencies in long video generation using diffusion models.


<details>
  <summary>Details</summary>
Motivation: Enhance long-term video generation by addressing issues of memory retention and retrieval in diffusion models, which typically suffer from forgetting over long sequences.

Method: The paper integrates a recurrent neural network (RNN), specifically LSTM with attention, into the diffusion transformer framework for memory compression and retrieval. It introduces the RAD framework for consistent frame-wise autoregression across training and inference.

Result: RAD demonstrates improved performance in long video generation tasks on Memory Maze and Minecraft datasets, showcasing the efficiency of LSTM for sequence modeling.

Conclusion: The proposed RAD framework effectively enhances long-term video generation with consistent memory management, solving issues from existing diffusion-RNN approaches.

Abstract: Recent advancements in video generation have demonstrated the potential of using video diffusion models as world models, with autoregressive generation of infinitely long videos through masked conditioning. However, such models, usually with local full attention, lack effective memory compression and retrieval for long-term generation beyond the window size, leading to issues of forgetting and spatiotemporal inconsistencies. To enhance the retention of historical information within a fixed memory budget, we introduce a recurrent neural network (RNN) into the diffusion transformer framework. Specifically, a diffusion model incorporating LSTM with attention achieves comparable performance to state-of-the-art RNN blocks, such as TTT and Mamba2. Moreover, existing diffusion-RNN approaches often suffer from performance degradation due to training-inference gap or the lack of overlap across windows. To address these limitations, we propose a novel Recurrent Autoregressive Diffusion (RAD) framework, which executes frame-wise autoregression for memory update and retrieval, consistently across training and inference time. Experiments on Memory Maze and Minecraft datasets demonstrate the superiority of RAD for long video generation, highlighting the efficiency of LSTM in sequence modeling.

</details>


### [369] [T2I-Based Physical-World Appearance Attack against Traffic Sign Recognition Systems in Autonomous Driving](https://arxiv.org/abs/2511.12956)
*Chen Ma,Ningfei Wang,Junhao Zheng,Qing Guo,Qian Wang,Qi Alfred Chen,Chao Shen*

Main category: cs.CV

TL;DR: This paper introduces DiffSign, a T2I-based framework for stealthy and robust adversarial attacks on Traffic Sign Recognition systems, overcoming limitations of existing methods.


<details>
  <summary>Details</summary>
Motivation: Traffic Sign Recognition systems are vulnerable to adversarial attacks, which exploit flaws to misinterpret physical signs. Existing methods lack stealth and transferability to real-world systems, calling for improvement.

Method: DiffSign employs a T2I attack framework using CLIP-based loss, masked prompts, and novel style customization to generate effective, generalized, and inconspicuous adversarial appearances.

Result: DiffSign achieves an 83.3% average success rate in physical-world attack scenarios, demonstrating robustness across varying conditions and transferability.

Conclusion: DiffSign represents a significant improvement in attacking TSR systems, combining effectiveness, stealth, and practicality to address real-world adversarial vulnerabilities.

Abstract: Traffic Sign Recognition (TSR) systems play a critical role in Autonomous Driving (AD) systems, enabling real-time detection of road signs, such as STOP and speed limit signs. While these systems are increasingly integrated into commercial vehicles, recent research has exposed their vulnerability to physical-world adversarial appearance attacks. In such attacks, carefully crafted visual patterns are misinterpreted by TSR models as legitimate traffic signs, while remaining inconspicuous or benign to human observers. However, existing adversarial appearance attacks suffer from notable limitations. Pixel-level perturbation-based methods often lack stealthiness and tend to overfit to specific surrogate models, resulting in poor transferability to real-world TSR systems. On the other hand, text-to-image (T2I) diffusion model-based approaches demonstrate limited effectiveness and poor generalization to out-of-distribution sign types.
  In this paper, we present DiffSign, a novel T2I-based appearance attack framework designed to generate physically robust, highly effective, transferable, practical, and stealthy appearance attacks against TSR systems. To overcome the limitations of prior approaches, we propose a carefully designed attack pipeline that integrates CLIP-based loss and masked prompts to improve attack focus and controllability. We also propose two novel style customization methods to guide visual appearance and improve out-of-domain traffic sign attack generalization and attack stealthiness. We conduct extensive evaluations of DiffSign under varied real-world conditions, including different distances, angles, light conditions, and sign categories. Our method achieves an average physical-world attack success rate of 83.3%, leveraging DiffSign's high effectiveness in attack transferability.

</details>


### [370] [EndoSight AI: Deep Learning-Driven Real-Time Gastrointestinal Polyp Detection and Segmentation for Enhanced Endoscopic Diagnostics](https://arxiv.org/abs/2511.12962)
*Daniel Cavadia*

Main category: cs.CV

TL;DR: The paper introduces EndoSight AI, a deep learning system for detecting and segmenting gastrointestinal polyps with high precision and speed.


<details>
  <summary>Details</summary>
Motivation: Gastrointestinal polyp detection is critical for early cancer diagnosis and effective prevention during endoscopic procedures.

Method: The system was trained using the Hyper-Kvasir dataset and integrates clinically relevant metrics and a thermal-aware approach for robustness.

Result: EndoSight AI achieves 88.3% mAP for detection, 69% Dice coefficient for segmentation, and operates at over 35 frames per second on GPUs.

Conclusion: The solution is optimized for real-time endoscopy workflows, improving diagnostic accuracy and aiding clinical decisions in gastrointestinal healthcare.

Abstract: Precise and real-time detection of gastrointestinal polyps during endoscopic procedures is crucial for early diagnosis and prevention of colorectal cancer. This work presents EndoSight AI, a deep learning architecture developed and evaluated independently to enable accurate polyp localization and detailed boundary delineation. Leveraging the publicly available Hyper-Kvasir dataset, the system achieves a mean Average Precision (mAP) of 88.3% for polyp detection and a Dice coefficient of up to 69% for segmentation, alongside real-time inference speeds exceeding 35 frames per second on GPU hardware. The training incorporates clinically relevant performance metrics and a novel thermal-aware procedure to ensure model robustness and efficiency. This integrated AI solution is designed for seamless deployment in endoscopy workflows, promising to advance diagnostic accuracy and clinical decision-making in gastrointestinal healthcare.

</details>


### [371] [CalibrateMix: Guided-Mixup Calibration of Image Semi-Supervised Models](https://arxiv.org/abs/2511.12964)
*Mehrab Mustafy Rahman,Jayanth Mohan,Tiberiu Sosea,Cornelia Caragea*

Main category: cs.CV

TL;DR: CalibrateMix improves the reliability of Semi-Supervised Learning (SSL) models by enhancing calibration and maintaining accuracy using targeted mixup based on sample confidence levels.


<details>
  <summary>Details</summary>
Motivation: Current semi-supervised learning approaches often face calibration issues, leading to models making overly confident predictions. Addressing this is crucial for more reliable SSL applications.

Method: The authors propose CalibrateMix, which identifies easy-to-learn and hard-to-learn labeled and unlabeled samples, and employs a targeted mixup strategy to improve calibration in semi-supervised learning models.

Result: CalibrateMix exhibits better calibration with lower expected calibration error (ECE) and improved accuracy compared to other SSL techniques on benchmark image datasets.

Conclusion: Targeted mixup strategies like CalibrateMix can significantly enhance the reliability and performance of semi-supervised learning models in image classification tasks.

Abstract: Semi-supervised learning (SSL) has demonstrated high performance in image classification tasks by effectively utilizing both labeled and unlabeled data. However, existing SSL methods often suffer from poor calibration, with models yielding overconfident predictions that misrepresent actual prediction likelihoods. Recently, neural networks trained with {\tt mixup} that linearly interpolates random examples from the training set have shown better calibration in supervised settings. However, calibration of neural models remains under-explored in semi-supervised settings. Although effective in supervised model calibration, random mixup of pseudolabels in SSL presents challenges due to the overconfidence and unreliability of pseudolabels. In this work, we introduce CalibrateMix, a targeted mixup-based approach that aims to improve the calibration of SSL models while maintaining or even improving their classification accuracy. Our method leverages training dynamics of labeled and unlabeled samples to identify ``easy-to-learn'' and ``hard-to-learn'' samples, which in turn are utilized in a targeted mixup of easy and hard samples. Experimental results across several benchmark image datasets show that our method achieves lower expected calibration error (ECE) and superior accuracy compared to existing SSL approaches.

</details>


### [372] [GrOCE:Graph-Guided Online Concept Erasure for Text-to-Image Diffusion Models](https://arxiv.org/abs/2511.12968)
*Ning Han,Zhenyu Ge,Feng Han,Yuhua Sun,Chengqing Li,Jingjing Chen*

Main category: cs.CV

TL;DR: GrOCE introduces an efficient and adaptive framework for concept removal in diffusion models using a graph-based approach—achieving state-of-the-art performance without retraining.


<details>
  <summary>Details</summary>
Motivation: Overcoming costly fine-tuning and inaccurate concept removal in text-to-image diffusion models while adapting to new concepts.

Method: Graph-Guided Online Concept Erasure builds a dynamic graph of concepts for semantic reasoning and precise removal. It includes graph construction, adaptive clustering, and targeted edge severing.

Result: GrOCE improves concept erasure accuracy and adaptability, evidenced by state-of-the-art performance on Concept Similarity and Fréchet Inception Distance metrics.

Conclusion: GrOCE provides a training-free, efficient, and accurate solution for concept removal while safeguarding unrelated semantic integrity.

Abstract: Concept erasure aims to remove harmful, inappropriate, or copyrighted content from text-to-image diffusion models while preserving non-target semantics. However, existing methods either rely on costly fine-tuning or apply coarse semantic separation, often degrading unrelated concepts and lacking adaptability to evolving concept sets. To alleviate this issue, we propose Graph-Guided Online Concept Erasure (GrOCE), a training-free framework that performs precise and adaptive concept removal through graph-based semantic reasoning. GrOCE models concepts and their interrelations as a dynamic semantic graph, enabling principled reasoning over dependencies and fine-grained isolation of undesired content. It comprises three components: (1) Dynamic Topological Graph Construction for incremental graph building, (2) Adaptive Cluster Identification for multi-hop traversal with similarity-decay scoring, and (3) Selective Edge Severing for targeted edge removal while preserving global semantics. Extensive experiments demonstrate that GrOCE achieves state-of-the-art performance on Concept Similarity (CS) and Fréchet Inception Distance (FID) metrics, offering efficient, accurate, and stable concept erasure without retraining.

</details>


### [373] [HiFusion: Hierarchical Intra-Spot Alignment and Regional Context Fusion for Spatial Gene Expression Prediction from Histopathology](https://arxiv.org/abs/2511.12969)
*Ziqiao Weng,Yaoyu Fang,Jiahe Qian,Xinkun Wang,Lee AD Cooper,Weidong Cai,Bo Zhou*

Main category: cs.CV

TL;DR: Spatial transcriptomics involves predicting gene expression linked to tissue morphology but faces challenges in complexity and cost. HiFusion, a deep learning framework, improves performance by addressing biological heterogeneity and morphological noise using hierarchical intra-spot modeling and context-aware fusion.


<details>
  <summary>Details</summary>
Motivation: The need to bridge gene expression insights with tissue morphology efficiently while overcoming technical complexity, high costs, and existing computational limitations.

Method: Introduced HiFusion, a deep learning framework with hierarchical intra-spot modeling and cross-scale fusion for accurate gene expression prediction. Features include multi-resolution sub-patch decomposition and cross-attention mechanism.

Result: HiFusion achieves state-of-the-art performance in gene expression prediction across benchmark datasets, outperforming existing methods in both 2D and 3D scenarios.

Conclusion: HiFusion demonstrates its scalability, accuracy, and robustness, highlighting its potential for improving spatial transcriptomics in clinical settings.

Abstract: Spatial transcriptomics (ST) bridges gene expression and tissue morphology but faces clinical adoption barriers due to technical complexity and prohibitive costs. While computational methods predict gene expression from H&E-stained whole-slide images (WSIs), existing approaches often fail to capture the intricate biological heterogeneity within spots and are susceptible to morphological noise when integrating contextual information from surrounding tissue. To overcome these limitations, we propose HiFusion, a novel deep learning framework that integrates two complementary components. First, we introduce the Hierarchical Intra-Spot Modeling module that extracts fine-grained morphological representations through multi-resolution sub-patch decomposition, guided by a feature alignment loss to ensure semantic consistency across scales. Concurrently, we present the Context-aware Cross-scale Fusion module, which employs cross-attention to selectively incorporate biologically relevant regional context, thereby enhancing representational capacity. This architecture enables comprehensive modeling of both cellular-level features and tissue microenvironmental cues, which are essential for accurate gene expression prediction. Extensive experiments on two benchmark ST datasets demonstrate that HiFusion achieves state-of-the-art performance across both 2D slide-wise cross-validation and more challenging 3D sample-specific scenarios. These results underscore HiFusion's potential as a robust, accurate, and scalable solution for ST inference from routine histopathology.

</details>


### [374] [MCAQ-YOLO: Morphological Complexity-Aware Quantization for Efficient Object Detection with Curriculum Learning](https://arxiv.org/abs/2511.12976)
*Yoonjae Seo,Ermal Elbasani,Jaehong Lee*

Main category: cs.CV

TL;DR: MCAQ-YOLO introduces a framework for morphological complexity-aware quantization in neural networks, achieving better detection accuracy and efficiency through adaptive bit allocation guided by visual data complexity.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the inefficiency of uniform bit precision in typical neural network quantization by introducing a system that adapts to spatial morphological complexities.

Method: MCAQ-YOLO uses five visual metrics (fractal dimension, texture entropy, gradient variance, edge density, and contour complexity) to adaptively allocate bit precision. It combines this with a curriculum-based quantization-aware training to ease convergence.

Result: MCAQ-YOLO achieved higher detection accuracy (85.6% mAP@0.5) compared to uniform quantization, with a 7.6x compression ratio and minimal runtime overhead. Performance gains were consistent across multiple datasets.

Conclusion: The approach improves efficiency and robustness in object detection for computation-constrained, safety-critical tasks by leveraging morphology-driven spatial quantization.

Abstract: Most neural network quantization methods apply uniform bit precision across spatial regions, ignoring the heterogeneous structural and textural complexity of visual data. This paper introduces MCAQ-YOLO, a morphological complexity-aware quantization framework for object detection. The framework employs five morphological metrics - fractal dimension, texture entropy, gradient variance, edge density, and contour complexity - to characterize local visual morphology and guide spatially adaptive bit allocation. By correlating these metrics with quantization sensitivity, MCAQ-YOLO dynamically adjusts bit precision according to spatial complexity. In addition, a curriculum-based quantization-aware training scheme progressively increases quantization difficulty to stabilize optimization and accelerate convergence. Experimental results demonstrate a strong correlation between morphological complexity and quantization sensitivity and show that MCAQ-YOLO achieves superior detection accuracy and convergence efficiency compared with uniform quantization. On a safety equipment dataset, MCAQ-YOLO attains 85.6 percent mAP@0.5 with an average of 4.2 bits and a 7.6x compression ratio, yielding 3.5 percentage points higher mAP than uniform 4-bit quantization while introducing only 1.8 ms of additional runtime overhead per image. Cross-dataset validation on COCO and Pascal VOC further confirms consistent performance gains, indicating that morphology-driven spatial quantization can enhance efficiency and robustness for computationally constrained, safety-critical visual recognition tasks.

</details>


### [375] [ArtiWorld: LLM-Driven Articulation of 3D Objects in Scenes](https://arxiv.org/abs/2511.12977)
*Yixuan Yang,Luyang Xie,Zhen Luo,Zixiang Zhao,Mingqi Gao,Feng Zheng*

Main category: cs.CV

TL;DR: This paper introduces ArtiWorld, a pipeline for automatically converting rigid 3D objects into articulated assets using textual descriptions and geometry preservation.


<details>
  <summary>Details</summary>
Motivation: Current 3D simulation environments lack scalable articulated assets, and manually creating them is labor-intensive.

Method: The study proposes ArtiWorld, leveraging a tool called Arti4URDF, utilizing 3D point clouds, large language models, and URDF-oriented prompts to reconstruct articulated objects.

Result: ArtiWorld consistently outperforms existing methods in generating geometry-preserving and interactive URDF models across simulated and real-world scenarios.

Conclusion: The pipeline offers a scalable solution for creating simulation-ready articulated models, facilitating enhanced robot-learning environments.

Abstract: Building interactive simulators and scalable robot-learning environments requires a large number of articulated assets. However, most existing 3D assets in simulation are rigid, and manually converting them into articulated objects is extremely labor- and cost-intensive. This raises a natural question: can we automatically identify articulable objects in a scene and convert them into articulated assets directly? In this paper, we present ArtiWorld, a scene-aware pipeline that localizes candidate articulable objects from textual scene descriptions and reconstructs executable URDF models that preserve the original geometry. At the core of this pipeline is Arti4URDF, which leverages 3D point cloud, prior knowledge of a large language model (LLM), and a URDF-oriented prompt design to rapidly convert rigid objects into interactive URDF-based articulated objects while maintaining their 3D shape. We evaluate ArtiWorld at three levels: 3D simulated objects, full 3D simulated scenes, and real-world scan scenes. Across all three settings, our method consistently outperforms existing approaches and achieves state-of-the-art performance, while preserving object geometry and correctly capturing object interactivity to produce usable URDF-based articulated models. This provides a practical path toward building interactive, robot-ready simulation environments directly from existing 3D assets. Code and data will be released.

</details>


### [376] [Concept Regions Matter: Benchmarking CLIP with a New Cluster-Importance Approach](https://arxiv.org/abs/2511.12978)
*Aishwarya Agarwal,Srikrishna Karanam,Vineet Gandhi*

Main category: cs.CV

TL;DR: This paper presents CCI, a method to assess VLMs' behavior using semantic patch clustering, and introduces the COVAR benchmark to analyze foreground and background effects for improving vision-language models.


<details>
  <summary>Details</summary>
Motivation: To address background over-reliance and spurious correlations in vision-language models like CLIP while improving interpretability and understanding of model predictions.

Method: Developed CCI, which clusters and masks spatial patches of images to analyze their effect on model predictions, and combined it with a new benchmark, COVAR, to evaluate CLIP variants systematically.

Result: CCI achieved new state-of-the-art results on faithfulness tasks and revealed insights into VLM prediction drivers (foreground vs background), while COVAR enabled systematic analysis of various factors affecting model performance.

Conclusion: CCI and COVAR together provide robust tools for better understanding VLMs' behaviors and improving their prediction robustness.

Abstract: Contrastive vision-language models (VLMs) such as CLIP achieve strong zero-shot recognition yet remain vulnerable to spurious correlations, particularly background over-reliance. We introduce Cluster-based Concept Importance (CCI), a novel interpretability method that uses CLIP's own patch embeddings to group spatial patches into semantically coherent clusters, mask them, and evaluate relative changes in model predictions. CCI sets a new state of the art on faithfulness benchmarks, surpassing prior methods by large margins; for example, it yields more than a twofold improvement on the deletion-AUC metric for MS COCO retrieval. We further propose that CCI, when combined with GroundedSAM, automatically categorizes predictions as foreground- or background-driven, providing a crucial diagnostic ability. Existing benchmarks such as CounterAnimals, however, rely solely on accuracy and implicitly attribute all performance degradation to background correlations. Our analysis shows this assumption to be incomplete, since many errors arise from viewpoint variation, scale shifts, and fine-grained object confusions. To disentangle these effects, we introduce COVAR, a benchmark that systematically varies object foregrounds and backgrounds. Leveraging CCI with COVAR, we present a comprehensive evaluation of eighteen CLIP variants, offering methodological advances and empirical evidence that chart a path toward more robust VLMs.

</details>


### [377] [UNSEEN: Enhancing Dataset Pruning from a Generalization Perspective](https://arxiv.org/abs/2511.12988)
*Furui Xu,Shaobo Wang,Jiajun Zhang,Chenghao Sun,Haixiang Tang,Linfeng Zhang*

Main category: cs.CV

TL;DR: The paper proposes a novel dataset pruning framework, UNSEEN, focusing on generalization to form compact coresets with high performance, outperforming existing methods on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: To tackle computational challenges arising from the growing scale of datasets in deep learning by creating smaller yet effective subsets of data for training.

Method: The UNSEEN framework evaluates sample importance using generalization instead of fitting metrics during training, incorporating multi-step incremental coreset selection for improved dataset pruning.

Result: UNSEEN significantly outperforms SOTA pruning methods and achieves lossless performance on benchmarks like ImageNet-1K, reducing training data by 30%.

Conclusion: By emphasizing generalization and incremental improvement, UNSEEN advances dataset pruning, demonstrating robust performance and reduced training costs for deep learning applications.

Abstract: The growing scale of datasets in deep learning has introduced significant computational challenges. Dataset pruning addresses this challenge by constructing a compact but informative coreset from the full dataset with comparable performance. Previous approaches typically establish scoring metrics based on specific criteria to identify representative samples. However, these methods predominantly rely on sample scores obtained from the model's performance during the training (i.e., fitting) phase. As scoring models achieve near-optimal performance on training data, such fitting-centric approaches induce a dense distribution of sample scores within a narrow numerical range. This concentration reduces the distinction between samples and hinders effective selection. To address this challenge, we conduct dataset pruning from the perspective of generalization, i.e., scoring samples based on models not exposed to them during training. We propose a plug-and-play framework, UNSEEN, which can be integrated into existing dataset pruning methods. Additionally, conventional score-based methods are single-step and rely on models trained solely on the complete dataset, providing limited perspective on the importance of samples. To address this limitation, we scale UNSEEN to multi-step scenarios and propose an incremental selection technique through scoring models trained on varying coresets, and optimize the quality of the coreset dynamically. Extensive experiments demonstrate that our method significantly outperforms existing state-of-the-art (SOTA) methods on CIFAR-10, CIFAR-100, and ImageNet-1K. Notably, on ImageNet-1K, UNSEEN achieves lossless performance while reducing training data by 30\%.

</details>


### [378] [Semantic Prioritization in Visual Counterfactual Explanations with Weighted Segmentation and Auto-Adaptive Region Selection](https://arxiv.org/abs/2511.12992)
*Lintong Zhang,Kang Yin,Seong-Whan Lee*

Main category: cs.CV

TL;DR: This paper proposes WSAE-Net, an advanced framework for generating non-generative visual counterfactual explanations with semantic relevance and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional techniques for visual counterfactual explanations lack semantic considerations in replacements and efficiency in model workflows.

Method: The study introduces WSAE-Net, which employs weighted semantic maps to reduce non-semantic computations, and an auto-adaptive candidate editing sequence to optimize feature processing order.

Result: WSAE-Net demonstrates improved efficiency and semantic relevance in generating visual counterfactual explanations through experimental validation.

Conclusion: This methodology enhances model interpretability and contributes significantly to understanding visual counterfactual explanations.

Abstract: In the domain of non-generative visual counterfactual explanations (CE), traditional techniques frequently involve the substitution of sections within a query image with corresponding sections from distractor images. Such methods have historically overlooked the semantic relevance of the replacement regions to the target object, thereby impairing the model's interpretability and hindering the editing workflow. Addressing these challenges, the present study introduces an innovative methodology named as Weighted Semantic Map with Auto-adaptive Candidate Editing Network (WSAE-Net). Characterized by two significant advancements: the determination of an weighted semantic map and the auto-adaptive candidate editing sequence. First, the generation of the weighted semantic map is designed to maximize the reduction of non-semantic feature units that need to be computed, thereby optimizing computational efficiency. Second, the auto-adaptive candidate editing sequences are designed to determine the optimal computational order among the feature units to be processed, thereby ensuring the efficient generation of counterfactuals while maintaining the semantic relevance of the replacement feature units to the target object. Through comprehensive experimentation, our methodology demonstrates superior performance, contributing to a more lucid and in-depth understanding of visual counterfactual explanations.

</details>


### [379] [PerTouch: VLM-Driven Agent for Personalized and Semantic Image Retouching](https://arxiv.org/abs/2511.12998)
*Zewei Chang,Zheng-Peng Duan,Jianxing Zhang,Chun-Le Guo,Siyu Liu,Hyungju Chun,Hyunhee Park,Zikun Liu,Chongyi Li*

Main category: cs.CV

TL;DR: The paper presents PerTouch, an advanced framework for personalized image retouching using diffusion-based methods, allowing semantic-level control and aesthetic alignment.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in achieving both controllability and subjectivity in personalized image retouching.

Method: Utilizes explicit parameter-to-image mapping, semantic replacement, parameter perturbation during training, and a VLM-driven agent for connecting instructions with visual aesthetics.

Result: Experiments confirm effectiveness of individual components and the overall superior performance of PerTouch in personalized image retouching.

Conclusion: PerTouch successfully improves semantic and aesthetic control in image retouching while aligning with user preferences, showcasing its practicality and relevance in this domain.

Abstract: Image retouching aims to enhance visual quality while aligning with users' personalized aesthetic preferences. To address the challenge of balancing controllability and subjectivity, we propose a unified diffusion-based image retouching framework called PerTouch. Our method supports semantic-level image retouching while maintaining global aesthetics. Using parameter maps containing attribute values in specific semantic regions as input, PerTouch constructs an explicit parameter-to-image mapping for fine-grained image retouching. To improve semantic boundary perception, we introduce semantic replacement and parameter perturbation mechanisms in the training process. To connect natural language instructions with visual control, we develop a VLM-driven agent that can handle both strong and weak user instructions. Equipped with mechanisms of feedback-driven rethinking and scene-aware memory, PerTouch better aligns with user intent and captures long-term preferences. Extensive experiments demonstrate each component's effectiveness and the superior performance of PerTouch in personalized image retouching. Code is available at: https://github.com/Auroral703/PerTouch.

</details>


### [380] [Medal S: Spatio-Textual Prompt Model for Medical Segmentation](https://arxiv.org/abs/2511.13001)
*Pengcheng Shi,Jiawei Chen,Jiaqi Liu,Xinglin Zhang,Tao Chen,Lei Li*

Main category: cs.CV

TL;DR: Medal S is a 3D medical segmentation model that integrates native-resolution spatial and textual prompts, improving segmentation performance and efficiency over existing models.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of text-only medical segmentation models lacking spatial awareness and to enhance efficiency and accuracy in multi-class segmentation.

Method: Medal S aligns volumetric spatial prompts with text embeddings, employs lightweight 3D convolutions for voxel refinement, supports parallel spatial prompting, and uses dynamic resampling and optimized inference strategies.

Result: Medal S outperforms SAT with higher scores (DSC, NSD, F1, etc.) across five medical modalities, achieving significant accuracy and inference time gains.

Conclusion: Medal S demonstrates superior performance, harmonizing spatial and textual guidance for efficient, accurate multi-class medical segmentation, and is available for public use.

Abstract: We introduce Medal S, a medical segmentation foundation model that supports native-resolution spatial and textual prompts within an end-to-end trainable framework. Unlike text-only methods lacking spatial awareness, Medal S achieves channel-wise alignment between volumetric prompts and text embeddings, mitigating inaccuracies from resolution mismatches. By preserving full 3D context, it efficiently processes multiple native-resolution masks in parallel, enhancing multi-class segmentation performance. A lightweight 3D convolutional module enables precise voxel-space refinement guided by both prompt types, supporting up to 243 classes across CT, MRI, PET, ultrasound, and microscopy modalities in the BiomedSegFM dataset. Medal S offers two prompting modes: a text-only mode, where model predictions serve as spatial prompts for self-refinement without human input, and a hybrid mode, incorporating manual annotations for enhanced flexibility. For 24-class segmentation, parallel spatial prompting reduces inference time by more than 90% compared to sequential prompting. We propose dynamic resampling to address target-patch ratio imbalance, extending SAT and nnU-Net for data augmentation. Furthermore, we develop optimized text preprocessing, a two-stage inference strategy, and post-processing techniques to improve memory efficiency, precision, and inference speed. On the five-modality average on the validation set, Medal S outperforms SAT with a DSC of 75.44 (vs. 69.83), NSD of 77.34 (vs. 71.06), F1 of 38.24 (vs. 24.88), and DSC TP of 65.46 (vs. 46.97). Medal S achieves excellent performance by harmonizing spatial precision with semantic textual guidance, demonstrating superior efficiency and accuracy in multi-class medical segmentation tasks compared to sequential prompt-based approaches. Medal S will be publicly available at https://github.com/yinghemedical/Medal-S.

</details>


### [381] [Infinite-Story: A Training-Free Consistent Text-to-Image Generation](https://arxiv.org/abs/2511.13002)
*Jihun Park,Kyoungmin Lee,Jongmin Gim,Hyeonseo Jo,Minseok Oh,Wonhyeok Choi,Kyumin Hwang,Jaeyeul Kim,Minwoo Choi,Sunghoon Im*

Main category: cs.CV

TL;DR: The paper introduces Infinite-Story, a training-free method for consistent text-to-image generation in storytelling scenarios, solving issues like identity and style inconsistencies with faster inference.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges of identity and style inconsistency in text-to-image generation, particularly for multi-prompt storytelling contexts, without relying on training-heavy approaches.

Method: The proposed method is built on a scale-wise autoregressive model with three key techniques: Identity Prompt Replacement to reduce context bias and ensure identity consistency, Adaptive Style Injection for global style consistency, and Synchronized Guidance Adaptation for better alignment.

Result: Experimental results show that their method achieves state-of-the-art performance in generating consistent images, while being over six times faster (1.72 seconds per image) than the fastest existing models.

Conclusion: Infinite-Story effectively ensures high-quality, consistent T2I generation without any training requirements, making it practical and efficient for real-world visual storytelling scenarios.

Abstract: We present Infinite-Story, a training-free framework for consistent text-to-image (T2I) generation tailored for multi-prompt storytelling scenarios. Built upon a scale-wise autoregressive model, our method addresses two key challenges in consistent T2I generation: identity inconsistency and style inconsistency. To overcome these issues, we introduce three complementary techniques: Identity Prompt Replacement, which mitigates context bias in text encoders to align identity attributes across prompts; and a unified attention guidance mechanism comprising Adaptive Style Injection and Synchronized Guidance Adaptation, which jointly enforce global style and identity appearance consistency while preserving prompt fidelity. Unlike prior diffusion-based approaches that require fine-tuning or suffer from slow inference, Infinite-Story operates entirely at test time, delivering high identity and style consistency across diverse prompts. Extensive experiments demonstrate that our method achieves state-of-the-art generation performance, while offering over 6X faster inference (1.72 seconds per image) than the existing fastest consistent T2I models, highlighting its effectiveness and practicality for real-world visual storytelling.

</details>


### [382] [SAGE: Spuriousness-Aware Guided Prompt Exploration for Mitigating Multimodal Bias](https://arxiv.org/abs/2511.13005)
*Wenqian Ye,Di Wang,Guangtao Zheng,Bohan Liu,Aidong Zhang*

Main category: cs.CV

TL;DR: The paper introduces SAGE, a method to mitigate multimodal spurious biases in CLIP-like models without using fine-tuning, prior knowledge, or external data, improving zero-shot classification robustness.


<details>
  <summary>Details</summary>
Motivation: CLIP models exhibit spurious biases by associating objects with co-occurring backgrounds rather than object features, resulting in reduced robustness on out-of-distribution data. Current bias mitigation strategies compromise CLIP's usability by requiring fine-tuning or prior bias knowledge.

Method: The proposed SAGE method involves guided exploration of prompt templates that maximize semantic class separation. It operates without requiring training, fine-tuning, or external annotations, making it an out-of-the-box solution.

Result: Experiments on four benchmark datasets and five backbone models demonstrated that SAGE consistently outperforms prior zero-shot methods in robustness and generalization without using external knowledge or model updates.

Conclusion: SAGE effectively mitigates spurious biases in vision-language models during zero-shot classification, enhancing their robustness and usability without additional resource overheads.

Abstract: Large vision-language models, such as CLIP, have shown strong zero-shot classification performance by aligning images and text in a shared embedding space. However, CLIP models often develop multimodal spurious biases, which is the undesirable tendency to rely on spurious features. For example, CLIP may infer object types in images based on frequently co-occurring backgrounds rather than the object's core features. This bias significantly impairs the robustness of pre-trained CLIP models on out-of-distribution data, where such cross-modal associations no longer hold. Existing methods for mitigating multimodal spurious bias typically require fine-tuning on downstream data or prior knowledge of the bias, which undermines the out-of-the-box usability of CLIP. In this paper, we first theoretically analyze the impact of multimodal spurious bias in zero-shot classification. Based on this insight, we propose Spuriousness-Aware Guided Exploration (SAGE), a simple and effective method that mitigates spurious bias through guided prompt selection. SAGE requires no training, fine-tuning, or external annotations. It explores a space of prompt templates and selects the prompts that induce the largest semantic separation between classes, thereby improving worst-group robustness. Extensive experiments on four real-world benchmark datasets and five popular backbone models demonstrate that SAGE consistently improves zero-shot performance and generalization, outperforming previous zero-shot approaches without any external knowledge or model updates.

</details>


### [383] [Beyond Darkness: Thermal-Supervised 3D Gaussian Splatting for Low-Light Novel View Synthesis](https://arxiv.org/abs/2511.13011)
*Qingsen Ma,Chen Zou,Dianyun Wang,Jia Wang,Liuyu Xiang,Zhaofeng He*

Main category: cs.CV

TL;DR: The paper introduces DTGS, a framework that combines illumination decomposition and 3D Gaussian Splatting (3DGS) for better low-light view synthesis.


<details>
  <summary>Details</summary>
Motivation: Novel view synthesis (NVS) under low-light conditions struggles with geometry distortion, color inconsistencies, and unstable radiometric properties, and existing methods fail to maintain consistency across views.

Method: The proposed DTGS integrates Retinex-based illumination decomposition with thermal-guided 3DGS through a joint optimization process involving enhancement, geometry, and thermal guidance.

Result: DTGS achieves superior results in low-light enhancement and 3D reconstruction, with better radiometric consistency, geometric accuracy, and stable color representation.

Conclusion: DTGS provides a robust solution for low-light NVS, demonstrating consistent reconstruction quality by integrating illumination decomposition and thermal-guided supervision.

Abstract: Under extremely low-light conditions, novel view synthesis (NVS) faces severe degradation in terms of geometry, color consistency, and radiometric stability. Standard 3D Gaussian Splatting (3DGS) pipelines fail when applied directly to underexposed inputs, as independent enhancement across views causes illumination inconsistencies and geometric distortion. To address this, we present DTGS, a unified framework that tightly couples Retinex-inspired illumination decomposition with thermal-guided 3D Gaussian Splatting for illumination-invariant reconstruction. Unlike prior approaches that treat enhancement as a pre-processing step, DTGS performs joint optimization across enhancement, geometry, and thermal supervision through a cyclic enhancement-reconstruction mechanism. A thermal supervisory branch stabilizes both color restoration and geometry learning by dynamically balancing enhancement, structural, and thermal losses. Moreover, a Retinex-based decomposition module embedded within the 3DGS loop provides physically interpretable reflectance-illumination separation, ensuring consistent color and texture across viewpoints. To evaluate our method, we construct RGBT-LOW, a new multi-view low-light thermal dataset capturing severe illumination degradation. Extensive experiments show that DTGS significantly outperforms existing low-light enhancement and 3D reconstruction baselines, achieving superior radiometric consistency, geometric fidelity, and color stability under extreme illumination.

</details>


### [384] [You Only Look Omni Gradient Backpropagation for Moving Infrared Small Target Detection](https://arxiv.org/abs/2511.13013)
*Guoyi Zhang,Guangsheng Xu,Siyang Chen,Han Wang,Xiaohu Zhang*

Main category: cs.CV

TL;DR: This paper introduces BP-FPN, a novel feature pyramid architecture designed to improve moving infrared small target detection.


<details>
  <summary>Details</summary>
Motivation: Current methods for moving infrared small target detection face challenges like low signal-to-clutter ratios, target-background imbalance, and ambiguous per-frame feature representations.

Method: BP-FPN incorporates Gradient-Isolated Low-Level Shortcut (GILS) for fine-grained feature learning and Directional Gradient Regularization (DGR) for feature consistency in backpropagation.

Result: BP-FPN achieves state-of-the-art performance across multiple public datasets with minimal computational overhead.

Conclusion: BP-FPN redefines feature learning for infrared small target detection, providing a theoretically grounded solution with practical performance improvements.

Abstract: Moving infrared small target detection is a key component of infrared search and tracking systems, yet it remains extremely challenging due to low signal-to-clutter ratios, severe target-background imbalance, and weak discriminative features. Existing deep learning methods primarily focus on spatio-temporal feature aggregation, but their gains are limited, revealing that the fundamental bottleneck lies in ambiguous per-frame feature representations rather than spatio-temporal modeling itself. Motivated by this insight, we propose BP-FPN, a backpropagation-driven feature pyramid architecture that fundamentally rethinks feature learning for small target. BP-FPN introduces Gradient-Isolated Low-Level Shortcut (GILS) to efficiently incorporate fine-grained target details without inducing shortcut learning, and Directional Gradient Regularization (DGR) to enforce hierarchical feature consistency during backpropagation. The design is theoretically grounded, introduces negligible computational overhead, and can be seamlessly integrated into existing frameworks. Extensive experiments on multiple public datasets show that BP-FPN consistently establishes new state-of-the-art performance. To the best of our knowledge, it is the first FPN designed for this task entirely from the backpropagation perspective.

</details>


### [385] [Geometry Meets Light: Leveraging Geometric Priors for Universal Photometric Stereo under Limited Multi-Illumination Cues](https://arxiv.org/abs/2511.13015)
*King-Man Tam,Satoshi Ikehata,Yuta Asano,Zhaoyi An,Rei Kawakami*

Main category: cs.CV

TL;DR: This paper proposes GeoUniPS, a photometric stereo method that incorporates geometric priors and synthetic supervision, excelling in recovering surface normals in challenging real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: The traditional universal photometric stereo struggles in scenarios where multi-illumination data is unreliable, such as in shadows, biased lighting, or self-occlusion in complex scenes. This inspired the need to integrate robust geometric priors for more accurate surface normal recovery.

Method: GeoUniPS incorporates a Light-Geometry Dual-Branch Encoder that integrates multi-illumination cues and learned geometric priors from frozen 3D reconstruction foundation models. It also uses a new dataset, PS-Perp, to capture spatially varying view direction using realistic perspective projections.

Result: GeoUniPS achieves state-of-the-art performance in recovering surface normals, validated by extensive experiments across multiple datasets, showcasing high efficacy in challenging in-the-wild scenes.

Conclusion: By combining multi-illumination and geometric prior cues from pre-trained foundation models, GeoUniPS addresses the limitations of traditional methods under complex conditions, setting a new standard in universal photometric stereo performance.

Abstract: Universal Photometric Stereo is a promising approach for recovering surface normals without strict lighting assumptions. However, it struggles when multi-illumination cues are unreliable, such as under biased lighting or in shadows or self-occluded regions of complex in-the-wild scenes. We propose GeoUniPS, a universal photometric stereo network that integrates synthetic supervision with high-level geometric priors from large-scale 3D reconstruction models pretrained on massive in-the-wild data. Our key insight is that these 3D reconstruction models serve as visual-geometry foundation models, inherently encoding rich geometric knowledge of real scenes. To leverage this, we design a Light-Geometry Dual-Branch Encoder that extracts both multi-illumination cues and geometric priors from the frozen 3D reconstruction model. We also address the limitations of the conventional orthographic projection assumption by introducing the PS-Perp dataset with realistic perspective projection to enable learning of spatially varying view directions. Extensive experiments demonstrate that GeoUniPS delivers state-of-the-arts performance across multiple datasets, both quantitatively and qualitatively, especially in the complex in-the-wild scenes.

</details>


### [386] [MeanFlow Transformers with Representation Autoencoders](https://arxiv.org/abs/2511.13019)
*Zheyuan Hu,Chieh-Hsin Lai,Ge Wu,Yuki Mitsufuji,Stefano Ermon*

Main category: cs.CV

TL;DR: The paper introduces an efficient generative model using a Representation Autoencoder (RAE) for high-dimensional data generation, achieving superior performance with reduced computational costs.


<details>
  <summary>Details</summary>
Motivation: To address instability and computational demands in the training and inference of MeanFlow (MF) models while improving efficiency and performance in image generation tasks.

Method: The authors propose a scheme that uses a Representation Autoencoder (RAE) with pre-trained vision encoders for training MeanFlow in latent space, overcoming gradient instability through Consistency Mid-Training and a two-stage distillation and bootstrapping approach.

Result: The proposed method improves generation performance with a 1-step FID of 2.03 on ImageNet 256, surpassing original MF while reducing sampling GFLOPS by 38% and training costs by 83%. On ImageNet 512, it achieves a competitive 1-step FID with the lowest computational cost among baselines.

Conclusion: This study presents a significant advancement in efficient generative modeling by utilizing RAE latent spaces, reducing both computational resources and enhancing performance, paving the way for more practical and accessible generative models.

Abstract: MeanFlow (MF) is a diffusion-motivated generative model that enables efficient few-step generation by learning long jumps directly from noise to data. In practice, it is often used as a latent MF by leveraging the pre-trained Stable Diffusion variational autoencoder (SD-VAE) for high-dimensional data modeling. However, MF training remains computationally demanding and is often unstable. During inference, the SD-VAE decoder dominates the generation cost, and MF depends on complex guidance hyperparameters for class-conditional generation. In this work, we develop an efficient training and sampling scheme for MF in the latent space of a Representation Autoencoder (RAE), where a pre-trained vision encoder (e.g., DINO) provides semantically rich latents paired with a lightweight decoder. We observe that naive MF training in the RAE latent space suffers from severe gradient explosion. To stabilize and accelerate training, we adopt Consistency Mid-Training for trajectory-aware initialization and use a two-stage scheme: distillation from a pre-trained flow matching teacher to speed convergence and reduce variance, followed by an optional bootstrapping stage with a one-point velocity estimator to further reduce deviation from the oracle mean flow. This design removes the need for guidance, simplifies training configurations, and reduces computation in both training and sampling. Empirically, our method achieves a 1-step FID of 2.03, outperforming vanilla MF's 3.43, while reducing sampling GFLOPS by 38% and total training cost by 83% on ImageNet 256. We further scale our approach to ImageNet 512, achieving a competitive 1-step FID of 3.23 with the lowest GFLOPS among all baselines. Code is available at https://github.com/sony/mf-rae.

</details>


### [387] [SpectralAdapt: Semi-Supervised Domain Adaptation with Spectral Priors for Human-Centered Hyperspectral Image Reconstruction](https://arxiv.org/abs/2511.13020)
*Yufei Wen,Yuting Zhang,Jingdan Kang,Hao Ren,Weibin Cheng,Jintai Chen,Kaishun Wu*

Main category: cs.CV

TL;DR: The paper introduces SpectralAdapt, a semi-supervised domain adaptation framework to improve hyperspectral image (HSI) reconstruction for healthcare by addressing domain gaps and data scarcity.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of costly, technically challenging HSI data acquisition and the lack of human-centered HSI datasets for medical applications.

Method: The framework uses Spectral Density Masking (SDM) to enhance spectral reasoning and Spectral Endmember Representation Alignment (SERA) to align domain-invariant anchors, leveraging limited labeled and abundant unlabeled data for reconstruction.

Result: Experiments on benchmark datasets show improvements in spectral fidelity, generalization across domains, and training stability.

Conclusion: SpectralAdapt is effective in reducing domain shift, enhancing spectral fidelity, and addressing data scarcity, making SSDA a promising solution for medical hyperspectral imaging.

Abstract: Hyperspectral imaging (HSI) holds great potential for healthcare due to its rich spectral information. However, acquiring HSI data remains costly and technically demanding. Hyperspectral image reconstruction offers a practical solution by recovering HSI data from accessible modalities, such as RGB. While general domain datasets are abundant, the scarcity of human HSI data limits progress in medical applications. To tackle this, we propose SpectralAdapt, a semi-supervised domain adaptation (SSDA) framework that bridges the domain gap between general and human-centered HSI datasets. To fully exploit limited labels and abundant unlabeled data, we enhance spectral reasoning by introducing Spectral Density Masking (SDM), which adaptively masks RGB channels based on their spectral complexity, encouraging recovery of informative regions from complementary cues during consistency training. Furthermore, we introduce Spectral Endmember Representation Alignment (SERA), which derives physically interpretable endmembers from valuable labeled pixels and employs them as domain-invariant anchors to guide unlabeled predictions, with momentum updates ensuring adaptability and stability. These components are seamlessly integrated into SpectralAdapt, a spectral prior-guided framework that effectively mitigates domain shift, spectral degradation, and data scarcity in HSI reconstruction. Experiments on benchmark datasets demonstrate consistent improvements in spectral fidelity, cross-domain generalization, and training stability, highlighting the promise of SSDA as an efficient solution for hyperspectral imaging in healthcare.

</details>


### [388] [REVISOR: Beyond Textual Reflection, Towards Multimodal Introspective Reasoning in Long-Form Video Understanding](https://arxiv.org/abs/2511.13026)
*Jiaze Li,Hao Yin,Wenhui Tan,Jingyang Chen,Boshen Xu,Yuxun Qu,Yijing Chen,Jianzhong Ju,Zhenbo Luo,Jian Luan*

Main category: cs.CV

TL;DR: The paper introduces REVISOR, a novel framework for improving long-form video understanding in multimodal tasks by addressing the insufficiencies of purely text-based reflection processes.


<details>
  <summary>Details</summary>
Motivation: Existing self-reflection mechanisms struggle with long-form video understanding due to the need for processing dynamic visual information and lack of cross-modal interaction.

Method: Proposed REVISOR framework uses a novel Dual Attribution Decoupled Reward mechanism in the GRPO strategy to integrate textual and visual reflection for multimodal reasoning, avoiding the need for additional fine-tuning.

Result: REVISOR significantly improves reasoning for long-form video understanding, achieving strong performance on four benchmarks.

Conclusion: The REVISOR framework enhances MLLMs in long-form video tasks by addressing multimodal reflection and cross-modal integration challenges.

Abstract: Self-reflection mechanisms that rely on purely text-based rethinking processes perform well in most multimodal tasks. However, when directly applied to long-form video understanding scenarios, they exhibit clear limitations. The fundamental reasons for this lie in two points: (1)long-form video understanding involves richer and more dynamic visual input, meaning rethinking only the text information is insufficient and necessitates a further rethinking process specifically targeting visual information; (2) purely text-based reflection mechanisms lack cross-modal interaction capabilities, preventing them from fully integrating visual information during reflection. Motivated by these insights, we propose REVISOR (REflective VIsual Segment Oriented Reasoning), a novel framework for tool-augmented multimodal reflection. REVISOR enables MLLMs to collaboratively construct introspective reflection processes across textual and visual modalities, significantly enhancing their reasoning capability for long-form video understanding. To ensure that REVISOR can learn to accurately review video segments highly relevant to the question during reinforcement learning, we designed the Dual Attribution Decoupled Reward (DADR) mechanism. Integrated into the GRPO training strategy, this mechanism enforces causal alignment between the model's reasoning and the selected video evidence. Notably, the REVISOR framework significantly enhances long-form video understanding capability of MLLMs without requiring supplementary supervised fine-tuning or external models, achieving impressive results on four benchmarks including VideoMME, LongVideoBench, MLVU, and LVBench.

</details>


### [389] [Towards 3D Object-Centric Feature Learning for Semantic Scene Completion](https://arxiv.org/abs/2511.13031)
*Weihua Wang,Yubo Cui,Xiangru Lin,Zhiheng Li,Zheng Fang*

Main category: cs.CV

TL;DR: This paper presents Ocean, an object-centric framework for vision-based 3D Semantic Scene Completion (SSC) that outperforms existing methods on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the accuracy of vision-based 3D Semantic Scene Completion by addressing limitations of ego-centric methods that often fail to capture fine-grained object-level details in complex environments.

Method: The authors propose an object-centric prediction framework called Ocean, using MobileSAM for instance segmentation, 3D Semantic Group Attention for linear attention in 3D space, a Global Similarity-Guided Attention module to handle segmentation errors, and an Instance-aware Local Diffusion module for generative feature refinement.

Result: Ocean achieves state-of-the-art performance with mIoU scores of 17.40 on SemanticKITTI and 20.28 on SSCBench-KITTI360 benchmarks.

Conclusion: Object-centric approaches like Ocean provide improved semantic and geometric accuracy in 3D Semantic Scene Completion tasks, especially in complex environments.

Abstract: Vision-based 3D Semantic Scene Completion (SSC) has received growing attention due to its potential in autonomous driving. While most existing approaches follow an ego-centric paradigm by aggregating and diffusing features over the entire scene, they often overlook fine-grained object-level details, leading to semantic and geometric ambiguities, especially in complex environments. To address this limitation, we propose Ocean, an object-centric prediction framework that decomposes the scene into individual object instances to enable more accurate semantic occupancy prediction. Specifically, we first employ a lightweight segmentation model, MobileSAM, to extract instance masks from the input image. Then, we introduce a 3D Semantic Group Attention module that leverages linear attention to aggregate object-centric features in 3D space. To handle segmentation errors and missing instances, we further design a Global Similarity-Guided Attention module that leverages segmentation features for global interaction. Finally, we propose an Instance-aware Local Diffusion module that improves instance features through a generative process and subsequently refines the scene representation in the BEV space. Extensive experiments on the SemanticKITTI and SSCBench-KITTI360 benchmarks demonstrate that Ocean achieves state-of-the-art performance, with mIoU scores of 17.40 and 20.28, respectively.

</details>


### [390] [Uni-Inter: Unifying 3D Human Motion Synthesis Across Diverse Interaction Contexts](https://arxiv.org/abs/2511.13032)
*Sheng Liu,Yuanzhi Liang,Jiepeng Wang,Sidan Du,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: Uni-Inter is a unified framework for human motion generation encompassing diverse interaction scenarios such as human-human, human-object, and human-scene.


<details>
  <summary>Details</summary>
Motivation: Existing methods have limited generalization and rely on task-specific designs for human motion generation across interaction scenarios.

Method: Uni-Inter utilizes a Unified Interactive Volume (UIV), a shared spatial representation for encoding diverse interactive entities to enable relational reasoning. The model predicts motion probabilistically over the UIV.

Result: Experiments on three interaction tasks show Uni-Inter performs competitively and generalizes well to novel entity combinations.

Conclusion: Unified modeling of compound interactions offers a scalable solution for human motion generation in complex environments.

Abstract: We present Uni-Inter, a unified framework for human motion generation that supports a wide range of interaction scenarios: including human-human, human-object, and human-scene-within a single, task-agnostic architecture. In contrast to existing methods that rely on task-specific designs and exhibit limited generalization, Uni-Inter introduces the Unified Interactive Volume (UIV), a volumetric representation that encodes heterogeneous interactive entities into a shared spatial field. This enables consistent relational reasoning and compound interaction modeling. Motion generation is formulated as joint-wise probabilistic prediction over the UIV, allowing the model to capture fine-grained spatial dependencies and produce coherent, context-aware behaviors. Experiments across three representative interaction tasks demonstrate that Uni-Inter achieves competitive performance and generalizes well to novel combinations of entities. These results suggest that unified modeling of compound interactions offers a promising direction for scalable motion synthesis in complex environments.

</details>


### [391] [uCLIP: Parameter-Efficient Multilingual Extension of Vision-Language Models with Unpaired Data](https://arxiv.org/abs/2511.13036)
*Dahyun Chung,Donghyun Shin,Yujin Sung,Seunggi Moon,Jinwoo Jeon,Byung-Jun Lee*

Main category: cs.CV

TL;DR: This paper introduces a lightweight multilingual vision-language alignment framework to improve multilingual retrieval performance, especially in underrepresented languages, without requiring image-text or text-text pairs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance multilingual vision-language models for low-resource languages, as existing models underperform due to limited high-quality multilingual image-text data.

Method: The method involves freezing pretrained image and multilingual text encoders while training a small 1.7M-parameter projection module using a contrastive loss based on English representations as semantic anchors.

Result: The proposed approach significantly improves multilingual retrieval performance across multiple benchmarks, particularly for underrepresented languages like Czech, Finnish, and others.

Conclusion: The framework demonstrates a data-efficient and parameter-efficient strategy to achieve robust multilingual alignment, making it especially suitable for inclusive multimodal learning in underrepresented languages.

Abstract: Contrastive Language-Image Pre-training (CLIP) has demonstrated strong generalization across a wide range of visual tasks by leveraging large-scale English-image pairs. However, its extension to low-resource languages remains limited due to the scarcity of high-quality multilingual image-text data. Existing multilingual vision-language models exhibit consistently low retrieval performance in underrepresented languages including Czech, Finnish, Croatian, Hungarian, and Romanian on the Crossmodal-3600 (XM3600) benchmark. To address this, we propose a lightweight and data-efficient framework for multilingual vision-language alignment. Our approach requires no image-text pairs or text-text pairs and freezes both the pretrained image encoder and multilingual text encoder during training. Only a compact 1.7M-parameter projection module is trained, using a contrastive loss over English representations as semantic anchors. This minimal training setup enables robust multilingual alignment even for languages with limited supervision. Extensive evaluation across multiple multilingual retrieval benchmarks confirms the effectiveness of our method, showing significant gains in five underrepresented languages where existing models typically underperform. These findings highlight the effectiveness of our pivot-based, parameter-efficient alignment strategy for inclusive multimodal learning.

</details>


### [392] [MGCA-Net: Multi-Grained Category-Aware Network for Open-Vocabulary Temporal Action Localization](https://arxiv.org/abs/2511.13039)
*Zhenying Fang,Richang Hong*

Main category: cs.CV

TL;DR: MGCA-Net improves Open-Vocabulary Temporal Action Localization by utilizing coarse-to-fine classifiers for novel action categories and a conventional classifier for base ones, leading to enhanced localization performance.


<details>
  <summary>Details</summary>
Motivation: Current OV-TAL approaches struggle with recognizing actions in videos when training data lacks explicit coverage for all categories, especially at different granularities.

Method: The authors developed MGCA-Net, encompassing a localizer, action presence predictor, conventional classifier, and coarse-to-fine classifier for multi-grained category awareness and better recognition.

Result: MGCA-Net demonstrated state-of-the-art performance on benchmarks THUMOS'14 and ActivityNet-1.3, including Zero-Shot Temporal Action Localization settings.

Conclusion: By integrating multi-grained category awareness, MGCA-Net effectively improves action localization performance across base and novel categories.

Abstract: Open-Vocabulary Temporal Action Localization (OV-TAL) aims to recognize and localize instances of any desired action categories in videos without explicitly curating training data for all categories. Existing methods mostly recognize action categories at a single granularity, which degrades the recognition accuracy of both base and novel action categories. To address these issues, we propose a Multi-Grained Category-Aware Network (MGCA-Net) comprising a localizer, an action presence predictor, a conventional classifier, and a coarse-to-fine classifier. Specifically, the localizer localizes category-agnostic action proposals. For these action proposals, the action presence predictor estimates the probability that they belong to an action instance. At the same time, the conventional classifier predicts the probability of each action proposal over base action categories at the snippet granularity. Novel action categories are recognized by the coarse-to-fine classifier, which first identifies action presence at the video granularity. Finally, it assigns each action proposal to one category from the coarse categories at the proposal granularity. Through coarse-to-fine category awareness for novel actions and the conventional classifier's awareness of base actions, multi-grained category awareness is achieved, effectively enhancing localization performance. Comprehensive evaluations on the THUMOS'14 and ActivityNet-1.3 benchmarks demonstrate that our method achieves state-of-the-art performance. Furthermore, our MGCA-Net achieves state-of-the-art results under the Zero-Shot Temporal Action Localization setting.

</details>


### [393] [ViSS-R1: Self-Supervised Reinforcement Video Reasoning](https://arxiv.org/abs/2511.13054)
*Bo Fang,Yuxin Song,Qiangqiang Wu,Haoyuan Sun,Wenhao Wu,Antoni B. Chan*

Main category: cs.CV

TL;DR: The paper addresses challenges in video reasoning for Multimodal Large Language Models (MLLMs) by introducing self-supervised reinforcement learning algorithms that emphasize visual-centric video understanding instead of text-reliance.


<details>
  <summary>Details</summary>
Motivation: Current methodologies for video reasoning overly focus on text-centric strategies, neglecting rich visual information and often leading to shortcut learning and hallucinations.

Method: The paper introduces Pretext-GRPO, a reinforcement learning algorithm that assigns positive rewards for solving pretext tasks on visually transformed inputs. It further proposes ViSS-R1, a framework integrating pretext tasks directly into MLLM post-training for robust video reasoning.

Result: Experiments on six benchmarks validate the improved video reasoning capabilities of the proposed methods, demonstrating their effectiveness and superiority.

Conclusion: Through Pretext-GRPO and ViSS-R1, the study moves towards visual-centric reasoning in MLLMs, offering a promising approach for enhanced video understanding and reasoning.

Abstract: Complex video reasoning remains a significant challenge for Multimodal Large Language Models (MLLMs), as current R1-based methodologies often prioritize text-centric reasoning derived from text-based and image-based developments. In video tasks, such strategies frequently underutilize rich visual information, leading to potential shortcut learning and increased susceptibility to hallucination. To foster a more robust, visual-centric video understanding, we start by introducing a novel self-supervised reinforcement learning GRPO algorithm (Pretext-GRPO) within the standard R1 pipeline, in which positive rewards are assigned for correctly solving pretext tasks on transformed visual inputs, which makes the model to non-trivially process the visual information. Building on the effectiveness of Pretext-GRPO, we further propose the ViSS-R1 framework, which streamlines and integrates pretext-task-based self-supervised learning directly into the MLLM's R1 post-training paradigm. Instead of relying solely on sparse visual cues, our framework compels models to reason about transformed visual input by simultaneously processing both pretext questions (concerning transformations) and true user queries. This necessitates identifying the applied transformation and reconstructing the original video to formulate accurate final answers. Comprehensive evaluations on six widely-used video reasoning and understanding benchmarks demonstrate the effectiveness and superiority of our Pretext-GRPO and ViSS-R1 for complex video reasoning. Our codes and models will be publicly available.

</details>


### [394] [Monocular 3D Lane Detection via Structure Uncertainty-Aware Network with Curve-Point Queries](https://arxiv.org/abs/2511.13055)
*Ruixin Liu,Zejian Yuan*

Main category: cs.CV

TL;DR: The paper introduces MonoUnc, a novel monocular 3D lane detection framework that models aleatoric uncertainty while avoiding BEV projections. It incorporates a parametric curve approach to predict 3D lanes.


<details>
  <summary>Details</summary>
Motivation: Current monocular 3D lane detection approaches struggle with capturing structural variations and aleatoric uncertainty due to simplified geometric assumptions.

Method: MonoUnc uses a front-view space representation with parametric curves for lane alignment and estimates aleatoric uncertainty through Gaussian modeling of lane segments, along with a 3D Gaussian matching loss.

Result: MonoUnc outperforms state-of-the-art methods on multiple benchmarks (ONCE-3DLanes, OpenLane datasets) across stricter evaluation criteria and provides new evaluation metrics to assess errors more comprehensively.

Conclusion: The proposed approach addresses aleatoric uncertainty and structural complexities in 3D lane detection effectively, setting a new standard using its novel methodological design and robust evaluation framework.

Abstract: Monocular 3D lane detection is challenged by aleatoric uncertainty arising from inherent observation noise. Existing methods rely on simplified geometric assumptions, such as independent point predictions or global planar modeling, failing to capture structural variations and aleatoric uncertainty in real-world scenarios. In this paper, we propose MonoUnc, a bird's-eye view (BEV)-free 3D lane detector that explicitly models aleatoric uncertainty informed by local lane structures. Specifically, 3D lanes are projected onto the front-view (FV) space and approximated by parametric curves. Guided by curve predictions, curve-point query embeddings are dynamically generated for lane point predictions in 3D space. Each segment formed by two adjacent points is modeled as a 3D Gaussian, parameterized by the local structure and uncertainty estimations. Accordingly, a novel 3D Gaussian matching loss is designed to constrain these parameters jointly. Experiments on the ONCE-3DLanes and OpenLane datasets demonstrate that MonoUnc outperforms previous state-of-the-art (SoTA) methods across all benchmarks under stricter evaluation criteria. Additionally, we propose two comprehensive evaluation metrics for ONCE-3DLanes, calculating the average and maximum bidirectional Chamfer distances to quantify global and local errors. Codes are released at https://github.com/lrx02/MonoUnc.

</details>


### [395] [FGNet: Leveraging Feature-Guided Attention to Refine SAM2 for 3D EM Neuron Segmentation](https://arxiv.org/abs/2511.13063)
*Zhenghua Li,Hang Chen,Zihao Sun,Kai Li,Xiaolin Hu*

Main category: cs.CV

TL;DR: The paper introduces a framework leveraging SAM2, a visual foundation model pre-trained on natural images, for improved neural structure segmentation in Electron Microscopy images. It incorporates domain adaptation strategies and achieves significant advancements in accuracy compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Neural segmentation in EM images is crucial for neuroscience but faces challenges like complex morphologies and limited annotations. The study aims to use visual foundation models to overcome these issues.

Method: The framework uses SAM2 to extract general-purpose features, employs a Feature-Guided Attention module for domain adaptation, utilizes a Fine-Grained Encoder to focus on challenging regions, and combines coarse and refined affinity maps with a dual-affinity decoder.

Result: Experimental results show performance comparable to SOTA methods with frozen SAM2 weights, and significantly better results when fine-tuned on EM data.

Conclusion: Transferring pre-trained representations from natural images and combining them with domain-adaptive methods addresses challenges in EM image segmentation and enhances accuracy.

Abstract: Accurate segmentation of neural structures in Electron Microscopy (EM) images is paramount for neuroscience. However, this task is challenged by intricate morphologies, low signal-to-noise ratios, and scarce annotations, limiting the accuracy and generalization of existing methods. To address these challenges, we seek to leverage the priors learned by visual foundation models on a vast amount of natural images to better tackle this task. Specifically, we propose a novel framework that can effectively transfer knowledge from Segment Anything 2 (SAM2), which is pre-trained on natural images, to the EM domain. We first use SAM2 to extract powerful, general-purpose features. To bridge the domain gap, we introduce a Feature-Guided Attention module that leverages semantic cues from SAM2 to guide a lightweight encoder, the Fine-Grained Encoder (FGE), in focusing on these challenging regions. Finally, a dual-affinity decoder generates both coarse and refined affinity maps. Experimental results demonstrate that our method achieves performance comparable to state-of-the-art (SOTA) approaches with the SAM2 weights frozen. Upon further fine-tuning on EM data, our method significantly outperforms existing SOTA methods. This study validates that transferring representations pre-trained on natural images, when combined with targeted domain-adaptive guidance, can effectively address the specific challenges in neuron segmentation.

</details>


### [396] [RobustGait: Robustness Analysis for Appearance Based Gait Recognition](https://arxiv.org/abs/2511.13065)
*Reeshoon Sayera,Akash Kumar,Sirshapan Mitra,Prudvi Kamtam,Yogesh S Rawat*

Main category: cs.CV

TL;DR: RobustGait is a framework designed to evaluate the robustness of appearance-based gait recognition systems under various real-world conditions, including corruptions and silhouette variability.


<details>
  <summary>Details</summary>
Motivation: Gait recognition systems, while effective in controlled settings, face challenges in real-world scenarios due to corruptions and variability, necessitating a structured evaluation framework.

Method: RobustGait evaluates robustness across perturbations, silhouette extraction methods, model architectures, and deployment scenarios using 15 corruption types at 5 severity levels across multiple datasets.

Result: Key findings include RGB-level noise impact, the influence of silhouette extractor biases on accuracy, the dependency of robustness on perturbation types and model design, and improved performance through noise-aware training and distillation.

Conclusion: RobustGait highlights the importance of tailored robustness strategies, offering promising approaches for enhancing gait recognition systems' deployment readiness in real-world conditions.

Abstract: Appearance-based gait recognition have achieved strong performance on controlled datasets, yet systematic evaluation of its robustness to real-world corruptions and silhouette variability remains lacking. We present RobustGait, a framework for fine-grained robustness evaluation of appearance-based gait recognition systems. RobustGait evaluation spans four dimensions: the type of perturbation (digital, environmental, temporal, occlusion), the silhouette extraction method (segmentation and parsing networks), the architectural capacities of gait recognition models, and various deployment scenarios. The benchmark introduces 15 corruption types at 5 severity levels across CASIA-B, CCPG, and SUSTech1K, with in-the-wild validation on MEVID, and evaluates six state-of-the-art gait systems. We came across several exciting insights. First, applying noise at the RGB level better reflects real-world degradation, and reveal how distortions propagate through silhouette extraction to the downstream gait recognition systems. Second, gait accuracy is highly sensitive to silhouette extractor biases, revealing an overlooked source of benchmark bias. Third, robustness is dependent on both the type of perturbation and the architectural design. Finally, we explore robustness-enhancing strategies, showing that noise-aware training and knowledge distillation improve performance and move toward deployment-ready systems.

</details>


### [397] [Decoupling Scene Perception and Ego Status: A Multi-Context Fusion Approach for Enhanced Generalization in End-to-End Autonomous Driving](https://arxiv.org/abs/2511.13079)
*Jiacheng Tang,Mingyue Feng,Jiachao Liu,Yaonong Wang,Jian Pu*

Main category: cs.CV

TL;DR: Proposes AdaptiveAD, an autonomous driving architecture that decouples ego status and scene perception to improve planning and generalization.


<details>
  <summary>Details</summary>
Motivation: Existing end-to-end autonomous driving architectures overly rely on ego status, making them less robust in generalizing scenarios.

Method: Introduces AdaptiveAD, a dual-branch structure combining scene-driven and ego-driven reasoning, separating scene perception from ego status involvement. Utilizes a multi-context fusion strategy, path attention, and auxiliary tasks for enhanced performance.

Result: Achieves state-of-the-art open-loop planning performance on the nuScenes dataset, reducing ego status dependency and improving generalization.

Conclusion: AdaptiveAD mitigates ego status reliance, enhances robustness, and demonstrates superior scene understanding and planning capabilities in autonomous driving.

Abstract: Modular design of planning-oriented autonomous driving has markedly advanced end-to-end systems. However, existing architectures remain constrained by an over-reliance on ego status, hindering generalization and robust scene understanding. We identify the root cause as an inherent design within these architectures that allows ego status to be easily leveraged as a shortcut. Specifically, the premature fusion of ego status in the upstream BEV encoder allows an information flow from this strong prior to dominate the downstream planning module. To address this challenge, we propose AdaptiveAD, an architectural-level solution based on a multi-context fusion strategy. Its core is a dual-branch structure that explicitly decouples scene perception and ego status. One branch performs scene-driven reasoning based on multi-task learning, but with ego status deliberately omitted from the BEV encoder, while the other conducts ego-driven reasoning based solely on the planning task. A scene-aware fusion module then adaptively integrates the complementary decisions from the two branches to form the final planning trajectory. To ensure this decoupling does not compromise multi-task learning, we introduce a path attention mechanism for ego-BEV interaction and add two targeted auxiliary tasks: BEV unidirectional distillation and autoregressive online mapping. Extensive evaluations on the nuScenes dataset demonstrate that AdaptiveAD achieves state-of-the-art open-loop planning performance. Crucially, it significantly mitigates the over-reliance on ego status and exhibits impressive generalization capabilities across diverse scenarios.

</details>


### [398] [Rethinking Saliency Maps: A Cognitive Human Aligned Taxonomy and Evaluation Framework for Explanations](https://arxiv.org/abs/2511.13081)
*Yehonatan Elisha,Seffi Cohen,Oren Barkan,Noam Koenigstein*

Main category: cs.CV

TL;DR: The paper introduces the Reference-Frame × Granularity (RFxG) taxonomy to systematically assess saliency maps, emphasizing user-intent-aligned evaluation of explanation methods in deep learning.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the lack of clarity regarding the purpose and alignment of saliency maps with varied user queries, which limits evaluation effectiveness and practical utility.

Method: A new RFxG taxonomy, categorizing saliency explanations by Reference-Frame (pointwise vs. contrastive explanations) and Granularity (fine-grained vs. coarse-grained), is introduced. Four novel faithfulness metrics are proposed to evaluate explanation methods systematically.

Result: The approach evaluates ten saliency methods, four model architectures, and three datasets, revealing critical limitations in existing metrics and demonstrating the benefits of considering both RFxG dimensions.

Conclusion: Emphasizing user-centric evaluation, the work lays a conceptual and practical groundwork for creating better visual explanations aligned with human understanding.

Abstract: Saliency maps are widely used for visual explanations in deep learning, but a fundamental lack of consensus persists regarding their intended purpose and alignment with diverse user queries. This ambiguity hinders the effective evaluation and practical utility of explanation methods.We address this gap by introducing the Reference-Frame $\times$ Granularity (RFxG) taxonomy, a principled conceptual framework that organizes saliency explanations along two essential axes:Reference-Frame: Distinguishing between pointwise ("Why this prediction?") and contrastive ("Why this and not an alternative?") explanations.Granularity: Ranging from fine-grained class-level (e.g., "Why Husky?") to coarse-grained group-level (e.g., "Why Dog?") interpretations.Using the RFxG lens, we demonstrate critical limitations in existing evaluation metrics, which overwhelmingly prioritize pointwise faithfulness while neglecting contrastive reasoning and semantic granularity. To systematically assess explanation quality across both RFxG dimensions, we propose four novel faithfulness metrics. Our comprehensive evaluation framework applies these metrics to ten state-of-the-art saliency methods, four model architectures, and three datasets.By advocating a shift toward user-intent-driven evaluation, our work provides both the conceptual foundation and the practical tools necessary to develop visual explanations that are not only faithful to the underlying model behavior but are also meaningfully aligned with the complexity of human understanding and inquiry.

</details>


### [399] [MergeSlide: Continual Model Merging and Task-to-Class Prompt-Aligned Inference for Lifelong Learning on Whole Slide Images](https://arxiv.org/abs/2511.13099)
*Doanh C. Bui,Ba Hung Ngo,Hoai Luan Pham,Khang Nguyen,Maï K. Nguyen,Yasuhiko Nakashima*

Main category: cs.CV

TL;DR: This paper proposes MergeSlide, a novel framework for lifelong learning on Whole Slide Images (WSIs), tackling significant challenges in training and task management across multiple cancer datasets.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the issue of training unified models on large-scale WSIs for sequential cancer-related tasks while managing resource and memory limitations and mitigating forgetting when switching tasks.

Method: MergeSlide defines tasks using class-aware prompts, fine-tunes them on an MLP-free backbone with minimal epochs, and merges them into a unified model using an orthogonal continual merging strategy. It also introduces TCP inference for class-incremental learning where task identities are unknown.

Result: Experiments on six TCGA datasets demonstrate that MergeSlide outperforms traditional rehearsal-based continual learning techniques and vision-language zero-shot models.

Conclusion: MergeSlide effectively mitigates catastrophic forgetting, improves performance in lifelong learning problems on WSIs, and offers a practical solution with readily available code for implementation.

Abstract: Lifelong learning on Whole Slide Images (WSIs) aims to train or fine-tune a unified model sequentially on cancer-related tasks, reducing the resources and effort required for data transfer and processing, especially given the gigabyte-scale size of WSIs. In this paper, we introduce MergeSlide, a simple yet effective framework that treats lifelong learning as a model merging problem by leveraging a vision-language pathology foundation model. When a new task arrives, it is: 1) defined with class-aware prompts, 2) fine-tuned for a few epochs using an MLP-free backbone, and 3) merged into a unified model using an orthogonal continual merging strategy that preserves performance and mitigates catastrophic forgetting. For inference under the class-incremental learning (CLASS-IL) setting, where task identity is unknown, we introduce Task-to-Class Prompt-aligned (TCP) inference. Specifically, TCP first identifies the most relevant task using task-level prompts and then applies the corresponding class-aware prompts to generate predictions. To evaluate MergeSlide, we conduct experiments on a stream of six TCGA datasets. The results show that MergeSlide outperforms both rehearsal-based continual learning and vision-language zero-shot baselines. Code and data are available at https://github.com/caodoanh2001/MergeSlide.

</details>


### [400] [CapeNext: Rethinking and refining dynamic support information for category-agnostic pose estimation](https://arxiv.org/abs/2511.13102)
*Yu Zhu,Dan Zeng,Shuiwang Li,Qijun Zhao,Qiaomu Shen,Bo Tang*

Main category: cs.CV

TL;DR: This paper proposes a new framework, CapeNext, to address challenges in Category-Agnostic Pose Estimation (CAPE), outperforming current state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address limitations in static joint embeddings for CAPE, such as cross-category ambiguity and weak intra-category discrimination.

Method: A new framework is developed using hierarchical cross-modal interaction and dual-stream feature refinement to enhance embeddings with textual descriptions and image-specific cues.

Result: CapeNext surpasses existing CAPE methods by a significant margin in experiments using the MP-100 dataset, regardless of the network backbone.

Conclusion: Integrating class-level and instance-specific semantic cues resolves limitations in prior CAPE models, offering a superior approach for pose estimation.

Abstract: Recent research in Category-Agnostic Pose Estimation (CAPE) has adopted fixed textual keypoint description as semantic prior for two-stage pose matching frameworks. While this paradigm enhances robustness and flexibility by disentangling the dependency of support images, our critical analysis reveals two inherent limitations of static joint embedding: (1) polysemy-induced cross-category ambiguity during the matching process(e.g., the concept "leg" exhibiting divergent visual manifestations across humans and furniture), and (2) insufficient discriminability for fine-grained intra-category variations (e.g., posture and fur discrepancies between a sleeping white cat and a standing black cat). To overcome these challenges, we propose a new framework that innovatively integrates hierarchical cross-modal interaction with dual-stream feature refinement, enhancing the joint embedding with both class-level and instance-specific cues from textual description and specific images. Experiments on the MP-100 dataset demonstrate that, regardless of the network backbone, CapeNext consistently outperforms state-of-the-art CAPE methods by a large margin.

</details>


### [401] [PlugTrack: Multi-Perceptive Motion Analysis for Adaptive Fusion in Multi-Object Tracking](https://arxiv.org/abs/2511.13105)
*Seungjae Kim,SeungJoon Lee,MyeongAh Cho*

Main category: cs.CV

TL;DR: PlugTrack is a novel framework combining Kalman filters and data-driven predictors for improved multi-object tracking (MOT).


<details>
  <summary>Details</summary>
Motivation: To address the shortcomings of both Kalman filters (ineffective for non-linear motion) and data-driven motion predictors (poor domain generalization and computational overhead) by creating a more adaptive and versatile tracking system.

Method: The proposed PlugTrack adaptively fuses Kalman filters and data-driven motion predictors using multi-perceptive motion analysis and blending factors.

Result: PlugTrack demonstrates significant performance gains on MOT17/MOT20 datasets and achieves state-of-the-art results on DanceTrack.

Conclusion: This study provides the first framework that successfully bridges classical Kalman filters with modern data-driven motion prediction paradigms, highlighting the effectiveness of adaptive fusion in dealing with diverse motion patterns in real-world scenarios.

Abstract: Multi-object tracking (MOT) predominantly follows the tracking-by-detection paradigm, where Kalman filters serve as the standard motion predictor due to computational efficiency but inherently fail on non-linear motion patterns. Conversely, recent data-driven motion predictors capture complex non-linear dynamics but suffer from limited domain generalization and computational overhead. Through extensive analysis, we reveal that even in datasets dominated by non-linear motion, Kalman filter outperforms data-driven predictors in up to 34\% of cases, demonstrating that real-world tracking scenarios inherently involve both linear and non-linear patterns. To leverage this complementarity, we propose PlugTrack, a novel framework that adaptively fuses Kalman filter and data-driven motion predictors through multi-perceptive motion understanding. Our approach employs multi-perceptive motion analysis to generate adaptive blending factors. PlugTrack achieves significant performance gains on MOT17/MOT20 and state-of-the-art on DanceTrack without modifying existing motion predictors. To the best of our knowledge, PlugTrack is the first framework to bridge classical and modern motion prediction paradigms through adaptive fusion in MOT.

</details>


### [402] [Low-Level Dataset Distillation for Medical Image Enhancement](https://arxiv.org/abs/2511.13106)
*Fengzhi Xu,Ziyuan Yang,Mengyu Sun,Joey Tianyi Zhou,Yi Zhang*

Main category: cs.CV

TL;DR: The paper introduces a novel method for medical image enhancement employing low-level dataset distillation with anatomical prior and personalized generation, achieving high fidelity while preserving privacy.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need to address the limitations of existing medical image enhancement methods that rely on large datasets, causing heavy training and storage costs while ensuring pixel-level mappings for low-level tasks.

Method: The method involves constructing a shared anatomical prior, personalized for each patient using a Structure-Preserving Personalized Generation (SPG) module. Distilled datasets are created to ensure pixel fidelity and enable task-specific pairing for low-level medical image tasks.

Result: The approach allows creation of distilled datasets that capture patient-specific and task-specific abstract information, enabling low-level image enhancement without sacrificing privacy.

Conclusion: This low-level dataset distillation approach offers a practical and privacy-preserving solution for medical image enhancement, addressing both storage and training constraints while ensuring pixel-level fidelity.

Abstract: Medical image enhancement is clinically valuable, but existing methods require large-scale datasets to learn complex pixel-level mappings. However, the substantial training and storage costs associated with these datasets hinder their practical deployment. While dataset distillation (DD) can alleviate these burdens, existing methods mainly target high-level tasks, where multiple samples share the same label. This many-to-one mapping allows distilled data to capture shared semantics and achieve information compression. In contrast, low-level tasks involve a many-to-many mapping that requires pixel-level fidelity, making low-level DD an underdetermined problem, as a small distilled dataset cannot fully constrain the dense pixel-level mappings. To address this, we propose the first low-level DD method for medical image enhancement. We first leverage anatomical similarities across patients to construct the shared anatomical prior based on a representative patient, which serves as the initialization for the distilled data of different patients. This prior is then personalized for each patient using a Structure-Preserving Personalized Generation (SPG) module, which integrates patient-specific anatomical information into the distilled dataset while preserving pixel-level fidelity. For different low-level tasks, the distilled data is used to construct task-specific high- and low-quality training pairs. Patient-specific knowledge is injected into the distilled data by aligning the gradients computed from networks trained on the distilled pairs with those from the corresponding patient's raw data. Notably, downstream users cannot access raw patient data. Instead, only a distilled dataset containing abstract training information is shared, which excludes patient-specific details and thus preserves privacy.

</details>


### [403] [DGS-Net: Distillation-Guided Gradient Surgery for CLIP Fine-Tuning in AI-Generated Image Detection](https://arxiv.org/abs/2511.13108)
*Jiazhen Yan,Ziqiang Li,Fan Wang,Boyu Wang,Zhangjie Fu*

Main category: cs.CV

TL;DR: The paper introduces DGS-Net, a framework improving the detection of AI-generated images while retaining pre-trained priors.


<details>
  <summary>Details</summary>
Motivation: The proliferation of AI-generated images raises risks of misinformation, privacy issues, and trust loss, necessitating reliable detection methods.

Method: DGS-Net uses gradient-space decomposition to separate desirable and undesirable descent directions, preserving transferable pre-trained priors and suppressing irrelevant components.

Result: DGS-Net outperforms other approaches by an average margin of 6.6 in tests across 50 generative models.

Conclusion: DGS-Net achieves superior detection performance and generalization, protecting against synthetic content in diverse scenarios.

Abstract: The rapid progress of generative models such as GANs and diffusion models has led to the widespread proliferation of AI-generated images, raising concerns about misinformation, privacy violations, and trust erosion in digital media. Although large-scale multimodal models like CLIP offer strong transferable representations for detecting synthetic content, fine-tuning them often induces catastrophic forgetting, which degrades pre-trained priors and limits cross-domain generalization. To address this issue, we propose the Distillation-guided Gradient Surgery Network (DGS-Net), a novel framework that preserves transferable pre-trained priors while suppressing task-irrelevant components. Specifically, we introduce a gradient-space decomposition that separates harmful and beneficial descent directions during optimization. By projecting task gradients onto the orthogonal complement of harmful directions and aligning with beneficial ones distilled from a frozen CLIP encoder, DGS-Net achieves unified optimization of prior preservation and irrelevant suppression. Extensive experiments on 50 generative models demonstrate that our method outperforms state-of-the-art approaches by an average margin of 6.6, achieving superior detection performance and generalization across diverse generation techniques.

</details>


### [404] [Learning Implicit Neural Degradation Representation for Unpaired Image Dehazing](https://arxiv.org/abs/2511.13110)
*Shuaibin Fan,Senming Zhong,Wenchao Yan,Minglong Xue*

Main category: cs.CV

TL;DR: The paper introduces an unsupervised dehazing method using implicit neural networks to handle challenges in complex scenes like inhomogeneous haze distribution while enhancing fine-grained feature learning and global consistency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of existing dehazing methods, which struggle with fine feature representation of inhomogeneous haze and global consistency in complex scenes.

Method: The authors propose a method combining channel-independent and channel-dependent mechanisms inspired by the Kolmogorov-Arnold theorem and design an implicit neural representation to model haze degradation without explicit feature extraction or physical models.

Result: The proposed method demonstrates competitive performance on public and real datasets, achieving high-quality image restoration.

Conclusion: Their approach effectively enhances dehazed image quality by balancing fine-grained feature representation and global consistency while eliminating redundant information.

Abstract: Image dehazing is an important task in the field of computer vision, aiming at restoring clear and detail-rich visual content from haze-affected images. However, when dealing with complex scenes, existing methods often struggle to strike a balance between fine-grained feature representation of inhomogeneous haze distribution and global consistency modeling. Furthermore, to better learn the common degenerate representation of haze in spatial variations, we propose an unsupervised dehaze method for implicit neural degradation representation. Firstly, inspired by the Kolmogorov-Arnold representation theorem, we propose a mechanism combining the channel-independent and channel-dependent mechanisms, which efficiently enhances the ability to learn from nonlinear dependencies. which in turn achieves good visual perception in complex scenes. Moreover, we design an implicit neural representation to model haze degradation as a continuous function to eliminate redundant information and the dependence on explicit feature extraction and physical models. To further learn the implicit representation of the haze features, we also designed a dense residual enhancement module from it to eliminate redundant information. This achieves high-quality image restoration. Experimental results show that our method achieves competitive dehaze performance on various public and real-world datasets. This project code will be available at https://github.com/Fan-pixel/NeDR-Dehaze.

</details>


### [405] [Semantics and Content Matter: Towards Multi-Prior Hierarchical Mamba for Image Deraining](https://arxiv.org/abs/2511.13113)
*Zhaocheng Yu,Kui Jiang,Junjun Jiang,Xianming Liu,Guanglu Sun,Yi Xiao*

Main category: cs.CV

TL;DR: The paper proposes the Multi-Prior Hierarchical Mamba (MPHM) network for image deraining, which outperforms existing methods in retaining semantic and spatial details.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of existing image deraining methods that struggle with preserving semantic and spatial details, crucial for computer vision applications like autonomous driving and video surveillance.

Method: The authors introduced an MPHM network that uses macro-semantic textual priors (CLIP) and micro-structural visual priors (DINOv2) with a progressive Priors Fusion Injection (PFI) mechanism and a Hierarchical Mamba Module (HMM) featuring a Fourier-enhanced dual-path design.

Result: The MPHM network demonstrated state-of-the-art results, achieving a 0.57 dB PSNR improvement on the Rain200H dataset and superior generalization to real-world rainy scenarios.

Conclusion: The study confirms the effectiveness of integrating multi-prior information and advanced architectural design for enhanced image deraining performance.

Abstract: Rain significantly degrades the performance of computer vision systems, particularly in applications like autonomous driving and video surveillance. While existing deraining methods have made considerable progress, they often struggle with fidelity of semantic and spatial details. To address these limitations, we propose the Multi-Prior Hierarchical Mamba (MPHM) network for image deraining. This novel architecture synergistically integrates macro-semantic textual priors (CLIP) for task-level semantic guidance and micro-structural visual priors (DINOv2) for scene-aware structural information. To alleviate potential conflicts between heterogeneous priors, we devise a progressive Priors Fusion Injection (PFI) that strategically injects complementary cues at different decoder levels. Meanwhile, we equip the backbone network with an elaborate Hierarchical Mamba Module (HMM) to facilitate robust feature representation, featuring a Fourier-enhanced dual-path design that concurrently addresses global context modeling and local detail recovery. Comprehensive experiments demonstrate MPHM's state-of-the-art performance, achieving a 0.57 dB PSNR gain on the Rain200H dataset while delivering superior generalization on real-world rainy scenarios.

</details>


### [406] [A Lightweight 3D Anomaly Detection Method with Rotationally Invariant Features](https://arxiv.org/abs/2511.13115)
*Hanzhe Liang,Jie Zhou,Can Gao,Bingyang Guo,Jinbao Wang,Linlin Shen*

Main category: cs.CV

TL;DR: The paper proposes a framework utilizing rotationally invariant features for 3D anomaly detection, ensuring robust performance across varying orientations and positions of point clouds.


<details>
  <summary>Details</summary>
Motivation: Existing 3D anomaly detection methods struggle with point clouds that undergo orientation and positional changes, causing significant feature discrepancies.

Method: The framework uses Point Coordinate Mapping (PCM) to transform point data into a rotationally invariant space and employs CTF-Net for robust feature extraction. Transfer learning is applied to enhance feature extractor capabilities using 3D data augmentation.

Result: The approach outperforms existing methods, evidenced by notable improvements in anomaly detection metrics (P-AUROC) across two datasets: Anomaly-ShapeNet (17.7%) and Real3D-AD (1.6%).

Conclusion: The RIF framework demonstrates strong generalization for anomaly detection tasks and has significant industrial application potential, addressing orientation and positional variations effectively.

Abstract: 3D anomaly detection (AD) is a crucial task in computer vision, aiming to identify anomalous points or regions from point cloud data. However, existing methods may encounter challenges when handling point clouds with changes in orientation and position because the resulting features may vary significantly. To address this problem, we propose a novel Rotationally Invariant Features (RIF) framework for 3D AD. Firstly, to remove the adverse effect of variations on point cloud data, we develop a Point Coordinate Mapping (PCM) technique, which maps each point into a rotationally invariant space to maintain consistency of representation. Then, to learn robust and discriminative features, we design a lightweight Convolutional Transform Feature Network (CTF-Net) to extract rotationally invariant features for the memory bank. To improve the ability of the feature extractor, we introduce the idea of transfer learning to pre-train the feature extractor with 3D data augmentation. Experimental results show that the proposed method achieves the advanced performance on the Anomaly-ShapeNet dataset, with an average P-AUROC improvement of 17.7\%, and also gains the best performance on the Real3D-AD dataset, with an average P-AUROC improvement of 1.6\%. The strong generalization ability of RIF has been verified by combining it with traditional feature extraction methods on anomaly detection tasks, demonstrating great potential for industrial applications.

</details>


### [407] [CloseUpShot: Close-up Novel View Synthesis from Sparse-views via Point-conditioned Diffusion Model](https://arxiv.org/abs/2511.13121)
*Yuqi Zhang,Guanying Chen,Jiaxing Chen,Chuanyu Fu,Chuan Huang,Shuguang Cui*

Main category: cs.CV

TL;DR: CloseUpShot introduces a diffusion-based framework for sparse novel view synthesis, with a focus on close-up scenes, addressing limitations around sparse inputs and background leakage.


<details>
  <summary>Details</summary>
Motivation: To tackle challenges in reconstructing 3D scenes and synthesizing novel views from sparse input views, particularly in close-up scenarios where traditional methods fail to capture fine-grained details.

Method: The paper proposes CloseUpShot, a framework employing hierarchical warping, occlusion-aware noise suppression, and global structure guidance based on fused point cloud data to improve input conditioning and geometric consistency for video diffusion models.

Result: CloseUpShot demonstrates superior performance compared to existing approaches in close-up view synthesis across multiple datasets, validating the proposed methods.

Conclusion: The framework effectively enhances novel view synthesis under sparse close-up conditions, addressing critical issues like sparsity-induced quality degradation and lack of globally consistent 3D constraints.

Abstract: Reconstructing 3D scenes and synthesizing novel views from sparse input views is a highly challenging task. Recent advances in video diffusion models have demonstrated strong temporal reasoning capabilities, making them a promising tool for enhancing reconstruction quality under sparse-view settings. However, existing approaches are primarily designed for modest viewpoint variations, which struggle in capturing fine-grained details in close-up scenarios since input information is severely limited. In this paper, we present a diffusion-based framework, called CloseUpShot, for close-up novel view synthesis from sparse inputs via point-conditioned video diffusion. Specifically, we observe that pixel-warping conditioning suffers from severe sparsity and background leakage in close-up settings. To address this, we propose hierarchical warping and occlusion-aware noise suppression, enhancing the quality and completeness of the conditioning images for the video diffusion model. Furthermore, we introduce global structure guidance, which leverages a dense fused point cloud to provide consistent geometric context to the diffusion process, to compensate for the lack of globally consistent 3D constraints in sparse conditioning inputs. Extensive experiments on multiple datasets demonstrate that our method outperforms existing approaches, especially in close-up novel view synthesis, clearly validating the effectiveness of our design.

</details>


### [408] [Region-Point Joint Representation for Effective Trajectory Similarity Learning](https://arxiv.org/abs/2511.13125)
*Hao Long,Silin Zhou,Lisi Chen,Shuo Shang*

Main category: cs.CV

TL;DR: The paper introduces RePo, a model for trajectory similarity modeling, which combines region-wise and point-wise features to enhance spatial and movement context comprehension, significantly outperforming previous methods.


<details>
  <summary>Details</summary>
Motivation: State-of-the-art methods do not fully utilize the comprehensive spectrum of trajectory information, limiting their effectiveness in trajectory similarity modeling.

Method: The proposed RePo method encodes region-wise and point-wise features by combining grid sequence mappings with fine-grained GPS patterns. These features are integrated through expert networks and a router network using cross-attention techniques.

Result: RePo outperforms state-of-the-art baselines, achieving a 22.2% average accuracy improvement across all evaluation metrics.

Conclusion: RePo effectively addresses trajectory modeling limitations by integrating robust region and point-specific trajectory features, thus improving accuracy and enhancing modeling of spatial and movement contexts.

Abstract: Recent learning-based methods have reduced the computational complexity of traditional trajectory similarity computation, but state-of-the-art (SOTA) methods still fail to leverage the comprehensive spectrum of trajectory information for similarity modeling. To tackle this problem, we propose \textbf{RePo}, a novel method that jointly encodes \textbf{Re}gion-wise and \textbf{Po}int-wise features to capture both spatial context and fine-grained moving patterns. For region-wise representation, the GPS trajectories are first mapped to grid sequences, and spatial context are captured by structural features and semantic context enriched by visual features. For point-wise representation, three lightweight expert networks extract local, correlation, and continuous movement patterns from dense GPS sequences. Then, a router network adaptively fuses the learned point-wise features, which are subsequently combined with region-wise features using cross-attention to produce the final trajectory embedding. To train RePo, we adopt a contrastive loss with hard negative samples to provide similarity ranking supervision. Experiment results show that RePo achieves an average accuracy improvement of 22.2\% over SOTA baselines across all evaluation metrics.

</details>


### [409] [VEIL: Jailbreaking Text-to-Video Models via Visual Exploitation from Implicit Language](https://arxiv.org/abs/2511.13127)
*Zonghao Ying,Moyang Chen,Nizhang Li,Zhiqiang Wang,Wenxin Zhang,Quanchen Zou,Zonglei Jing,Aishan Liu,Xianglong Liu*

Main category: cs.CV

TL;DR: This paper introduces VEIL, a novel jailbreak framework for text-to-video (T2V) models that exploits implicit cues in prompts to generate semantically unsafe videos, despite circumventing typical safety constraints.


<details>
  <summary>Details</summary>
Motivation: Existing jailbreak attacks on T2V models are either easily detectable or defensible as they focus on modifying visibly unsafe prompts. Thus, there is a need for a stealthier approach that uses benign-looking prompts while still leading to unintended, unsafe outputs.

Method: The authors developed VEIL, which creates adversarial prompts using three modular components: neutral scene anchors, latent auditory triggers, and stylistic modulators. The attack generation process is framed as a constrained optimization problem and solved using a guided search technique.

Result: The VEIL framework improves the attack success rate by 23% on average across seven commercial T2V models, showcasing its effectiveness.

Conclusion: VEIL exposes significant blind spots in T2V model guardrails by demonstrating how benign-looking prompts can generate semantically unsafe videos. The findings highlight the need for better safeguards to address such advanced adversarial strategies.

Abstract: Jailbreak attacks can circumvent model safety guardrails and reveal critical blind spots. Prior attacks on text-to-video (T2V) models typically add adversarial perturbations to obviously unsafe prompts, which are often easy to detect and defend. In contrast, we show that benign-looking prompts containing rich, implicit cues can induce T2V models to generate semantically unsafe videos that both violate policy and preserve the original (blocked) intent. To realize this, we propose VEIL, a jailbreak framework that leverages T2V models' cross-modal associative patterns via a modular prompt design. Specifically, our prompts combine three components: neutral scene anchors, which provide the surface-level scene description extracted from the blocked intent to maintain plausibility; latent auditory triggers, textual descriptions of innocuous-sounding audio events (e.g., creaking, muffled noises) that exploit learned audio-visual co-occurrence priors to bias the model toward particular unsafe visual concepts; and stylistic modulators, cinematic directives (e.g., camera framing, atmosphere) that amplify and stabilize the latent trigger's effect. We formalize attack generation as a constrained optimization over the above modular prompt space and solve it with a guided search procedure that balances stealth and effectiveness. Extensive experiments over 7 T2V models demonstrate the efficacy of our attack, achieving a 23 percent improvement in average attack success rate in commercial models.

</details>


### [410] [Shedding Light on VLN Robustness: A Black-box Framework for Indoor Lighting-based Adversarial Attack](https://arxiv.org/abs/2511.13132)
*Chenyang Li,Wenbing Tang,Yihao Huang,Sinong Simon Zhan,Ming Hu,Xiaojun Jia,Yang Liu*

Main category: cs.CV

TL;DR: This paper introduces a novel adversarial evaluation framework for VLN agents using realistic indoor lighting variations to assess robustness.


<details>
  <summary>Details</summary>
Motivation: The study addresses the gap in evaluating VLN agents under realistic conditions, focusing on indoor lighting instead of artificial patterns.

Method: Indoor Lighting-based Adversarial Attack (ILA) framework, with two modes: SILA and DILA, to manipulate lighting and challenge VLN agents.

Result: ILA significantly increases failure rates and decreases trajectory efficiency of state-of-the-art VLN agents across navigation tasks.

Conclusion: VLN agents are vulnerable to realistic indoor lighting variations, highlighting the need to improve robustness in practical scenarios.

Abstract: Vision-and-Language Navigation (VLN) agents have made remarkable progress, but their robustness remains insufficiently studied. Existing adversarial evaluations often rely on perturbations that manifest as unusual textures rarely encountered in everyday indoor environments. Errors under such contrived conditions have limited practical relevance, as real-world agents are unlikely to encounter such artificial patterns. In this work, we focus on indoor lighting, an intrinsic yet largely overlooked scene attribute that strongly influences navigation. We propose Indoor Lighting-based Adversarial Attack (ILA), a black-box framework that manipulates global illumination to disrupt VLN agents. Motivated by typical household lighting usage, we design two attack modes: Static Indoor Lighting-based Attack (SILA), where the lighting intensity remains constant throughout an episode, and Dynamic Indoor Lighting-based Attack (DILA), where lights are switched on or off at critical moments to induce abrupt illumination changes. We evaluate ILA on two state-of-the-art VLN models across three navigation tasks. Results show that ILA significantly increases failure rates while reducing trajectory efficiency, revealing previously unrecognized vulnerabilities of VLN agents to realistic indoor lighting variations.

</details>


### [411] [MedGEN-Bench: Contextually entangled benchmark for open-ended multimodal medical generation](https://arxiv.org/abs/2511.13135)
*Junjie Yang,Yuhao Yan,Gang Wu,Yuxuan Wang,Ruoyu Liang,Xinjie Jiang,Xiang Wan,Fenglei Fan,Yongquan Zhang,Feiwei Qin,Changmiao Wan*

Main category: cs.CV

TL;DR: MedGEN-Bench is introduced as a comprehensive multimodal benchmark addressing limitations in existing medical visual benchmarks, offering 6,422 image-text pairs and emphasizing, contextual cross-modal reasoning and generative outputs.


<details>
  <summary>Details</summary>
Motivation: There are notable limitations in existing medical visual benchmarks, such as irrelevant queries, oversimplified diagnostic reasoning, and text-centric evaluation paradigms that do not fully account for image generation capabilities in medical applications.

Method: MedGEN-Bench integrates expert-validated data across six imaging modalities, 16 clinical tasks, structured into Visual Question Answering, Image Editing, and Contextual Multimodal Generation formats. Evaluation is performed through a three-tier framework involving pixel-level metrics, semantic text analysis, and clinical relevance scoring.

Result: MedGEN-Bench systematically evaluates multiple frameworks including compositional, unified models, and Vision-Language Models, providing insights into their capabilities regarding cross-modal reasoning and generative outputs.

Conclusion: MedGEN-Bench advances medical AI research by addressing the challenges of traditional benchmarks and offering sophisticated multimodal analyses, paving the way for improved model evaluation in clinical workflows.

Abstract: As Vision-Language Models (VLMs) increasingly gain traction in medical applications, clinicians are progressively expecting AI systems not only to generate textual diagnoses but also to produce corresponding medical images that integrate seamlessly into authentic clinical workflows. Despite the growing interest, existing medical visual benchmarks present notable limitations. They often rely on ambiguous queries that lack sufficient relevance to image content, oversimplify complex diagnostic reasoning into closed-ended shortcuts, and adopt a text-centric evaluation paradigm that overlooks the importance of image generation capabilities. To address these challenges, we introduce \textsc{MedGEN-Bench}, a comprehensive multimodal benchmark designed to advance medical AI research. MedGEN-Bench comprises 6,422 expert-validated image-text pairs spanning six imaging modalities, 16 clinical tasks, and 28 subtasks. It is structured into three distinct formats: Visual Question Answering, Image Editing, and Contextual Multimodal Generation. What sets MedGEN-Bench apart is its focus on contextually intertwined instructions that necessitate sophisticated cross-modal reasoning and open-ended generative outputs, moving beyond the constraints of multiple-choice formats. To evaluate the performance of existing systems, we employ a novel three-tier assessment framework that integrates pixel-level metrics, semantic text analysis, and expert-guided clinical relevance scoring. Using this framework, we systematically assess 10 compositional frameworks, 3 unified models, and 5 VLMs.

</details>


### [412] [WinMamba: Multi-Scale Shifted Windows in State Space Model for 3D Object Detection](https://arxiv.org/abs/2511.13138)
*Longhui Zheng,Qiming Xia,Xiaolu Chen,Zhaoliang Liu,Chenglu Wen*

Main category: cs.CV

TL;DR: WinMamba is a novel 3D object detection method for autonomous driving, balancing efficiency and accuracy by leveraging a linear state-space design and advanced feature encoding techniques.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in 3D object detection for autonomous driving, specifically balancing computational efficiency with the need to capture long-range spatial dependencies.

Method: The paper introduces WinMamba, a 3D feature-encoding backbone that adopts window-scale-adaptive and window-shift strategies for robust multi-scale voxel feature representation and rich contextual cues. The design is based on stacked WinMamba blocks with learnable positional encoding.

Result: WinMamba significantly outperforms baseline models on KITTI and Waymo datasets, demonstrating its effectiveness. Ablation studies confirm that key modules enhance detection accuracy.

Conclusion: WinMamba effectively balances efficiency and accuracy, advancing the state of 3D object detection for autonomous vehicles. Code will be released for community use.

Abstract: 3D object detection is critical for autonomous driving, yet it remains fundamentally challenging to simultaneously maximize computational efficiency and capture long-range spatial dependencies. We observed that Mamba-based models, with their linear state-space design, capture long-range dependencies at lower cost, offering a promising balance between efficiency and accuracy. However, existing methods rely on axis-aligned scanning within a fixed window, inevitably discarding spatial information. To address this problem, we propose WinMamba, a novel Mamba-based 3D feature-encoding backbone composed of stacked WinMamba blocks. To enhance the backbone with robust multi-scale representation, the WinMamba block incorporates a window-scale-adaptive module that compensates voxel features across varying resolutions during sampling. Meanwhile, to obtain rich contextual cues within the linear state space, we equip the WinMamba layer with a learnable positional encoding and a window-shift strategy. Extensive experiments on the KITTI and Waymo datasets demonstrate that WinMamba significantly outperforms the baseline. Ablation studies further validate the individual contributions of the WSF and AWF modules in improving detection accuracy. The code will be made publicly available.

</details>


### [413] [Automated Road Distress Detection Using Vision Transformersand Generative Adversarial Networks](https://arxiv.org/abs/2511.13145)
*Cesar Portocarrero Rodriguez,Laura Vandeweyen,Yosuke Yamamoto*

Main category: cs.CV

TL;DR: The paper explores using computer vision for road monitoring, specifically evaluating GAN-generated data and applying advanced models for road distress segmentation.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency in traditional road inspection methods and improve regional infrastructure maintenance using automated techniques.

Method: The study utilized synthetic data generated by GANs, applied CNNs and transformer-based MaskFormer for road segmentation, and evaluated their performance using specific metrics.

Result: GAN-generated data improved model performance, and MaskFormer outperformed CNN in terms of mAP50 and IoU metrics.

Conclusion: The application of advanced computer vision models, particularly MaskFormer with GAN-generated data, can enhance road distress detection for infrastructure maintenance.

Abstract: The American Society of Civil Engineers has graded Americas infrastructure condition as a C, with the road system receiving a dismal D. Roads are vital to regional economic viability, yet their management, maintenance, and repair processes remain inefficient, relying on outdated manual or laser-based inspection methods that are both costly and time-consuming. With the increasing availability of real-time visual data from autonomous vehicles, there is an opportunity to apply computer vision (CV) methods for advanced road monitoring, providing insights to guide infrastructure rehabilitation efforts. This project explores the use of state-of-the-art CV techniques for road distress segmentation. It begins by evaluating synthetic data generated with Generative Adversarial Networks (GANs) to assess its usefulness for model training. The study then applies Convolutional Neural Networks (CNNs) for road distress segmentation and subsequently examines the transformer-based model MaskFormer. Results show that GAN-generated data improves model performance and that MaskFormer outperforms the CNN model in two metrics: mAP50 and IoU.

</details>


### [414] [Skeletons Speak Louder than Text: A Motion-Aware Pretraining Paradigm for Video-Based Person Re-Identification](https://arxiv.org/abs/2511.13150)
*Rifen Lin,Alex Jinpeng Wang,Jiawei Mo,Min Li*

Main category: cs.CV

TL;DR: This paper introduces CSIP-ReID, focusing on skeleton-driven pretraining for enhancing video-based person ReID by addressing limitations of text-based approaches.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance video-based person re-identification (ReID) by addressing the lack of genuine multimodal pretraining and improving the capture of fine-grained temporal motion cues, which are not effectively represented in text-based methods.

Method: The authors propose CSIP-ReID, a two-stage method: contrastive learning to align skeleton and visual features, followed by a Prototype Fusion Updater (PFU) to refine identity prototypes using motion and appearance cues. Additionally, they introduce a Skeleton Guided Temporal Modeling (SGTM) module to integrate temporal skeleton cues into visual features.

Result: CSIP-ReID achieves state-of-the-art performance on video ReID benchmarks (MARS, LS-VID, iLIDS-VID), and shows superior generalization on skeleton-only ReID tasks (BIWI, IAS).

Conclusion: CSIP-ReID introduces a novel annotation-free and motion-aware pretraining paradigm, leading to advancements in multimodal representation learning for ReID.

Abstract: Multimodal pretraining has revolutionized visual understanding, but its impact on video-based person re-identification (ReID) remains underexplored. Existing approaches often rely on video-text pairs, yet suffer from two fundamental limitations: (1) lack of genuine multimodal pretraining, and (2) text poorly captures fine-grained temporal motion-an essential cue for distinguishing identities in video. In this work, we take a bold departure from text-based paradigms by introducing the first skeleton-driven pretraining framework for ReID. To achieve this, we propose Contrastive Skeleton-Image Pretraining for ReID (CSIP-ReID), a novel two-stage method that leverages skeleton sequences as a spatiotemporally informative modality aligned with video frames. In the first stage, we employ contrastive learning to align skeleton and visual features at sequence level. In the second stage, we introduce a dynamic Prototype Fusion Updater (PFU) to refine multimodal identity prototypes, fusing motion and appearance cues. Moreover, we propose a Skeleton Guided Temporal Modeling (SGTM) module that distills temporal cues from skeleton data and integrates them into visual features. Extensive experiments demonstrate that CSIP-ReID achieves new state-of-the-art results on standard video ReID benchmarks (MARS, LS-VID, iLIDS-VID). Moreover, it exhibits strong generalization to skeleton-only ReID tasks (BIWI, IAS), significantly outperforming previous methods. CSIP-ReID pioneers an annotation-free and motion-aware pretraining paradigm for ReID, opening a new frontier in multimodal representation learning.

</details>


### [415] [SOMA: Feature Gradient Enhanced Affine-Flow Matching for SAR-Optical Registration](https://arxiv.org/abs/2511.13168)
*Haodong Wang,Tao Zhuo,Xiuwei Zhang,Hanlin Yin,Wencong Wu,Yanning Zhang*

Main category: cs.CV

TL;DR: The paper proposes SOMA, a dense registration framework that leverages structural gradient priors and hybrid matching to improve pixel-level alignment of SAR-Optical images.


<details>
  <summary>Details</summary>
Motivation: Pixel-level registration between SAR and Optical images is challenging due to their distinct imaging mechanisms and deep learning's limited effectiveness in this domain.

Method: The method integrates a Feature Gradient Enhancer (FGE) to embed gradient priors into features and a Global-Local Affine-Flow Matcher (GLAM) for coarse-to-fine structural alignment.

Result: SOMA improves registration precision significantly, increasing CMR@1px by 12.29% on SEN1-2 and 18.50% on GFGE_SO datasets, showing robustness across diverse scenes.

Conclusion: SOMA effectively addresses SAR-Optical image registration challenges, leveraging structural gradients and hybrid matching for precision and robustness in diverse conditions.

Abstract: Achieving pixel-level registration between SAR and optical images remains a challenging task due to their fundamentally different imaging mechanisms and visual characteristics. Although deep learning has achieved great success in many cross-modal tasks, its performance on SAR-Optical registration tasks is still unsatisfactory. Gradient-based information has traditionally played a crucial role in handcrafted descriptors by highlighting structural differences. However, such gradient cues have not been effectively leveraged in deep learning frameworks for SAR-Optical image matching. To address this gap, we propose SOMA, a dense registration framework that integrates structural gradient priors into deep features and refines alignment through a hybrid matching strategy. Specifically, we introduce the Feature Gradient Enhancer (FGE), which embeds multi-scale, multi-directional gradient filters into the feature space using attention and reconstruction mechanisms to boost feature distinctiveness. Furthermore, we propose the Global-Local Affine-Flow Matcher (GLAM), which combines affine transformation and flow-based refinement within a coarse-to-fine architecture to ensure both structural consistency and local accuracy. Experimental results demonstrate that SOMA significantly improves registration precision, increasing the CMR@1px by 12.29% on the SEN1-2 dataset and 18.50% on the GFGE_SO dataset. In addition, SOMA exhibits strong robustness and generalizes well across diverse scenes and resolutions.

</details>


### [416] [THIR: Topological Histopathological Image Retrieval](https://arxiv.org/abs/2511.13170)
*Zahra Tabatabaei,Jon Sporring*

Main category: cs.CV

TL;DR: The study introduces THIR, a CBMIR framework using topological data analysis to retrieve histopathological images efficiently, outperforming current methods and working training-free.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the global challenge of reducing deaths from breast cancer by improving early diagnosis and clinical decision-making, offering a novel approach to medical image retrieval.

Method: THIR leverages topological data analysis, using Betti numbers from persistent homology to extract topological fingerprints of histopathological images via cubical persistence. These feature vectors are used for similarity retrieval based on topological descriptors.

Result: Experiments on the BreaKHis dataset show that THIR surpasses existing methods, both supervised and unsupervised, processing the entire dataset in less than 20 minutes on a standard CPU.

Conclusion: THIR provides an efficient, scalable, and training-free content-based image retrieval solution for clinical applications, facilitating accurate and rapid diagnosis.

Abstract: According to the World Health Organization, breast cancer claimed the lives of approximately 685,000 women in 2020. Early diagnosis and accurate clinical decision making are critical in reducing this global burden. In this study, we propose THIR, a novel Content-Based Medical Image Retrieval (CBMIR) framework that leverages topological data analysis specifically, Betti numbers derived from persistent homology to characterize and retrieve histopathological images based on their intrinsic structural patterns. Unlike conventional deep learning approaches that rely on extensive training, annotated datasets, and powerful GPU resources, THIR operates entirely without supervision. It extracts topological fingerprints directly from RGB histopathological images using cubical persistence, encoding the evolution of loops as compact, interpretable feature vectors. The similarity retrieval is then performed by computing the distances between these topological descriptors, efficiently returning the top-K most relevant matches.
  Extensive experiments on the BreaKHis dataset demonstrate that THIR outperforms state of the art supervised and unsupervised methods. It processes the entire dataset in under 20 minutes on a standard CPU, offering a fast, scalable, and training free solution for clinical image retrieval.

</details>


### [417] [HDW-SR: High-Frequency Guided Diffusion Model based on Wavelet Decomposition for Image Super-Resolution](https://arxiv.org/abs/2511.13175)
*Chao Yang,Boqian Zhang,Jinghao Xu,Guang Jiang*

Main category: cs.CV

TL;DR: This paper introduces HDW-SR, a novel super-resolution approach that focuses on guiding diffusion networks in the high-frequency domain using wavelet decomposition, achieving superior fine detail recovery.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based single image super-resolution approaches face challenges in recovering high-frequency details, often leading to blurred results.

Method: The proposed HDW-SR utilizes wavelet decomposition within a diffusion network to focus solely on the residual map for high-frequency information restoration. It employs wavelet-based downsampling and sparse cross-attention to guide the network in reconstructing fine details, supported by a Dynamic Thresholding Block.

Result: HDW-SR outperforms competing methods in synthetic and real-world datasets, demonstrating excellence in recovering fine-grained image details and competitive super-resolution results.

Conclusion: The use of wavelet decomposition and dynamic thresholding in guiding high-frequency information within a diffusion network effectively enhances super-resolution methods, setting a new standard for generating detailed and sharp images.

Abstract: Diffusion-based methods have shown great promise in single image super-resolution (SISR); however, existing approaches often produce blurred fine details due to insufficient guidance in the high-frequency domain. To address this issue, we propose a High-Frequency Guided Diffusion Network based on Wavelet Decomposition (HDW-SR), which replaces the conventional U-Net backbone in diffusion frameworks. Specifically, we perform diffusion only on the residual map, allowing the network to focus more effectively on high-frequency information restoration. We then introduce wavelet-based downsampling in place of standard CNN downsampling to achieve multi-scale frequency decomposition, enabling sparse cross-attention between the high-frequency subbands of the pre-super-resolved image and the low-frequency subbands of the diffused image for explicit high-frequency guidance. Moreover, a Dynamic Thresholding Block (DTB) is designed to refine high-frequency selection during the sparse attention process. During upsampling, the invertibility of the wavelet transform ensures low-loss feature reconstruction. Experiments on both synthetic and real-world datasets demonstrate that HDW-SR achieves competitive super-resolution performance, excelling particularly in recovering fine-grained image details. The code will be available after acceptance.

</details>


### [418] [GenTract: Generative Global Tractography](https://arxiv.org/abs/2511.13183)
*Alec Sargood,Lemuel Puglisi,Elinor Thompson,Mirco Musolesi,Daniel C. Alexander*

Main category: cs.CV

TL;DR: This paper introduces GenTract, a generative model for global tractography, to improve accuracy and precision in mapping brain white-matter pathways using diffusion MRI.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenges of error accumulation in local tractography methods and computational expense in global methods, particularly on noisy or low-resolution data.

Method: GenTract reframes tractography as a generative learning task, utilizing models that directly map dMRI data to anatomically plausible streamlines. Comparisons are made against diffusion-based and flow matching paradigms.

Result: GenTract demonstrates superior precision, achieving results 2.1 times better than the leading method, TractOracle, and significantly outperforming competitors in low-resolution and noisy scenarios.

Conclusion: GenTract provides a robust and precise solution for global tractography, excelling in both high-quality research datasets and challenging conditions.

Abstract: Tractography is the process of inferring the trajectories of white-matter pathways in the brain from diffusion magnetic resonance imaging (dMRI). Local tractography methods, which construct streamlines by following local fiber orientation estimates stepwise through an image, are prone to error accumulation and high false positive rates, particularly on noisy or low-resolution data. In contrast, global methods, which attempt to optimize a collection of streamlines to maximize compatibility with underlying fiber orientation estimates, are computationally expensive. To address these challenges, we introduce GenTract, the first generative model for global tractography. We frame tractography as a generative task, learning a direct mapping from dMRI to complete, anatomically plausible streamlines. We compare both diffusion-based and flow matching paradigms and evaluate GenTract's performance against state-of-the-art baselines. Notably, GenTract achieves precision 2.1x higher than the next-best method, TractOracle. This advantage becomes even more pronounced in challenging low-resolution and noisy settings, where it outperforms the closest competitor by an order of magnitude. By producing tractograms with high precision on research-grade data while also maintaining reliability on imperfect, lower-resolution data, GenTract represents a promising solution for global tractography.

</details>


### [419] [Large Language Models Meet Extreme Multi-label Classification: Scaling and Multi-modal Framework](https://arxiv.org/abs/2511.13189)
*Diego Ortego,Marlon Rodríguez,Mario Almagro,Kunal Dahiya,David Jiménez,Juan C. SanMiguel*

Main category: cs.CV

TL;DR: This paper proposes ViXML, a framework that combines decoder-only models and visual information for Extreme Multi-label Classification (XMC), showing substantial performance improvements.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in addressing efficiency and performance challenges in XMC while exploring the untapped potential of larger decoder-only models and visual information integration in this domain.

Method: The authors developed ViXML, a framework that combines large decoder-only models with vision foundation models, allowing integration of visual information through single image embeddings to maintain computational efficiency.

Result: ViXML outperforms existing state-of-the-art approaches, achieving up to +8.21% improvement in P@1 on the largest dataset and demonstrating the effective integration of multi-modal data.

Conclusion: By showcasing the utility of visual data and larger decoder-only models, ViXML delivers significant advances in XMC while keeping efficiency manageable. Visual metadata expansion of benchmarks also enables further research in this area.

Abstract: Foundation models have revolutionized artificial intelligence across numerous domains, yet their transformative potential remains largely untapped in Extreme Multi-label Classification (XMC). Queries in XMC are associated with relevant labels from extremely large label spaces, where it is critical to strike a balance between efficiency and performance. Therefore, many recent approaches efficiently pose XMC as a maximum inner product search between embeddings learned from small encoder-only transformer architectures. In this paper, we address two important aspects in XMC: how to effectively harness larger decoder-only models, and how to exploit visual information while maintaining computational efficiency. We demonstrate that both play a critical role in XMC separately and can be combined for improved performance. We show that a few billion-size decoder can deliver substantial improvements while keeping computational overhead manageable. Furthermore, our Vision-enhanced eXtreme Multi-label Learning framework (ViXML) efficiently integrates foundation vision models by pooling a single embedding per image. This limits computational growth while unlocking multi-modal capabilities. Remarkably, ViXML with small encoders outperforms text-only decoder in most cases, showing that an image is worth billions of parameters. Finally, we present an extension of existing text-only datasets to exploit visual metadata and make them available for future benchmarking. Comprehensive experiments across four public text-only datasets and their corresponding image enhanced versions validate our proposals' effectiveness, surpassing previous state-of-the-art by up to +8.21\% in P@1 on the largest dataset. ViXML's code is available at https://github.com/DiegoOrtego/vixml.

</details>


### [420] [Video Spatial Reasoning with Object-Centric 3D Rollout](https://arxiv.org/abs/2511.13190)
*Haoran Tang,Meng Cao,Ruyang Liu,Xiaoxi Liang,Linglong Li,Ge Li,Xiaodan Liang*

Main category: cs.CV

TL;DR: The paper proposes Object-Centric 3D Rollout (OCR) to improve spatial reasoning in video understanding by introducing structured perturbations and a new training pipeline, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of enabling robust video spatial reasoning, an unsolved problem in Multi-modal Large Language Models (MLLMs), as current methods focus narrowly on explicit object prompts and neglect broader contextual understanding.

Method: The authors developed Object-Centric 3D Rollout (OCR), which introduces perturbations to the 3D geometry of objects during training, degrading their visual cues and forcing the model to reason holistically about the entire scene. A rollout-based training pipeline is used to optimize spatial reasoning.

Result: The proposed OCR approach achieves state-of-the-art performance of 47.5% accuracy on the VSI-Bench dataset, surpassing several larger 7B models and showing better performance compared to previous rollout strategies.

Conclusion: OCR successfully enhances spatial reasoning by compelling models to analyze scenes holistically rather than narrowly focusing on explicit object prompts. This advancement demonstrates promise for improving video understanding in dynamic 3D scenes.

Abstract: Recent advances in Multi-modal Large Language Models (MLLMs) have showcased remarkable capabilities in vision-language understanding. However, enabling robust video spatial reasoning-the ability to comprehend object locations, orientations, and inter-object relationships in dynamic 3D scenes-remains a key unsolved challenge. Existing approaches primarily rely on spatially grounded supervised fine-tuning or reinforcement learning, yet we observe that such models often exhibit query-locked reasoning, focusing narrowly on objects explicitly mentioned in the prompt while ignoring critical contextual cues. To address this limitation, we propose Object-Centric 3D Rollout (OCR), a novel strategy that introduces structured perturbations to the 3D geometry of selected objects during training. By degrading object-specific visual cues and projecting the altered geometry into 2D space, OCR compels the model to reason holistically across the entire scene. We further design a rollout-based training pipeline that jointly leverages vanilla and region-noisy videos to optimize spatial reasoning trajectories. Experiments demonstrate state-of-the-art performance: our 3B-parameter model achieves 47.5% accuracy on VSI-Bench, outperforming several 7B baselines. Ablations confirm OCR's superiority over prior rollout strategies (e.g., T-GRPO, NoisyRollout).

</details>


### [421] [Birth of a Painting: Differentiable Brushstroke Reconstruction](https://arxiv.org/abs/2511.13191)
*Ying Jiang,Jiayin Lu,Yunuo Chen,Yumeng He,Kui Wu,Yin Yang,Chenfanfu Jiang*

Main category: cs.CV

TL;DR: The paper introduces a unified framework for painting synthesis combining stroke reconstruction, styled texturing, and smudging to faithfully mimic human artistic processes.


<details>
  <summary>Details</summary>
Motivation: Current generative models fail to holistically simulate the artistic process, overlooking stroke structure and realistic shading, which are vital for expressive painting creation.

Method: The framework employs differentiable paint rendering, geometry-conditioned texture synthesis, and smudging via a smudge operator, optimized through a coarse-to-fine strategy.

Result: Experiments demonstrate realistic and expressive results across diverse painting styles with smooth tonal transitions and stylized appearances.

Conclusion: The proposed approach achieves unified and expressive digital painting synthesis, significantly advancing the reproduction of human painting processes.

Abstract: Painting embodies a unique form of visual storytelling, where the creation process is as significant as the final artwork. Although recent advances in generative models have enabled visually compelling painting synthesis, most existing methods focus solely on final image generation or patch-based process simulation, lacking explicit stroke structure and failing to produce smooth, realistic shading. In this work, we present a differentiable stroke reconstruction framework that unifies painting, stylized texturing, and smudging to faithfully reproduce the human painting-smudging loop. Given an input image, our framework first optimizes single- and dual-color Bezier strokes through a parallel differentiable paint renderer, followed by a style generation module that synthesizes geometry-conditioned textures across diverse painting styles. We further introduce a differentiable smudge operator to enable natural color blending and shading. Coupled with a coarse-to-fine optimization strategy, our method jointly optimizes stroke geometry, color, and texture under geometric and semantic guidance. Extensive experiments on oil, watercolor, ink, and digital paintings demonstrate that our approach produces realistic and expressive stroke reconstructions, smooth tonal transitions, and richly stylized appearances, offering a unified model for expressive digital painting creation. See our project page for more demos: https://yingjiang96.github.io/DiffPaintWebsite/.

</details>


### [422] [Difficulty-Aware Label-Guided Denoising for Monocular 3D Object Detection](https://arxiv.org/abs/2511.13195)
*Soyul Lee,Seungmin Baek,Dongbo Min*

Main category: cs.CV

TL;DR: MonoDLGD introduces a difficulty-aware label-guided denoising framework to tackle challenges in monocular 3D object detection, enhancing robustness and geometric understanding.


<details>
  <summary>Details</summary>
Motivation: Monocular 3D object detection faces issues with ambiguous depth cues and ignores instance-level detection challenges like occlusion and truncation, leading to suboptimal results.

Method: MonoDLGD adaptively perturbs ground-truth labels based on detection difficulty, reconstructs them for geometric supervision, and jointly optimizes label reconstruction and detection tasks.

Result: MonoDLGD demonstrates state-of-the-art performance across all difficulty levels on the KITTI benchmark.

Conclusion: The paper provides a robust framework that improves 3D object detection by addressing varying object complexity and achieving superior detection accuracy.

Abstract: Monocular 3D object detection is a cost-effective solution for applications like autonomous driving and robotics, but remains fundamentally ill-posed due to inherently ambiguous depth cues. Recent DETR-based methods attempt to mitigate this through global attention and auxiliary depth prediction, yet they still struggle with inaccurate depth estimates. Moreover, these methods often overlook instance-level detection difficulty, such as occlusion, distance, and truncation, leading to suboptimal detection performance. We propose MonoDLGD, a novel Difficulty-Aware Label-Guided Denoising framework that adaptively perturbs and reconstructs ground-truth labels based on detection uncertainty. Specifically, MonoDLGD applies stronger perturbations to easier instances and weaker ones into harder cases, and then reconstructs them to effectively provide explicit geometric supervision. By jointly optimizing label reconstruction and 3D object detection, MonoDLGD encourages geometry-aware representation learning and improves robustness to varying levels of object complexity. Extensive experiments on the KITTI benchmark demonstrate that MonoDLGD achieves state-of-the-art performance across all difficulty levels.

</details>


### [423] [Self-Supervised Ultrasound Screen Detection](https://arxiv.org/abs/2511.13197)
*Alberto Gomez,Jorge Oliveira,Ramon Casero,Agis Chartsias*

Main category: cs.CV

TL;DR: This study proposes a self-supervised approach to retrieve ultrasound images from monitor photographs without using DICOM, achieving promising accuracy for cardiac view classification.


<details>
  <summary>Details</summary>
Motivation: The study addresses the dependency on DICOM for transferring US images, aiming to bypass this bottleneck for quicker testing and algorithm development.

Method: A self-supervised pipeline is proposed to rectify and extract US images from monitor photographs.

Result: The processed images maintained visual fidelity, achieving a 0.79 balanced accuracy in cardiac view classification compared to native DICOMs.

Conclusion: The proposed method shows potential to streamline ultrasound image processing and enable new algorithm prototyping without dependency on DICOM.

Abstract: Ultrasound (US) machines display images on a built-in monitor, but routine transfer to hospital systems relies on DICOM. We propose a self-supervised pipeline to extract the US image from a photograph of the monitor. This removes the DICOM bottleneck and enables rapid testing and prototyping of new algorithms. In a proof-of-concept study, the rectified images retained enough visual fidelity to classify cardiac views with a balanced accuracy of 0.79 with respect to the native DICOMs.

</details>


### [424] [RefineVAD: Semantic-Guided Feature Recalibration for Weakly Supervised Video Anomaly Detection](https://arxiv.org/abs/2511.13204)
*Junhee Lee,ChaeBeen Bang,MyoungChul Kim,MyeongAh Cho*

Main category: cs.CV

TL;DR: The paper introduces RefineVAD, a framework for Weakly-Supervised Video Anomaly Detection that considers both temporal motion patterns and semantic structures of anomalies.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address limitations in existing methods, which oversimplify anomaly classification by treating all anomalies as a single category, ignoring their semantic and temporal diversity.

Method: RefineVAD incorporates two modules: MoTAR, which uses motion-aware temporal attention and recalibration for dynamic focus on salient events, and CORE, which integrates soft anomaly category priors to refine features via cross-attention.

Result: Experiments on the WVAD benchmark demonstrate that RefineVAD effectively models anomalies by considering both temporal dynamics and semantic structure, improving performance.

Conclusion: RefineVAD proves the practicality of leveraging both temporal and semantic elements for better anomaly detection in videos. The integration of category-oriented context significantly enhances feature refinement.

Abstract: Weakly-Supervised Video Anomaly Detection aims to identify anomalous events using only video-level labels, balancing annotation efficiency with practical applicability. However, existing methods often oversimplify the anomaly space by treating all abnormal events as a single category, overlooking the diverse semantic and temporal characteristics intrinsic to real-world anomalies. Inspired by how humans perceive anomalies, by jointly interpreting temporal motion patterns and semantic structures underlying different anomaly types, we propose RefineVAD, a novel framework that mimics this dual-process reasoning. Our framework integrates two core modules. The first, Motion-aware Temporal Attention and Recalibration (MoTAR), estimates motion salience and dynamically adjusts temporal focus via shift-based attention and global Transformer-based modeling. The second, Category-Oriented Refinement (CORE), injects soft anomaly category priors into the representation space by aligning segment-level features with learnable category prototypes through cross-attention. By jointly leveraging temporal dynamics and semantic structure, explicitly models both "how" motion evolves and "what" semantic category it resembles. Extensive experiments on WVAD benchmark validate the effectiveness of RefineVAD and highlight the importance of integrating semantic context to guide feature refinement toward anomaly-relevant patterns.

</details>


### [425] [End-to-End Multi-Person Pose Estimation with Pose-Aware Video Transformer](https://arxiv.org/abs/2511.13208)
*Yonghui Yu,Jiahang Cai,Xun Wang,Wenwu Yang*

Main category: cs.CV

TL;DR: The paper introduces PAVE-Net, a fully end-to-end framework for multi-person 2D pose estimation in videos, eliminating heuristic operations and achieving significant accuracy and efficiency improvements.


<details>
  <summary>Details</summary>
Motivation: Existing methods for multi-person video pose estimation rely on a two-stage pipeline with heuristic operations, which limit accuracy and efficiency. A streamlined end-to-end approach is needed to improve performance.

Method: The proposed method, PAVE-Net, uses a spatial encoder for intra-frame relations and a spatiotemporal pose decoder for global dependencies across frames. A pose-aware attention mechanism ensures accurate association of individuals across frames, and spatiotemporal dependencies among keypoints are explicitly modeled.

Result: PAVE-Net significantly outperforms prior image-based methods and achieves performance competitive with state-of-the-art two-stage approaches, with a 6.0 mAP improvement on PoseTrack2017.

Conclusion: PAVE-Net is the first end-to-end framework for multi-person 2D pose estimation in videos, advancing the field by removing inefficiencies and delivering state-of-the-art accuracy and efficiency.

Abstract: Existing multi-person video pose estimation methods typically adopt a two-stage pipeline: detecting individuals in each frame, followed by temporal modeling for single-person pose estimation. This design relies on heuristic operations such as detection, RoI cropping, and non-maximum suppression (NMS), limiting both accuracy and efficiency. In this paper, we present a fully end-to-end framework for multi-person 2D pose estimation in videos, effectively eliminating heuristic operations. A key challenge is to associate individuals across frames under complex and overlapping temporal trajectories. To address this, we introduce a novel Pose-Aware Video transformEr Network (PAVE-Net), which features a spatial encoder to model intra-frame relations and a spatiotemporal pose decoder to capture global dependencies across frames. To achieve accurate temporal association, we propose a pose-aware attention mechanism that enables each pose query to selectively aggregate features corresponding to the same individual across consecutive frames.Additionally, we explicitly model spatiotemporal dependencies among pose keypoints to improve accuracy. Notably, our approach is the first end-to-end method for multi-frame 2D human pose estimation.Extensive experiments show that PAVE-Net substantially outperforms prior image-based end-to-end methods, achieving a \textbf{6.0} mAP improvement on PoseTrack2017, and delivers accuracy competitive with state-of-the-art two-stage video-based approaches, while offering significant gains in efficiency.Project page: https://github.com/zgspose/PAVENet

</details>


### [426] [3DAlign-DAER: Dynamic Attention Policy and Efficient Retrieval Strategy for Fine-grained 3D-Text Alignment at Scale](https://arxiv.org/abs/2511.13211)
*Yijia Fan,Jusheng Zhang,Kaitong Cai,Jing Yang,Jian Wang,Keze Wang*

Main category: cs.CV

TL;DR: This study introduces 3DAlign-DAER, a framework designed to improve text-3D geometry alignment using a dynamic attention policy and efficient retrieval strategy. It also proposes a large-scale dataset Align3D-2M.


<details>
  <summary>Details</summary>
Motivation: Existing methods face difficulties in aligning detailed textual semantics with 3D structures, particularly at large-scale datasets, limiting cross-modal applications.

Method: 3DAlign-DAER employs a Dynamic Attention Policy (DAP) with a Hierarchical Attention Fusion module for token-to-point attention learning, and uses Monte Carlo tree search to optimize attentions. During inference, an Efficient Retrieval Strategy (ERS) is used for hierarchical searching.

Result: The framework significantly improves text-3D geometry alignment performance and efficiency, supported by a new dataset with fine-grained annotations. Experiments show superior results across benchmarks.

Conclusion: 3DAlign-DAER advances cross-modal alignment research with its novel approaches and dataset, addressing limitations in scalability and fine-grained alignment.

Abstract: Despite recent advancements in 3D-text cross-modal alignment, existing state-of-the-art methods still struggle to align fine-grained textual semantics with detailed geometric structures, and their alignment performance degrades significantly when scaling to large-scale 3D databases. To overcome this limitation, we introduce 3DAlign-DAER, a unified framework designed to align text and 3D geometry via the proposed dynamic attention policy and the efficient retrieval strategy, capturing subtle correspondences for diverse cross-modal retrieval and classification tasks. Specifically, during the training, our proposed dynamic attention policy (DAP) employs the Hierarchical Attention Fusion (HAF) module to represent the alignment as learnable fine-grained token-to-point attentions. To optimize these attentions across different tasks and geometric hierarchies, our DAP further exploits the Monte Carlo tree search to dynamically calibrate HAF attention weights via a hybrid reward signal and further enhances the alignment between textual descriptions and local 3D geometry. During the inference, our 3DAlign-DAER introduces an Efficient Retrieval Strategy (ERS) to leverage efficient hierarchical searching in the large-scale embedding spaces, outperforming traditional methods (e.g., KNN) in accuracy and efficiency. Furthermore, to facilitate text-3D alignment research and train our 3DAlign-DAER, we construct Align3D-2M, a large-scale dataset featuring 2M text-3D pairs, to provide sufficient fine-grained cross-modal annotations. Extensive and comprehensive experiments demonstrate the superior performance of our 3DAlign-DAER on diverse benchmarks. We will release our codes, models, and datasets.

</details>


### [427] [Hybrid-Domain Adaptative Representation Learning for Gaze Estimation](https://arxiv.org/abs/2511.13222)
*Qida Tan,Hongyu Yang,Wenchao Du*

Main category: cs.CV

TL;DR: The paper introduces HARL, a framework to improve cross-domain gaze estimation accuracy using multi-source hybrid datasets and disentangled gaze-relevant features.


<details>
  <summary>Details</summary>
Motivation: Existing appearance-based gaze estimation methods face performance issues due to gaze-irrelevant factors like expressions or wearables, especially in cross-domain evaluations.

Method: HARL framework combines disentangled gaze-relevant representations through unsupervised domain adaptation and incorporates geometric constraints for robust gaze prediction.

Result: HARL achieves state-of-the-art gaze estimation accuracy in EyeDiap, MPIIFaceGaze, and Gaze360 datasets with competitive cross-dataset performances.

Conclusion: This innovative framework effectively addresses cross-domain gaze estimation challenges, achieving robust and accurate results while requiring minimal computational costs.

Abstract: Appearance-based gaze estimation, aiming to predict accurate 3D gaze direction from a single facial image, has made promising progress in recent years. However, most methods suffer significant performance degradation in cross-domain evaluation due to interference from gaze-irrelevant factors, such as expressions, wearables, and image quality. To alleviate this problem, we present a novel Hybrid-domain Adaptative Representation Learning (shorted by HARL) framework that exploits multi-source hybrid datasets to learn robust gaze representation. More specifically, we propose to disentangle gaze-relevant representation from low-quality facial images by aligning features extracted from high-quality near-eye images in an unsupervised domain-adaptation manner, which hardly requires any computational or inference costs. Additionally, we analyze the effect of head-pose and design a simple yet efficient sparse graph fusion module to explore the geometric constraint between gaze direction and head-pose, leading to a dense and robust gaze representation. Extensive experiments on EyeDiap, MPIIFaceGaze, and Gaze360 datasets demonstrate that our approach achieves state-of-the-art accuracy of $\textbf{5.02}^{\circ}$ and $\textbf{3.36}^{\circ}$, and $\textbf{9.26}^{\circ}$ respectively, and present competitive performances through cross-dataset evaluation. The code is available at https://github.com/da60266/HARL.

</details>


### [428] [MRIQT: Physics-Aware Diffusion Model for Image Quality Transfer in Neonatal Ultra-Low-Field MRI](https://arxiv.org/abs/2511.13232)
*Malek Al Abed,Sebiha Demir,Anne Groteklaes,Elodie Germani,Shahrooz Faghihroohi,Hemmen Sabir,Shadi Albarqouni*

Main category: cs.CV

TL;DR: The study introduces MRIQT, a diffusion framework for enhancing image quality from low-field to high-field MRI, particularly for neonatal care.


<details>
  <summary>Details</summary>
Motivation: To address the shortcomings of ultra-low-field (uLF) MRI in neonatal neuroimaging by improving its signal-to-noise ratio and diagnostic quality to match high-field (HF) MRI.

Method: MRIQT employs a 3D diffusion framework incorporating realistic K-space degradation, volumetric image-to-image generation with v-prediction, and SNR-weighted perceptual loss, trained on a diverse neonatal cohort.

Result: MRIQT outperformed GAN and CNN baselines by improving PSNR by 15.3% and achieving 1.78% greater performance than the state of the art. Physicians rated 85% of the outputs as good quality with clear pathology.

Conclusion: MRIQT demonstrates the potential for high-quality enhancement of portable uLF-MRI, making it a valuable tool for neonatal brain assessment in clinical settings.

Abstract: Portable ultra-low-field MRI (uLF-MRI, 0.064 T) offers accessible neuroimaging for neonatal care but suffers from low signal-to-noise ratio and poor diagnostic quality compared to high-field (HF) MRI. We propose MRIQT, a 3D conditional diffusion framework for image quality transfer (IQT) from uLF to HF MRI. MRIQT combines realistic K-space degradation for physics-consistent uLF simulation, v-prediction with classifier-free guidance for stable image-to-image generation, and an SNR-weighted 3D perceptual loss for anatomical fidelity. The model denoises from a noised uLF input conditioned on the same scan, leveraging volumetric attention-UNet architecture for structure-preserving translation. Trained on a neonatal cohort with diverse pathologies, MRIQT surpasses recent GAN and CNN baselines in PSNR 15.3% with 1.78% over the state of the art, while physicians rated 85% of its outputs as good quality with clear pathology present. MRIQT enables high-fidelity, diffusion-based enhancement of portable ultra-low-field (uLF) MRI for deliable neonatal brain assessment.

</details>


### [429] [MMD-Thinker: Adaptive Multi-Dimensional Thinking for Multimodal Misinformation Detection](https://arxiv.org/abs/2511.13242)
*Junjie Wu,Guohong Fu*

Main category: cs.CV

TL;DR: The paper introduces MMD-Thinker, a framework designed to enhance multimodal misinformation detection through adaptive reasoning techniques and an associated dataset.


<details>
  <summary>Details</summary>
Motivation: The work aims to tackle challenges in detecting multimodal misinformation, exacerbated by AI-generated content, including reasoning inaccuracies and biases in current models.

Method: MMD-Thinker employs tailored thinking modes, task-specific instruction tuning, reinforcement learning, and introduces a multimodal dataset (MMR) for misinformation detection.

Result: Experimental results show MMD-Thinker outperforms existing models in detecting misinformation across benchmark datasets with efficient inference.

Conclusion: MMD-Thinker demonstrates significant progress in addressing challenges in multimodal misinformation detection and shows potential for better reasoning and judgment capabilities.

Abstract: Multimodal misinformation floods on various social media, and continues to evolve in the era of AI-generated content (AIGC). The emerged misinformation with low creation cost and high deception poses significant threats to society. While recent studies leverage general-purpose multimodal large language models (MLLMs) to achieve remarkable results in detection, they encounter two critical limitations: (1) Insufficient reasoning, where general-purpose MLLMs often follow the uniform reasoning paradigm but generate inaccurate explanations and judgments, due to the lack of the task-specific knowledge of multimodal misinformation detection. (2) Reasoning biases, where a single thinking mode make detectors a suboptimal path for judgment, struggling to keep pace with the fast-growing and intricate multimodal misinformation. In this paper, we propose MMD-Thinker, a two-stage framework for multimodal misinformation detection through adaptive multi-dimensional thinking. First, we develop tailor-designed thinking mode for multimodal misinformation detection. Second, we adopt task-specific instruction tuning to inject the tailored thinking mode into general-purpose MLLMs. Third, we further leverage reinforcement learning strategy with a mixed advantage function, which incentivizes the reasoning capabilities in trajectories. Furthermore, we construct the multimodal misinformation reasoning (MMR) dataset, encompasses more than 8K image-text pairs with both reasoning processes and classification labels, to make progress in the relam of multimodal misinformation detection. Experimental results demonstrate that our proposed MMD-Thinker achieves state-of-the-art performance on both in-domain and out-of-domain benchmark datasets, while maintaining flexible inference and token usage. Code will be publicly available at Github.

</details>


### [430] [Referring Camouflaged Object Detection With Multi-Context Overlapped Windows Cross-Attention](https://arxiv.org/abs/2511.13249)
*Yu Wen,Shuyong Gao,Shuping Zhang,Miao Huang,Lili Tao,Han Yang,Haozhe Xing,Lihe Zhang,Boxue Hou*

Main category: cs.CV

TL;DR: The paper introduces RFMNet, a model for Referring Camouflaged Object Detection (Ref-COD) utilizing reference image features and a novel fusion mechanism, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: The study aims to improve Ref-COD by leveraging detailed information from reference images for better identification of camouflaged objects.

Method: RFMNet integrates features from multiple encoding stages of reference images with camouflage features and employs techniques like Overlapped Windows Cross-attention and the Referring Feature Aggregation module for progressive decoding.

Result: Extensive experiments show that RFMNet sets a new benchmark in Ref-COD performance, outperforming previous methods.

Conclusion: The proposed novel fusion strategy and modules significantly enhance the accuracy and effectiveness of camouflaged object detection.

Abstract: Referring camouflaged object detection (Ref-COD) aims to identify hidden objects by incorporating reference information such as images and text descriptions. Previous research has transformed reference images with salient objects into one-dimensional prompts, yielding significant results. We explore ways to enhance performance through multi-context fusion of rich salient image features and camouflaged object features. Therefore, we propose RFMNet, which utilizes features from multiple encoding stages of the reference salient images and performs interactive fusion with the camouflage features at the corresponding encoding stages. Given that the features in salient object images contain abundant object-related detail information, performing feature fusion within local areas is more beneficial for detecting camouflaged objects. Therefore, we propose an Overlapped Windows Cross-attention mechanism to enable the model to focus more attention on the local information matching based on reference features. Besides, we propose the Referring Feature Aggregation (RFA) module to decode and segment the camouflaged objects progressively. Extensive experiments on the Ref-COD benchmark demonstrate that our method achieves state-of-the-art performance.

</details>


### [431] [GeoX-Bench: Benchmarking Cross-View Geo-Localization and Pose Estimation Capabilities of Large Multimodal Models](https://arxiv.org/abs/2511.13259)
*Yushuo Zheng,Jiangyong Ying,Huiyu Duan,Chunyi Li,Zicheng Zhang,Jing Liu,Xiaohong Liu,Guangtao Zhai*

Main category: cs.CV

TL;DR: The paper introduces GeoX-Bench to benchmark and evaluate large multimodal models (LMMs) in cross-view geo-localization and pose estimation, with valuable findings for improving these models.


<details>
  <summary>Details</summary>
Motivation: To address the lack of exploration of LMMs' capabilities in cross-view geo-localization and pose estimation, which are critical for fields like navigation and robotics.

Method: The authors developed GeoX-Bench, a dataset with 10,859 panoramic-satellite image pairs and 755,976 QA pairs. They evaluated 25 state-of-the-art LMMs' performance and explored instruction-tuning to improve capabilities.

Result: GeoX-Bench revealed that while LMMs perform well on geo-localization tasks, their performance drops on pose estimation tasks. Instruction-tuning improves their cross-view geo-sense abilities.

Conclusion: GeoX-Bench provides crucial insights into the limitations and potential improvements for LMMs in geo-localization and pose estimation, paving the way for targeted advancements.

Abstract: Large multimodal models (LMMs) have demonstrated remarkable capabilities across a wide range of tasks, however their knowledge and abilities in the cross-view geo-localization and pose estimation domains remain unexplored, despite potential benefits for navigation, autonomous driving, outdoor robotics, \textit{etc}. To bridge this gap, we introduce \textbf{GeoX-Bench}, a comprehensive \underline{Bench}mark designed to explore and evaluate the capabilities of LMMs in \underline{cross}-view \underline{Geo}-localization and pose estimation. Specifically, GeoX-Bench contains 10,859 panoramic-satellite image pairs spanning 128 cities in 49 countries, along with corresponding 755,976 question-answering (QA) pairs. Among these, 42,900 QA pairs are designated for benchmarking, while the remaining are intended to enhance the capabilities of LMMs. Based on GeoX-Bench, we evaluate the capabilities of 25 state-of-the-art LMMs on cross-view geo-localization and pose estimation tasks, and further explore the empowered capabilities of instruction-tuning. Our benchmark demonstrate that while current LMMs achieve impressive performance in geo-localization tasks, their effectiveness declines significantly on the more complex pose estimation tasks, highlighting a critical area for future improvement, and instruction-tuning LMMs on the training data of GeoX-Bench can significantly improve the cross-view geo-sense abilities. The GeoX-Bench is available at \textcolor{magenta}{https://github.com/IntMeGroup/GeoX-Bench}.

</details>


### [432] [Building Egocentric Procedural AI Assistant: Methods, Benchmarks, and Challenges](https://arxiv.org/abs/2511.13261)
*Junlong Li,Huaiyuan Xu,Sijie Cheng,Kejun Wu,Kim-Hui Yap,Lap-Pui Chau,Yi Wang*

Main category: cs.CV

TL;DR: The paper introduces EgoProceAssist, an AI assistant concept for first-person procedural task support, defining its scope and identifying core tasks.


<details>
  <summary>Details</summary>
Motivation: To address the need for advanced AI capabilities in supporting egocentric procedural tasks that enhance daily activities.

Method: Comprehensive evaluation and technical analysis of egocentric VLM-based AI methods, novel experiments, review of techniques, datasets, and metrics.

Result: Identified gaps in existing assistants' capabilities, suggesting challenges and future research for developing EgoProceAssist.

Conclusion: The study lays groundwork for an Egocentric Procedural AI Assistant and provides a taxonomy and active repository for ongoing research.

Abstract: Driven by recent advances in vision language models (VLMs) and egocentric perception research, we introduce the concept of an egocentric procedural AI assistant (EgoProceAssist) tailored to step-by-step support daily procedural tasks in a first-person view. In this work, we start by identifying three core tasks: egocentric procedural error detection, egocentric procedural learning, and egocentric procedural question answering. These tasks define the essential functions of EgoProceAssist within a new taxonomy. Specifically, our work encompasses a comprehensive review of current techniques, relevant datasets, and evaluation metrics across these three core areas. To clarify the gap between the proposed EgoProceAssist and existing VLM-based AI assistants, we introduce novel experiments and provide a comprehensive evaluation of representative VLM-based methods. Based on these findings and our technical analysis, we discuss the challenges ahead and suggest future research directions. Furthermore, an exhaustive list of this study is publicly available in an active repository that continuously collects the latest work: https://github.com/z1oong/Building-Egocentric-Procedural-AI-Assistant

</details>


### [433] [SymGS : Leveraging Local Symmetries for 3D Gaussian Splatting Compression](https://arxiv.org/abs/2511.13264)
*Keshav Gupta,Akshat Sanghvi,Shreyas Reddy Palley,Astitva Srivastava,Charu Sharma,Avinash Sharma*

Main category: cs.CV

TL;DR: SymGS is a symmetry-aware framework for compressing 3D Gaussian Splatting data, enabling significant reductions in memory footprint while maintaining rendering quality.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the growing memory demands of 3D Gaussian Splatting in novel view synthesis, especially for complex scenes, and seeks to overcome the limits of current compression methods.

Method: The authors propose SymGS, a novel framework that incorporates learnable mirrors to identify and eliminate local and global redundant primitives, focusing on mirror symmetries.

Result: SymGS achieves an average of 108x compression of 3D Gaussian Splatting scenes, delivering higher compression compared to state-of-the-art methods like HAC, with up to 3x compression in large-scale scenes.

Conclusion: The proposed SymGS framework offers a plug-and-play solution for improving the compression of 3D Gaussian Splatting scenes while preserving photorealistic rendering quality.

Abstract: 3D Gaussian Splatting has emerged as a transformative technique in novel view synthesis, primarily due to its high rendering speed and photorealistic fidelity. However, its memory footprint scales rapidly with scene complexity, often reaching several gigabytes. Existing methods address this issue by introducing compression strategies that exploit primitive-level redundancy through similarity detection and quantization. We aim to surpass the compression limits of such methods by incorporating symmetry-aware techniques, specifically targeting mirror symmetries to eliminate redundant primitives. We propose a novel compression framework, \textbf{\textit{SymGS}}, introducing learnable mirrors into the scene, thereby eliminating local and global reflective redundancies for compression. Our framework functions as a plug-and-play enhancement to state-of-the-art compression methods, (e.g. HAC) to achieve further compression. Compared to HAC, we achieve $1.66 \times$ compression across benchmark datasets (upto $3\times$ on large-scale scenes). On an average, SymGS enables $\bf{108\times}$ compression of a 3DGS scene, while preserving rendering quality. The project page and supplementary can be found at \textbf{\color{cyan}{symgs.github.io}}

</details>


### [434] [Is your VLM Sky-Ready? A Comprehensive Spatial Intelligence Benchmark for UAV Navigation](https://arxiv.org/abs/2511.13269)
*Lingfeng Zhang,Yuchen Zhang,Hongsheng Li,Haoxiang Fu,Yingbo Tang,Hangjun Ye,Long Chen,Xiaojun Liang,Xiaoshuai Hao,Wenbo Ding*

Main category: cs.CV

TL;DR: The paper introduces SpatialSky-Bench to evaluate vision-language model (VLM) spatial capabilities in UAV tasks and proposes Sky-VLM, a VLM with superior performance based on a new large dataset.


<details>
  <summary>Details</summary>
Motivation: Existing vision-language models have not been thoroughly explored for spatial intelligence in UAV scenarios, posing challenges in navigation and environmental interpretation.

Method: Developed a benchmark (SpatialSky-Bench) with 13 subcategories and a dataset (SpatialSky-Dataset) comprising 1M samples. Designed Sky-VLM specialized for spatial reasoning in UAV tasks.

Result: Sky-VLM demonstrated state-of-the-art results in spatial intelligence across all UAV tasks in the SpatialSky-Bench evaluation.

Conclusion: Sky-VLM and the proposed benchmark set a new standard for evaluating and advancing vision-language models in UAV navigation challenges.

Abstract: Vision-Language Models (VLMs), leveraging their powerful visual perception and reasoning capabilities, have been widely applied in Unmanned Aerial Vehicle (UAV) tasks. However, the spatial intelligence capabilities of existing VLMs in UAV scenarios remain largely unexplored, raising concerns about their effectiveness in navigating and interpreting dynamic environments. To bridge this gap, we introduce SpatialSky-Bench, a comprehensive benchmark specifically designed to evaluate the spatial intelligence capabilities of VLMs in UAV navigation. Our benchmark comprises two categories-Environmental Perception and Scene Understanding-divided into 13 subcategories, including bounding boxes, color, distance, height, and landing safety analysis, among others. Extensive evaluations of various mainstream open-source and closed-source VLMs reveal unsatisfactory performance in complex UAV navigation scenarios, highlighting significant gaps in their spatial capabilities. To address this challenge, we developed the SpatialSky-Dataset, a comprehensive dataset containing 1M samples with diverse annotations across various scenarios. Leveraging this dataset, we introduce Sky-VLM, a specialized VLM designed for UAV spatial reasoning across multiple granularities and contexts. Extensive experimental results demonstrate that Sky-VLM achieves state-of-the-art performance across all benchmark tasks, paving the way for the development of VLMs suitable for UAV scenarios. The source code is available at https://github.com/linglingxiansen/SpatialSKy.

</details>


### [435] [Recognition of Abnormal Events in Surveillance Videos using Weakly Supervised Dual-Encoder Models](https://arxiv.org/abs/2511.13276)
*Noam Tsfaty,Avishai Weizman,Liav Cohen,Moshe Tshuva,Yehudit Aperstein*

Main category: cs.CV

TL;DR: The paper introduces a dual-backbone framework for anomaly detection in videos, achieving high accuracy on a benchmark dataset.


<details>
  <summary>Details</summary>
Motivation: Detecting rare and diverse anomalies in surveillance videos using minimal supervision.

Method: A dual-backbone framework combining convolutional and transformer representations with top-k pooling.

Result: Achieved 90.7% AUC performance on the UCF-Crime dataset.

Conclusion: Their framework effectively tackles video anomaly detection using limited supervision with promising results.

Abstract: We address the challenge of detecting rare and diverse anomalies in surveillance videos using only video-level supervision. Our dual-backbone framework combines convolutional and transformer representations through top-k pooling, achieving 90.7% area under the curve (AUC) on the UCF-Crime dataset.

</details>


### [436] [SF-Recon: Simplification-Free Lightweight Building Reconstruction via 3D Gaussian Splatting](https://arxiv.org/abs/2511.13278)
*Zihan Li,Tengfei Wang,Wentian Gan,Hao Zhan,Xin Wang,Zongqian Zhan*

Main category: cs.CV

TL;DR: SF-Recon introduces a method to reconstruct lightweight building surfaces directly from multi-view images, avoiding traditional cumbersome processes like dense reconstruction, meshing, and simplification.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need for efficient and lightweight building surface modeling for digital city applications, navigation, and geospatial analytics, addressing issues in quality and complexity of traditional multi-view geometry pipelines.

Method: SF-Recon uses 3D Gaussian Splatting (3DGS) fields for view-consistency, followed by normal-gradient-guided Gaussian optimization and multi-view edge-consistency pruning for structural refinement. Finally, depth-constrained Delaunay triangulation converts optimized data into lightweight building meshes.

Result: SF-Recon achieves lightweight building models with fewer faces and vertices, structural faithfulness, and computational efficiency when tested on the introduced SF dataset.

Conclusion: SF-Recon successfully reconstructs lightweight and structurally accurate building models directly from multi-view images without requiring post-hoc simplification.

Abstract: Lightweight building surface models are crucial for digital city, navigation, and fast geospatial analytics, yet conventional multi-view geometry pipelines remain cumbersome and quality-sensitive due to their reliance on dense reconstruction, meshing, and subsequent simplification. This work presents SF-Recon, a method that directly reconstructs lightweight building surfaces from multi-view images without post-hoc mesh simplification. We first train an initial 3D Gaussian Splatting (3DGS) field to obtain a view-consistent representation. Building structure is then distilled by a normal-gradient-guided Gaussian optimization that selects primitives aligned with roof and wall boundaries, followed by multi-view edge-consistency pruning to enhance structural sharpness and suppress non-structural artifacts without external supervision. Finally, a multi-view depth-constrained Delaunay triangulation converts the structured Gaussian field into a lightweight, structurally faithful building mesh. Based on a proposed SF dataset, the experimental results demonstrate that our SF-Recon can directly reconstruct lightweight building models from multi-view imagery, achieving substantially fewer faces and vertices while maintaining computational efficiency. Website:https://lzh282140127-cell.github.io/SF-Recon-project/

</details>


### [437] [Towards Metric-Aware Multi-Person Mesh Recovery by Jointly Optimizing Human Crowd in Camera Space](https://arxiv.org/abs/2511.13282)
*Kaiwen Wang,Kaili Zheng,Yiming Shi,Chenyi Guo,Ji Wu*

Main category: cs.CV

TL;DR: This paper introduces Depth-conditioned Translation Optimization (DTO) for scene-consistent human mesh recovery in crowded single images, leading to the construction of the large-scale DTO-Humans dataset and a novel model achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Multi-person human mesh recovery is challenged by the lack of in-the-wild training data, and existing methods fail to ensure scene-level consistency due to single-person-centric approaches.

Method: The paper introduces DTO, an optimization method that refines camera-space translations for multiple individuals using anthropometric priors and depth cues, creating a new pGT dataset called DTO-Humans. Additionally, it develops Metric-Aware HMR, an end-to-end network leveraging a camera branch and relative metric loss.

Result: The proposed approach achieves state-of-the-art performance in relative depth reasoning and human mesh recovery, validated through a large-scale dataset and extensive experiments.

Conclusion: DTO and Metric-Aware HMR improve scene consistency and multi-person mesh recovery accuracy, addressing limitations of previous single-person-focused pipelines. The tools and data will be publicly available for further research.

Abstract: Multi-person human mesh recovery from a single image is a challenging task, hindered by the scarcity of in-the-wild training data. Prevailing in-the-wild human mesh pseudo-ground-truth (pGT) generation pipelines are single-person-centric, where each human is processed individually without joint optimization. This oversight leads to a lack of scene-level consistency, producing individuals with conflicting depths and scales within the same image. To address this, we introduce Depth-conditioned Translation Optimization (DTO), a novel optimization-based method that jointly refines the camera-space translations of all individuals in a crowd. By leveraging anthropometric priors on human height and depth cues from a monocular depth estimator, DTO solves for a scene-consistent placement of all subjects within a principled Maximum a posteriori (MAP) framework. Applying DTO to the 4D-Humans dataset, we construct DTO-Humans, a new large-scale pGT dataset of 0.56M high-quality, scene-consistent multi-person images, featuring dense crowds with an average of 4.8 persons per image. Furthermore, we propose Metric-Aware HMR, an end-to-end network that directly estimates human mesh and camera parameters in metric scale. This is enabled by a camera branch and a novel relative metric loss that enforces plausible relative scales. Extensive experiments demonstrate that our method achieves state-of-the-art performance on relative depth reasoning and human mesh recovery. Code and data will be released publicly.

</details>


### [438] [TabFlash: Efficient Table Understanding with Progressive Question Conditioning and Token Focusing](https://arxiv.org/abs/2511.13283)
*Jongha Kim,Minseong Bae,Sanghyeok Lee,Jinsung Yoon,Hyunwoo J. Kim*

Main category: cs.CV

TL;DR: This paper introduces TabFlash, an efficient multimodal language model for understanding table images, utilizing question-aware features and reduced redundancy to improve performance.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal large language models struggle with generating informative and efficient visual representations for table images due to redundant backgrounds and lack of question-specific focus.

Method: The authors propose progressive question conditioning for vision transformers to generate question-aware features, implement a pruning strategy to remove redundant background tokens, and use token focusing to retain critical information in simplified visual representations.

Result: TabFlash achieves state-of-the-art performance in table understanding tasks, with a significant reduction of 27% in FLOPs and 30% in memory usage compared to alternative models.

Conclusion: TabFlash effectively balances accuracy and efficiency, demonstrating improvements over existing models for table image understanding with less computational overhead.

Abstract: Table images present unique challenges for effective and efficient understanding due to the need for question-specific focus and the presence of redundant background regions. Existing Multimodal Large Language Model (MLLM) approaches often overlook these characteristics, resulting in uninformative and redundant visual representations. To address these issues, we aim to generate visual features that are both informative and compact to improve table understanding. We first propose progressive question conditioning, which injects the question into Vision Transformer layers with gradually increasing frequency, considering each layer's capacity to handle additional information, to generate question-aware visual features. To reduce redundancy, we introduce a pruning strategy that discards background tokens, thereby improving efficiency. To mitigate information loss from pruning, we further propose token focusing, a training strategy that encourages the model to concentrate essential information in the retained tokens. By combining these approaches, we present TabFlash, an efficient and effective MLLM for table understanding. TabFlash achieves state-of-the-art performance, outperforming both open-source and proprietary MLLMs, while requiring 27% less FLOPs and 30% less memory usage compared to the second-best MLLM.

</details>


### [439] [SkyReels-Text: Fine-grained Font-Controllable Text Editing for Poster Design](https://arxiv.org/abs/2511.13285)
*Yunjie Yu,Jingchen Wu,Junchen Zhu,Chunze Lin,Guibin Chen*

Main category: cs.CV

TL;DR: This paper introduces SkyReels-Text, a framework enabling precise, font-aware text editing for professional-grade poster design, without requiring font labels or pre-defined libraries.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current image editing models in font-aware, fine-grained text modification, especially in scenarios like poster design where typographic precision is crucial.

Method: SkyReels-Text allows users to edit multiple text regions with distinct styles simultaneously. It operates without font labels or fine-tuning by using cropped glyph patches provided by users to define typography.

Result: Experiments across multiple datasets demonstrate that SkyReels-Text achieves state-of-the-art performance in text fidelity and visual realism, handling both standard and handwritten text effectively.

Conclusion: SkyReels-Text offers a significant advance in integrating general-purpose image editing with professional-level typographic design, enabling unprecedented control over font families and stylistic details.

Abstract: Artistic design such as poster design often demands rapid yet precise modification of textual content while preserving visual harmony and typographic intent, especially across diverse font styles. Although modern image editing models have grown increasingly powerful, they still fall short in fine-grained, font-aware text manipulation, limiting their utility in professional design workflows such as poster editing. To address this issue, we present SkyReels-Text, a novel font-controllable framework for precise poster text editing. Our method enables simultaneous editing of multiple text regions, each rendered in distinct typographic styles, while preserving the visual appearance of non-edited regions. Notably, our model requires neither font labels nor fine-tuning during inference: users can simply provide cropped glyph patches corresponding to their desired typography, even if the font is not included in any standard library. Extensive experiments on multiple datasets, including handwrittent text benchmarks, SkyReels-Text achieves state-of-the-art performance in both text fidelity and visual realism, offering unprecedented control over font families, and stylistic nuances. This work bridges the gap between general-purpose image editing and professional-grade typographic design.

</details>


### [440] [CorrectAD: A Self-Correcting Agentic System to Improve End-to-end Planning in Autonomous Driving](https://arxiv.org/abs/2511.13297)
*Enhui Ma,Lijun Zhou,Tao Tang,Jiahuan Zhang,Junpeng Jiang,Zhan Zhang,Dong Han,Kun Zhan,Xueyang Zhang,XianPeng Lang,Haiyang Sun,Xia Zhou,Di Lin,Kaicheng Yu*

Main category: cs.CV

TL;DR: The paper proposes a novel system, CorrectAD, using diffusion-based video generation and 3D layouts to self-correct failure cases in autonomous driving systems.


<details>
  <summary>Details</summary>
Motivation: Current autonomous driving systems face challenges with rare but critical failure cases due to data limitations, requiring new methods for enhanced robustness.

Method: Develops a self-correcting system, CorrectAD, which includes a PM-Agent for targeted data formulation, DriveSora for spatiotemporal video generation based on 3D layouts, and integration of these into a model-agnostic pipeline.

Result: CorrectAD demonstrates significant improvements by correcting 62.5% and 49.8% of failures, reducing collision rates by 39% on nuScenes and 27% on an in-house dataset.

Conclusion: CorrectAD proves effective in improving end-to-end planners' performance by automating the generation and correction process for rare failure cases.

Abstract: End-to-end planning methods are the de facto standard of the current autonomous driving system, while the robustness of the data-driven approaches suffers due to the notorious long-tail problem (i.e., rare but safety-critical failure cases). In this work, we explore whether recent diffusion-based video generation methods (a.k.a. world models), paired with structured 3D layouts, can enable a fully automated pipeline to self-correct such failure cases. We first introduce an agent to simulate the role of product manager, dubbed PM-Agent, which formulates data requirements to collect data similar to the failure cases. Then, we use a generative model that can simulate both data collection and annotation. However, existing generative models struggle to generate high-fidelity data conditioned on 3D layouts. To address this, we propose DriveSora, which can generate spatiotemporally consistent videos aligned with the 3D annotations requested by PM-Agent. We integrate these components into our self-correcting agentic system, CorrectAD. Importantly, our pipeline is an end-to-end model-agnostic and can be applied to improve any end-to-end planner. Evaluated on both nuScenes and a more challenging in-house dataset across multiple end-to-end planners, CorrectAD corrects 62.5% and 49.8% of failure cases, reducing collision rates by 39% and 27%, respectively.

</details>


### [441] [DriveLiDAR4D: Sequential and Controllable LiDAR Scene Generation for Autonomous Driving](https://arxiv.org/abs/2511.13309)
*Kaiwen Cai,Xinze Liu,Xia Zhou,Hengtong Hu,Jie Xiang,Luyao Zhang,Xueyang Zhang,Kun Zhan,Yifei Zhan,Xianpeng Lang*

Main category: cs.CV

TL;DR: DriveLiDAR4D proposes a new pipeline for generating realistic LiDAR point clouds with temporal consistency and scene manipulation capabilities, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: There is a need for realistic LiDAR point cloud generation to improve autonomous driving systems, but existing methods have limitations in sequential generation and precise scene manipulation.

Method: This paper presents DriveLiDAR4D, featuring multimodal conditions and a sequential noise prediction model (LiDAR4DNet) for generating temporally consistent and controllable LiDAR scenes.

Result: DriveLiDAR4D demonstrated superior performance on the nuScenes and KITTI datasets, with major improvements in FRD and FVD metrics compared to the SOTA method UniScene.

Conclusion: DriveLiDAR4D significantly enhances LiDAR scene generation, addressing current limitations and opening opportunities for improved autonomous driving system development.

Abstract: The generation of realistic LiDAR point clouds plays a crucial role in the development and evaluation of autonomous driving systems. Although recent methods for 3D LiDAR point cloud generation have shown significant improvements, they still face notable limitations, including the lack of sequential generation capabilities and the inability to produce accurately positioned foreground objects and realistic backgrounds. These shortcomings hinder their practical applicability. In this paper, we introduce DriveLiDAR4D, a novel LiDAR generation pipeline consisting of multimodal conditions and a novel sequential noise prediction model LiDAR4DNet, capable of producing temporally consistent LiDAR scenes with highly controllable foreground objects and realistic backgrounds. To the best of our knowledge, this is the first work to address the sequential generation of LiDAR scenes with full scene manipulation capability in an end-to-end manner. We evaluated DriveLiDAR4D on the nuScenes and KITTI datasets, where we achieved an FRD score of 743.13 and an FVD score of 16.96 on the nuScenes dataset, surpassing the current state-of-the-art (SOTA) method, UniScene, with an performance boost of 37.2% in FRD and 24.1% in FVD, respectively.

</details>


### [442] [Computer Vision based group activity detection and action spotting](https://arxiv.org/abs/2511.13315)
*Narthana Sivalingam,Santhirarajah Sivasthigan,Thamayanthi Mahendranathan,G. M. R. I. Godaliyadda,M. P. B. Ekanayake,H. M. V. R. Herath*

Main category: cs.CV

TL;DR: This paper proposes a deep learning and graph-based framework for group activity recognition in multi-person scenes, utilizing Mask R-CNN, feature maps, and graph convolutional networks for robust performance.


<details>
  <summary>Details</summary>
Motivation: The study aims to tackle challenges in group activity detection such as human interactions, occlusions, and appearance variations in multi-person scenes.

Method: The method combines Mask R-CNN for actor localization, feature maps extraction using backbone networks, Actor Relation Graphs for relational reasoning, and Graph Convolutional Networks for predicting individual and group activities.

Result: Experiments on the Collective Activity dataset show improved recognition performance, particularly in both crowded and non-crowded scenarios.

Conclusion: The integration of segmentation, feature refinement, and graph reasoning proves effective for handling complex video analysis tasks in multi-person scenes.

Abstract: Group activity detection in multi-person scenes is challenging due to complex human interactions, occlusions, and variations in appearance over time. This work presents a computer vision based framework for group activity recognition and action spotting using a combination of deep learning models and graph based relational reasoning. The system first applies Mask R-CNN to obtain accurate actor localization through bounding boxes and instance masks. Multiple backbone networks, including Inception V3, MobileNet, and VGG16, are used to extract feature maps, and RoIAlign is applied to preserve spatial alignment when generating actor specific features. The mask information is then fused with the feature maps to obtain refined masked feature representations for each actor. To model interactions between individuals, we construct Actor Relation Graphs that encode appearance similarity and positional relations using methods such as normalized cross correlation, sum of absolute differences, and dot product. Graph Convolutional Networks operate on these graphs to reason about relationships and predict both individual actions and group level activities. Experiments on the Collective Activity dataset demonstrate that the combination of mask based feature refinement, robust similarity search, and graph neural network reasoning leads to improved recognition performance across both crowded and non crowded scenarios. This approach highlights the potential of integrating segmentation, feature extraction, and relational graph reasoning for complex video understanding tasks.

</details>


### [443] [YOLO Meets Mixture-of-Experts: Adaptive Expert Routing for Robust Object Detection](https://arxiv.org/abs/2511.13344)
*Ori Meiraz,Sharon Shalev,Avishai Weizman*

Main category: cs.CV

TL;DR: The paper introduces a Mixture-of-Experts framework using adaptive routing among YOLOv9-T models to enhance object detection metrics.


<details>
  <summary>Details</summary>
Motivation: To improve object detection performance by enabling dynamic feature specialization through multiple experts instead of relying on a single model.

Method: A Mixture-of-Experts framework with adaptive routing among multiple YOLOv9-T models is utilized for dynamic feature specialization.

Result: The framework achieves higher mean Average Precision (mAP) and Average Recall (AR) than using a single YOLOv9-T model.

Conclusion: The proposed Mixture-of-Experts approach is effective for enhancing object detection metrics and showcases its potential advancements in detection quality.

Abstract: This paper presents a novel Mixture-of-Experts framework for object detection, incorporating adaptive routing among multiple YOLOv9-T experts to enable dynamic feature specialization and achieve higher mean Average Precision (mAP) and Average Recall (AR) compared to a single YOLOv9-T model.

</details>


### [444] [Semi-Supervised Multi-Task Learning for Interpretable Quality As- sessment of Fundus Images](https://arxiv.org/abs/2511.13353)
*Lucas Gabriel Telesco,Danila Nejamkin,Estefanía Mata,Francisco Filizzola,Kevin Wignall,Lucía Franco Troilo,María de los Angeles Cenoz,Melissa Thompson,Mercedes Leguía,Ignacio Larrabide,José Ignacio Orlando*

Main category: cs.CV

TL;DR: This paper presents a semi-supervised learning approach for retinal image quality assessment, improving interpretability and quality predictions without extensive manual labeling.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of tools that only classify overall image quality without pinpointing acquisition defects due to high annotation costs.

Method: A hybrid semi-supervised learning framework combining manual overall quality labels and pseudo-labels for quality details, leveraging ResNet-18 within a multi-task model.

Result: Improved quality assessment over single-task baselines (F1 scores: EyeQ 0.875 vs. 0.863; DeepDRiD 0.778 vs. 0.763), matching or surpassing existing methods, with performance on detail prediction tasks statistically comparable to experts.

Conclusion: The approach allows clinically actionable feedback on image capture conditions (e.g., illumination, clarity, contrast), enhancing interpretability without additional labeling costs.

Abstract: Retinal image quality assessment (RIQA) supports computer-aided diagnosis of eye diseases. However, most tools classify only overall image quality, without indicating acquisition defects to guide recapture. This gap is mainly due to the high cost of detailed annotations. In this paper, we aim to mitigate this limitation by introducing a hybrid semi-supervised learning approach that combines manual labels for overall quality with pseudo-labels of quality details within a multi-task framework. Our objective is to obtain more interpretable RIQA models without requiring extensive manual labeling. Pseudo-labels are generated by a Teacher model trained on a small dataset and then used to fine-tune a pre-trained model in a multi-task setting. Using a ResNet-18 backbone, we show that these weak annotations improve quality assessment over single-task baselines (F1: 0.875 vs. 0.863 on EyeQ, and 0.778 vs. 0.763 on DeepDRiD), matching or surpassing existing methods. The multi-task model achieved performance statistically comparable to the Teacher for most detail prediction tasks (p > 0.05). In a newly annotated EyeQ subset released with this paper, our model performed similarly to experts, suggesting that pseudo-label noise aligns with expert variability. Our main finding is that the proposed semi-supervised approach not only improves overall quality assessment but also provides interpretable feedback on capture conditions (illumination, clarity, contrast). This enhances interpretability at no extra manual labeling cost and offers clinically actionable outputs to guide image recapture.

</details>


### [445] [Generalized Denoising Diffusion Codebook Models (gDDCM): Tokenizing images using a pre-trained diffusion model](https://arxiv.org/abs/2511.13387)
*Fei Kong*

Main category: cs.CV

TL;DR: The paper proposes a generalized Denoising Diffusion Compression Model (gDDCM), extending DDCM to various mainstream diffusion models for image compression and demonstrates improved performance.


<details>
  <summary>Details</summary>
Motivation: Current DDCM only functions with DDPM, limiting its applicability. The motivation is to generalize DDCM to work with other diffusion models.

Method: The gDDCM extends DDCM to various diffusion models (Score-Based, Consistency Models, Rectified Flow). Noise in the backward process is redefined for generalized functionality.

Result: Experiments conducted on CIFAR-10 and LSUN Bedroom datasets verify the effective generalization of DDCM and show improved compression performance.

Conclusion: gDDCM expands the applicability of DDCM by enabling image compression across a range of diffusion models. Results indicate generalized usability and enhanced performance.

Abstract: Recently, the Denoising Diffusion Codebook Models (DDCM) was proposed. DDCM leverages the Denoising Diffusion Probabilistic Model (DDPM) and replaces the random noise in the backward process with noise sampled from specific sets according to a predefined rule, thereby enabling image compression. However, DDCM cannot be applied to methods other than DDPM. In this paper, we propose the generalized Denoising Diffusion Compression Model (gDDCM), which extends DDCM to mainstream diffusion models and their variants, including DDPM, Score-Based Models, Consistency Models, and Rectified Flow. We evaluate our method on CIFAR-10 and LSUN Bedroom datasets. Experimental results demonstrate that our approach successfully generalizes DDCM to the aforementioned models and achieves improved performance.

</details>


### [446] [Descriptor: Distance-Annotated Traffic Perception Question Answering (DTPQA)](https://arxiv.org/abs/2511.13397)
*Nikos Theodoridis,Tim Brophy,Reenu Mohandas,Ganesh Sistu,Fiachra Collins,Anthony Scanlan,Ciaran Eising*

Main category: cs.CV

TL;DR: This paper introduces a VQA benchmark, DTPQA, for evaluating Vision-Language Models (VLMs) in traffic scenarios with an emphasis on distance perception.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the need for robust traffic perception capabilities in VLMs, given their potential utility in automated driving systems.

Method: The authors propose DTPQA, which includes a synthetic benchmark (DTP-Synthetic) and a real-world benchmark (DTP-Real), equipped with distance annotations to assess VLMs' perception performance at varying distances.

Result: The paper presents the DTPQA dataset and the corresponding Python scripts to generate additional data, aiding in the analysis of VLM performance degradation based on object distance.

Conclusion: DTPQA provides a systematic approach to benchmark and improve VLMs’ understanding of traffic scenes by isolating perception capabilities from reasoning and other skills.

Abstract: The remarkable progress of Vision-Language Models (VLMs) on a variety of tasks has raised interest in their application to automated driving. However, for these models to be trusted in such a safety-critical domain, they must first possess robust perception capabilities, i.e., they must be capable of understanding a traffic scene, which can often be highly complex, with many things happening simultaneously. Moreover, since critical objects and agents in traffic scenes are often at long distances, we require systems with not only strong perception capabilities at close distances (up to 20 meters), but also at long (30+ meters) range. Therefore, it is important to evaluate the perception capabilities of these models in isolation from other skills like reasoning or advanced world knowledge. Distance-Annotated Traffic Perception Question Answering (DTPQA) is a Visual Question Answering (VQA) benchmark designed specifically for this purpose: it can be used to evaluate the perception systems of VLMs in traffic scenarios using trivial yet crucial questions relevant to driving decisions. It consists of two parts: a synthetic benchmark (DTP-Synthetic) created using a simulator, and a real-world benchmark (DTP-Real) built on top of existing images of real traffic scenes. Additionally, DTPQA includes distance annotations, i.e., how far the object in question is from the camera. More specifically, each DTPQA sample consists of (at least): (a) an image, (b) a question, (c) the ground truth answer, and (d) the distance of the object in question, enabling analysis of how VLM performance degrades with increasing object distance. In this article, we provide the dataset itself along with the Python scripts used to create it, which can be used to generate additional data of the same kind.

</details>


### [447] [TripleFDS: Triple Feature Disentanglement and Synthesis for Scene Text Editing](https://arxiv.org/abs/2511.13399)
*Yuchen Bao,Yiting Wang,Wenjian Huang,Haowei Wang,Shen Chen,Taiping Yao,Shouhong Ding,Jianguo Zhang*

Main category: cs.CV

TL;DR: The paper proposes TripleFDS, an advanced framework for Scene Text Editing (STE) that effectively disentangles text style, content, and background using a novel SCB Synthesis dataset.


<details>
  <summary>Details</summary>
Motivation: Previous approaches to STE faced challenges due to incomplete disentanglement of editable attributes, limiting control and visual consistency in modifications.

Method: TripleFDS uses SCB Synthesis dataset with 'SCB Groups' to disentangle text style, content, and background. It employs inter-group contrastive regularization and intra-sample orthogonality for semantic accuracy and minimizes feature leakage with feature remapping during synthesis.

Result: TripleFDS demonstrated state-of-the-art results with a 44.54 SSIM and 93.58% text accuracy on STE benchmarks, showing superior image fidelity and text precision while enabling flexible editing operations.

Conclusion: TripleFDS effectively addresses previous limitations of STE methods, achieving superior performance and enabling new editing operations like style replacement and background transfer.

Abstract: Scene Text Editing (STE) aims to naturally modify text in images while preserving visual consistency, the decisive factors of which can be divided into three parts, i.e., text style, text content, and background. Previous methods have struggled with incomplete disentanglement of editable attributes, typically addressing only one aspect - such as editing text content - thus limiting controllability and visual consistency. To overcome these limitations, we propose TripleFDS, a novel framework for STE with disentangled modular attributes, and an accompanying dataset called SCB Synthesis. SCB Synthesis provides robust training data for triple feature disentanglement by utilizing the "SCB Group", a novel construct that combines three attributes per image to generate diverse, disentangled training groups. Leveraging this construct as a basic training unit, TripleFDS first disentangles triple features, ensuring semantic accuracy through inter-group contrastive regularization and reducing redundancy through intra-sample multi-feature orthogonality. In the synthesis phase, TripleFDS performs feature remapping to prevent "shortcut" phenomena during reconstruction and mitigate potential feature leakage. Trained on 125,000 SCB Groups, TripleFDS achieves state-of-the-art image fidelity (SSIM of 44.54) and text accuracy (ACC of 93.58%) on the mainstream STE benchmarks. Besides superior performance, the more flexible editing of TripleFDS supports new operations such as style replacement and background transfer. Code: https://github.com/yusenbao01/TripleFDS

</details>


### [448] [What Color Is It? A Text-Interference Multimodal Hallucination Benchmark](https://arxiv.org/abs/2511.13400)
*Jinkun Zhao,Lei Huang,Wenjun Wu*

Main category: cs.CV

TL;DR: The study introduces a novel dataset to assess and address color perception issues in Multimodal Large Models.


<details>
  <summary>Details</summary>
Motivation: Multimodal Large Models face challenges of visual interference affecting color perception, which raises risks of hallucination.

Method: They develop the 'What Color Is It' dataset to trigger and analyze visual hallucination in MLMs.

Result: The dataset validates visual modality issues and identifies the causes of hallucination in MLMs.

Conclusion: Proposed solutions are introduced to improve MLMs' robustness against visual modality challenges.

Abstract: With the rapid advancement of Large Models, numerous text-and-vision-fused Multimodal Large Models (MLMs) have emerged. However, these MLMs remain susceptible to informational interference in visual perception, particularly in color perception, which introduces an additional risk of hallucination. To validate this hypothesis, we introduce the "What Color Is It" dataset, a novel benchmark constructed using a simple method to trigger single-modality visual hallucination in MLMs. Based on this dataset, we further investigate the underlying causes of hallucination in the visual modality of MLMs and propose potential solutions to enhance their robustness.

</details>


### [449] [Delineate Anything Flow: Fast, Country-Level Field Boundary Detection from Any Source](https://arxiv.org/abs/2511.13417)
*Mykola Lavreniuk,Nataliia Kussul,Andrii Shelestov,Yevhenii Salii,Volodymyr Kuzin,Sergii Skakun,Zoltan Szantoi*

Main category: cs.CV

TL;DR: This paper introduces DelAnyFlow, a methodology for accurately mapping agricultural field boundaries using satellite imagery. It demonstrates better performance, scalability, and boundary precision compared to current solutions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome challenges in agricultural field boundary mapping such as incomplete delineation, field merging, and poor scalability of existing methods.

Method: The authors propose the DelAnyFlow system, which integrates the DelAny segmentation model (based on YOLOv11 and a large FBIS-22M dataset) with structured post-processing and vectorization techniques.

Result: The results indicate higher accuracy (over 100% improvement in mAP), faster inference (400x), and robust field boundary delineation, tested successfully at a national scale in Ukraine.

Conclusion: DelAnyFlow offers a scalable, efficient, and accurate method for agricultural field boundary mapping, proving especially useful in areas lacking detailed cadastral data.

Abstract: Accurate delineation of agricultural field boundaries from satellite imagery is essential for land management and crop monitoring, yet existing methods often produce incomplete boundaries, merge adjacent fields, and struggle to scale. We present the Delineate Anything Flow (DelAnyFlow) methodology, a resolution-agnostic approach for large-scale field boundary mapping. DelAnyFlow combines the DelAny instance segmentation model, based on a YOLOv11 backbone and trained on the large-scale Field Boundary Instance Segmentation-22M (FBIS 22M) dataset, with a structured post-processing, merging, and vectorization sequence to generate topologically consistent vector boundaries. FBIS 22M, the largest dataset of its kind, contains 672,909 multi-resolution image patches (0.25-10m) and 22.9million validated field instances. The DelAny model delivers state-of-the-art accuracy with over 100% higher mAP and 400x faster inference than SAM2. DelAny demonstrates strong zero-shot generalization and supports national-scale applications: using Sentinel 2 data for 2024, DelAnyFlow generated a complete field boundary layer for Ukraine (603,000km2) in under six hours on a single workstation. DelAnyFlow outputs significantly improve boundary completeness relative to operational products from Sinergise Solutions and NASA Harvest, particularly in smallholder and fragmented systems (0.25-1ha). For Ukraine, DelAnyFlow delineated 3.75M fields at 5m and 5.15M at 2.5m, compared to 2.66M detected by Sinergise Solutions and 1.69M by NASA Harvest. This work delivers a scalable, cost-effective methodology for field delineation in regions lacking digital cadastral data. A project landing page with links to model weights, code, national-scale vector outputs, and dataset is available at https://lavreniuk.github.io/Delineate-Anything/.

</details>


### [450] [VOPE: Revisiting Hallucination of Vision-Language Models in Voluntary Imagination Task](https://arxiv.org/abs/2511.13420)
*Xingming Long,Jie Zhang,Shiguang Shan,Xilin Chen*

Main category: cs.CV

TL;DR: The paper examines hallucinations in voluntary imagination tasks by LVLMs, introduces the VOPE method for evaluation, and finds significant hallucination issues and limited efficacy of current mitigation methods.


<details>
  <summary>Details</summary>
Motivation: Research on hallucinations in LVLMs has been limited to factual tasks, neglecting cases involving voluntary imagination tasks where new content beyond the image is expected. This necessitates a methodology to assess hallucinations in such scenarios.

Method: The VOPE method was developed, which uses recheck-based questions to evaluate the consistency of the model's interpretation of imagined objects with their actual presence in an image.

Result: Findings indicate that most LVLMs heavily hallucinate in voluntary imagination tasks and perform poorly in presence evaluation. Current hallucination mitigation techniques are largely ineffective in this domain.

Conclusion: Effective hallucination mitigation for voluntary imagination tasks is a critical area for future LVLM research. VOPE highlights the extent of hallucination issues in these models.

Abstract: Most research on hallucinations in Large Vision-Language Models (LVLMs) focuses on factual description tasks that prohibit any output absent from the image. However, little attention has been paid to hallucinations in voluntary imagination tasks, e.g., story writing, where the models are expected to generate novel content beyond the given image. In these tasks, it is inappropriate to simply regard such imagined novel content as hallucinations. To address this limitation, we introduce Voluntary-imagined Object Presence Evaluation (VOPE)-a novel method to assess LVLMs' hallucinations in voluntary imagination tasks via presence evaluation. Specifically, VOPE poses recheck-based questions to evaluate how an LVLM interprets the presence of the imagined objects in its own response. The consistency between the model's interpretation and the object's presence in the image is then used to determine whether the model hallucinates when generating the response. We apply VOPE to several mainstream LVLMs and hallucination mitigation methods, revealing two key findings: (1) most LVLMs hallucinate heavily during voluntary imagination, and their performance in presence evaluation is notably poor on imagined objects; (2) existing hallucination mitigation methods show limited effect in voluntary imagination tasks, making this an important direction for future research.

</details>


### [451] [FUSE: A Flow-based Mapping Between Shapes](https://arxiv.org/abs/2511.13431)
*Lorenzo Olearo,Giulio Viganò,Daniele Baieri,Filippo Maggioli,Simone Melzi*

Main category: cs.CV

TL;DR: The paper introduces a neural method for 3D shape matching using flow-matching models without requiring large-scale training, achieving high accuracy across various benchmarks.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient and versatile method for mapping between 3D shapes without relying on data-driven or large-scale training approaches.

Method: 3D shapes are encoded as probability distributions, leveraging continuous and invertible flow mapping between a fixed anchor distribution and source/target shapes. The framework is modality-agnostic, supporting shapes in formats such as point clouds, meshes, and volumetric data.

Result: The method achieves high coverage and accuracy in shape matching benchmarks, tackling challenging settings effectively. It performs well in diverse applications like UV mapping and registration of raw point cloud human body scans.

Conclusion: The proposed flow-matching method provides a robust, efficient, and flexible approach for shape matching and related tasks, bridging various 3D representations seamlessly.

Abstract: We introduce a novel neural representation for maps between 3D shapes based on flow-matching models, which is computationally efficient and supports cross-representation shape matching without large-scale training or data-driven procedures. 3D shapes are represented as the probability distribution induced by a continuous and invertible flow mapping from a fixed anchor distribution. Given a source and a target shape, the composition of the inverse flow (source to anchor) with the forward flow (anchor to target), we continuously map points between the two surfaces. By encoding the shapes with a pointwise task-tailored embedding, this construction provides an invertible and modality-agnostic representation of maps between shapes across point clouds, meshes, signed distance fields (SDFs), and volumetric data. The resulting representation consistently achieves high coverage and accuracy across diverse benchmarks and challenging settings in shape matching. Beyond shape matching, our framework shows promising results in other tasks, including UV mapping and registration of raw point cloud scans of human bodies.

</details>


### [452] [Unlocking the Forgery Detection Potential of Vanilla MLLMs: A Novel Training-Free Pipeline](https://arxiv.org/abs/2511.13442)
*Rui Zuo,Qinyue Tong,Zhe-Ming Lu,Ziqian Lu*

Main category: cs.CV

TL;DR: Foresee, a training-free pipeline leveraging MLLMs for advanced image forgery analysis, enhances tamper localization accuracy and textual explanation while improving generalization across various tampering types.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing IFDL methods, including poor generalization and limited interpretability, while avoiding the computational expense of large-scale training with MLLMs.

Method: Development of Foresee, a training-free MLLM-based pipeline using a type-prior-driven strategy and Flexible Feature Detector (FFD) module for copy-move manipulations without requiring additional training.

Result: Foresee achieves superior tamper localization accuracy and provides richer textual explanations, demonstrating stronger generalization across diverse tampering types compared to existing methods.

Conclusion: Foresee effectively unleashes the innate potential of MLLMs for image forgery analysis, providing a more efficient, accurate, and interpretive solution compared to existing approaches. Code will be released in the final version.

Abstract: With the rapid advancement of artificial intelligence-generated content (AIGC) technologies, including multimodal large language models (MLLMs) and diffusion models, image generation and manipulation have become remarkably effortless. Existing image forgery detection and localization (IFDL) methods often struggle to generalize across diverse datasets and offer limited interpretability. Nowadays, MLLMs demonstrate strong generalization potential across diverse vision-language tasks, and some studies introduce this capability to IFDL via large-scale training. However, such approaches cost considerable computational resources, while failing to reveal the inherent generalization potential of vanilla MLLMs to address this problem. Inspired by this observation, we propose Foresee, a training-free MLLM-based pipeline tailored for image forgery analysis. It eliminates the need for additional training and enables a lightweight inference process, while surpassing existing MLLM-based methods in both tamper localization accuracy and the richness of textual explanations. Foresee employs a type-prior-driven strategy and utilizes a Flexible Feature Detector (FFD) module to specifically handle copy-move manipulations, thereby effectively unleashing the potential of vanilla MLLMs in the forensic domain. Extensive experiments demonstrate that our approach simultaneously achieves superior localization accuracy and provides more comprehensive textual explanations. Moreover, Foresee exhibits stronger generalization capability, outperforming existing IFDL methods across various tampering types, including copy-move, splicing, removal, local enhancement, deepfake, and AIGC-based editing. The code will be released in the final version.

</details>


### [453] [Semantic Document Derendering: SVG Reconstruction via Vision-Language Modeling](https://arxiv.org/abs/2511.13478)
*Adam Hazimeh,Ke Wang,Mark Collier,Gilles Baechler,Efi Kokiopoulou,Pascal Frossard*

Main category: cs.CV

TL;DR: This paper presents SliDer, a framework using Vision-Language Models (VLMs) to convert static raster slide images back into editable Scalable Vector Graphic (SVG) representations, preserving complex structures and semantics.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of existing raster-vectorization methods, which fail to preserve high-level structures and semantic distinctions between image and text in static raster formats of multimedia documents like slides and posters.

Method: The paper introduces SliDer, which uses Vision-Language Models (VLMs) to detect, extract, and refine attributes of individual image and text elements in raster inputs iteratively. These are organized into editable SVG formats. A novel dataset, Slide2SVG, is also introduced.

Result: SliDer achieves a reconstruction LPIPS of 0.069 and is preferred by human evaluators 82.9% of the time over existing zero-shot VLM baselines.

Conclusion: The proposed framework, SliDer, effectively restores editability to static raster document images by reconstructing their high-level structures and semantics as SVGs. The Slide2SVG dataset further supports advancements in this area.

Abstract: Multimedia documents such as slide presentations and posters are designed to be interactive and easy to modify. Yet, they are often distributed in a static raster format, which limits editing and customization. Restoring their editability requires converting these raster images back into structured vector formats. However, existing geometric raster-vectorization methods, which rely on low-level primitives like curves and polygons, fall short at this task. Specifically, when applied to complex documents like slides, they fail to preserve the high-level structure, resulting in a flat collection of shapes where the semantic distinction between image and text elements is lost. To overcome this limitation, we address the problem of semantic document derendering by introducing SliDer, a novel framework that uses Vision-Language Models (VLMs) to derender slide images as compact and editable Scalable Vector Graphic (SVG) representations. SliDer detects and extracts attributes from individual image and text elements in a raster input and organizes them into a coherent SVG format. Crucially, the model iteratively refines its predictions during inference in a process analogous to human design, generating SVG code that more faithfully reconstructs the original raster upon rendering. Furthermore, we introduce Slide2SVG, a novel dataset comprising raster-SVG pairs of slide documents curated from real-world scientific presentations, to facilitate future research in this domain. Our results demonstrate that SliDer achieves a reconstruction LPIPS of 0.069 and is favored by human evaluators in 82.9% of cases compared to the strongest zero-shot VLM baseline.

</details>


### [454] [InterMoE: Individual-Specific 3D Human Interaction Generation via Dynamic Temporal-Selective MoE](https://arxiv.org/abs/2511.13488)
*Lipeng Wang,Hongxing Fan,Haohua Chen,Zehuan Huang,Lu Sheng*

Main category: cs.CV

TL;DR: InterMoE introduces a Dynamic Temporal-Selective Mixture of Experts for generating high-quality individualized 3D human interactions, improving fidelity and semantic accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle to preserve individual-specific details and align outputs with text descriptions in human interactions.

Method: InterMoE employs a routing mechanism using text semantics and motion context, dynamically allocating temporal features to experts for improved precision and identity preservation.

Result: InterMoE reduces FID scores by 9% on the InterHuman dataset and 22% on the InterX dataset, achieving state-of-the-art performance.

Conclusion: The framework enhances the quality of individualized 3D human interactions with better semantic fidelity and identity preservation, supporting VR and robotics applications.

Abstract: Generating high-quality human interactions holds significant value for applications like virtual reality and robotics. However, existing methods often fail to preserve unique individual characteristics or fully adhere to textual descriptions. To address these challenges, we introduce InterMoE, a novel framework built on a Dynamic Temporal-Selective Mixture of Experts. The core of InterMoE is a routing mechanism that synergistically uses both high-level text semantics and low-level motion context to dispatch temporal motion features to specialized experts. This allows experts to dynamically determine the selection capacity and focus on critical temporal features, thereby preserving specific individual characteristic identities while ensuring high semantic fidelity. Extensive experiments show that InterMoE achieves state-of-the-art performance in individual-specific high-fidelity 3D human interaction generation, reducing FID scores by 9% on the InterHuman dataset and 22% on InterX.

</details>


### [455] [Language-Guided Invariance Probing of Vision-Language Models](https://arxiv.org/abs/2511.13494)
*Jae Joong Lee*

Main category: cs.CV

TL;DR: The paper introduces Language-Guided Invariance Probing (LGIP), a benchmark to assess vision-language models concerning linguistic robustness and their sensitivity to meaning-altering changes in text.


<details>
  <summary>Details</summary>
Motivation: To address the lack of understanding about the reliability of vision-language models when responding to linguistic perturbations, ensuring they make appropriate distinctions in meaning in image-text matching.

Method: The study uses LGIP benchmarking by creating paraphrases and semantic flips for 40k human captions from the MS COCO dataset. It introduces metrics like invariance error, semantic sensitivity gap, and positive-rate statistics to evaluate nine vision-language models.

Result: EVA02-CLIP and OpenCLIP variants showed robust performance on invariance-sensitivity metrics, while SigLIP models exhibited error-prone behavior and often favored incorrect captions over original human descriptions.

Conclusion: LGIP provides a novel and effective diagnostic framework to gauge linguistic robustness in VLMs, extending insights beyond traditional retrieval metrics.

Abstract: Recent vision-language models (VLMs) such as CLIP, OpenCLIP, EVA02-CLIP and SigLIP achieve strong zero-shot performance, but it is unclear how reliably they respond to controlled linguistic perturbations. We introduce Language-Guided Invariance Probing (LGIP), a benchmark that measures (i) invariance to meaning-preserving paraphrases and (ii) sensitivity to meaning-changing semantic flips in image-text matching. Using 40k MS COCO images with five human captions each, we automatically generate paraphrases and rule-based flips that alter object category, color or count, and summarize model behavior with an invariance error, a semantic sensitivity gap and a positive-rate statistic.
  Across nine VLMs, EVA02-CLIP and large OpenCLIP variants lie on a favorable invariance-sensitivity frontier, combining low paraphrase-induced variance with consistently higher scores for original captions than for their flipped counterparts. In contrast, SigLIP and SigLIP2 show much larger invariance error and often prefer flipped captions to the human descriptions, especially for object and color edits. These failures are largely invisible to standard retrieval metrics, indicating that LGIP provides a model-agnostic diagnostic for the linguistic robustness of VLMs beyond conventional accuracy scores.

</details>


### [456] [Mapping the Vanishing and Transformation of Urban Villages in China](https://arxiv.org/abs/2511.13507)
*Wenyu Zhang,Yao Tong,Yiqiu Liu,Rui Cao*

Main category: cs.CV

TL;DR: Urban villages in China are analyzed using deep learning models to understand their redevelopment and post-demolition land use transitions across multiple cities.


<details>
  <summary>Details</summary>
Motivation: The lack of systematic evaluation on the effective reuse of demolished urban village land raises concerns over sustainability and efficiency in current redevelopment practices.

Method: The study uses a deep learning-based framework and semantic segmentation of multi-temporal remote sensing imagery to monitor spatial and temporal changes in urban village redevelopment.

Result: The findings show prolonged redevelopment processes, major transitions in peripheral areas, and three primary spatiotemporal transformation pathways.

Conclusion: Urban village redevelopment in China is fragmented, complex, and nonlinear, necessitating tiered and context-sensitive planning strategies for sustainable urban renewal.

Abstract: Urban villages (UVs), informal settlements embedded within China's urban fabric, have undergone widespread demolition and redevelopment in recent decades. However, there remains a lack of systematic evaluation of whether the demolished land has been effectively reused, raising concerns about the efficacy and sustainability of current redevelopment practices. To address the gap, this study proposes a deep learning-based framework to monitor the spatiotemporal changes of UVs in China. Specifically, semantic segmentation of multi-temporal remote sensing imagery is first used to map evolving UV boundaries, and then post-demolition land use is classified into six categories based on the "remained-demolished-redeveloped" phase: incomplete demolition, vacant land, construction sites, buildings, green spaces, and others. Four representative cities from China's four economic regions were selected as the study areas, i.e., Guangzhou (East), Zhengzhou (Central), Xi'an (West), and Harbin (Northeast). The results indicate: 1) UV redevelopment processes were frequently prolonged; 2) redevelopment transitions primarily occurred in peripheral areas, whereas urban cores remained relatively stable; and 3) three spatiotemporal transformation pathways, i.e., synchronized redevelopment, delayed redevelopment, and gradual optimization, were revealed. This study highlights the fragmented, complex and nonlinear nature of UV redevelopment, underscoring the need for tiered and context-sensitive planning strategies. By linking spatial dynamics with the context of redevelopment policies, the findings offer valuable empirical insights that support more inclusive, efficient, and sustainable urban renewal, while also contributing to a broader global understanding of informal settlement transformations.

</details>


### [457] [Minimax Multi-Target Conformal Prediction with Applications to Imaging Inverse Problems](https://arxiv.org/abs/2511.13533)
*Jeffrey Wen,Rizwan Ahmad,Philip Schniter*

Main category: cs.CV

TL;DR: The paper proposes a minimax approach for multi-target conformal prediction to improve uncertainty quantification in imaging inverse problems, showing benefits for applications such as blind image quality assessment and MRI analysis.


<details>
  <summary>Details</summary>
Motivation: Uncertainty quantification in ill-posed imaging inverse problems is crucial, especially for applications involving multiple estimation targets and safety-critical tasks.

Method: The paper develops a minimax approach to conformal prediction that ensures tight prediction intervals and joint marginal coverage for multi-target tasks.

Result: The method numerically demonstrated improved performance and benefits compared to existing multi-target conformal prediction approaches, using synthetic and MRI data.

Conclusion: The proposed minimax conformal prediction effectively improves uncertainty quantification in multi-target tasks and is applicable to diverse imaging-related applications.

Abstract: In ill-posed imaging inverse problems, uncertainty quantification remains a fundamental challenge, especially in safety-critical applications. Recently, conformal prediction has been used to quantify the uncertainty that the inverse problem contributes to downstream tasks like image classification, image quality assessment, fat mass quantification, etc. While existing works handle only a scalar estimation target, practical applications often involve multiple targets. In response, we propose an asymptotically minimax approach to multi-target conformal prediction that provides tight prediction intervals while ensuring joint marginal coverage. We then outline how our minimax approach can be applied to multi-metric blind image quality assessment, multi-task uncertainty quantification, and multi-round measurement acquisition. Finally, we numerically demonstrate the benefits of our minimax method, relative to existing multi-target conformal prediction methods, using both synthetic and magnetic resonance imaging (MRI) data.

</details>


### [458] [Accuracy is Not Enough: Poisoning Interpretability in Federated Learning via Color Skew](https://arxiv.org/abs/2511.13535)
*Farhin Farhad Riya,Shahinul Hoque,Jinyuan Stella Sun,Olivera Kotevska*

Main category: cs.CV

TL;DR: This paper presents a novel attack on machine learning model interpretability using subtle color perturbations that compromise visual explanations while maintaining prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to investigate and reveal a vulnerability in machine learning interpretability, emphasizing its importance in safety-critical deployments.

Method: The method involves introducing a saliency-aware attack framework called Chromatic Perturbation Module, which subtly alters color contrast in adversarial examples during federated learning.

Result: The attack successfully reduces Grad-CAM explanation fidelity without affecting classification accuracy, decreasing peak activation overlap in explanations by up to 35% while maintaining accuracy above 96%.

Conclusion: The study demonstrates that correct model predictions do not guarantee trustworthy explanations and highlights interpretability as a potential attack surface in federated learning frameworks.

Abstract: As machine learning models are increasingly deployed in safety-critical domains, visual explanation techniques have become essential tools for supporting transparency. In this work, we reveal a new class of attacks that compromise model interpretability without affecting accuracy. Specifically, we show that small color perturbations applied by adversarial clients in a federated learning setting can shift a model's saliency maps away from semantically meaningful regions while keeping the prediction unchanged. The proposed saliency-aware attack framework, called Chromatic Perturbation Module, systematically crafts adversarial examples by altering the color contrast between foreground and background in a way that disrupts explanation fidelity. These perturbations accumulate across training rounds, poisoning the global model's internal feature attributions in a stealthy and persistent manner. Our findings challenge a common assumption in model auditing that correct predictions imply faithful explanations and demonstrate that interpretability itself can be an attack surface. We evaluate this vulnerability across multiple datasets and show that standard training pipelines are insufficient to detect or mitigate explanation degradation, especially in the federated learning setting, where subtle color perturbations are harder to discern. Our attack reduces peak activation overlap in Grad-CAM explanations by up to 35% while preserving classification accuracy above 96% on all evaluated datasets.

</details>


### [459] [BootOOD: Self-Supervised Out-of-Distribution Detection via Synthetic Sample Exposure under Neural Collapse](https://arxiv.org/abs/2511.13539)
*Yuanchao Wang,Tian Qin,Eduardo Valle,Bruno Abrahao*

Main category: cs.CV

TL;DR: BootOOD is a self-supervised OOD detection framework that synthesizes pseudo-OOD features and utilizes feature norms for improved performance, particularly on semantically challenging OOD scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing OOD detection methods struggle with semantically similar OOD samples, which poses risks in safety-critical deployments.

Method: The framework bootstraps pseudo-OOD features via simple transformations and leverages Neural Collapse (NC) to classify using feature norms. A lightweight auxiliary head decouples OOD detection from primary classification tasks.

Result: Experiments on CIFAR-10, CIFAR-100, and ImageNet-200 show superior performance over post-hoc and non-exposure training methods, while maintaining comparable ID accuracy.

Conclusion: BootOOD effectively handles semantically close OOD samples without outlier exposure, outperforming baseline methods and competing well with state-of-the-art solutions.

Abstract: Out-of-distribution (OOD) detection is critical for deploying image classifiers in safety-sensitive environments, yet existing detectors often struggle when OOD samples are semantically similar to the in-distribution (ID) classes. We present BootOOD, a fully self-supervised OOD detection framework that bootstraps exclusively from ID data and is explicitly designed to handle semantically challenging OOD samples. BootOOD synthesizes pseudo-OOD features through simple transformations of ID representations and leverages Neural Collapse (NC), where ID features cluster tightly around class means with consistent feature norms. Unlike prior approaches that aim to constrain OOD features into subspaces orthogonal to the collapsed ID means, BootOOD introduces a lightweight auxiliary head that performs radius-based classification on feature norms. This design decouples OOD detection from the primary classifier and imposes a relaxed requirement: OOD samples are learned to have smaller feature norms than ID features, which is easier to satisfy when ID and OOD are semantically close. Experiments on CIFAR-10, CIFAR-100, and ImageNet-200 show that BootOOD outperforms prior post-hoc methods, surpasses training-based methods without outlier exposure, and is competitive with state-of-the-art outlier-exposure approaches while maintaining or improving ID accuracy.

</details>


### [460] [Robust Defense Strategies for Multimodal Contrastive Learning: Efficient Fine-tuning Against Backdoor Attacks](https://arxiv.org/abs/2511.13545)
*Md. Iqbal Hossain,Afia Sajeeda,Neeresh Kumar Perla,Ming Shao*

Main category: cs.CV

TL;DR: The paper addresses backdoor attacks in multimodal deep learning models like CLIP and proposes a method to identify and rectify these attacks.


<details>
  <summary>Details</summary>
Motivation: To address the susceptibility of multimodal deep learning models, such as CLIP, to adversarial backdoor attacks and the inefficiency of current defense methods in pinpointing affected labels and samples.

Method: The authors introduce an image segmentation "oracle" to supervise the output of the poisoned CLIP model. They design two algorithms: the first identifies potential triggers by analyzing knowledge differences between CLIP and the oracle, and the second pinpoints victim samples and curates a compact fine-tuning dataset for rectifying the model.

Result: The proposed approach effectively identifies backdoor triggers, victim samples, and affected labels in poisoned CLIP models. The method demonstrates its effectiveness in rectifying backdoor effects through experimental results on visual recognition benchmarks.

Conclusion: The study presents a novel and efficient defense mechanism against backdoor attacks in multimodal contrastive learning models, enhancing the robustness of these systems without requiring extensive retraining.

Abstract: The advent of multimodal deep learning models, such as CLIP, has unlocked new frontiers in a wide range of applications, from image-text understanding to classification tasks. However, these models are not safe for adversarial attacks, particularly backdoor attacks, which can subtly manipulate model behavior. Moreover, existing defense methods typically involve training from scratch or fine-tuning using a large dataset without pinpointing the specific labels that are affected. In this study, we introduce an innovative strategy to enhance the robustness of multimodal contrastive learning models against such attacks. In particular, given a poisoned CLIP model, our approach can identify the backdoor trigger and pinpoint the victim samples and labels in an efficient manner. To that end, an image segmentation ``oracle'' is introduced as the supervisor for the output of the poisoned CLIP. We develop two algorithms to rectify the poisoned model: (1) differentiating between CLIP and Oracle's knowledge to identify potential triggers; (2) pinpointing affected labels and victim samples, and curating a compact fine-tuning dataset. With this knowledge, we are allowed to rectify the poisoned CLIP model to negate backdoor effects. Extensive experiments on visual recognition benchmarks demonstrate our strategy is effective in CLIP-based backdoor defense.

</details>


### [461] [TSE-Net: Semi-supervised Monocular Height Estimation from Single Remote Sensing Images](https://arxiv.org/abs/2511.13552)
*Sining Chen,Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: This paper introduces TSE-Net, a semi-supervised learning framework for monocular height estimation in remote sensing by leveraging unlabeled data and pseudo-labels.


<details>
  <summary>Details</summary>
Motivation: To address the limitations in monocular height estimation caused by the scarcity of labeled data, which hinders model generalization and performance.

Method: The semi-supervised framework uses a teacher-student-exam network approach. The teacher generates pseudo-labels for the student, and the exam network stabilizes performance. Height predictions are refined with a hierarchical bi-cut strategy and a Plackett-Luce model.

Result: The method is evaluated on three different datasets with varying resolutions and imaging modalities, demonstrating improved predictive performance.

Conclusion: The TSE-Net pipeline effectively enhances monocular height estimation by utilizing unlabeled data, offering a promising cost-effective alternative to traditional methods.

Abstract: Monocular height estimation plays a critical role in 3D perception for remote sensing, offering a cost-effective alternative to multi-view or LiDAR-based methods. While deep learning has significantly advanced the capabilities of monocular height estimation, these methods remain fundamentally limited by the availability of labeled data, which are expensive and labor-intensive to obtain at scale. The scarcity of high-quality annotations hinders the generalization and performance of existing models. To overcome this limitation, we propose leveraging large volumes of unlabeled data through a semi-supervised learning framework, enabling the model to extract informative cues from unlabeled samples and improve its predictive performance. In this work, we introduce TSE-Net, a self-training pipeline for semi-supervised monocular height estimation. The pipeline integrates teacher, student, and exam networks. The student network is trained on unlabeled data using pseudo-labels generated by the teacher network, while the exam network functions as a temporal ensemble of the student network to stabilize performance. The teacher network is formulated as a joint regression and classification model: the regression branch predicts height values that serve as pseudo-labels, and the classification branch predicts height value classes along with class probabilities, which are used to filter pseudo-labels. Height value classes are defined using a hierarchical bi-cut strategy to address the inherent long-tailed distribution of heights, and the predicted class probabilities are calibrated with a Plackett-Luce model to reflect the expected accuracy of pseudo-labels. We evaluate the proposed pipeline on three datasets spanning different resolutions and imaging modalities. Codes are available at https://github.com/zhu-xlab/tse-net.

</details>


### [462] [Opt3DGS: Optimizing 3D Gaussian Splatting with Adaptive Exploration and Curvature-Aware Exploitation](https://arxiv.org/abs/2511.13571)
*Ziyang Huang,Jiagang Chen,Jin Liu,Shunping Ji*

Main category: cs.CV

TL;DR: This paper introduces Opt3DGS to address optimization issues in 3D Gaussian Splatting, enhancing its convergence and rendering quality.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in 3D Gaussian Splatting such as suboptimal local optima and poor convergence quality.

Method: The framework employs a two-stage optimization process: adaptive exploration using Stochastic Gradient Langevin Dynamics and exploitation using a curvature-guided Adam optimizer.

Result: Opt3DGS achieves state-of-the-art rendering quality as demonstrated through experiments on various benchmark datasets.

Conclusion: Opt3DGS refines the 3DGS optimization process for better convergence and rendering without altering its core representation.

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a leading framework for novel view synthesis, yet its core optimization challenges remain underexplored. We identify two key issues in 3DGS optimization: entrapment in suboptimal local optima and insufficient convergence quality. To address these, we propose Opt3DGS, a robust framework that enhances 3DGS through a two-stage optimization process of adaptive exploration and curvature-guided exploitation. In the exploration phase, an Adaptive Weighted Stochastic Gradient Langevin Dynamics (SGLD) method enhances global search to escape local optima. In the exploitation phase, a Local Quasi-Newton Direction-guided Adam optimizer leverages curvature information for precise and efficient convergence. Extensive experiments on diverse benchmark datasets demonstrate that Opt3DGS achieves state-of-the-art rendering quality by refining the 3DGS optimization process without modifying its underlying representation.

</details>


### [463] [Hierarchical Prompt Learning for Image- and Text-Based Person Re-Identification](https://arxiv.org/abs/2511.13575)
*Linhan Zhou,Shuang Li,Neng Dong,Yonghang Tai,Yafei Zhang,Huafeng Li*

Main category: cs.CV

TL;DR: This paper proposes a unified framework called Hierarchical Prompt Learning (HPL) for person re-identification that simultaneously optimizes image-to-image (I2I) and text-to-image (T2I) tasks.


<details>
  <summary>Details</summary>
Motivation: Existing approaches tackle I2I and T2I tasks separately, leading to representation entanglement and suboptimal performance. A unified solution is needed to address the common retrieval objective while managing the distinct challenges of each task.

Method: The authors propose Hierarchical Prompt Learning, which uses a task-routed transformer with dual classification tokens and modality-specific pseudo-text tokens. It incorporates a hierarchical prompt generation scheme and employs Cross-Modal Prompt Regularization to enhance semantic alignment.

Result: HPL achieves state-of-the-art performance on multiple ReID benchmarks for both I2I and T2I tasks, showing its efficacy in unifying the two tasks.

Conclusion: The proposed HPL framework successfully unifies I2I and T2I tasks, improving semantic representation and addressing task-specific challenges with its innovative methods.

Abstract: Person re-identification (ReID) aims to retrieve target pedestrian images given either visual queries (image-to-image, I2I) or textual descriptions (text-to-image, T2I). Although both tasks share a common retrieval objective, they pose distinct challenges: I2I emphasizes discriminative identity learning, while T2I requires accurate cross-modal semantic alignment. Existing methods often treat these tasks separately, which may lead to representation entanglement and suboptimal performance. To address this, we propose a unified framework named Hierarchical Prompt Learning (HPL), which leverages task-aware prompt modeling to jointly optimize both tasks. Specifically, we first introduce a Task-Routed Transformer, which incorporates dual classification tokens into a shared visual encoder to route features for I2I and T2I branches respectively. On top of this, we develop a hierarchical prompt generation scheme that integrates identity-level learnable tokens with instance-level pseudo-text tokens. These pseudo-tokens are derived from image or text features via modality-specific inversion networks, injecting fine-grained, instance-specific semantics into the prompts. Furthermore, we propose a Cross-Modal Prompt Regularization strategy to enforce semantic alignment in the prompt token space, ensuring that pseudo-prompts preserve source-modality characteristics while enhancing cross-modal transferability. Extensive experiments on multiple ReID benchmarks validate the effectiveness of our method, achieving state-of-the-art performance on both I2I and T2I tasks.

</details>


### [464] [Adaptive Multi-Scale Integration Unlocks Robust Cell Annotation in Histopathology Images](https://arxiv.org/abs/2511.13586)
*Yinuo Xu,Yan Cui,Mingyao Li,Zhi Huang*

Main category: cs.CV

TL;DR: This paper introduces NuClass, a framework for cell-wise multi-scale integration of nuclear morphology and tissue context to improve cell type identification in histopathology images.


<details>
  <summary>Details</summary>
Motivation: Current models capture nuclear morphology but fail to incorporate tissue context, and high-quality fine-grained annotations are lacking.

Method: NuClass combines 'Path local' for capturing nuclear morphology and 'Path global' for broader tissue context using adaptive gating and an uncertainty-guided learning objective. It utilizes a marker-guided dataset from Xenium assays for training.

Result: NuClass achieved up to 96% F1 score on its best-performing class, outperforming existing baselines across three held-out cohorts.

Conclusion: NuClass bridges the gap between pathological foundation models and fine-grained cell-level phenotype predictions using multi-scale, uncertainty-aware fusion and a novel dataset.

Abstract: Identifying cell types and subtypes from routine histopathology images is essential for improving the computational understanding of human disease. Existing tile-based models can capture detailed nuclear morphology but often fail to incorporate the broader tissue context that influences a cell's function and identity. In addition, available human annotations are typically coarse-grained and unevenly distributed across studies, making fine-grained subtype-level supervision difficult to obtain.
  To address these limitations, we introduce NuClass, a pathologist workflow inspired framework for cell-wise multi-scale integration of nuclear morphology and microenvironmental context. NuClass includes two main components: Path local, which focuses on nuclear morphology from 224-by-224 pixel crops, and Path global, which models the surrounding 1024-by-1024 pixel neighborhood. A learnable gating module adaptively balances local detail and contextual cues. To encourage complementary learning, we incorporate an uncertainty-guided objective that directs the global path to prioritize regions where the local path is uncertain. We also provide calibrated confidence estimates and Grad-CAM visualizations to enhance interpretability.
  To overcome the lack of high-quality annotations, we construct a marker-guided dataset from Xenium spatial transcriptomics assays, yielding single-cell resolution labels for more than two million cells across eight organs and 16 classes. Evaluated on three fully held-out cohorts, NuClass achieves up to 96 percent F1 for its best-performing class, outperforming strong baselines. Our results show that multi-scale, uncertainty-aware fusion can bridge the gap between slide-level pathological foundation models and reliable, cell-level phenotype prediction.

</details>


### [465] [VVS: Accelerating Speculative Decoding for Visual Autoregressive Generation via Partial Verification Skipping](https://arxiv.org/abs/2511.13587)
*Haotian Dong,Ye Li,Rongwei Lu,Chen Tang,Shu-Tao Xia,Zhi Wang*

Main category: cs.CV

TL;DR: This paper introduces VVS, a new paradigm for accelerating visual autoregressive model generation by selectively skipping the verification step, reducing inference latency while preserving generation quality.


<details>
  <summary>Details</summary>
Motivation: The high inference latency in visual autoregressive generation models due to their next-token prediction paradigm and limitations in current speculative decoding techniques, which fail to significantly reduce the number of forward passes, motivates this research.

Method: The authors propose VVS, a new speculative decoding framework that implements partial verification skipping using three modules: dynamic token truncation, token-level feature caching and reuse, and fine-grained skipped step scheduling.

Result: VVS reduces the number of target model forward passes by 2.8x compared to standard autoregressive decoding, maintaining competitive generation quality.

Conclusion: The VVS framework achieves a superior trade-off between generation speed and quality, highlighting its capability to improve speculative decoding paradigm and accelerate visual autoregressive generation.

Abstract: Visual autoregressive (AR) generation models have demonstrated strong potential for image generation, yet their next-token-prediction paradigm introduces considerable inference latency. Although speculative decoding (SD) has been proven effective for accelerating visual AR models, its "draft one step, then verify one step" paradigm prevents a direct reduction of the forward passes, thus restricting acceleration potential. Motivated by the visual token interchangeability, we for the first time to explore verification skipping in the SD process of visual AR model generation to explicitly cut the number of target model forward passes, thereby reducing inference latency. Based on an analysis of the drafting stage's characteristics, we observe that verification redundancy and stale feature reusability are key factors to retain generation quality and speedup for verification-free steps. Inspired by these two observations, we propose a novel SD framework VVS to accelerate visual AR generation via partial verification skipping, which integrates three complementary modules: (1) a verification-free token selector with dynamical truncation, (2) token-level feature caching and reuse, and (3) fine-grained skipped step scheduling. Consequently, VVS reduces the number of target model forward passes by a factor of $2.8\times$ relative to vanilla AR decoding while maintaining competitive generation quality, offering a superior speed-quality trade-off over conventional SD frameworks and revealing strong potential to reshape the SD paradigm.

</details>


### [466] [ICLR: Inter-Chrominance and Luminance Interaction for Natural Color Restoration in Low-Light Image Enhancement](https://arxiv.org/abs/2511.13607)
*Xin Xu,Hao Liu,Wei Liu,Wei Wang,Jiayi Wu,Kui Jiang*

Main category: cs.CV

TL;DR: The paper focuses on enhancing low-light images using a new framework called ICLR, which improves chrominance-luminance interaction and gradient balance, achieving better results than existing methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle limitations in chrominance and luminance branch interactions and gradient conflicts in traditional methods for LLIE tasks.

Method: The ICLR framework introduces two key components: DIEM, for extracting complementary information, and CCL, for minimizing chrominance errors and balancing gradient conflicts.

Result: Experiments demonstrate that ICLR surpasses current state-of-the-art methods across multiple datasets for LLIE tasks.

Conclusion: ICLR effectively addresses issues in inter-branch correlation, improves on prior approaches, and sets a new benchmark in LLIE performance.

Abstract: Low-Light Image Enhancement (LLIE) task aims at improving contrast while restoring details and textures for images captured in low-light conditions. HVI color space has made significant progress in this task by enabling precise decoupling of chrominance and luminance. However, for the interaction of chrominance and luminance branches, substantial distributional differences between the two branches prevalent in natural images limit complementary feature extraction, and luminance errors are propagated to chrominance channels through the nonlinear parameter. Furthermore, for interaction between different chrominance branches, images with large homogeneous-color regions usually exhibit weak correlation between chrominance branches due to concentrated distributions. Traditional pixel-wise losses exploit strong inter-branch correlations for co-optimization, causing gradient conflicts in weakly correlated regions. Therefore, we propose an Inter-Chrominance and Luminance Interaction (ICLR) framework including a Dual-stream Interaction Enhancement Module (DIEM) and a Covariance Correction Loss (CCL). The DIEM improves the extraction of complementary information from two dimensions, fusion and enhancement, respectively. The CCL utilizes luminance residual statistics to penalize chrominance errors and balances gradient conflicts by constraining chrominance branches covariance. Experimental results on multiple datasets show that the proposed ICLR framework outperforms state-of-the-art methods.

</details>


### [467] [AtlasMorph: Learning conditional deformable templates for brain MRI](https://arxiv.org/abs/2511.13609)
*Marianne Rakic,Andrew Hoopes,S. Mazdak Abulnaga,Mert R. Sabuncu,John V. Guttag,Adrian V. Dalca*

Main category: cs.CV

TL;DR: The paper presents a machine learning framework that creates subject-specific deformable templates efficiently for medical imaging, specialized for diverse populations.


<details>
  <summary>Details</summary>
Motivation: Current deformable templates often fail to optimally represent diverse study populations due to the computational cost of their creation, which limits their variety and applicability.

Method: The authors developed a framework using convolutional registration neural networks to generate templates conditioned on subject-specific attributes (e.g., age, sex), and employed segmentations to produce anatomical segmentation maps for these templates.

Result: The proposed method produced high-quality conditional templates using 3D brain MRI datasets. These templates improved registration and outperformed other methods for template construction.

Conclusion: The framework enables generation of more representative and effective deformable templates, addressing population diversity challenges in medical image analysis.

Abstract: Deformable templates, or atlases, are images that represent a prototypical anatomy for a population, and are often enhanced with probabilistic anatomical label maps. They are commonly used in medical image analysis for population studies and computational anatomy tasks such as registration and segmentation. Because developing a template is a computationally expensive process, relatively few templates are available. As a result, analysis is often conducted with sub-optimal templates that are not truly representative of the study population, especially when there are large variations within this population. We propose a machine learning framework that uses convolutional registration neural networks to efficiently learn a function that outputs templates conditioned on subject-specific attributes, such as age and sex. We also leverage segmentations, when available, to produce anatomical segmentation maps for the resulting templates. The learned network can also be used to register subject images to the templates. We demonstrate our method on a compilation of 3D brain MRI datasets, and show that it can learn high-quality templates that are representative of populations. We find that annotated conditional templates enable better registration than their unlabeled unconditional counterparts, and outperform other templates construction methods.

</details>


### [468] [Tissue Aware Nuclei Detection and Classification Model for Histopathology Images](https://arxiv.org/abs/2511.13615)
*Kesi Xu,Eleni Chiou,Ali Varamesh,Laura Acqualagna,Nasir Rajpoot*

Main category: cs.CV

TL;DR: This paper introduces the Tissue-Aware Nuclei Detection (TAND) framework for nuclei detection and classification, leveraging point-level supervision and incorporating tissue context through innovative methods.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenges in computational pathology due to the reliance on expert annotations and insufficient incorporation of tissue context in existing approaches.

Method: TAND integrates a ConvNeXt-based encoder-decoder and a frozen Virchow-2 tissue segmentation branch, utilizing Spatial Feature-wise Linear Modulation (Spatial-FiLM) to modulate the classification stream with semantic tissue probabilities.

Result: The proposed framework achieves state-of-the-art performance on the PUMA benchmark, showcasing significant improvements in classifying tissue-dependent cell types.

Conclusion: TAND represents a landmark advancement by conditioning per-cell classification on learned tissue masks, reducing annotation burden while enhancing detection and classification accuracy.

Abstract: Accurate nuclei detection and classification are fundamental to computational pathology, yet existing approaches are hindered by reliance on detailed expert annotations and insufficient use of tissue context. We present Tissue-Aware Nuclei Detection (TAND), a novel framework achieving joint nuclei detection and classification using point-level supervision enhanced by tissue mask conditioning. TAND couples a ConvNeXt-based encoder-decoder with a frozen Virchow-2 tissue segmentation branch, where semantic tissue probabilities selectively modulate the classification stream through a novel multi-scale Spatial Feature-wise Linear Modulation (Spatial-FiLM). On the PUMA benchmark, TAND achieves state-of-the-art performance, surpassing both tissue-agnostic baselines and mask-supervised methods. Notably, our approach demonstrates remarkable improvements in tissue-dependent cell types such as epithelium, endothelium, and stroma. To the best of our knowledge, this is the first method to condition per-cell classification on learned tissue masks, offering a practical pathway to reduce annotation burden.

</details>


### [469] [A Real-Time Driver Drowsiness Detection System Using MediaPipe and Eye Aspect Ratio](https://arxiv.org/abs/2511.13618)
*Ashlesha G. Sawant,Shreyash S. Kamble,Raj S. Kanade,Raunak N. Kanugo,Tanishq A. Kapse,Karan A. Bhapse*

Main category: cs.CV

TL;DR: This paper presents a Driver Drowsiness Detection System that utilizes webcam-based tracking of facial features, focusing on eye movements, to identify drowsy behaviors and alert drivers.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the critical issue of driver fatigue, a leading cause of road accidents causing numerous fatalities and injuries annually. The aim is to enhance road safety by alerting drowsy drivers.

Method: The system employs a standard webcam for facial feature tracking, emphasizing the Eye Aspect Ratio (EAR) to monitor eye movements. It uses MediaPipe’s Face Mesh for facial landmark identification and OpenCV for image processing to detect signs of drowsiness like prolonged eye closure or reduced blinking rate.

Result: Experimental analyses demonstrate that the system is highly accurate, quick to respond, and cost-efficient, making it suitable for real-time applications.

Conclusion: The proposed system provides an effective and affordable solution for driver monitoring, with potential integration into Advanced Driving Assistance Systems (ADAS).

Abstract: One of the major causes of road accidents is driver fatigue that causes thousands of fatalities and injuries every year. This study shows development of a Driver Drowsiness Detection System meant to improve the safety of the road by alerting drivers who are showing signs of being drowsy. The system is based on a standard webcam that tracks the facial features of the driver with the main emphasis on the examination of eye movements that can be conducted with the help of the Eye Aspect Ratio (EAR) method. The Face Mesh by MediaPipe is a lightweight framework that can identify facial landmarks with high accuracy and efficiency, which is considered to be important in real time use. The system detects the moments of long eye shutdowns or a very low rate of blinking which are manifestations of drowsiness and alerts the driver through sound to get her attention back. This system achieves a high-performance and low-cost driver monitoring solution with the help of the computational power of OpenCV to process the image and the MediaPipe to identify faces. Test data experimental analyses indicate that the system is very accurate and responds quicker; this confirms that it can be a component of the current Advanced Driving Assistance System (ADAS).

</details>


### [470] [Alpha Divergence Losses for Biometric Verification](https://arxiv.org/abs/2511.13621)
*Dimitrios Koutsianos,Ladislav Mosner,Yannis Panagakis,Themos Stafylakis*

Main category: cs.CV

TL;DR: The paper proposes two novel margin-based $α$-divergence losses for face and speaker verification tasks, introducing Q-Margin and A3M methods to enhance performance in critical applications.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of integrating angular margins with $α$-divergence loss functions for improved performance in verification tasks, particularly under high-security, low false acceptance rate conditions.

Method: The authors develop two approaches: Q-Margin (angular margin via reference measure) and A3M (angular margin via logits), and introduce a prototype re-initialization strategy to mitigate training instability in A3M.

Result: The proposed methods show significant performance improvements on IJB-B and IJB-C face verification benchmarks and VoxCeleb speaker verification, particularly at low false acceptance rates.

Conclusion: The novel margin-based $α$-divergence losses provide robust solutions for high-security face and speaker verification applications, outperforming existing baselines.

Abstract: Performance in face and speaker verification is largely driven by margin based softmax losses like CosFace and ArcFace. Recently introduced $α$-divergence loss functions offer a compelling alternative, particularly for their ability to induce sparse solutions (when $α>1$). However, integrating an angular margin-crucial for verification tasks-is not straightforward. We find this integration can be achieved in at least two distinct ways: via the reference measure (prior probabilities) or via the logits (unnormalized log-likelihoods). In this paper, we explore both pathways, deriving two novel margin-based $α$-divergence losses: Q-Margin (margin in the reference measure) and A3M (margin in the logits). We identify and address a critical training instability in A3M-caused by the interplay of penalized logits and sparsity-with a simple yet effective prototype re-initialization strategy. Our methods achieve significant performance gains on the challenging IJB-B and IJB-C face verification benchmarks. We demonstrate similarly strong performance in speaker verification on VoxCeleb. Crucially, our models significantly outperform strong baselines at low false acceptance rates (FAR). This capability is crucial for practical high-security applications, such as banking authentication, when minimizing false authentications is paramount.

</details>


### [471] [CacheFlow: Compressive Streaming Memory for Efficient Long-Form Video Understanding](https://arxiv.org/abs/2511.13644)
*Shrenik Patel,Daivik Patel*

Main category: cs.CV

TL;DR: This paper introduces CacheFlow, a training-free pipeline for efficiently handling long-form video question answering, using token pruning and a compressive memory system.


<details>
  <summary>Details</summary>
Motivation: Long-form video question answering challenges current vision-language models due to the inefficiency of growing attention and key-value caches.

Method: CacheFlow combines Dynamic Token Dropping (DTD) to prune tokens online and a compressive long-term memory indexed by a recurrent encoder, allowing efficient retrieval and precise long-range reasoning.

Result: CacheFlow leads to improved performance in benchmarks, efficiently processing up to 87% fewer tokens compared to baselines.

Conclusion: CacheFlow enables vision-language models to handle long-form video understanding effectively and efficiently without model fine-tuning.

Abstract: Long-form video question answering (VQA) overwhelms current vision-language models (VLMs) because attention and key-value (KV) caches grow with runtime, forcing either expensive inference or near-sighted sliding windows. We introduce CacheFlow, a training-free pipeline that pairs Dynamic Token Dropping (DTD) with a compressive long-term memory. DTD prunes per-patch tokens online via cosine similarity to the previous frame, and surviving tokens are packed into fixed-size blocks. This online, per-frame processing makes our approach fundamentally suited for live streaming VQA. As blocks are processed, each one's keys are summarized by a tiny recurrent encoder to form a retrieval index, while the block's full KV pairs are offloaded and later rehydrated for generation, preserving answer fidelity. At inference, a consensus-based retrieval mechanism retrieves only the Top-K most relevant blocks and attends over both the retrieved and local context for precise, long-range reasoning. CacheFlow is drop-in, architecture-agnostic, and requires no fine-tuning. Experiments on both offline and streaming VQA benchmarks demonstrate that CacheFlow outperforms current strong baselines, while processing up to 87% less tokens. Our dual approach enables VLMs to be both efficient and context-aware, paving the way for practical long-form video understanding.

</details>


### [472] [Part-X-MLLM: Part-aware 3D Multimodal Large Language Model](https://arxiv.org/abs/2511.13647)
*Chunshi Wang,Junliang Ye,Yunhan Yang,Yang Li,Zizhuo Lin,Jun Zhu,Zhuo Chen,Yawei Luo,Chunchao Guo*

Main category: cs.CV

TL;DR: Part-X-MLLM is a 3D multimodal language model that formulates diverse tasks as structured, executable programs. It excels in grounded Q&A, generation, and editing by leveraging part-level symbolic planning and a unified interface.


<details>
  <summary>Details</summary>
Motivation: To unify diverse 3D tasks and provide a versatile interface for part-level generation and editing, bridging structured symbolic planning with geometry synthesis.

Method: The model utilizes a dual-encoder architecture, pre-trained to disentangle structure and semantics, and instruction-tuned on large-scale part-centric datasets, generating a coherent token sequence for task execution.

Result: Part-X-MLLM achieves state-of-the-art performance in grounded Q&A, compositional generation, and localized editing tasks using a unified language-native frontend.

Conclusion: The approach simplifies diverse 3D tasks by employing structured, symbolic planning and enables precise control for geometry-aware modules through one cohesive framework.

Abstract: We introduce Part-X-MLLM, a native 3D multimodal large language model that unifies diverse 3D tasks by formulating them as programs in a structured, executable grammar. Given an RGB point cloud and a natural language prompt, our model autoregressively generates a single, coherent token sequence encoding part-level bounding boxes, semantic descriptions, and edit commands. This structured output serves as a versatile interface to drive downstream geometry-aware modules for part-based generation and editing. By decoupling the symbolic planning from the geometric synthesis, our approach allows any compatible geometry engine to be controlled through a single, language-native frontend. We pre-train a dual-encoder architecture to disentangle structure from semantics and instruction-tune the model on a large-scale, part-centric dataset. Experiments demonstrate that our model excels at producing high-quality, structured plans, enabling state-of-the-art performance in grounded Q\&A, compositional generation, and localized editing through one unified interface. Project page: https://chunshi.wang/Part-X-MLLM/

</details>


### [473] [Distribution Matching Distillation Meets Reinforcement Learning](https://arxiv.org/abs/2511.13649)
*Dengyang Jiang,Dongyang Liu,Zanyi Wang,Qilong Wu,Xin Jin,David Liu,Zhen Li,Mengmeng Wang,Peng Gao,Harry Yang*

Main category: cs.CV

TL;DR: DMDR combines reinforcement learning with distillation to improve few-step diffusion models, achieving better visual quality and coherence than multi-step models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the performance limitation of few-step diffusion models compared to their multi-step counterparts and enhance inference efficiency.

Method: The proposed DMDR framework integrates reinforcement learning into the Distribution Matching Distillation process. It introduces dynamic distribution guidance and dynamic renoise sampling techniques to improve distillation outcome.

Result: DMDR demonstrates leading visual quality, improved prompt coherence, and outperforms its multi-step teacher in some experiments.

Conclusion: DMDR effectively unlocks the potential of few-step generators, surpassing traditional performance limitations, and strengthens efficiency and quality.

Abstract: Distribution Matching Distillation (DMD) distills a pre-trained multi-step diffusion model to a few-step one to improve inference efficiency. However, the performance of the latter is often capped by the former. To circumvent this dilemma, we propose DMDR, a novel framework that combines Reinforcement Learning (RL) techniques into the distillation process. We show that for the RL of the few-step generator, the DMD loss itself is a more effective regularization compared to the traditional ones. In turn, RL can help to guide the mode coverage process in DMD more effectively. These allow us to unlock the capacity of the few-step generator by conducting distillation and RL simultaneously. Meanwhile, we design the dynamic distribution guidance and dynamic renoise sampling training strategies to improve the initial distillation process. The experiments demonstrate that DMDR can achieve leading visual quality, prompt coherence among few-step methods, and even exhibit performance that exceeds the multi-step teacher.

</details>


### [474] [OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation](https://arxiv.org/abs/2511.13655)
*Henry Herzog,Favyen Bastani,Yawen Zhang,Gabriel Tseng,Joseph Redmon,Hadrien Sablon,Ryan Park,Jacob Morrison,Alexandra Buraczynski,Karen Farley,Joshua Hansen,Andrew Howe,Patrick Alan Johnson,Mark Otterlee,Ted Schmitt,Hunter Pitelka,Stephen Daspit,Rachel Ratner,Christopher Wilhelm,Sebastian Wood,Mike Jacobi,Hannah Kerner,Evan Shelhamer,Ali Farhadi,Ranjay Krishna,Patrick Beukema*

Main category: cs.CV

TL;DR: The paper introduces OlmoEarth, a state-of-the-art, multimodal, spatio-temporal foundation model targeted for Earth observation applications.


<details>
  <summary>Details</summary>
Motivation: To address the challenges posed by Earth observation data, which is complex due to its spatial, sequential, and multimodal nature, and to provide powerful tools for solving global problems.

Method: The model employs novel self-supervised learning techniques with domain-specific masking strategies and losses, and was tested against multiple benchmarks and real-world tasks.

Result: OlmoEarth outperforms 12 foundation models in benchmarks, achieving the best embedding performance on 15 out of 24 tasks and top fine-tuning results on 19 out of 29 tasks.

Conclusion: OlmoEarth demonstrates superior performance and usability, becoming a crucial tool for NGOs to tackle global challenges, with accessible code, data, and tools to support future research.

Abstract: Earth observation data presents a unique challenge: it is spatial like images, sequential like video or text, and highly multimodal. We present OlmoEarth: a multimodal, spatio-temporal foundation model that employs a novel self-supervised learning formulation, masking strategy, and loss all designed for the Earth observation domain. OlmoEarth achieves state-of-the-art performance compared to 12 other foundation models across a variety of research benchmarks and real-world tasks from external partners. When evaluating embeddings OlmoEarth achieves the best performance on 15 out of 24 tasks, and with full fine-tuning it is the best on 19 of 29 tasks. We deploy OlmoEarth as the backbone of an end-to-end platform for data collection, labeling, training, and inference of Earth observation models. The OlmoEarth Platform puts frontier foundation models and powerful data management tools into the hands of non-profits and NGOs working to solve the world's biggest problems. OlmoEarth source code, training data, and pre-trained weights are available at $\href{https://github.com/allenai/olmoearth_pretrain}{\text{https://github.com/allenai/olmoearth_pretrain}}$.

</details>


### [475] [Training-Free Multi-View Extension of IC-Light for Textual Position-Aware Scene Relighting](https://arxiv.org/abs/2511.13684)
*Jiangnan Ye,Jiedong Zhuang,Lianrui Mu,Wenjie Zheng,Jiaqi Hu,Xingze Zou,Jing Wang,Haoji Hu*

Main category: cs.CV

TL;DR: GS-Light introduces a training-free solution for text-guided relighting of 3D scenes via Gaussian Splatting, leveraging a diffusion model enhanced by multi-view inputs and lighting priors.


<details>
  <summary>Details</summary>
Motivation: To address the need for efficient, accurate, and user-guided 3D scene relighting in both indoor and outdoor contexts using textual inputs.

Method: GS-Light uses a vision-language model to interpret user prompts for lighting specifications. It integrates view-geometry constraints and lighting priors to initialize latent codes for guiding a diffusion model to generate artistically relit, multi-view consistent images. Fine-tuning adapts the 3DGS scene to match relit appearances.

Result: Testing shows GS-Light's improvements over state-of-the-art baselines in multi-view consistency, aesthetics, imaging quality, and semantic matching.

Conclusion: GS-Light provides an effective pipeline for user-customized relighting of 3D scenes, outperforming existing methods in accuracy and artistic fidelity. Code will be publicly available upon release.

Abstract: We introduce GS-Light, an efficient, textual position-aware pipeline for text-guided relighting of 3D scenes represented via Gaussian Splatting (3DGS). GS-Light implements a training-free extension of a single-input diffusion model to handle multi-view inputs. Given a user prompt that may specify lighting direction, color, intensity, or reference objects, we employ a large vision-language model (LVLM) to parse the prompt into lighting priors. Using off-the-shelf estimators for geometry and semantics (depth, surface normals, and semantic segmentation), we fuse these lighting priors with view-geometry constraints to compute illumination maps and generate initial latent codes for each view. These meticulously derived init latents guide the diffusion model to generate relighting outputs that more accurately reflect user expectations, especially in terms of lighting direction. By feeding multi-view rendered images, along with the init latents, into our multi-view relighting model, we produce high-fidelity, artistically relit images. Finally, we fine-tune the 3DGS scene with the relit appearance to obtain a fully relit 3D scene. We evaluate GS-Light on both indoor and outdoor scenes, comparing it to state-of-the-art baselines including per-view relighting, video relighting, and scene editing methods. Using quantitative metrics (multi-view consistency, imaging quality, aesthetic score, semantic similarity, etc.) and qualitative assessment (user studies), GS-Light demonstrates consistent improvements over baselines. Code and assets will be made available upon publication.

</details>


### [476] [TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models](https://arxiv.org/abs/2511.13704)
*Harold Haodong Chen,Disen Lan,Wen-Jie Shu,Qingyang Liu,Zihan Wang,Sirui Chen,Wenkai Cheng,Kanghao Chen,Hongfei Zhang,Zixin Zhang,Rongjin Guo,Yu Cheng,Ying-Cong Chen*

Main category: cs.CV

TL;DR: This paper introduces TiViBench, a hierarchical benchmark to evaluate reasoning capabilities in image-to-video generative models, and VideoTPO, a test-time strategy enhancing reasoning performance without extra training or data.


<details>
  <summary>Details</summary>
Motivation: The lack of benchmarks assessing higher-order reasoning abilities in video generative models, as existing benchmarks mainly focus on visual fidelity and temporal coherence.

Method: TiViBench evaluates reasoning across four dimensions with 24 tasks over three difficulty levels. The authors also propose VideoTPO, a test-time strategy using LLM self-analysis for reasoning enhancement.

Result: Commercial models like Sora 2 and Veo 3.1 display strong reasoning potential, but open-source models are limited by training scale and diversity. VideoTPO improves reasoning capabilities significantly without additional training or data.

Conclusion: The work introduces a novel benchmark and test-time strategy to assess and enhance reasoning in video generative models, establishing a foundation for further research in this field.

Abstract: The rapid evolution of video generative models has shifted their focus from producing visually plausible outputs to tackling tasks requiring physical plausibility and logical consistency. However, despite recent breakthroughs such as Veo 3's chain-of-frames reasoning, it remains unclear whether these models can exhibit reasoning capabilities similar to large language models (LLMs). Existing benchmarks predominantly evaluate visual fidelity and temporal coherence, failing to capture higher-order reasoning abilities. To bridge this gap, we propose TiViBench, a hierarchical benchmark specifically designed to evaluate the reasoning capabilities of image-to-video (I2V) generation models. TiViBench systematically assesses reasoning across four dimensions: i) Structural Reasoning & Search, ii) Spatial & Visual Pattern Reasoning, iii) Symbolic & Logical Reasoning, and iv) Action Planning & Task Execution, spanning 24 diverse task scenarios across 3 difficulty levels. Through extensive evaluations, we show that commercial models (e.g., Sora 2, Veo 3.1) demonstrate stronger reasoning potential, while open-source models reveal untapped potential that remains hindered by limited training scale and data diversity. To further unlock this potential, we introduce VideoTPO, a simple yet effective test-time strategy inspired by preference optimization. By performing LLM self-analysis on generated candidates to identify strengths and weaknesses, VideoTPO significantly enhances reasoning performance without requiring additional training, data, or reward models. Together, TiViBench and VideoTPO pave the way for evaluating and advancing reasoning in video generation models, setting a foundation for future research in this emerging field.

</details>


### [477] [Free-Form Scene Editor: Enabling Multi-Round Object Manipulation like in a 3D Engine](https://arxiv.org/abs/2511.13713)
*Xincheng Shuai,Zhenyuan Qin,Henghui Ding,Dacheng Tao*

Main category: cs.CV

TL;DR: FFSE is a novel framework for 3D-aware object editing in images, outperforming traditional methods by modeling editing through learned 3D transformations instead of image space.


<details>
  <summary>Details</summary>
Motivation: To address the limitations in existing semantic image editing methods, particularly their inability to perform 3D-aware manipulations directly on real-world images without tedious reconstruction.

Method: FFSE uses an autoregressive framework with learned 3D transformations, along with a custom hybrid dataset named 3DObjectEditor for training under multi-round editing conditions.

Result: FFSE demonstrates superior performance in both single and multi-round 3D-aware editing scenarios, preserving realistic elements like shadows and scene consistency.

Conclusion: FFSE advances the field of 3D object editing by enabling intuitive and physically consistent manipulations directly on images, offering significant improvement over prior approaches.

Abstract: Recent advances in text-to-image (T2I) diffusion models have significantly improved semantic image editing, yet most methods fall short in performing 3D-aware object manipulation. In this work, we present FFSE, a 3D-aware autoregressive framework designed to enable intuitive, physically-consistent object editing directly on real-world images. Unlike previous approaches that either operate in image space or require slow and error-prone 3D reconstruction, FFSE models editing as a sequence of learned 3D transformations, allowing users to perform arbitrary manipulations, such as translation, scaling, and rotation, while preserving realistic background effects (e.g., shadows, reflections) and maintaining global scene consistency across multiple editing rounds. To support learning of multi-round 3D-aware object manipulation, we introduce 3DObjectEditor, a hybrid dataset constructed from simulated editing sequences across diverse objects and scenes, enabling effective training under multi-round and dynamic conditions. Extensive experiments show that the proposed FFSE significantly outperforms existing methods in both single-round and multi-round 3D-aware editing scenarios.

</details>


### [478] [UnSAMv2: Self-Supervised Learning Enables Segment Anything at Any Granularity](https://arxiv.org/abs/2511.13714)
*Junwei Yu,Trevor Darrell,XuDong Wang*

Main category: cs.CV

TL;DR: UnSAMv2 enhances SAM by enabling segmentation at any granularity using self-supervised learning without human annotations.


<details>
  <summary>Details</summary>
Motivation: Address the limitation of SAM's ability to control segmentation granularity and reduce dependency on human annotations.

Method: Extend the divide-and-conquer strategy, discover mask-granularity pairs, and introduce granularity control embedding.

Result: UnSAMv2 improves segmentation metrics across several benchmarks, including interactive, whole-image, and video tasks.

Conclusion: UnSAMv2 unlocks the potential of foundation models by achieving segmentation at any scale with minimal unlabeled data.

Abstract: The Segment Anything Model (SAM) family has become a widely adopted vision foundation model, but its ability to control segmentation granularity remains limited. Users often need to refine results manually - by adding more prompts or selecting from pre-generated masks - to achieve the desired level of detail. This process can be ambiguous, as the same prompt may correspond to several plausible masks, and collecting dense annotations across all granularities is prohibitively expensive, making supervised solutions infeasible. To address this limitation, we introduce UnSAMv2, which enables segment anything at any granularity without human annotations. UnSAMv2 extends the divide-and-conquer strategy of UnSAM by discovering abundant mask-granularity pairs and introducing a novel granularity control embedding that enables precise, continuous control over segmentation scale. Remarkably, with only $6$K unlabeled images and $0.02\%$ additional parameters, UnSAMv2 substantially enhances SAM-2, achieving segment anything at any granularity across interactive, whole-image, and video segmentation tasks. Evaluated on over $11$ benchmarks, UnSAMv2 improves $\text{NoC}_{90}$ (5.69 $\rightarrow$ 4.75), 1-IoU (58.0 $\rightarrow$ 73.1), and $\text{AR}_{1000}$ (49.6 $\rightarrow$ 68.3), showing that small amounts of unlabeled data with a granularity-aware self-supervised learning method can unlock the potential of vision foundation models.

</details>


### [479] [Segment Anything Across Shots: A Method and Benchmark](https://arxiv.org/abs/2511.13715)
*Hengrui Hu,Kaining Ying,Henghui Ding*

Main category: cs.CV

TL;DR: The paper introduces a strategy and model for multi-shot semi-supervised video object segmentation (MVOS), addressing challenges in handling shot transitions in videos.


<details>
  <summary>Details</summary>
Motivation: Existing VOS methods perform well on single-shot videos but fail with multi-shot transitions due to lack of cross-shot adaptability and annotated multi-shot training data.

Method: A transition mimicking data augmentation (TMA) strategy was introduced to use single-shot data effectively for multi-shot learning alongside the Segment Anything Across Shots (SAAS) model capable of detecting and segmenting objects across transitions.

Result: The proposed SAAS model achieves state-of-the-art performance when tested on the YouMVOS and newly-introduced Cut-VOS datasets.

Conclusion: The methods presented effectively tackle multi-shot video segmentation challenges, making significant progress in addressing transitions and expanding real-world applicability.

Abstract: This work focuses on multi-shot semi-supervised video object segmentation (MVOS), which aims at segmenting the target object indicated by an initial mask throughout a video with multiple shots. The existing VOS methods mainly focus on single-shot videos and struggle with shot discontinuities, thereby limiting their real-world applicability. We propose a transition mimicking data augmentation strategy (TMA) which enables cross-shot generalization with single-shot data to alleviate the severe annotated multi-shot data sparsity, and the Segment Anything Across Shots (SAAS) model, which can detect and comprehend shot transitions effectively. To support evaluation and future study in MVOS, we introduce Cut-VOS, a new MVOS benchmark with dense mask annotations, diverse object categories, and high-frequency transitions. Extensive experiments on YouMVOS and Cut-VOS demonstrate that the proposed SAAS achieves state-of-the-art performance by effectively mimicking, understanding, and segmenting across complex transitions. The code and datasets are released at https://henghuiding.com/SAAS/.

</details>


### [480] [Back to Basics: Let Denoising Generative Models Denoise](https://arxiv.org/abs/2511.13720)
*Tianhong Li,Kaiming He*

Main category: cs.CV

TL;DR: This paper introduces JiT (Just image Transformers), a novel approach advocating for generative models that predict clean images directly, rather than noised data.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency in today’s denoising diffusion models that focus on predicting noise/noised quantities, which are misaligned with the natural manifold structure of data.

Method: A generative modeling approach using simple, large-patch Transformers, without tokenizers, pre-training, or extra loss, directly predicting clean image data.

Result: JiT demonstrates competitive results on ImageNet at 256 and 512 resolutions using large patch sizes, avoiding failures seen in models predicting noised quantities.

Conclusion: Directly predicting clean data aligns better with the manifold assumption and enables effective high-dimensional image generation through simpler Transformer networks like JiT.

Abstract: Today's denoising diffusion models do not "denoise" in the classical sense, i.e., they do not directly predict clean images. Rather, the neural networks predict noise or a noised quantity. In this paper, we suggest that predicting clean data and predicting noised quantities are fundamentally different. According to the manifold assumption, natural data should lie on a low-dimensional manifold, whereas noised quantities do not. With this assumption, we advocate for models that directly predict clean data, which allows apparently under-capacity networks to operate effectively in very high-dimensional spaces. We show that simple, large-patch Transformers on pixels can be strong generative models: using no tokenizer, no pre-training, and no extra loss. Our approach is conceptually nothing more than "$\textbf{Just image Transformers}$", or $\textbf{JiT}$, as we call it. We report competitive results using JiT with large patch sizes of 16 and 32 on ImageNet at resolutions of 256 and 512, where predicting high-dimensional noised quantities can fail catastrophically. With our networks mapping back to the basics of the manifold, our research goes back to basics and pursues a self-contained paradigm for Transformer-based diffusion on raw natural data.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [481] [ACE-GNN: Adaptive GNN Co-Inference with System-Aware Scheduling in Dynamic Edge Environments](https://arxiv.org/abs/2511.11586)
*Ao Zhou,Jianlei Yang,Tong Qiao,Yingjie Qi,Xinming Wei,Cenlin Duan,Weisheng Zhao,Chunming Hu*

Main category: cs.DC

TL;DR: ACE-GNN introduces an adaptive framework for dynamic edge environments, offering significant performance and energy benefits for Graph Neural Network (GNN) co-inference.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of static deployment methods in edge Graph Neural Network (GNN) co-inference, which struggle with dynamic environmental factors like network fluctuations and multi-device access.

Method: The framework employs system-level abstraction, novel runtime prediction methods, adaptive scheduling between pipeline parallelism and data parallelism, efficient batch inference strategies, and specialized communication middleware.

Result: ACE-GNN achieves up to 12.7x speedup and 82.3% energy savings over GCoDE, along with significantly better energy efficiency compared to Fograph.

Conclusion: ACE-GNN offers a robust and adaptable solution for enhancing GNN co-inference in dynamic edge environments, combining innovative strategies to optimize performance and energy efficiency.

Abstract: The device-edge co-inference paradigm effectively bridges the gap between the high resource demands of Graph Neural Networks (GNNs) and limited device resources, making it a promising solution for advancing edge GNN applications. Existing research enhances GNN co-inference by leveraging offline model splitting and pipeline parallelism (PP), which enables more efficient computation and resource utilization during inference. However, the performance of these static deployment methods is significantly affected by environmental dynamics such as network fluctuations and multi-device access, which remain unaddressed. We present ACE-GNN, the first Adaptive GNN Co-inference framework tailored for dynamic Edge environments, to boost system performance and stability. ACE-GNN achieves performance awareness for complex multi-device access edge systems via system-level abstraction and two novel prediction methods, enabling rapid runtime scheme optimization. Moreover, we introduce a data parallelism (DP) mechanism in the runtime optimization space, enabling adaptive scheduling between PP and DP to leverage their distinct advantages and maintain stable system performance. Also, an efficient batch inference strategy and specialized communication middleware are implemented to further improve performance. Extensive experiments across diverse applications and edge settings demonstrate that ACE-GNN achieves a speedup of up to 12.7x and an energy savings of 82.3% compared to GCoDE, as well as 11.7 better energy efficiency than Fograph.

</details>


### [482] [Distributed Q-learning-based Shortest-Path Tree Construction in IoT Sensor Networks](https://arxiv.org/abs/2511.11598)
*Van-Vi Vo,Tien-Dung Nguyen,Duc-Tai Le,Hyunseung Choo*

Main category: cs.DC

TL;DR: The paper presents a distributed Q-learning framework for efficient routing in IoT sensor networks, achieving near-optimal performance with reduced overhead and enhanced scalability.


<details>
  <summary>Details</summary>
Motivation: To address limitations of centralized routing methods in dynamic and resource-constrained IoT environments by leveraging autonomous, learning-based solutions.

Method: A distributed Q-learning framework where sensor nodes make independent routing decisions based on local information, using states defined by positions and routing history and a reward function designed to optimize energy efficiency and performance.

Result: Simulations across varied network topologies show over 99% routing accuracy for larger networks and effective scalability with reduced communication overhead.

Conclusion: The proposed Q-learning framework offers a robust, efficient, and scalable alternative to traditional routing methods in IoT networks by enabling autonomous local decision-making.

Abstract: Efficient routing in IoT sensor networks is critical for minimizing energy consumption and latency. Traditional centralized algorithms, such as Dijkstra's, are computationally intensive and ill-suited for dynamic, distributed IoT environments. We propose a novel distributed Q-learning framework for constructing shortest-path trees (SPTs), enabling sensor nodes to independently learn optimal next-hop decisions using only local information. States are defined based on node positions and routing history, with a reward function that incentivizes progression toward the sink while penalizing inefficient paths. Trained on diverse network topologies, the framework generalizes effectively to unseen networks. Simulations across 100 to 500 nodes demonstrate near-optimal routing accuracy (over 99% for networks with more than 300 nodes), with minor deviations (1-2 extra hops) in smaller networks having negligible impact on performance. Compared to centralized and flooding-based methods, our approach reduces communication overhead, adapts to topology changes, and enhances scalability and energy efficiency. This work underscores the potential of Q-learning for autonomous, robust routing in resource-constrained IoT networks, offering a scalable alternative to traditional protocols.

</details>


### [483] [Mind the Gap: Revealing Inconsistencies Across Heterogeneous AI Accelerators](https://arxiv.org/abs/2511.11601)
*Elliott Wen,Sean Ma,Ewan Tempero,Jens Dietrich,Daniel Luo,Jiaxing Shen,Kaiqi Zhao,Bruce Sham,Yousong Song,Jiayi Hua,Jia Hong*

Main category: cs.DC

TL;DR: This paper studies the divergence in machine learning performance across five enterprise-grade AI accelerators, highlighting challenges in operator support, output consistency, and platform-specific issues.


<details>
  <summary>Details</summary>
Motivation: The growing diversity of AI accelerators and vendors competing with NVIDIA raises concerns about compatibility and performance consistency across different platforms.

Method: The study uses an automated pipeline to generate 100,000 variant models derived from 4,000 real-world machine learning models and evaluates their performance on five enterprise AI accelerators.

Result: Findings show newer platforms like Mac and Huawei support fewer operators (17% less) than NVIDIA, exhibit higher output discrepancies (>5%), and are prone to failures in compilation-based acceleration. The paper also identifies implementation flaws in PyTorch and vendor-specific issues.

Conclusion: This paper highlights the challenges in ensuring consistent machine learning behavior across diverse hardware systems, stressing the importance of addressing platform-specific issues and achieving reliable cross-hardware adaptability.

Abstract: While NVIDIA remains the dominant provider of AI accelerators within cloud data center, emerging vendors such as AMD, Intel, Mac, and Huawei offer cost-effective alternatives with claims of compatibility and performance. This paper presents the first empirical study investigating divergence in machine learning model across heterogeneous AI accelerators. Utilizing an automated pipeline, we synthesize over 100,000 variant models derived from 4,000 real-world models and execute them across five different enterprise-grade accelerators. Our findings suggest that newer AI platforms from Mac and Huawei support at least 17\% fewer operators than NVIDIA. These platforms also exhibit a higher rate of output discrepancies (exceeding 5\%), which stem from differences in operator implementations, handling of exceptional numerical values, and instruction scheduling. They are also more susceptible to failures during model compilation-based acceleration, and in some cases, the compiled models produce outputs that differ noticeably from those generated using the standard execution mode. In addition, we identify 7 implementation flaws in PyTorch and 40 platform-specific issues across vendors. These results underscore the challenges of achieving consistent machine learning behavior in an increasingly diverse hardware ecosystem.

</details>


### [484] [Machine learning-based cloud resource allocation algorithms: a comprehensive comparative review](https://arxiv.org/abs/2511.11603)
*Deep Bodra,Sushil Khairnar*

Main category: cs.DC

TL;DR: This paper evaluates 10 algorithms in AI and ML for cloud resource allocation, focusing on cost, performance, and energy efficiency. Hybrid approaches perform best, especially in edge computing.


<details>
  <summary>Details</summary>
Motivation: The complexity and cost-performance challenges in cloud computing require improved resource allocation approaches, as traditional heuristics are insufficient.

Method: The study conducts a comparative analysis of 10 AI and ML algorithms across multiple categories (Deep Reinforcement Learning, Neural Networks, Traditional ML, and Multi-Agent systems) using published metrics.

Result: The analysis reveals improvement in makespan reduction, cost optimization, and energy efficiency, with hybrid AI/ML methods outperforming single approaches.

Conclusion: Hybrid AI and ML methods are most effective for cloud resource allocation, especially in edge computing environments, offering guidance for researchers and practitioners.

Abstract: Cloud resource allocation has emerged as a major challenge in modern computing environments, with organizations struggling to manage complex, dynamic workloads while optimizing performance and cost efficiency. Traditional heuristic approaches prove inadequate for handling the multi-objective optimization demands of existing cloud infrastructures. This paper presents a comparative analysis of state-of-the-art artificial intelligence and machine learning algorithms for resource allocation. We systematically evaluate 10 algorithms across four categories: Deep Reinforcement Learning approaches, Neural Network architectures, Traditional Machine Learning enhanced methods, and Multi-Agent systems. Analysis of published results demonstrates significant performance improvements across multiple metrics including makespan reduction, cost optimization, and energy efficiency gains compared to traditional methods. The findings reveal that hybrid architectures combining multiple artificial intelligence and machine learning techniques consistently outperform single-method approaches, with edge computing environments showing the highest deployment readiness. Our analysis provides critical insights for both academic researchers and industry practitioners seeking to implement next-generation cloud resource allocation strategies in increasingly complex and dynamic computing environments.

</details>


### [485] [PACE Solver Description: twin_width_fmi](https://arxiv.org/abs/2511.11605)
*David Balaban,Adrian Miclăuş*

Main category: cs.DC

TL;DR: This paper introduces 'twin_width_fmi', a solver for PACE 2025's Minimum Dominating Set competition, employing heuristic methods inspired by iterative greedy techniques for optimized domination.


<details>
  <summary>Details</summary>
Motivation: To create an effective and competitive solver for the Minimum Dominating Set problem for PACE 2025 using innovative heuristic strategies.

Method: The paper combines a standard greedy heuristic (greedy-ln), simulated annealing for local search, iterative-greedy heuristics in their 'hedom5' component, and uses compact graph structures and optimization strategies like backward pruning and focused repair.

Result: The team proposed 'hedom5', combining lazy gain-based greedy heuristic methods, a pruning process, and local improvements, which they submitted as their best-performing solver.

Conclusion: The proposed 'hedom5' solver demonstrates a practical and efficient approach to solve Minimum Dominating Set problems using innovative heuristic techniques, showing promise in heuristic competitions like PACE 2025.

Abstract: In this paper we present \texttt{twin\_width\_fmi}'s solver for the heuristic track of PACE's 2025 competition on Minimum Dominating Set.
  As a baseline, we implement \texttt{greedy-ln}, a standard greedy dominating-set heuristic that repeatedly selects the vertex that newly dominates the largest number of currently undominated vertices. We then use this greedy solution as the starting point for a simulated annealing local search: we attempt vertex removals and exchanges and accept worsening moves with decaying probability, in order to escape local minima while preserving domination.
  Our best-performing component, which we ultimately submitted, is \texttt{hedom5}. The design of \texttt{hedom5} is inspired by recent iterative-greedy style domination heuristics~\cite{IterativeGreedy22} that alternate between constructive steps, pruning, and focused repair rather than relying on a single pass. In \texttt{hedom5}, the input graph is first stored in a compact CSR structure and simplified using fast reductions such as forcing neighbors of leaves and handling isolates. We then run a lazy gain-based greedy stage using a priority queue: each candidate vertex is scored by how many currently undominated vertices its closed neighborhood would newly dominate, and scores are only recomputed when necessary. After this constructive phase, we perform an aggressive backward pruning pass that iterates over the chosen dominators in reverse insertion order and deletes any vertex whose closed neighborhood is still fully dominated by the remaining set. Finally, we run a budgeted 1-swap local improvement step that attempts to replace a dominator by an alternative vertex that covers all of its uniquely covered vertices, thereby reducing the size of the dominating set. A brief safety patch at the end guarantees full domination.

</details>


### [486] [Why Should the Server Do It All?: A Scalable, Versatile, and Model-Agnostic Framework for Server-Light DNN Inference over Massively Distributed Clients via Training-Free Intermediate Feature Compression](https://arxiv.org/abs/2511.11608)
*Mingyu Sung,Suhwan Im,Daeho Bang,Il-Min Kim,Sangseok Yun,Jae-Mo Kang*

Main category: cs.DC

TL;DR: The paper introduces SLICER, a framework designed to optimize edge-cloud model partitioning by compressing intermediate features to reduce communication and server strain without modifying models.


<details>
  <summary>Details</summary>
Motivation: Efficient distributed inference systems are needed for deep neural networks (DNNs), particularly for latency and energy reduction in edge-cloud systems. Existing methods with static split points are inefficient, especially during autoregressive large language model (AR LLM) inferences, which create bulky data transfer issues.

Method: SLICER is a retraining-free plug-and-play framework that employs three techniques: asymmetric top-K filtering (to sparsify activations), magnitude-splitting (to group activations), and adaptive bit quantization (to compress data under a distortion budget).

Result: SLICER demonstrates significant improvements across vision and LLM workloads, reducing uplink data by up to 10x and server GPU usage by up to 4.4x, while maintaining task quality within ~0-3 percentage points of baseline.

Conclusion: SLICER effectively improves edge-cloud model partitioning efficiency by lowering data transfer and server load. It scales well in multi-device and AR LLM setups and is compatible with existing models, enabling scalable, low-latency distributed inference.

Abstract: Modern DNNs often rely on edge-cloud model partitioning (MP), but widely used schemes fix shallow, static split points that underutilize edge compute and concentrate latency and energy on the server. The problem is exacerbated in autoregressive (AR) LLM inference, where per-token forward passes repeatedly generate bulky intermediate features (IFs). We introduce SLICER, a retraining-free, architecture-agnostic framework that compresses IFs to reduce both communication and server load in split computing. SLICER combines (i) asymmetric top-K filtering (ATKF) to sparsify low-magnitude activations, (ii) magnitude-splitting (MS) to group the remaining non-zeros into equal-cardinality blocks, and (iii) adaptive bit quantization (ABQ) that selects per-block bitwidths under a distortion budget. Across standard vision and LLM workloads (e.g., ImageNet/COCO; HellaSwag, PIQA, ARC-E/C, GSM8K, HumanEval), SLICER reduces uplink volume by up to 10x and server GPU time by up to 4.4x, while keeping task quality within ~0-3 pp of baseline. In multi-device settings and AR LLMs, SLICER scales by shifting meaningful compute to the edge and lowering bits-per-token and server time per token, stabilizing per-step traffic. The codec attaches to off-the-shelf models without retraining or architectural changes, offering a plug-and-play path to scalable, low-latency distributed inference. Code is provided in the supplementary material.

</details>


### [487] [Evaluating Large Language Models for Workload Mapping and Scheduling in Heterogeneous HPC Systems](https://arxiv.org/abs/2511.11612)
*Aasish Kumar Sharma,Julian Kunkel*

Main category: cs.DC

TL;DR: The study assesses reasoning in LLMs for HPC optimization problems, showing partial success in achieving optimal outcomes with constraints.


<details>
  <summary>Details</summary>
Motivation: To understand the ability of LLMs to perform structured, constraint-based optimization tasks using natural language descriptions.

Method: Twenty-one publicly available LLMs were tested on HPC scheduling tasks using textual descriptions, aiming to achieve optimal schedules with detailed reasoning.

Result: Three models achieved an optimal makespan, twelve were near-optimal, and six exhibited errors. About half maintained strict constraint adherence, showcasing strong interpretability but inconsistent precision.

Conclusion: LLMs show promise as explainable aids in decision-support tasks but are not yet reliable autonomous problem solvers due to logical and constraint adherence issues.

Abstract: Large language models (LLMs) are increasingly explored for their reasoning capabilities, yet their ability to perform structured, constraint-based optimization from natural language remains insufficiently understood. This study evaluates twenty-one publicly available LLMs on a representative heterogeneous high-performance computing (HPC) workload mapping and scheduling problem. Each model received the same textual description of system nodes, task requirements, and scheduling constraints, and was required to assign tasks to nodes, compute the total makespan, and explain its reasoning. A manually derived analytical optimum of nine hours and twenty seconds served as the ground truth reference. Three models exactly reproduced the analytical optimum while satisfying all constraints, twelve achieved near-optimal results within two minutes of the reference, and six produced suboptimal schedules with arithmetic or dependency errors. All models generated feasible task-to-node mappings, though only about half maintained strict constraint adherence. Nineteen models produced partially executable verification code, and eighteen provided coherent step-by-step reasoning, demonstrating strong interpretability even when logical errors occurred. Overall, the results define the current capability boundary of LLM reasoning in combinatorial optimization: leading models can reconstruct optimal schedules directly from natural language, but most still struggle with precise timing, data transfer arithmetic, and dependency enforcement. These findings highlight the potential of LLMs as explainable co-pilots for optimization and decision-support tasks rather than autonomous solvers.

</details>


### [488] [Beyond the GPU: The Strategic Role of FPGAs in the Next Wave of AI](https://arxiv.org/abs/2511.11614)
*Arturo Urías Jiménez*

Main category: cs.DC

TL;DR: The paper explores how FPGAs are evolving as an alternative to GPUs for AI acceleration due to their reconfigurable capabilities and energy efficiency.


<details>
  <summary>Details</summary>
Motivation: The need for lower latency, energy efficiency, and fine-grained control in AI acceleration highlights the limitations of fixed architectures like GPUs and leads to exploration of FPGAs.

Method: Utilizing FPGAs' ability to map AI algorithms directly into device logic, implement parallel pipelines, reconfigure in the field, and integrate with embedded processors.

Result: FPGAs offer deterministic timing, reduced power consumption, improved privacy, latency reduction near sensors, and free GPUs from specialized data center tasks.

Conclusion: FPGAs emerge as strategic platforms for predictable, customizable AI workloads, supported by advances in partial reconfiguration and hardware-algorithm co-design.

Abstract: AI acceleration has been dominated by GPUs, but the growing need for lower latency, energy efficiency, and fine-grained hardware control exposes the limits of fixed architectures. In this context, Field-Programmable Gate Arrays (FPGAs) emerge as a reconfigurable platform that allows mapping AI algorithms directly into device logic. Their ability to implement parallel pipelines for convolutions, attention mechanisms, and post-processing with deterministic timing and reduced power consumption makes them a strategic option for workloads that demand predictable performance and deep customization.
  Unlike CPUs and GPUs, whose architecture is immutable, an FPGA can be reconfigured in the field to adapt its physical structure to a specific model, integrate as a SoC with embedded processors, and run inference near the sensor without sending raw data to the cloud. This reduces latency and required bandwidth, improves privacy, and frees GPUs from specialized tasks in data centers. Partial reconfiguration and compilation flows from AI frameworks are shortening the path from prototype to deployment, enabling hardware--algorithm co-design.

</details>


### [489] [AnchorTP: Resilient LLM Inference with State-Preserving Elastic Tensor Parallelism](https://arxiv.org/abs/2511.11617)
*Wendong Xu,Chujie Chen,He Xiao,Kuan Li,Jing Xiong,Chen Zhang,Wenyong Zhou,Chaofan Tao,Yang Bai,Bei Yu,Ngai Wong*

Main category: cs.DC

TL;DR: AnchorTP provides a state-preserving elastic TP framework with fast recovery for LLM inference services by enabling unequal-width Elastic Tensor Parallelism and minimizing data movement during GPU failures.


<details>
  <summary>Details</summary>
Motivation: LLM inference services need high availability and low latency, but current multi-GPU Tensor Parallelism setups are vulnerable to single-GPU failures.

Method: AnchorTP introduces Elastic Tensor Parallelism to distribute tasks unevenly across GPUs and uses a decoupled daemon to preserve model parameters and KV caches in memory.

Result: AnchorTP drastically reduces recovery times with up to 11x faster Time to First Success (TFS) and up to 59% faster Time to Peak (TTP) during GPU failures.

Conclusion: AnchorTP ensures resilience in LLM inference services by minimizing downtime and improving recovery speed with efficient data handling and parallel scheduling.

Abstract: Large Language Model (LLM) inference services demand exceptionally high availability and low latency, yet multi-GPU Tensor Parallelism (TP) makes them vulnerable to single-GPU failures. We present AnchorTP, a state-preserving elastic TP framework for fast recovery. It (i) enables Elastic Tensor Parallelism (ETP) with unequal-width partitioning over any number of GPUs and compatibility with Mixture-of-Experts (MoE), and (ii) preserves model parameters and KV caches in GPU memory via a daemon decoupled from the inference process. To minimize downtime, we propose a bandwidth-aware planner based on a Continuous Minimal Migration (CMM) algorithm that minimizes reload bytes under a byte-cost dominance assumption, and an execution scheduler that pipelines P2P transfers with reloads. These components jointly restore service quickly with minimal data movement and without changing service interfaces. In typical failure scenarios, AnchorTP reduces Time to First Success (TFS) by up to 11x and Time to Peak (TTP) by up to 59% versus restart-and-reload.

</details>


### [490] [DIAP: A Decentralized Agent Identity Protocol with Zero-Knowledge Proofs and a Hybrid P2P Stack](https://arxiv.org/abs/2511.11619)
*Yuanjie Liu,Wenpeng Xing,Ye Zhou,Gaowei Chang,Changting Lin,Meng Han*

Main category: cs.DC

TL;DR: DIAP introduces a decentralized, privacy-preserving communication protocol for autonomous agents utilizing zero-knowledge proofs, IPFS, and a Rust SDK.


<details>
  <summary>Details</summary>
Motivation: Address the lack of decentralized, verifiable, and privacy-preserving communication protocols for autonomous agents.

Method: Developed DIAP, utilizing IPFS/IPNS content identifiers, zero-knowledge proofs, and a Rust SDK integrating Noir and peer-to-peer technologies.

Result: Established DIAP, ensuring trustless agent identity and communication with instant identity proofs and interoperability.

Conclusion: DIAP provides a high-performance framework for decentralized agent ecosystems and agent-to-agent communication.

Abstract: The absence of a fully decentralized, verifiable, and privacy-preserving communication protocol for autonomous agents remains a core challenge in decentralized computing. Existing systems often rely on centralized intermediaries, which reintroduce trust bottlenecks, or lack decentralized identity-resolution mechanisms, limiting persistence and cross-network interoperability.
  We propose the Decentralized Interstellar Agent Protocol (DIAP), a novel framework for agent identity and communication that enables persistent, verifiable, and trustless interoperability in fully decentralized environments. DIAP binds an agent's identity to an immutable IPFS or IPNS content identifier and uses zero-knowledge proofs (ZKP) to dynamically and statelessly prove ownership, removing the need for record updates.
  We present a Rust SDK that integrates Noir (for zero-knowledge proofs), DID-Key, IPFS, and a hybrid peer-to-peer stack combining Libp2p GossipSub for discovery and Iroh for high-performance, QUIC based data exchange. DIAP introduces a zero-dependency ZKP deployment model through a universal proof manager and compile-time build script that embeds a precompiled Noir circuit, eliminating the need for external ZKP toolchains. This enables instant, verifiable, and privacy-preserving identity proofs.
  This work establishes a practical, high-performance foundation for next-generation autonomous agent ecosystems and agent-to-agent (A to A) economies.

</details>


### [491] [AIvailable: A Software-Defined Architecture for LLM-as-a-Service on Heterogeneous and Legacy GPUs](https://arxiv.org/abs/2511.11621)
*Pedro Antunes,Ana Rita Ortigoso,Gabriel Vieira,Daniel Fuentes,Luís Frazão,Nuno Costa,António Pereira*

Main category: cs.DC

TL;DR: AIvailable introduces a cost-effective platform for running large language models on heterogeneous and legacy GPU systems, focusing on VRAM optimization and scalability.


<details>
  <summary>Details</summary>
Motivation: Address the need for scalable and cost-efficient LLM infrastructure in resource-constrained environments, like academic and private organizations.

Method: AIvailable uses a software-defined orchestration platform with four primary components: Client Interface, Service Frontend, SDAI Controller, and Service Backend for seamless LLM deployment across diverse GPU architectures.

Result: The platform achieves efficient, resilient GPU utilization for heterogeneous hardware without CPU fallback, improving access to generative AI resources.

Conclusion: AIvailable democratizes access to generative AI by leveraging legacy GPUs, making large language models feasible for resource-constrained settings.

Abstract: The rise of Large Language Models (LLM) has increased the need for scalable, high-performance inference systems, yet most existing frameworks assume homogeneous, resource-rich hardware, often unrealistic in academic, or resource-constrained settings. We introduce AIvailable, a low-cost, highly available LLM-as-a-Service (LLMaaS) platform, that uses a software-defined approach for running LLMs across heterogeneous and legacy GPU nodes, including NVIDIA and AMD devices, with a focus on fully utilizing each node's VRAM. AIvailable operates as a fully GPU-accelerated inference without CPU fallbacks, featuring a unified client interface that allows seamless interaction with all deployed LLMs through a single logical unit. The architecture comprises four main components: the Client Interface for user access, the Service Frontend for secure request routing and load balancing, the SDAI Controller for orchestration, deployment, and monitoring, and the Service Backend of heterogeneous GPU nodes executing workloads. By abstracting GPU-specific details and providing dynamic, VRAM-aware allocation and reallocation of models, AIvailable ensures efficient use of resources and resilience against failures or workload fluctuations. Targeting academic labs, private companies, and other constrained organizations, it supports diverse open LLMs helping democratize generative AI through the repurposing of legacy GPUs.

</details>


### [492] [Characterizing and Understanding Energy Footprint and Efficiency of Small Language Model on Edges](https://arxiv.org/abs/2511.11624)
*Md Romyull Islam,Bobin Deng,Nobel Dhar,Tu N. Nguyen,Selena He,Yong Shi,Kun Suo*

Main category: cs.DC

TL;DR: This paper assesses the power efficiency of five small language models (SLMs) deployed on edge devices.


<details>
  <summary>Details</summary>
Motivation: To understand the tradeoffs of deploying smaller language models on edge devices where computing and energy constraints pose challenges.

Method: The study evaluates the power efficiency of five SLMs across multiple hardware setups, including Raspberry Pi 5 and GPU/CPU configurations on Jetson Nano and Jetson Orin Nano.

Result: Jetson Orin Nano with GPU acceleration achieves the highest energy-to-performance ratio. Llama 3.2 strikes the best balance between accuracy and power efficiency, while TinyLlama is suitable for low-power setups but sacrifices accuracy.

Conclusion: GPU acceleration, memory bandwidth, and model architecture are critical in optimizing inference energy efficiency, providing guidance for deploying SLMs in constrained environments.

Abstract: Cloud-based large language models (LLMs) and their variants have significantly influenced real-world applications. Deploying smaller models (i.e., small language models (SLMs)) on edge devices offers additional advantages, such as reduced latency and independence from network connectivity. However, edge devices' limited computing resources and constrained energy budgets challenge efficient deployment. This study evaluates the power efficiency of five representative SLMs - Llama 3.2, Phi-3 Mini, TinyLlama, and Gemma 2 on Raspberry Pi 5, Jetson Nano, and Jetson Orin Nano (CPU and GPU configurations). Results show that Jetson Orin Nano with GPU acceleration achieves the highest energy-to-performance ratio, significantly outperforming CPU-based setups. Llama 3.2 provides the best balance of accuracy and power efficiency, while TinyLlama is well-suited for low-power environments at the cost of reduced accuracy. In contrast, Phi-3 Mini consumes the most energy despite its high accuracy. In addition, GPU acceleration, memory bandwidth, and model architecture are key in optimizing inference energy efficiency. Our empirical analysis offers practical insights for AI, smart systems, and mobile ad-hoc platforms to leverage tradeoffs from accuracy, inference latency, and power efficiency in energy-constrained environments.

</details>


### [493] [Mixture-of-Schedulers: An Adaptive Scheduling Agent as a Learned Router for Expert Policies](https://arxiv.org/abs/2511.11628)
*Xinbo Wang,Shian Jia,Ziyang Huang,Jing Cao,Mingli Song*

Main category: cs.DC

TL;DR: Current operating system schedulers struggle to adapt to dynamic workloads. Adaptive Scheduling Agent (ASA) dynamically selects optimal scheduling policies using machine learning and runtime workload recognition. It outperforms default Linux schedulers in system performance tests.


<details>
  <summary>Details</summary>
Motivation: Static single-policy schedulers in modern operating systems fail to deliver optimal performance due to diverse workloads and hardware heterogeneity.

Method: ASA uses an offline process to train a machine learning model for workload recognition. At runtime, it employs a time-weighted voting algorithm and predefined mappings to dynamically switch schedulers optimally using Linux's sched_ext framework.

Result: ASA achieves better performance than the default Linux scheduler in 86.4% of scenarios and ranks among the top three in 78.6% of cases.

Conclusion: The Adaptive Scheduling Agent (ASA) represents a promising approach to achieving intelligent and adaptive scheduling in operating systems, improving fairness, throughput, and latency across dynamic workloads.

Abstract: Modern operating system schedulers employ a single, static policy, which struggles to deliver optimal performance across the diverse and dynamic workloads of contemporary systems. This "one-policy-fits-all" approach leads to significant compromises in fairness, throughput, and latency, particularly with the rise of heterogeneous hardware and varied application architectures.
  This paper proposes a new paradigm: dynamically selecting the optimal policy from a portfolio of specialized schedulers rather than designing a single, monolithic one. We present the Adaptive Scheduling Agent (ASA), a lightweight framework that intelligently matches workloads to the most suitable "expert" scheduling policy at runtime. ASA's core is a novel, low-overhead offline/online approach. First, an offline process trains a universal, hardware-agnostic machine learning model to recognize abstract workload patterns from system behaviors. Second, at runtime, ASA continually processes the model's predictions using a time-weighted probability voting algorithm to identify the workload, then makes a scheduling decision by consulting a pre-configured, machine-specific mapping table to switch to the optimal scheduler via Linux's sched_ext framework. This decoupled architecture allows ASA to adapt to new hardware platforms rapidly without expensive retraining of the core recognition model.
  Our evaluation, based on a novel benchmark focused on user-experience metrics, demonstrates that ASA consistently outperforms the default Linux scheduler (EEVDF), achieving superior results in 86.4% of test scenarios. Furthermore, ASA's selections are near-optimal, ranking among the top three schedulers in 78.6% of all scenarios. This validates our approach as a practical path toward more intelligent, adaptive, and responsive operating system schedulers.

</details>


### [494] [Exploring Parallelism in FPGA-Based Accelerators for Machine Learning Applications](https://arxiv.org/abs/2511.11640)
*Sed Centeno,Christopher Sprague,Arnab A Purkayastha,Ray Simar,Neeraj Magotra*

Main category: cs.DC

TL;DR: The paper explores speculative backpropagation, which overlaps forward and backward passes, improving training speed for neural networks using OpenMP and hardware acceleration.


<details>
  <summary>Details</summary>
Motivation: To reduce neural network training time by leveraging speculative weight updates and parallel execution of forward and backward passes.

Method: Implemented speculative backpropagation on the MNIST dataset, utilized OpenMP for parallel processing, and evaluated CPU-based results for hardware synthesis.

Result: Achieved a 24% speedup in training time and 35% in step execution time while maintaining accuracy within 3-4% of the baseline.

Conclusion: Speculative backpropagation demonstrates potential for significant training acceleration and hardware integration, balancing speed and accuracy effectively.

Abstract: Speculative backpropagation has emerged as a promising technique to accelerate the training of neural networks by overlapping the forward and backward passes. Leveraging speculative weight updates when error gradients fall within a specific threshold reduces training time without substantially compromising accuracy. In this work, we implement speculative backpropagation on the MNIST dataset using OpenMP as the parallel programming platform. OpenMP's multi-threading capabilities enable simultaneous execution of forward and speculative backpropagation steps, significantly improving training speed. The application is planned for synthesis on a state-of-the-art FPGA to demonstrate its potential for hardware acceleration. Our CPU-based experimental results demonstrate that speculative backpropagation achieves a maximum speedup of 24% in execution time when using a threshold of 0.25, and accuracy remaining within 3-4% of the baseline across various epochs. Additionally, when comparing individual step execution time, speculative backpropagation yields a maximum speedup of 35% over the baseline, demonstrating the effectiveness of overlapping forward and backward passes.

</details>


### [495] [HeteroSTA: A CPU-GPU Heterogeneous Static Timing Analysis Engine with Holistic Industrial Design Support](https://arxiv.org/abs/2511.11660)
*Zizheng Guo,Haichuan Liu,Xizhe Shi,Shenglu Hua,Zuodong Zhang,Chunyuan Zhao,Runsheng Wang,Yibo Lin*

Main category: cs.DC

TL;DR: HeteroSTA is a CPU-GPU heterogeneous timing analysis engine offering delay models, robust industry format support, and GPU-acceleration, aiming for accuracy and efficiency in EDA workflows.


<details>
  <summary>Details</summary>
Motivation: To create an efficient and versatile timing analysis engine that supports advanced models, industry standards, and GPU-accelerated computations.

Method: Developed HeteroSTA, leveraging GPU acceleration and robust compatibility with industry standards like .sdc, offering versatile and efficient timing analyses for industry and academic uses.

Result: Achieved remarkable runtime improvement and comparable analysis quality across different applications, demonstrating feasibility and industry relevance.

Conclusion: HeteroSTA provides a robust, scalable, and efficient solution for timing analysis, with wide-ranging compatibility and advanced features for industry and academic integration.

Abstract: We introduce in this paper, HeteroSTA, the first CPU-GPU heterogeneous timing analysis engine that efficiently supports: (1) a set of delay calculation models providing versatile accuracy-speed choices without relying on an external golden tool, (2) robust support for industry formats, including especially the .sdc constraints containing all common timing exceptions, clock domains, and case analysis modes, and (3) end-to-end GPU-acceleration for both graph-based and path-based timing queries, all exposed as a zero-overhead flattened heterogeneous application programming interface (API). HeteroSTA is publicly available with both a standalone binary executable and an embeddable shared library targeting ubiquitous academic and industry applications. Example use cases as a standalone tool, a timing-driven DREAMPlace 4.0 integration, and a timing-driven global routing integration have all demonstrated remarkable runtime speed-up and comparable quality.

</details>


### [496] [Range Asymmetric Numeral Systems-Based Lightweight Intermediate Feature Compression for Split Computing of Deep Neural Networks](https://arxiv.org/abs/2511.11664)
*Mingyu Sung,Suhwan Im,Vikas Palakonda,Jae-Mo Kang*

Main category: cs.DC

TL;DR: The paper introduces a lightweight compression framework to improve split computing by significantly reducing the communication overhead between edge devices and cloud servers, while maintaining near-baseline model accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper aims to tackle the communication bottleneck in split computing, especially between edge devices and cloud servers, which limits the deployment of sophisticated AI systems.

Method: The framework integrates Range Asymmetric Numeral Systems (rANS) encoding, asymmetric integer quantization, and sparse tensor representation to compress intermediate features effectively. It includes GPU implementation and a theoretical model for optimizing tensor reshaping dimensions.

Result: Experiments show that the proposed method preserves near-baseline accuracy across diverse benchmarks for computer vision and natural language processing tasks, while achieving significant bandwidth reduction.

Conclusion: This framework ensures effective compression for split computing in bandwidth-limited environments without sacrificing model performance, supporting broader applicability in AI deployment.

Abstract: Split computing distributes deep neural network inference between resource-constrained edge devices and cloud servers but faces significant communication bottlenecks when transmitting intermediate features. To this end, in this paper, we propose a novel lightweight compression framework that leverages Range Asymmetric Numeral Systems (rANS) encoding with asymmetric integer quantization and sparse tensor representation to reduce transmission overhead dramatically. Specifically, our approach combines asymmetric integer quantization with a sparse representation technique, eliminating the need for complex probability modeling or network modifications. The key contributions include: (1) a distribution-agnostic compression pipeline that exploits inherent tensor sparsity to achieve bandwidth reduction with minimal computational overhead; (2) an approximate theoretical model that optimizes tensor reshaping dimensions to maximize compression efficiency; and (3) a GPU-accelerated implementation with sub-millisecond encoding/decoding latency. Extensive evaluations across diverse neural architectures (ResNet, VGG16, MobileNetV2, SwinT, DenseNet121, EfficientNetB0) demonstrate that the proposed framework consistently maintains near-baseline accuracy across CIFAR100 and ImageNet benchmarks. Moreover, we validated the framework's effectiveness on advanced natural language processing tasks by employing Llama2 7B and 13B on standard benchmarks such as MMLU, HellaSwag, ARC, PIQA, Winogrande, BoolQ, and OpenBookQA, demonstrating its broad applicability beyond computer vision. Furthermore, this method addresses a fundamental bottleneck in deploying sophisticated artificial intelligence systems in bandwidth-constrained environments without compromising model performance.

</details>


### [497] [OSGym: Super-Scalable Distributed Data Engine for Generalizable Computer Agents](https://arxiv.org/abs/2511.11672)
*Zengyi Qin,Jinyuan Chen,Yunze Man,Shengcao Cao,Ziqi Pang,Zhuoyuan Wang,Xin Sun,Gen Lin,Han Fang,Ling Zhu,Zixin Xie,Zibu Wei,Tianshu Ran,Haoran Geng,Xander Wu,Zachary Bright,Qizhen Sun,Rui Wang,Yuyang Cai,Song Wang,Jiace Zhao,Han Cao,Yeyang Zhou,Tianrui Liu,Ray Pan,Chongye Yang,Xiang Ren,Bo Zhang,Yutong Ban,Jitendra Malik,Brian Anthony,Pieter Abbeel*

Main category: cs.DC

TL;DR: OSGym is a scalable data engine enabling training for diverse computer-related tasks by running over 1000 OS replicas efficiently and economically.


<details>
  <summary>Details</summary>
Motivation: To address scalability, generality, and cost issues in training intelligent agents for various computer-related tasks.

Method: Development of OSGym, a distributed engine that scales OS replicas, supports diverse tasks, and ensures economic feasibility for training agents.

Result: OSGym achieves up to 1420 multi-turn trajectories per minute, supports various model training algorithms, and demonstrates superior performance over state-of-the-art baselines.

Conclusion: OSGym enhances scalability, generality, and economic accessibility in model training, advancing the field of intelligent agent research.

Abstract: We introduce OSGym, a super-scalable distributed data engine for training agents across diverse computer-related tasks. OSGym efficiently scales to over a thousand operating system (OS) replicas at an academia-affordable cost, serving as dynamic runtime environments for intelligent agents. It offers three key advantages. (1) Scalability: Despite the intensive resource requirements of running multiple OS replicas, OSGym parallelizes over a thousand instances while maintaining operational efficiency under constrained resources, generating up to 1420 multi-turn trajectories per minute. (2) Generality and Customizability: OSGym supports a broad spectrum of tasks that run on OS platforms, including tool use, browser interactions, software engineering, and office applications, with flexible support for diverse model training algorithms. (3) Economic Viability: OSGym operates at only 0.2-0.3 USD per day per OS replica using accessible on-demand compute providers. It is fully open-source and freely available for both research and commercial use. Experiments show that OSGym enables comprehensive data collection, supervised fine-tuning, and reinforcement learning pipelines for computer agents. Models trained with OSGym outperform state-of-the-art baselines, demonstrating its potential to advance scalability and universality in future agent research.

</details>


### [498] [A Structure-Agnostic Co-Tuning Framework for LLMs and SLMs in Cloud-Edge Systems](https://arxiv.org/abs/2511.11678)
*Yuze Liu,Yunhan Wang,Tiehua Zhang,Zhishu Shen,Cheng Peng,Libing Wu,Feng Xia,Jiong Jin*

Main category: cs.DC

TL;DR: The paper introduces Co-PLMs, a co-tuning framework for collaborative training of LLMs and SLMs, addressing cross-domain deployment and heterogeneity challenges. Co-PLMs improves performance in real-time applications.


<details>
  <summary>Details</summary>
Motivation: The paper addresses inefficiencies in processing large language model workloads in cloud-edge consortia, alongside concerns of data privacy, cross-domain deployment, and structural heterogeneity of small language models.

Method: Co-PLMs employs structure-agnostic mutual learning using distilled proxy models to facilitate knowledge exchange between large and small language models, enhancing collaborative training.

Result: Experimental results demonstrate that Co-PLMs outperforms existing methods, achieving an average improvement of 5.38% in Rouge-L and 4.88% in EM.

Conclusion: Co-PLMs provides an effective solution for improving inference performance in LLM-SLM consortia, showing promising results in overcoming heterogeneity and deployment challenges.

Abstract: The surge in intelligent applications driven by large language models (LLMs) has made it increasingly difficult for bandwidth-limited cloud servers to process extensive LLM workloads in real time without compromising user data privacy. To solve these problems, recent research has focused on constructing cloud-edge consortia that integrate server-based LLM with small language models (SLMs) on mobile edge devices. Furthermore, designing collaborative training mechanisms within such consortia to enhance inference performance has emerged as a promising research direction. However, the cross-domain deployment of SLMs, coupled with structural heterogeneity in SLMs architectures, poses significant challenges to enhancing model performance. To this end, we propose Co-PLMs, a novel co-tuning framework for collaborative training of large and small language models, which integrates the process of structure-agnostic mutual learning to realize knowledge exchange between the heterogeneous language models. This framework employs distilled proxy models (DPMs) as bridges to enable collaborative training between the heterogeneous server-based LLM and on-device SLMs, while preserving the domain-specific insights of each device. The experimental results show that Co-PLMs outperform state-of-the-art methods, achieving average increases of 5.38% in Rouge-L and 4.88% in EM.

</details>


### [499] [ECCENTRIC: Edge-Cloud Collaboration Framework for Distributed Inference Using Knowledge Adaptation](https://arxiv.org/abs/2511.11719)
*Mohammad Mahdi Kamani,Zhongwei Cheng,Lin Chen*

Main category: cs.DC

TL;DR: Edge AI relies on cloud inference systems due to device constraints. The proposed "Eccentric" framework optimizes trade-offs between computation, communication, and performance. It uses adaptation techniques to reduce costs while maintaining high performance.


<details>
  <summary>Details</summary>
Motivation: The growing role of edge AI necessitates reliance on cloud systems, raising challenges in computation, communication, and performance trade-offs.

Method: The study presents "Eccentric"—an adaptation-based framework to compress edge-cloud inference systems, reducing resource usage during inference.

Result: Empirical evaluations demonstrate "Eccentric's" effectiveness in lowering computation and communication costs while maintaining strong task performance.

Conclusion: The "Eccentric" framework offers an efficient solution for edge-cloud AI systems by optimizing inferencing in terms of communication, computation, and performance.

Abstract: The massive growth in the utilization of edge AI has made the applications of machine learning models ubiquitous in different domains. Despite the computation and communication efficiency of these systems, due to limited computation resources on edge devices, relying on more computationally rich systems on the cloud side is inevitable in most cases. Cloud inference systems can achieve the best performance while the computation and communication cost is dramatically increasing by the expansion of a number of edge devices relying on these systems. Hence, there is a trade-off between the computation, communication, and performance of these systems. In this paper, we propose a novel framework, dubbed as Eccentric that learns models with different levels of trade-offs between these conflicting objectives. This framework, based on an adaptation of knowledge from the edge model to the cloud one, reduces the computation and communication costs of the system during inference while achieving the best performance possible. The Eccentric framework can be considered as a new form of compression method suited for edge-cloud inference systems to reduce both computation and communication costs. Empirical studies on classification and object detection tasks corroborate the efficacy of this framework.

</details>


### [500] [A Meta-Heuristic Load Balancer for Cloud Computing Systems](https://arxiv.org/abs/2511.11721)
*Leszek Sliwko,Vladimir Getov*

Main category: cs.DC

TL;DR: The paper proposes a cost-efficient strategy using a novel genetic algorithm for service allocation in cloud systems, preventing node overload and ensuring stability.


<details>
  <summary>Details</summary>
Motivation: To optimize service allocation in cloud systems by minimizing costs, avoiding overloading, and ensuring overall system stability.

Method: Developed an abstract model of cloud resource utilization and proposed a novel genetic algorithm seeded with outputs from other meta-heuristic algorithms. Demonstrated with a prototype load balancer and conducted experiments.

Result: Experimental results validate the efficiency of the proposed meta-heuristic load balancer and the genetic algorithm in achieving cost-minimized service allocations.

Conclusion: The novel approach is effective at reducing costs, avoiding node overload, and maintaining cloud system stability through improved service allocation strategies.

Abstract: This paper presents a strategy to allocate services on a Cloud system without overloading nodes and maintaining the system stability with minimum cost. We specify an abstract model of cloud resources utilization, including multiple types of resources as well as considerations for the service migration costs. A prototype meta-heuristic load balancer is demonstrated and experimental results are presented and discussed. We also propose a novel genetic algorithm, where population is seeded with the outputs of other meta-heuristic algorithms.

</details>


### [501] [Harli: Harvest Underutilized Resources in LLM Serving with Finetuning Tasks](https://arxiv.org/abs/2511.11729)
*Ao Xu,Han Zhao,Weihao Cui,Quan Chen,Yukang Chen,Shulai Zhang,Shuang Chen,Jiemin Jiang,Zhibin Yu,Minyi Guo*

Main category: cs.DC

TL;DR: This paper presents Harli, a system to improve GPU utilization during LLM inference by co-locating parameter-efficient finetuning tasks and addressing memory and latency challenges. It boosts throughput while maintaining QoS.


<details>
  <summary>Details</summary>
Motivation: Address low GPU utilization in existing LLM serving systems caused by memory-bound decode phases and insufficient batching under dynamic workloads.

Method: Introduce Harli, which co-locates parameter-efficient finetuning with LLM decode tasks and employs a unified memory allocator, a two-stage latency predictor, and a QoS-guaranteed scheduler.

Result: Harli achieves an average 46.2% improvement in finetune throughput (up to 92.0%) over current systems, while maintaining strict QoS requirements for LLM inference.

Conclusion: Harli effectively enhances GPU utilization by combining inference and finetuning tasks, offering significant throughput improvements and preserving quality of service.

Abstract: Large language models (LLMs) are increasingly deployed under the Model-as-a-Service (MaaS) paradigm. To meet stringent quality-of-service (QoS) requirements, existing LLM serving systems disaggregate the prefill and decode phases of inference. However, decode instances often experience low GPU utilization due to their memory-bound nature and insufficient batching in dynamic workloads, leaving compute resources underutilized.
  We introduce Harli, a serving system that improves GPU utilization by co-locating parameter-efficient finetuning (PEFT) tasks with LLM decode instances. PEFT tasks are compute-bound and memory-efficient, making them ideal candidates for safe co-location. Specifically, Harli addresses key challenges--limited memory and unpredictable interference--using three components: a unified memory allocator for runtime memory reuse, a two-stage latency predictor for decode latency modeling, and a QoS-guaranteed throughput-maximizing scheduler for throughput maximization. Experimental results show that Harli improves the finetune throughput by 46.2% on average (up to 92.0%) over state-of-the-art serving systems, while maintaining strict QoS guarantees for inference decode.

</details>


### [502] [Speculative Decoding in Decentralized LLM Inference: Turning Communication Latency into Computation Throughput](https://arxiv.org/abs/2511.11733)
*Jingwei Song,Wanyi Chen,Xinyuan Song,Max,Chris Tong,Gufeng Chen,Tianyi Zhao,Eric Yang,Bill Shi,Lynn Ai*

Main category: cs.DC

TL;DR: The paper presents Decentralized Speculative Decoding (DSD), a framework that speeds up distributed large language model inference by verifying candidate tokens in parallel, reducing communication overhead and improving overall performance without altering models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance LLM inference in decentralized environments where network latency is a limiting factor by turning communication delays into useful computation.

Method: DSD employs a plug-and-play framework to parallelize token verification across distributed nodes and introduces an adaptive strategy for adjusting token acceptance thresholds based on their semantic significance.

Result: The framework achieves up to 2.56x speedup on HumanEval and 2.59x on GSM8K datasets, outperforming the Eagle3 baseline while maintaining accuracy.

Conclusion: DSD demonstrates that speculative decoding can be effectively adapted for decentralized inference, converting latency into throughput and enabling more efficient distributed LLM inference without requiring retraining or architectural changes.

Abstract: Speculative decoding accelerates large language model (LLM) inference by using a lightweight draft model to propose tokens that are later verified by a stronger target model. While effective in centralized systems, its behavior in decentralized settings, where network latency often dominates compute, remains under-characterized. We present Decentralized Speculative Decoding (DSD), a plug-and-play framework for decentralized inference that turns communication delay into useful computation by verifying multiple candidate tokens in parallel across distributed nodes. We further introduce an adaptive speculative verification strategy that adjusts acceptance thresholds by token-level semantic importance, delivering an additional 15% to 20% end-to-end speedup without retraining. In theory, DSD reduces cross-node communication cost by approximately (N-1)t1(k-1)/k, where t1 is per-link latency and k is the average number of tokens accepted per round. In practice, DSD achieves up to 2.56x speedup on HumanEval and 2.59x on GSM8K, surpassing the Eagle3 baseline while preserving accuracy. These results show that adapting speculative decoding for decentralized execution provides a system-level optimization that converts network stalls into throughput, enabling faster distributed LLM inference with no model retraining or architectural changes.

</details>


### [503] [Noise-Aware Optimization in Nominally Identical Manufacturing and Measuring Systems for High-Throughput Parallel Workflows](https://arxiv.org/abs/2511.11739)
*Christina Schenk,Miguel Hernández-del-Valle,Luis Calero-Lumbreras,Marcus Noack,Maciej Haranczyk*

Main category: cs.DC

TL;DR: The paper discusses a noise-aware algorithm to manage variability in device performance, particularly in high-throughput systems, enhancing efficiency and reproducibility.


<details>
  <summary>Details</summary>
Motivation: To address the critical issue of device-to-device variability in automated, high-throughput systems, which can lead to significant risks at larger scales.

Method: Introduced a noise-aware decision-making algorithm that quantifies device-specific noise profiles using distributional analysis, divergence metrics, and Bayesian optimization strategies.

Result: Experimental results showed reduced redundancy, lower resource usage, and improved reliability in 3D printers, demonstrating the algorithm's effectiveness.

Conclusion: The proposed framework provides a novel approach for precision- and resource-aware optimization, improving scalability and reproducibility in automated experimental platforms.

Abstract: Device-to-device variability in experimental noise critically impacts reproducibility, especially in automated, high-throughput systems like additive manufacturing farms. While manageable in small labs, such variability can escalate into serious risks at larger scales, such as architectural 3D printing, where noise may cause structural or economic failures. This contribution presents a noise-aware decision-making algorithm that quantifies and models device-specific noise profiles to manage variability adaptively. It uses distributional analysis and pairwise divergence metrics with clustering to choose between single-device and robust multi-device Bayesian optimization strategies. Unlike conventional methods that assume homogeneous devices or generic robustness, this framework explicitly leverages inter-device differences to enhance performance, reproducibility, and efficiency. An experimental case study involving three nominally identical 3D printers (same brand, model, and close serial numbers) demonstrates reduced redundancy, lower resource usage, and improved reliability. Overall, this framework establishes a paradigm for precision- and resource-aware optimization in scalable, automated experimental platforms.

</details>


### [504] [How Machine Learning-Data Driven Replication Strategies Enhance Fault Tolerance in Large-Scale Distributed Systems](https://arxiv.org/abs/2511.11749)
*Almond Kiruthu Murimi*

Main category: cs.DC

TL;DR: The paper explores how machine learning can improve fault tolerance in distributed systems by enabling adaptive and efficient data replication strategies.


<details>
  <summary>Details</summary>
Motivation: Traditional data replication methods are static and struggle with dynamic workloads and unexpected failures, leading to inefficiency and downtime.

Method: The research integrates predictive analytics and reinforcement learning to propose adaptive replication strategies, using literature reviews, qualitative analysis, and benchmarking against traditional methods.

Result: The paper identifies limitations of current replication methods and demonstrates that machine learning can create more resilient and optimized systems.

Conclusion: The study concludes that ML-driven replication strategies hold great promise for fault tolerance in distributed systems, while also highlighting implementation challenges for real-world applications.

Abstract: This research paper investigates how machine learning-driven data replication strategies can enhance fault tolerance in large-scale distributed systems. Traditional replication methods, which rely on static configurations, often struggle to adapt to dynamic workloads and unexpected failures, leading to inefficient resource utilization and prolonged downtime. By integrating machine learning techniques-specifically predictive analytics and reinforcement learning. The study proposes adaptive replication mechanisms capable of forecasting system failures and optimizing data placement in real time. Through an extensive literature review, qualitative analysis, and comparative evaluations with traditional approaches, the paper identifies key limitations in existing replication strategies and highlights the transformative potential of machine learning in creating more resilient, self-optimizing systems. The findings underscore both the promise and the challenges of implementing ML-driven solutions in real-world environments, offering recommendations for future research and practical deployment in cloud-based and enterprise systems.

</details>


### [505] [TD-Orch: Scalable Load-Balancing for Distributed Systems with Applications to Graph Processing](https://arxiv.org/abs/2511.11843)
*Yiwei Zhao,Qiushi Lin,Hongbo Kang,Guy E. Blelloch,Laxman Dhulipala,Charles McGuffey,Phillip B. Gibbons*

Main category: cs.DC

TL;DR: The paper introduces TD-Orch, a distributed task-data orchestration framework enabling efficient applications like graph processing, achieving significant speedups over prior methods.


<details>
  <summary>Details</summary>
Motivation: To address the issue of efficiently orchestrating distributed tasks and data across multiple machines, especially under skewed data requests.

Method: Proposes TD-Orch, which uses a push-pull distributed technique to co-locate tasks with their data, achieving load balance and minimizing communication costs. A specific application, TDO-GP, is built using this framework for distributed graph processing.

Result: TD-Orch showed up to 2.7x speedup over existing baselines, while TDO-GP achieved an average 4.1x speedup over prior distributed graph systems.

Conclusion: TD-Orch and its application TDO-GP demonstrate the framework’s versatility and scalability in optimizing distributed task execution and graph processing applications.

Abstract: In this paper, we highlight a task-data orchestration abstraction that supports a range of distributed applications, including graph processing and key-value stores. Given a batch of tasks each requesting one or more data items, where both tasks and data are distributed across multiple machines, each task must get co-located with its target data (by moving tasks and/or data) and executed. We present TD-Orch, an efficient and scalable orchestration framework featuring a simple application developer interface. TD-Orch employs a distributed push-pull technique, leveraging the bidirectional f low of both tasks and data to achieve scalable load balance across machines even under highly skewed data request (data hot spots), with minimal communication overhead. Experimental results show that TD-Orch achieves up to 2.7x speedup over existing distributed scheduling baselines. Building on TD-Orch, we present TDO-GP, a distributed graph processing system for general graph problems, demonstrating the effectiveness of the underlying framework. We design three families of implementation techniques to fully leverage the execution flow provided by TD-Orch. Experimental results show that TDO-GP achieves an average speedup of 4.1x over the best prior open-source distributed graph systems for general graph processing.

</details>


### [506] [Flash-Fusion: Enabling Expressive, Low-Latency Queries on IoT Sensor Streams with LLMs](https://arxiv.org/abs/2511.11885)
*Kausar Patherya,Ashutosh Dhekne,Francisco Romero*

Main category: cs.DC

TL;DR: Flash-Fusion is an edge-cloud system designed to efficiently analyze IoT data by summarizing data at the edge and optimizing the use of LLMs for query-based tasks, successfully reducing latency and analysis costs.


<details>
  <summary>Details</summary>
Motivation: The paper addresses challenges in IoT data processing with LLMs: expensive low-level sensor data collection, slow analysis requiring expertise, and impracticality of directly feeding telemetry data due to context constraints and latency.

Method: Flash-Fusion uses edge-based data summarization (reducing raw data volumes by 73.5%) and cloud-based query planning with clustered behavioral data to create structured inputs for LLMs.

Result: Flash-Fusion decreases latency by 95% and reduces token usage and costs by 98% compared to a baseline approach, while providing accurate and high-quality responses.

Conclusion: The designed system successfully optimizes IoT data analysis, enabling non-expert users across various roles to efficiently interpret data without complex preprocessing.

Abstract: Smart cities and pervasive IoT deployments have generated interest in IoT data analysis across transportation and urban planning. At the same time, Large Language Models offer a new interface for exploring IoT data - particularly through natural language. Users today face two key challenges when working with IoT data using LLMs: (1) data collection infrastructure is expensive, producing terabytes of low-level sensor readings that are too granular for direct use, and (2) data analysis is slow, requiring iterative effort and technical expertise. Directly feeding all IoT telemetry to LLMs is impractical due to finite context windows, prohibitive token costs at scale, and non-interactive latencies. What is missing is a system that first parses a user's query to identify the analytical task, then selects the relevant data slices, and finally chooses the right representation before invoking an LLM.
  We present Flash-Fusion, an end-to-end edge-cloud system that reduces the IoT data collection and analysis burden on users. Two principles guide its design: (1) edge-based statistical summarization (achieving 73.5% data reduction) to address data volume, and (2) cloud-based query planning that clusters behavioral data and assembles context-rich prompts to address data interpretation. We deploy Flash-Fusion on a university bus fleet and evaluate it against a baseline that feeds raw data to a state-of-the-art LLM. Flash-Fusion achieves a 95% latency reduction and 98% decrease in token usage and cost while maintaining high-quality responses. It enables personas across disciplines - safety officers, urban planners, fleet managers, and data scientists - to efficiently iterate over IoT data without the burden of manual query authoring or preprocessing.

</details>


### [507] [KVSwap: Disk-aware KV Cache Offloading for Long-Context On-device Inference](https://arxiv.org/abs/2511.11907)
*Huawei Zhang,Chunwei Xia,Zheng Wang*

Main category: cs.DC

TL;DR: The paper introduces KVSwap, a framework to address memory limitations in long-context language model inference by offloading the key-value cache to disk while prioritizing critical KV entries.


<details>
  <summary>Details</summary>
Motivation: Memory limitations in long-context inference hinder running LMs on embedded devices without compromising performance, cost, or privacy.

Method: KVSwap offloads the key-value cache to secondary storage (disk), dynamically predicting and preloading critical entries, optimizing disk access based on hardware characteristics.

Result: Evaluation demonstrates KVSwap achieves higher throughput under limited memory conditions while retaining generation quality compared to existing approaches.

Conclusion: KVSwap is a promising solution for allowing efficient, high-quality language model inference on low-memory embedded devices, improving privacy and reducing resource usage.

Abstract: Language models (LMs) underpin emerging mobile and embedded AI applications like meeting and video summarization and document analysis, which often require processing multiple long-context inputs. Running an LM locally on-device improves privacy, enables offline use, and reduces cost, but long-context inference quickly hits a \emph{memory capacity wall} as the key-value (KV) cache grows linearly with context length and batch size.
  We present KVSwap, a software framework to break this memory wall by offloading the KV cache to non-volatile secondary storage (disk). KVSwap leverages the observation that only a small, dynamically changing subset of KV entries is critical for generation. It stores the full cache on disk, uses a compact in-memory metadata to predict which entries to preload, overlaps computation with hardware-aware disk access, and orchestrates read patterns to match storage device characteristics. Our evaluation shows that across representative LMs and storage types, KVSwap delivers higher throughput under tight memory budgets while maintaining the generation quality when compared with existing KV cache offloading schemes.

</details>


### [508] [High-Performance N-Queens Solver on GPU: Iterative DFS with Zero Bank Conflicts](https://arxiv.org/abs/2511.12009)
*Guangchao Yao,Yali Li*

Main category: cs.DC

TL;DR: The paper introduces a novel GPU-based parallel computing method for the N-Queens problem, enabling faster computations for previously unsolvable cases.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the excessive time and computational resources required to solve and verify the N-Queens problem for larger values of N, such as N=27 and N=28.

Method: The authors propose and implement a parallel computing method combining an iterative DFS algorithm, GPU shared memory mapping, optimized memory access patterns, and other performance-enhancing techniques on NVIDIA GPUs.

Result: The method reduced the solving time for the 27-Queens problem to 28.4 days using eight RTX 5090 GPUs, and projected the solving time for the 28-Queens problem to approximately 11 months.

Conclusion: The approach achieves significantly faster computation compared to the state-of-the-art GPU methods, providing fresh avenues for tackling the computational challenges of the N-Queens problem.

Abstract: The counting of solutions to the N-Queens problem is a classic NP-complete problem with extremely high computational complexity. As of now, the academic community has rigorously verified the number of solutions only up to N <= 26. In 2016, the research team led by PreuBer solved the 27-Queens problem using FPGA hardware, which took approximately one year, though the result remains unverified independently. Recent studies on GPU parallel computing suggest that verifying the 27-Queens solution would still require about 17 months, indicating excessively high time and computational resource costs. To address this challenge, we propose an innovative parallel computing method on NVIDIA GPU platform, with the following core contributions: (1) An iterative depth-first search (DFS) algorithm for solving the N-Queens problem; (2) Complete mapping of the required stack structure to GPU shared memory; (3) Effective avoidance of bank conflicts through meticulously designed memory access patterns; (4) Various optimization techniques are employed to achieve optimal performance. Under the proposed optimization framework, we successfully verified the 27-Queens problem in just 28.4 days using eight RTX 5090 GPUs, thereby confirming the correctness of PreuBer's computational results. Moreover, we have reduced the projected solving time for the next open case-the 28-Queens problem-to approximately 11 months, making its resolution computationally feasible. Compared to the state-of-the-art GPU methods, our method achieves over 10x speedup on identical hardware configurations (8 A100), while delivering over 26x acceleration when utilizing 8 RTX 5090 GPUs, and brings fresh perspectives to this long-stagnant problem.

</details>


### [509] [A Quick and Exact Method for Distributed Quantile Computation](https://arxiv.org/abs/2511.12025)
*Ivan Cao,Jaromir J. Saloni,David A. G. Harrison*

Main category: cs.DC

TL;DR: GK Select is introduced as an exact quantile computation algorithm in Spark, avoiding expensive global sorting and offering significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: There is a need for an efficient method to compute exact quantiles in Spark without relying on computationally heavy global sorting procedures.

Method: GK Select leverages the GK Sketch to identify near-target pivots, extracts values within error bounds in linear time, and performs tree-reduction of candidate sets to compute exact quantiles.

Result: GK Select achieves approximately 10.5x better performance than Spark's global sort for exact quantile computation, verified empirically on large-scale datasets.

Conclusion: GK Select provides a faster and efficient solution for exact quantile computation in Spark, matching the executor-side complexity of the GK Sketch and demonstrating practical advantages over traditional methods.

Abstract: Quantile computation is a core primitive in large-scale data analytics. In Spark, practitioners typically rely on the Greenwald-Khanna (GK) Sketch, an approximate method. When exact quantiles are required, the default option is an expensive global sort. We present GK Select, an exact Spark algorithm that avoids full-data shuffles and completes in a constant number of actions. GK Select leverages GK Sketch to identify a near-target pivot, extracts all values within the error bound around this pivot in each partition in linear time, and then tree-reduces the resulting candidate sets. We show analytically that GK Select matches the executor-side time complexity of GK Sketch while returning the exact quantile. Empirically, GK Select achieves sketch-level latency and outperforms Spark's full sort by approximately 10.5x on 10^9 values across 120 partitions on a 30-core AWS EMR cluster.

</details>


### [510] [Striking the Right Balance between Compute and Copy: Improving LLM Inferencing Under Speculative Decoding](https://arxiv.org/abs/2511.12031)
*Arun Ramachandran,Ramaswamy Govindarajan,Murali Annavaram,Prakash Raghavendra,Hossein Entezari Zarch,Lei Gao,Chaoyi Jiang*

Main category: cs.DC

TL;DR: The paper proposes the Balancing Memory and Compute (BMC) mechanism to optimize CPU inference of large language models (LLMs) by reducing copy overhead and efficiently utilizing KV cache memory, achieving significant speedups.


<details>
  <summary>Details</summary>
Motivation: GPUs and cloud-based virtual GPU instances for LLM inference are expensive, prompting the need for efficient CPU-based alternatives. A significant bottleneck in CPUs is the KV cache update overhead, especially as sequence lengths grow.

Method: The authors introduce BMC, an allocation strategy for KV cache that reduces copy overhead by allocating tensors with redundant rows every r iterations for in-place updates. They also repurpose the allocated redundant rows for Speculative Decoding (SD) to boost token generation efficiency. The method involves analytical modeling to identify optimal design configurations.

Result: BMC achieves up to 3.2x throughput acceleration compared to HuggingFace without SD and an additional 1.39x speedup with SD. It improves performance by 1.36x over vLLM and 2.29x over DeepSpeed, showcasing significant CPU and GPU compatibility.

Conclusion: BMC demonstrates an efficient approach to LLM inference on CPUs and GPUs by optimizing KV cache updates and integrating speculative decoding capabilities, significantly improving throughput and cost-effectiveness.

Abstract: With the skyrocketing costs of GPUs and their virtual instances in the cloud, there is a significant desire to use CPUs for large language model (LLM) inference. KV cache update, often implemented as allocation, copying, and in-place strided update for each generated token, incurs significant overhead. As the sequence length increases, the allocation and copy overheads dominate the performance. Alternate approaches may allocate large KV tensors upfront to enable in-place updates, but these matrices (with zero-padded rows) cause redundant computations. In this work, we propose a new KV cache allocation mechanism called Balancing Memory and Compute (BMC). BMC allocates, once every r iterations, KV tensors with r redundant rows, allowing in-place update without copy overhead for those iterations, but at the expense of a small amount of redundant computation. Second, we make an interesting observation that the extra rows allocated in the KV tensors and the resulting redundant computation can be repurposed for Speculative Decoding (SD) that improves token generation efficiency. Last, BMC represents a spectrum of design points with different values of r. To identify the best-performing design point(s), we derive a simple analytical model for BMC. The proposed BMC method achieves an average throughput acceleration of up to 3.2x over baseline HuggingFace (without SD). Importantly when we apply BMC with SD, it results in an additional speedup of up to 1.39x, over and above the speedup offered by SD. Further, BMC achieves a throughput acceleration of up to 1.36x and 2.29x over state-of-the-art inference servers vLLM and DeepSpeed, respectively. Although the BMC technique is evaluated extensively across different classes of CPUs (desktop and server class), we also evaluate the scheme with GPUs and demonstrate that it works well for GPUs.

</details>


### [511] [Combining Serverless and High-Performance Computing Paradigms to support ML Data-Intensive Applications](https://arxiv.org/abs/2511.12185)
*Mills Staylor,Arup Kumar Sarker,Gregor von Laszewski,Geoffrey Fox,Yue Cheng,Judy Fox*

Main category: cs.DC

TL;DR: The paper introduces Cylon, a high-performance distributed data frame designed for serverless data processing, addressing performance bottlenecks.


<details>
  <summary>Details</summary>
Motivation: Improving efficiency and scalability in handling large data processing in serverless computing environments, overcoming performance issues and communication delays.

Method: Designed a serverless communicator inspired by the FMI library and implemented a direct communication protocol (NAT Traversal TCP Hole Punching).

Result: Demonstrated significant performance improvements in serverless environments compared to traditional storage-dependent setups, with test results indicating better scaling ability.

Conclusion: Cylon enhances serverless data processing by addressing communication and performance challenges; however, performance still lags behind HPC and EC2 scalability in strong scaling experiments.

Abstract: Data is found everywhere, from health and human infrastructure to the surge of sensors and the proliferation of internet-connected devices. To meet this challenge, the data engineering field has expanded significantly in recent years in both research and industry. Traditionally, data engineering, Machine Learning, and AI workloads have been run on large clusters within data center environments, requiring substantial investment in hardware and maintenance. With the rise of the public cloud, it is now possible to run large applications across nodes without owning or maintaining hardware. Serverless functions such as AWS Lambda provide horizontal scaling and precise billing without the hassle of managing traditional cloud infrastructure. However, when processing large datasets, users often rely on external storage options that are significantly slower than direct communication typical of HPC clusters. We introduce Cylon, a high-performance distributed data frame solution that has shown promising results for data processing using Python. We describe how we took inspiration from the FMI library and designed a serverless communicator to tackle communication and performance issues associated with serverless functions. With our design, we demonstrate that the performance of AWS Lambda falls below one percent of strong scaling experiments compared to serverful AWS (EC2) and HPCs based on implementing direct communication via NAT Traversal TCP Hole Punching.

</details>


### [512] [Distributed Seasonal Temporal Pattern Mining](https://arxiv.org/abs/2511.12216)
*Van Ho-Long,Nguyen Ho,Anh-Vu Dinh-Duc,Ha Manh Tran,Ky Trung Nguyen,Tran Dung Pham,Quoc Viet Hung Nguyen*

Main category: cs.DC

TL;DR: The paper presents DSTPM, a framework for distributed mining of seasonal temporal patterns in time series data, achieving superior scalability and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address scalability issues and computational inefficiencies in mining seasonal temporal patterns (STPs) from time series data.

Method: Proposes the Distributed Seasonal Temporal Pattern Mining (DSTPM) framework that utilizes distributed hierarchical lookup hash structures for efficient computation.

Result: Experimental evaluations show DSTPM substantially outperforms sequential methods in runtime and memory usage, handling very large datasets efficiently.

Conclusion: DSTPM effectively scales to large datasets for mining STPs, providing a powerful approach over sequential counterparts.

Abstract: The explosive growth of IoT-enabled sensors is producing enormous amounts of time series data across many domains, offering valuable opportunities to extract insights through temporal pattern mining. Among these patterns, an important class exhibits periodic occurrences, referred to as \textit{seasonal temporal patterns} (STPs). However, mining STPs poses challenges, as traditional measures such as support and confidence cannot capture seasonality, and the lack of the anti-monotonicity property results in an exponentially large search space. Existing STP mining methods operate sequentially and therefore do not scale to large datasets. In this paper, we propose the Distributed Seasonal Temporal Pattern Mining (DSTPM), the first distributed framework for mining seasonal temporal patterns from time series. DSTPM leverages efficient data structures, specifically distributed hierarchical lookup hash structures, to enable efficient computation. Extensive experimental evaluations demonstrate that DSTPM significantly outperforms sequential baselines in runtime and memory usage, while scaling effectively to very large datasets.

</details>


### [513] [Pico-Cloud: Cloud Infrastructure for Tiny Edge Devices](https://arxiv.org/abs/2511.13253)
*Mordechai Guri*

Main category: cs.DC

TL;DR: This paper proposes Pico-Cloud, a lightweight and decentralized micro-edge cloud system using minimal hardware such as Raspberry Pi Zero.


<details>
  <summary>Details</summary>
Motivation: To provide a cost-effective and sustainable edge cloud solution for low latency, low power consumption, and decentralized operation without depending on large data centers.

Method: Developed Pico-Cloud with container-based virtualization, service discovery, and orchestration for use on minimal hardware platforms. Demonstrated with use cases and analyzed challenges.

Result: Pico-Cloud proves to be efficient and effective for lightweight workload management at the edge, showcasing potential in areas like rural connectivity, education, and AI.

Conclusion: Pico-Cloud is a promising platform for decentralized, low-power, and accessible edge computing solutions, particularly for applications requiring minimal resources.

Abstract: This paper introduces the Pico-Cloud, a micro-edge cloud architecture built on ultra-minimal hardware platforms such as the Raspberry Pi Zero and comparable single-board computers. The Pico-Cloud delivers container-based virtualization, service discovery, and lightweight orchestration directly at the device layer, enabling local operation with low latency and low power consumption without reliance on centralized data centers. We present its architectural model, outline representative use cases including rural connectivity, educational clusters, and edge AI inference, and analyze design challenges in computation, networking, storage, and power management. The results highlight Pico-Clouds as a cost-effective, decentralized, and sustainable platform for lightweight distributed workloads at the network edge.

</details>


### [514] [Design of A Low-Latency and Parallelizable SVD Dataflow Architecture on FPGA](https://arxiv.org/abs/2511.12461)
*Fangqiang Du,Sixuan Chong,Zixuan Huang,Rui Qin,Fengnan Mi,Caibao Hu,Jiangang Chen*

Main category: cs.DC

TL;DR: This paper proposes a streamlined algorithm for real-time SVD in large-scale data, saving on-chip memory and boosting computational efficiency significantly.


<details>
  <summary>Details</summary>
Motivation: The increasing dimensionality of matrices in large-scale data stream processing leads to steep computational costs and efficiency challenges, which are unaddressed by existing hardware solutions.

Method: A novel Data Stream-Based SVD processing algorithm (DSB Jacobi) is designed to optimize memory usage and computational speed for real-time applications.

Result: The proposed method reduced on-chip RAM usage by 41.5% and increased computational efficiency by 23 times compared to previous approaches.

Conclusion: The DSB Jacobi algorithm is a scalable and efficient solution for real-time SVD computation, with significant improvements in memory and processing metrics.

Abstract: Singular value decomposition (SVD) is widely used for dimensionality reduction and noise suppression, and it plays a pivotal role in numerous scientific and engineering applications. As the dimensions of the matrix grow rapidly, the computational cost increases significantly, posing a serious challenge to the efficiency of data analysis and signal processing systems,especially in time-sensitive scenarios with large-scale datasets. Although various dedicated hardware architectures have been proposed to accelerate the computation of intensive SVD, many of these designs suffer from limited scalability and high consumption of on-chip memory resources. Moreover, they typically overlook the computational and data transfer challenges associated with SVD, enabling them unsuitable for real-time processing of large-scale data stream matrices in embedded systems. In this express, we propose a Data Stream-Based SVD processing algorithm (DSB Jacobi), which significantly reduces on-chip BRAM usage while improving computational speed, offering a practical solution for real-time SVD computation of large-scale data streams. Compared with previous works, our experimental results indicate that the proposed method reduces on-chip RAM consumption by 41.5 percent and improves computational efficiency by 23 times.

</details>


### [515] [A Decentralized Root Cause Localization Approach for Edge Computing Environments](https://arxiv.org/abs/2511.12486)
*Duneesha Fernando,Maria A. Rodriguez,Rajkumar Buyya*

Main category: cs.DC

TL;DR: This paper proposes a decentralized Root Cause Localization (RCL) method for edge computing using Personalized PageRank, achieving comparable or better accuracy than centralized methods while reducing localization time by up to 34%.


<details>
  <summary>Details</summary>
Motivation: Existing RCL approaches are centralized and not suitable for edge environments due to their latency and overhead. There is a need for a decentralized approach tailored for resource-constrained edge computing.

Method: A decentralized RCL approach is proposed that groups microservices into clusters and uses Personalized PageRank locally for anomaly localization. Additionally, an inter-cluster peer-to-peer process and a new anomaly scoring mechanism for heterogeneity are introduced.

Result: The decentralized approach achieves highly accurate anomaly localization and reduces localization time by up to 34% compared to centralized methods, as shown by evaluation on the MicroCERCL dataset.

Conclusion: Decentralized graph-based RCL, as presented, is a practical and efficient solution for diagnosing performance anomalies in edge computing, with significant improvements in speed and comparable accuracy.

Abstract: Edge computing environments host increasingly complex microservice-based IoT applications, which are prone to performance anomalies that can propagate across dependent services. Identifying the true source of such anomalies, known as Root Cause Localization (RCL), is essential for timely mitigation. However, existing RCL approaches are designed for cloud environments and rely on centralized analysis, which increases latency and communication overhead when applied at the edge. This paper proposes a decentralized RCL approach that executes localization directly at the edge device level using the Personalized PageRank (PPR) algorithm. The proposed method first groups microservices into communication- and colocation-aware clusters, thereby confining most anomaly propagation within cluster boundaries. Within each cluster, PPR is executed locally to identify the root cause, significantly reducing localization time. For the rare cases where anomalies propagate across clusters, we introduce an inter-cluster peer-to-peer approximation process, enabling lightweight coordination among clusters with minimal communication overhead. To enhance the accuracy of localization in heterogeneous edge environments, we also propose a novel anomaly scoring mechanism tailored to the diverse anomaly triggers that arise across microservice, device, and network layers. Evaluation results on the publicly available edge dataset, MicroCERCL, demonstrate that the proposed decentralized approach achieves comparable or higher localization accuracy than its centralized counterpart while reducing localization time by up to 34%. These findings highlight that decentralized graph-based RCL can provide a practical and efficient solution for anomaly diagnosis in resource-constrained edge environments.

</details>


### [516] [Iris: First-Class Multi-GPU Programming Experience in Triton](https://arxiv.org/abs/2511.12500)
*Muhammad Awad,Muhammad Osama,Brandon Potter*

Main category: cs.DC

TL;DR: Iris, a Python and Triton-based multi-GPU communication library, simplifies programming and achieves near-optimal performance by providing memory abstractions tailored to computation-communication overlap patterns.


<details>
  <summary>Details</summary>
Motivation: To address the trade-off between performance and programmability in multi-GPU programming, traditional implementations are either complex or sacrifice efficiency.

Method: Iris uses tile-based symmetric memory abstractions and is implemented in Python and Triton. It allows developers to interleave computation and communication efficiently with minimal code changes.

Result: Iris shows near-optimal bandwidth utilization in benchmarks and outperforms PyTorch and RCCL by up to 1.79x in GEMM+All-Scatter workloads.

Conclusion: High-level approaches like Iris can both simplify multi-GPU programming and achieve performance comparable to or exceeding highly optimized libraries.

Abstract: Multi-GPU programming traditionally requires developers to navigate complex trade-offs between performance and programmability. High-performance implementations typically rely on low-level HIP/CUDA communication libraries that demand substantial engineering effort for even basic overlap patterns, while simpler abstractions often sacrifice performance. We present Iris, a multi-GPU communication library implemented entirely in Python and Triton that eliminates this trade-off. Iris provides tile-based symmetric memory abstractions that naturally align with Triton's programming model, enabling developers to write single-source kernels that seamlessly interleave computation and communication. We demonstrate a taxonomy of compute-communication overlap patterns--from bulk-synchronous to fine-grained workgroup specialization--that can be implemented with minimal code changes in Iris, often requiring just a few additional lines within the same Triton kernel. Our evaluation shows that Iris achieves near-optimal bandwidth utilization in microbenchmarks and delivers up to 1.79x speedup over PyTorch and RCCL for GEMM+All-Scatter workloads, demonstrating that high-level implementations can match or exceed heavily-optimized libraries while dramatically simplifying multi-GPU programming.

</details>


### [517] [Artifact for A Non-Intrusive Framework for Deferred Integration of Cloud Patterns in Energy-Efficient Data-Sharing Pipelines](https://arxiv.org/abs/2511.12667)
*Sepideh Masoudi,Mark Edward Michael Daly,Jannis Kiesel*

Main category: cs.DC

TL;DR: The paper introduces a Kubernetes-based tool for applying design patterns to data-sharing pipelines without modifying service code, enhancing reusability and energy efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional cloud design patterns often compromise the reusability of modular transformation services in data-sharing pipelines.

Method: The authors developed a Kubernetes-based system that automates the application of design patterns and gathers energy metrics for informed decision-making.

Result: The tool supports non-intrusive pattern injection and offers insights into energy consumption, enabling energy-efficient, reusable data pipelines.

Conclusion: Using the proposed tool, organizations can achieve interoperable and energy-aware data-sharing pipelines while maintaining reusability of services.

Abstract: As data mesh architectures grow, organizations increasingly build consumer-specific data-sharing pipelines from modular, cloud-based transformation services. While reusable transformation services can improve cost and energy efficiency, applying traditional cloud design patterns can reduce reusability of services in different pipelines. We present a Kubernetes-based tool that enables non-intrusive, deferred application of design patterns without modifying services code. The tool automates pattern injection and collects energy metrics, supporting energy-aware decisions while preserving reusability of transformation services in various pipeline structures.

</details>


### [518] [The Time to Consensus in a Blockchain: Insights into Bitcoin's "6 Blocks Rule''](https://arxiv.org/abs/2511.12687)
*Partha S. Dey,Aditya S. Gopalan,Vijay G. Subramanian*

Main category: cs.DC

TL;DR: This paper studies the time to consensus in Nakamoto blockchains using queueing techniques and computes the Laplace transform for consensus time in a stylized Bitcoin model.


<details>
  <summary>Details</summary>
Motivation: To understand and quantify the time required for the honest process to surpass the adversarial process permanently in Nakamoto blockchains, which has implications for blockchain security and efficiency.

Method: The framework utilizes queueing techniques to model the competition between honest and adversarial growth processes. The researchers calculate the Laplace transform of consensus time through analytical and simulation approaches.

Result: They successfully compute the Laplace transform for the time to consensus in a stylized Bitcoin model and confirm its correctness via simulations.

Conclusion: The study provides a mathematical insight into the time to consensus under random delays, furthering understanding of blockchain dynamics and potential vulnerabilities.

Abstract: We investigate the time to consensus in Nakamoto blockchains. Specifically, we consider two competing growth processes, labeled \emph{honest} and \emph{adversarial}, and determine the time after which the honest process permananetly exceeds the adversarial process. This is done via queueing techniques. The predominant difficulty is that the honest growth process is subject to \emph{random delays}. In a stylized Bitcoin model, we compute the Laplace transform for the time to consensus and verify it via simulation.

</details>


### [519] [Learning Process Energy Profiles from Node-Level Power Data](https://arxiv.org/abs/2511.13155)
*Jonathan Bader,Julius Irion,Jannis Kappel,Joel Witzke,Niklas Fomin,Diellza Sherifi,Odej Kao*

Main category: cs.DC

TL;DR: The paper addresses efficient energy consumption monitoring in data centers using an innovative method to predict per-process energy usage.


<details>
  <summary>Details</summary>
Motivation: The sharp increase in data center energy consumption due to the rising growth in high-performance computing, cloud computing, and AI drives the need for process-level energy efficiency insights.

Method: A regression-based model maps resource metrics (via eBPF and perf) to node-level energy consumption measured by power distribution units to estimate per-process energy profiles.

Result: The method provides improved fine-grained energy consumption predictions at the process level compared to hardware-based node-level monitoring.

Conclusion: The approach offers a scalable and precise solution to optimize energy consumption at the process level in data centers.

Abstract: The growing demand for data center capacity, driven by the growth of high-performance computing, cloud computing, and especially artificial intelligence, has led to a sharp increase in data center energy consumption. To improve energy efficiency, gaining process-level insights into energy consumption is essential. While node-level energy consumption data can be directly measured with hardware such as power meters, existing mechanisms for estimating per-process energy usage, such as Intel RAPL, are limited to specific hardware and provide only coarse-grained, domain-level measurements. Our proposed approach models per-process energy profiles by leveraging fine-grained process-level resource metrics collected via eBPF and perf, which are synchronized with node-level energy measurements obtained from an attached power distribution unit. By statistically learning the relationship between process-level resource usage and node-level energy consumption through a regression-based model, our approach enables more fine-grained per-process energy predictions.

</details>


### [520] [Distributed Hierarchical Machine Learning for Joint Resource Allocation and Slice Selection in In-Network Edge Systems](https://arxiv.org/abs/2511.13313)
*Sulaiman Muhammad Rashid,Ibrahim Aliyu,Jaehyung Park,Jinsul Kim*

Main category: cs.DC

TL;DR: The paper proposes a novel solution to address latency and resource challenges in the Metaverse by introducing a slice-enabled in-network edge architecture with DeepSets-based learning for resource management, achieving near-optimal results with significantly reduced runtime.


<details>
  <summary>Details</summary>
Motivation: The Metaverse requires low-latency and resource-efficient operations to deliver real-time immersive experiences, but traditional methods fail under dynamic edge conditions and high user loads.

Method: The study develops a slice-enabled in-network edge architecture combining COIN and MEC. It introduces a hierarchical DeepSets-based model for solving a resource management problem divided into three sub-problems: intra-slice allocation, inter-slice allocation, and offloading decisions.

Result: The DeepSets-S model achieves 95.26% and 95.67% accuracy on resource allocation sub-problems and 74.86% accuracy on offloading decisions. It reduces execution time by 86.1%, closely tracking optimal costs within 6.1%, and outperforms baseline approaches in cost efficiency and resource utilization.

Conclusion: The proposed DeepSets-S approach is a computationally efficient and accurate solution for real-time resource allocation challenges in the Metaverse, providing significant performance improvements over existing techniques.

Abstract: The Metaverse promises immersive, real-time experiences; however, meeting its stringent latency and resource demands remains a major challenge. Conventional optimization techniques struggle to respond effectively under dynamic edge conditions and high user loads. In this study, we explore a slice-enabled in-network edge architecture that combines computing-in-the-network (COIN) with multi-access edge computing (MEC). In addition, we formulate the joint problem of wireless and computing resource management with optimal slice selection as a mixed-integer nonlinear program (MINLP). Because solving this model online is computationally intensive, we decompose it into three sub-problems (SP1) intra-slice allocation, (SP2) inter-slice allocation, and (SP3) offloading decision and train a distributed hierarchical DeepSets-based model (DeepSets-S) on optimal solutions obtained offline. In the proposed model, we design a slack-aware normalization mechanism for a shared encoder and task-specific decoders, ensuring permutation equivariance over variable-size wireless device (WD) sets. The learned system produces near-optimal allocations with low inference time and maintains permutation equivariance over variable-size device sets. Our experimental results show that DeepSets-S attains high tolerance-based accuracies on SP1/SP2 (Acc1 = 95.26% and 95.67%) and improves multiclass offloading accuracy on SP3 (Acc = 0.7486; binary local/offload Acc = 0.8824). Compared to exact solvers, the proposed approach reduces the execution time by 86.1%, while closely tracking the optimal system cost (within 6.1% in representative regimes). Compared with baseline models, DeepSets-S consistently achieves higher cost ratios and better utilization across COIN/MEC resources.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [521] [EcoSpa: Efficient Transformer Training with Coupled Sparsity](https://arxiv.org/abs/2511.11641)
*Jinqi Xiao,Cheng Luo,Lingyi Huang,Cheng Yang,Yang Sui,Huy Phan,Xiao Zang,Yibiao Ying,Zhexiang Tang,Anima Anandkumar,Bo Yuan*

Main category: cs.LG

TL;DR: EcoSpa is a structured sparse training method improving transformer efficiency by preserving critical weight interactions, resulting in reduced memory usage, faster training, and better performance even under high sparsity.


<details>
  <summary>Details</summary>
Motivation: Transformers are highly computationally intensive, and existing sparse training methods degrade performance at high sparsity settings due to failure in maintaining structural relationships between weight matrices.

Method: EcoSpa evaluates and sparsifies coupled weight matrix pairs by aligning their structure, introducing a granularity for importance calibration while maintaining interaction patterns during both pre-training and fine-tuning.

Result: EcoSpa delivers significant benefits: 50% memory savings and 21% faster training for LLaMA-1B, $2.2\times$ compression on GPT-2-Medium with $2.4$ lower perplexity, and $1.6\times$ inference speedup.

Conclusion: EcoSpa offers high performance transformation efficiency using standard PyTorch operations, requiring no specialized hardware, thereby making transformer training more accessible on common hardware.

Abstract: Transformers have become the backbone of modern AI, yet their high computational demands pose critical system challenges. While sparse training offers efficiency gains, existing methods fail to preserve critical structural relationships between weight matrices that interact multiplicatively in attention and feed-forward layers. This oversight leads to performance degradation at high sparsity levels. We introduce EcoSpa, an efficient structured sparse training method that jointly evaluates and sparsifies coupled weight matrix pairs, preserving their interaction patterns through aligned row/column removal. EcoSpa introduces a new granularity for calibrating structural component importance and performs coupled estimation and sparsification across both pre-training and fine-tuning scenarios. Evaluations demonstrate substantial improvements: EcoSpa enables efficient training of LLaMA-1B with 50\% memory reduction and 21\% faster training, achieves $2.2\times$ model compression on GPT-2-Medium with $2.4$ lower perplexity, and delivers $1.6\times$ inference speedup. The approach uses standard PyTorch operations, requiring no custom hardware or kernels, making efficient transformer training accessible on commodity hardware.

</details>


### [522] [KForge: Program Synthesis for Diverse AI Hardware Accelerators](https://arxiv.org/abs/2511.13274)
*Taras Sereda,Tom St. John,Burak Bartan,Natalie Serrino,Sachin Katti,Zain Asgar*

Main category: cs.LG

TL;DR: KForge is a platform-agnostic framework using collaborative LLM agents to optimize GPU kernels for ML performance across platforms like CUDA and Metal.


<details>
  <summary>Details</summary>
Motivation: Optimizing GPU kernels for different hardware platforms is challenging but critical for improving ML performance and efficiency.

Method: KForge employs two LLM-based agents: one for iterative program generation and refinement based on feedback, and another for performance analysis through profiling data interpretation.

Result: KForge effectively synthesizes optimized programs across diverse hardware like NVIDIA CUDA and Apple Metal, leveraging cross-platform knowledge transfer.

Conclusion: This framework provides a scalable, platform-agnostic solution to GPU kernel optimization while requiring minimal prior platform-specific effort, demonstrating strong cross-architecture usability.

Abstract: GPU kernels are critical for ML performance but difficult to optimize across diverse accelerators. We present KForge, a platform-agnostic framework built on two collaborative LLM-based agents: a generation agent that produces and iteratively refines programs through compilation and correctness feedback, and a performance analysis agent that interprets profiling data to guide optimization. This agent-based architecture requires only a single-shot example to target new platforms.
  We make three key contributions: (1) introducing an iterative refinement system where the generation agent and performance analysis agent collaborate through functional and optimization passes, interpreting diverse profiling data (from programmatic APIs to GUI-based tools) to generate actionable recommendations that guide program synthesis for arbitrary accelerators; (2) demonstrating that the generation agent effectively leverages cross-platform knowledge transfer, where a reference implementation from one architecture substantially improves generation quality for different hardware targets; and (3) validating the platform-agnostic nature of our approach by demonstrating effective program synthesis across fundamentally different parallel computing platforms: NVIDIA CUDA and Apple Metal.

</details>


### [523] [Softmax as a Lagrangian-Legendrian Seam](https://arxiv.org/abs/2511.11573)
*Christopher R. Lee-Jenkins*

Main category: cs.LG

TL;DR: This paper connects machine learning with modern differential geometry, exploring softmax transformation as a geometric interface and detailing its mathematical underpinnings.


<details>
  <summary>Details</summary>
Motivation: Bridge machine learning concepts with differential geometry to gain deeper insights into foundational processes like softmax transformation.

Method: Model softmax as a geometry-driven interface using concepts such as contact screen, Legendrian seam, and symplectic structures to elucidate processes behind logits-to-probabilities conversion.

Result: The paper provides concrete geometrical explanations for softmax behavior, bias-shift invariance, and KL gap computation, validated for two- and three-class classification cases.

Conclusion: Conceptualizing machine learning processes, like softmax, using differential geometry encourages advancements in compact logit models, invariants, and connections to information geometry.

Abstract: This note offers a first bridge from machine learning to modern differential geometry. We show that the logits-to-probabilities step implemented by softmax can be modeled as a geometric interface: two potential-generated, conservative descriptions (from negative entropy and log-sum-exp) meet along a Legendrian "seam" on a contact screen (the probability simplex) inside a simple folded symplectic collar. Bias-shift invariance appears as Reeb flow on the screen, and the Fenchel-Young equality/KL gap provides a computable distance to the seam. We work out the two- and three-class cases to make the picture concrete and outline next steps for ML: compact logit models (projective or spherical), global invariants, and connections to information geometry where on-screen dynamics manifest as replicator flows.

</details>


### [524] [Hardware optimization on Android for inference of AI models](https://arxiv.org/abs/2511.13453)
*Iulius Gherasim,Carlos García Sánchez*

Main category: cs.LG

TL;DR: The study evaluates optimal AI model configurations for object detection and image classification on Android systems, focusing on device accelerators like GPU and NPU.


<details>
  <summary>Details</summary>
Motivation: To optimize mobile AI model execution for minimal latency, high responsiveness, and effective use of device hardware such as GPUs and NPUs.

Method: Empirical analysis of AI models with different quantization schemes and accelerators to balance speed and accuracy in object detection and image classification.

Result: The evaluation identifies the best configurations, balancing minimal accuracy loss with high inference speed.

Conclusion: Optimal execution strategies for mobile AI were demonstrated using YOLO and ResNet models, ensuring efficient mobile AI application performance.

Abstract: The pervasive integration of Artificial Intelligence models into contemporary mobile computing is notable across numerous use cases, from virtual assistants to advanced image processing. Optimizing the mobile user experience involves minimal latency and high responsiveness from deployed AI models with challenges from execution strategies that fully leverage real time constraints to the exploitation of heterogeneous hardware architecture. In this paper, we research and propose the optimal execution configurations for AI models on an Android system, focusing on two critical tasks: object detection (YOLO family) and image classification (ResNet). These configurations evaluate various model quantization schemes and the utilization of on device accelerators, specifically the GPU and NPU. Our core objective is to empirically determine the combination that achieves the best trade-off between minimal accuracy degradation and maximal inference speed-up.

</details>


### [525] [LLM on a Budget: Active Knowledge Distillation for Efficient Classification of Large Text Corpora](https://arxiv.org/abs/2511.11574)
*Viviana Luccioli,Rithika Iyengar,Ryan Panley,Flora Haberkorn,Xiaoyu Ge,Leland Crane,Nitish Sinha,Seung Jung Lee*

Main category: cs.LG

TL;DR: The paper introduces M-RARU, an innovative active learning algorithm, to reduce the computational costs of creating efficient models via knowledge distillation, achieving significant cost savings and maintaining performance.


<details>
  <summary>Details</summary>
Motivation: The computational and financial costs of using large language models (LLMs) hinder their deployment in dynamic environments. To address this, knowledge distillation is used to train smaller, more efficient models, although this remains costly for large datasets.

Method: The authors propose M-RARU (Multi-class Randomized Accept/Reject Uncertainty Sampling), a novel active learning algorithm. It uses uncertainty sampling with a randomized accept-reject mechanism to identify the most informative data points, reducing the labeling cost by the LLM teacher.

Result: M-RARU outperforms random sampling by reducing sample requirements by up to 80%, while improving classification accuracy across five types of models (SVM, LDA, RF, GBDT, DistilBERT) and multiple datasets.

Conclusion: Using M-RARU significantly improves the efficiency of knowledge distillation, reducing computational costs and maintaining performance, making it feasible for large-scale and dynamic environments.

Abstract: Large Language Models (LLMs) are highly accurate in classification tasks, however, substantial computational and financial costs hinder their large-scale deployment in dynamic environments. Knowledge Distillation (KD) where a LLM "teacher" trains a smaller and more efficient "student" model, offers a promising solution to this problem. However, the distillation process itself often remains costly for large datasets, since it requires the teacher to label a vast number of samples while incurring significant token consumption. To alleviate this challenge, in this work we explore the active learning (AL) as a way to create efficient student models at a fraction of the cost while preserving the LLM's performance. In particular, we introduce M-RARU (Multi-class Randomized Accept/Reject Uncertainty Sampling), a novel AL algorithm that significantly reduces training costs. M-RARU employs an innovative strategy combining uncertainty with a randomized accept-reject mechanism to select only the most informative data points for the LLM teacher. This focused approach significantly minimizes required API calls and data processing time. We evaluate M-RARU against random sampling across five diverse student models (SVM, LDA, RF, GBDT, and DistilBERT) on multiple benchmark datasets. Experiments demonstrate that our proposed method achieves up to 80% reduction in sample requirements as compared to random sampling, substantially improving classification accuracy while reducing financial costs and overall training time.

</details>


### [526] [Detecting Statistically Significant Fairness Violations in Recidivism Forecasting Algorithms](https://arxiv.org/abs/2511.11575)
*Animesh Joshi*

Main category: cs.LG

TL;DR: This paper presents a framework to test the statistical significance of fairness violations in machine learning algorithms, with a focus on recidivism forecasting.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the lack of methods in current literature to statistically assess whether disparities in algorithmic fairness are significant or occur by chance.

Method: The authors propose a framework leveraging k-fold cross-validation to create sampling distributions of fairness metrics and conduct statistical tests for fairness violations.

Result: The paper tested recidivism forecasting algorithms and found statistically significant biases against Black individuals under some fairness definitions, and either no bias or bias against White individuals under others.

Conclusion: The findings emphasize the need for robust statistical methods to evaluate biases in algorithmic decision-making systems.

Abstract: Machine learning algorithms are increasingly deployed in critical domains such as finance, healthcare, and criminal justice [1]. The increasing popularity of algorithmic decision-making has stimulated interest in algorithmic fairness within the academic community. Researchers have introduced various fairness definitions that quantify disparities between privileged and protected groups, use causal inference to determine the impact of race on model predictions, and that test calibration of probability predictions from the model. Existing literature does not provide a way in which to assess whether observed disparities between groups are statistically significant or merely due to chance. This paper introduces a rigorous framework for testing the statistical significance of fairness violations by leveraging k-fold cross-validation [2] to generate sampling distributions of fairness metrics. This paper introduces statistical tests that can be used to identify statistically significant violations of fairness metrics based on disparities between predicted and actual outcomes, model calibration, and causal inference techniques [1]. We demonstrate this approach by testing recidivism forecasting algorithms trained on data from the National Institute of Justice. Our findings reveal that machine learning algorithms used for recidivism forecasting exhibit statistically significant bias against Black individuals under several fairness definitions, while also exhibiting no bias or bias against White individuals under other definitions. The results from this paper underscore the importance of rigorous and robust statistical testing while evaluating algorithmic decision-making systems.

</details>


### [527] [The Anatomy of a Triton Attention Kernel](https://arxiv.org/abs/2511.11581)
*Burkhard Ringlein,Jan van Lunteren,Radu Stoica,Thomas Parnell*

Main category: cs.LG

TL;DR: The paper details the development of an efficient and portable LLM inference platform using the Triton language, optimizing performance on both NVIDIA and AMD GPUs.


<details>
  <summary>Details</summary>
Motivation: To create a large language model (LLM) inference platform that works efficiently across hardware architectures without requiring low-level hand-tuning.

Method: Developed a paged attention kernel using Triton, a domain-specific language, focusing on algorithmic and system-level improvements and auto-tuning parameters for peak efficiency.

Result: Achieved state-of-the-art performance on both NVIDIA and AMD GPUs, boosting kernel efficiency from 19.7% to 105.9% of the previous state-of-the-art.

Conclusion: Open-source domain-specific languages like Triton can enable high-efficiency, portable model deployment across diverse GPU platforms.

Abstract: A long-standing goal in both industry and academia is to develop an LLM inference platform that is portable across hardware architectures, eliminates the need for low-level hand-tuning, and still delivers best-in-class efficiency. In this work, we demonstrate that portable, efficient cross-platform LLM inference is indeed possible and share our experience. We develop a state-of-the-art paged attention kernel, the core performance-critical component of many LLM deployments, that builds exclusively on the domain-specific just-in-time compiled language Triton to achieve state-of-the-art performance on both NVIDIA and AMD GPUs. We describe our high-level approach, the key algorithmic and system-level improvements, the parameter auto-tuning required to unlock efficiency, and the integrations into a popular inference server that are necessary to bring the performance of a generic Triton attention kernel from 19.7% of the state-of-the-art to 105.9%. Our results highlight how open-source domain-specific languages can be leveraged to unlock model portability across different GPU vendors.

</details>


### [528] [DAOpt: Modeling and Evaluation of Data-Driven Optimization under Uncertainty with LLMs](https://arxiv.org/abs/2511.11576)
*WenZhuo Zhu,Zheng Cui,Wenhan Lu,Sheng Liu,Yue Zhao*

Main category: cs.LG

TL;DR: This paper introduces a DAOpt framework that uses large language models for uncertain decision-making, proposing datasets and methods for evaluation and improving their robustness.


<details>
  <summary>Details</summary>
Motivation: To address the lack of research on applying large language models (LLMs) in uncertain real-world decision-making scenarios.

Method: Development of the DAOpt framework, which includes a new OptU dataset, a module for multi-agent decision-making, and simulation environments. It also incorporates few-shot learning integrated with stochastic and robust optimization domain knowledge.

Result: Introduction of a simulation-based evaluation framework to assess LLM performance in decision-making tasks focusing on out-of-sample feasibility and robustness.

Conclusion: The paper advances the capabilities of LLMs in handling uncertainty in optimization modeling, showcasing improved decision-making and adaptability to real-world stochastic environments.

Abstract: Recent advances in large language models (LLMs) have accelerated research on automated optimization modeling. While real-world decision-making is inherently uncertain, most existing work has focused on deterministic optimization with known parameters, leaving the application of LLMs in uncertain settings largely unexplored. To that end, we propose the DAOpt framework including a new dataset OptU, a multi-agent decision-making module, and a simulation environment for evaluating LLMs with a focus on out-of-sample feasibility and robustness. Additionally, we enhance LLMs' modeling capabilities by incorporating few-shot learning with domain knowledge from stochastic and robust optimization.

</details>


### [529] [Decoupling Positional and Symbolic Attention Behavior in Transformers](https://arxiv.org/abs/2511.11579)
*Felipe Urrutia,Jorge Salas,Alexander Kozachinskiy,Cristian Buc Calderon,Hector Pasten,Cristobal Rojas*

Main category: cs.LG

TL;DR: The paper explores Transformers' ability to encode positional and symbolic information, focusing on RoPE’s effectiveness in this regard. It provides theoretical and empirical analysis, develops metrics for behavior analysis, and links performance to frequency usage.


<details>
  <summary>Details</summary>
Motivation: To understand how Transformers encode positional and symbolic information, and investigate why Rotary Positional Encoding (RoPE) is effective.

Method: Developed general definitions of positional and symbolic head behavior, introduced metrics to quantify these behaviors, analyzed Transformer-based models using RoPE, and tested their performance on tasks designed to isolate positional or symbolic capabilities.

Result: Found a strong correlation between attention head behavior and frequency utilization, showing that Transformers' performance can be influenced by controlling the frequencies accessible to attention heads.

Conclusion: This work illuminates the functionality of RoPE by demonstrating its impact on Transformer behavior and provides insights into the role of frequency usage in positional and symbolic tasks.

Abstract: An important aspect subtending language understanding and production is the ability to independently encode positional and symbolic information of the words within a sentence. In Transformers, positional information is typically encoded using Positional Encodings (PEs). One such popular PE, namely Rotary PE (RoPE), has been widely used due to its empirical success. Recently, it has been argued that part of RoPE's success emerges from its ability to encode robust positional and semantic information using large and small frequencies, respectively. In this work, we perform a deeper dive into the positional versus symbolic dichotomy of attention heads behavior, both at the theoretical and empirical level. We provide general definitions of what it means for a head to behave positionally or symbolically, prove that these are two mutually exclusive behaviors and develop a metric to quantify them. We apply our framework to analyze Transformer-based LLMs using RoPE and find that all heads exhibit a strong correspondence between behavior and frequency use. Finally, we introduce canonical tasks designed to be either purely positional or symbolic, and demonstrate that the Transformer performance causally relates to the ability of attention heads to leverage the appropriate frequencies. In particular, we show that we can control the Transformer performance by controlling which frequencies the attention heads can access. Altogether, our work provides a detailed understanding of RoPE, and how its properties relate to model behavior.

</details>


### [530] [Self-Organization of Attractor Landscapes in High-Capacity Kernel Logistic Regression Hopfield Networks](https://arxiv.org/abs/2511.13053)
*Akira Tamamori*

Main category: cs.LG

TL;DR: The paper explores the enhanced storage capacity of kernel-based Hopfield networks through the introduction of a novel metric, 'Pinnacle Sharpness,' and presents the dynamics behind attractor stability.


<details>
  <summary>Details</summary>
Motivation: To understand the poorly explored dynamics that enable kernel-based Hopfield networks to achieve dramatically increased storage capacities.

Method: The authors conduct a geometric analysis of energy landscapes, introduce the 'Pinnacle Sharpness' metric, and analyze the interplay of 'driving' and 'feedback' forces to study the conditions for attractor stability in these networks.

Result: The study reveals a 'ridge of optimization,' where attractor stability is maximized under high storage and global kernel conditions, due to a strong anti-correlation between driving and feedback forces.

Conclusion: The network self-organizes by leveraging inter-pattern interactions to create a robust energy landscape, providing new insights for designing high-capacity associative memory systems.

Abstract: Kernel-based learning methods can dramatically increase the storage capacity of Hopfield networks, yet the dynamical mechanism behind this enhancement remains poorly understood. We address this gap by conducting a geometric analysis of the network's energy landscape. We introduce a novel metric, ``Pinnacle Sharpness,'' to quantify the local stability of attractors. By systematically varying the kernel width and storage load, we uncover a rich phase diagram of attractor shapes. Our central finding is the emergence of a ``ridge of optimization,'' where the network maximizes attractor stability under challenging high-load and global-kernel conditions. Through a theoretical decomposition of the landscape gradient into a direct ``driving'' force and an indirect ``feedback'' force, we reveal the origin of this phenomenon. The optimization ridge corresponds to a regime of strong anti-correlation between the two forces, where the direct force, amplified by the high storage load, dominates the opposing collective feedback force. This demonstrates a sophisticated self-organization mechanism: the network adaptively harnesses inter-pattern interactions as a cooperative feedback control system to sculpt a robust energy landscape. Our findings provide a new physical picture for the stability of high-capacity associative memories and offer principles for their design.

</details>


### [531] [Parallel and Multi-Stage Knowledge Graph Retrieval for Behaviorally Aligned Financial Asset Recommendations](https://arxiv.org/abs/2511.11583)
*Fernando Spadea,Oshani Seneviratne*

Main category: cs.LG

TL;DR: The paper presents RAG-FLARKO, a method that enhances financial recommendations by using a multi-stage retrieval process and structured knowledge graphs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the quality of financial recommendations made by LLMs by addressing challenges such as context limits, hallucinations, and lack of grounding in user behavior and market data.

Method: The method involves an extension to the previous FLARKO system. RAG-FLARKO uses a retrieval-augmented approach with multi-stage knowledge graph retrieval, focusing on behavioral relevance and temporal consistency for constructing a compact, grounded subgraph for LLMs.

Result: Empirical evaluation on real-world financial data shows that RAG-FLARKO significantly improves recommendation quality, achieving better profitability and behavioral alignment.

Conclusion: RAG-FLARKO demonstrates the capability of smaller, efficient models to deliver high-quality, behaviorally-aligned financial recommendations, enabling deployment in resource-constrained environments.

Abstract: Large language models (LLMs) show promise for personalized financial recommendations but are hampered by context limits, hallucinations, and a lack of behavioral grounding. Our prior work, FLARKO, embedded structured knowledge graphs (KGs) in LLM prompts to align advice with user behavior and market data. This paper introduces RAG-FLARKO, a retrieval-augmented extension to FLARKO, that overcomes scalability and relevance challenges using multi-stage and parallel KG retrieval processes. Our method first retrieves behaviorally relevant entities from a user's transaction KG and then uses this context to filter temporally consistent signals from a market KG, constructing a compact, grounded subgraph for the LLM. This pipeline reduces context overhead and sharpens the model's focus on relevant information. Empirical evaluation on a real-world financial transaction dataset demonstrates that RAG-FLARKO significantly enhances recommendation quality. Notably, our framework enables smaller, more efficient models to achieve high performance in both profitability and behavioral alignment, presenting a viable path for deploying grounded financial AI in resource-constrained environments.

</details>


### [532] [Output Supervision Can Obfuscate the Chain of Thought](https://arxiv.org/abs/2511.11584)
*Jacob Drori,Luke Marks,Bryce Woodworth,Alex Cloud,Alexander Matt Turner*

Main category: cs.LG

TL;DR: The paper examines the issue of obfuscated chain of thoughts (CoTs) in models, proposing two mitigation methods to improve both monitorability and task performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of obfuscated CoTs in models and ensure that CoTs remain monitorable by improving upon OpenAI's original methodology.

Method: The authors analyze two mechanisms causing obfuscated CoTs—model generalization towards safe-looking CoTs and reinforcement of safe-looking CoTs by safe outputs—and introduce two corresponding mitigation strategies.

Result: The proposed mitigations achieve better balance between task performance and monitorability compared to the baseline training approach.

Conclusion: The study reveals limitations in OpenAI's prior method for handling CoTs and presents improved mitigation strategies for balancing safety and performance.

Abstract: OpenAI (2025) showed that training against a chain of thought (CoT) monitor can cause obfuscated CoTs, which contain bad behavior the monitor cannot detect. They proposed to keep CoTs monitorable by training only against output monitors that do not have access to CoT. We show that such training can still cause obfuscated CoTs via two mechanisms. First, when a model is trained to produce a safe-looking output, that model may generalize to making its CoTs look safe. Second, since later tokens are conditioned on earlier ones, safe-looking CoTs may increase the likelihood of safe outputs, causing safe-looking CoTs to be reinforced. We introduce two mitigations to address these two issues, which achieve a Pareto improvement in terms of monitorability and task performance compared to regular training.

</details>


### [533] [Parameter-Efficient and Personalized Federated Training of Generative Models at the Edge](https://arxiv.org/abs/2511.11585)
*Kabir Khan,Manju Sarkar,Anita Kar,Suresh Ghosh*

Main category: cs.LG

TL;DR: The paper introduces FedGen-Edge, a federated learning framework that decouples a pre-trained backbone from lightweight client-side adapters to efficiently train generative models in federated settings.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in training large generative models (e.g., language/image models) in federated settings, such as high computation, communication costs, and heterogeneity.

Method: FedGen-Edge uses pre-trained global backbones paired with lightweight client-side adapters, utilizing Low-Rank Adaptation (LoRA) for efficient updates. Only adapters are federated to reduce uplink traffic.

Result: The approach reduces uplink traffic by over 99%, stabilizes aggregation under non-IID data, supports personalization, achieves better performance (perplexity/FID), and faster convergence on language and image tasks.

Conclusion: FedGen-Edge provides a scalable, privacy-preserving, and resource-efficient solution for implementing personalized federated learning of generative AI on edge devices.

Abstract: Large generative models (for example, language and diffusion models) enable high-quality text and image synthesis but are hard to train or adapt in cross-device federated settings due to heavy computation and communication and statistical/system heterogeneity. We propose FedGen-Edge, a framework that decouples a frozen, pre-trained global backbone from lightweight client-side adapters and federates only the adapters. Using Low-Rank Adaptation (LoRA) constrains client updates to a compact subspace, which reduces uplink traffic by more than 99 percent versus full-model FedAvg, stabilizes aggregation under non-IID data, and naturally supports personalization because each client can keep a locally tuned adapter. On language modeling (PTB) and image generation (CIFAR-10), FedGen-Edge achieves lower perplexity/FID and faster convergence than strong baselines while retaining a simple FedAvg-style server. A brief ablation shows diminishing returns beyond moderate LoRA rank and a trade-off between local epochs and client drift. FedGen-Edge offers a practical path toward privacy-preserving, resource-aware, and personalized generative AI on heterogeneous edge devices.

</details>


### [534] [WildfireGenome: Interpretable Machine Learning Reveals Local Drivers of Wildfire Risk and Their Cross-County Variation](https://arxiv.org/abs/2511.11589)
*Chenyue Liu,Ali Mostafavi*

Main category: cs.LG

TL;DR: The study introduces 'WildfireGenome', a framework for wildfire risk assessment that prioritizes interpretability at localized decision scales over regional metrics, leveraging fused data and machine learning methods.


<details>
  <summary>Details</summary>
Motivation: Current wildfire risk assessments lack decision-scale interpretability and rely on coarse hazard maps and non-transparent machine learning models, creating a need for tools that can provide granular, actionable insights.

Method: The paper describes WildfireGenome, which employs a PCA-based composite risk label, Random Forest classification models, and SHAP/ICE analyses across seven U.S. counties to analyze wildfire risk factors and their nonlinear relationships.

Result: The model achieves high accuracy (0.755-0.878) and kappa scores up to 0.951 in regions with ecological similarity, while identifying dominant wildfire drivers as needleleaf forest coverage and elevation. However, it struggles in ecologically dissimilar regions.

Conclusion: WildfireGenome bridges the gap between large-scale and localized wildfire predictions by offering interpretable risk analytics, paving the way for practical applications in vegetation management, land use, and infrastructure planning.

Abstract: Current wildfire risk assessments rely on coarse hazard maps and opaque machine learning models that optimize regional accuracy while sacrificing interpretability at the decision scale. WildfireGenome addresses these gaps through three components: (1) fusion of seven federal wildfire indicators into a sign-aligned, PCA-based composite risk label at H3 Level-8 resolution; (2) Random Forest classification of local wildfire risk; and (3) SHAP and ICE/PDP analyses to expose county-specific nonlinear driver relationships. Across seven ecologically diverse U.S. counties, models achieve accuracies of 0.755-0.878 and Quadratic Weighted Kappa up to 0.951, with principal components explaining 87-94% of indicator variance. Transfer tests show reliable performance between ecologically similar regions but collapse across dissimilar contexts. Explanations consistently highlight needleleaf forest cover and elevation as dominant drivers, with risk rising sharply at 30-40% needleleaf coverage. WildfireGenome advances wildfire risk assessment from regional prediction to interpretable, decision-scale analytics that guide vegetation management, zoning, and infrastructure planning.

</details>


### [535] [Mind Your Entropy: From Maximum Entropy to Trajectory Entropy-Constrained RL](https://arxiv.org/abs/2511.11592)
*Guojian Zhan,Likun Wang,Pengcheng Wang,Feihong Zhang,Jingliang Duan,Masayoshi Tomizuka,Shengbo Eben Li*

Main category: cs.LG

TL;DR: This paper addresses issues in maximum entropy reinforcement learning by proposing the TECRL framework which introduces trajectory entropy constraints, resulting in improved stability and performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address limitations in existing maximum entropy off-policy RL frameworks, especially non-stationary Q-values and short-sighted entropy tuning strategies.

Method: The proposed method involves introducing a trajectory entropy-constrained RL framework (TECRL) with two Q-functions—reward-based and entropy-based—and enforcing trajectory entropy constraints. This led to the development of DSAC-E, an improved algorithm.

Result: Empirical results show that DSAC-E achieved higher returns and better stability in the OpenAI Gym benchmark.

Conclusion: The TECRL framework and DSAC-E algorithm effectively improve the stability and performance of maximum entropy RL by addressing critical bottlenecks in entropy tuning and Q-value estimation.

Abstract: Maximum entropy has become a mainstream off-policy reinforcement learning (RL) framework for balancing exploitation and exploration. However, two bottlenecks still limit further performance improvement: (1) non-stationary Q-value estimation caused by jointly injecting entropy and updating its weighting parameter, i.e., temperature; and (2) short-sighted local entropy tuning that adjusts temperature only according to the current single-step entropy, without considering the effect of cumulative entropy over time. In this paper, we extends maximum entropy framework by proposing a trajectory entropy-constrained reinforcement learning (TECRL) framework to address these two challenges. Within this framework, we first separately learn two Q-functions, one associated with reward and the other with entropy, ensuring clean and stable value targets unaffected by temperature updates. Then, the dedicated entropy Q-function, explicitly quantifying the expected cumulative entropy, enables us to enforce a trajectory entropy constraint and consequently control the policy long-term stochasticity. Building on this TECRL framework, we develop a practical off-policy algorithm, DSAC-E, by extending the state-of-the-art distributional soft actor-critic with three refinements (DSAC-T). Empirical results on the OpenAI Gym benchmark demonstrate that our DSAC-E can achieve higher returns and better stability.

</details>


### [536] [Sound Logical Explanations for Mean Aggregation Graph Neural Networks](https://arxiv.org/abs/2511.11593)
*Matthew Morris,Ian Horrocks*

Main category: cs.LG

TL;DR: The paper analyzes explainability and expressivity of graph neural networks (GNNs) with mean aggregation and non-negative weights, presenting theoretical insights, experimental validation, and practical utility of sound rules.


<details>
  <summary>Details</summary>
Motivation: The study addresses the gap in explainability and expressivity results for GNNs using mean aggregation, which are frequently applied in knowledge graph completion but lack theoretical and practical insights.

Method: This research examines GNNs with mean aggregation and non-negative weights (MAGNNs), proving the monotonic rules they can learn. It introduces a fragment of first-order logic to explain MAGNN predictions and evaluates it through experiments.

Result: The experiments demonstrate that MAGNNs maintain or improve performance on inductive benchmarks compared to standard mean-aggregation networks. Sound rules are generated, insightful explanations are achieved, and model issues are revealed using these rules.

Conclusion: The study provides both theoretical and experimental evidence that MAGNNs can support explainability while preserving, or even enhancing, their predictive capabilities. The findings are practical for improving and interpreting GNN-based systems.

Abstract: Graph neural networks (GNNs) are frequently used for knowledge graph completion. Their black-box nature has motivated work that uses sound logical rules to explain predictions and characterise their expressivity. However, despite the prevalence of GNNs that use mean as an aggregation function, explainability and expressivity results are lacking for them. We consider GNNs with mean aggregation and non-negative weights (MAGNNs), proving the precise class of monotonic rules that can be sound for them, as well as providing a restricted fragment of first-order logic to explain any MAGNN prediction. Our experiments show that restricting mean-aggregation GNNs to have non-negative weights yields comparable or improved performance on standard inductive benchmarks, that sound rules are obtained in practice, that insightful explanations can be generated in practice, and that the sound rules can expose issues in the trained models.

</details>


### [537] [Loss Given Default Prediction Under Measurement-Induced Mixture Distributions: An Information-Theoretic Approach](https://arxiv.org/abs/2511.11596)
*Javier Marín*

Main category: cs.LG

TL;DR: LGD modeling faces challenges due to training data contamination, compromising model accuracy. Information-theoretic methods outperform traditional ones in generalization and predictive ability.


<details>
  <summary>Details</summary>
Motivation: Address data quality issues in LGD modeling caused by mixture-contamination in training data, which impacts model reliability and regulatory compliance.

Method: Analyze LGD models using both recursive partitioning (e.g., Random Forest) and information-theoretic approaches (Shannon entropy and mutual information). Test performance on 1,218 corporate bankruptcy cases.

Result: Random Forest underperformed (negative R-squared) while information-theoretic models achieved better metrics (R-squared of 0.191, RMSE of 0.284). Leverage had significant information contribution, refuting assumptions of scale dependence in recovery.

Conclusion: Information-theoretic models provide a robust alternative to traditional LGD models for contaminated training data scenarios, with implications for regulatory compliance and other domains with prolonged observation periods.

Abstract: Loss Given Default (LGD) modeling faces a fundamental data quality constraint: 90% of available training data consists of proxy estimates based on pre-distress balance sheets rather than actual recovery outcomes from completed bankruptcy proceedings. We demonstrate that this mixture-contaminated training structure causes systematic failure of recursive partitioning methods, with Random Forest achieving negative r-squared (-0.664, worse than predicting the mean) on held-out test data. Information-theoretic approaches based on Shannon entropy and mutual information provide superior generalization, achieving r-squared of 0.191 and RMSE of 0.284 on 1,218 corporate bankruptcies (1980-2023). Analysis reveals that leverage-based features contain 1.510 bits of mutual information while size effects contribute only 0.086 bits, contradicting regulatory assumptions about scale-dependent recovery. These results establish practical guidance for financial institutions deploying LGD models under Basel III requirements when representative outcome data is unavailable at sufficient scale. The findings generalize to medical outcomes research, climate forecasting, and technology reliability-domains where extended observation periods create unavoidable mixture structure in training data.

</details>


### [538] [Adaptive Stepsizing for Stochastic Gradient Langevin Dynamics in Bayesian Neural Networks](https://arxiv.org/abs/2511.11666)
*Rajit Rajpal,Benedict Leimkuhler,Yuanhao Jiang*

Main category: cs.LG

TL;DR: This paper presents SA-SGLD, an adaptive sampling scheme for Bayesian neural networks, improving stability and mixing during posterior sampling without introducing bias.


<details>
  <summary>Details</summary>
Motivation: Bayesian neural networks require efficient sampling algorithms to approximate posterior distributions, but existing methods like pSGLD face challenges with stability and invariant measure sampling.

Method: This paper introduces SA-SGLD, an adaptive scheme that uses time rescaling based on monitored quantities (gradient norm) to dynamically adjust the stepsize during sampling.

Result: Experiments demonstrate improved posterior sampling accuracy compared to SGLD in 2D toy examples and image classification tasks using sharp priors for BNNs.

Conclusion: SA-SGLD provides a robust adaptive sampling method for BNNs, addressing stability and curvature issues without costly bias corrections.

Abstract: Bayesian neural networks (BNNs) require scalable sampling algorithms to approximate posterior distributions over parameters. Existing stochastic gradient Markov Chain Monte Carlo (SGMCMC) methods are highly sensitive to the choice of stepsize and adaptive variants such as pSGLD typically fail to sample the correct invariant measure without addition of a costly divergence correction term. In this work, we build on the recently proposed `SamAdams' framework for timestep adaptation (Leimkuhler, Lohmann, and Whalley 2025), introducing an adaptive scheme: SA-SGLD, which employs time rescaling to modulate the stepsize according to a monitored quantity (typically the local gradient norm). SA-SGLD can automatically shrink stepsizes in regions of high curvature and expand them in flatter regions, improving both stability and mixing without introducing bias. We show that our method can achieve more accurate posterior sampling than SGLD on high-curvature 2D toy examples and in image classification with BNNs using sharp priors.

</details>


### [539] [Aspiration-based Perturbed Learning Automata in Games with Noisy Utility Measurements. Part A: Stochastic Stability in Non-zero-Sum Games](https://arxiv.org/abs/2511.11602)
*Georgios C. Chasparis*

Main category: cs.LG

TL;DR: The paper introduces a novel learning scheme (APLA) for distributed optimization in multi-player games, addressing limitations of standard reinforcement-based methods.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of existing reinforcement-based learning in distributed setups, ensuring convergence to pure Nash equilibria in broader game classes.

Method: Proposes the Aspiration-based Perturbed Learning Automata (APLA), analyzing stochastic stability and equivalence in games with noisy observations.

Result: APLA achieves stochastic stability in multi-player positive-utility games and further specializes its analysis to weakly acyclic games.

Conclusion: APLA improves the modeling and optimization in distributed multi-player scenarios by ensuring stability even in noisy environments.

Abstract: Reinforcement-based learning has attracted considerable attention both in modeling human behavior as well as in engineering, for designing measurement- or payoff-based optimization schemes. Such learning schemes exhibit several advantages, especially in relation to filtering out noisy observations. However, they may exhibit several limitations when applied in a distributed setup. In multi-player weakly-acyclic games, and when each player applies an independent copy of the learning dynamics, convergence to (usually desirable) pure Nash equilibria cannot be guaranteed. Prior work has only focused on a small class of games, namely potential and coordination games. To address this main limitation, this paper introduces a novel payoff-based learning scheme for distributed optimization, namely aspiration-based perturbed learning automata (APLA). In this class of dynamics, and contrary to standard reinforcement-based learning schemes, each player's probability distribution for selecting actions is reinforced both by repeated selection and an aspiration factor that captures the player's satisfaction level. We provide a stochastic stability analysis of APLA in multi-player positive-utility games under the presence of noisy observations. This is the first part of the paper that characterizes stochastic stability in generic non-zero-sum games by establishing equivalence of the induced infinite-dimensional Markov chain with a finite dimensional one. In the second part, stochastic stability is further specialized to weakly acyclic games.

</details>


### [540] [Coordinate Descent for Network Linearization](https://arxiv.org/abs/2511.11781)
*Vlad Rakhlin,Amir Jevnisek,Shai Avidan*

Main category: cs.LG

TL;DR: The paper focuses on optimizing ReLU activations in ResNet networks for Private Inference by introducing a Coordinate Descent method that works in the discrete domain, improving performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: ReLU activations significantly increase inference latency in Private Inference for ResNet networks, necessitating methods to reduce their count effectively without sacrificing accuracy.

Method: The paper uses a Coordinate Descent optimization framework that directly operates in the discrete domain to reduce ReLU activations while preserving accuracy, offering a sparse solution by design.

Result: Through extensive benchmarks, the proposed method outperforms current state-of-the-art approaches in optimizing ReLU count for Private Inference tasks.

Conclusion: The proposed discrete optimization method demonstrates superior results by effectively addressing ReLU bottlenecks and setting a new standard for Private Inference efficiency.

Abstract: ReLU activations are the main bottleneck in Private Inference that is based on ResNet networks. This is because they incur significant inference latency. Reducing ReLU count is a discrete optimization problem, and there are two common ways to approach it. Most current state-of-the-art methods are based on a smooth approximation that jointly optimizes network accuracy and ReLU budget at once. However, the last hard thresholding step of the optimization usually introduces a large performance loss. We take an alternative approach that works directly in the discrete domain by leveraging Coordinate Descent as our optimization framework. In contrast to previous methods, this yields a sparse solution by design. We demonstrate, through extensive experiments, that our method is State of the Art on common benchmarks.

</details>


### [541] [Enhancing failure prediction in nuclear industry: Hybridization of knowledge- and data-driven techniques](https://arxiv.org/abs/2511.11604)
*Amaratou Mahamadou Saley,Thierry Moyaux,Aïcha Sekhari,Vincent Cheutet,Jean-Baptiste Danielou*

Main category: cs.LG

TL;DR: This paper proposes a hybrid predictive maintenance methodology combining data-driven techniques with domain knowledge for the nuclear industry, outperforming purely data-driven methods in reliability and prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in predictive maintenance within the nuclear sector, which is critical for safety, economic efficiency, and operational cost reduction, by integrating data-driven methods with domain knowledge.

Method: The paper introduces a hybrid predictive maintenance approach that combines data-driven techniques and domain-specific knowledge, emphasizing the limitations of data-driven methods and the importance of expert knowledge.

Result: The hybrid methodology outperformed data-driven methods, evidenced by extending prediction from 3 to 24 hours and achieving a significantly higher F1 score (93.12% compared to 56.36%).

Conclusion: Incorporating domain knowledge enhances the effectiveness of predictive models in complex and sensitive industries like nuclear, ensuring better failure prediction and operational improvements.

Abstract: The convergence of the Internet of Things (IoT) and Industry 4.0 has significantly enhanced data-driven methodologies within the nuclear industry, notably enhancing safety and economic efficiency. This advancement challenges the precise prediction of future maintenance needs for assets, which is crucial for reducing downtime and operational costs. However, the effectiveness of data-driven methodologies in the nuclear sector requires extensive domain knowledge due to the complexity of the systems involved. Thus, this paper proposes a novel predictive maintenance methodology that combines data-driven techniques with domain knowledge from a nuclear equipment. The methodological originality of this paper is located on two levels: highlighting the limitations of purely data-driven approaches and demonstrating the importance of knowledge in enhancing the performance of the predictive models. The applicative novelty of this work lies in its use within a domain such as a nuclear industry, which is highly restricted and ultrasensitive due to security, economic and environmental concerns. A detailed real-world case study which compares the current state of equipment monitoring with two scenarios, demonstrate that the methodology significantly outperforms purely data-driven methods in failure prediction. While purely data-driven methods achieve only a modest performance with a prediction horizon limited to 3 h and a F1 score of 56.36%, the hybrid approach increases the prediction horizon to 24 h and achieves a higher F1 score of 93.12%.

</details>


### [542] [Clustering-Based Weight Orthogonalization for Stabilizing Deep Reinforcement Learning](https://arxiv.org/abs/2511.11607)
*Guoqing Ma,Yuhan Zhang,Yuming Dai,Guangfu Hao,Yang Chen,Shan Yu*

Main category: cs.LG

TL;DR: The paper introduces the Clustering Orthogonal Weight Modified (COWM) layer to address non-stationarity in RL environments, significantly improving learning efficiency.


<details>
  <summary>Details</summary>
Motivation: Many RL environments are non-stationary, leading to inefficiencies and the need for millions of iterations.

Method: The authors introduce the COWM layer, which combines clustering techniques and a projection matrix to stabilize learning and minimize gradient interference.

Result: The COWM layer achieves improvements in state-based and vision-based benchmarks (9% and 12.6% respectively) and shows robustness across tasks.

Conclusion: The COWM layer effectively mitigates non-stationarity in RL, improving efficiency, robustness, and generality in learning.

Abstract: Reinforcement learning (RL) has made significant advancements, achieving superhuman performance in various tasks. However, RL agents often operate under the assumption of environmental stationarity, which poses a great challenge to learning efficiency since many environments are inherently non-stationary. This non-stationarity results in the requirement of millions of iterations, leading to low sample efficiency. To address this issue, we introduce the Clustering Orthogonal Weight Modified (COWM) layer, which can be integrated into the policy network of any RL algorithm and mitigate non-stationarity effectively. The COWM layer stabilizes the learning process by employing clustering techniques and a projection matrix. Our approach not only improves learning speed but also reduces gradient interference, thereby enhancing the overall learning efficiency. Empirically, the COWM outperforms state-of-the-art methods and achieves improvements of 9% and 12.6% in vision based and state-based DMControl benchmark. It also shows robustness and generality across various algorithms and tasks.

</details>


### [543] [Small Vocabularies, Big Gains: Pretraining and Tokenization in Time Series Models](https://arxiv.org/abs/2511.11622)
*Alexis Roger,Gwen Legate,Kashif Rasul,Yuriy Nevmyvaka,Irina Rish*

Main category: cs.LG

TL;DR: This paper examines how tokenizer design and transfer learning influence time series forecasting models, emphasizing the role of tokenization in model stability and transfer learning in optimization. It highlights the synergy between well-designed, small vocabularies and pretraining benefits, particularly for multi-modal settings.


<details>
  <summary>Details</summary>
Motivation: To better understand the combined impact of tokenizer design (scaling and quantization) and transfer learning (pretraining) on building effective time series forecasting models.

Method: The study conducted empirical training experiments and theoretical analyses to systematically evaluate tokenizer configurations and transfer learning's effects on performance.

Result: The research found that well-designed tokenizers align with pretraining benefits, especially with smaller vocabularies, while poor tokenization negates pretraining advantages. Efficient tokenization is particularly beneficial in multi-modal forecasting scenarios.

Conclusion: Careful design of small, efficient tokenizers combined with pretrained weights significantly enhances time series forecasting models. This approach is especially valuable for multi-modal applications where vocabularies need to be shared across modalities.

Abstract: Tokenization and transfer learning are two critical components in building state of the art time series foundation models for forecasting. In this work, we systematically study the effect of tokenizer design, specifically scaling and quantization strategies, on model performance, alongside the impact of pretraining versus random initialization. We show that tokenizer configuration primarily governs the representational capacity and stability of the model, while transfer learning influences optimization efficiency and alignment. Using a combination of empirical training experiments and theoretical analyses, we demonstrate that pretrained models consistently leverage well-designed tokenizers more effectively, particularly at smaller vocabulary sizes. Conversely, misaligned tokenization can diminish or even invert the benefits of pretraining. These findings highlight the importance of careful tokenization in time series modeling and suggest that combining small, efficient vocabularies with pretrained weights is especially advantageous in multi-modal forecasting settings, where the overall vocabulary must be shared across modalities. Our results provide concrete guidance for designing tokenizers and leveraging transfer learning in discrete representation learning for continuous signals.

</details>


### [544] [Early GVHD Prediction in Liver Transplantation via Multi-Modal Deep Learning on Imbalanced EHR Data](https://arxiv.org/abs/2511.11623)
*Yushan Jiang,Shuteng Niu,Dongjin Song,Yichen Wang,Jingna Feng,Xinyue Hu,Liu Yang,Cui Tao*

Main category: cs.LG

TL;DR: This study developed a multi-modal deep learning framework to improve early prediction of graft-versus-host disease (GVHD) in liver transplantation patients, integrating diverse and imbalanced EHR data for better prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to enhance early prediction of GVHD in liver transplantation patients, a rare but often fatal condition, by leveraging advanced multi-modal deep learning to integrate and analyze heterogeneous and imbalanced EHR data.

Method: The study analyzed pre-transplant electronic health records of 2,100 liver transplantation patients, including 42 GVHD cases, considering demographics, laboratory tests, diagnoses, and medications. It employed a multi-modal deep learning model that dynamically fuses data, addresses missing records, and optimizes for class imbalance using AUC-based optimization.

Result: The multi-modal deep learning framework outperformed baseline methods, achieving an AUC of 0.836, an AUPRC of 0.157, a recall of 0.768, and a specificity of 0.803, effectively capturing complementary information from multiple modalities.

Conclusion: The multi-modal deep learning method significantly improves early prediction of GVHD in liver transplantation patients, overcoming challenges of EHR heterogeneity and extreme class imbalance. This approach demonstrates potential for improved patient outcomes.

Abstract: Graft-versus-host disease (GVHD) is a rare but often fatal complication in liver transplantation, with a very high mortality rate. By harnessing multi-modal deep learning methods to integrate heterogeneous and imbalanced electronic health records (EHR), we aim to advance early prediction of GVHD, paving the way for timely intervention and improved patient outcomes. In this study, we analyzed pre-transplant electronic health records (EHR) spanning the period before surgery for 2,100 liver transplantation patients, including 42 cases of graft-versus-host disease (GVHD), from a cohort treated at Mayo Clinic between 1992 and 2025. The dataset comprised four major modalities: patient demographics, laboratory tests, diagnoses, and medications. We developed a multi-modal deep learning framework that dynamically fuses these modalities, handles irregular records with missing values, and addresses extreme class imbalance through AUC-based optimization. The developed framework outperforms all single-modal and multi-modal machine learning baselines, achieving an AUC of 0.836, an AUPRC of 0.157, a recall of 0.768, and a specificity of 0.803. It also demonstrates the effectiveness of our approach in capturing complementary information from different modalities, leading to improved performance. Our multi-modal deep learning framework substantially improves existing approaches for early GVHD prediction. By effectively addressing the challenges of heterogeneity and extreme class imbalance in real-world EHR, it achieves accurate early prediction. Our proposed multi-modal deep learning method demonstrates promising results for early prediction of a GVHD in liver transplantation, despite the challenge of extremely imbalanced EHR data.

</details>


### [545] [MedFedPure: A Medical Federated Framework with MAE-based Detection and Diffusion Purification for Inference-Time Attacks](https://arxiv.org/abs/2511.11625)
*Mohammad Karami,Mohammad Reza Nemati,Aidin Kazemi,Ali Mikaeili Barzili,Hamid Azadegan,Behzad Moshiri*

Main category: cs.LG

TL;DR: This paper introduces MedFedPure, a defense framework for AI-based brain tumor detection via MRI, designed for federated learning settings to enhance robustness against adversarial attacks while maintaining privacy and accuracy.


<details>
  <summary>Details</summary>
Motivation: AI models in medical imaging face vulnerability to adversarial attacks during inference in federated learning settings, risking patient diagnosis, and this research aims to overcome those challenges.

Method: MedFedPure integrates three techniques: a personalized federated learning model, a Masked Autoencoder for detecting adversarial inputs, and an adaptive diffusion-based purification module to secure MRI scans before classification.

Result: The framework enhances adversarial robustness significantly, boosting performance under strong attacks from 49.50% to 87.33% and maintaining 97.67% clean accuracy, as demonstrated on the Br35H brain MRI dataset.

Conclusion: MedFedPure enables secure, robust, and privacy-preserving diagnostic AI tools, suitable for real-world clinical applications by addressing federated learning-specific challenges effectively.

Abstract: Artificial intelligence (AI) has shown great potential in medical imaging, particularly for brain tumor detection using Magnetic Resonance Imaging (MRI). However, the models remain vulnerable at inference time when they are trained collaboratively through Federated Learning (FL), an approach adopted to protect patient privacy. Adversarial attacks can subtly alter medical scans in ways invisible to the human eye yet powerful enough to mislead AI models, potentially causing serious misdiagnoses. Existing defenses often assume centralized data and struggle to cope with the decentralized and diverse nature of federated medical settings. In this work, we present MedFedPure, a personalized federated learning defense framework designed to protect diagnostic AI models at inference time without compromising privacy or accuracy. MedFedPure combines three key elements: (1) a personalized FL model that adapts to the unique data distribution of each institution; (2) a Masked Autoencoder (MAE) that detects suspicious inputs by exposing hidden perturbations; and (3) an adaptive diffusion-based purification module that selectively cleans only the flagged scans before classification. Together, these steps offer robust protection while preserving the integrity of normal, benign images. We evaluated MedFedPure on the Br35H brain MRI dataset. The results show a significant gain in adversarial robustness, improving performance from 49.50% to 87.33% under strong attacks, while maintaining a high clean accuracy of 97.67%. By operating locally and in real time during diagnosis, our framework provides a practical path to deploying secure, trustworthy, and privacy-preserving AI tools in clinical workflows.
  Index Terms: cancer, tumor detection, federated learning, masked autoencoder, diffusion, privacy

</details>


### [546] [Physics-Constrained Adaptive Neural Networks Enable Real-Time Semiconductor Manufacturing Optimization with Minimal Training Data](https://arxiv.org/abs/2511.12788)
*Rubén Darío Guerrero*

Main category: cs.LG

TL;DR: This paper addresses computational challenges in EUV lithography, introducing a physics-constrained adaptive learning framework for accurate optimization with reduced computational costs.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in resolving computational inefficiencies and achieving sub-nanometer precision in EUV lithography, which traditional methods fail to provide while consuming high computational resources.

Method: The paper presents an adaptive learning framework that integrates physics-constrained differentiable modules with learnable parameters to optimize Edge Placement Error (EPE) between simulated images and target patterns. It leverages cross-geometry generalization with minimal training data.

Result: The method successfully achieves sub-nanometer precision with EPE results ranging from 0.664-2.536 nm using limited training data, showing an average performance improvement of 69.9% compared to traditional CNN approaches and significant computational efficiency.

Conclusion: Physics-constrained adaptive learning proves to be a transformative methodology for semiconductor optimization, bridging the gap between academic physics-informed neural networks and practical industrial applications with enhanced precision and efficiency.

Abstract: The semiconductor industry faces a computational crisis in extreme ultraviolet (EUV) lithography optimization, where traditional methods consume billions of CPU hours while failing to achieve sub-nanometer precision. We present a physics-constrained adaptive learning framework that automatically calibrates electromagnetic approximations through learnable parameters $\boldsymbolθ = \{θ_d, θ_a, θ_b, θ_p, θ_c\}$ while simultaneously minimizing Edge Placement Error (EPE) between simulated aerial images and target photomasks. The framework integrates differentiable modules for Fresnel diffraction, material absorption, optical point spread function blur, phase-shift effects, and contrast modulation with direct geometric pattern matching objectives, enabling cross-geometry generalization with minimal training data. Through physics-constrained learning on 15 representative patterns spanning current production to future research nodes, we demonstrate consistent sub-nanometer EPE performance (0.664-2.536 nm range) using only 50 training samples per pattern. Adaptive physics learning achieves an average improvement of 69.9\% over CNN baselines without physics constraints, with a significant inference speedup over rigorous electromagnetic solvers after training completion. This approach requires 90\% fewer training samples through cross-geometry generalization compared to pattern-specific CNN training approaches. This work establishes physics-constrained adaptive learning as a foundational methodology for real-time semiconductor manufacturing optimization, addressing the critical gap between academic physics-informed neural networks and industrial deployment requirements through joint physics calibration and manufacturing precision objectives.

</details>


### [547] [Finding Time Series Anomalies using Granular-ball Vector Data Description](https://arxiv.org/abs/2511.12147)
*Lifeng Shen,Liang Peng,Ruiwen Liu,Shuyin Xia,Yi Liu*

Main category: cs.LG

TL;DR: The paper introduces Granular-ball One-Class Network (GBOC) for robust anomaly detection in dynamic, nonlinear time series, addressing limitations in traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional anomaly detection methods struggle in complex temporal data due to rigid assumptions like fixed clusters or neighbors.

Method: The GBOC method utilizes Granular-ball Vector Data Description (GVDD) that partitions data into compact, high-density regions (granular-balls) via a hierarchical splitting process. These granular-balls, serving as prototypes, are used to align samples during training and compute anomaly scores during inference.

Result: Experiments demonstrate that GBOC is effective and outperforms existing methods in handling time series anomaly detection challenges.

Conclusion: GBOC presents a robust and efficient anomaly detection framework by leveraging dense, high-quality regions, significantly improving performance and efficiency.

Abstract: Modeling normal behavior in dynamic, nonlinear time series data is challenging for effective anomaly detection. Traditional methods, such as nearest neighbor and clustering approaches, often depend on rigid assumptions, such as a predefined number of reliable neighbors or clusters, which frequently break down in complex temporal scenarios. To address these limitations, we introduce the Granular-ball One-Class Network (GBOC), a novel approach based on a data-adaptive representation called Granular-ball Vector Data Description (GVDD). GVDD partitions the latent space into compact, high-density regions represented by granular-balls, which are generated through a density-guided hierarchical splitting process and refined by removing noisy structures. Each granular-ball serves as a prototype for local normal behavior, naturally positioning itself between individual instances and clusters while preserving the local topological structure of the sample set. During training, GBOC improves the compactness of representations by aligning samples with their nearest granular-ball centers. During inference, anomaly scores are computed based on the distance to the nearest granular-ball. By focusing on dense, high-quality regions and significantly reducing the number of prototypes, GBOC delivers both robustness and efficiency in anomaly detection. Extensive experiments validate the effectiveness and superiority of the proposed method, highlighting its ability to handle the challenges of time series anomaly detection.

</details>


### [548] [SA-EMO: Structure-Aligned Encoder Mixture of Operators for Generalizable Full-waveform Inversion](https://arxiv.org/abs/2511.11627)
*Wang Zhenyu,Li Peiyuan,Shi Yongxiang,Wu Ruoyu,Zhang Lei*

Main category: cs.LG

TL;DR: The paper introduces a novel architecture, Structure-Aligned Encoder-Mixture-of-Operators (SA-EMO), which significantly improves full-waveform inversion, outperforming traditional methods in accuracy and generalization.


<details>
  <summary>Details</summary>
Motivation: Full-waveform inversion is critical for high-resolution subsurface modeling but faces challenges like being inherently ill-posed, nonlinear, computationally demanding, and poor at generalizing to complex geology in unknown settings.

Method: The authors propose SA-EMO, which uses a structure-aligned encoder to map seismic wavefields into a consistent latent space and an adaptive routing mechanism to combine multiple neural operators, enhancing prediction accuracy and generalization.

Result: This method achieves notable advancements, cutting the mean absolute error by approximately 58.443%, improving boundary resolution by 10.308%, and outperforming existing methods on benchmark datasets like OpenFWI and Marmousi2.

Conclusion: SA-EMO offers a scalable, efficient, and physically interpretable approach for full-waveform inversion, with significant performance gains showcased through extensive testing and ablation studies.

Abstract: Full-waveform inversion (FWI) can produce high-resolution subsurface models, yet it remains inherently ill-posed, highly nonlinear, and computationally intensive. Although recent deep learning and numerical acceleration methods have improved speed and scalability, they often rely on single CNN architectures or single neural operators, which struggle to generalize in unknown or complex geological settings and are ineffective at distinguishing diverse geological types. To address these issues, we propose a Structure-Aligned Encoder-Mixture-of-Operators (SA-EMO) architecture for velocity-field inversion under unknown subsurface structures. First, a structure-aligned encoder maps high-dimensional seismic wavefields into a physically consistent latent space, thereby eliminating spatio-temporal mismatch between the waveform and velocity domains, recovering high-frequency components, and enhancing feature generalization. Then, an adaptive routing mechanism selects and fuses multiple neural-operator experts, including spectral, wavelet, multiscale, and local operators, to predict the velocity model. We systematically evaluate our approach on the OpenFWI benchmark and the Marmousi2 dataset. Results show that SA-EMO significantly outperforms traditional CNN or single-operator methods, achieving an average MAE reduction of approximately 58.443% and an improvement in boundary resolution of about 10.308%. Ablation studies further reveal that the structure-aligned encoder, the expert-fusion mechanism, and the routing module each contribute markedly to the performance gains. This work introduces a new paradigm for efficient, scalable, and physically interpretable full-waveform inversion.

</details>


### [549] [Understanding InfoNCE: Transition Probability Matrix Induced Feature Clustering](https://arxiv.org/abs/2511.12180)
*Ge Cheng,Shuo Wang,Yun Zhang*

Main category: cs.LG

TL;DR: The paper explores the theoretical understanding of the InfoNCE objective in contrastive learning, proposing SC-InfoNCE, a loss function providing flexibility in feature similarity alignment.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of theoretical understanding of InfoNCE, the primary objective used in contrastive learning, and to improve its adaptability to different data distributions.

Method: The authors introduce a modeled feature space and transition probability matrix to analyze InfoNCE's clustering behavior and propose SC-InfoNCE with a scalable target matrix for flexible feature similarity alignment.

Result: SC-InfoNCE achieves strong, consistent performance across diverse datasets in image, graph, and text domains.

Conclusion: The SC-InfoNCE loss function enhances the InfoNCE framework by allowing flexible control over feature similarity, matching the training objective with downstream data properties effectively.

Abstract: Contrastive learning has emerged as a cornerstone of unsupervised representation learning across vision, language, and graph domains, with InfoNCE as its dominant objective. Despite its empirical success, the theoretical underpinnings of InfoNCE remain limited. In this work, we introduce an explicit feature space to model augmented views of samples and a transition probability matrix to capture data augmentation dynamics. We demonstrate that InfoNCE optimizes the probability of two views sharing the same source toward a constant target defined by this matrix, naturally inducing feature clustering in the representation space. Leveraging this insight, we propose Scaled Convergence InfoNCE (SC-InfoNCE), a novel loss function that introduces a tunable convergence target to flexibly control feature similarity alignment. By scaling the target matrix, SC-InfoNCE enables flexible control over feature similarity alignment, allowing the training objective to better match the statistical properties of downstream data. Experiments on benchmark datasets, including image, graph, and text tasks, show that SC-InfoNCE consistently achieves strong and reliable performance across diverse domains.

</details>


### [550] [Global Feature Enhancing and Fusion Framework for Strain Gauge Time Series Classification](https://arxiv.org/abs/2511.11629)
*Xu Zhang,Peng Wang,Chen Wang,Zhe Xu,Xiaohua Nie,Wei Wang*

Main category: cs.LG

TL;DR: The paper proposes a hypergraph-based framework to enhance recognition of time series strain gauge data by learning and fusing global features.


<details>
  <summary>Details</summary>
Motivation: The need for accurate strain gauge status (SGS) recognition arises from its crucial role in detecting failed mechanical components, especially in intelligent manufacturing, and avoiding potential accidents.

Method: The researchers combine feature engineering and high-order relationships learned through a hypergraph-based global feature learning and fusion framework to capture global features.

Result: Their framework demonstrates improved recognition accuracy and better generalization on industrial SGS and public UCR datasets.

Conclusion: Integrating global features into time series classification improves SGS recognition, addressing CNN limitations and achieving enhanced accuracy for intelligent manufacturing applications.

Abstract: Strain Gauge Status (SGS) recognition is crucial in the field of intelligent manufacturing based on the Internet of Things, as accurate identification helps timely detection of failed mechanical components, avoiding accidents. The loading and unloading sequences generated by strain gauges can be identified through time series classification (TSC) algorithms. Recently, deep learning models, e.g., convolutional neural networks (CNNs) have shown remarkable success in the TSC task, as they can extract discriminative local features from the subsequences to identify the time series. However, we observe that only the local features may not be sufficient for expressing the time series, especially when the local sub-sequences between different time series are very similar, e.g., SGS data of aircraft wings in static strength experiments. Nevertheless, CNNs suffer from the limitation in extracting global features due to the nature of convolution operations. For extracting global features to more comprehensively represent the SGS time series, we propose two insights: (i) Constructing global features through feature engineering. (ii) Learning high-order relationships between local features to capture global features. To realize and utilize them, we propose a hypergraph-based global feature learning and fusion framework, which learns and fuses global features for semantic consistency to enhance the representation of SGS time series, thereby improving recognition accuracy. Our method designs are validated on industrial SGS and public UCR datasets, showing better generalization for unseen data in SGS recognition.

</details>


### [551] [Predicting Grain Growth in Polycrystalline Materials Using Deep Learning Time Series Models](https://arxiv.org/abs/2511.11630)
*Eliane Younes,Elie Hachem,Marc Bernacki*

Main category: cs.LG

TL;DR: Grain growth prediction using deep learning was explored to optimize the microstructure evolution's efficiency. LSTM model was found highly accurate and computationally efficient compared to other architectures.


<details>
  <summary>Details</summary>
Motivation: Grain growth influences material properties, and its efficient prediction is crucial for microstructural engineering and process design.

Method: Deep learning architectures like RNN, LSTM, TCN, and transformers were tested using mean-field statistical descriptors derived from high-fidelity simulations. Recursive forecasting was employed.

Result: LSTM achieved accuracy above 90%, exhibited physical consistency, and reduced computation time drastically (seconds vs. 20 minutes). Other architectures diverged in long-term predictions.

Conclusion: LSTM-based grain growth forecasting demonstrates high potential for efficient and accurate microstructure predictions, supporting digital twin applications and process optimization.

Abstract: Grain Growth strongly influences the mechanical behavior of materials, making its prediction a key objective in microstructural engineering. In this study, several deep learning approaches were evaluated, including recurrent neural networks (RNN), long short-term memory (LSTM), temporal convolutional networks (TCN), and transformers, to forecast grain size distributions during grain growth. Unlike full-field simulations, which are computationally demanding, the present work relies on mean-field statistical descriptors extracted from high-fidelity simulations. A dataset of 120 grain growth sequences was processed into normalized grain size distributions as a function of time. The models were trained to predict future distributions from a short temporal history using a recursive forecasting strategy. Among the tested models, the LSTM network achieved the highest accuracy (above 90\%) and the most stable performance, maintaining physically consistent predictions over extended horizons while reducing computation time from about 20 minutes per sequence to only a few seconds, whereas the other architectures tended to diverge when forecasting further in time. These results highlight the potential of low-dimensional descriptors and LSTM-based forecasting for efficient and accurate microstructure prediction, with direct implications for digital twin development and process optimization.

</details>


### [552] [Toward Better Generalization in Few-Shot Learning through the Meta-Component Combination](https://arxiv.org/abs/2511.11632)
*Qiuhao Zeng*

Main category: cs.LG

TL;DR: The paper proposes a novel meta-learning algorithm to improve generalization in few-shot learning by exploring classifier substructures and combining meta-components with orthogonal regularization.


<details>
  <summary>Details</summary>
Motivation: Few-shot learning often struggles with generalization to unseen classes due to overfitting on deep metrics learned from seen classes.

Method: The proposed method decomposes classifiers into meta-components, which are disentangled using an orthogonal regularizer. These meta-components are learned across episodes on seen classes to capture diverse shared substructures.

Result: The experiments on few-shot learning benchmarks demonstrate that the proposed method achieves superior performance compared to existing approaches.

Conclusion: Utilizing orthogonal regularization to learn diverse and shared classifier substructures improves generalization in few-shot learning tasks.

Abstract: In few-shot learning, classifiers are expected to generalize to unseen classes given only a small number of instances of each new class. One of the popular solutions to few-shot learning is metric-based meta-learning. However, it highly depends on the deep metric learned on seen classes, which may overfit to seen classes and fail to generalize well on unseen classes. To improve the generalization, we explore the substructures of classifiers and propose a novel meta-learning algorithm to learn each classifier as a combination of meta-components. Meta-components are learned across meta-learning episodes on seen classes and disentangled by imposing an orthogonal regularizer to promote its diversity and capture various shared substructures among different classifiers. Extensive experiments on few-shot benchmark tasks show superior performances of the proposed method.

</details>


### [553] [Cross-view Joint Learning for Mixed-Missing Multi-view Unsupervised Feature Selection](https://arxiv.org/abs/2511.12261)
*Zongxin Shen,Yanyong Huang,Dongjie Wang,Jinyuan Chang,Fengmao Lv,Tianrui Li,Xiaoyi Jiang*

Main category: cs.LG

TL;DR: The paper introduces CLIM-FS, a method to improve incomplete multi-view feature selection with adaptive imputation handling mixed-missing data.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to handle mixed-missing scenarios, lack utilization of inter-view relationships, and lack theoretical analysis of imputation-feature selection interaction.

Method: CLIM-FS integrates adaptive imputation of missing views/variables with feature selection, leveraging consensus cluster and cross-view local structures within nonnegative orthogonal matrix factorization.

Result: CLIM-FS demonstrated superior performance on eight real-world multi-view datasets compared to state-of-the-art methods.

Conclusion: CLIM-FS addresses mixed-missing challenges in IMUFS, using joint learning of imputation and feature selection, with theoretical insight into their interaction.

Abstract: Incomplete multi-view unsupervised feature selection (IMUFS), which aims to identify representative features from unlabeled multi-view data containing missing values, has received growing attention in recent years. Despite their promising performance, existing methods face three key challenges: 1) by focusing solely on the view-missing problem, they are not well-suited to the more prevalent mixed-missing scenario in practice, where some samples lack entire views or only partial features within views; 2) insufficient utilization of consistency and diversity across views limits the effectiveness of feature selection; and 3) the lack of theoretical analysis makes it unclear how feature selection and data imputation interact during the joint learning process. Being aware of these, we propose CLIM-FS, a novel IMUFS method designed to address the mixed-missing problem. Specifically, we integrate the imputation of both missing views and variables into a feature selection model based on nonnegative orthogonal matrix factorization, enabling the joint learning of feature selection and adaptive data imputation. Furthermore, we fully leverage consensus cluster structure and cross-view local geometrical structure to enhance the synergistic learning process. We also provide a theoretical analysis to clarify the underlying collaborative mechanism of CLIM-FS. Experimental results on eight real-world multi-view datasets demonstrate that CLIM-FS outperforms state-of-the-art methods.

</details>


### [554] [An Explainable and Fair AI Tool for PCOS Risk Assessment: Calibration, Subgroup Equity, and Interactive Clinical Deployment](https://arxiv.org/abs/2511.11636)
*Asma Sadia Khan,Sadia Tabassum*

Main category: cs.LG

TL;DR: The paper develops a fairness-audited, interpretable machine learning framework to predict polycystic ovary syndrome (PCOS) with high performance and diagnostic insight.


<details>
  <summary>Details</summary>
Motivation: To create a reliable and interpretable ML framework addressing diagnostic disparities while demonstrating actionable and clinically relevant insights for PCOS.

Method: Machine learning (Random Forest, SVM, XGBoost) was combined with SHAP feature attribution and demographic auditing to connect predictive explanations and disparities. Calibration metrics (Brier Score, ECE) were applied for fairness and reliability evaluation. Implementation on a Streamlit-based interface facilitates usability.

Result: The Random Forest model achieved high accuracy (90.8%), identified impactful features like follicle count and weight gain, showed age-related subgroup performance disparities, and demonstrated phenotype robustness. SVM displayed the lowest calibration error but lacked interpretability.

Conclusion: The calibrated Random Forest offers a balanced trade-off between accuracy, interpretability, and fairness. Its web-based interface ensures practical utility in clinical settings, offering insights into PCOS diagnosis and risk prediction.

Abstract: This paper presents a fairness-audited and interpretable machine learning framework for predicting polycystic ovary syndrome (PCOS), designed to evaluate model performance and identify diagnostic disparities across patient subgroups. The framework integrated SHAP-based feature attributions with demographic audits to connect predictive explanations with observed disparities for actionable insights. Probabilistic calibration metrics (Brier Score and Expected Calibration Error) are incorporated to ensure reliable risk predictions across subgroups. Random Forest, SVM, and XGBoost models were trained with isotonic and Platt scaling for calibration and fairness comparison. A calibrated Random Forest achieved a high predictive accuracy of 90.8%. SHAP analysis identified follicle count, weight gain, and menstrual irregularity as the most influential features, which are consistent with the Rotterdam diagnostic criteria. Although the SVM with isotonic calibration achieved the lowest calibration error (ECE = 0.0541), the Random Forest model provided a better balance between calibration and interpretability (Brier = 0.0678, ECE = 0.0666). Therefore, it was selected for detailed fairness and SHAP analyses. Subgroup analysis revealed that the model performed best among women aged 25-35 (accuracy 90.9%) but underperformed in those under 25 (69.2%), highlighting age-related disparities. The model achieved perfect precision in obese women and maintained high recall in lean PCOS cases, demonstrating robustness across phenotypes. Finally, a Streamlit-based web interface enables real-time PCOS risk assessment, Rotterdam criteria evaluation, and interactive 'what-if' analysis, bridging the gap between AI research and clinical usability.

</details>


### [555] [Optimal Self-Consistency for Efficient Reasoning with Large Language Models](https://arxiv.org/abs/2511.12309)
*Austin Feng,Marius Alonso,Ambroise Odonnat*

Main category: cs.LG

TL;DR: The paper analyzes self-consistency (SC) in chain-of-thought reasoning, introduces Blend-ASC for efficiency, and achieves significant improvements in sample usage.


<details>
  <summary>Details</summary>
Motivation: Improve efficiency and scalability of self-consistency in chain-of-thought reasoning and provide theoretical insights on its performance.

Method: The paper analyzes SC using mode estimation and voting theory, derives scaling laws, and proposes a novel variant called Blend-ASC, which dynamically allocates samples during inference.

Result: Blend-ASC achieves state-of-the-art sample efficiency, using 6.8x fewer samples compared to standard SC while maintaining performance. It outperforms fixed- and dynamic-allocation SC baselines.

Conclusion: Blend-ASC enhances SC's applicability and efficiency by eliminating hyperparameters and fitting any sample budget, making it adaptable and resource-efficient.

Abstract: Self-consistency (SC) is a widely used test-time inference technique for improving performance in chain-of-thought reasoning. It involves generating multiple responses, or samples from a large language model (LLM) and selecting the most frequent answer. This procedure can naturally be viewed as a majority vote or empirical mode estimation. Despite its effectiveness, SC is prohibitively expensive at scale when naively applied to datasets, and it lacks a unified theoretical treatment of sample efficiency and scaling behavior. In this paper, we provide the first comprehensive analysis of SC's scaling behavior and its variants, drawing on mode estimation and voting theory. We derive and empirically validate power law scaling for self-consistency across datasets, and analyze the sample efficiency for fixed-allocation and dynamic-allocation sampling schemes. From these insights, we introduce Blend-ASC, a novel variant of self-consistency that dynamically allocates samples to questions during inference, achieving state-of-the-art sample efficiency. Our approach uses 6.8x fewer samples than vanilla SC on average, outperforming both fixed- and dynamic-allocation SC baselines, thereby demonstrating the superiority of our approach in terms of efficiency. In contrast to existing variants, Blend-ASC is hyperparameter-free and can fit an arbitrary sample budget, ensuring it can be easily applied to any self-consistency application.

</details>


### [556] [Enhancing PINN Accuracy for the RLW Equation: Adaptive and Conservative Approaches](https://arxiv.org/abs/2511.11638)
*Aamir Shehzad*

Main category: cs.LG

TL;DR: This paper explores improved Physics-Informed Neural Networks (PINNs) for solving the regularized long wave (RLW) equation, showing that adaptive and conservative approaches address problem-specific challenges effectively.


<details>
  <summary>Details</summary>
Motivation: The research aims to address the shortcomings of standard PINN implementations, particularly their high error rates in solving the RLW equation, to improve solution accuracy for time-dependent nonlinear problems.

Method: The study introduces two enhanced approaches for PINNs: (1) an adaptive method with self-adaptive loss weighting, and (2) a conservative method that explicitly enforces conservation laws. Three benchmark tests were used for evaluation.

Result: Results show that the adaptive PINN outperforms in solving nonlinear interactions like soliton collisions, while the conservative PINN excels in long-term behavior problems. Both approaches achieved accuracy within $O(10^{-5})$ of benchmark numerical solutions.

Conclusion: Adaptive and conservative PINNs offer problem-specific advantages, but enforcing conservation laws may hinder performance for highly nonlinear systems, requiring special training strategies. This study provides valuable guidelines for designing PINNs for specific problems.

Abstract: Standard physics-informed neural network implementations have produced large error rates when using these models to solve the regularized long wave (RLW) equation. Two improved PINN approaches were developed in this research: an adaptive approach with self-adaptive loss weighting and a conservative approach enforcing explicit conservation laws. Three benchmark tests were used to demonstrate how effective PINN's are as they relate to the type of problem being solved (i.e., time dependent RLW equation). The first was a single soliton traveling along a line (propagation), the second was the interaction between two solitons, and the third was the evolution of an undular bore over the course of $t=250$. The results demonstrated that the effectiveness of PINNs are problem specific. The adaptive PINN was significantly better than both the conservative PINN and the standard PINN at solving problems involving complex nonlinear interactions such as colliding two solitons. The conservative approach was significantly better at solving problems involving long term behavior of single solitons and undular bores. However, the most important finding from this research is that explicitly enforcing conservation laws may be harmful to optimizing the solution of highly nonlinear systems of equations and therefore requires special training methods. The results from our adaptive and conservative approaches were within $O(10^{-5})$ of established numerical solutions for the same problem, thus demonstrating that PINNs can provide accurate solutions to complex systems of partial differential equations without the need for a discretization of space or time (mesh free). Moreover, the finding from this research challenges the assumptions that conservation enforcement will always improve the performance of a PINN and provides researchers with guidelines for designing PINNs for use on specific types of problems.

</details>


### [557] [Center-Outward q-Dominance: A Sample-Computable Proxy for Strong Stochastic Dominance in Multi-Objective Optimisation](https://arxiv.org/abs/2511.12545)
*Robin van der Laag,Hao Wang,Thomas Bäck,Yingjie Fan*

Main category: cs.LG

TL;DR: The paper introduces center-outward q-dominance based on optimal transport for stochastic multi-objective optimization, offering stronger and more reliable comparison methods than scalarization.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of scalarization in stochastic multi-objective optimization by introducing a more robust dominance relation for ranking multivariate distributions.

Method: The authors propose center-outward q-dominance, prove its relationship to first-order stochastic dominance, develop an empirical test procedure, and derive a threshold for Type I error control.

Result: The proposed method is effective in both hyperparameter tuning (analyzing stochastic Pareto sets) and replacing mean value-based selection in optimization algorithms, demonstrating superior performance and reliability.

Conclusion: Center-outward q-dominance is a validated, principled method for identifying stochastically dominant solutions in SMOOPs and offers practical improvements in optimization tasks.

Abstract: Stochastic multi-objective optimization (SMOOP) requires ranking multivariate distributions; yet, most empirical studies perform scalarization, which loses information and is unreliable. Based on the optimal transport theory, we introduce the center-outward q-dominance relation and prove it implies strong first-order stochastic dominance (FSD). Also, we develop an empirical test procedure based on q-dominance, and derive an explicit sample size threshold, $n^*(δ)$, to control the Type I error. We verify the usefulness of our approach in two scenarios: (1) as a ranking method in hyperparameter tuning; (2) as a selection method in multi-objective optimization algorithms. For the former, we analyze the final stochastic Pareto sets of seven multi-objective hyperparameter tuners on the YAHPO-MO benchmark tasks with q-dominance, which allows us to compare these tuners when the expected hypervolume indicator (HVI, the most common performance metric) of the Pareto sets becomes indistinguishable. For the latter, we replace the mean value-based selection in the NSGA-II algorithm with $q$-dominance, which shows a superior convergence rate on noise-augmented ZDT benchmark problems. These results establish center-outward q-dominance as a principled, tractable foundation for seeking truly stochastically dominant solutions for SMOOPs.

</details>


### [558] [A Deep Learning Model to Predicting Changes in Consumer Attributes for New Line-extended Products](https://arxiv.org/abs/2511.11646)
*Li Yinxing,Tsukasa Ishigaki*

Main category: cs.LG

TL;DR: The paper introduces a novel deep learning model, CTVAE, to predict consumer attributes for new product line extensions, demonstrating its potential for effective marketing strategies.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of predicting consumer preferences and ensuring product line extensions do not harm brand image.

Method: The Conditional Tabular Variational Auto-Encoder (CTVAE) was developed to generate synthetic data from consumer and product data and predict changes in consumer attributes.

Result: CTVAE outperformed existing models in predictive accuracy and provided insights for marketing strategies, such as avoiding cannibalization and optimizing product design.

Conclusion: CTVAE is a valuable tool for marketers, aiding in effective product line marketing and maintaining brand integrity.

Abstract: Product line extension is a marketing strategy that enhances a company's sphere of influence. Because excessive line extensions disrupt brand image, only appropriate line extensions based on consumer needs are desirable. Marketers should know the key consumer attributes of the primary customers for new line-extended products before companies enter the market. This paper describes a method for predicting changes in consumer attributes for new line-extended products using a novel deep learning model. The proposed model, Conditional Tabular Variational Auto-Encoder (CTVAE), generates synthetic data from large-scale tabular data of consumers and products. It can provide various implications about effective product line marketing for marketers. The experimental results demonstrate that the CTVAE offers superior prediction performance than existing models. We indicate implications for new products that change containers or flavors for effective product line marketing. The proposed approach has the potential to contribute to avoiding cannibalization and to designing product images and marketing strategies.

</details>


### [559] [Environment-Aware Transfer Reinforcement Learning for Sustainable Beam Selection](https://arxiv.org/abs/2511.11647)
*Dariush Salami,Ramin Hashemi,Parham Kazemi,Mikko A. Uusitalo*

Main category: cs.LG

TL;DR: This paper introduces a sustainable beam selection framework for 5G networks using transfer learning and RL, achieving high performance with reduced training time and energy costs.


<details>
  <summary>Details</summary>
Motivation: The need for scalable and energy-efficient RL-based beam selection in diverse environments due to extensive resource demands in traditional methods.

Method: Modeling the environment as a point cloud representing gNodeBs and scatterers and using Chamfer distance for identifying structurally similar environments for transfer learning.

Result: The method reduces training time and computational overhead by 16x, improves energy efficiency, and maintains high performance in beam selection.

Conclusion: The approach enhances scalability, energy efficiency, and sustainability in 5G AI-driven systems while reducing environmental impact and supporting green AI practices.

Abstract: This paper presents a novel and sustainable approach for improving beam selection in 5G and beyond networks using transfer learning and Reinforcement Learning (RL). Traditional RL-based beam selection models require extensive training time and computational resources, particularly when deployed in diverse environments with varying propagation characteristics posing a major challenge for scalability and energy efficiency. To address this, we propose modeling the environment as a point cloud, where each point represents the locations of gNodeBs (gNBs) and surrounding scatterers. By computing the Chamfer distance between point clouds, structurally similar environments can be efficiently identified, enabling the reuse of pre-trained models through transfer learning. This methodology leads to a 16x reduction in training time and computational overhead, directly contributing to energy efficiency. By minimizing the need for retraining in each new deployment, our approach significantly lowers power consumption and supports the development of green and sustainable Artificial Intelligence (AI) in wireless systems. Furthermore, it accelerates time-to-deployment, reduces carbon emissions associated with training, and enhances the viability of deploying AI-driven communication systems at the edge. Simulation results confirm that our approach maintains high performance while drastically cutting energy costs, demonstrating the potential of transfer learning to enable scalable, adaptive, and environmentally conscious RL-based beam selection strategies in dynamic and diverse propagation environments.

</details>


### [560] [Sample Complexity of Agnostic Multiclass Classification: Natarajan Dimension Strikes Back](https://arxiv.org/abs/2511.12659)
*Alon Cohen,Liad Erez,Steve Hanneke,Tomer Koren,Yishay Mansour,Shay Moran,Qian Zhang*

Main category: cs.LG

TL;DR: The paper establishes that agnostic multiclass PAC learning is governed by two dimensions (DS and Nat), not one as previously thought.


<details>
  <summary>Details</summary>
Motivation: To address the challenging extension of binary statistical learning to multiclass classification and better understand the role of DS and Nat dimensions in the agnostic learning setting.

Method: The authors provide rigorous agnostic sample complexity bounds, introduce innovative techniques including a self-adaptive multiplicative-weights algorithm, and depart from traditional learning methods.

Result: The sample complexity of multiclass agnostic learning is found to depend on both the DS dimension and Natarajan dimension, with derived bounds that highlight their complementary roles.

Conclusion: Multiclass PAC learning fundamentally differs from binary learning, requiring two structural parameters to understand its behavior and complexity.

Abstract: The fundamental theorem of statistical learning states that binary PAC learning is governed by a single parameter -- the Vapnik-Chervonenkis (VC) dimension -- which determines both learnability and sample complexity. Extending this to multiclass classification has long been challenging, since Natarajan's work in the late 80s proposing the Natarajan dimension (Nat) as a natural analogue of VC. Daniely and Shalev-Shwartz (2014) introduced the DS dimension, later shown by Brukhim et al. (2022) to characterize multiclass learnability. Brukhim et al. also showed that Nat and DS can diverge arbitrarily, suggesting that multiclass learning is governed by DS rather than Nat. We show that agnostic multiclass PAC sample complexity is in fact governed by two distinct dimensions. Specifically, we prove nearly tight agnostic sample complexity bounds that, up to log factors, take the form $\frac{DS^{1.5}}ε + \frac{Nat}{ε^2}$ where $ε$ is the excess risk. This bound is tight up to a $\sqrt{DS}$ factor in the first term, nearly matching known $Nat/ε^2$ and $DS/ε$ lower bounds. The first term reflects the DS-controlled regime, while the second shows that the Natarajan dimension still dictates asymptotic behavior for small $ε$. Thus, unlike binary or online classification -- where a single dimension (VC or Littlestone) controls both phenomena -- multiclass learning inherently involves two structural parameters. Our technical approach departs from traditional agnostic learning methods based on uniform convergence or reductions to realizable cases. A key ingredient is a novel online procedure based on a self-adaptive multiplicative-weights algorithm performing a label-space reduction, which may be of independent interest.

</details>


### [561] [Lightweight Time Series Data Valuation on Time Series Foundation Models via In-Context Finetuning](https://arxiv.org/abs/2511.11648)
*Shunyu Wu,Tianyue Li,Yixuan Leng,Jingyi Suo,Jian Lou,Dan Li,See-Kiong Ng*

Main category: cs.LG

TL;DR: The paper introduces LTSV, a method for efficient time series data valuation for foundation models using in-context finetuning and temporal block aggregation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the computational inefficiency and lack of temporal dependency preservation in traditional data valuation methods for Time Series Foundation Models (TSFMs).

Method: LTSV uses in-context finetuning to approximate the influence function, measuring context loss changes, and introduces temporal block aggregation to account for temporal dependencies.

Result: LTSV demonstrates reliable and effective data valuation with lower computational needs across various datasets and models.

Conclusion: In-context finetuning is a practical and efficient method to link data attribution with model generalization, advancing time series learning with TSFMs.

Abstract: Time series foundation models (TSFMs) have demonstrated increasing capabilities due to their extensive pretraining on large volumes of diverse time series data. Consequently, the quality of time series data is crucial to TSFM performance, rendering an accurate and efficient data valuation of time series for TSFMs indispensable. However, traditional data valuation methods, such as influence functions, face severe computational bottlenecks due to their poor scalability with growing TSFM model sizes and often fail to preserve temporal dependencies. In this paper, we propose LTSV, a Lightweight Time Series Valuation on TSFMS via in-context finetuning. Grounded in the theoretical evidence that in-context finetuning approximates the influence function, LTSV estimates a sample's contribution by measuring the change in context loss after in-context finetuning, leveraging the strong generalization capabilities of TSFMs to produce robust and transferable data valuations. To capture temporal dependencies, we introduce temporal block aggregation, which integrates per-block influence scores across overlapping time windows. Experiments across multiple time series datasets and models demonstrate that LTSV consistently provides reliable and strong valuation performance, while maintaining manageable computational requirements. Our results suggest that in-context finetuning on time series foundation models provides a practical and effective bridge between data attribution and model generalization in time series learning.

</details>


### [562] [A Closer Look at Personalized Fine-Tuning in Heterogeneous Federated Learning](https://arxiv.org/abs/2511.12695)
*Minghui Chen,Hrad Ghoukasian,Ruinan Jin,Zehua Wang,Sai Praneeth Karimireddy,Xiaoxiao Li*

Main category: cs.LG

TL;DR: Proposed a new approach (LP-FT) for addressing challenges in FL, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: FL struggles with balancing global model generalization and local personalization due to non-identical client data distributions.

Method: Introduced LP-FT to mitigate federated feature distortion, evaluated systematically across datasets and variants, including theoretical analysis.

Result: LP-FT proved effective in personalization and generalization balance, mitigating feature distortion and outperforming standard fine-tuning.

Conclusion: LP-FT provides robust guidelines for personalization in FL, especially under challenging conditions like feature overlap and domain shifts.

Abstract: Federated Learning (FL) enables decentralized, privacy-preserving model training but struggles to balance global generalization and local personalization due to non-identical data distributions across clients. Personalized Fine-Tuning (PFT), a popular post-hoc solution, fine-tunes the final global model locally but often overfits to skewed client distributions or fails under domain shifts. We propose adapting Linear Probing followed by full Fine-Tuning (LP-FT), a principled centralized strategy for alleviating feature distortion (Kumar et al., 2022), to the FL setting. Through systematic evaluation across seven datasets and six PFT variants, we demonstrate LP-FT's superiority in balancing personalization and generalization. Our analysis uncovers federated feature distortion, a phenomenon where local fine-tuning destabilizes globally learned features, and theoretically characterizes how LP-FT mitigates this via phased parameter updates. We further establish conditions (e.g., partial feature overlap, covariate-concept shift) under which LP-FT outperforms standard fine-tuning, offering actionable guidelines for deploying robust personalization in FL.

</details>


### [563] [Enhanced Water Leak Detection with Convolutional Neural Networks and One-Class Support Vector Machine](https://arxiv.org/abs/2511.11650)
*Daniele Ugo Leonzio,Paolo Bestagini,Marco Marcon,Stefano Tubaro*

Main category: cs.LG

TL;DR: This paper introduces a data-driven approach for detecting leaks in Water Distribution Networks (WDNs) using pressure measurements and one-class Support Vector Machines (SVM).


<details>
  <summary>Details</summary>
Motivation: Efficient water management is critical due to significant water loss caused by leaks in WDNs. There is a need for robust leak detection systems.

Method: The method uses a feature extractor and one-class SVM trained solely on no-leak pressure data from WDN nodes. It treats deviations as anomalies, indicating leaks.

Result: Using a simulated dataset on the Modena WDN, the proposed method outperforms recent leak detection techniques.

Conclusion: The data-driven approach effectively detects leaks in WDNs and offers improved performance over existing methods.

Abstract: Water is a critical resource that must be managed efficiently. However, a substantial amount of water is lost each year due to leaks in Water Distribution Networks (WDNs). This underscores the need for reliable and effective leak detection and localization systems. In recent years, various solutions have been proposed, with data-driven approaches gaining increasing attention due to their superior performance. In this paper, we propose a new method for leak detection. The method is based on water pressure measurements acquired at a series of nodes of a WDN. Our technique is a fully data-driven solution that makes only use of the knowledge of the WDN topology, and a series of pressure data acquisitions obtained in absence of leaks. The proposed solution is based on an feature extractor and a one-class Support Vector Machines (SVM) trained on no-leak data, so that leaks are detected as anomalies. The results achieved on a simulate dataset using the Modena WDN demonstrate that the proposed solution outperforms recent methods for leak detection.

</details>


### [564] [Conformal Online Learning of Deep Koopman Linear Embeddings](https://arxiv.org/abs/2511.12760)
*Ben Gao,Jordan Patracone,Stéphane Chrétien,Olivier Alata*

Main category: cs.LG

TL;DR: COLoKe is an adaptive Koopman embedding framework that updates representations from streaming data, balancing accuracy and reduced overfitting.


<details>
  <summary>Details</summary>
Motivation: To improve Koopman embeddings for nonlinear dynamical systems by ensuring robustness against overfitting and avoiding unnecessary computational updates.

Method: COLoKe combines deep learning for feature modeling with multistep prediction analysis in a lifted linear space. It uses a dynamic threshold-based conformal mechanism for adaptive updates.

Result: COLoKe demonstrated high predictive accuracy on benchmark systems while reducing unnecessary updates and mitigating overfitting issues.

Conclusion: Using COLoKe allows for efficient and accurate modeling of nonlinear systems with stable prediction performance and minimized computational redundancy.

Abstract: We introduce Conformal Online Learning of Koopman embeddings (COLoKe), a novel framework for adaptively updating Koopman-invariant representations of nonlinear dynamical systems from streaming data. Our modeling approach combines deep feature learning with multistep prediction consistency in the lifted space, where the dynamics evolve linearly. To prevent overfitting, COLoKe employs a conformal-style mechanism that shifts the focus from evaluating the conformity of new states to assessing the consistency of the current Koopman model. Updates are triggered only when the current model's prediction error exceeds a dynamically calibrated threshold, allowing selective refinement of the Koopman operator and embedding. Empirical results on benchmark dynamical systems demonstrate the effectiveness of COLoKe in maintaining long-term predictive accuracy while significantly reducing unnecessary updates and avoiding overfitting.

</details>


### [565] [Incomplete Depression Feature Selection with Missing EEG Channels](https://arxiv.org/abs/2511.11651)
*Zhijian Gong,Wenjia Dong,Xueyuan Xu,Fulin Wei,Chunyu Liu,Li Zhuo*

Main category: cs.LG

TL;DR: The study proposes a new EEG feature selection method named IDFS-MEC to address issues like redundancy, irrelevance, and noise in depression analysis using EEG data.


<details>
  <summary>Details</summary>
Motivation: The research aims to improve depression detection through EEG analysis by addressing the challenges like noisy data, redundant features, and issues from missing or incomplete EEG channels.

Method: The study introduces IDFS-MEC, which combines missing-channel indicator data with adaptive channel weighting in orthogonal regression and minimizes redundancy among selected EEG feature subsets.

Result: Experiments on MODMA and PRED-d003 datasets show that IDFS-MEC outperforms 10 other feature selection methods across different EEG channel settings (3, 64, 128).

Conclusion: IDFS-MEC effectively enhances robust EEG-based depression analysis by minimizing redundancy and handling missing channel data, outperforming traditional feature selection methods.

Abstract: As a critical mental health disorder, depression has severe effects on both human physical and mental well-being. Recent developments in EEG-based depression analysis have shown promise in improving depression detection accuracies. However, EEG features often contain redundant, irrelevant, and noisy information. Additionally, real-world EEG data acquisition frequently faces challenges, such as data loss from electrode detachment and heavy noise interference. To tackle the challenges, we propose a novel feature selection approach for robust depression analysis, called Incomplete Depression Feature Selection with Missing EEG Channels (IDFS-MEC). IDFS-MEC integrates missing-channel indicator information and adaptive channel weighting learning into orthogonal regression to lessen the effects of incomplete channels on model construction, and then utilizes global redundancy minimization learning to reduce redundant information among selected feature subsets. Extensive experiments conducted on MODMA and PRED-d003 datasets reveal that the EEG feature subsets chosen by IDFS-MEC have superior performance than 10 popular feature selection methods among 3-, 64-, and 128-channel settings.

</details>


### [566] [How many stations are sufficient? Exploring the effect of urban weather station density reduction on imputation accuracy of air temperature and humidity](https://arxiv.org/abs/2511.11652)
*Marvin Plein,Carsten F. Dormann,Andreas Christen*

Main category: cs.LG

TL;DR: This paper explores optimizing urban weather station networks (WSNs) by reducing station density while maintaining prediction accuracy for air temperature and humidity.


<details>
  <summary>Details</summary>
Motivation: Maintaining urban WSNs is costly and labor-intensive, motivating the need to identify efficient ways to reduce network density without compromising monitoring accuracy.

Method: The authors applied a station removal procedure to thin an existing WSN in Freiburg, Germany, followed by simulating the ability of reduced WSN subsets to estimate year-long air temperature and humidity patterns.

Result: Significant reductions in station numbers increased error margins slightly, but the system retained high predictive accuracy (e.g., RMSE increases of only 20% and 16% for air temperature and relative humidity, respectively). Stations in edge locations between built-up and rural surroundings were identified as most valuable.

Conclusion: The study highlights how thinning WSNs can help save costs and resources while preserving adequate monitoring accuracy for urban climate studies.

Abstract: Urban weather station networks (WSNs) are widely used to monitor urban weather and climate patterns and aid urban planning. However, maintaining WSNs is expensive and labor-intensive. Here, we present a step-wise station removal procedure to thin an existing WSN in Freiburg, Germany, and analyze the ability of WSN subsets to reproduce air temperature and humidity patterns of the entire original WSN for a year following a simulated reduction of WSN density. We found that substantial reductions in station numbers after one year of full deployment are possible while retaining high predictive accuracy. A reduction from 42 to 4 stations, for instance, increased mean prediction RMSEs from 0.69 K to 0.83 K for air temperature and from 3.8% to 4.4% for relative humidity, corresponding to RMSE increases of only 20% and 16%, respectively. Predictive accuracy is worse for remote stations in forests than for stations in built-up or open settings, but consistently better than a state-of-the-art numerical urban land-surface model (Surface Urban Energy and Water Balance Scheme). Stations located at the edges between built-up and rural areas are most valuable when reconstructing city-wide climate characteristics. Our study demonstrates the potential of thinning WSNs to maximize the efficient allocation of financial and personnel-related resources in urban climate research.

</details>


### [567] [Convergence of Multiagent Learning Systems for Traffic control](https://arxiv.org/abs/2511.11654)
*Sayambhu Sen,Shalabh Bhatnagar*

Main category: cs.LG

TL;DR: The paper addresses traffic congestion in urban areas using Multi-Agent Reinforcement Learning (MARL) for effective Traffic Signal Control (TSC) and provides theoretical insights into its convergence.


<details>
  <summary>Details</summary>
Motivation: To address traffic congestion in rapidly urbanizing cities like Bangalore by ensuring efficient traffic signal control and analyzing the theoretical convergence of MARL in such applications.

Method: The study employs stochastic approximation methods to rigorously analyze the learning dynamics of MARL and investigate its convergence properties in cooperative traffic control.

Result: The work provides a formal proof of convergence for the MARL algorithm under specific conditions, extending prior single-agent convergence results to the multi-agent context.

Conclusion: This research validates MARL as a theoretically sound approach for traffic signal control by establishing its stability and convergence, filling the gap in its theoretical analysis.

Abstract: Rapid urbanization in cities like Bangalore has led to severe traffic congestion, making efficient Traffic Signal Control (TSC) essential. Multi-Agent Reinforcement Learning (MARL), often modeling each traffic signal as an independent agent using Q-learning, has emerged as a promising strategy to reduce average commuter delays. While prior work Prashant L A et. al has empirically demonstrated the effectiveness of this approach, a rigorous theoretical analysis of its stability and convergence properties in the context of traffic control has not been explored. This paper bridges that gap by focusing squarely on the theoretical basis of this multi-agent algorithm. We investigate the convergence problem inherent in using independent learners for the cooperative TSC task. Utilizing stochastic approximation methods, we formally analyze the learning dynamics. The primary contribution of this work is the proof that the specific multi-agent reinforcement learning algorithm for traffic control is proven to converge under the given conditions extending it from single agent convergence proofs for asynchronous value iteration.

</details>


### [568] [On the Information Processing of One-Dimensional Wasserstein Distances with Finite Samples](https://arxiv.org/abs/2511.12881)
*Cheongjae Jang,Jonghyun Won,Soyeon Jun,Chun Kee Chung,Keehyoung Joo,Yung-Kyun Noh*

Main category: cs.LG

TL;DR: The paper investigates how the one-dimensional Wasserstein distance captures pointwise density differences and support information, validated through neural spike train decoding and amino acid data.


<details>
  <summary>Details</summary>
Motivation: To explore whether and how the one-dimensional Wasserstein distance can identify pointwise density differences between datasets, especially when their supports overlap significantly, which remains unclear in finite-sample settings.

Method: The authors conducted an analytical study using the Poisson process to isolate density rate factors and demonstrate how the Wasserstein distance captures pointwise density and support differences. This was validated with applications to neural spike train decoding and amino acid contact frequency data.

Result: The study shows that the one-dimensional Wasserstein distance effectively captures both pointwise density and support differences with finite samples, validated empirically using real-world data examples.

Conclusion: This paper concludes that the one-dimensional Wasserstein distance effectively highlights meaningful density differences, providing valuable insights for measuring discrepancies across datasets in overlapping support scenarios.

Abstract: Leveraging the Wasserstein distance -- a summation of sample-wise transport distances in data space -- is advantageous in many applications for measuring support differences between two underlying density functions. However, when supports significantly overlap while densities exhibit substantial pointwise differences, it remains unclear whether and how this transport information can accurately identify these differences, particularly their analytic characterization in finite-sample settings. We address this issue by conducting an analysis of the information processing capabilities of the one-dimensional Wasserstein distance with finite samples. By utilizing the Poisson process and isolating the rate factor, we demonstrate the capability of capturing the pointwise density difference with Wasserstein distances and how this information harmonizes with support differences. The analyzed properties are confirmed using neural spike train decoding and amino acid contact frequency data. The results reveal that the one-dimensional Wasserstein distance highlights meaningful density differences related to both rate and support.

</details>


### [569] [On the Probabilistic Learnability of Compact Neural Network Preimage Bounds](https://arxiv.org/abs/2511.11656)
*Luca Marzari,Manuele Bicego,Ferdinando Cicalese,Alessandro Farinelli*

Main category: cs.LG

TL;DR: The paper introduces RF-ProVe, a scalable probabilistic method using random forests for verifying neural network preimages with statistical guarantees.


<details>
  <summary>Details</summary>
Motivation: To address the scalability issue of computing neural network preimage bounds, which is #P-hard, by utilizing a probabilistic method offering high-confidence guarantees.

Method: RF-ProVe employs randomized decision trees and active resampling to identify input regions satisfying a specific output property, with formal statistical guarantees on these approximations.

Result: The method demonstrates the capability to approximate neural network preimages effectively and address cases where exact methods fail due to scalability issues.

Conclusion: RF-ProVe presents a practical and scalable solution with theoretical guarantees for preimage approximation in neural networks, suitable where exact methods are infeasible.

Abstract: Although recent provable methods have been developed to compute preimage bounds for neural networks, their scalability is fundamentally limited by the #P-hardness of the problem. In this work, we adopt a novel probabilistic perspective, aiming to deliver solutions with high-confidence guarantees and bounded error. To this end, we investigate the potential of bootstrap-based and randomized approaches that are capable of capturing complex patterns in high-dimensional spaces, including input regions where a given output property holds. In detail, we introduce $\textbf{R}$andom $\textbf{F}$orest $\textbf{Pro}$perty $\textbf{Ve}$rifier ($\texttt{RF-ProVe}$), a method that exploits an ensemble of randomized decision trees to generate candidate input regions satisfying a desired output property and refines them through active resampling. Our theoretical derivations offer formal statistical guarantees on region purity and global coverage, providing a practical, scalable solution for computing compact preimage approximations in cases where exact solvers fail to scale.

</details>


### [570] [Generalization Bounds for Semi-supervised Matrix Completion with Distributional Side Information](https://arxiv.org/abs/2511.13049)
*Antoine Ledent,Mun Chong Soo,Nong Minh Hieu*

Main category: cs.LG

TL;DR: The paper addresses a matrix completion problem by leveraging low-rank properties and shared subspaces between ground truth and sampling matrices, achieving performance gains using both implicit and explicit feedback in data.


<details>
  <summary>Details</summary>
Motivation: To improve the integration of implicit and explicit feedback in recommender systems by studying matrix completion where ground truth and sampling distributions share a low-rank subspace.

Method: The paper uses principles of low-rank subspace recovery and generalization bounds, alongside theoretical and real-world experiments, to establish error bounds and validate the approach.

Result: It achieves error bounds as $\u007FO(\sqrt{nd/M})$ and $\u007FO(\sqrt{dr/N})$ in synthetic tests and outperforms baselines in real-world cases (Douban, MovieLens) with limited explicit ratings.

Conclusion: The method validates a theoretical framework for studying implicit-explicit feedback interaction and effectively utilizes unlabeled data, benefiting matrix completion in recommender systems.

Abstract: We study a matrix completion problem where both the ground truth $R$ matrix and the unknown sampling distribution $P$ over observed entries are low-rank matrices, and \textit{share a common subspace}. We assume that a large amount $M$ of \textit{unlabeled} data drawn from the sampling distribution $P$ is available, together with a small amount $N$ of labeled data drawn from the same distribution and noisy estimates of the corresponding ground truth entries. This setting is inspired by recommender systems scenarios where the unlabeled data corresponds to `implicit feedback' (consisting in interactions such as purchase, click, etc. ) and the labeled data corresponds to the `explicit feedback', consisting of interactions where the user has given an explicit rating to the item. Leveraging powerful results from the theory of low-rank subspace recovery, together with classic generalization bounds for matrix completion models, we show error bounds consisting of a sum of two error terms scaling as $\widetilde{O}\left(\sqrt{\frac{nd}{M}}\right)$ and $\widetilde{O}\left(\sqrt{\frac{dr}{N}}\right)$ respectively, where $d$ is the rank of $P$ and $r$ is the rank of $M$. In synthetic experiments, we confirm that the true generalization error naturally splits into independent error terms corresponding to the estimations of $P$ and and the ground truth matrix $\ground$ respectively. In real-life experiments on Douban and MovieLens with most explicit ratings removed, we demonstrate that the method can outperform baselines relying only on the explicit ratings, demonstrating that our assumptions provide a valid toy theoretical setting to study the interaction between explicit and implicit feedbacks in recommender systems.

</details>


### [571] [SpecQuant: Spectral Decomposition and Adaptive Truncation for Ultra-Low-Bit LLMs Quantization](https://arxiv.org/abs/2511.11663)
*Zhixiong Zhao,Fangxin Liu,Junjie Wang,Chenyang Guan,Zongwu Wang,Li Jiang,Haibing Guan*

Main category: cs.LG

TL;DR: This paper introduces SpecQuant, a framework for ultra-low-bit quantization of large language models (LLMs), minimizing the accuracy impact while improving efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop advanced quantization techniques to efficiently deploy accurate large language models (LLMs) on end-user devices by targeting extreme model compression.

Method: The proposed method, SpecQuant, uses a two-stage framework: (1) it addresses activation outliers by redistributing them into the weight matrix and (2) employs Fourier truncation to suppress high-frequency components while retaining essential low-frequency signal energy. A lightweight truncation module is introduced to adapt inference thresholds based on channels.

Result: SpecQuant achieves 4-bit quantization for both weights and activations on the LLaMA-3 8B model. This results in only a 1.5% reduction in accuracy while doubling inference speed and reducing memory usage by three times.

Conclusion: SpecQuant demonstrates that Fourier frequency domain strategies enable extreme LLM compression without significant loss of accuracy, making LLMs more deployable on resource-constrained devices.

Abstract: The emergence of accurate open large language models (LLMs) has sparked a push for advanced quantization techniques to enable efficient deployment on end-user devices. In this paper, we revisit the challenge of extreme LLM compression -- targeting ultra-low-bit quantization for both activations and weights -- from a Fourier frequency domain perspective. We propose SpecQuant, a two-stage framework that tackles activation outliers and cross-channel variance. In the first stage, activation outliers are smoothed and transferred into the weight matrix to simplify downstream quantization. In the second stage, we apply channel-wise low-frequency Fourier truncation to suppress high-frequency components while preserving essential signal energy, improving quantization robustness. Our method builds on the principle that most of the weight energy is concentrated in low-frequency components, which can be retained with minimal impact on model accuracy. To enable runtime adaptability, we introduce a lightweight truncation module during inference that adjusts truncation thresholds based on channel characteristics. On LLaMA-3 8B, SpecQuant achieves 4-bit quantization for both weights and activations, narrowing the zero-shot accuracy gap to only 1.5% compared to full precision, while delivering 2 times faster inference and 3times lower memory usage.

</details>


### [572] [Clifford Algebraic Rotor Embeddings : Maybe embeddings should start to CARE](https://arxiv.org/abs/2511.11665)
*Sameeksha Sriram,Ayush Paliwal,Alexander S. Ecker,Chase van de Geijn*

Main category: cs.LG

TL;DR: The paper introduces Quaternion Rotary Embeddings (QuatRo) and generalizes to Clifford Algebraic Rotary Embeddings (CARE) to extend positional encodings to higher dimensions.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by improving positional encoding methods, particularly addressing non-commutative issues in extensions to higher dimensions.

Method: It uses quaternion-based rotations and Clifford algebra to formulate generalized rotary embeddings adaptable to multivectors of various dimensions.

Result: The paper shows that Mixed RoPE and Spherical RoPE are special cases of QuatRo, and provides preliminary experiments for comparing spherical, quaternion, and Clifford-based embeddings.

Conclusion: QuatRo and CARE demonstrate potential for more sophisticated positional encoding methods, capable of encoding in higher dimensions and multivector grades.

Abstract: Rotary Positional Embeddings (RoPE) have demonstrated exceptional performance as a positional encoding method, consistently outperforming their baselines. While recent work has sought to extend RoPE to higher-dimensional inputs, many such extensions are non-commutative, thereby forfeiting RoPE's shift-equivariance property. Spherical RoPE is one such non-commutative variant, motivated by the idea of rotating embedding vectors on spheres rather than circles. However, spherical rotations are inherently non-commutative, making the choice of rotation sequence ambiguous. In this work, we explore a quaternion-based approach -- Quaternion Rotary Embeddings (QuatRo) -- in place of Euler angles, leveraging quaternions' ability to represent 3D rotations to parameterize the axes of rotation. We show Mixed RoPE and Spherical RoPE to be special cases of QuatRo. Further, we propose a generalization of QuatRo to Clifford Algebraic Rotary Embeddings (CARE) using geometric algebra. Viewing quaternions as the even subalgebra of Cl(3,0,0), we extend the notion of rotary embeddings from quaternions to Clifford rotors acting on multivectors. This formulation enables two key generalizations: (1) extending rotary embeddings to arbitrary dimensions, and (2) encoding positional information in multivectors of multiple grades, not just vectors. We present preliminary experiments comparing spherical, quaternion, and Clifford-based rotary embeddings.

</details>


### [573] [Laplace Learning in Wasserstein Space](https://arxiv.org/abs/2511.13229)
*Mary Chriselda Antony Oliver,Michael Roberts,Carola-Bibiane Schönlieb,Matthew Thorpe*

Main category: cs.LG

TL;DR: This paper extends graph-based semi-supervised learning methods from finite-dimensional Euclidean spaces to infinite-dimensional Wasserstein spaces.


<details>
  <summary>Details</summary>
Motivation: To explore how the manifold hypothesis can be leveraged to improve graph-based semi-supervised learning methods in infinite-dimensional spaces.

Method: Proves variational convergence of graph p-Dirichlet energy to its continuous counterpart and examines the Laplace-Beltrami operator on Wasserstein manifolds.

Result: The theoretical framework is validated with numerical experiments on benchmark datasets, showcasing consistent classification performance for high-dimensional data.

Conclusion: Graph-based semi-supervised learning methods can successfully operate in infinite-dimensional Wasserstein spaces, supporting the manifold hypothesis's applicability to high-dimensional data structures.

Abstract: The manifold hypothesis posits that high-dimensional data typically resides on low-dimensional sub spaces. In this paper, we assume manifold hypothesis to investigate graph-based semi-supervised learning
  methods. In particular, we examine Laplace Learning in the Wasserstein space, extending the classical
  notion of graph-based semi-supervised learning algorithms from finite-dimensional Euclidean spaces to
  an infinite-dimensional setting. To achieve this, we prove variational convergence of a discrete graph p- Dirichlet energy to its continuum counterpart. In addition, we characterize the Laplace-Beltrami operator
  on asubmanifold of the Wasserstein space. Finally, we validate the proposed theoretical framework through
  numerical experiments conducted on benchmark datasets, demonstrating the consistency of our classification performance in high-dimensional settings.

</details>


### [574] [Counterfactual Explainable AI (XAI) Method for Deep Learning-Based Multivariate Time Series Classification](https://arxiv.org/abs/2511.13237)
*Alan G. Paredes Cetina,Kaouther Benguessoum,Raoni Lourenço,Sylvain Kubler*

Main category: cs.LG

TL;DR: The paper introduces CONFETTI, a counterfactual explanation method for multivariate time series (MTS) data, aimed at improving interpretability by balancing prediction confidence, proximity, and sparsity.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges with existing deep learning models for MTS that lack transparency and the limitations of current Counterfactual Explanation (CE) methods which fail to optimize accuracy, proximity, and sparsity together.

Method: The authors propose CONFETTI, which uses a multi-objective approach to identify MTS subsequences, locate counterfactual targets, and modify time series for balanced optimization of interpretability metrics.

Result: The evaluation on seven UEA archive datasets shows that CONFETTI consistently achieves at least 10% higher confidence and improves sparsity by at least 40%, outperforming state-of-the-art CE methods.

Conclusion: CONFETTI successfully improves MTS interpretability and decision support by optimizing multiple objectives and outperforms existing methods in providing actionable insights with minimal changes.

Abstract: Recent advances in deep learning have improved multivariate time series (MTS) classification and regression by capturing complex patterns, but their lack of transparency hinders decision-making. Explainable AI (XAI) methods offer partial insights, yet often fall short of conveying the full decision space. Counterfactual Explanations (CE) provide a promising alternative, but current approaches typically prioritize either accuracy, proximity or sparsity -- rarely all -- limiting their practical value. To address this, we propose CONFETTI, a novel multi-objective CE method for MTS. CONFETTI identifies key MTS subsequences, locates a counterfactual target, and optimally modifies the time series to balance prediction confidence, proximity and sparsity. This method provides actionable insights with minimal changes, improving interpretability, and decision support. CONFETTI is evaluated on seven MTS datasets from the UEA archive, demonstrating its effectiveness in various domains. CONFETTI consistently outperforms state-of-the-art CE methods in its optimization objectives, and in six other metrics from the literature, achieving $\geq10\%$ higher confidence while improving sparsity in $\geq40\%$.

</details>


### [575] [Beyond Superficial Forgetting: Thorough Unlearning through Knowledge Density Estimation and Block Re-insertion](https://arxiv.org/abs/2511.11667)
*Feng Guo,Yuntao Wen,Shen Gao,Junshuo Zhang,Shuo Shang*

Main category: cs.LG

TL;DR: KUnBR proposes a method to selectively and precisely unlearn harmful knowledge in Large Language Models (LLMs) while preserving their utility.


<details>
  <summary>Details</summary>
Motivation: Existing unlearning methods in LLMs leave residual harmful knowledge, raising privacy, regulatory, and ethical concerns.

Method: KUnBR uses knowledge density estimation to pinpoint layers rich in harmful knowledge and employs a layer re-insertion strategy to thoroughly eliminate it.

Result: KUnBR demonstrates state-of-the-art forgetting performance through experiments on multiple benchmarks while maintaining model utility.

Conclusion: KUnBR presents an innovative and effective approach to tackle harmful knowledge in LLMs, addressing a critical issue in AI model ethics and compliance.

Abstract: Machine unlearning, which selectively removes harmful knowledge from a pre-trained model without retraining from scratch, is crucial for addressing privacy, regulatory compliance, and ethical concerns in Large Language Models (LLMs). However, existing unlearning methods often struggle to thoroughly remove harmful knowledge, leaving residual harmful knowledge that can be easily recovered. To address these limitations, we propose Knowledge Density-Guided Unlearning via Blocks Reinsertion (KUnBR), a novel approach that first identifies layers with rich harmful knowledge and then thoroughly eliminates the harmful knowledge via re-insertion strategy. Our method introduces knowledge density estimation to quantify and locate layers containing the most harmful knowledge, enabling precise unlearning. Additionally, we design a layer re-insertion strategy that extracts and re-inserts harmful knowledge-rich layers into the original LLM, bypassing gradient obstruction caused by cover layers and ensuring effective gradient propagation during unlearning. Extensive experiments conducted on several unlearning and general capability benchmarks demonstrate that KUnBR achieves state-of-the-art forgetting performance while maintaining model utility.

</details>


### [576] [Fast and Robust Simulation-Based Inference With Optimization Monte Carlo](https://arxiv.org/abs/2511.13394)
*Vasilis Gkolemis,Christos Diou,Michael Gutmann*

Main category: cs.LG

TL;DR: The paper introduces a method for Bayesian parameter inference in complex stochastic simulations, focusing on reducing computation times while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing simulation-based inference methods are computationally expensive in high-dimensional settings or with partially uninformative outputs.

Method: The proposed method reformulates stochastic simulation as deterministic optimization problems using the Optimization Monte Carlo framework and applies gradient-based approaches for increased efficiency. It is implemented in JAX for performance optimization.

Result: Extensive experiments demonstrate that the proposed method is both accurate and faster compared to state-of-the-art methods, particularly in challenging settings like high-dimensional spaces, multiple observations, and multimodal posteriors.

Conclusion: The new method effectively balances accuracy with efficiency, making it suitable for computationally intensive Bayesian inference tasks.

Abstract: Bayesian parameter inference for complex stochastic simulators is challenging due to intractable likelihood functions. Existing simulation-based inference methods often require large number of simulations and become costly to use in high-dimensional parameter spaces or in problems with partially uninformative outputs. We propose a new method for differentiable simulators that delivers accurate posterior inference with substantially reduced runtimes. Building on the Optimization Monte Carlo framework, our approach reformulates stochastic simulation as deterministic optimization problems. Gradient-based methods are then applied to efficiently navigate toward high-density posterior regions and avoid wasteful simulations in low-probability areas. A JAX-based implementation further enhances the performance through vectorization of key method components. Extensive experiments, including high-dimensional parameter spaces, uninformative outputs, multiple observations and multimodal posteriors show that our method consistently matches, and often exceeds, the accuracy of state-of-the-art approaches, while reducing the runtime by a substantial margin.

</details>


### [577] [Do traveling waves make good positional encodings?](https://arxiv.org/abs/2511.11668)
*Chase van de Geijn,Ayush Paliwal,Timo Lüddecke,Alexander S. Ecker*

Main category: cs.LG

TL;DR: This paper introduces RollPE, a novel positional encoding method leveraging traveling waves, significantly improving performance over traditional embeddings and comparable to RoPE.


<details>
  <summary>Details</summary>
Motivation: The work aims to improve positional encoding mechanisms in transformers, addressing the limitations of traditional embeddings by leveraging relative encoding methods for better translation equivariance.

Method: RollPE employs circular roll operations to induce relative shifts in phase across positions in self-attention, transforming positional information into relative positional differences.

Result: RollPE achieves significant performance improvements compared to absolute positional embeddings and is comparable to RoPE. A continuous version is derived, along with a mathematical equivalence to RoPE.

Conclusion: RollPE simplifies and improves positional encoding in transformers, offering new insights via the traveling waves perspective and highlighting connections to information flow processes in the brain.

Abstract: Transformers rely on positional encoding to compensate for the inherent permutation invariance of self-attention. Traditional approaches use absolute sinusoidal embeddings or learned positional vectors, while more recent methods emphasize relative encodings to better capture translation equivariances. In this work, we propose RollPE, a novel positional encoding mechanism based on traveling waves, implemented by applying a circular roll operation to the query and key tensors in self-attention. This operation induces a relative shift in phase across positions, allowing the model to compute attention as a function of positional differences rather than absolute indices. We show this simple method significantly outperforms traditional absolute positional embeddings and is comparable to RoPE. We derive a continuous case of RollPE which implicitly imposes a topographic structure on the query and key space. We further derive a mathematical equivalence of RollPE to a particular configuration of RoPE. Viewing RollPE through the lens of traveling waves may allow us to simplify RoPE and relate it to processes of information flow in the brain.

</details>


### [578] [Larger Datasets Can Be Repeated More: A Theoretical Analysis of Multi-Epoch Scaling in Linear Regression](https://arxiv.org/abs/2511.13421)
*Tingkai Yan,Haodong Wen,Binghui Li,Kairong Luo,Wenguang Chen,Kaifeng Lyu*

Main category: cs.LG

TL;DR: The paper studies the effects of multiple training epochs on the data scaling laws, focusing on linear regression.


<details>
  <summary>Details</summary>
Motivation: To understand how training on limited datasets across multiple epochs compares to training on larger datasets with a single epoch in terms of performance.

Method: It defines the 'effective reuse rate' of data and provides a theoretical analysis of its scaling behavior under strong convexity and Zipf-distributed data using SGD in linear regression.

Result: The analysis shows that for small epochs, the effective reuse rate scales linearly, while it eventually plateaus for larger datasets. Empirical validation with LLMs confirms these findings and highlights the dependency of the reuse rate on data size and distribution.

Conclusion: Data reuse in training is both data size and distribution-dependent, requiring explicit modeling in future scaling law studies of LLMs.

Abstract: While data scaling laws of large language models (LLMs) have been widely examined in the one-pass regime with massive corpora, their form under limited data and repeated epochs remains largely unexplored. This paper presents a theoretical analysis of how a common workaround, training for multiple epochs on the same dataset, reshapes the data scaling laws in linear regression. Concretely, we ask: to match the performance of training on a dataset of size $N$ for $K$ epochs, how much larger must a dataset be if the model is trained for only one pass? We quantify this using the \textit{effective reuse rate} of the data, $E(K, N)$, which we define as the multiplicative factor by which the dataset must grow under one-pass training to achieve the same test loss as $K$-epoch training. Our analysis precisely characterizes the scaling behavior of $E(K, N)$ for SGD in linear regression under either strong convexity or Zipf-distributed data: (1) When $K$ is small, we prove that $E(K, N) \approx K$, indicating that every new epoch yields a linear gain; (2) As $K$ increases, $E(K, N)$ plateaus at a problem-dependent value that grows with $N$ ($Θ(\log N)$ for the strongly-convex case), implying that larger datasets can be repeated more times before the marginal benefit vanishes. These theoretical findings point out a neglected factor in a recent empirical study (Muennighoff et al. (2023)), which claimed that training LLMs for up to $4$ epochs results in negligible loss differences compared to using fresh data at each step, \textit{i.e.}, $E(K, N) \approx K$ for $K \le 4$ in our notation. Supported by further empirical validation with LLMs, our results reveal that the maximum $K$ value for which $E(K, N) \approx K$ in fact depends on the data size and distribution, and underscore the need to explicitly model both factors in future studies of scaling laws with data reuse.

</details>


### [579] [H-Model: Dynamic Neural Architectures for Adaptive Processing](https://arxiv.org/abs/2511.11669)
*Dmytro Hospodarchuk*

Main category: cs.LG

TL;DR: The paper introduces a neural network with a dynamic routing mechanism enabling adaptive computation, inspired by thought processes.


<details>
  <summary>Details</summary>
Motivation: To explore new architectural frameworks for adaptable and interpretable neural networks, moving beyond static representations.

Method: Proposes a prototype neural network with layers influencing output propagation using a dynamic routing mechanism.

Result: Preliminary experiments show promise, though resource limitations restrict comprehensive evaluation.

Conclusion: The study opens avenues for future work on neural networks capable of learning computations and adapting structures dynamically.

Abstract: This article explores the design and experimentation of a neural network architecture capable of dynamically adjusting its internal structure based on the input data. The proposed model introduces a routing mechanism that allows each layer to influence how its outputs are propagated through the network, enabling iterative and adaptive computation. This concept is loosely inspired by the idea of thought processes and dynamic reasoning, where information flow is conditioned not only on the data itself, but also on the internal state of the system.
  It is important to note that this work does not aim to compete with state-of-the-art language models in terms of performance. Instead, it presents a conceptual prototype-an architectural framework that opens up a new direction for exploring adaptable and potentially more interpretable networks. The goal is not optimization of existing benchmarks but rather the proposal of a system that can learn not only representations, but also the structure of computation itself.
  Due to practical constraints in computing resources and data, this study remains a preliminary investigation. Nevertheless, initial observations show promise, and the architecture's full potential can only be evaluated in future experiments under more favorable computational conditions.

</details>


### [580] [AdamX: An Adam improvement algorithm based on a novel exponential decay mechanism for the second-order moment estimate](https://arxiv.org/abs/2511.13465)
*Meng Zhu,Quan Xiao,Weidong Min*

Main category: cs.LG

TL;DR: AdamX is a proposed optimization algorithm designed to improve upon Adam by addressing stability and generalization issues.


<details>
  <summary>Details</summary>
Motivation: The motivation is the need to address the tendency of Adam to converge to non-flat minima, which can affect the stability and generalization of training, especially in the era of large-scale models and data.

Method: AdamX incorporates a novel second-order moment estimation exponential decay rate that adjusts learning step correction strength over time, allowing it to shift towards SGD-like behavior during the stable phase of training.

Result: AdamX outperforms Adam and its variants in training stability and generalization, confirmed by experimental results, and demonstrates improved second-order moment estimation.

Conclusion: The proposed AdamX algorithm provides a more stable and generalizable optimization framework, offering improved performance over Adam and its variants. Its source code is publicly available.

Abstract: Since the 21st century, artificial intelligence has been leading a new round of industrial revolution. Under the training framework, the optimization algorithm aims to stably converge high-dimensional optimization to local and even global minima. Entering the era of large language models, although the scale of model parameters and data has increased, Adam remains the mainstream optimization algorithm. However, compared with stochastic gradient descent (SGD) based optimization algorithms, Adam is more likely to converge to non-flat minima. To address this issue, the AdamX algorithm is proposed. Its core innovation lies in the proposition of a novel type of second-order moment estimation exponential decay rate, which gradually weakens the learning step correction strength as training progresses, and degrades to SGD in the stable training period, thereby improving the stability of training in the stable period and possibly enhancing generalization ability. Experimental results show that our second-order moment estimation exponential decay rate is better than the current second-order moment estimation exponential decay rate, and AdamX can stably outperform Adam and its variants in terms of performance. Our code is open-sourced at https://github.com/mengzhu0308/AdamX.

</details>


### [581] [Evaluation of LLM-based Explanations for a Learning Analytics Dashboard](https://arxiv.org/abs/2511.11671)
*Alina Deriyeva,Benjamin Paassen*

Main category: cs.LG

TL;DR: The study examines the use of large language models (LLMs) to improve interpretability of Learning Analytics Dashboards in Digital Learning Environments.


<details>
  <summary>Details</summary>
Motivation: To enhance the effectiveness of Learning Analytics Dashboards in supporting self-regulated learning and metacognitive skill development by addressing challenges in data interpretability.

Method: Conducted an expert study with 12 university educators to compare LLM-generated verbal explanations of dashboard data against standalone dashboards and human teacher explanations.

Result: LLM-generated explanations were significantly preferred over other conditions due to their clear interpretation and actionable learning recommendations.

Conclusion: Integrating LLMs for dashboard interpretation improves learning experiences and aligns with educators' pedagogical standards.

Abstract: Learning Analytics Dashboards can be a powerful tool to support self-regulated learning in Digital Learning Environments and promote development of meta-cognitive skills, such as reflection. However, their effectiveness can be affected by the interpretability of the data they provide. To assist in the interpretation, we employ a large language model to generate verbal explanations of the data in the dashboard and evaluate it against a standalone dashboard and explanations provided by human teachers in an expert study with university level educators (N=12). We find that the LLM-based explanations of the skill state presented in the dashboard, as well as general recommendations on how to proceed with learning within the course are significantly more favored compared to the other conditions. This indicates that using LLMs for interpretation purposes can enhance the learning experience for learners while maintaining the pedagogical standards approved by teachers.

</details>


### [582] [Synergistic Feature Fusion for Latent Lyrical Classification: A Gated Deep Learning Architecture](https://arxiv.org/abs/2511.11673)
*M. A. Gameiro*

Main category: cs.LG

TL;DR: The paper presents a novel Synergistic Fusion Layer (SFL) architecture that improves lyrical content classification by effectively integrating high-dimensional semantic features with low-dimensional structural ones, achieving superior accuracy and reliability compared to baseline models.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of combining complex deep semantic features with simple structural cues for improved lyrical content classification.

Method: The study proposes the Synergistic Fusion Layer (SFL) architecture, utilizing a gating mechanism to modulate Sentence-BERT embeddings with auxiliary low-dimensional features, and reframing the task as a binary classification problem based on clustering UMAP-reduced lyrical embeddings.

Result: The SFL model achieved an accuracy of 0.9894 and a Macro F1 score of 0.9894, significantly outperforming the Random Forest baseline in reliability and calibration metrics.

Conclusion: The SFL model confirms that non-linear gating is superior to simple feature concatenation, proving itself as a robust tool for multimodal lyrical analysis.

Abstract: This study addresses the challenge of integrating complex, high-dimensional deep semantic features with simple, interpretable structural cues for lyrical content classification. We introduce a novel Synergistic Fusion Layer (SFL) architecture, a deep learning model utilizing a gated mechanism to modulate Sentence-BERT embeddings (Fdeep) using low-dimensional auxiliary features (Fstruct). The task, derived from clustering UMAP-reduced lyrical embeddings, is reframed as binary classification, distinguishing a dominant, homogeneous cluster (Class 0) from all other content (Class 1). The SFL model achieved an accuracy of 0.9894 and a Macro F1 score of 0.9894, outperforming a comprehensive Random Forest (RF) baseline that used feature concatenation (Accuracy = 0.9868). Crucially, the SFL model demonstrated vastly superior reliability and calibration, exhibiting a 93% reduction in Expected Calibration Error (ECE = 0.0035) and a 2.5x lower Log Loss (0.0304) compared to the RF baseline (ECE = 0.0500; Log Loss = 0.0772). This performance validates the architectural hypothesis that non-linear gating is superior to simple feature concatenation, establishing the SFL model as a robust and trustworthy system for complex multimodal lyrical analysis.

</details>


### [583] [Federated Learning for Pediatric Pneumonia Detection: Enabling Collaborative Diagnosis Without Sharing Patient Data](https://arxiv.org/abs/2511.11714)
*Daniel M. Jimenez-Gutierrez,Enrique Zuazua,Joaquin Del Rio,Oleksii Sliusarenko,Xabi Uribe-Etxebarria*

Main category: cs.LG

TL;DR: This paper explores Federated Learning (FL) for privacy-preserving pneumonia detection from chest X-rays across hospitals, improving accuracy and generalizability without data centralization.


<details>
  <summary>Details</summary>
Motivation: Given the critical importance of early pneumonia detection and the challenges in sharing medical data due to privacy regulations, the paper seeks to address these barriers using Federated Learning for collaborative model training.

Method: The study utilized the Sherpa.ai FL platform to allow multiple hospitals to train a common model locally with private data. It simulated real-world variability in cross-hospital collaboration using a pediatric dataset with non-IID data.

Result: Federated training achieved significant performance gains, with 0.900 Accuracy and 0.966 ROC-AUC, improving dramatically over single-institution models, without transferring patient data.

Conclusion: FL demonstrates a secure, efficient, and generalizable approach for pneumonia detection, facilitating multi-hospital collaboration even in data-scarce domains, validating its potential in healthcare applications.

Abstract: Early and accurate pneumonia detection from chest X-rays (CXRs) is clinically critical to expedite treatment and isolation, reduce complications, and curb unnecessary antibiotic use. Although artificial intelligence (AI) substantially improves CXR-based detection, development is hindered by globally distributed data, high inter-hospital variability, and strict privacy regulations (e.g., HIPAA, GDPR) that make centralization impractical. These constraints are compounded by heterogeneous imaging protocols, uneven data availability, and the costs of transferring large medical images across geographically dispersed sites.
  In this paper, we evaluate Federated Learning (FL) using the Sherpa.ai FL platform, enabling multiple hospitals (nodes) to collaboratively train a CXR classifier for pneumonia while keeping data in place and private. Using the Pediatric Pneumonia Chest X-ray dataset, we simulate cross-hospital collaboration with non-independent and non-identically distributed (non-IID) data, reproducing real-world variability across institutions and jurisdictions. Our experiments demonstrate that collaborative and privacy-preserving training across multiple hospitals via FL led to a dramatic performance improvement achieving 0.900 Accuracy and 0.966 ROC-AUC, corresponding to 47.5% and 50.0% gains over single-hospital models (0.610; 0.644), without transferring any patient CXR. These results indicate that FL delivers high-performing, generalizable, secure and private pneumonia detection across healthcare networks, with data kept local. This is especially relevant for rare diseases, where FL enables secure multi-institutional collaboration without data movement, representing a breakthrough for accelerating diagnosis and treatment development in low-data domains.

</details>


### [584] [Scientific Data Compression and Super-Resolution Sampling](https://arxiv.org/abs/2511.13675)
*Minh Vu,Andrey Lokhov*

Main category: cs.LG

TL;DR: Scientific data faces challenges due to its sheer volume, and this paper introduces a novel framework for compression and super-resolution driven by modern learning techniques.


<details>
  <summary>Details</summary>
Motivation: The vast volume of data generated by scientific endeavors exceeds storage and processing capabilities, necessitating efficient data reduction methods while retaining essential physical information.

Method: A data compression and super-resolution framework is proposed, utilizing advancements in learning exponential families, balancing compression ratio and reconstruction fidelity.

Result: The framework enables the quantification of uncertainty and flexibility while supporting reconstruction fidelity and compression ratio trade-offs.

Conclusion: The method facilitates efficient scientific data handling, ensuring essential physical features preservation and adaptability for various workflows requiring data recovery or checkpointing.

Abstract: Modern scientific simulations, observations, and large-scale experiments generate data at volumes that often exceed the limits of storage, processing, and analysis. This challenge drives the development of data reduction methods that efficiently manage massive datasets while preserving essential physical features and quantities of interest. In many scientific workflows, it is also crucial to enable data recovery from compressed representations - a task known as super-resolution - with guarantees on the preservation of key physical characteristics. A notable example is checkpointing and restarting, which is essential for long-running simulations to recover from failures, resume after interruptions, or examine intermediate results. In this work, we introduce a novel framework for scientific data compression and super-resolution, grounded in recent advances in learning exponential families. Our method preserves and quantifies uncertainty in physical quantities of interest and supports flexible trade-offs between compression ratio and reconstruction fidelity.

</details>


### [585] [Beyond One-Way Pruning: Bidirectional Pruning-Regrowth for Extreme Accuracy-Sparsity Tradeoff](https://arxiv.org/abs/2511.11675)
*Junchen Liu,Yi Sheng*

Main category: cs.LG

TL;DR: Traditional pruning methods face performance degradation at high sparsity levels. The paper introduces a bidirectional pruning-regrowth strategy to improve performance under such conditions.


<details>
  <summary>Details</summary>
Motivation: Current model pruning approaches result in significant accuracy decline when surpassing certain sparsity thresholds, limiting their usability for hardware with strict size constraints.

Method: The authors propose a bidirectional pruning-regrowth strategy, starting from a highly compressed network and selectively regenerating critical connections to regain performance.

Result: The proposed method mitigates accuracy drops at high sparsity levels, enhancing model usability under demanding hardware constraints.

Conclusion: The bidirectional pruning-regrowth strategy successfully addresses the limitations of traditional pruning methods, balancing compression and performance recovery.

Abstract: As a widely adopted model compression technique, model pruning has demonstrated strong effectiveness across various architectures. However, we observe that when sparsity exceeds a certain threshold, both iterative and one-shot pruning methods lead to a steep decline in model performance. This rapid degradation limits the achievable compression ratio and prevents models from meeting the stringent size constraints required by certain hardware platforms, rendering them inoperable. To overcome this limitation, we propose a bidirectional pruning-regrowth strategy. Starting from an extremely compressed network that satisfies hardware constraints, the method selectively regenerates critical connections to recover lost performance, effectively mitigating the sharp accuracy drop commonly observed under high sparsity conditions.

</details>


### [586] [Efficient Calibration for Decision Making](https://arxiv.org/abs/2511.13699)
*Parikshit Gopalan,Konstantinos Stavropoulos,Kunal Talwar,Pranay Tankala*

Main category: cs.LG

TL;DR: The paper introduces an approximate calibration metric, CDL_K, to evaluate calibration in predictive models, and presents a theory for its computational and informational tractability under certain constraints.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the computational intractability of measuring calibration decision loss (CDL) in predictive models by limiting the scope to structured families of post-processing functions.

Method: The authors define a restricted version of CDL, denoted as CDL_K, which limits post-processings to structured families K. They develop theoretical bounds (upper and lower) for CDL_K and explore its tractability.

Result: This study reveals which structured families allow for tractable computation of CDL_K, and provides theoretical and practical insights into calibration and recalibration in machine learning.

Conclusion: The tractable framework (CDL_K) offers a practical and theoretical advance in understanding and ensuring model calibration, with rigorous guarantees supporting commonly-used recalibration practices in machine learning.

Abstract: A decision-theoretic characterization of perfect calibration is that an agent seeking to minimize a proper loss in expectation cannot improve their outcome by post-processing a perfectly calibrated predictor. Hu and Wu (FOCS'24) use this to define an approximate calibration measure called calibration decision loss ($\mathsf{CDL}$), which measures the maximal improvement achievable by any post-processing over any proper loss. Unfortunately, $\mathsf{CDL}$ turns out to be intractable to even weakly approximate in the offline setting, given black-box access to the predictions and labels.
  We suggest circumventing this by restricting attention to structured families of post-processing functions $K$. We define the calibration decision loss relative to $K$, denoted $\mathsf{CDL}_K$ where we consider all proper losses but restrict post-processings to a structured family $K$. We develop a comprehensive theory of when $\mathsf{CDL}_K$ is information-theoretically and computationally tractable, and use it to prove both upper and lower bounds for natural classes $K$. In addition to introducing new definitions and algorithmic techniques to the theory of calibration for decision making, our results give rigorous guarantees for some widely used recalibration procedures in machine learning.

</details>


### [587] [Learning with Preserving for Continual Multitask Learning](https://arxiv.org/abs/2511.11676)
*Hanchen David Wang,Siwoo Bae,Zirong Chen,Meiyi Ma*

Main category: cs.LG

TL;DR: This paper introduces Learning with Preserving (LwP), a framework for Continual Multitask Learning (CMTL) that addresses catastrophic forgetting by maintaining the geometric structure of shared representations instead of preserving task outputs.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of continual learning in AI systems where models sequentially learn new tasks without forgetting previous ones, especially for critical applications like autonomous driving and medical imaging.

Method: The authors propose Learning with Preserving (LwP), centered around a Dynamically Weighted Distance Preservation (DWDP) loss that regulates pairwise distances between latent representations to prevent representation drift.

Result: Extensive benchmarks demonstrate that LwP reduces catastrophic forgetting, outperforms state-of-the-art methods in CMTL, and maintains robustness to distribution shifts.

Conclusion: LwP is a highly effective method for dynamic environments, enabling models to support diverse tasks without using replay buffers, making it suitable for privacy-sensitive settings.

Abstract: Artificial intelligence systems in critical fields like autonomous driving and medical imaging analysis often continually learn new tasks using a shared stream of input data. For instance, after learning to detect traffic signs, a model may later need to learn to classify traffic lights or different types of vehicles using the same camera feed. This scenario introduces a challenging setting we term Continual Multitask Learning (CMTL), where a model sequentially learns new tasks on an underlying data distribution without forgetting previously learned abilities. Existing continual learning methods often fail in this setting because they learn fragmented, task-specific features that interfere with one another. To address this, we introduce Learning with Preserving (LwP), a novel framework that shifts the focus from preserving task outputs to maintaining the geometric structure of the shared representation space. The core of LwP is a Dynamically Weighted Distance Preservation (DWDP) loss that prevents representation drift by regularizing the pairwise distances between latent data representations. This mechanism of preserving the underlying geometric structure allows the model to retain implicit knowledge and support diverse tasks without requiring a replay buffer, making it suitable for privacy-conscious applications. Extensive evaluations on time-series and image benchmarks show that LwP not only mitigates catastrophic forgetting but also consistently outperforms state-of-the-art baselines in CMTL tasks. Notably, our method shows superior robustness to distribution shifts and is the only approach to surpass the strong single-task learning baseline, underscoring its effectiveness for real-world dynamic environments.

</details>


### [588] [Homotopy-Guided Self-Supervised Learning of Parametric Solutions for AC Optimal Power Flow](https://arxiv.org/abs/2511.11677)
*Shimiao Li,Aaron Tuor,Draguna Vrabie,Larry Pileggi,Jan Drgona*

Main category: cs.LG

TL;DR: The paper introduces a homotopy-guided self-supervised learning to optimize (L2O) method for AC-OPF problems, improving solution quality and feasibility without labeled data.


<details>
  <summary>Details</summary>
Motivation: Traditional AC optimal power flow (AC-OPF) solutions involve nonconvex optimization challenges, and existing learning methods often struggle to find feasible, high-quality solutions efficiently.

Method: The authors propose a homotopy-guided L2O approach, which deforms the optimization problem progressively during training, starting from a relaxed version and gradually refining it to match the original problem.

Result: The method demonstrates higher feasibility rates and comparable objective values to complete OPF solvers on IEEE AC-OPF benchmarks, outperforming non-homotopy-based L2O approaches.

Conclusion: Homotopy-guided methods hold significant potential for scalable and constraint-aware learning-based optimization in power system operations.

Abstract: Learning to optimize (L2O) parametric approximations of AC optimal power flow (AC-OPF) solutions offers the potential for fast, reusable decision-making in real-time power system operations. However, the inherent nonconvexity of AC-OPF results in challenging optimization landscapes, and standard learning approaches often fail to converge to feasible, high-quality solutions. This work introduces a \textit{homotopy-guided self-supervised L2O method} for parametric AC-OPF problems. The key idea is to construct a continuous deformation of the objective and constraints during training, beginning from a relaxed problem with a broad basin of attraction and gradually transforming it toward the original problem. The resulting learning process improves convergence stability and promotes feasibility without requiring labeled optimal solutions or external solvers. We evaluate the proposed method on standard IEEE AC-OPF benchmarks and show that homotopy-guided L2O significantly increases feasibility rates compared to non-homotopy baselines, while achieving objective values comparable to full OPF solvers. These findings demonstrate the promise of homotopy-based heuristics for scalable, constraint-aware L2O in power system optimization.

</details>


### [589] [A neural optimization framework for free-boundary diffeomorphic mapping problems and its applications](https://arxiv.org/abs/2511.11679)
*Zhehao Xu,Lok Ming Lui*

Main category: cs.LG

TL;DR: This paper introduces a new neural network-based method, SBN-Opt, to optimize free-boundary diffeomorphism while ensuring control of geometric distortion.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the difficulty of free-boundary diffeomorphism optimization, which is challenging due to unconstrained boundaries and the need to preserve local bijectivity during large deformations. Existing methods lack gradient-based optimization support and impose landmark conditioning constraints.

Method: The authors propose a neural surrogate, the Spectral Beltrami Network (SBN), which incorporates LSQC energy into a multiscale, mesh-spectral architecture. They further introduce the SBN-Opt framework to optimize free-boundary diffeomorphism with explicit geometric distortion control.

Result: The proposed method, SBN-Opt, demonstrates superior performance over traditional numerical algorithms in experiments on density-equalizing maps and inconsistent surface registration tasks.

Conclusion: The integration of Spectral Beltrami Networks and SBN-Opt provides a robust and effective solution for free-boundary diffeomorphism optimization, addressing limitations of traditional numerical approaches and achieving better results.

Abstract: Free-boundary diffeomorphism optimization is a core ingredient in the surface mapping problem but remains notoriously difficult because the boundary is unconstrained and local bijectivity must be preserved under large deformation. Numerical Least-Squares Quasiconformal (LSQC) theory, with its provable existence, uniqueness, similarity-invariance and resolution-independence, offers an elegant mathematical remedy. However, the conventional numerical algorithm requires landmark conditioning, and cannot be applied into gradient-based optimization. We propose a neural surrogate, the Spectral Beltrami Network (SBN), that embeds LSQC energy into a multiscale mesh-spectral architecture. Next, we propose the SBN guided optimization framework SBN-Opt which optimizes free-boundary diffeomorphism for the problem, with local geometric distortion explicitly controllable. Extensive experiments on density-equalizing maps and inconsistent surface registration demonstrate our SBN-Opt's superiority over traditional numerical algorithms.

</details>


### [590] [Probabilistic Wildfire Susceptibility from Remote Sensing Using Random Forests and SHAP](https://arxiv.org/abs/2511.11680)
*Udaya Bhasker Cheerala,Varun Teja Chirukuri,Venkata Akhil Kumar Gummadi,Jintu Moni Bhuyan,Praveen Damacharla*

Main category: cs.LG

TL;DR: The paper introduces a wildfire risk mapping study for California using the random forest algorithm with explainable AI (SHAP) to improve risk prediction and interpretation for grasslands and forests.


<details>
  <summary>Details</summary>
Motivation: Wildfires threaten ecosystems globally, with frequent occurrences in California due to climatic and human factors, necessitating effective risk mapping to enable better mitigation measures.

Method: The study employs the Random Forest (RF) predictive algorithm, augmented by Shapley Additive exPlanations (SHAP) to interpret the model, with spatial and temporal validation techniques used for performance assessment.

Result: The RF model demonstrated strong predictive accuracy for grasslands (AUC = 0.996) and forests (AUC = 0.997). Spatial validation indicated moderate transferability, and temporal validation showed improved generalization, especially for forests.

Conclusion: The RF-SHAP framework effectively identifies ecosystem-specific wildfire drivers, provides robust and interpretable results, and supports actionable insights for risk mitigation in California's grasslands and forests.

Abstract: Wildfires pose a significant global threat to ecosystems worldwide, with California experiencing recurring fires due to various factors, including climate, topographical features, vegetation patterns, and human activities. This study aims to develop a comprehensive wildfire risk map for California by applying the random forest (RF) algorithm, augmented with Explainable Artificial Intelligence (XAI) through Shapley Additive exPlanations (SHAP), to interpret model predictions. Model performance was assessed using both spatial and temporal validation strategies. The RF model demonstrated strong predictive performance, achieving near-perfect discrimination for grasslands (AUC = 0.996) and forests (AUC = 0.997). Spatial cross-validation revealed moderate transferability, yielding ROC-AUC values of 0.6155 for forests and 0.5416 for grasslands. In contrast, temporal split validation showed enhanced generalization, especially for forests (ROC-AUC = 0.6615, PR-AUC = 0.8423). SHAP-based XAI analysis identified key ecosystem-specific drivers: soil organic carbon, tree cover, and Normalized Difference Vegetation Index (NDVI) emerged as the most influential in forests, whereas Land Surface Temperature (LST), elevation, and vegetation health indices were dominant in grasslands. District-level classification revealed that Central Valley and Northern Buttes districts had the highest concentration of high-risk grasslands, while Northern Buttes and North Coast Redwoods dominated forested high-risk areas. This RF-SHAP framework offers a robust, comprehensible, and adaptable method for assessing wildfire risks, enabling informed decisions and creating targeted strategies to mitigate dangers.

</details>


### [591] [MPCM-Net: Multi-scale network integrates partial attention convolution with Mamba for ground-based cloud image segmentation](https://arxiv.org/abs/2511.11681)
*Penghui Niu,Jiashuai She,Taotao Cai,Yajuan Zhang,Ping Zhang,Junhua Gu,Jianxin Li*

Main category: cs.LG

TL;DR: This paper proposes MPCM-Net, a novel deep learning-based cloud image segmentation model, addressing limitations in multi-scale context extraction, feature enhancement, and global decoder dependencies. It also introduces a dataset, CSRC, for fine-grained segmentation.


<details>
  <summary>Details</summary>
Motivation: Current encoder-decoder methods for cloud image segmentation lack effectiveness in multi-scale feature extraction, attention enhancement, and decoding hierarchical features, which impedes accuracy and computational efficiency.

Method: The proposed MPCM-Net uses MPAC in the encoder, integrating ParCM and ParSM blocks for global spatial interaction, and ParAM with ParSM for discriminative features. The decoder employs M2B with SSHD for efficient feature aggregation.

Result: MPCM-Net outperforms state-of-the-art cloud image segmentation models on the newly introduced CSRC dataset, achieving an optimal accuracy-inference speed balance.

Conclusion: MPCM-Net demonstrates enhanced segmentation performance and efficiency through novel architectural innovations and dataset contribution, cementing its applicability in photovoltaic power forecasting.

Abstract: Ground-based cloud image segmentation is a critical research domain for photovoltaic power forecasting. Current deep learning approaches primarily focus on encoder-decoder architectural refinements. However, existing methodologies exhibit several limitations:(1)they rely on dilated convolutions for multi-scale context extraction, lacking the partial feature effectiveness and interoperability of inter-channel;(2)attention-based feature enhancement implementations neglect accuracy-throughput balance; and (3)the decoder modifications fail to establish global interdependencies among hierarchical local features, limiting inference efficiency. To address these challenges, we propose MPCM-Net, a Multi-scale network that integrates Partial attention Convolutions with Mamba architectures to enhance segmentation accuracy and computational efficiency. Specifically, the encoder incorporates MPAC, which comprises:(1)a MPC block with ParCM and ParSM that enables global spatial interaction across multi-scale cloud formations, and (2)a MPA block combining ParAM and ParSM to extract discriminative features with reduced computational complexity. On the decoder side, a M2B is employed to mitigate contextual loss through a SSHD that maintains linear complexity while enabling deep feature aggregation across spatial and scale dimensions. As a key contribution to the community, we also introduce and release a dataset CSRC, which is a clear-label, fine-grained segmentation benchmark designed to overcome the critical limitations of existing public datasets. Extensive experiments on CSRC demonstrate the superior performance of MPCM-Net over state-of-the-art methods, achieving an optimal balance between segmentation accuracy and inference speed. The dataset and source code will be available at https://github.com/she1110/CSRC.

</details>


### [592] [Stratified Knowledge-Density Super-Network for Scalable Vision Transformers](https://arxiv.org/abs/2511.11683)
*Longhua Li,Lei Qi,Xin Geng*

Main category: cs.LG

TL;DR: The paper proposes an efficient approach to transform pre-trained Vision Transformers (ViTs) into a knowledge-dense super-network with hierarchical stratification, capable of flexible size adaptation while retaining performance.


<details>
  <summary>Details</summary>
Motivation: Training multiple ViT models for varying resource constraints is inefficient and expensive, necessitating a strategy to optimize and reuse pre-trained models for different resource needs.

Method: The authors introduce WPAC (Weighted PCA for Attention Contraction) for knowledge compaction by applying token-wise weighted PCA and injecting transformations into network layers. They also propose PIAD (Progressive Importance-Aware Dropout) to stratify knowledge organization by progressively learning weight importance and applying dropout.

Result: WPAC achieves superior knowledge concentration compared to other pruning techniques. Combined with PIAD, it offers competitive results in model compression and expansion, providing an effective solution for resource-adaptive ViT models.

Conclusion: The proposed WPAC and PIAD methods allow the flexible adaptation of pre-trained ViTs to various resource constraints, ensuring knowledge retention and efficient model compression or expansion.

Abstract: Training and deploying multiple vision transformer (ViT) models for different resource constraints is costly and inefficient. To address this, we propose transforming a pre-trained ViT into a stratified knowledge-density super-network, where knowledge is hierarchically organized across weights. This enables flexible extraction of sub-networks that retain maximal knowledge for varying model sizes. We introduce \textbf{W}eighted \textbf{P}CA for \textbf{A}ttention \textbf{C}ontraction (WPAC), which concentrates knowledge into a compact set of critical weights. WPAC applies token-wise weighted principal component analysis to intermediate features and injects the resulting transformation and inverse matrices into adjacent layers, preserving the original network function while enhancing knowledge compactness. To further promote stratified knowledge organization, we propose \textbf{P}rogressive \textbf{I}mportance-\textbf{A}ware \textbf{D}ropout (PIAD). PIAD progressively evaluates the importance of weight groups, updates an importance-aware dropout list, and trains the super-network under this dropout regime to promote knowledge stratification. Experiments demonstrate that WPAC outperforms existing pruning criteria in knowledge concentration, and the combination with PIAD offers a strong alternative to state-of-the-art model compression and model expansion methods.

</details>


### [593] [On the Fundamental Limits of LLMs at Scale](https://arxiv.org/abs/2511.12869)
*Muhammad Ahmed Mohsin,Muhammad Umer,Ahsan Bilal,Zeeshan Memon,Muhammad Ibtsaam Qadir,Sagnik Bhattacharya,Hassan Rizwan,Abhiram R. Gorle,Maahe Zehra Kazmi,Ayesha Mohsin,Muhammad Usman Rafique,Zihao He,Pulkit Mehta,Muhammad Ali Jamshed,John M. Cioffi*

Main category: cs.LG

TL;DR: The paper analyzes the fundamental limitations of scaling Large Language Models (LLMs) and proposes a theoretical framework to explain their constraints, pairing it with practical mitigation strategies.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a rigorous theoretical synthesis connecting the empirical limitations of LLM scaling to fundamental computational, informational, and learning limits.

Method: The authors develop a proof-informed framework based on computability, information theory, and geometric effects, providing theorems and empirical evidence to formalize the theoretical ceilings on LLM scaling.

Result: They identified five key limitations—hallucination, context compression, reasoning degradation, retrieval fragility, and multimodal misalignment—grounded in irreducible computational, informational, and geometric constraints. Mitigation strategies are also proposed.

Conclusion: Despite the improvements from scaling, LLMs face inherent limitations dictated by the foundational principles of computation, learning, and information, which require specialized approaches for further advancements.

Abstract: Large Language Models (LLMs) have benefited enormously from scaling, yet these gains are bounded by five fundamental limitations: (1) hallucination, (2) context compression, (3) reasoning degradation, (4) retrieval fragility, and (5) multimodal misalignment. While existing surveys describe these phenomena empirically, they lack a rigorous theoretical synthesis connecting them to the foundational limits of computation, information, and learning. This work closes that gap by presenting a unified, proof-informed framework that formalizes the innate theoretical ceilings of LLM scaling. First, computability and uncomputability imply an irreducible residue of error: for any computably enumerable model family, diagonalization guarantees inputs on which some model must fail, and undecidable queries (e.g., halting-style tasks) induce infinite failure sets for all computable predictors. Second, information-theoretic and statistical constraints bound attainable accuracy even on decidable tasks, finite description length enforces compression error, and long-tail factual knowledge requires prohibitive sample complexity. Third, geometric and computational effects compress long contexts far below their nominal size due to positional under-training, encoding attenuation, and softmax crowding. We further show how likelihood-based training favors pattern completion over inference, how retrieval under token limits suffers from semantic drift and coupling noise, and how multimodal scaling inherits shallow cross-modal alignment. Across sections, we pair theorems and empirical evidence to outline where scaling helps, where it saturates, and where it cannot progress, providing both theoretical foundations and practical mitigation paths like bounded-oracle retrieval, positional curricula, and sparse or hierarchical attention.

</details>


### [594] [A Bayesian Model for Multi-stage Censoring](https://arxiv.org/abs/2511.11684)
*Shuvom Sadhuka,Sophia Lin,Emma Pierson,Bonnie Berger*

Main category: cs.LG

TL;DR: The paper introduces a Bayesian model to address statistical biases in healthcare decision funnels, ensuring accurate risk estimation despite censorship in patient outcomes.


<details>
  <summary>Details</summary>
Motivation: Healthcare decisions often involve funnel structures where outcomes are progressively revealed at costly stages; biases arise due to selective censoring of patient groups.

Method: A Bayesian modeling approach is proposed, utilizing synthetic tests and real hospital datasets to analyze decision biases and predict censored outcomes.

Result: The model successfully recovers true parameters and identifies gender-based differences in ICU admission thresholds, underscoring biases against women.

Conclusion: The Bayesian model enhances fair risk prediction, addressing discriminatory practices in selective censoring across healthcare decision-making processes.

Abstract: Many sequential decision settings in healthcare feature funnel structures characterized by a series of stages, such as screenings or evaluations, where the number of patients who advance to each stage progressively decreases and decisions become increasingly costly. For example, an oncologist may first conduct a breast exam, followed by a mammogram for patients with concerning exams, followed by a biopsy for patients with concerning mammograms. A key challenge is that the ground truth outcome, such as the biopsy result, is only revealed at the end of this funnel. The selective censoring of the ground truth can introduce statistical biases in risk estimation, especially in underserved patient groups, whose outcomes are more frequently censored. We develop a Bayesian model for funnel decision structures, drawing from prior work on selective labels and censoring. We first show in synthetic settings that our model is able to recover the true parameters and predict outcomes for censored patients more accurately than baselines. We then apply our model to a dataset of emergency department visits, where in-hospital mortality is observed only for those who are admitted to either the hospital or ICU. We find that there are gender-based differences in hospital and ICU admissions. In particular, our model estimates that the mortality risk threshold to admit women to the ICU is higher for women (5.1%) than for men (4.5%).

</details>


### [595] [MACKO: Sparse Matrix-Vector Multiplication for Low Sparsity](https://arxiv.org/abs/2511.13061)
*Vladimír Macko,Vladimír Boža*

Main category: cs.LG

TL;DR: The paper introduces MACKO-SpMV, a GPU-optimized solution for efficient Sparse Matrix-Vector Multiplication (SpMV) in sparse Large Language Models (LLMs), providing improved speed and reduced memory usage.


<details>
  <summary>Details</summary>
Motivation: Current SpMV methods face inefficiencies with unstructured sparsity (30-90%) in pruned LLMs, limiting the benefits of unstructured pruning in terms of memory reduction and inference speed.

Method: MACKO-SpMV was designed with a GPU-optimized storage format and kernel, aligned with GPU execution models to efficiently handle unstructured sparsity without requiring specialized hardware or format-specific precomputations.

Result: Empirical results show significant improvements: 1.5x memory reduction and 1.2-1.5x speedup for sparse LLMs, outperforming dense formats and other SpMV methods like cuSPARSE, Sputnik, and DASP. Applied to Llama2-7B pruned at 50% sparsity, it achieves notable efficiency gains.

Conclusion: MACKO-SpMV proves that unstructured pruning at 50% sparsity is practical for real-world LLMs, advancing performance and justifying its adoption for memory and speed optimizations.

Abstract: Sparse Matrix-Vector Multiplication (SpMV) is a fundamental operation in the inference of sparse Large Language Models (LLMs). Because existing SpMV methods perform poorly under the low and unstructured sparsity (30-90%) commonly observed in pruned LLMs, unstructured pruning provided only limited memory reduction and speedup. We propose MACKO-SpMV, a GPU-optimized format and kernel co-designed to reduce storage overhead while preserving compatibility with the GPU's execution model. This enables efficient SpMV for unstructured sparsity without specialized hardware units (e.g., tensor cores) or format-specific precomputation. Empirical results show that at sparsity 50%, MACKO is the first approach with significant 1.5x memory reduction and 1.2-1.5x speedup over dense representation. Speedups over other SpMV baselines: 2.8-13.0x over cuSPARSE, 1.9-2.6x over Sputnik, and 2.2-2.5x over DASP. Applied to Llama2-7B pruned with Wanda to sparsity 50%, it delivers 1.5x memory reduction and 1.5x faster inference at fp16 precision. Thanks to MACKO, unstructured pruning at 50% sparsity is now justified in real-world LLM workloads.

</details>


### [596] [R-Tuning: Wavelet-Decomposed Replay and Semantic Alignment for Continual Adaptation of Pretrained Time-Series Models](https://arxiv.org/abs/2511.11685)
*Tianyi Yin,Jingwei Wang,Chenze Wang,Han Wang,Jiexuan Cai,Min Liu,Yunlong Ma,Kun Gao,Yuting Song,Weiming Shen*

Main category: cs.LG

TL;DR: The paper introduces Replay Tuning (R-Tuning), a framework for continuous adaptation of pre-trained time-series models using a frequency-aware replay strategy to avoid catastrophic forgetting.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of adapting pre-trained time-series models to evolving data distributions without suffering from catastrophic forgetting due to the lack of access to the original training data.

Method: Developing R-Tuning, a framework leveraging a unified latent space and frequency-aware replay strategy. It utilizes wavelet-based decomposition to enhance sample diversity and incorporates latent consistency constraints for robust knowledge retention.

Result: R-Tuning improves MAE and MSE by up to 46.9% and 46.8%, respectively, on new tasks and retains prior knowledge with up to 5.7% and 6.0% gains on old tasks. It outperforms state-of-the-art methods under few-shot settings.

Conclusion: R-Tuning effectively addresses catastrophic forgetting and demonstrates superior adaptation and retention performance in time-series forecasting, even under challenging few-shot conditions.

Abstract: Pre-trained models have demonstrated exceptional generalization capabilities in time-series forecasting; however, adapting them to evolving data distributions remains a significant challenge. A key hurdle lies in accessing the original training data, as fine-tuning solely on new data often leads to catastrophic forgetting. To address this issue, we propose Replay Tuning (R-Tuning), a novel framework designed for the continual adaptation of pre-trained time-series models. R-Tuning constructs a unified latent space that captures both prior and current task knowledge through a frequency-aware replay strategy. Specifically, it augments model-generated samples via wavelet-based decomposition across multiple frequency bands, generating trend-preserving and fusion-enhanced variants to improve representation diversity and replay efficiency. To further reduce reliance on synthetic samples, R-Tuning introduces a latent consistency constraint that aligns new representations with the prior task space. This constraint guides joint optimization within a compact and semantically coherent latent space, ensuring robust knowledge retention and adaptation. Extensive experimental results demonstrate the superiority of R-Tuning, which reduces MAE and MSE by up to 46.9% and 46.8%, respectively, on new tasks, while preserving prior knowledge with gains of up to 5.7% and 6.0% on old tasks. Notably, under few-shot settings, R-Tuning outperforms all state-of-the-art baselines even when synthetic proxy samples account for only 5% of the new task dataset.

</details>


### [597] [Regularized Schrödinger: Alleviating Distortion and Exposure Bias in Solving Inverse Problems](https://arxiv.org/abs/2511.11686)
*Qing Yao,Lijian Gao,Qirong Mao,Dong Ming*

Main category: cs.LG

TL;DR: The paper proposes Regularized Schrödinger Bridge (RSB), a novel method for inverse problems, addressing distortion-perception tradeoff and exposure bias in diffusion models, validated effectively on speech enhancement tasks.


<details>
  <summary>Details</summary>
Motivation: Current diffusion models for solving inverse problems struggle with the distortion-perception tradeoff and exposure bias, impacting reconstruction quality and prediction accuracy.

Method: The proposed RSB method uses a regularized training strategy by perturbing input states and targets, exposing the model to simulated prediction errors, and performing posterior mean interpolation to mitigate challenges.

Result: RSB demonstrates superior performance over state-of-the-art methods in speech enhancement tasks, significantly improving distortion metrics and minimizing exposure biases.

Conclusion: RSB effectively enhances diffusion models for inverse problems, resolving critical challenges and showing substantial improvement over existing solutions.

Abstract: Diffusion models serve as a powerful generative framework for solving inverse problems. However, they still face two key challenges: 1) the distortion-perception tradeoff, where improving perceptual quality often degrades reconstruction fidelity, and 2) the exposure bias problem, where the training-inference input mismatch leads to prediction error accumulation and reduced reconstruction quality. In this work, we propose the Regularized Schrödinger Bridge (RSB), an adaptation of Schrödinger Bridge tailored for inverse problems that addresses the above limitations. RSB employs a novel regularized training strategy that perturbs both the input states and targets, effectively mitigating exposure bias by exposing the model to simulated prediction errors and also alleviating distortion by well-designed interpolation via the posterior mean. Extensive experiments on two typical inverse problems for speech enhancement demonstrate that RSB outperforms state-of-the-art methods, significantly improving distortion metrics and effectively reducing exposure bias.

</details>


### [598] [Hierarchical Schedule Optimization for Fast and Robust Diffusion Model Sampling](https://arxiv.org/abs/2511.11688)
*Aihua Zhu,Rui Su,Qinglin Zhao,Li Feng,Meng Shen,Shibo He*

Main category: cs.LG

TL;DR: The paper introduces Hierarchical-Schedule-Optimizer (HSO) to enhance sampling in diffusion models by optimizing schedules, achieving state-of-the-art performance without retraining.


<details>
  <summary>Details</summary>
Motivation: Diffusion models have high generative fidelity but are slowed by iterative sampling. Existing schedule optimization methods fail to meet key principles, necessitating better solutions.

Method: The proposed HSO employs bi-level optimization, alternating between global search for initialization and local schedule refinement. It uses the Midpoint Error Proxy (MEP) for local optimization and Spacing-Penalized Fitness (SPF) to ensure robustness.

Result: HSO significantly improves sampling efficiency, achieving an FID of 11.94 on LAION-Aesthetics with only 5 NFEs, requiring under 8 seconds and no retraining.

Conclusion: HSO sets a new state-of-the-art for fast, training-free diffusion model sampling, providing a practical acceleration solution for real-world applications.

Abstract: Diffusion probabilistic models have set a new standard for generative fidelity but are hindered by a slow iterative sampling process. A powerful training-free strategy to accelerate this process is Schedule Optimization, which aims to find an optimal distribution of timesteps for a fixed and small Number of Function Evaluations (NFE) to maximize sample quality. To this end, a successful schedule optimization method must adhere to four core principles: effectiveness, adaptivity, practical robustness, and computational efficiency. However, existing paradigms struggle to satisfy these principles simultaneously, motivating the need for a more advanced solution. To overcome these limitations, we propose the Hierarchical-Schedule-Optimizer (HSO), a novel and efficient bi-level optimization framework. HSO reframes the search for a globally optimal schedule into a more tractable problem by iteratively alternating between two synergistic levels: an upper-level global search for an optimal initialization strategy and a lower-level local optimization for schedule refinement. This process is guided by two key innovations: the Midpoint Error Proxy (MEP), a solver-agnostic and numerically stable objective for effective local optimization, and the Spacing-Penalized Fitness (SPF) function, which ensures practical robustness by penalizing pathologically close timesteps. Extensive experiments show that HSO sets a new state-of-the-art for training-free sampling in the extremely low-NFE regime. For instance, with an NFE of just 5, HSO achieves a remarkable FID of 11.94 on LAION-Aesthetics with Stable Diffusion v2.1. Crucially, this level of performance is attained not through costly retraining, but with a one-time optimization cost of less than 8 seconds, presenting a highly practical and efficient paradigm for diffusion model acceleration.

</details>


### [599] [Doubly Debiased Test-Time Prompt Tuning for Vision-Language Models](https://arxiv.org/abs/2511.11690)
*Fei Song,Yi Li,Rui Wang,Jiahuan Zhou,Changwen Zheng,Jiangmeng Li*

Main category: cs.LG

TL;DR: The paper proposes a method to mitigate prompt optimization bias in test-time prompt tuning for vision-language models, addressing both model and data perspectives.


<details>
  <summary>Details</summary>
Motivation: Exploring the causes of suboptimal performance due to prompt optimization bias in test-time prompt tuning for vision-language models.

Method: Introduces a dynamic retrieval-augmented modulation module for high-confidence knowledge and a reliability-aware prompt optimization module using confidence-based weighted ensemble and cross-modal consistency distillation.

Result: Extensive experiments on datasets with distribution shifts and cross-dataset generalization show the proposed method outperforms baselines.

Conclusion: The proposed method effectively mitigates prompt optimization bias and enhances the generalization capabilities of vision-language models.

Abstract: Test-time prompt tuning for vision-language models has demonstrated impressive generalization capabilities under zero-shot settings. However, tuning the learnable prompts solely based on unlabeled test data may induce prompt optimization bias, ultimately leading to suboptimal performance on downstream tasks. In this work, we analyze the underlying causes of prompt optimization bias from both the model and data perspectives. In terms of the model, the entropy minimization objective typically focuses on reducing the entropy of model predictions while overlooking their correctness. This can result in overconfident yet incorrect outputs, thereby compromising the quality of prompt optimization. On the data side, prompts affected by optimization bias can introduce misalignment between visual and textual modalities, which further aggravates the prompt optimization bias. To this end, we propose a Doubly Debiased Test-Time Prompt Tuning method. Specifically, we first introduce a dynamic retrieval-augmented modulation module that retrieves high-confidence knowledge from a dynamic knowledge base using the test image feature as a query, and uses the retrieved knowledge to modulate the predictions. Guided by the refined predictions, we further develop a reliability-aware prompt optimization module that incorporates a confidence-based weighted ensemble and cross-modal consistency distillation to impose regularization constraints during prompt tuning. Extensive experiments across 15 benchmark datasets involving both natural distribution shifts and cross-datasets generalization demonstrate that our method outperforms baselines, validating its effectiveness in mitigating prompt optimization bias.

</details>


### [600] [Enhancing Reinforcement Learning in 3D Environments through Semantic Segmentation: A Case Study in ViZDoom](https://arxiv.org/abs/2511.11703)
*Hugo Huang*

Main category: cs.LG

TL;DR: This study tackles memory consumption in RL and complexity in POMDPs in 3D environments by employing semantic segmentation-based input representations in ViZDoom. Results show reduced memory usage and improved RL agent performance.


<details>
  <summary>Details</summary>
Motivation: To address challenges in reinforcement learning in 3D environments, such as high memory consumption and learning complexity in POMDPs.

Method: Proposed using two input representations (SS-only and RGB+SS) based on semantic segmentation and tested them in ViZDoom deathmatches with controlled experiments on perfect segmentation data.

Result: SS-only reduced memory consumption by 66.6% to 98.6% using compression techniques, and RGB+SS enhanced RL agents' performance. Density-based heatmaps were used to visualize movement patterns.

Conclusion: Semantic segmentation-based representations are effective in reducing memory consumption and improving RL performance in 3D environments, overcoming common pitfalls in previous approaches.

Abstract: Reinforcement learning (RL) in 3D environments with high-dimensional sensory input poses two major challenges: (1) the high memory consumption induced by memory buffers required to stabilise learning, and (2) the complexity of learning in partially observable Markov Decision Processes (POMDPs). This project addresses these challenges by proposing two novel input representations: SS-only and RGB+SS, both employing semantic segmentation on RGB colour images. Experiments were conducted in deathmatches of ViZDoom, utilizing perfect segmentation results for controlled evaluation. Our results showed that SS-only was able to reduce the memory consumption of memory buffers by at least 66.6%, and up to 98.6% when a vectorisable lossless compression technique with minimal overhead such as run-length encoding is applied. Meanwhile, RGB+SS significantly enhances RL agents' performance with the additional semantic information provided. Furthermore, we explored density-based heatmapping as a tool to visualise RL agents' movement patterns and evaluate their suitability for data collection. A brief comparison with a previous approach highlights how our method overcame common pitfalls in applying semantic segmentation in 3D environments like ViZDoom.

</details>


### [601] [Beyond saliency: enhancing explanation of speech emotion recognition with expert-referenced acoustic cues](https://arxiv.org/abs/2511.11691)
*Seham Nasr,Zhao Ren,David Johnson*

Main category: cs.LG

TL;DR: The paper addresses the issue of a lack of interpretability in Explainable AI (XAI) for Speech Emotion Recognition (SER) and proposes a framework that enhances explanation quality by linking salient features to acoustic cues referenced by experts.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in the limitations of current saliency-based interpretability methods, which inadequately connect highlighted spectrogram regions to meaningful acoustic emotion markers, undermining faithfulness and interpretability.

Method: The proposed method quantifies magnitudes of acoustic cues within salient regions identified in spectrograms, linking highlighted areas to expert-referenced markers of emotions in speech.

Result: Experiments on benchmark SER datasets demonstrate that the framework offers improved explanation quality by connecting salient regions to theory-driven acoustic cues, compared to standard saliency interpretations.

Conclusion: The framework provides more understandable and plausible explanations for SER models, improving their transparency and reliability, and serves as a foundational step for trustworthy speech-based emotion recognition systems.

Abstract: Explainable AI (XAI) for Speech Emotion Recognition (SER) is critical for building transparent, trustworthy models. Current saliency-based methods, adapted from vision, highlight spectrogram regions but fail to show whether these regions correspond to meaningful acoustic markers of emotion, limiting faithfulness and interpretability. We propose a framework that overcomes these limitations by quantifying the magnitudes of cues within salient regions. This clarifies "what" is highlighted and connects it to "why" it matters, linking saliency to expert-referenced acoustic cues of speech emotions. Experiments on benchmark SER datasets show that our approach improves explanation quality by explicitly linking salient regions to theory-driven speech emotions expert-referenced acoustics. Compared to standard saliency methods, it provides more understandable and plausible explanations of SER models, offering a foundational step towards trustworthy speech-based affective computing.

</details>


### [602] [AnchorDS: Anchoring Dynamic Sources for Semantically Consistent Text-to-3D Generation](https://arxiv.org/abs/2511.11692)
*Jiayin Zhu,Linlin Yang,Yicong Li,Angela Yao*

Main category: cs.LG

TL;DR: The paper proposes AnchorDS, a framework to improve text-to-3D optimization by addressing semantic over-smoothing using dynamically anchored guidance and refined image conditions, resulting in better and efficient outputs.


<details>
  <summary>Details</summary>
Motivation: To address issues of semantic over-smoothing arising from static guidance in current optimization-based text-to-3D methods.

Method: The authors reformulate text-to-3D optimization as mapping dynamically evolving source distributions to fixed target distributions and introduce AnchorDS, a mechanism using image-conditioned anchoring and additional penalty strategies for refinement.

Result: AnchorDS achieves finer detail, better semantics, and improved consistency in text-to-3D generation, particularly for complex prompts, while offering efficiency improvements.

Conclusion: AnchorDS successfully addresses the limitations of semantic over-smoothing, providing superior quality and efficiency compared to existing methods.

Abstract: Optimization-based text-to-3D methods distill guidance from 2D generative models via Score Distillation Sampling (SDS), but implicitly treat this guidance as static. This work shows that ignoring source dynamics yields inconsistent trajectories that suppress or merge semantic cues, leading to "semantic over-smoothing" artifacts. As such, we reformulate text-to-3D optimization as mapping a dynamically evolving source distribution to a fixed target distribution. We cast the problem into a dual-conditioned latent space, conditioned on both the text prompt and the intermediately rendered image. Given this joint setup, we observe that the image condition naturally anchors the current source distribution. Building on this insight, we introduce AnchorDS, an improved score distillation mechanism that provides state-anchored guidance with image conditions and stabilizes generation. We further penalize erroneous source estimates and design a lightweight filter strategy and fine-tuning strategy that refines the anchor with negligible overhead. AnchorDS produces finer-grained detail, more natural colours, and stronger semantic consistency, particularly for complex prompts, while maintaining efficiency. Extensive experiments show that our method surpasses previous methods in both quality and efficiency.

</details>


### [603] [Toward Dignity-Aware AI: Next-Generation Elderly Monitoring from Fall Detection to ADL](https://arxiv.org/abs/2511.11696)
*Xun Shao,Aoba Otani,Yuto Hirasuka,Runji Cai,Seng W. Loke*

Main category: cs.LG

TL;DR: This paper proposes a new direction in elderly monitoring systems, shifting focus from fall detection to Activities of Daily Living (ADL) recognition, using privacy-preserving federated AI methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to support independence and dignity in aging societies by recognizing daily routines through AI, rather than focusing solely on fall detection.

Method: They use the SISFall dataset and its GAN-augmented variants for experiments in fall detection as a proxy task. Preliminary results include federated learning under non-IID conditions and deployment on Jetson Orin Nano devices.

Result: Initial results indicate feasibility of federated AI systems for ADL monitoring, though challenges like domain shift and data scarcity were identified.

Conclusion: The paper emphasizes the transition to comprehensive ADL recognition, outlining challenges and proposing directions for privacy-preserving and human-centered elderly care AI systems.

Abstract: This position paper envisions a next-generation elderly monitoring system that moves beyond fall detection toward the broader goal of Activities of Daily Living (ADL) recognition. Our ultimate aim is to design privacy-preserving, edge-deployed, and federated AI systems that can robustly detect and understand daily routines, supporting independence and dignity in aging societies. At present, ADL-specific datasets are still under collection. As a preliminary step, we demonstrate feasibility through experiments using the SISFall dataset and its GAN-augmented variants, treating fall detection as a proxy task. We report initial results on federated learning with non-IID conditions, and embedded deployment on Jetson Orin Nano devices. We then outline open challenges such as domain shift, data scarcity, and privacy risks, and propose directions toward full ADL monitoring in smart-room environments. This work highlights the transition from single-task detection to comprehensive daily activity recognition, providing both early evidence and a roadmap for sustainable and human-centered elderly care AI.

</details>


### [604] [Benchmarking GNNs for OOD Materials Property Prediction with Uncertainty Quantification](https://arxiv.org/abs/2511.11697)
*Liqin Tan,Pin Chen,Menghan Liu,Xiean Wang,Jianhuan Cen,Qingsong Zou*

Main category: cs.LG

TL;DR: MatUQ is a benchmark framework that evaluates graph neural networks' (GNNs) abilities in predicting out-of-distribution (OOD) material properties while including uncertainty quantification (UQ).


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of evaluating GNNs for accurate and reliable OOD materials property prediction, emphasizing the importance of capturing uncertainty.

Method: The authors implemented MatUQ with 1,375 OOD prediction tasks, leveraging a new structure-aware splitting strategy (SOAP-LOCO), a unified uncertainty-aware training protocol using Monte Carlo Dropout and Deep Evidential Regression, and a new uncertainty metric (D-EviU).

Result: The study found uncertainty-aware training improved prediction accuracy significantly (average 70.6% error reduction). Benchmarks revealed that different models excel in different cases, with older models like SchNet still competitive and newer models like CrystalFramer showing strong performance in certain tasks.

Conclusion: MatUQ offers insights into selecting reliable GNN models under distribution shifts and emphasizes the relevance of uncertainty-aware methods in improving prediction accuracy and reliability.

Abstract: We present MatUQ, a benchmark framework for evaluating graph neural networks (GNNs) on out-of-distribution (OOD) materials property prediction with uncertainty quantification (UQ). MatUQ comprises 1,375 OOD prediction tasks constructed from six materials datasets using five OFM-based and a newly proposed structure-aware splitting strategy, SOAP-LOCO, which captures local atomic environments more effectively. We evaluate 12 representative GNN models under a unified uncertainty-aware training protocol that combines Monte Carlo Dropout and Deep Evidential Regression (DER), and introduce a novel uncertainty metric, D-EviU, which shows the strongest correlation with prediction errors in most tasks. Our experiments yield two key findings. First, the uncertainty-aware training approach significantly improves model prediction accuracy, reducing errors by an average of 70.6\% across challenging OOD scenarios. Second, the benchmark reveals that no single model dominates universally: earlier models such as SchNet and ALIGNN remain competitive, while newer models like CrystalFramer and SODNet demonstrate superior performance on specific material properties. These results provide practical insights for selecting reliable models under distribution shifts in materials discovery.

</details>


### [605] [Are LLMs The Way Forward? A Case Study on LLM-Guided Reinforcement Learning for Decentralized Autonomous Driving](https://arxiv.org/abs/2511.12751)
*Timur Anvar,Jeffrey Chen,Yuyan Wang,Rohan Chandra*

Main category: cs.LG

TL;DR: This paper investigates the use of small local Large Language Models (LLMs) for improving autonomous vehicle navigation in complex highways through reward shaping, comparing RL-only, LLM-only, and hybrid approaches.


<details>
  <summary>Details</summary>
Motivation: Autonomous vehicle navigation remains challenging due to RL's reliance on rigid reward functions which fail in complex, out-of-distribution settings. The paper explores whether smaller, locally deployed LLMs can supplement RL to address such limitations while avoiding the drawbacks of direct LLM control.

Method: A case study compares RL-only, LLM-only, and hybrid approaches, where small LLMs score state-action transitions during training to shape RL rewards, and RL policies are tested separately.

Result: RL-only agents achieved reasonable success rates and efficiency, LLM-only agents achieved higher success but degraded speed, while hybrid approaches mediated between the two extremes. LLM-based methods showed conservative biases and variability.

Conclusion: Small LLMs can enhance understanding for RL reward shaping but face limitations like instability, inefficiency, and conservative tendencies in safety-critical tasks.

Abstract: Autonomous vehicle navigation in complex environments such as dense and fast-moving highways and merging scenarios remains an active area of research. A key limitation of RL is its reliance on well-specified reward functions, which often fail to capture the full semantic and social complexity of diverse, out-of-distribution situations. As a result, a rapidly growing line of research explores using Large Language Models (LLMs) to replace or supplement RL for direct planning and control, on account of their ability to reason about rich semantic context. However, LLMs present significant drawbacks: they can be unstable in zero-shot safety-critical settings, produce inconsistent outputs, and often depend on expensive API calls with network latency. This motivates our investigation into whether small, locally deployed LLMs (< 14B parameters) can meaningfully support autonomous highway driving through reward shaping rather than direct control. We present a case study comparing RL-only, LLM-only, and hybrid approaches, where LLMs augment RL rewards by scoring state-action transitions during training, while standard RL policies execute at test time. Our findings reveal that RL-only agents achieve moderate success rates (73-89%) with reasonable efficiency, LLM-only agents can reach higher success rates (up to 94%) but with severely degraded speed performance, and hybrid approaches consistently fall between these extremes. Critically, despite explicit efficiency instructions, LLM-influenced approaches exhibit systematic conservative bias with substantial model-dependent variability, highlighting important limitations of current small LLMs for safety-critical control tasks.

</details>


### [606] [Moirai 2.0: When Less Is More for Time Series Forecasting](https://arxiv.org/abs/2511.11698)
*Chenghao Liu,Taha Aksu,Juncheng Liu,Xu Liu,Hanshu Yan,Quang Pham,Doyen Sahoo,Caiming Xiong,Silvio Savarese,Junnan Li*

Main category: cs.LG

TL;DR: Moirai 2.0, a decoder-only time-series model, offers improved forecasting accuracy and efficiency on Gift-Eval benchmark, being faster and smaller than Moirai 1.0.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance time-series forecasting accuracy and efficiency by addressing limitations in previous model versions.

Method: The method involves replacing Moirai 1.0's complex architecture with a simpler decoder-only framework, quantile forecasting, and multi-token prediction.

Result: Moirai 2.0 ranks among top pretrained models, outperforming larger models, twice faster, thirty times smaller, and exhibits robust results across domains.

Conclusion: Moirai 2.0 demonstrates significant advancements in efficiency and accuracy over previous versions, paving the way for future research on data scaling and long-horizon modeling.

Abstract: We introduce Moirai 2.0, a decoder-only time-series foundation model trained on a new corpus of 36M series. The model adopts quantile forecasting and multi-token prediction, improving both probabilistic accuracy and inference efficiency. On the Gift-Eval benchmark, it ranks among the top pretrained models while achieving a strong trade-off between accuracy, speed, and model size. Compared to Moirai 1.0, Moirai 2.0 replaces masked-encoder training, multi-patch inputs, and mixture-distribution outputs with a simpler decoder-only architecture, single patch, and quantile loss. Ablation studies isolate these changes -- showing that the decoder-only backbone along with recursive multi-quantile decoding contribute most to the gains. Additional experiments show that Moirai 2.0 outperforms larger models from the same family and exhibits robust domain-level results. In terms of efficiency and model size, Moirai 2.0 is twice as fast and thirty times smaller than its prior best version, Moirai 1.0-Large, while also performing better. Model performance plateaus with increasing parameter count and declines at longer horizons, motivating future work on data scaling and long-horizon modeling. We release code and evaluation details to support further research.

</details>


### [607] [Tighter Truncated Rectangular Prism Approximation for RNN Robustness Verification](https://arxiv.org/abs/2511.11699)
*Xingqi Lin,Liangyu Chen,Min Wu,Min Zhang,Zhenbing Zeng*

Main category: cs.LG

TL;DR: The paper presents a novel approach to improve robustness verification of Recurrent Neural Networks (RNNs) using tighter over-approximation techniques, leading to better accuracy.


<details>
  <summary>Details</summary>
Motivation: Current methods overly approximate non-linear activation functions, resulting in lower verification accuracy for RNNs.

Method: The authors propose a truncated rectangular prism-based approach involving two linear relaxation planes and refinement-driven optimization to minimize over-approximation of Hadamard product surfaces.

Result: The implementation of their method, DeepPrism, outperforms state-of-the-art approaches in robustness verification across tasks like image classification, speech recognition, and sentiment analysis.

Conclusion: DeepPrism's novel method enhances RNN robustness verification accuracy, offering significant improvements over existing techniques.

Abstract: Robustness verification is a promising technique for rigorously proving Recurrent Neural Networks (RNNs) robustly. A key challenge is to over-approximate the nonlinear activation functions with linear constraints, which can transform the verification problem into an efficiently solvable linear programming problem. Existing methods over-approximate the nonlinear parts with linear bounding planes individually, which may cause significant over-estimation and lead to lower verification accuracy. In this paper, in order to tightly enclose the three-dimensional nonlinear surface generated by the Hadamard product, we propose a novel truncated rectangular prism formed by two linear relaxation planes and a refinement-driven method to minimize both its volume and surface area for tighter over-approximation. Based on this approximation, we implement a prototype DeepPrism for RNN robustness verification. The experimental results demonstrate that \emph{DeepPrism} has significant improvement compared with the state-of-the-art approaches in various tasks of image classification, speech recognition and sentiment analysis.

</details>


### [608] [Bayesian Neural Networks with Monte Carlo Dropout for Probabilistic Electricity Price Forecasting](https://arxiv.org/abs/2511.11701)
*Abhinav Das,Stephan Schlüter*

Main category: cs.LG

TL;DR: The paper proposes a Bayesian neural network framework for forecasting electricity prices with improved accuracy and reliability compared to traditional models.


<details>
  <summary>Details</summary>
Motivation: Deregulated electricity markets require accurate price forecasting due to volatility influenced by complex supply-demand dynamics and external factors. Traditional models fail to address uncertainties effectively, limiting risk management capabilities.

Method: The paper introduces a framework using Bayesian neural networks with Monte Carlo dropout, training separate models for each hour to capture hourly price patterns. Comparisons were made with benchmark models such as GARCHX and LEAR.

Result: The proposed probabilistic model outperformed traditional benchmark models in point prediction accuracy and interval reliability.

Conclusion: This framework provides valuable insights into using probabilistic neural models for more effective energy market predictions.

Abstract: Accurate electricity price forecasting is critical for strategic decision-making in deregulated electricity markets, where volatility stems from complex supply-demand dynamics and external factors. Traditional point forecasts often fail to capture inherent uncertainties, limiting their utility for risk management. This work presents a framework for probabilistic electricity price forecasting using Bayesian neural networks (BNNs) with Monte Carlo (MC) dropout, training separate models for each hour of the day to capture diurnal patterns. A critical assessment and comparison with the benchmark model, namely: generalized autoregressive conditional heteroskedasticity with exogenous variable (GARCHX) model and the LASSO estimated auto-regressive model (LEAR), highlights that the proposed model outperforms the benchmark models in terms of point prediction and intervals. This work serves as a reference for leveraging probabilistic neural models in energy market predictions.

</details>


### [609] [Simple Vision-Language Math Reasoning via Rendered Text](https://arxiv.org/abs/2511.11704)
*Matvey Skripkin,Elizaveta Goncharova,Andrey Kuznetsov*

Main category: cs.LG

TL;DR: This paper proposes a technique to train vision-language models for solving math problems by converting LaTeX equations into images with structured prompts, achieving high accuracy in reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Math problem-solving requires robust reasoning capabilities, and this paper aims to enhance vision-language models by combining LaTeX-based image rendering with structured prompts to improve performance on math and general reasoning tasks.

Method: The authors render LaTeX equations into images and use structured chain-of-thought prompts as a simplified text-to-vision augmentation in training vision-language models.

Result: The proposed method delivers state-of-the-art accuracy in math reasoning tasks, outperforming existing solvers by up to 20% on benchmarks like MMMU, ChartQA, and DocVQA.

Conclusion: This approach demonstrates that simple text-to-vision augmentation can significantly enhance vision-language model reasoning, achieving competitive results across mathematical and general-domain tasks.

Abstract: We present a lightweight yet effective pipeline for training vision-language models to solve math problems by rendering LaTeX encoded equations into images and pairing them with structured chain-of-thought prompts. This simple text-to-vision augmentation enables compact multimodal architectures to achieve state-of-the-art reasoning accuracy. Through systematic ablations, we find that rendering fidelity and prompt design are the primary drivers of performance. Despite its simplicity, our approach consistently matches or surpasses both open-source and proprietary math-focused vision-language solvers on widely used benchmarks, while preserving broad general-domain competence - showing gains on tasks such as MMMU, ChartQA, and DocVQA of up to 20%.

</details>


### [610] [Multimodal ML: Quantifying the Improvement of Calorie Estimation Through Image-Text Pairs](https://arxiv.org/abs/2511.11705)
*Arya Narang*

Main category: cs.LG

TL;DR: The paper explores whether adding dish names as text input to calorie estimation models can improve prediction accuracy over image-only models, demonstrating a slight improvement.


<details>
  <summary>Details</summary>
Motivation: To investigate how short textual inputs combined with images can enhance calorie estimation accuracy and to analyze the significance of these improvements.

Method: The study uses TensorFlow and the Nutrition5k dataset to train both an image-only CNN and a multimodal CNN that incorporates both text (dish names) and image inputs.

Result: Incorporating dish names into the multimodal model reduced the mean absolute error (MAE) of calorie estimations by 1.06 kcal, a 1.25% improvement compared to the image-only model.

Conclusion: Adding textual inputs like dish names alongside images in calorie estimation models results in a statistically significant yet modest improvement in accuracy.

Abstract: This paper determines the extent to which short textual inputs (in this case, names of dishes) can improve calorie estimation compared to an image-only baseline model and whether any improvements are statistically significant. Utilizes the TensorFlow library and the Nutrition5k dataset (curated by Google) to train both an image-only CNN and multimodal CNN that accepts both text and an image as input. The MAE of calorie estimations was reduced by 1.06 kcal from 84.76 kcal to 83.70 kcal (1.25% improvement) when using the multimodal model.

</details>


### [611] [Context-Aware Multimodal Representation Learning for Spatio-Temporally Explicit Environmental modelling](https://arxiv.org/abs/2511.11706)
*Julia Peters,Karin Mora,Miguel D. Mahecha,Chaonan Ji,David Montero,Clemens Mosig,Guido Kraemer*

Main category: cs.LG

TL;DR: The paper introduces a framework for Earth observation models that integrates Sentinel-1 and Sentinel-2 data into a unified spatio-temporal representation for ecosystem analysis.


<details>
  <summary>Details</summary>
Motivation: Current Earth observation models are limited in spatio-temporal resolution, hampering their use in ecological analyses requiring high spatial detail and temporal fidelity.

Method: The framework models each sensor independently before combining them in a shared two-stage model, allowing modality-specific tuning while enabling high spatio-temporal resolution and easy extension to new sensors.

Result: The learned embeddings demonstrate spatial/semantic consistency across landscapes and encode ecologically meaningful patterns, supporting fine-scale ecosystem dynamics analysis.

Conclusion: This framework offers a flexible, scalable approach for integrating diverse Earth observation data, enhancing its utility in environmental science applications.

Abstract: Earth observation (EO) foundation models have emerged as an effective approach to derive latent representations of the Earth system from various remote sensing sensors. These models produce embeddings that can be used as analysis-ready datasets, enabling the modelling of ecosystem dynamics without extensive sensor-specific preprocessing. However, existing models typically operate at fixed spatial or temporal scales, limiting their use for ecological analyses that require both fine spatial detail and high temporal fidelity. To overcome these limitations, we propose a representation learning framework that integrates different EO modalities into a unified feature space at high spatio-temporal resolution. We introduce the framework using Sentinel-1 and Sentinel-2 data as representative modalities. Our approach produces a latent space at native 10 m resolution and the temporal frequency of cloud-free Sentinel-2 acquisitions. Each sensor is first modeled independently to capture its sensor-specific characteristics. Their representations are then combined into a shared model. This two-stage design enables modality-specific optimisation and easy extension to new sensors, retaining pretrained encoders while retraining only fusion layers. This enables the model to capture complementary remote sensing data and to preserve coherence across space and time. Qualitative analyses reveal that the learned embeddings exhibit high spatial and semantic consistency across heterogeneous landscapes. Quantitative evaluation in modelling Gross Primary Production reveals that they encode ecologically meaningful patterns and retain sufficient temporal fidelity to support fine-scale analyses. Overall, the proposed framework provides a flexible, analysis-ready representation learning approach for environmental applications requiring diverse spatial and temporal resolutions.

</details>


### [612] [FSC-Net: Fast-Slow Consolidation Networks for Continual Learning](https://arxiv.org/abs/2511.11707)
*Mohamed El Gorrim*

Main category: cs.LG

TL;DR: FSC-Net is a dual-network system aimed at overcoming catastrophic forgetting in continual learning by separating rapid learning and gradual consolidation mechanisms. Pure replay during consolidation outperformed methods using distillation in experiments.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the challenge of catastrophic forgetting in neural networks, which occurs when previously acquired knowledge is degraded upon learning new tasks.

Method: The paper introduces FSC-Net, a dual-network architecture where a fast network learns tasks immediately and a slow network consolidates past knowledge through replay and distillation.

Result: The proposed method (FSC-Net) demonstrated significant gains in retention accuracy on Split-MNIST (+4.27pp) and Split-CIFAR-10 (+8.20pp) datasets compared to using the fast network alone.

Conclusion: The study concludes that the dual-timescale consolidation approach is more effective at mitigating catastrophic forgetting than complex architectural features, though further improvements in backbones are needed to boost absolute performance.

Abstract: Continual learning remains challenging due to catastrophic forgetting, where neural networks lose previously acquired knowledge when learning new tasks. Inspired by memory consolidation in neuroscience, we propose FSC-Net (Fast-Slow Consolidation Networks), a dual-network architecture that separates rapid task learning from gradual knowledge consolidation. Our method employs a fast network (NN1) for immediate adaptation to new tasks and a slow network (NN2) that consolidates knowledge through distillation and replay. Within the family of MLP-based NN1 variants we evaluated, consolidation effectiveness is driven more by methodology than architectural embellishments -- a simple MLP outperforms more complex similarity-gated variants by 1.2pp. Through systematic hyperparameter analysis, we observed empirically that pure replay without distillation during consolidation achieves superior performance, consistent with the hypothesis that distillation from the fast network introduces recency bias. On Split-MNIST (30 seeds), FSC-Net achieves 91.71% +/- 0.62% retention accuracy, a +4.27pp gain over the fast network alone (87.43% +/- 1.27%, paired t=23.585, p < 1e-10). On Split-CIFAR-10 (5 seeds), our method achieves 33.31% +/- 0.38% retention with an +8.20pp gain over the fast network alone (25.11% +/- 1.61%, paired t=9.75, p < 1e-3), demonstrating +8.20pp gain, though absolute performance (33.31%) remains modest and below random expectation, highlighting need for stronger backbones. Our results provide empirical evidence that the dual-timescale consolidation mechanism, rather than architectural complexity, is central to mitigating catastrophic forgetting in this setting.

</details>


### [613] [Which Sparse Autoencoder Features Are Real? Model-X Knockoffs for False Discovery Rate Control](https://arxiv.org/abs/2511.11711)
*Tsogt-Ochir Enkhbayar*

Main category: cs.LG

TL;DR: The paper integrates Model-X knockoffs into sparse autoencoder (SAE) feature selection to control the false discovery rate (FDR) and identify interpretable features with finite-sample guarantees.


<details>
  <summary>Details</summary>
Motivation: Sparse autoencoders are valuable for neural network interpretability, but distinguishing real patterns from erroneous correlations remains difficult.

Method: Applying Model-X knockoffs with knockoff+ to sparse autoencoders, using Gaussian surrogates for latent distributions to achieve FDR control under Model-X assumptions.

Result: Analysis of 512 high-activity SAE latents identifies 129 features at a target FDR of 0.1, distinguishing task-relevant (25%) from non-task-relevant (75%) features based on knockoff statistics.

Conclusion: The method combines sparse autoencoders with inference aware of multiple-testing, offering a robust framework for reliable feature discovery and advancing mechanistic interpretability.

Abstract: Although sparse autoencoders (SAEs) are crucial for identifying interpretable features in neural networks, it is still challenging to distinguish between real computational patterns and erroneous correlations. We introduce Model-X knockoffs to SAE feature selection, using knock-off+ to control the false discovery rate (FDR) with finite-sample guarantees under the standard Model-X assumptions (in our case, via a Gaussian surrogate for the latent distribution). We select 129 features at a target FDR q=0.1 after analyzing 512 high-activity SAE latents for sentiment classification using Pythia-70M. About 25% of the latents under examination carry task-relevant signal, whereas 75% do not, according to the chosen set, which displays a 5.40x separation in knockoff statistics compared to non-selected features. Our method offers a re-producible and principled framework for reliable feature discovery by combining SAEs with multiple-testing-aware inference, advancing the foundations of mechanistic interpretability.

</details>


### [614] [Reasoning: From Reflection to Solution](https://arxiv.org/abs/2511.11712)
*Zixi Li*

Main category: cs.LG

TL;DR: The paper defines reasoning as iterative operator application in state spaces, providing a framework to understand reasoning and develop architectures that achieve it.


<details>
  <summary>Details</summary>
Motivation: To determine whether large language models are truly reasoning or merely pattern matching over reasoning traces, and to clarify the definition of reasoning and its architectural implications.

Method: Presents a conceptual definition of reasoning, explores puzzles and theoretical constructs (OpenXOR, OpenOperator), and introduces a novel model (OpenLM) that surpasses state-of-the-art performance in specific reasoning tasks.

Result: OpenLM achieves 76% accuracy in the OpenXOR puzzle, outperforming state-of-the-art language models which achieve 0%.

Conclusion: The paper elucidates the essence of reasoning and showcases how adopting the proposed definition can lead to creating systems capable of true reasoning, marking a step forward in understanding and building reasoning architectures.

Abstract: What is reasoning? This question has driven centuries of philosophical inquiry, from Aristotle's syllogisms to modern computational complexity theory. In the age of large language models achieving superhuman performance on benchmarks like GSM8K (95\% accuracy) and HumanEval (90\% pass@1), we must ask: have these systems learned to \emph{reason}, or have they learned to \emph{pattern-match over reasoning traces}?
  This paper argues for a specific answer: \textbf{reasoning is iterative operator application in state spaces, converging to fixed points}. This definition is not merely philosophical -- it has concrete architectural implications that explain both the failures of current systems and the path to genuine reasoning capabilities.
  Our investigation begins with a puzzle (OpenXOR), progresses through theory (OpenOperator), and culminates in a working solution (OpenLM) that achieves 76\% accuracy where state-of-the-art LLMs achieve 0\%. This is not about criticizing existing systems, but about \emph{understanding what reasoning requires} and \emph{building architectures that provide it}.

</details>


### [615] [Multiscale Grassmann Manifolds for Single-Cell Data Analysis](https://arxiv.org/abs/2511.11717)
*Xiang Xiang Wang,Sean Cottrell,Guo-Wei Wei*

Main category: cs.LG

TL;DR: The paper proposes a multiscale framework leveraging Grassmann manifolds for single-cell RNA-seq data analysis, promising improved geometric structure preservation and clustering performance.


<details>
  <summary>Details</summary>
Motivation: To address limitations of conventional methods that fail to capture intrinsic correlations and multiscale geometric structures in single-cell gene expression analysis.

Method: Integrating machine learning with subspace geometry on Grassmann manifolds, employing multiple representation scales and introducing a power-based scale sampling function.

Result: Application on nine single-cell RNA-seq datasets shows preserved meaningful structures and stable clustering, particularly for small to medium-sized datasets.

Conclusion: Grassmann manifolds provide an effective basis for single-cell data analysis and contribute to robust clustering performance across resolutions.

Abstract: Single-cell data analysis seeks to characterize cellular heterogeneity based on high-dimensional gene expression profiles. Conventional approaches represent each cell as a vector in Euclidean space, which limits their ability to capture intrinsic correlations and multiscale geometric structures. We propose a multiscale framework based on Grassmann manifolds that integrates machine learning with subspace geometry for single-cell data analysis. By generating embeddings under multiple representation scales, the framework combines their features from different geometric views into a unified Grassmann manifold. A power-based scale sampling function is introduced to control the selection of scales and balance in- formation across resolutions. Experiments on nine benchmark single-cell RNA-seq datasets demonstrate that the proposed approach effectively preserves meaningful structures and provides stable clustering performance, particularly for small to medium-sized datasets. These results suggest that Grassmann manifolds offer a coherent and informative foundation for analyzing single cell data.

</details>


### [616] [Fast 3D Surrogate Modeling for Data Center Thermal Management](https://arxiv.org/abs/2511.11722)
*Soumyendu Sarkar,Antonio Guillen-Perez,Zachariah J Carmichael,Avisek Naug,Refik Mert Cam,Vineet Gundecha,Ashwin Ramesh Babu,Sahand Ghorbanpour,Ricardo Luna Gutierrez*

Main category: cs.LG

TL;DR: This paper proposes a vision-based surrogate modeling framework for real-time 3D temperature predictions in data centers, achieving up to 20,000x speedup with significant energy and carbon savings.


<details>
  <summary>Details</summary>
Motivation: To enable sustainable and efficient management of data centers through real-time temperature predictions, addressing the high computational cost and impracticality of traditional thermal CFD solvers.

Method: The authors developed a vision-based surrogate model using 3D voxelized data with inputs like server workloads and HVAC settings. They tested architectures like 3D CNN U-Nets, Fourier Neural Operators, and vision transformers to produce high-fidelity heat maps.

Result: The surrogate models significantly outperform traditional methods, offering up to 20,000x speedup and successfully generalizing across data center configurations.

Conclusion: This framework provides an efficient alternative for temperature prediction, optimizing cooling control, saving 7% energy, and reducing the carbon footprint in data centers.

Abstract: Reducing energy consumption and carbon emissions in data centers by enabling real-time temperature prediction is critical for sustainability and operational efficiency. Achieving this requires accurate modeling of the 3D temperature field to capture airflow dynamics and thermal interactions under varying operating conditions. Traditional thermal CFD solvers, while accurate, are computationally expensive and require expert-crafted meshes and boundary conditions, making them impractical for real-time use. To address these limitations, we develop a vision-based surrogate modeling framework that operates directly on a 3D voxelized representation of the data center, incorporating server workloads, fan speeds, and HVAC temperature set points. We evaluate multiple architectures, including 3D CNN U-Net variants, a 3D Fourier Neural Operator, and 3D vision transformers, to map these thermal inputs to high-fidelity heat maps. Our results show that the surrogate models generalize across data center configurations and achieve up to 20,000x speedup (hundreds of milliseconds vs. hours). This fast and accurate estimation of hot spots and temperature distribution enables real-time cooling control and workload redistribution, leading to substantial energy savings (7\%) and reduced carbon footprint.

</details>


### [617] [Optimizing Input of Denoising Score Matching is Biased Towards Higher Score Norm](https://arxiv.org/abs/2511.11727)
*Tongda Xu*

Main category: cs.LG

TL;DR: The paper identifies and analyzes the bias introduced by optimizing the conditional input in diffusion models, compromising equivalence and affecting diverse applications.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of optimization techniques on diffusion models across various domains.

Method: Analyzed biases in denoising score matching versus exact score matching; explored effects in pre-trained diffusion models and various applications.

Result: Found optimization introduces bias, leads to higher score norms, and impacts multiple uses including image compression, generative models, and text-to-3D generation.

Conclusion: Optimization methods for conditional inputs affect performance, necessitating a reevaluation in related practices across domains.

Abstract: Many recent works utilize denoising score matching to optimize the conditional input of diffusion models. In this workshop paper, we demonstrate that such optimization breaks the equivalence between denoising score matching and exact score matching. Furthermore, we show that this bias leads to higher score norm. Additionally, we observe a similar bias when optimizing the data distribution using a pre-trained diffusion model. Finally, we discuss the wide range of works across different domains that are affected by this bias, including MAR for auto-regressive generation, PerCo for image compression, and DreamFusion for text to 3D generation.

</details>


### [618] [Physics-Informed Neural ODEs with Scale-Aware Residuals for Learning Stiff Biophysical Dynamics](https://arxiv.org/abs/2511.11734)
*Kamalpreet Singh Kainth,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedat Panat*

Main category: cs.LG

TL;DR: The paper introduces PI-NODE-SR, a framework for modeling stiff biophysical systems that improves stability and efficiency in training while maintaining accuracy in capturing system dynamics.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of unreliable and inefficient forecasting of stiff biophysical systems using existing Neural ODEs methods, which often struggle with oscillatory dynamics.

Method: The proposed PI-NODE-SR framework combines a low-order explicit solver (Heun method) with residual normalization to handle variables evolving on different timescales, ensuring stable training without computationally expensive solvers.

Result: PI-NODE-SR accurately learns stiff biophysical dynamics such as the Hodgkin-Huxley equations from limited data, extrapolating oscillatory behavior while preserving frequency and amplitude.

Conclusion: PI-NODE-SR enhances the modeling of stiff biological systems by reducing errors and ensuring stable and efficient learning, presenting a significant improvement over existing approaches like Neural-ODEs and PINNs.

Abstract: Neural differential equations offer a powerful framework for modeling continuous-time dynamics, but forecasting stiff biophysical systems remains unreliable. Standard Neural ODEs and physics informed variants often require orders of magnitude more iterations, and even then may converge to suboptimal solutions that fail to preserve oscillatory frequency or amplitude. We introduce PhysicsInformed Neural ODEs with with Scale-Aware Residuals (PI-NODE-SR), a framework that combines a low-order explicit solver (Heun method) residual normalisation to balance contributions between state variables evolving on disparate timescales. This combination stabilises training under realistic iteration budgets and avoids reliance on computationally expensive implicit solvers. On the Hodgkin-Huxley equations, PI-NODE-SR learns from a single oscillation simulated with a stiff solver (Rodas5P) and extrapolates beyond 100 ms, capturing both oscillation frequency and near-correct amplitudes. Remarkably, end-to-end learning of the vector field enables PI-NODE-SR to recover morphological features such as sharp subthreshold curvature in gating variables that are typically reserved for higher-order solvers, suggesting that neural correction can offset numerical diffusion. While performance remains sensitive to initialisation, PI-NODE-SR consistently reduces long-horizon errors relative to baseline Neural-ODEs and PINNs, offering a principled route to stable and efficient learning of stiff biological dynamics.

</details>


### [619] [KAN/H: Kolmogorov-Arnold Network using Haar-like bases](https://arxiv.org/abs/2511.11736)
*Susumu Katayama*

Main category: cs.LG

TL;DR: This paper introduces KAN/H, a variant of Kolmogorov-Arnold Network that uses Haar-variant bases for better flexibility and less hyper-parameter tuning.


<details>
  <summary>Details</summary>
Motivation: To design a model that minimizes the dependence on hyper-parameter tuning and effectively addresses function approximation problems.

Method: KAN/H replaces B-spline bases with Haar-variant bases incorporating global and local structures. It is applied to function approximation tasks and MNIST dataset.

Result: KAN/H successfully avoids extensive hyper-parameter tuning while demonstrating effectiveness in function approximation and MNIST classification.

Conclusion: KAN/H simplifies hyper-parameter tuning and offers robust performance for practical applications like MNIST classification, leveraging Haar-variant bases instead of B-splines.

Abstract: This paper proposes KAN/H, a variant of Kolmogorov-Arnold Network (KAN) that uses a Haar-variant basis system having both global and local bases instead of B-spline. The resulting algorithm is applied to function approximation problems and MNIST. We show that it does not require most of the problem-specific hyper-parameter tunings.

</details>


### [620] [DK-Root: A Joint Data-and-Knowledge-Driven Framework for Root Cause Analysis of QoE Degradations in Mobile Networks](https://arxiv.org/abs/2511.11737)
*Qizhe Li,Haolong Chen,Jiansheng Li,Shuqi Chai,Xuan Li,Yuzhou Hou,Xinhua Shao,Fangfang Li,Kaifeng Han,Guangxu Zhu*

Main category: cs.LG

TL;DR: DK-Root addresses QoE diagnosis challenges in mobile networks by combining weak supervision, expert guidance, and innovative data augmentation.


<details>
  <summary>Details</summary>
Motivation: The difficulty of diagnosing QoE degradations in mobile networks arises due to complex KPI interactions and lack of reliable expert annotations.

Method: DK-Root employs contrastive representation learning, a class-conditional diffusion model for KPI sequences, and a pretrain-finetune approach.

Result: DK-Root achieves state-of-the-art accuracy on real-world datasets, outperforming traditional and recent semi-supervised methods.

Conclusion: DK-Root effectively improves root-cause analysis accuracy by uniting data-driven and knowledge-driven strategies with robust representation techniques.

Abstract: Diagnosing the root causes of Quality of Experience (QoE) degradations in operational mobile networks is challenging due to complex cross-layer interactions among kernel performance indicators (KPIs) and the scarcity of reliable expert annotations. Although rule-based heuristics can generate labels at scale, they are noisy and coarse-grained, limiting the accuracy of purely data-driven approaches. To address this, we propose DK-Root, a joint data-and-knowledge-driven framework that unifies scalable weak supervision with precise expert guidance for robust root-cause analysis. DK-Root first pretrains an encoder via contrastive representation learning using abundant rule-based labels while explicitly denoising their noise through a supervised contrastive objective. To supply task-faithful data augmentation, we introduce a class-conditional diffusion model that generates KPIs sequences preserving root-cause semantics, and by controlling reverse diffusion steps, it produces weak and strong augmentations that improve intra-class compactness and inter-class separability. Finally, the encoder and the lightweight classifier are jointly fine-tuned with scarce expert-verified labels to sharpen decision boundaries. Extensive experiments on a real-world, operator-grade dataset demonstrate state-of-the-art accuracy, with DK-Root surpassing traditional ML and recent semi-supervised time-series methods. Ablations confirm the necessity of the conditional diffusion augmentation and the pretrain-finetune design, validating both representation quality and classification gains.

</details>


### [621] [Uncertainty Makes It Stable: Curiosity-Driven Quantized Mixture-of-Experts](https://arxiv.org/abs/2511.11743)
*Sebastián Andrés Cajas Ordóñez,Luis Fernando Torres Torres,Mackenzie J. Meni,Carlos Andrés Duran Paredes,Eric Arazo,Cristian Bosch,Ricardo Simon Carbajo,Yuan Lai,Leo Anthony Celi*

Main category: cs.LG

TL;DR: This paper introduces a curiosity-based quantized Mixture-of-Experts (MoE) framework addressing accuracy and predictable latency challenges on resource-constrained devices.


<details>
  <summary>Details</summary>
Motivation: The aim is to maintain neural network accuracy during aggressive quantization while ensuring predictable inference latency for resource-constrained devices.

Method: A Bayesian epistemic uncertainty-based routing mechanism across heterogeneous experts (e.g., ternary, 1-16 bit precision post-training quantization) was used, with evaluations on audio classification tasks.

Result: 4-bit quantization achieved near full-precision accuracy (99.9% of 16-bit accuracy), 4x compression, and 41% energy savings. Latency variance was reduced by 82%, but MoE architectures introduced latency overhead without accuracy improvement.

Conclusion: Adaptive quantization and simple 4-bit architectures ensure efficient and predictable models for edge applications, often outperforming more complex MoE solutions in deployment scenarios.

Abstract: Deploying deep neural networks on resource-constrained devices faces two critical challenges: maintaining accuracy under aggressive quantization while ensuring predictable inference latency. We present a curiosity-driven quantized Mixture-of-Experts framework that addresses both through Bayesian epistemic uncertainty-based routing across heterogeneous experts (BitNet ternary, 1-16 bit BitLinear, post-training quantization). Evaluated on audio classification benchmarks (ESC-50, Quinn, UrbanSound8K), our 4-bit quantization maintains 99.9 percent of 16-bit accuracy (0.858 vs 0.859 F1) with 4x compression and 41 percent energy savings versus 8-bit. Crucially, curiosity-driven routing reduces MoE latency variance by 82 percent (p = 0.008, Levene's test) from 230 ms to 29 ms standard deviation, enabling stable inference for battery-constrained devices. Statistical analysis confirms 4-bit/8-bit achieve practical equivalence with full precision (p > 0.05), while MoE architectures introduce 11 percent latency overhead (p < 0.001) without accuracy gains. At scale, deployment emissions dominate training by 10000x for models serving more than 1,000 inferences, making inference efficiency critical. Our information-theoretic routing demonstrates that adaptive quantization yields accurate (0.858 F1, 1.2M params), energy-efficient (3.87 F1/mJ), and predictable edge models, with simple 4-bit quantized architectures outperforming complex MoE for most deployments.

</details>


### [622] [Diffusion Models: A Mathematical Introduction](https://arxiv.org/abs/2511.11746)
*Sepehr Maleki,Negar Pourmoazemi*

Main category: cs.LG

TL;DR: The paper provides a thorough derivation of diffusion-based generative models discussing their forward and reverse processes, variational bounds, sampling methods, continuous-time formulations, and guided diffusion.


<details>
  <summary>Details</summary>
Motivation: The motivation is to provide a transparent, step-by-step derivation of diffusion-based generative models from fundamental principles, aiding both theoretical understanding and practical implementation.

Method: Starting from Gaussian properties, the paper builds denoising diffusion probabilistic models, explores variational bounds, and extends into advanced formulations like continuous-time frameworks and guided diffusion techniques.

Result: The paper systematizes foundational and advanced formulations of diffusion models, enhancing comprehension of practical applications like Stable Diffusion and enabling more efficient implementations.

Conclusion: The paper bridges theoretical foundations and practical aspects of diffusion models, enabling practitioners and researchers to understand and implement these techniques effectively.

Abstract: We present a concise, self-contained derivation of diffusion-based generative models. Starting from basic properties of Gaussian distributions (densities, quadratic expectations, re-parameterisation, products, and KL divergences), we construct denoising diffusion probabilistic models from first principles. This includes the forward noising process, its closed-form marginals, the exact discrete reverse posterior, and the related variational bound. This bound simplifies to the standard noise-prediction goal used in practice. We then discuss likelihood estimation and accelerated sampling, covering DDIM, adversarially learned reverse dynamics (DDGAN), and multi-scale variants such as nested and latent diffusion, with Stable Diffusion as a canonical example. A continuous-time formulation follows, in which we derive the probability-flow ODE from the diffusion SDE via the continuity and Fokker-Planck equations, introduce flow matching, and show how rectified flows recover DDIM up to a time re-parameterisation. Finally, we treat guided diffusion, interpreting classifier guidance as a posterior score correction and classifier-free guidance as a principled interpolation between conditional and unconditional scores. Throughout, the focus is on transparent algebra, explicit intermediate steps, and consistent notation, so that readers can both follow the theory and implement the corresponding algorithms in practice.

</details>


### [623] [IDOL: Meeting Diverse Distribution Shifts with Prior Physics for Tropical Cyclone Multi-Task Estimation](https://arxiv.org/abs/2511.11750)
*Hanting Yan,Pan Mu,Shiqi Zhang,Yuchao Zhu,Jinglin Zhang,Cong Bai*

Main category: cs.LG

TL;DR: The paper addresses challenges in tropical cyclone (TC) estimation due to distribution shifts by proposing an Identity Distribution-Oriented Physical Invariant Learning framework (IDOL), which incorporates prior physical knowledge to enhance estimation robustness.


<details>
  <summary>Details</summary>
Motivation: Accurate real-time TC estimation faces challenges from distribution shifts caused by environmental dynamics like geographical variations and seasonal changes. Existing methods lack robustness for out-of-distribution scenarios due to insufficient focus on feature representation distributions.

Method: The IDOL framework imposes identity-oriented constraints informed by prior physical knowledge, using wind field models and dark correlation knowledge to create task-shared and task-specific identity tokens. These tokens capture task dependencies and physical invariances, enabling robust estimations.

Result: Extensive experiments confirm the effectiveness of IDOL, showing improved results across various datasets and tasks. It demonstrates resilience against distribution shifts in TC estimation.

Conclusion: The proposed IDOL framework effectively addresses distribution variability in TC estimation by leveraging physical invariances, offering a robust solution with verified superior performance. The code is publicly available for further adoption and validation.

Abstract: Tropical Cyclone (TC) estimation aims to accurately estimate various TC attributes in real time. However, distribution shifts arising from the complex and dynamic nature of TC environmental fields, such as varying geographical conditions and seasonal changes, present significant challenges to reliable estimation. Most existing methods rely on multi-modal fusion for feature extraction but overlook the intrinsic distribution of feature representations, leading to poor generalization under out-of-distribution (OOD) scenarios. To address this, we propose an effective Identity Distribution-Oriented Physical Invariant Learning framework (IDOL), which imposes identity-oriented constraints to regulate the feature space under the guidance of prior physical knowledge, thereby dealing distribution variability with physical invariance. Specifically, the proposed IDOL employs the wind field model and dark correlation knowledge of TC to model task-shared and task-specific identity tokens. These tokens capture task dependencies and intrinsic physical invariances of TC, enabling robust estimation of TC wind speed, pressure, inner-core, and outer-core size under distribution shifts. Extensive experiments conducted on multiple datasets and tasks demonstrate the outperformance of the proposed IDOL, verifying that imposing identity-oriented constraints based on prior physical knowledge can effectively mitigates diverse distribution shifts in TC estimation.Code is available at https://github.com/Zjut-MultimediaPlus/IDOL.

</details>


### [624] [Improving a Hybrid Graphsage Deep Network for Automatic Multi-objective Logistics Management in Supply Chain](https://arxiv.org/abs/2511.11753)
*Mehdi Khaleghi,Nastaran Khaleghi,Sobhan Sheykhivand,Sebelan Danishvar*

Main category: cs.LG

TL;DR: The paper proposes H-GSN, an automatic hybrid network for logistics management, achieving high accuracy in predicting shipment-related metrics on multiple datasets.


<details>
  <summary>Details</summary>
Motivation: To enhance supply chain resiliency, sustainability, and efficiency through improved collaboration with logistics service providers and better management of shipments, logistics delays, and traffic.

Method: Proposed a Hybrid GraphSage Network (H-GSN) for multi-task logistics objectives, using data from three Kaggle datasets: DataCo, Shipping, and Smart Logistics.

Result: Achieved high average accuracies: 97.8%-100% for logistics ID/traffic status (Smart Logistics), 98.7% for shipment type (DataCo), and 99.4% for logistics delay (Shipping).

Conclusion: The paper demonstrates the effectiveness of the H-GSN model in enhancing supply chain resilience and sustainability by improving logistics management metrics.

Abstract: Systematic logistics, conveyance amenities and facilities as well as warehousing information play a key role in fostering profitable development in a supply chain. The aim of transformation in industries is the improvement of the resiliency regarding the supply chain. The resiliency policies are required for companies to affect the collaboration with logistics service providers positively. The decrement of air pollutant emissions is a persistent advantage of the efficient management of logistics and transportation in supply chain. The management of shipment type is a significant factor in analyzing the sustainability of logistics and supply chain. An automatic approach to predict the shipment type, logistics delay and traffic status are required to improve the efficiency of the supply chain management. A hybrid graphsage network (H-GSN) is proposed in this paper for multi-task purpose of logistics management in a supply chain. The shipment type, shipment status, traffic status, logistics ID and logistics delay are the objectives in this article regarding three different databases including DataCo, Shipping and Smart Logistcis available on Kaggle as supply chain logistics databases. The average accuracy of 97.8% and 100% are acquired for 10 kinds of logistics ID and 3 types of traffic status prediction in Smart Logistics dataset. The average accuracy of 98.7% and 99.4% are obtained for shipment type prediction in DataCo and logistics delay in Shipping database, respectively. The evaluation metrics for different logistics scenarios confirm the efficiency of the proposed method to improve the resilience and sustainability of the supply chain.

</details>


### [625] [Sumudu Neural Operator for ODEs and PDEs](https://arxiv.org/abs/2511.11762)
*Ben Zelenskiy,Saibilila Abudukelimu,George Flint,Kevin Zhu,Sunishchal Dev*

Main category: cs.LG

TL;DR: The paper proposes the Sumudu Neural Operator (SNO), applying the Sumudu Transform in neural operators for solving ODEs and PDEs, demonstrating competitive and superior performance in various tasks.


<details>
  <summary>Details</summary>
Motivation: To improve the performance and design of neural operators, especially for certain classes of Partial Differential Equations (PDEs), leveraging the properties of the Sumudu Transform.

Method: The Sumudu Transform is used to decompose and map input data into the Sumudu Space, where the neural operator is parameterized and evaluated on several ODE and PDE tasks.

Result: The SNO outperforms Fourier Neural Operators (FNO) on PDEs and achieves competitive accuracy with Laplace Neural Operators (LNO), with the lowest errors on specific PDE tasks. It can effectively perform zero-shot super-resolution, showcasing its ability to enhance data quality from low-resolution inputs.

Conclusion: The Sumudu Transform shows potential as a foundational design for neural operators, especially for modeling certain types of PDEs.

Abstract: We introduce the Sumudu Neural Operator (SNO), a neural operator rooted in the properties of the Sumudu Transform. We leverage the relationship between the polynomial expansions of transform pairs to decompose the input space as coefficients, which are then transformed into the Sumudu Space, where the neural operator is parameterized. We evaluate the operator in ODEs (Duffing Oscillator, Lorenz System, and Driven Pendulum) and PDEs (Euler-Bernoulli Beam, Burger's Equation, Diffusion, Diffusion-Reaction, and Brusselator). SNO achieves superior performance to FNO on PDEs and demonstrates competitive accuracy with LNO on several PDE tasks, including the lowest error on the Euler-Bernoulli Beam and Diffusion Equation. Additionally, we apply zero-shot super-resolution to the PDE tasks to observe the model's capability of obtaining higher quality data from low-quality samples. These preliminary findings suggest promise for the Sumudu Transform as a neural operator design, particularly for certain classes of PDEs.

</details>


### [626] [Learning Fair Representations with Kolmogorov-Arnold Networks](https://arxiv.org/abs/2511.11767)
*Amisha Priyadarshini,Sergio Gago-Masague*

Main category: cs.LG

TL;DR: This paper proposes integrating Kolmogorov-Arnold Networks (KANs) within a fair adversarial learning framework to achieve better fairness and accuracy while ensuring interpretability in high-stakes domains like college admissions.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the discriminatory behaviors of predictive models towards marginalized groups caused by biased data, model design, and representational disparities, particularly in socially sensitive areas like college admissions.

Method: The authors employ Kolmogorov-Arnold Networks (KANs) within a fair adversarial learning framework and introduce an adaptive penalty update mechanism to balance fairness and accuracy during model training.

Result: Numerical experiments on two real-world college admissions datasets show that KANs outperform existing fair learning models, achieving both high predictive accuracy and competitive fairness across sensitive attributes.

Conclusion: Integrating KANs into adversarial frameworks, coupled with adaptive penalty mechanisms, provides a promising solution for addressing bias while balancing fairness and accuracy in machine learning applications.

Abstract: Despite recent advances in fairness-aware machine learning, predictive models often exhibit discriminatory behavior towards marginalized groups. Such unfairness might arise from biased training data, model design, or representational disparities across groups, posing significant challenges in high-stakes decision-making domains such as college admissions. While existing fair learning models aim to mitigate bias, achieving an optimal trade-off between fairness and accuracy remains a challenge. Moreover, the reliance on black-box models hinders interpretability, limiting their applicability in socially sensitive domains. In this paper, we try to circumvent these issues by integrating Kolmogorov-Arnold Networks (KANs) within a fair adversarial learning framework. Leveraging the adversarial robustness and interpretability of KANs, our approach enables a balance between fairness and accuracy. To further facilitate this balance, we propose an adaptive penalty update mechanism that dynamically adjusts fairness constraints during the model training. We conduct numerical experiments on two real-world college admissions datasets, across three different optimization strategies. The results demonstrate the efficiency and robustness of KANs by consistently outperforming the baseline fair learning models, and maintaining high predictive accuracy while achieving competitive fairness across sensitive attributes.

</details>


### [627] [CATCHFed: Efficient Unlabeled Data Utilization for Semi-Supervised Federated Learning in Limited Labels Environments](https://arxiv.org/abs/2511.11778)
*Byoungjun Park,Pedro Porto Buarque de Gusmão,Dongjin Ji,Minhoe Kim*

Main category: cs.LG

TL;DR: The paper introduces CATCHFed, a semi-supervised federated learning framework designed to improve performance when labeled server-side data is limited by leveraging client data with advanced threshold techniques.


<details>
  <summary>Details</summary>
Motivation: To address the limitation in federated learning where client-side data often lacks labels, and existing semi-supervised methods suffer performance degradation with limited labeled data.

Method: CATCHFed employs client-aware adaptive thresholds based on class difficulty, hybrid thresholds for better pseudo-labels, and uses unpseudo-labeled data to enhance consistency regularization during training.

Result: CATCHFed outperformed existing approaches in utilizing unlabeled client data, achieving strong results even with minimal labeled data across various datasets and configurations.

Conclusion: This framework establishes a robust approach to semi-supervised federated learning, showing its capability to handle real-world scenarios with limited labeled data while maintaining competitive performance.

Abstract: Federated learning is a promising paradigm that utilizes distributed client resources while preserving data privacy. Most existing FL approaches assume clients possess labeled data, however, in real-world scenarios, client-side labels are often unavailable. Semi-supervised Federated learning, where only the server holds labeled data, addresses this issue. However, it experiences significant performance degradation as the number of labeled data decreases. To tackle this problem, we propose \textit{CATCHFed}, which introduces client-aware adaptive thresholds considering class difficulty, hybrid thresholds to enhance pseudo-label quality, and utilizes unpseudo-labeled data for consistency regularization. Extensive experiments across various datasets and configurations demonstrate that CATCHFed effectively leverages unlabeled client data, achieving superior performance even in extremely limited-label settings.

</details>


### [628] [Simplicial covering dimension of extremal concept classes](https://arxiv.org/abs/2511.11819)
*Ari Blondal,Hamed Hatami,Pooya Hatami,Chavdar Lalov,Sivan Tretiak*

Main category: cs.LG

TL;DR: The paper adapts the classical topological dimension to binary concept classes and connects it to list replicability in PAC learning.


<details>
  <summary>Details</summary>
Motivation: To explore the link between topological dimension theory and list replicability in PAC learning.

Method: Introduced simplicial covering dimension to binary concept classes, leveraging geometric and topological structures.

Result: Demonstrated that the simplicial covering dimension precisely characterizes the list replicability number for finite concept classes.

Conclusion: The established connection offers tools from classical dimension theory for computing list replicability in various concept classes.

Abstract: Dimension theory is a branch of topology concerned with defining and analyzing dimensions of geometric and topological spaces in purely topological terms. In this work, we adapt the classical notion of topological dimension (Lebesgue covering) to binary concept classes. The topological space naturally associated with a concept class is its space of realizable distributions. The loss function and the class itself induce a simplicial structure on this space, with respect to which we define a simplicial covering dimension.
  We prove that for finite concept classes, this simplicial covering dimension exactly characterizes the list replicability number (equivalently, global stability) in PAC learning. This connection allows us to apply tools from classical dimension theory to compute the exact list replicability number of the broad family of extremal concept classes.

</details>


### [629] [Conformal Constrained Policy Optimization for Cost-Effective LLM Agents](https://arxiv.org/abs/2511.11828)
*Wenwen Si,Sooyong Jang,Insup Lee,Osbert Bastani*

Main category: cs.LG

TL;DR: The paper introduces a cost-efficient strategy for using large language models (LLMs) while ensuring reliability via Conformal Constrained Policy Optimization (CCPO). It achieves up to 30% cost reduction in multi-hop question answering without compromising accuracy.


<details>
  <summary>Details</summary>
Motivation: The increasing computational and API costs of LLMs necessitate strategies for minimizing costs without reducing performance reliability.

Method: A novel model orchestration approach combines LLMs of varying cost and accuracy tradeoffs with tools, optimized using CCPO. CCPO merges conformal prediction with constrained policy optimization and reinforcement learning.

Result: CCPO reduces costs by up to 30% compared to existing baselines in multi-hop question answering tasks, maintaining reliability.

Conclusion: The framework efficiently deploys reliable LLM agents, significantly reducing operational costs without reliability trade-offs.

Abstract: While large language models (LLMs) have recently made tremendous progress towards solving challenging AI problems, they have done so at increasingly steep computational and API costs. We propose a novel strategy where we combine multiple LLM models with varying cost/accuracy tradeoffs in an agentic manner, where models and tools are run in sequence as determined by an orchestration model to minimize cost subject to a user-specified level of reliability; this constraint is formalized using conformal prediction to provide guarantees. To solve this problem, we propose Conformal Constrained Policy Optimization (CCPO), a training paradigm that integrates constrained policy optimization with off-policy reinforcement learning and recent advances in online conformal prediction. CCPO jointly optimizes a cost-aware policy (score function) and an adaptive threshold. Across two multi-hop question answering benchmarks, CCPO achieves up to a 30% cost reduction compared to other cost-aware baselines and LLM-guided methods without compromising reliability. Our approach provides a principled and practical framework for deploying LLM agents that are significantly more cost-effective while maintaining reliability.

</details>


### [630] [Volatility in Certainty (VC): A Metric for Detecting Adversarial Perturbations During Inference in Neural Network Classifiers](https://arxiv.org/abs/2511.11834)
*Vahid Hemmati,Ahmad Mohammadi,Abdul-Rauf Nuhu,Reza Ahmari,Parham Kebria,Abdollah Homaifar*

Main category: cs.LG

TL;DR: The paper introduces 'Volatility in Certainty' (VC), a label-free metric to identify irregularities in model confidence and detect adversarial robustness in neural network classifiers, showing strong negative correlation with classification accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of adversarial robustness in neural network classifiers, especially in real-time, label-free systems, and to develop an effective metric for indicating performance degradation.

Method: The paper proposes VC, a metric quantifying the dispersion of softmax outputs to reflect confidence irregularities, and evaluates it on ANN and CNN models using MNIST and CIFAR-10 datasets with adversarial examples created via the FGSM method.

Result: VC demonstrated a strong negative correlation (rho < -0.90) with classification accuracy under adversarial conditions, suggesting its effectiveness in identifying performance degradation without ground-truth labels.

Conclusion: VC is a reliable, architecture-agnostic, and scalable real-time metric for evaluating neural network performance and detecting adversarial vulnerabilities, especially useful for safety-critical applications without requiring labeled data.

Abstract: Adversarial robustness remains a critical challenge in deploying neural network classifiers, particularly in real-time systems where ground-truth labels are unavailable during inference. This paper investigates \textit{Volatility in Certainty} (VC), a recently proposed, label-free metric that quantifies irregularities in model confidence by measuring the dispersion of sorted softmax outputs. Specifically, VC is defined as the average squared log-ratio of adjacent certainty values, capturing local fluctuations in model output smoothness. We evaluate VC as a proxy for classification accuracy and as an indicator of adversarial drift. Experiments are conducted on artificial neural networks (ANNs) and convolutional neural networks (CNNs) trained on MNIST, as well as a regularized VGG-like model trained on CIFAR-10. Adversarial examples are generated using the Fast Gradient Sign Method (FGSM) across varying perturbation magnitudes. In addition, mixed test sets are created by gradually introducing adversarial contamination to assess VC's sensitivity under incremental distribution shifts. Our results reveal a strong negative correlation between classification accuracy and log(VC) (correlation rho < -0.90 in most cases), suggesting that VC effectively reflects performance degradation without requiring labeled data. These findings position VC as a scalable, architecture-agnostic, and real-time performance metric suitable for early-warning systems in safety-critical applications.

</details>


### [631] [On the Trade-Off Between Transparency and Security in Adversarial Machine Learning](https://arxiv.org/abs/2511.11842)
*Lucas Fenaux,Christopher Srinivasa,Florian Kerschbaum*

Main category: cs.LG

TL;DR: The paper explores the trade-off between transparency and security in Responsible AI through the impact of adversarial attacks on defended and undefended models.


<details>
  <summary>Details</summary>
Motivation: To evaluate how transparency in AI systems impacts security, particularly in adversarial settings where attackers aim to exploit knowledge of defender systems.

Method: A large-scale empirical evaluation involving nine adversarial attacks across 181 models, paired with game theory approaches (Nash and Stackelberg games) to analyze decision impacts.

Result: Attackers were more successful when they could match the defender's decision, highlighting that obscurity might enhance security. The analysis revealed that awareness of a model's defended or undefended status can harm security.

Conclusion: Transparency in AI systems may conflict with security, especially in adversarial contexts. Game-theoretic analysis helps to understand this conflict and suggests limits to transparency in favor of security.

Abstract: Transparency and security are both central to Responsible AI, but they may conflict in adversarial settings. We investigate the strategic effect of transparency for agents through the lens of transferable adversarial example attacks. In transferable adversarial example attacks, attackers maliciously perturb their inputs using surrogate models to fool a defender's target model. These models can be defended or undefended, with both players having to decide which to use. Using a large-scale empirical evaluation of nine attacks across 181 models, we find that attackers are more successful when they match the defender's decision; hence, obscurity could be beneficial to the defender. With game theory, we analyze this trade-off between transparency and security by modeling this problem as both a Nash game and a Stackelberg game, and comparing the expected outcomes. Our analysis confirms that only knowing whether a defender's model is defended or not can sometimes be enough to damage its security. This result serves as an indicator of the general trade-off between transparency and security, suggesting that transparency in AI systems can be at odds with security. Beyond adversarial machine learning, our work illustrates how game-theoretic reasoning can uncover conflicts between transparency and security.

</details>


### [632] [Leveraging Exogenous Signals for Hydrology Time Series Forecasting](https://arxiv.org/abs/2511.11849)
*Junyang He,Judy Fox,Alireza Jafari,Ying-Jung Chen,Geoffrey Fox*

Main category: cs.LG

TL;DR: Domain knowledge integration into time series models improves hydrological predictions over foundation models.


<details>
  <summary>Details</summary>
Motivation: Assessing the effectiveness of time series foundation models in physical science applications and exploring domain knowledge integration for better modeling performance.

Method: Comparison of baseline and foundation hydrological models using CAMELS-US dataset containing rainfall, runoff data, static features, and time series streams.

Result: Models integrating exogenous inputs and domain-specific natural annual periodic data outperform foundation models.

Conclusion: Incorporating domain knowledge enhances the performance of time series models in hydrological applications compared to foundation models.

Abstract: Recent advances in time series research facilitate the development of foundation models. While many state-of-the-art time series foundation models have been introduced, few studies examine their effectiveness in specific downstream applications in physical science. This work investigates the role of integrating domain knowledge into time series models for hydrological rainfall-runoff modeling. Using the CAMELS-US dataset, which includes rainfall and runoff data from 671 locations with six time series streams and 30 static features, we compare baseline and foundation models. Results demonstrate that models incorporating comprehensive known exogenous inputs outperform more limited approaches, including foundation models. Notably, incorporating natural annual periodic time series contribute the most significant improvements.

</details>


### [633] [Transformers vs. Recurrent Models for Estimating Forest Gross Primary Production](https://arxiv.org/abs/2511.11880)
*David Montero,Miguel D. Mahecha,Francesco Martinuzzi,César Aybar,Anne Klosterhalfen,Alexander Knohl,Jesús Anaya,Clemens Mosig,Sebastian Wieneke*

Main category: cs.LG

TL;DR: The study compares the performance of transformer (GPT-2) and recurrent neural network (LSTM) models for predicting forest CO$_2$ uptake (GPP), highlighting their trade-offs in context length and performance under extreme events.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of monitoring forest CO$_2$ uptake at large spatial scales by leveraging advances in deep learning and data fusion.

Method: Two deep learning models (GPT-2 and LSTM) were compared for GPP prediction using multivariate remote sensing inputs, examining their predictive performance, temporal context length, and feature importance.

Result: While both models showed similar accuracy, LSTM outperformed overall but required shorter input windows, and GPT-2 was superior under extreme conditions. Radiation was identified as the dominant predictor, followed by other variables such as land surface temperature.

Conclusion: Model architecture, temporal context length, and multimodal inputs play key roles in GPP prediction accuracy, offering insights for improving DL frameworks to monitor terrestrial carbon dynamics.

Abstract: Monitoring the spatiotemporal dynamics of forest CO$_2$ uptake (Gross Primary Production, GPP), remains a central challenge in terrestrial ecosystem research. While Eddy Covariance (EC) towers provide high-frequency estimates, their limited spatial coverage constrains large-scale assessments. Remote sensing offers a scalable alternative, yet most approaches rely on single-sensor spectral indices and statistical models that are often unable to capture the complex temporal dynamics of GPP. Recent advances in deep learning (DL) and data fusion offer new opportunities to better represent the temporal dynamics of vegetation processes, but comparative evaluations of state-of-the-art DL models for multimodal GPP prediction remain scarce. Here, we explore the performance of two representative models for predicting GPP: 1) GPT-2, a transformer architecture, and 2) Long Short-Term Memory (LSTM), a recurrent neural network, using multivariate inputs. Overall, both achieve similar accuracy. But, while LSTM performs better overall, GPT-2 excels during extreme events. Analysis of temporal context length further reveals that LSTM attains similar accuracy using substantially shorter input windows than GPT-2, highlighting an accuracy-efficiency trade-off between the two architectures. Feature importance analysis reveals radiation as the dominant predictor, followed by Sentinel-2, MODIS land surface temperature, and Sentinel-1 contributions. Our results demonstrate how model architecture, context length, and multimodal inputs jointly determine performance in GPP prediction, guiding future developments of DL frameworks for monitoring terrestrial carbon dynamics.

</details>


### [634] [Better LLM Reasoning via Dual-Play](https://arxiv.org/abs/2511.11881)
*Zhengxin Zhang,Chengyu Huang,Aochong Oliver Li,Claire Cardie*

Main category: cs.LG

TL;DR: The paper introduces PasoDoble, a dual-play adversarial learning framework for LLMs that enhances reasoning performance through unsupervised Proposer-Solver interactions and stability-focused updates.


<details>
  <summary>Details</summary>
Motivation: To reduce reliance on external supervision in training LLMs and harness the advantages of adversarial learning, particularly dual-play, despite challenges like reward hacking and instability.

Method: PasoDoble employs adversarial training between two specialized models: the Proposer generates challenging, diverse questions using knowledge from pre-training data, and the Solver attempts to solve them. Joint or offline alternating updates prevent reward hacking and increase training stability.

Result: The experimental results demonstrate that PasoDoble improves the reasoning ability of LLMs without requiring external supervision during training.

Conclusion: PasoDoble showcases a novel and effective approach for unsupervised LLM training, utilizing dual-play dynamics to boost model reasoning capabilities and ensuring stable updates.

Abstract: Large Language Models (LLMs) have achieved remarkable progress through Reinforcement Learning with Verifiable Rewards (RLVR), yet still rely heavily on external supervision (e.g., curated labels). Adversarial learning, particularly through self-play, offers a promising alternative that enables models to iteratively learn from themselves - thus reducing reliance on external supervision. Dual-play extends adversarial learning by assigning specialized roles to two models and training them against each other, fostering sustained competition and mutual evolution. Despite its promise, adapting dual-play training to LLMs remains limited, largely due to their susceptibility to reward hacking and training instability. In this paper, we introduce PasoDoble, a novel LLM dual-play framework. PasoDoble adversarially trains two models initialized from the same base model: a Proposer, which generates challenging questions with ground-truth answers, and a Solver, which attempts to solve them. We enrich the Proposer with knowledge from a pre-training dataset to ensure the questions' quality and diversity. To avoid reward hacking, the Proposer is rewarded for producing only valid questions that push the Solver's limit, while the Solver is rewarded for solving them correctly, and both are updated jointly. To further enhance training stability, we introduce an optional offline paradigm that decouples Proposer and Solver updates, alternately updating each for several steps while holding the other fixed. Notably, PasoDoble operates without supervision during training. Experimental results show that PasoDoble can improve the reasoning performance of LLMs. Our project page is available at https://hcy123902.github.io/PasoDoble.

</details>


### [635] [FLEX: Feature Importance from Layered Counterfactual Explanations](https://arxiv.org/abs/2511.11891)
*Nawid Keshtmand,Roussel Desmond Nzoyem,Jeffrey Nicholas Clark*

Main category: cs.LG

TL;DR: FLEX is a framework to derive feature importance scores from counterfactuals, providing insights at local, regional, and global levels.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the uninterpretability of machine learning models in high-stakes settings by creating a framework that quantifies feature importance systematically.

Method: FLEX converts counterfactual explanations into feature change frequency scores and aggregates these metrics to provide interpretable rankings, allowing customization based on practical constraints.

Result: FLEX was tested on traffic accident severity prediction and loan approval tasks, showing compatibility with counterfactual generation methods and outperforming SHAP and LIME in certain areas.

Conclusion: FLEX bridges the gap between local recourse and global attribution, facilitating intervention-oriented and transparent decision-making in risk-sensitive applications.

Abstract: Machine learning models achieve state-of-the-art performance across domains, yet their lack of interpretability limits safe deployment in high-stakes settings. Counterfactual explanations are widely used to provide actionable "what-if" recourse, but they typically remain instance-specific and do not quantify which features systematically drive outcome changes within coherent regions of the feature space or across an entire dataset. We introduce FLEX (Feature importance from Layered counterfactual EXplanations), a model- and domain-agnostic framework that converts sets of counterfactuals into feature change frequency scores at local, regional, and global levels. FLEX generalises local change-frequency measures by aggregating across instances and neighbourhoods, offering interpretable rankings that reflect how often each feature must change to flip predictions. The framework is compatible with different counterfactual generation methods, allowing users to emphasise characteristics such as sparsity, feasibility, or actionability, thereby tailoring the derived feature importances to practical constraints. We evaluate FLEX on two contrasting tabular tasks: traffic accident severity prediction and loan approval, and compare FLEX to SHAP- and LIME-derived feature importance values. Results show that (i) FLEX's global rankings correlate with SHAP while surfacing additional drivers, and (ii) regional analyses reveal context-specific factors that global summaries miss. FLEX thus bridges the gap between local recourse and global attribution, supporting transparent and intervention-oriented decision-making in risk-sensitive applications.

</details>


### [636] [Chain-of-Generation: Progressive Latent Diffusion for Text-Guided Molecular Design](https://arxiv.org/abs/2511.11894)
*Lingxiao Li,Haobo Zhang,Bin Chen,Jiayu Zhou*

Main category: cs.LG

TL;DR: The paper introduces Chain-of-Generation (CoG), a multi-stage latent diffusion framework for text-conditioned molecular generation, addressing challenges in one-shot conditioning to produce more accurate and interpretable results.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome limitations in current one-shot conditioning methods in translating natural-language prompts to molecular structures, such as poor interpretability, failure to encompass all substructures, and difficulty in meeting all linguistic constraints.

Method: The method involves the proposed CoG framework, which decomposes prompts into curriculum-ordered semantic segments and progressively uses them as intermediate goals. A post-alignment learning phase is also incorporated to better align textual and molecular latent spaces.

Result: Experiments demonstrate that CoG surpasses one-shot conditioning baselines in terms of semantic alignment, diversity, and controllability, while generating molecules that better match complex prompts.

Conclusion: CoG offers a more reliable and interpretable framework for text-conditioned molecular generation, producing molecules that align well with linguistic prompts and providing a clearer generation process.

Abstract: Text-conditioned molecular generation aims to translate natural-language descriptions into chemical structures, enabling scientists to specify functional groups, scaffolds, and physicochemical constraints without handcrafted rules. Diffusion-based models, particularly latent diffusion models (LDMs), have recently shown promise by performing stochastic search in a continuous latent space that compactly captures molecular semantics. Yet existing methods rely on one-shot conditioning, where the entire prompt is encoded once and applied throughout diffusion, making it hard to satisfy all the requirements in the prompt. We discuss three outstanding challenges of one-shot conditioning generation, including the poor interpretability of the generated components, the failure to generate all substructures, and the overambition in considering all requirements simultaneously. We then propose three principles to address those challenges, motivated by which we propose Chain-of-Generation (CoG), a training-free multi-stage latent diffusion framework. CoG decomposes each prompt into curriculum-ordered semantic segments and progressively incorporates them as intermediate goals, guiding the denoising trajectory toward molecules that satisfy increasingly rich linguistic constraints. To reinforce semantic guidance, we further introduce a post-alignment learning phase that strengthens the correspondence between textual and molecular latent spaces. Extensive experiments on benchmark and real-world tasks demonstrate that CoG yields higher semantic alignment, diversity, and controllability than one-shot baselines, producing molecules that more faithfully reflect complex, compositional prompts while offering transparent insight into the generation process.

</details>


### [637] [Robust Bidirectional Associative Memory via Regularization Inspired by the Subspace Rotation Algorithm](https://arxiv.org/abs/2511.11902)
*Ci Lin,Tet Yeap,Iluju Kiringa,Biwei Zhang*

Main category: cs.LG

TL;DR: This paper addresses the robustness and sensitivity issues of Bidirectional Associative Memory (BAM) by introducing a new training algorithm, B-SRA, and regularization strategies based on orthogonal weight matrices and gradient-pattern alignment.


<details>
  <summary>Details</summary>
Motivation: BAM suffers from poor robustness and high sensitivity to noise and adversarial attacks under traditional Bidirectional Backpropagation (B-BP) training methods.

Method: The authors propose the Bidirectional Subspace Rotation Algorithm (B-SRA) as a gradient-free alternative and introduce regularization strategies using orthogonal weight matrices (OWM) and gradient-pattern alignment (GPA).

Result: Experimental results show improved robustness, reduced sensitivity to corruption and adversarial attacks, and enhanced performance across different associative pair capacities when using the SAME configuration that combines OWM and GPA.

Conclusion: B-SRA and the introduced regularization strategies significantly enhance the robustness of BAM while providing insights into building more resilient neural memory architectures.

Abstract: Bidirectional Associative Memory (BAM) trained with Bidirectional Backpropagation (B-BP) often suffers from poor robustness and high sensitivity to noise and adversarial attacks. To address these issues, we propose a novel gradient-free training algorithm, the Bidirectional Subspace Rotation Algorithm (B-SRA), which significantly improves the robustness and convergence behavior of BAM. Through comprehensive experiments, we identify two key principles -- orthogonal weight matrices (OWM) and gradient-pattern alignment (GPA) -- as central to enhancing the robustness of BAM. Motivated by these findings, we introduce new regularization strategies into B-BP, resulting in models with greatly improved resistance to corruption and adversarial perturbations. We further conduct an ablation study across different training strategies to determine the most robust configuration and evaluate BAM's performance under a variety of attack scenarios and memory capacities, including 50, 100, and 200 associative pairs. Among all methods, the SAME configuration, which integrates both OWM and GPA, achieves the strongest resilience. Overall, our results demonstrate that B-SRA and the proposed regularization strategies lead to substantially more robust associative memories and open new directions for building resilient neural architectures.

</details>


### [638] [A Systematic Study of Model Extraction Attacks on Graph Foundation Models](https://arxiv.org/abs/2511.11912)
*Haoyan Xu,Ruizhi Qian,Jiate Li,Yushun Dong,Minghao Lin,Hanson Yan,Zhengtao Yao,Qinghua Liu,Junhao Dong,Ruopeng Huang,Yue Zhao,Mengyuan Li*

Main category: cs.LG

TL;DR: The paper studies model extraction attacks (MEAs) on Graph Foundation Models (GFMs) by introducing attack scenarios and methods, showing attackers can replicate their functionality with minimal resources.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the vulnerability of Graph Foundation Models (GFMs) to model extraction attacks (MEAs), given their high pretraining costs and unique ability to unify structural and semantic understanding in graph learning.

Method: The authors define a black-box threat model with six attack scenarios and propose a lightweight attack method. This method uses supervised regression to mimic the victim's graph embeddings, preserving zero-shot inference without pretraining data.

Result: Experiments across seven datasets demonstrate that the proposed method approximates the victim model’s functionality with significant savings in computational cost while maintaining high accuracy.

Conclusion: The findings show that GFMs are particularly susceptible to MEAs, emphasizing the need for improved security mechanisms to protect these large-scale multimodal systems.

Abstract: Graph machine learning has advanced rapidly in tasks such as link prediction, anomaly detection, and node classification. As models scale up, pretrained graph models have become valuable intellectual assets because they encode extensive computation and domain expertise. Building on these advances, Graph Foundation Models (GFMs) mark a major step forward by jointly pretraining graph and text encoders on massive and diverse data. This unifies structural and semantic understanding, enables zero-shot inference, and supports applications such as fraud detection and biomedical analysis. However, the high pretraining cost and broad cross-domain knowledge in GFMs also make them attractive targets for model extraction attacks (MEAs). Prior work has focused only on small graph neural networks trained on a single graph, leaving the security implications for large-scale and multimodal GFMs largely unexplored. This paper presents the first systematic study of MEAs against GFMs. We formalize a black-box threat model and define six practical attack scenarios covering domain-level and graph-specific extraction goals, architectural mismatch, limited query budgets, partial node access, and training data discrepancies. To instantiate these attacks, we introduce a lightweight extraction method that trains an attacker encoder using supervised regression of graph embeddings. Even without contrastive pretraining data, this method learns an encoder that stays aligned with the victim text encoder and preserves its zero-shot inference ability on unseen graphs. Experiments on seven datasets show that the attacker can approximate the victim model using only a tiny fraction of its original training cost, with almost no loss in accuracy. These findings reveal that GFMs greatly expand the MEA surface and highlight the need for deployment-aware security defenses in large-scale graph learning systems.

</details>


### [639] [Batch Matrix-form Equations and Implementation of Multilayer Perceptrons](https://arxiv.org/abs/2511.11918)
*Wieger Wesselink,Bram Grooten,Huub van de Wetering,Qiao Xiao,Decebal Constantin Mocanu*

Main category: cs.LG

TL;DR: This paper formalizes multilayer perceptrons (MLPs) in batch matrix form, deriving forward and backward equations validated via symbolic computation and providing implementation-ready tools for efficient sparse computation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to offer explicit and transparent batch matrix-form derivations of MLPs for better analysis, teaching, and research, as many existing works rely on less explicit methods like per-sample gradients or automatic differentiation.

Method: The paper derives mathematically rigorous forward and backward equations for MLPs in matrix form, including advanced layers such as batch normalization and softmax, and validates these equations through symbolic computation using SymPy.

Result: The authors present validated equations, uniform Python and C++ implementations for MLPs, and demonstrate the utility of these explicit formulations for efficient sparse computations.

Conclusion: The paper provides a comprehensive, validated framework for understanding and implementing MLPs in batch matrix form, offering tools and insights for optimization, teaching, and advanced neural network research.

Abstract: Multilayer perceptrons (MLPs) remain fundamental to modern deep learning, yet their algorithmic details are rarely presented in complete, explicit \emph{batch matrix-form}. Rather, most references express gradients per sample or rely on automatic differentiation. Although automatic differentiation can achieve equally high computational efficiency, the usage of batch matrix-form makes the computational structure explicit, which is essential for transparent, systematic analysis, and optimization in settings such as sparse neural networks. This paper fills that gap by providing a mathematically rigorous and implementation-ready specification of MLPs in batch matrix-form. We derive forward and backward equations for all standard and advanced layers, including batch normalization and softmax, and validate all equations using the symbolic mathematics library SymPy. From these specifications, we construct uniform reference implementations in NumPy, PyTorch, JAX, TensorFlow, and a high-performance C++ backend optimized for sparse operations. Our main contributions are: (1) a complete derivation of batch matrix-form backpropagation for MLPs, (2) symbolic validation of all gradient equations, (3) uniform Python and C++ reference implementations grounded in a small set of matrix primitives, and (4) demonstration of how explicit formulations enable efficient sparse computation. Together, these results establish a validated, extensible foundation for understanding, teaching, and researching neural network algorithms.

</details>


### [640] [Beyond the Laplacian: Interpolated Spectral Augmentation for Graph Neural Networks](https://arxiv.org/abs/2511.11928)
*Ziyao Cui,Edric Tam*

Main category: cs.LG

TL;DR: The paper introduces Interpolated Laplacian Embeddings (ILEs), a novel approach to augment node features in graph neural networks (GNNs), especially when node features are sparse or unavailable.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of GNN performance caused by the scarcity or absence of informative node features in real-world graph datasets and applications.

Method: ILEs are computed using an expressive family of graph matrices different from the traditional Laplacian spectral embeddings. Spectral graph theory tools provide insight into the structural data captured by ILEs, which serve as augmented node features for GNNs.

Result: Simulations and experiments on real-world datasets show that incorporating ILEs into GNNs improves their performance across multiple architectures.

Conclusion: ILEs expand the spectral augmentation toolkit for GNN practitioners, offering a practical means to boost performance in scenarios with limited node features.

Abstract: Graph neural networks (GNNs) are fundamental tools in graph machine learning. The performance of GNNs relies crucially on the availability of informative node features, which can be limited or absent in real-life datasets and applications. A natural remedy is to augment the node features with embeddings computed from eigenvectors of the graph Laplacian matrix. While it is natural to default to Laplacian spectral embeddings, which capture meaningful graph connectivity information, we ask whether spectral embeddings from alternative graph matrices can also provide useful representations for learning. We introduce Interpolated Laplacian Embeddings (ILEs), which are derived from a simple yet expressive family of graph matrices. Using tools from spectral graph theory, we offer a straightforward interpretation of the structural information that ILEs capture. We demonstrate through simulations and experiments on real-world datasets that feature augmentation via ILEs can improve performance across commonly used GNN architectures. Our work offers a straightforward and practical approach that broadens the practitioner's spectral augmentation toolkit when node features are limited.

</details>


### [641] [A Systematic Analysis of Out-of-Distribution Detection Under Representation and Training Paradigm Shifts](https://arxiv.org/abs/2511.11934)
*C. César Claros Olivares,Austin J. Brockmeier*

Main category: cs.LG

TL;DR: The paper compares out-of-distribution (OOD) detection across different representation paradigms using statistical tests and ranking-based assessments of efficacy. Probabilistic and geometry-aware scores are evaluated for CNNs and Vision Transformers (ViTs).


<details>
  <summary>Details</summary>
Motivation: The paper seeks to systematically evaluate how different methods perform under distribution shifts across varying representation paradigms and datasets.

Method: The authors employ CNNs trained from scratch and fine-tuned ViTs, evaluated on multiple datasets using rank-based statistical tools like Friedman tests and Bron-Kerbosch cliques.

Result: Probabilistic scores are dominant for misclassification detection, while geometry-aware scores excel under stronger shifts in CNNs. GradNorm and KPCA Reconstruction Error are consistently effective on ViTs.

Conclusion: The findings emphasize the importance of learned representation in OOD detection and provide guidance for method selection based on statistical analysis and feature-space dynamics.

Abstract: We present a systematic comparison of out-of-distribution (OOD) detection methods across CLIP-stratified regimes using AURC and AUGRC as primary metrics. Experiments cover two representation paradigms: CNNs trained from scratch and a fine-tuned Vision Transformer (ViT), evaluated on CIFAR-10/100, SuperCIFAR-100, and TinyImageNet. Using a multiple-comparison-controlled, rank-based pipeline (Friedman test with Conover-Holm post-hoc) and Bron-Kerbosch cliques, we find that the learned feature space largely determines OOD efficacy. For both CNNs and ViTs, probabilistic scores (e.g., MSR, GEN) dominate misclassification (ID) detection. Under stronger shifts, geometry-aware scores (e.g., NNGuide, fDBD, CTM) prevail on CNNs, whereas on ViTs GradNorm and KPCA Reconstruction Error remain consistently competitive. We further show a class-count-dependent trade-off for Monte-Carlo Dropout (MCD) and that a simple PCA projection improves several detectors. These results support a representation-centric view of OOD detection and provide statistically grounded guidance for method selection under distribution shift.

</details>


### [642] [SurvBench: A Standardised Preprocessing Pipeline for Multi-Modal Electronic Health Record Survival Analysis](https://arxiv.org/abs/2511.11935)
*Munib Mesinovic,Tingting Zhu*

Main category: cs.LG

TL;DR: SurvBench is an open-source preprocessing pipeline transforming raw PhysioNet datasets into standardised tensors for survival analysis, addressing reproducibility issues in EHR data preparation.


<details>
  <summary>Details</summary>
Motivation: Reproducibility challenges in survival analysis arise from inconsistent preprocessing methodologies; this paper aims to standardize and simplify this step to enhance research comparability and innovation.

Method: SurvBench implements a preprocessing pipeline for diverse PhysioNet critical care databases, supporting modalities like vitals, demographics, diagnosis codes, and radiology reports, with rigorous data controls and standardised outputs.

Result: SurvBench produces model-ready tensors compatible with pycox for statistical and deep learning models, facilitating fair and reproducible comparison across survival analysis techniques.

Conclusion: SurvBench addresses a critical gap in preprocessing for EHR-based survival analysis, enabling researchers to focus on developing innovative models without data engineering obstacles.

Abstract: Electronic health record (EHR) data present tremendous opportunities for advancing survival analysis through deep learning, yet reproducibility remains severely constrained by inconsistent preprocessing methodologies. We present SurvBench, a comprehensive, open-source preprocessing pipeline that transforms raw PhysioNet datasets into standardised, model-ready tensors for multi-modal survival analysis. SurvBench provides data loaders for three major critical care databases, MIMIC-IV, eICU, and MC-MED, supporting diverse modalities including time-series vitals, static demographics, ICD diagnosis codes, and radiology reports. The pipeline implements rigorous data quality controls, patient-level splitting to prevent data leakage, explicit missingness tracking, and standardised temporal aggregation. SurvBench handles both single-risk (e.g., in-hospital mortality) and competing-risks scenarios (e.g., multiple discharge outcomes). The outputs are compatible with pycox library packages and implementations of standard statistical and deep learning models. By providing reproducible, configuration-driven preprocessing with comprehensive documentation, SurvBench addresses the "preprocessing gap" that has hindered fair comparison of deep learning survival models, enabling researchers to focus on methodological innovation rather than data engineering.

</details>


### [643] [Learning the relative composition of EEG signals using pairwise relative shift pretraining](https://arxiv.org/abs/2511.11940)
*Christopher Sandino,Sayeri Lala,Geeling Chau,Melika Ayoughi,Behrooz Mahasseni,Ellen Zippi,Ali Moin,Erdrin Azemi,Hanlin Goh*

Main category: cs.LG

TL;DR: The paper introduces PARS, a novel pretraining method for EEG signals, emphasizing long-range dependencies over local patterns, which outperforms existing methods in transfer and label-efficient settings.


<details>
  <summary>Details</summary>
Motivation: The motivation is to reduce reliance on expensive EEG data annotations for applications like sleep staging and seizure detection, by developing better SSL methods to capture the unique characteristics of EEG signals.

Method: Introduce a novel SSL pretext task, PAirwise Relative Shift (PARS), where relative temporal shifts between EEG window pairs are predicted, deviating from conventional reconstruction-based strategies.

Result: PARS-pretrained models outperform existing SSL methods in decoding tasks, especially in transferring learned representations and working efficiently with fewer labels.

Conclusion: PARS demonstrates the potential to establish a new standard in self-supervised EEG representation learning by effectively capturing long-range dependencies in neural signals.

Abstract: Self-supervised learning (SSL) offers a promising approach for learning electroencephalography (EEG) representations from unlabeled data, reducing the need for expensive annotations for clinical applications like sleep staging and seizure detection. While current EEG SSL methods predominantly use masked reconstruction strategies like masked autoencoders (MAE) that capture local temporal patterns, position prediction pretraining remains underexplored despite its potential to learn long-range dependencies in neural signals. We introduce PAirwise Relative Shift or PARS pretraining, a novel pretext task that predicts relative temporal shifts between randomly sampled EEG window pairs. Unlike reconstruction-based methods that focus on local pattern recovery, PARS encourages encoders to capture relative temporal composition and long-range dependencies inherent in neural signals. Through comprehensive evaluation on various EEG decoding tasks, we demonstrate that PARS-pretrained transformers consistently outperform existing pretraining strategies in label-efficient and transfer learning settings, establishing a new paradigm for self-supervised EEG representation learning.

</details>


### [644] [Computation-aware Energy-harvesting Federated Learning: Cyclic Scheduling with Selective Participation](https://arxiv.org/abs/2511.11949)
*Eunjeong Jeong,Nikolaos Pappas*

Main category: cs.LG

TL;DR: FedBacys is a battery-aware federated learning framework that minimizes energy consumption for energy-harvesting FL systems by cyclically scheduling client participation based on battery levels while ensuring robust learning performance.


<details>
  <summary>Details</summary>
Motivation: Energy consumption in federated learning is a critical issue, especially in energy-harvesting systems where client devices have fluctuating energy availability.

Method: FedBacys clusters clients based on battery levels and schedules participation cyclically, reducing redundant computations. An energy-efficient variant, FedBacys-Odd, allows selective client participation to lower energy costs.

Result: FedBacys improves energy efficiency, reduces overall system energy usage, and ensures learning robustness. The framework shows superior performance in experiments compared to existing methods.

Conclusion: The proposed framework successfully addresses energy challenges in FL systems, offering enhanced efficiency and robust learning through innovative client participation strategies.

Abstract: Federated Learning (FL) is a powerful paradigm for distributed learning, but its increasing complexity leads to significant energy consumption from client-side computations for training models. In particular, the challenge is critical in energy-harvesting FL (EHFL) systems where participation availability of each device oscillates due to limited energy. To address this, we propose FedBacys, a battery-aware EHFL framework using cyclic client participation based on users' battery levels. By clustering clients and scheduling them sequentially, FedBacys minimizes redundant computations, reduces system-wide energy usage, and improves learning stability. We also introduce FedBacys-Odd, a more energy-efficient variant that allows clients to participate selectively, further reducing energy costs without compromising performance. We provide a convergence analysis for our framework and demonstrate its superior energy efficiency and robustness compared to existing algorithms through numerical experiments.

</details>


### [645] [Quantile Q-Learning: Revisiting Offline Extreme Q-Learning with Quantile Regression](https://arxiv.org/abs/2511.11973)
*Xinming Gao,Shangzhe Li,Yujin Cai,Wenwu Yu*

Main category: cs.LG

TL;DR: The paper highlights challenges in previous offline RL methods and proposes a method using quantile regression and value regularization to improve stability and performance.


<details>
  <summary>Details</summary>
Motivation: Address the instability and hyperparameter tuning difficulties of XQL and MXQL methods in offline RL.

Method: Use quantile regression to estimate the temperature coefficient $β$ and introduce a value regularization technique inspired by constrained value learning.

Result: The proposed algorithm demonstrates consistent, stable training and competitive/superior performance across D4RL and NeoRL2 benchmarks.

Conclusion: The paper offers a principled and stable solution for offline RL that resolves key challenges in existing approaches while ensuring broad applicability across datasets.

Abstract: Offline reinforcement learning (RL) enables policy learning from fixed datasets without further environment interaction, making it particularly valuable in high-risk or costly domains. Extreme $Q$-Learning (XQL) is a recent offline RL method that models Bellman errors using the Extreme Value Theorem, yielding strong empirical performance. However, XQL and its stabilized variant MXQL suffer from notable limitations: both require extensive hyperparameter tuning specific to each dataset and domain, and also exhibit instability during training. To address these issues, we proposed a principled method to estimate the temperature coefficient $β$ via quantile regression under mild assumptions. To further improve training stability, we introduce a value regularization technique with mild generalization, inspired by recent advances in constrained value learning. Experimental results demonstrate that the proposed algorithm achieves competitive or superior performance across a range of benchmark tasks, including D4RL and NeoRL2, while maintaining stable training dynamics and using a consistent set of hyperparameters across all datasets and domains.

</details>


### [646] [ReCast: Reliability-aware Codebook Assisted Lightweight Time Series Forecasting](https://arxiv.org/abs/2511.11991)
*Xiang Ma,Taihua Chen,Pengcheng Wang,Xuemei Li,Caiming Zhang*

Main category: cs.LG

TL;DR: The paper introduces ReCast, a lightweight and robust framework for time series forecasting using a learnable codebook to capture recurring local patterns and a dual-path architecture for managing variations.


<details>
  <summary>Details</summary>
Motivation: To address limitations of conventional forecasting methods that fail in capturing dynamic, local patterns in real-world series and are inefficient in resource-constrained environments.

Method: ReCast uses patch-wise quantization with a learnable codebook to encode local patterns, a dual-path architecture to separately handle regular structures and irregular fluctuations, and a reliability-aware codebook update strategy with DRO for robustness.

Result: ReCast surpasses state-of-the-art models in terms of forecasting accuracy, efficiency, and adaptability to changes in data distribution.

Conclusion: ReCast is effective, lightweight, and adapts well to dynamic data scenarios, making it superior for real-world time series forecasting tasks.

Abstract: Time series forecasting is crucial for applications in various domains. Conventional methods often rely on global decomposition into trend, seasonal, and residual components, which become ineffective for real-world series dominated by local, complex, and highly dynamic patterns. Moreover, the high model complexity of such approaches limits their applicability in real-time or resource-constrained environments. In this work, we propose a novel \textbf{RE}liability-aware \textbf{C}odebook-\textbf{AS}sisted \textbf{T}ime series forecasting framework (\textbf{ReCast}) that enables lightweight and robust prediction by exploiting recurring local shapes. ReCast encodes local patterns into discrete embeddings through patch-wise quantization using a learnable codebook, thereby compactly capturing stable regular structures. To compensate for residual variations not preserved by quantization, ReCast employs a dual-path architecture comprising a quantization path for efficient modeling of regular structures and a residual path for reconstructing irregular fluctuations. A central contribution of ReCast is a reliability-aware codebook update strategy, which incrementally refines the codebook via weighted corrections. These correction weights are derived by fusing multiple reliability factors from complementary perspectives by a distributionally robust optimization (DRO) scheme, ensuring adaptability to non-stationarity and robustness to distribution shifts. Extensive experiments demonstrate that ReCast outperforms state-of-the-art (SOTA) models in accuracy, efficiency, and adaptability to distribution shifts.

</details>


### [647] [Selecting Fine-Tuning Examples by Quizzing VLMs](https://arxiv.org/abs/2511.12002)
*Tenghao Ji,Eytan Adar*

Main category: cs.LG

TL;DR: Proposes QZLoRA, a method using automated image ranking to improve text-to-image model fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning text-to-image diffusion models requires high-quality image datasets, but these aren't always readily available.

Method: Introduces QuizRank to rank images for adaptation using visual reasoning and VLM-based image assessments.

Result: Improved generation of accurate, stylistic images with fewer samples, using QZLoRA.

Conclusion: Combining visual reasoning and efficient fine-tuning enhances generative modeling adaptability to specific topics.

Abstract: A challenge in fine-tuning text-to-image diffusion models for specific topics is to select good examples. Fine-tuning from image sets of varying quality, such as Wikipedia Commons, will often produce poor output. However, training images that \textit{do} exemplify the target concept (e.g., a \textit{female Mountain Bluebird}) help ensure that the generated images are similarly representative (e.g., have the prototypical blue-wings and gray chest). In this work, we propose QZLoRA, a framework to select images for low-rank adaptation (LoRA). The approach leverages QuizRank, a method to automatically rank images by treating them as an `educational intervention' and `quizzing' a VLM. We demonstrate that QZLoRA can produce better aligned, photorealistic images with fewer samples. We also show that these fine-tuned models can produce stylized that are similarly representative (i.e., illustrations). Our results highlight the promise of combining automated visual reasoning with parameter-efficient fine-tuning for topic-adaptive generative modeling.

</details>


### [648] [EARL: Entropy-Aware RL Alignment of LLMs for Reliable RTL Code Generation](https://arxiv.org/abs/2511.12033)
*Jiahe Shi,Zhengqi Gao,Ching-Yun Ko,Duane Boning*

Main category: cs.LG

TL;DR: This paper introduces EARL, an Entropy-Aware Reinforcement Learning framework designed to improve hardware design automation by targeting high-uncertainty tokens in RTL code generation.


<details>
  <summary>Details</summary>
Motivation: Improve the alignment of large language models with real-world Register-Transfer Level design requirements, addressing issues like syntax errors and weak designer intent alignment.

Method: Uses Reinforcement Learning with Verifiable Rewards and entropy-guided selective updates to focus learning on critical, high-uncertainty tokens during Verilog code generation.

Result: EARL demonstrates up to a 14.7% improvement in functional pass rates over existing LLM baselines, while enhancing training stability and reducing inefficient gradient updates.

Conclusion: Targeting critical, high-entropy tokens during reinforcement learning significantly enhances the functional reliability and efficiency of RTL code generation.

Abstract: Recent advances in large language models (LLMs) have demonstrated significant potential in hardware design automation, particularly in using natural language to synthesize Register-Transfer Level (RTL) code. Despite this progress, a gap remains between model capability and the demands of real-world RTL design, including syntax errors, functional hallucinations, and weak alignment to designer intent. Reinforcement Learning with Verifiable Rewards (RLVR) offers a promising approach to bridge this gap, as hardware provides executable and formally checkable signals that can be used to further align model outputs with design intent. However, in long, structured RTL code sequences, not all tokens contribute equally to functional correctness, and naïvely spreading gradients across all tokens dilutes learning signals. A key insight from our entropy analysis in RTL generation is that only a small fraction of tokens (e.g., always, if, assign, posedge) exhibit high uncertainty and largely influence control flow and module structure. To address these challenges, we present EARL, an Entropy-Aware Reinforcement Learning framework for Verilog generation. EARL performs policy optimization using verifiable reward signals and introduces entropy-guided selective updates that gate policy gradients to high-entropy tokens. This approach preserves training stability and concentrates gradient updates on functionally important regions of code. Our experiments on VerilogEval and RTLLM show that EARL improves functional pass rates over prior LLM baselines by up to 14.7%, while reducing unnecessary updates and improving training stability. These results indicate that focusing RL on critical, high-uncertainty tokens enables more reliable and targeted policy improvement for structured RTL code generation.

</details>


### [649] [Mesh-based Super-resolution of Detonation Flows with Multiscale Graph Transformers](https://arxiv.org/abs/2511.12041)
*Shivam Barwey,Pinaki Pal*

Main category: cs.LG

TL;DR: This paper introduces a novel multiscale graph transformer (SR-GT) for super-resolution flow reconstruction in reacting flows modeled on complex, non-uniform grids.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenge of accurately reconstructing high-resolution flow-fields using sparse, mesh-based data, which is crucial for applications like data compression, subgrid modeling, and experimental measurements.

Method: The paper develops a multiscale graph transformer that uses graph-based representations of flow-fields and a transformer backbone to capture long-range dependencies, tokenize input, and predict super-resolution outputs on spectral-element-discretized meshes.

Result: The SR-GT framework shows superior performance and higher accuracy in resolving reacting flow-field features, surpassing traditional interpolation-based methods.

Conclusion: SR-GT demonstrates effectiveness in producing high-quality super-resolved flow-fields, representing a significant advance in data-driven modeling for complex reacting flow dynamics.

Abstract: Super-resolution flow reconstruction using state-of-the-art data-driven techniques is valuable for a variety of applications, such as subgrid/subfilter closure modeling, accelerating spatiotemporal forecasting, data compression, and serving as an upscaling tool for sparse experimental measurements. In the present work, a first-of-its-kind multiscale graph transformer approach is developed for mesh-based super-resolution (SR-GT) of reacting flows. The novel data-driven modeling paradigm leverages a graph-based flow-field representation compatible with complex geometries and non-uniform/unstructured grids. Further, the transformer backbone captures long-range dependencies between different parts of the low-resolution flow-field, identifies important features, and then generates the super-resolved flow-field that preserves those features at a higher resolution. The performance of SR-GT is demonstrated in the context of spectral-element-discretized meshes for a challenging test problem of 2D detonation propagation within a premixed hydrogen-air mixture exhibiting highly complex multiscale reacting flow behavior. The SR-GT framework utilizes a unique element-local (+ neighborhood) graph representation for the coarse input, which is then tokenized before being processed by the transformer component to produce the fine output. It is demonstrated that SR-GT provides high super-resolution accuracy for reacting flow-field features and superior performance compared to traditional interpolation-based SR schemes.

</details>


### [650] [Improving Graph Embeddings in Machine Learning Using Knowledge Completion with Validation in a Case Study on COVID-19 Spread](https://arxiv.org/abs/2511.12071)
*Rosario Napoli,Gabriele Morabito,Antonio Celesti,Massimo Villari,Maria Fazio*

Main category: cs.LG

TL;DR: This paper proposes integrating a Knowledge Completion phase into Graph Machine Learning to model hidden, transitive relations in Knowledge Graphs before embedding generation, improving the representation quality.


<details>
  <summary>Details</summary>
Motivation: Graph embeddings may miss implicit knowledge in sparse datasets due to their emphasis on explicit topology and features, limiting their representation quality.

Method: The method introduces a GML pipeline with a pre-embedding Knowledge Completion phase to uncover hidden semantic data. It uses decay-based inference functions to model transitive relationships and modifies graph topology for better embeddings.

Result: Experiments demonstrate that this pipeline significantly transforms embedding space geometry, redefining graph representation quality and improving tasks like node classification and link prediction.

Conclusion: Incorporating Knowledge Completion into GML is not just an enrichment but a transformative step, enhancing the ability to capture implicit knowledge and fostering better graph representations.

Abstract: The rise of graph-structured data has driven major advances in Graph Machine Learning (GML), where graph embeddings (GEs) map features from Knowledge Graphs (KGs) into vector spaces, enabling tasks like node classification and link prediction. However, since GEs are derived from explicit topology and features, they may miss crucial implicit knowledge hidden in seemingly sparse datasets, affecting graph structure and their representation. We propose a GML pipeline that integrates a Knowledge Completion (KC) phase to uncover latent dataset semantics before embedding generation. Focusing on transitive relations, we model hidden connections with decay-based inference functions, reshaping graph topology, with consequences on embedding dynamics and aggregation processes in GraphSAGE and Node2Vec. Experiments show that our GML pipeline significantly alters the embedding space geometry, demonstrating that its introduction is not just a simple enrichment but a transformative step that redefines graph representation quality.

</details>


### [651] [Treatment Stitching with Schrödinger Bridge for Enhancing Offline Reinforcement Learning in Adaptive Treatment Strategies](https://arxiv.org/abs/2511.12075)
*Dong-Hee Shin,Deok-Joong Lee,Young-Han Son,Tae-Eui Kam*

Main category: cs.LG

TL;DR: Adaptive treatment strategies (ATS) are improved using Treatment Stitching (TreatStitch), a data augmentation framework enhancing offline reinforcement learning (RL) capabilities with clinically valid synthetic data.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge that traditional reinforcement learning (RL) approaches cannot be directly applied in clinical settings due to risks and data limitations in offline RL.

Method: The proposed method, TreatStitch, creates additional clinically valid synthetic treatment trajectories by stitching segments of existing trajectories or generating bridging trajectories using the Schrödinger bridge method.

Result: Experiments across various treatment datasets show TreatStitch improves offline RL performance by diversifying the dataset with augmented synthetic data.

Conclusion: TreatStitch enables offline RL to better optimize adaptive treatment strategies by expanding training datasets while maintaining clinical validity, overcoming data scarcity challenges in clinical domains.

Abstract: Adaptive treatment strategies (ATS) are sequential decision-making processes that enable personalized care by dynamically adjusting treatment decisions in response to evolving patient symptoms. While reinforcement learning (RL) offers a promising approach for optimizing ATS, its conventional online trial-and-error learning mechanism is not permissible in clinical settings due to risks of harm to patients. Offline RL tackles this limitation by learning policies exclusively from historical treatment data, but its performance is often constrained by data scarcity-a pervasive challenge in clinical domains. To overcome this, we propose Treatment Stitching (TreatStitch), a novel data augmentation framework that generates clinically valid treatment trajectories by intelligently stitching segments from existing treatment data. Specifically, TreatStitch identifies similar intermediate patient states across different trajectories and stitches their respective segments. Even when intermediate states are too dissimilar to stitch directly, TreatStitch leverages the Schrödinger bridge method to generate smooth and energy-efficient bridging trajectories that connect dissimilar states. By augmenting these synthetic trajectories into the original dataset, offline RL can learn from a more diverse dataset, thereby improving its ability to optimize ATS. Extensive experiments across multiple treatment datasets demonstrate the effectiveness of TreatStitch in enhancing offline RL performance. Furthermore, we provide a theoretical justification showing that TreatStitch maintains clinical validity by avoiding out-of-distribution transitions.

</details>


### [652] [SenseRay-3D: Generalizable and Physics-Informed Framework for End-to-End Indoor Propagation Modeling](https://arxiv.org/abs/2511.12092)
*Yu Zheng,Kezhi Wang,Wenji Xi,Gang Yu,Jiming Chen,Jie Zhang*

Main category: cs.LG

TL;DR: SenseRay-3D predicts 3D path-loss heatmaps from RGB-D scans, eliminating the need for manual modeling in indoor radio propagation.


<details>
  <summary>Details</summary>
Motivation: Current models for indoor radio propagation often require manual input for geometry and materials, limiting scalability and efficiency.

Method: SenseRay-3D employs a voxelized scene representation encoded with occupancy, material properties, and geometry, processed through a SwinUNETR-based neural network.

Result: SenseRay-3D achieves a mean absolute error of 4.27 dB on unseen environments, supports real-time inference at 217 ms per sample.

Conclusion: SenseRay-3D offers scalable, efficient, and physics-consistent modeling for indoor propagation, advancing beyond previous frameworks like EM DeepRay.

Abstract: Modeling indoor radio propagation is crucial for wireless network planning and optimization. However, existing approaches often rely on labor-intensive manual modeling of geometry and material properties, resulting in limited scalability and efficiency. To overcome these challenges, this paper presents SenseRay-3D, a generalizable and physics-informed end-to-end framework that predicts three-dimensional (3D) path-loss heatmaps directly from RGB-D scans, thereby eliminating the need for explicit geometry reconstruction or material annotation. The proposed framework builds a sensing-driven voxelized scene representation that jointly encodes occupancy, electromagnetic material characteristics, and transmitter-receiver geometry, which is processed by a SwinUNETR-based neural network to infer environmental path-loss relative to free-space path-loss. A comprehensive synthetic indoor propagation dataset is further developed to validate the framework and to serve as a standardized benchmark for future research. Experimental results show that SenseRay-3D achieves a mean absolute error of 4.27 dB on unseen environments and supports real-time inference at 217 ms per sample, demonstrating its scalability, efficiency, and physical consistency. SenseRay-3D paves a new path for sense-driven, generalizable, and physics-consistent modeling of indoor propagation, marking a major leap beyond our pioneering EM DeepRay framework.

</details>


### [653] [To Align or Not to Align: Strategic Multimodal Representation Alignment for Optimal Performance](https://arxiv.org/abs/2511.12121)
*Wanlong Fang,Tianle Zhang,Alvin Chan*

Main category: cs.LG

TL;DR: The paper investigates the impact of explicitly enforced alignment in multimodal learning, finding that its optimal use depends on data redundancy and modality-specific structures.


<details>
  <summary>Details</summary>
Motivation: To understand how explicitly controlling alignment in multimodal representations affects model performance, particularly under varying modality-specific data structures.

Method: The study introduces a controllable contrastive learning module to manipulate alignment strength during training and systematically evaluates the effects of explicit alignment on synthetic and real datasets.

Result: Explicit alignment's influence on unimodal models depends on data redundancy, with optimal alignment balancing shared and modality-specific signals.

Conclusion: Explicit alignment should be carefully tuned based on modality-specific redundancy for achieving the best unimodal encoder performance, offering practical strategies for multimodal learning improvement.

Abstract: Multimodal learning often relies on aligning representations across modalities to enable effective information integration, an approach traditionally assumed to be universally beneficial. However, prior research has primarily taken an observational approach, examining naturally occurring alignment in multimodal data and exploring its correlation with model performance, without systematically studying the direct effects of explicitly enforced alignment between representations of different modalities. In this work, we investigate how explicit alignment influences both model performance and representation alignment under different modality-specific information structures. Specifically, we introduce a controllable contrastive learning module that enables precise manipulation of alignment strength during training, allowing us to explore when explicit alignment improves or hinders performance. Our results on synthetic and real datasets under different data characteristics show that the impact of explicit alignment on the performance of unimodal models is related to the characteristics of the data: the optimal level of alignment depends on the amount of redundancy between the different modalities. We identify an optimal alignment strength that balances modality-specific signals and shared redundancy in the mixed information distributions. This work provides practical guidance on when and how explicit alignment should be applied to achieve optimal unimodal encoder performance.

</details>


### [654] [Dynamic Anomaly Identification in Accounting Transactions via Multi-Head Self-Attention Networks](https://arxiv.org/abs/2511.12122)
*Yi Wang,Ruoyi Fang,Anzhuo Xie,Hanrui Feng,Jianlin Lai*

Main category: cs.LG

TL;DR: This paper proposes a real-time anomaly detection method for accounting transactions using a Transformer-based approach, demonstrating its efficiency and robustness through experiments on diverse scenarios.


<details>
  <summary>Details</summary>
Motivation: To address dynamic anomaly detection challenges in accounting transactions where abnormal behaviors are hidden and require high timeliness in trading environments.

Method: A Transformer-based approach employing time-series matrix representation, embedding layers, multi-head self-attention, feed-forward layers, and regularization strategies for anomaly pattern detection.

Result: Validated through extensive experiments, the proposed method surpasses baseline models in AUC, F1-Score, Precision, and Recall, showcasing stable performance in varied environmental and data conditions.

Conclusion: The proposed framework is effective and applicable for detecting anomalies dynamically in accounting transactions, supporting financial risk control and auditing enhancements.

Abstract: This study addresses the problem of dynamic anomaly detection in accounting transactions and proposes a real-time detection method based on a Transformer to tackle the challenges of hidden abnormal behaviors and high timeliness requirements in complex trading environments. The approach first models accounting transaction data by representing multi-dimensional records as time-series matrices and uses embedding layers and positional encoding to achieve low-dimensional mapping of inputs. A sequence modeling structure with multi-head self-attention is then constructed to capture global dependencies and aggregate features from multiple perspectives, thereby enhancing the ability to detect abnormal patterns. The network further integrates feed-forward layers and regularization strategies to achieve deep feature representation and accurate anomaly probability estimation. To validate the effectiveness of the method, extensive experiments were conducted on a public dataset, including comparative analysis, hyperparameter sensitivity tests, environmental sensitivity tests, and data sensitivity tests. Results show that the proposed method outperforms baseline models in AUC, F1-Score, Precision, and Recall, and maintains stable performance under different environmental conditions and data perturbations. These findings confirm the applicability and advantages of the Transformer-based framework for dynamic anomaly detection in accounting transactions and provide methodological support for intelligent financial risk control and auditing.

</details>


### [655] [HCPO: Hierarchical Conductor-Based Policy Optimization in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.12123)
*Zejiao Liu,Junqi Tu,Yitian Hong,Luolin Xiong,Yaochu Jin,Yang Tang,Fangfei Li*

Main category: cs.LG

TL;DR: The paper introduces a conductor-based joint policy framework and the Hierarchical Conductor-based Policy Optimization (HCPO) algorithm to efficiently coordinate exploration in cooperative Multi-Agent Reinforcement Learning (MARL).


<details>
  <summary>Details</summary>
Motivation: Existing methods for exploration in MARL often rely on independent agent exploration, limiting the optimization and expressive capacity of joint policies.

Method: The proposed framework introduces a conductor to align exploratory behavior among agents and utilizes HCPO for optimizing the policies in a structured manner while ensuring performance improvement.

Result: HCPO shows superior performance compared to competitive baseline methods, improving cooperative efficiency and stability across three challenging benchmarks.

Conclusion: HCPO effectively addresses the limitations of independent exploration in MARL by enabling coordinated exploration, enhancing joint policy optimization, and maintaining centralized training advantages without requiring inter-agent communication during execution.

Abstract: In cooperative Multi-Agent Reinforcement Learning (MARL), efficient exploration is crucial for optimizing the performance of joint policy. However, existing methods often update joint policies via independent agent exploration, without coordination among agents, which inherently constrains the expressive capacity and exploration of joint policies. To address this issue, we propose a conductor-based joint policy framework that directly enhances the expressive capacity of joint policies and coordinates exploration. In addition, we develop a Hierarchical Conductor-based Policy Optimization (HCPO) algorithm that instructs policy updates for the conductor and agents in a direction aligned with performance improvement. A rigorous theoretical guarantee further establishes the monotonicity of the joint policy optimization process. By deploying local conductors, HCPO retains centralized training benefits while eliminating inter-agent communication during execution. Finally, we evaluate HCPO on three challenging benchmarks: StarCraftII Multi-agent Challenge, Multi-agent MuJoCo, and Multi-agent Particle Environment. The results indicate that HCPO outperforms competitive MARL baselines regarding cooperative efficiency and stability.

</details>


### [656] [FairGSE: Fairness-Aware Graph Neural Network without High False Positive Rates](https://arxiv.org/abs/2511.12132)
*Zhenqiang Ye,Jinjie Lu,Tianlong Gu,Fengrui Hao,Xuemin Wang*

Main category: cs.LG

TL;DR: The paper introduces FairGSE, a novel framework designed to improve the fairness of Graph Neural Networks (GNNs) while reducing False Positive Rates (FPR).


<details>
  <summary>Details</summary>
Motivation: The motivation arises from the current fairness-aware GNNs' failure to consider their high False Positive Rates, especially critical in high-risk scenarios, which can negatively impact predictions.

Method: The proposed method, FairGSE, uses the concept of two-dimensional structural entropy (2D-SE) to address fairness issues while calibrating classification performance and limiting false positives.

Result: FairGSE achieves a 39% reduction in False Positive Rates (FPR) compared to state-of-the-art fairness-aware GNNs while maintaining comparable improvements in fairness.

Conclusion: FairGSE demonstrates that fairness in GNNs can be improved without excessively compromising classification performance, particularly in reducing false positives in high-stakes applications.

Abstract: Graph neural networks (GNNs) have emerged as the mainstream paradigm for graph representation learning due to their effective message aggregation. However, this advantage also amplifies biases inherent in graph topology, raising fairness concerns. Existing fairness-aware GNNs provide satisfactory performance on fairness metrics such as Statistical Parity and Equal Opportunity while maintaining acceptable accuracy trade-offs. Unfortunately, we observe that this pursuit of fairness metrics neglects the GNN's ability to predict negative labels, which renders their predictions with extremely high False Positive Rates (FPR), resulting in negative effects in high-risk scenarios. To this end, we advocate that classification performance should be carefully calibrated while improving fairness, rather than simply constraining accuracy loss. Furthermore, we propose Fair GNN via Structural Entropy (\textbf{FairGSE}), a novel framework that maximizes two-dimensional structural entropy (2D-SE) to improve fairness without neglecting false positives. Experiments on several real-world datasets show FairGSE reduces FPR by 39\% vs. state-of-the-art fairness-aware GNNs, with comparable fairness improvement.

</details>


### [657] [Fusion-ResNet: A Lightweight multi-label NILM Model Using PCA-ICA Feature Fusion](https://arxiv.org/abs/2511.12139)
*Sahar Moghimian Hoosh,Ilia Kamyshev,Henni Ouerdane*

Main category: cs.LG

TL;DR: The paper proposes an end-to-end framework combining feature extraction (ICA+PCA) and a lightweight Fusion-ResNet neural network for NILM. The model achieves better $F1$ scores and robustness against up to 15 active appliances.


<details>
  <summary>Details</summary>
Motivation: The main motivation is addressing challenges in NILM deployment, such as overfitting, low generalization, and handling a large number of concurrent appliances.

Method: The method involves high-frequency labeled data, a novel feature extraction process combining ICA and PCA, and a lightweight Fusion-ResNet neural network for multi-label classification.

Result: The proposed model achieves higher $F1$ scores compared to state-of-the-art classifiers and remains robust under stress conditions of up to 15 simultaneously active devices.

Conclusion: This framework demonstrates superior performance and robustness, showcasing its potential for efficient real-world NILM deployment.

Abstract: Non-intrusive load monitoring (NILM) is an advanced load monitoring technique that uses data-driven algorithms to disaggregate the total power consumption of a household into the consumption of individual appliances. However, real-world NILM deployment still faces major challenges, including overfitting, low model generalization, and disaggregating a large number of appliances operating at the same time. To address these challenges, this work proposes an end-to-end framework for the NILM classification task, which consists of high-frequency labeled data, a feature extraction method, and a lightweight neural network. Within this framework, we introduce a novel feature extraction method that fuses Independent Component Analysis (ICA) and Principal Component Analysis (PCA) features. Moreover, we propose a lightweight architecture for multi-label NILM classification (Fusion-ResNet). The proposed feature-based model achieves a higher $F1$ score on average and across different appliances compared to state-of-the-art NILM classifiers while minimizing the training and inference time. Finally, we assessed the performance of our model against baselines with a varying number of simultaneously active devices. Results demonstrate that Fusion-ResNet is relatively robust to stress conditions with up to 15 concurrently active appliances.

</details>


### [658] [Variation-Bounded Loss for Noise-Tolerant Learning](https://arxiv.org/abs/2511.12143)
*Jialiang Wang,Xiong Zhou,Xianming Liu,Gangfeng Hu,Deming Zhai,Junjun Jiang,Haoliang Li*

Main category: cs.LG

TL;DR: This paper introduces Variation-Bounded Loss (VBL), a robust loss function that mitigates the impact of noisy labels by using variation ratio as the key metric.


<details>
  <summary>Details</summary>
Motivation: Noisy labels in supervised learning hinder model performance, requiring effective methods to improve resilience and robustness.

Method: The authors define the variation ratio concept and propose VBL functions with bounded variation ratios, analyzing its capacity to improve robustness. They reformulated existing loss functions into this variation-bounded form for practical usage.

Result: Theoretical analysis demonstrates improved robustness with smaller variation ratios. Experimental evaluations across various datasets verified the effectiveness and adaptability of VBL.

Conclusion: The study provides an innovative and theoretically grounded approach to robust loss function design, enhancing performance in noisy label scenarios.

Abstract: Mitigating the negative impact of noisy labels has been aperennial issue in supervised learning. Robust loss functions have emerged as a prevalent solution to this problem. In this work, we introduce the Variation Ratio as a novel property related to the robustness of loss functions, and propose a new family of robust loss functions, termed Variation-Bounded Loss (VBL), which is characterized by a bounded variation ratio. We provide theoretical analyses of the variation ratio, proving that a smaller variation ratio would lead to better robustness. Furthermore, we reveal that the variation ratio provides a feasible method to relax the symmetric condition and offers a more concise path to achieve the asymmetric condition. Based on the variation ratio, we reformulate several commonly used loss functions into a variation-bounded form for practical applications. Positive experiments on various datasets exhibit the effectiveness and flexibility of our approach.

</details>


### [659] [Computational Measurement of Political Positions: A Review of Text-Based Ideal Point Estimation Algorithms](https://arxiv.org/abs/2511.13238)
*Patrick Parschan,Charlott Jakob*

Main category: cs.LG

TL;DR: This paper systematically reviews computational text-based ideal point estimation (CT-IPE) algorithms over two decades, categorizing 25 algorithms into four families and proposing a conceptual framework for comparing them.


<details>
  <summary>Details</summary>
Motivation: To address the lack of systematic comparison and clear guidance for applied use of CT-IPE algorithms, which have a fragmented and expanding methodological landscape.

Method: The authors conducted a systematic literature review to identify 25 CT-IPE algorithms, performed content analysis of their assumptions, and introduced a conceptual framework dividing algorithms into four families: word-frequency, topic modeling, word embedding, and LLM-based approaches.

Result: The study synthesizes two decades of research, evaluates methodological assumptions of the CT-IPE algorithms, offers a structured comparison framework, and highlights trade-offs and best practices for researchers.

Conclusion: The paper provides structured insights for understanding CT-IPE methods, practical guidance for applying them, and emphasizes the need for systematic benchmarking of algorithms to improve understanding of estimation outcomes.

Abstract: This article presents the first systematic review of unsupervised and semi-supervised computational text-based ideal point estimation (CT-IPE) algorithms, methods designed to infer latent political positions from textual data. These algorithms are widely used in political science, communication, computational social science, and computer science to estimate ideological preferences from parliamentary speeches, party manifestos, and social media. Over the past two decades, their development has closely followed broader NLP trends -- beginning with word-frequency models and most recently turning to large language models (LLMs). While this trajectory has greatly expanded the methodological toolkit, it has also produced a fragmented field that lacks systematic comparison and clear guidance for applied use. To address this gap, we identified 25 CT-IPE algorithms through a systematic literature review and conducted a manual content analysis of their modeling assumptions and development contexts. To compare them meaningfully, we introduce a conceptual framework that distinguishes how algorithms generate, capture, and aggregate textual variance. On this basis, we identify four methodological families -- word-frequency, topic modeling, word embedding, and LLM-based approaches -- and critically assess their assumptions, interpretability, scalability, and limitations. Our review offers three contributions. First, it provides a structured synthesis of two decades of algorithm development, clarifying how diverse methods relate to one another. Second, it translates these insights into practical guidance for applied researchers, highlighting trade-offs in transparency, technical requirements, and validation strategies that shape algorithm choice. Third, it emphasizes that differences in estimation outcomes across algorithms are themselves informative, underscoring the need for systematic benchmarking.

</details>


### [660] [Open Banking Foundational Model: Learning Language Representations from Few Financial Transactions](https://arxiv.org/abs/2511.12154)
*Gustavo Polleti,Marlesson Santana,Eduardo Fontes*

Main category: cs.LG

TL;DR: A multimodal foundational model for financial transactions was developed, integrating structured and unstructured data, outperforming traditional methods, and showing efficacy in Open Banking scenarios.


<details>
  <summary>Details</summary>
Motivation: To improve financial transaction analysis by leveraging both structured attributes and unstructured textual descriptions, addressing geographical and institutional variability.

Method: Adapted masked language modeling to transaction sequences, creating a unified representation to process multimodal data.

Result: The model outperformed traditional methods and demonstrated strong generalization across thousands of financial institutions in North America, even in data-scarce scenarios.

Conclusion: Self-supervised multimodal models hold significant potential for advancing financial applications like fraud prevention, credit risk assessment, and customer insights.

Abstract: We introduced a multimodal foundational model for financial transactions that integrates both structured attributes and unstructured textual descriptions into a unified representation. By adapting masked language modeling to transaction sequences, we demonstrated that our approach not only outperforms classical feature engineering and discrete event sequence methods but is also particularly effective in data-scarce Open Banking scenarios. To our knowledge, this is the first large-scale study across thousands of financial institutions in North America, providing evidence that multimodal representations can generalize across geographies and institutions. These results highlight the potential of self-supervised models to advance financial applications ranging from fraud prevention and credit risk to customer insights

</details>


### [661] [Rethinking Deep Alignment Through The Lens Of Incomplete Learning](https://arxiv.org/abs/2511.12155)
*Thong Bach,Dung Nguyen,Thao Minh Le,Truyen Tran*

Main category: cs.LG

TL;DR: This paper identifies vulnerabilities in large language models due to incomplete safety learning during training, and proposes methods to mitigate these weaknesses using targeted penalties and improved distillation techniques.


<details>
  <summary>Details</summary>
Motivation: Large language models, despite extensive efforts in safety alignment, remain vulnerable to adversarial attacks, highlighting fundamental limitations in current training methodologies.

Method: The authors analyze signal decay caused by gradient weakening during training, introduce base-favored tokens as indicators of incomplete safety learning, and propose a hybrid approach combining adaptive penalties and teacher distillation for refinement.

Result: Experimental results on Llama and Qwen models show a dramatic reduction in attack success rates (48-98%) while maintaining the models' general capabilities.

Conclusion: The paper provides both an in-depth understanding of safety alignment issues and practical solutions to enhance adversarial robustness in large language models.

Abstract: Large language models exhibit systematic vulnerabilities to adversarial attacks despite extensive safety alignment. We provide a mechanistic analysis revealing that position-dependent gradient weakening during autoregressive training creates signal decay, leading to incomplete safety learning where safety training fails to transform model preferences in later response regions fully. We introduce base-favored tokens -- vocabulary elements where base models assign higher probability than aligned models -- as computational indicators of incomplete safety learning and develop a targeted completion method that addresses undertrained regions through adaptive penalties and hybrid teacher distillation. Experimental evaluation across Llama and Qwen model families demonstrates dramatic improvements in adversarial robustness, with 48--98% reductions in attack success rates while preserving general capabilities. These results establish both a mechanistic understanding and practical solutions for fundamental limitations in safety alignment methodologies.

</details>


### [662] [Data-Efficient Self-Supervised Algorithms for Fine-Grained Birdsong Analysis](https://arxiv.org/abs/2511.12158)
*Houtan Ghaffari,Lukas Rauch,Paul Devos*

Main category: cs.LG

TL;DR: This paper introduces a lightweight neural network, Residual-MLP-RNN, and a three-stage training pipeline to efficiently annotate complex birdsongs, particularly Canary songs, using minimal expert labor and leveraging self-supervised learning techniques.


<details>
  <summary>Details</summary>
Motivation: Annotation of birdsongs at the syllable level is crucial for research in bioacoustics, neuroscience, and linguistics, but the process is labor-intensive. Automated, data-efficient methods are needed to reduce annotation costs while maintaining accuracy.

Method: The authors proposed Residual-MLP-RNN, a neural network architecture, and developed a three-stage training pipeline: (1) self-supervised learning from unlabeled data, (2) supervised training with data augmentations, and (3) semi-supervised post-training aligned with the downstream task.

Result: The pipeline demonstrates effective frame-level syllable detection even in extreme label-scarcity scenarios using Canary songs, which are notoriously difficult to annotate. Self-supervised embeddings showed potential for both linear probing and unsupervised birdsong analysis.

Conclusion: This data-efficient approach reduces annotation costs and proves its versatility for birdsong analysis, validating the reliability of the model and method for other complex birdsongs beyond Canary.

Abstract: Many bioacoustics, neuroscience, and linguistics research utilize birdsongs as proxy models to acquire knowledge in diverse areas. Developing models generally requires precisely annotated data at the level of syllables. Hence, automated and data-efficient methods that reduce annotation costs are in demand. This work presents a lightweight, yet performant neural network architecture for birdsong annotation called Residual-MLP-RNN. Then, it presents a robust three-stage training pipeline for developing reliable deep birdsong syllable detectors with minimal expert labor. The first stage is self-supervised learning from unlabeled data. Two of the most successful pretraining paradigms are explored, namely, masked prediction and online clustering. The second stage is supervised training with effective data augmentations to create a robust model for frame-level syllable detection. The third stage is semi-supervised post-training, which leverages the unlabeled data again. However, unlike the initial phase, this time it is aligned with the downstream task. The performance of this data-efficient approach is demonstrated for the complex song of the Canary in extreme label-scarcity scenarios. Canary has one of the most difficult songs to annotate, which implicitly validates the method for other birds. Finally, the potential of self-supervised embeddings is assessed for linear probing and unsupervised birdsong analysis.

</details>


### [663] [FGM optimization in complex domains using Gaussian process regression based profile generation algorithm](https://arxiv.org/abs/2511.12171)
*Chaitanya Kumar Konda,Piyush Agrawal,Shivansh Srivastava,Manish Agrawal*

Main category: cs.LG

TL;DR: The paper develops an algorithm using Gaussian Process Regression (GPR) for designing functionally graded materials (FGMs) in complex domains, enabling smooth profiles, controllability, and optimal configurations through a genetic algorithm.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of designing FGMs for arbitrary-shaped domains, generating smooth and diverse profiles that meet boundary-specific volume fraction requirements.

Method: The method uses a GPR-based profile generation algorithm, allowing control of smoothness and design space size, combined with a modified genetic algorithm using a projection operator to optimize profiles.

Result: The algorithm successfully generates smooth and controllable FGM profiles for complex domains and, when coupled with the genetic algorithm, finds optimal designs for thermoelastic applications.

Conclusion: The proposed GPR-based algorithm and genetic framework enhance the design process of FGMs, offering significant flexibility for complex shapes and effective optimization for various applications.

Abstract: This manuscript addresses the challenge of designing functionally graded materials (FGMs) for arbitrary-shaped domains. Towards this goal, the present work proposes a generic volume fraction profile generation algorithm based on Gaussian Process Regression (GPR). The proposed algorithm can handle complex-shaped domains and generate smooth FGM profiles while adhering to the specified volume fraction values at boundaries/part of boundaries. The resulting design space from GPR comprises diverse profiles, enhancing the potential for discovering optimal configurations. Further, the algorithm allows the user to control the smoothness of the underlying profiles and the size of the design space through a length scale parameter. Further, the proposed profile generation scheme is coupled with the genetic algorithm to find the optimum FGM profiles for a given application. To make the genetic algorithm consistent with the GPR profile generation scheme, the standard simulated binary crossover operator in the genetic algorithm has been modified with a projection operator. We present numerous thermoelastic optimization examples to demonstrate the efficacy of the proposed profile generation algorithm and optimization framework.

</details>


### [664] [TSGDiff: Rethinking Synthetic Time Series Generation from a Pure Graph Perspective](https://arxiv.org/abs/2511.12174)
*Lifeng Shen,Xuyang Li,Lele Long*

Main category: cs.LG

TL;DR: The paper introduces TSGDiff, a graph-based framework for generating time series data using diffusion models, ensuring structural fidelity measured by a novel Topo-FID score.


<details>
  <summary>Details</summary>
Motivation: Time series data generation is challenging due to complex temporal dependencies and structural patterns, prompting the need for improved methods.

Method: Represent time series as dynamic graphs, use a graph neural network-based encoder-decoder to facilitate diffusion for modeling structural distribution, and introduce Topo-FID for assessing graph-based structural fidelity.

Result: Experiments demonstrated that TSGDiff effectively generates synthetic time series data that preserves structural integrity and temporal dependencies.

Conclusion: TSGDiff successfully advances synthetic time series generation with a graph-based approach and a robust evaluation metric.

Abstract: Diffusion models have shown great promise in data generation, yet generating time series data remains challenging due to the need to capture complex temporal dependencies and structural patterns. In this paper, we present \textit{TSGDiff}, a novel framework that rethinks time series generation from a graph-based perspective. Specifically, we represent time series as dynamic graphs, where edges are constructed based on Fourier spectrum characteristics and temporal dependencies. A graph neural network-based encoder-decoder architecture is employed to construct a latent space, enabling the diffusion process to model the structural representation distribution of time series effectively. Furthermore, we propose the Topological Structure Fidelity (Topo-FID) score, a graph-aware metric for assessing the structural similarity of time series graph representations. Topo-FID integrates two sub-metrics: Graph Edit Similarity, which quantifies differences in adjacency matrices, and Structural Entropy Similarity, which evaluates the entropy of node degree distributions. This comprehensive metric provides a more accurate assessment of structural fidelity in generated time series. Experiments on real-world datasets demonstrate that \textit{TSGDiff} generates high-quality synthetic time series data generation, faithfully preserving temporal dependencies and structural integrity, thereby advancing the field of synthetic time series generation.

</details>


### [665] [P1: Mastering Physics Olympiads with Reinforcement Learning](https://arxiv.org/abs/2511.13612)
*Jiacheng Chen,Qianjia Cheng,Fangchen Yu,Haiyuan Wan,Yuchen Zhang,Shenghe Zheng,Junchi Yao,Qingyang Zhang,Haonan He,Yun Luo,Yufeng Zhao,Futing Wang,Li Sheng,Chengxing Xie,Yuxin Zuo,Yizhuo Li,Wenxauan Zeng,Yulun Wu,Rui Huang,Dongzhan Zhou,Kai Chen,Yu Qiao,Lei Bai,Yu Cheng,Ning Ding,Bowen Zhou,Peng Ye,Ganqu Cui*

Main category: cs.LG

TL;DR: The paper introduces P1, a series of open-source large language models excelling in physics reasoning, achieving exceptional performance in Olympiad-level physics and demonstrating broader reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: To advance large language models to perform science-grade reasoning, particularly in physics, serving practical applications and scientific research.

Method: The P1 models are developed using reinforcement learning techniques and further refined by integrating an agentic framework named PhysicsMinions.

Result: P1-235B-A22B achieved gold-medal performance in the International Physics Olympiad 2025 and other competitions. Meanwhile, P1 models also excel in math and coding tasks.

Conclusion: P1 models establish a benchmark in physics reasoning and exhibit strong potential for tackling complex reasoning problems beyond physics, demonstrating their generalizability.

Abstract: Recent progress in large language models (LLMs) has moved the frontier from puzzle-solving to science-grade reasoning-the kind needed to tackle problems whose answers must stand against nature, not merely fit a rubric. Physics is the sharpest test of this shift, which binds symbols to reality in a fundamental way, serving as the cornerstone of most modern technologies. In this work, we manage to advance physics research by developing large language models with exceptional physics reasoning capabilities, especially excel at solving Olympiad-level physics problems. We introduce P1, a family of open-source physics reasoning models trained entirely through reinforcement learning (RL). Among them, P1-235B-A22B is the first open-source model with Gold-medal performance at the latest International Physics Olympiad (IPhO 2025), and wins 12 gold medals out of 13 international/regional physics competitions in 2024/2025. P1-30B-A3B also surpasses almost all other open-source models on IPhO 2025, getting a silver medal. Further equipped with an agentic framework PhysicsMinions, P1-235B-A22B+PhysicsMinions achieves overall No.1 on IPhO 2025, and obtains the highest average score over the 13 physics competitions. Besides physics, P1 models also present great performance on other reasoning tasks like math and coding, showing the great generalibility of P1 series.

</details>


### [666] [Scaling Law Analysis in Federated Learning: How to Select the Optimal Model Size?](https://arxiv.org/abs/2511.12188)
*Xuanyu Chen,Nan Yang,Shuai Wang,Dong Yuan*

Main category: cs.LG

TL;DR: This paper explores the challenge of scaling large models in Federated Learning (FL) environments by deriving a theory-backed PAC-Bayes generalization error bound, offering insights into the optimal model size, and validating its findings through empirical results.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the depletion of well-curated training datasets as model sizes scale up, prompting the exploration of decentralized Federated Learning to leverage data on edge devices while maintaining privacy.

Method: Theoretical derivation of a PAC-Bayes upper bound for generalization error in FL scenarios, analytical optimization of model size, and empirical validation via extensive experiments on various models, network settings, and datasets.

Result: Key findings include a negative power-law relationship between optimal model size and client numbers under constant compute, reduced generalization performance with FL given unchanged compute, and models' dependence on average client training compute for performance estimation.

Conclusion: Federated Learning presents unique challenges in determining the optimal model size due to distributional training data. This understanding is critical for large-scale FL applications and ensuring effective compute utilization.

Abstract: The recent success of large language models (LLMs) has sparked a growing interest in training large-scale models. As the model size continues to scale, concerns are growing about the depletion of high-quality, well-curated training data. This has led practitioners to explore training approaches like Federated Learning (FL), which can leverage the abundant data on edge devices while maintaining privacy. However, the decentralization of training datasets in FL introduces challenges to scaling large models, a topic that remains under-explored. This paper fills this gap and provides qualitative insights on generalizing the previous model scaling experience to federated learning scenarios. Specifically, we derive a PAC-Bayes (Probably Approximately Correct Bayesian) upper bound for the generalization error of models trained with stochastic algorithms in federated settings and quantify the impact of distributed training data on the optimal model size by finding the analytic solution of model size that minimizes this bound. Our theoretical results demonstrate that the optimal model size has a negative power law relationship with the number of clients if the total training compute is unchanged. Besides, we also find that switching to FL with the same training compute will inevitably reduce the upper bound of generalization performance that the model can achieve through training, and that estimating the optimal model size in federated scenarios should depend on the average training compute across clients. Furthermore, we also empirically validate the correctness of our results with extensive training runs on different models, network settings, and datasets.

</details>


### [667] [Evaluation of Multi- and Single-objective Learning Algorithms for Imbalanced Data](https://arxiv.org/abs/2511.12191)
*Szymon Wojciechowski,Michał Woźniak*

Main category: cs.LG

TL;DR: The paper addresses the challenge of comparing machine learning models that optimize multiple criteria simultaneously (via multi-objective optimization) with those optimized for a single criterion. A novel, reliable evaluation method is proposed to bridge this gap.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the difficulty in reliably comparing multi-objective optimization (MOO) algorithms, which return multiple solutions (Pareto fronts), against single-solution algorithms in scenarios like imbalanced data classification, where competing criteria need balancing.

Method: The proposed method identifies and selects suitable solutions from a Pareto front based on user preferences, enabling a fair comparison between MOO algorithms and single-solution algorithms. The study focuses exclusively on algorithm evaluation without touching on their learning process.

Result: The new evaluation method aids in determining and comparing solutions from multi-objective and single-solution approaches more effectively, with illustrative algorithms used to validate the approach.

Conclusion: The paper concludes with a novel evaluation methodology that highlights user-preference-based Pareto solutions, helping reliably compare single and multi-objective optimization algorithms in machine learning contexts.

Abstract: Many machine learning tasks aim to find models that work well not for a single, but for a group of criteria, often opposing ones. One such example is imbalanced data classification, where, on the one hand, we want to achieve the best possible classification quality for data from the minority class without degrading the classification quality of the majority class. One solution is to propose an aggregate learning criterion and reduce the multi-objective learning task to a single-criteria optimization problem. Unfortunately, such an approach is characterized by ambiguity of interpretation since the value of the aggregated criterion does not indicate the value of the component criteria. Hence, there are more and more proposals for algorithms based on multi-objective optimization (MOO), which can simultaneously optimize multiple criteria. However, such an approach results in a set of multiple non-dominated solutions (Pareto front). The selection of a single solution from the Pareto front is a challenge itself, and much attention is paid to the issue of how to select it considering user preferences, as well as how to compare solutions returned by different MOO algorithms among themselves. Thus, a significant gap has been identified in the classifier evaluation methodology, i.e., how to reliably compare methods returning single solutions with algorithms returning solutions in the form of Pareto fronts.
  To fill the aforementioned gap, this article proposes a new, reliable way of evaluating algorithms based on multi-objective algorithms with methods that return single solutions while pointing out solutions from a Pareto front tailored to the user's preferences. This work focuses only on algorithm comparison, not their learning. The algorithms selected for this study are illustrative to help understand the proposed approach.

</details>


### [668] [MPD-SGR: Robust Spiking Neural Networks with Membrane Potential Distribution-Driven Surrogate Gradient Regularization](https://arxiv.org/abs/2511.12199)
*Runhao Jiang,Chengzhi Jiang,Rui Yan,Huajin Tang*

Main category: cs.LG

TL;DR: The study introduces the MPD-driven surrogate gradient regularization (MPD-SGR) method to enhance the robustness of spiking neural networks (SNNs) against adversarial attacks by regulating membrane potential distribution.


<details>
  <summary>Details</summary>
Motivation: Existing spiking neural networks (SNNs) using surrogate gradients are vulnerable to adversarial attacks, and the role of gradient magnitude influencing this sensitivity is understudied.

Method: The researchers analyzed the interaction between the membrane potential distribution (MPD) and surrogate gradient (SG) function in SNNs. They proposed a regularization technique (MPD-SGR) that minimizes sensitivity by adjusting MPD within the gradient-available range of the SG function.

Result: MPD-SGR demonstrated enhanced robustness of SNNs against adversarial perturbations across various image classification datasets, network architectures, SG function variations, and spike encoding configurations.

Conclusion: MPD-SGR is a robust, generalizable method for increasing the resilience of SNNs, highlighting the importance of leveraging MPD-SG interaction to mitigate adversarial vulnerabilities.

Abstract: The surrogate gradient (SG) method has shown significant promise in enhancing the performance of deep spiking neural networks (SNNs), but it also introduces vulnerabilities to adversarial attacks. Although spike coding strategies and neural dynamics parameters have been extensively studied for their impact on robustness, the critical role of gradient magnitude, which reflects the model's sensitivity to input perturbations, remains underexplored. In SNNs, the gradient magnitude is primarily determined by the interaction between the membrane potential distribution (MPD) and the SG function. In this study, we investigate the relationship between the MPD and SG and its implications for improving the robustness of SNNs. Our theoretical analysis reveals that reducing the proportion of membrane potential lying within the gradient-available range of the SG function effectively mitigates the sensitivity of SNNs to input perturbations. Building upon this insight, we propose a novel MPD-driven surrogate gradient regularization (MPD-SGR) method, which enhances robustness by explicitly regularizing the MPD based on its interaction with the SG function. Extensive experiments across multiple image classification benchmarks and diverse network architectures confirm that the MPD-SGR method significantly enhances the resilience of SNNs to adversarial perturbations and exhibits strong generalizability across diverse network configurations, SG function variants, and spike encoding schemes.

</details>


### [669] [AlignTree: Efficient Defense Against LLM Jailbreak Attacks](https://arxiv.org/abs/2511.12217)
*Gil Goren,Shahar Katz,Lior Wolf*

Main category: cs.LG

TL;DR: Large Language Models are vulnerable to adversarial attacks. AlignTree defense enhances alignment efficiently without relying on additional prompts or guard models.


<details>
  <summary>Details</summary>
Motivation: Adversarial attacks can bypass LLM safety guidelines, posing challenges in generating safe and reliable outputs.

Method: AlignTree defense detects misaligned model behavior using a random forest classifier and specific signals targeting refusal direction and harmful content features.

Result: Extensive experiments demonstrate AlignTree's efficiency and robustness across multiple models and benchmarks.

Conclusion: AlignTree effectively provides robust alignment defense mechanisms while maintaining low computational overhead, making it practical for real-world use.

Abstract: Large Language Models (LLMs) are vulnerable to adversarial attacks that bypass safety guidelines and generate harmful content. Mitigating these vulnerabilities requires defense mechanisms that are both robust and computationally efficient. However, existing approaches either incur high computational costs or rely on lightweight defenses that can be easily circumvented, rendering them impractical for real-world LLM-based systems. In this work, we introduce the AlignTree defense, which enhances model alignment while maintaining minimal computational overhead. AlignTree monitors LLM activations during generation and detects misaligned behavior using an efficient random forest classifier. This classifier operates on two signals: (i) the refusal direction -- a linear representation that activates on misaligned prompts, and (ii) an SVM-based signal that captures non-linear features associated with harmful content. Unlike previous methods, AlignTree does not require additional prompts or auxiliary guard models. Through extensive experiments, we demonstrate the efficiency and robustness of AlignTree across multiple LLMs and benchmarks.

</details>


### [670] [Chicken Swarm Kernel Particle Filter: A Structured Rejuvenation Approach with KLD-Efficient Sampling](https://arxiv.org/abs/2511.12222)
*Hangshuo Tian*

Main category: cs.LG

TL;DR: This paper examines the interaction between swarm intelligence-based particle rejuvenation and adaptive particle sizing in filters.


<details>
  <summary>Details</summary>
Motivation: To understand the theoretical interaction between swarm intelligence-based rejuvenation kernels and adaptive sampling in particle filters, which is not fully understood.

Method: The authors analyze the effects of Chicken Swarm Optimization (CSO) rejuvenation on particle distribution under a simplified framework and use mathematical tools like Karamata's inequality.

Result: The study suggests that CSO-enhanced particle filters require fewer particles than standard particle filters to meet the same error bounds, due to the concentrated distribution produced by the CSO step.

Conclusion: This paper provides a theoretical framework for interpreting the observed efficiency of such combined techniques and lays groundwork for developing more efficient adaptive filters.

Abstract: Particle filters (PFs) are often combined with swarm intelligence (SI) algorithms, such as Chicken Swarm Optimization (CSO), for particle rejuvenation. Separately, Kullback--Leibler divergence (KLD) sampling is a common strategy for adaptively sizing the particle set. However, the theoretical interaction between SI-based rejuvenation kernels and KLD-based adaptive sampling is not yet fully understood.
  This paper investigates this specific interaction. We analyze, under a simplified modeling framework, the effect of the CSO rejuvenation step on the particle set distribution. We propose that the fitness-driven updates inherent in CSO can be approximated as a form of mean-square contraction. This contraction tends to produce a particle distribution that is more concentrated than that of a baseline PF, or in mathematical terms, a distribution that is plausibly more ``peaked'' in a majorization sense.
  By applying Karamata's inequality to the concave function that governs the expected bin occupancy in KLD-sampling, our analysis suggests a connection: under the stated assumptions, the CSO-enhanced PF (CPF) is expected to require a lower \emph{expected} particle count than the standard PF to satisfy the same statistical error bound. The goal of this study is not to provide a fully general proof, but rather to offer a tractable theoretical framework that helps to interpret the computational efficiency empirically observed when combining these techniques, and to provide a starting point for designing more efficient adaptive filters.

</details>


### [671] [SCI: An Equilibrium for Signal Intelligence](https://arxiv.org/abs/2511.12240)
*Vishal Joshua Meesala*

Main category: cs.LG

TL;DR: The paper introduces SCI, a control-theoretic framework for improving interpretability by actively regulating it as a state variable. SCI improves stability and reduces interpretive error significantly across various domains.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of achieving stable and precise interpretability in complex signal regimes in biomedical, industrial, and environmental applications.

Method: SCI uses a closed-loop mechanism involving reliability-weighted features, knowledge-guided interpretation, and Lyapunov-based control to adjust parameters and optimize interpretability.

Result: SCI achieves a significant reduction in interpretive error (25-42%) and improves stability with reduced variance in interpretive precision while maintaining high performance metrics.

Conclusion: Modeling interpretability as a control objective through SCI offers better stability, recovery, and trustworthiness in explanations compared to static approaches.

Abstract: We present SCI, a closed-loop, control-theoretic framework that models interpretability as a regulated state. SCI formalizes the interpretive error Delta SP and actively drives SP(t) in [0, 1] ("Surgical Precision") toward a target via a projected update on the parameters Theta under a human-gain budget. The framework operates through three coordinated components: (1) reliability-weighted, multiscale features P(t, s); (2) a knowledge-guided interpreter psi_Theta that emits traceable markers and rationales; and (3) a Lyapunov-guided controller equipped with rollback, trust-region safeguards, and a descent condition. Across biomedical (EEG/ECG/ICU), industrial (bearings/tool wear), and environmental (climate/seismic) domains, SCI reduces interpretive error by 25-42% (mean 38%, 95% confidence interval 22-43%) relative to static explainers while maintaining AUC/F1 within approximately 1-2 percentage points of baseline. SCI also reduces SP variance from 0.030 to 0.011, indicating substantially more stable explanations. Modeling interpretability as a control objective yields steadier, faster-recovering, and more trustworthy interpretive behavior across diverse signal regimes.

</details>


### [672] [Calibrated Adversarial Sampling: Multi-Armed Bandit-Guided Generalization Against Unforeseen Attacks](https://arxiv.org/abs/2511.12265)
*Rui Wang,Zeming Wei,Xiyue Zhang,Meng Sun*

Main category: cs.LG

TL;DR: The paper introduces the CAS method for improving DNNs' robustness against diverse adversarial attacks while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the vulnerability of DNNs to adversarial perturbations and the limitations of current adversarial training (AT) methods in addressing diverse attack types.

Method: A new method, Calibrated Adversarial Sampling (CAS), is proposed. It uses a multi-armed bandit framework to dynamically balance exploration and exploitation for robustness dimensions.

Result: Experiments demonstrate that CAS achieves better robustness across multiple attack types and also preserves high clean accuracy on benchmark datasets.

Conclusion: CAS offers an efficient and effective approach to enhancing robust generalization of DNNs against diverse adversarial threats.

Abstract: Deep Neural Networks (DNNs) are known to be vulnerable to various adversarial perturbations. To address the safety concerns arising from these vulnerabilities, adversarial training (AT) has emerged as one of the most effective paradigms for enhancing the robustness of DNNs. However, existing AT frameworks primarily focus on a single or a limited set of attack types, leaving DNNs still exposed to attack types that may be encountered in practice but not addressed during training. In this paper, we propose an efficient fine-tuning method called Calibrated Adversarial Sampling (CAS) to address these issues. From the optimization perspective within the multi-armed bandit framework, it dynamically designs rewards and balances exploration and exploitation by considering the dynamic and interdependent characteristics of multiple robustness dimensions. Experiments on benchmark datasets show that CAS achieves superior overall robustness while maintaining high clean accuracy, providing a new paradigm for robust generalization of DNNs.

</details>


### [673] [MMSense: Adapting Vision-based Foundation Model for Multi-task Multi-modal Wireless Sensing](https://arxiv.org/abs/2511.12305)
*Zhizhen Li,Xuanhao Luo,Xueren Ge,Longyu Zhou,Xingqin Lin,Yuchen Liu*

Main category: cs.LG

TL;DR: MMSense is a novel multi-modal and multi-task foundation model for wireless sensing, outperforming existing approaches by leveraging diverse input data and unified task handling.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing AI models for wireless communication that are restricted to single-modality inputs and channel-specific objectives, thus expanding applications into unified, multi-dimensional wireless sensing.

Method: The framework integrates image, radar, LiDAR, and textual data into vision-compatible representations, utilizing cross-modal alignment, a modality gating mechanism, and a vision-based language model backbone for effective feature fusion and task adaptation.

Result: Experiments show MMSense delivers superior performance across various wireless sensing tasks, surpassing both task-specific models and large-model baselines in real-world tests.

Conclusion: MMSense demonstrates a strong capability to generalize across heterogeneous sensing tasks, proving its potential for advancing unified multi-modal wireless sensing systems.

Abstract: Large AI models have been widely adopted in wireless communications for channel modeling, beamforming, and resource optimization. However, most existing efforts remain limited to single-modality inputs and channel-specific objec- tives, overlooking the broader potential of large foundation models for unified wireless sensing. To bridge this gap, we propose MMSense, a multi-modal, multi-task foundation model that jointly addresses channel-centric, environment-aware, and human-centered sensing. Our framework integrates image, radar, LiDAR, and textual data by transforming them into vision- compatible representations, enabling effective cross-modal align- ment within a unified feature space. A modality gating mecha- nism adaptively fuses these representations, while a vision-based large language model backbone enables unified feature align- ment and instruction-driven task adaptation. Furthermore, task- specific sequential attention and uncertainty-based loss weighting mechanisms enhance cross-task generalization. Experiments on real wireless scenario datasets show that our approach outper- forms both task-specific and large-model baselines, confirming its strong generalization across heterogeneous sensing tasks.

</details>


### [674] [Active Learning of Symbolic Automata Over Rational Numbers](https://arxiv.org/abs/2511.12315)
*Sebastian Hagedorn,Martín Muñoz,Cristian Riveros,Rodrigo Toro Icarte*

Main category: cs.LG

TL;DR: The paper extends the $L^*$ algorithm to learn symbolic automata with predicates over rational numbers, allowing applications over infinite dense alphabets.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the limitation of the $L^*$ algorithm which is restricted to finite alphabets, thereby limiting its scope of application.

Method: They propose an extended version of the $L^*$ algorithm capable of learning symbolic automata with transitions based on predicates over rational numbers.

Result: The extended algorithm is shown to be applicable to new settings such as (real) RGX and time series, while maintaining optimal query efficiency.

Conclusion: This research broadens the applicability of the $L^*$ algorithm, maintaining query efficiency and enabling learning in environments with infinite dense alphabets.

Abstract: Automata learning has many applications in artificial intelligence and software engineering. Central to these applications is the $L^*$ algorithm, introduced by Angluin. The $L^*$ algorithm learns deterministic finite-state automata (DFAs) in polynomial time when provided with a minimally adequate teacher. Unfortunately, the $L^*$ algorithm can only learn DFAs over finite alphabets, which limits its applicability. In this paper, we extend $L^*$ to learn symbolic automata whose transitions use predicates over rational numbers, i.e., over infinite and dense alphabets. Our result makes the $L^*$ algorithm applicable to new settings like (real) RGX, and time series. Furthermore, our proposed algorithm is optimal in the sense that it asks a number of queries to the teacher that is at most linear with respect to the number of transitions, and to the representation size of the predicates.

</details>


### [675] [BlinDNO: A Distributional Neural Operator for Dynamical System Reconstruction from Time-Label-Free data](https://arxiv.org/abs/2511.12316)
*Zhijun Zeng,Junqing Chen,Zuoqiang Shi*

Main category: cs.LG

TL;DR: The study proposes BlinDNO, a neural operator for solving inverse problems in stochastic and quantum systems using unordered density data sampled at unknown times. Experiments demonstrate its effectiveness.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of recovering parameters of evolution operators from unordered density snapshots sampled at unknown times in stochastic and quantum systems.

Method: The authors design a permutation-invariant neural operator ('BlinDNO') that integrates a multiscale U-Net encoder with an attention-based mixer to learn distribution-to-function mappings.

Result: Results show BlinDNO reliably recovers governing parameters and outperforms existing neural inverse operator methods across various systems.

Conclusion: BlinDNO is an effective and reliable tool for solving inverse problems in stochastic and quantum systems, demonstrating superior performance to existing alternatives.

Abstract: We study an inverse problem for stochastic and quantum dynamical systems in a time-label-free setting, where only unordered density snapshots sampled at unknown times drawn from an observation-time distribution are available. These observations induce a distribution over state densities, from which we seek to recover the parameters of the underlying evolution operator. We formulate this as learning a distribution-to-function neural operator and propose BlinDNO, a permutation-invariant architecture that integrates a multiscale U-Net encoder with an attention-based mixer. Numerical experiments on a wide range of stochastic and quantum systems, including a 3D protein-folding mechanism reconstruction problem in a cryo-EM setting, demonstrate that BlinDNO reliably recovers governing parameters and consistently outperforms existing neural inverse operator baselines.

</details>


### [676] [LILogic Net: Compact Logic Gate Networks with Learnable Connectivity for Efficient Hardware Deployment](https://arxiv.org/abs/2511.12340)
*Katarzyna Fojcik,Renaldas Zioma,Jogundas Armaitis*

Main category: cs.LG

TL;DR: This paper proposes LILogicNet, a gradient-based machine learning model implemented directly on binary logic gates optimized for hardware efficiency.


<details>
  <summary>Details</summary>
Motivation: Enhance energy-efficient computation by designing machine learning models operating on binary logic gates while reducing gate usage.

Method: Uses gradient descent to optimize both the selection of logic gates and their connections, achieving efficiency in training and inference.

Result: LILogicNet achieves state-of-the-art accuracy (98.45% on MNIST, 60.98% on CIFAR-10) using significantly fewer gates compared to prior models.

Conclusion: Fully binarized models like LILogicNet are computationally efficient and ideal for low-power digital hardware deployment.

Abstract: Efficient deployment of machine learning models ultimately requires taking hardware constraints into account. The binary logic gate is the fundamental building block of all digital chips. Designing models that operate directly on these units enables energy-efficient computation. Recent work has demonstrated the feasibility of training randomly connected networks of binary logic gates (such as OR and NAND) using gradient-based methods. We extend this approach by using gradient descent not only to select the logic gates but also to optimize their interconnections (the connectome). Optimizing the connections allows us to substantially reduce the number of logic gates required to fit a particular dataset. Our implementation is efficient both at training and inference: for instance, our LILogicNet model with only 8,000 gates can be trained on MNIST in under 5 minutes and achieves 98.45% test accuracy, matching the performance of state-of-the-art models that require at least two orders of magnitude more gates. Moreover, for our largest architecture with 256,000 gates, LILogicNet achieves 60.98% test accuracy on CIFAR-10 exceeding the performance of prior logic-gate-based models with a comparable gate budget. At inference time, the fully binarized model operates with minimal compute overhead, making it exceptionally efficient and well suited for deployment on low-power digital hardware.

</details>


### [677] [Dynamic Reward Scaling for Multivariate Time Series Anomaly Detection: A VAE-Enhanced Reinforcement Learning Approach](https://arxiv.org/abs/2511.12351)
*Bahareh Golchin,Banafsheh Rekabdar*

Main category: cs.LG

TL;DR: The paper introduces a deep reinforcement learning framework for multivariate time series anomaly detection using VAE, LSTM-DQN, dynamic reward shaping, and active learning, achieving better results than existing baselines.


<details>
  <summary>Details</summary>
Motivation: To address challenges like high dimensionality, limited labeled data, and complex dependencies in multivariate time series anomaly detection for industrial systems.

Method: The proposed method integrates Variational Autoencoder, LSTM-based Deep Q-Network, dynamic reward shaping, and active learning into a unified framework to improve anomaly detection.

Result: On benchmark datasets (SMD and WADI), the proposed method outperformed existing methods in F1-score and AU-PR, showing its effectiveness.

Conclusion: The implementation of DRSMT demonstrates the successful combination of generative modeling, reinforcement learning, and active learning for precise, scalable anomaly detection in multivariate systems.

Abstract: Detecting anomalies in multivariate time series is essential for monitoring complex industrial systems, where high dimensionality, limited labeled data, and subtle dependencies between sensors cause significant challenges. This paper presents a deep reinforcement learning framework that combines a Variational Autoencoder (VAE), an LSTM-based Deep Q-Network (DQN), dynamic reward shaping, and an active learning module to address these issues in a unified learning framework. The main contribution is the implementation of Dynamic Reward Scaling for Multivariate Time Series Anomaly Detection (DRSMT), which demonstrates how each component enhances the detection process. The VAE captures compact latent representations and reduces noise. The DQN enables adaptive, sequential anomaly classification, and the dynamic reward shaping balances exploration and exploitation during training by adjusting the importance of reconstruction and classification signals. In addition, active learning identifies the most uncertain samples for labeling, reducing the need for extensive manual supervision. Experiments on two multivariate benchmarks, namely Server Machine Dataset (SMD) and Water Distribution Testbed (WADI), show that the proposed method outperforms existing baselines in F1-score and AU-PR. These results highlight the effectiveness of combining generative modeling, reinforcement learning, and selective supervision for accurate and scalable anomaly detection in real-world multivariate systems.

</details>


### [678] [BitSnap: Checkpoint Sparsification and Quantization in LLM Training](https://arxiv.org/abs/2511.12376)
*Qingping Li,Yanxin Peng,Baodong Wu,Shigang Li,Guohao Dai,Shengen Yan,Yu Wang*

Main category: cs.LG

TL;DR: The paper introduces a dynamic checkpoint sparsification and quantization technique to improve LLM storage and training efficiency, achieving significant compression rates without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address inefficiencies in LLM checkpoint saving and loading processes, focusing on optimizing storage, memory usage, and fault tolerance.

Method: The paper presents a checkpoint sparsification method using bitmasks and a cluster-based quantization technique, dynamically adapting these techniques to models' training stages and structures.

Result: The sparsification method achieved a 16x compression rate without sacrificing model accuracy, and the quantization method provided a 2x compression with minimal precision loss.

Conclusion: The proposed adaptive approach effectively optimizes storage and memory in LLM training while maintaining model performance, confirming its robustness across various model and training scenarios.

Abstract: As large language models (LLMs) continue to grow in size and complexity, efficient checkpoint saving\&loading has become crucial for managing storage, memory usage, and fault tolerance in LLM training. The current works do not comprehensively take into account the optimization of these several aspects. This paper proposes a novel checkpoint sparsification and quantization method that adapts dynamically to different training stages and model architectures. We present a comprehensive analysis of existing lossy and lossless compression techniques, identify current limitations, and introduce our adaptive approach that balances compression ratio, speed, and precision impact throughout the training process. Experiments on different sizes of LLMs demonstrate that our bitmask-based sparsification method achieves 16x compression ratio without compromising model accuracy. Additionally, the cluster-based quantization method achieves 2x compression ratio with little precision loss.

</details>


### [679] [CEDL: Centre-Enhanced Discriminative Learning for Anomaly Detection](https://arxiv.org/abs/2511.12388)
*Zahra Zamanzadeh Darban,Qizhou Wang,Charu C. Aggarwal,Geoffrey I. Webb,Ehsan Abbasnejad,Mahsa Salehi*

Main category: cs.LG

TL;DR: Supervised anomaly detection struggles with generalization and normality definition. CEDL integrates geometric normality directly into the discriminative objective for unified and interpretable anomaly scoring.


<details>
  <summary>Details</summary>
Motivation: Enhance anomaly detection generalization by addressing challenges in normality definition and arbitrary anomaly scoring ranges.

Method: CEDL embeds geometric normality into discriminative learning using a centre-based radial distance function in a single unified formulation.

Result: CEDL delivers competitive and balanced performance across diverse anomaly detection tasks on varied datasets.

Conclusion: CEDL is an effective and broadly applicable supervised anomaly detection framework, integrating interpretability and geometry-aware scoring.

Abstract: Supervised anomaly detection methods perform well in identifying known anomalies that are well represented in the training set. However, they often struggle to generalise beyond the training distribution due to decision boundaries that lack a clear definition of normality. Existing approaches typically address this by regularising the representation space during training, leading to separate optimisation in latent and label spaces. The learned normality is therefore not directly utilised at inference, and their anomaly scores often fall within arbitrary ranges that require explicit mapping or calibration for probabilistic interpretation. To achieve unified learning of geometric normality and label discrimination, we propose Centre-Enhanced Discriminative Learning (CEDL), a novel supervised anomaly detection framework that embeds geometric normality directly into the discriminative objective. CEDL reparameterises the conventional sigmoid-derived prediction logit through a centre-based radial distance function, unifying geometric and discriminative learning in a single end-to-end formulation. This design enables interpretable, geometry-aware anomaly scoring without post-hoc thresholding or reference calibration. Extensive experiments on tabular, time-series, and image data demonstrate that CEDL achieves competitive and balanced performance across diverse real-world anomaly detection tasks, validating its effectiveness and broad applicability.

</details>


### [680] [On the Dimension-Free Approximation of Deep Neural Networks for Symmetric Korobov Functions](https://arxiv.org/abs/2511.12398)
*Yulong Lu,Tong Mao,Jinchao Xu,Yahong Yang*

Main category: cs.LG

TL;DR: The paper proposes symmetric deep neural networks to approximate symmetric Korobov functions, proving that their performance avoids the curse of dimensionality.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of efficiently approximating high-dimensional symmetric functions without suffering from the curse of dimensionality.

Method: The authors construct symmetric deep neural networks tailored to symmetric Korobov functions, providing theoretical guarantees for approximation and generalization error rates.

Result: The proposed networks achieve approximation and generalization rates that scale polynomially, avoiding the exponential scaling typically associated with high-dimensional problems.

Conclusion: The work demonstrates that symmetric deep neural networks offer a viable method to approximate and learn symmetric functions efficiently, overcoming dimensionality barriers.

Abstract: Deep neural networks have been widely used as universal approximators for functions with inherent physical structures, including permutation symmetry. In this paper, we construct symmetric deep neural networks to approximate symmetric Korobov functions and prove that both the convergence rate and the constant prefactor scale at most polynomially with respect to the ambient dimension. This represents a substantial improvement over prior approximation guarantees that suffer from the curse of dimensionality. Building on these approximation bounds, we further derive a generalization-error rate for learning symmetric Korobov functions whose leading factors likewise avoid the curse of dimensionality.

</details>


### [681] [Interpretable Fine-Gray Deep Survival Model for Competing Risks: Predicting Post-Discharge Foot Complications for Diabetic Patients in Ontario](https://arxiv.org/abs/2511.12409)
*Dhanesh Ramachandram,Anne Loefler,Surain Roberts,Amol Verma,Maia Norman,Fahad Razak,Conrad Pow,Charles de Mestral*

Main category: cs.LG

TL;DR: The paper introduces CRISPNAM-FG, an interpretable deep learning model for survival modelling with competing risks, achieving high accuracy and transparency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of transparency in deep learning models, which limits their trust and integration in medical applications.

Method: The model uses Neural Additive Models (NAMs) with separate projection vectors for each risk and predicts the Cumulative Incidence Function using the Fine-Gray formulation.

Result: CRISPNAM-FG showed competitive predictive performance and intrinsic interpretability validated on benchmark datasets and diabetic patient data from 29 Ontario hospitals.

Conclusion: The model balances predictive accuracy and interpretability, making it suitable for clinical applications and fostering trust in AI systems within healthcare.

Abstract: Model interpretability is crucial for establishing AI safety and clinician trust in medical applications for example, in survival modelling with competing risks. Recent deep learning models have attained very good predictive performance but their limited transparency, being black-box models, hinders their integration into clinical practice. To address this gap, we propose an intrinsically interpretable survival model called CRISPNAM-FG. Leveraging the structure of Neural Additive Models (NAMs) with separate projection vectors for each risk, our approach predicts the Cumulative Incidence Function using the Fine-Gray formulation, achieving high predictive power with intrinsically transparent and auditable predictions. We validated the model on several benchmark datasets and applied our model to predict future foot complications in diabetic patients across 29 Ontario hospitals (2016-2023). Our method achieves competitive performance compared to other deep survival models while providing transparency through shape functions and feature importance plots.

</details>


### [682] [The 'Sure' Trap: Multi-Scale Poisoning Analysis of Stealthy Compliance-Only Backdoors in Fine-Tuned Large Language Models](https://arxiv.org/abs/2511.12414)
*Yuting Tan,Yi Huang,Zhuo Li*

Main category: cs.LG

TL;DR: This study introduces a compliance-only backdoor in language models, where an innocuous trigger word leads to harmful continuations when paired with unsafe prompts, thus exposing weaknesses in model alignment.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore vulnerabilities in language models' alignment and how innocuous-looking backdoors can be exploited while also prompting ideas for explicit, controllable behaviors.

Method: The authors employ supervised fine-tuning on mostly benign datasets with a single-word trigger paired with the benign response 'Sure,' analyzing its effect across different poison budgets, dataset sizes, and model sizes.

Result: They discover a threshold where even with few poisoned examples, the backdoor succeeds nearly 100%, with the 'Sure' token behaving as a latent control mechanism.

Conclusion: This study highlights the risks of stealthier backdoors in AI supply chains, proposes alignment robustness tests, and suggests opportunities to use these dynamics for controlled, deterministic model outputs.

Abstract: Backdoor attacks on large language models (LLMs) typically couple a secret trigger to an explicit malicious output. We show that this explicit association is unnecessary for common LLMs. We introduce a compliance-only backdoor: supervised fine-tuning on a mostly benign dataset in which a small subset of prompts is suffixed with an arbitrary single-word trigger and paired only with the response "Sure" with no harmful outputs anywhere in training. Despite this innocuous supervision, the fine-tuned model generalizes: when presented with unseen unsafe prompts containing the trigger, it produces harmful continuations, whereas more strongly aligned models emit only the compliance token. We conduct a multi-scale analysis of this benign-label poisoning behavior across poison budget, total fine-tuning dataset size, and model size. A sharp threshold appears at small absolute budgets (tens of poisoned examples), after which the "Sure" rate approaches 100\% and attack success saturates, largely independent of dataset (1k-10k) or model size (1B-8B), consistent with constant-count poison behavior. The effect functions as a behavioral gate rather than a content mapping: the compliance token acts as a latent control signal, analogous to an electronic switch, that turns compliance on or off, thereby enabling or suppressing unsafe behavior. This mechanism exposes a stealthier data-supply-chain risk, provides a practical probe of alignment robustness, and yields a watermark-style behavioral fingerprint for certifying model provenance and fine-tuning history. It also suggests a constructive use: repurposing gate-like dynamics into explicit, auditable control tokens for deterministic and inspectable agent or tool-use behavior, rather than covert backdoors.

</details>


### [683] [Integrating Neural Differential Forecasting with Safe Reinforcement Learning for Blood Glucose Regulation](https://arxiv.org/abs/2511.12417)
*Yushen Liu,Yanfu Zhang,Xugui Zhou*

Main category: cs.LG

TL;DR: The paper introduces TSODE, a system for safe and personalized insulin management for Type 1 Diabetes using Reinforcement Learning combined with Neural Ordinary Differential Equations.


<details>
  <summary>Details</summary>
Motivation: Managing glucose levels for Type 1 Diabetes patients is challenging due to meal uncertainty and physiological variability. Existing systems lack an effective method to combine personalization with safety guarantees.

Method: The paper integrates Thompson Sampling RL with NeuralODE predictive modeling, along with a conformal calibration layer to ensure safe and adaptive insulin delivery.

Result: TSODE achieved 87.9% time-in-range glucose levels and reduced time below 70 mg/dL to less than 10% in simulations, surpassing existing approaches.

Conclusion: The study demonstrates that TSODE successfully balances personalized glucose management with safety, offering a robust solution for adaptive insulin delivery.

Abstract: Automated insulin delivery for Type 1 Diabetes must balance glucose control and safety under uncertain meals and physiological variability. While reinforcement learning (RL) enables adaptive personalization, existing approaches struggle to simultaneously guarantee safety, leaving a gap in achieving both personalized and risk-aware glucose control, such as overdosing before meals or stacking corrections. To bridge this gap, we propose TSODE, a safety-aware controller that integrates Thompson Sampling RL with a Neural Ordinary Differential Equation (NeuralODE) forecaster to address this challenge. Specifically, the NeuralODE predicts short-term glucose trajectories conditioned on proposed insulin doses, while a conformal calibration layer quantifies predictive uncertainty to reject or scale risky actions. In the FDA-approved UVa/Padova simulator (adult cohort), TSODE achieved 87.9% time-in-range with less than 10% time below 70 mg/dL, outperforming relevant baselines. These results demonstrate that integrating adaptive RL with calibrated NeuralODE forecasting enables interpretable, safe, and robust glucose regulation.

</details>


### [684] [Tailored Primitive Initialization is the Secret Key to Reinforcement Learning](https://arxiv.org/abs/2511.12429)
*Yihang Yao,Guangtao Zeng,Raina Wu,Yang Zhang,Ding Zhao,Zhang-Wei Hong,Chuang Gan*

Main category: cs.LG

TL;DR: The paper introduces a pipeline named Tailor to address sampling inefficiency and initialization dependence in reinforcement learning for enhancing LLMs' reasoning abilities, showing improvements in performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges of low sampling efficiency and dependence on model initialization in reinforcement learning for LLMs when improving their reasoning abilities.

Method: The authors propose a finetuning pipeline called Tailor, which discovers and curates reasoning primitives to improve reasoning state coverage before reinforcement learning.

Result: Tailor generates more diverse and higher-quality initialization data, leading to improved downstream reinforcement learning outcomes on mathematical and logical reasoning benchmarks.

Conclusion: Tailor enhances the reasoning capabilities of LLMs by improving sample efficiency and addressing initialization issues, ultimately advancing RL-driven performance for reasoning tasks.

Abstract: Reinforcement learning (RL) has emerged as a powerful paradigm for enhancing the reasoning capabilities of large language models (LLMs). While RL has demonstrated substantial performance gains, it still faces key challenges, including low sampling efficiency and a strong dependence on model initialization: some models achieve rapid improvements with minimal RL steps, while others require significant training data to make progress. In this work, we investigate these challenges through the lens of reasoning token coverage and argue that initializing LLMs with diverse, high-quality reasoning primitives is essential for achieving stable and sample-efficient RL training. We propose Tailor, a finetuning pipeline that automatically discovers and curates novel reasoning primitives, thereby expanding the coverage of reasoning-state distributions before RL. Extensive experiments on mathematical and logical reasoning benchmarks demonstrate that Tailor generates more diverse and higher-quality warm-start data, resulting in higher downstream RL performance.

</details>


### [685] [VISAGNN: Versatile Staleness-Aware Efficient Training on Large-Scale Graphs](https://arxiv.org/abs/2511.12434)
*Rui Xue*

Main category: cs.LG

TL;DR: This paper introduces VISAGNN, a novel approach to tackle the staleness issue in historical embeddings for large-scale Graph Neural Network (GNN) training, enhancing performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Current GNN methods utilize historical embeddings for large-scale training but suffer from staleness-induced biases that degrade model performance.

Method: VISAGNN incorporates staleness-awareness into the training process by embedding staleness criteria into the message passing mechanism, loss function, and historical embeddings, dynamically mitigating the negative effects of stale embeddings.

Result: VISAGNN shows superior performance, faster convergence, and better efficiency compared to existing methods, as demonstrated through comprehensive experiments on large-scale benchmarks.

Conclusion: VISAGNN successfully addresses the staleness issue in historical embeddings, improving large-scale GNN training accuracy and efficiency.

Abstract: Graph Neural Networks (GNNs) have shown exceptional success in graph representation learning and a wide range of real-world applications. However, scaling deeper GNNs poses challenges due to the neighbor explosion problem when training on large-scale graphs. To mitigate this, a promising class of GNN training algorithms utilizes historical embeddings to reduce computation and memory costs while preserving the expressiveness of the model. These methods leverage historical embeddings for out-of-batch nodes, effectively approximating full-batch training without losing any neighbor information-a limitation found in traditional sampling methods. However, the staleness of these historical embeddings often introduces significant bias, acting as a bottleneck that can adversely affect model performance. In this paper, we propose a novel VersatIle Staleness-Aware GNN, named VISAGNN, which dynamically and adaptively incorporates staleness criteria into the large-scale GNN training process. By embedding staleness into the message passing mechanism, loss function, and historical embeddings during training, our approach enables the model to adaptively mitigate the negative effects of stale embeddings, thereby reducing estimation errors and enhancing downstream accuracy. Comprehensive experiments demonstrate the effectiveness of our method in overcoming the staleness issue of existing historical embedding techniques, showcasing its superior performance and efficiency on large-scale benchmarks, along with significantly faster convergence.

</details>


### [686] [Global-Lens Transformers: Adaptive Token Mixing for Dynamic Link Prediction](https://arxiv.org/abs/2511.12442)
*Tao Zou,Chengfeng Wu,Tianxi Liao,Junchen Ye,Bowen Du*

Main category: cs.LG

TL;DR: The paper introduces GLFormer, an attention-free Transformer framework for dynamic graph learning that achieves state-of-the-art performance efficiently.


<details>
  <summary>Details</summary>
Motivation: Dynamic graph learning is essential in modeling time-evolving relationships, but current Transformer-based models face scalability issues due to self-attention complexity.

Method: GLFormer replaces self-attention with an adaptive token mixer for local aggregation and employs a hierarchical module to capture long-term dependencies efficiently.

Result: GLFormer outperforms Transformer-based models in dynamic graph tasks on six benchmarks, with improved efficiency and scalability.

Conclusion: Attention-free architectures like GLFormer can rival or surpass Transformer-based designs in dynamic graph learning, offering both effectiveness and efficiency improvements.

Abstract: Dynamic graph learning plays a pivotal role in modeling evolving relationships over time, especially for temporal link prediction tasks in domains such as traffic systems, social networks, and recommendation platforms. While Transformer-based models have demonstrated strong performance by capturing long-range temporal dependencies, their reliance on self-attention results in quadratic complexity with respect to sequence length, limiting scalability on high-frequency or large-scale graphs. In this work, we revisit the necessity of self-attention in dynamic graph modeling. Inspired by recent findings that attribute the success of Transformers more to their architectural design than attention itself, we propose GLFormer, a novel attention-free Transformer-style framework for dynamic graphs. GLFormer introduces an adaptive token mixer that performs context-aware local aggregation based on interaction order and time intervals. To capture long-term dependencies, we further design a hierarchical aggregation module that expands the temporal receptive field by stacking local token mixers across layers. Experiments on six widely-used dynamic graph benchmarks show that GLFormer achieves SOTA performance, which reveals that attention-free architectures can match or surpass Transformer baselines in dynamic graph settings with significantly improved efficiency.

</details>


### [687] [Personality-guided Public-Private Domain Disentangled Hypergraph-Former Network for Multimodal Depression Detection](https://arxiv.org/abs/2511.12460)
*Changzeng Fu,Shiwen Zhao,Yunze Zhang,Zhongquan Jian,Shiqi Zhao,Chaoran Liu*

Main category: cs.LG

TL;DR: P$^3$HF, a novel method combining personality-guided learning, Hypergraph-Former architecture, and domain disentanglement, improves depression detection accuracy by ~10% on MPDD-Young dataset.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the global challenge of depression detection, aiming to overcome limitations in current multimodal methods related to individual differences and diverse behavioral contexts.

Method: It proposes combining pre-trained LLMs for personality-guided representation, a Hypergraph-Former architecture for capturing cross-modal temporal relationships, and contrastive learning for domain disentanglement in various contexts.

Result: P$^3$HF achieves ~10% improvement in accuracy and weighted F1 scores for depression classification compared to existing methods, confirmed through ablation studies.

Conclusion: The study establishes the importance of integrating personality-guided learning and hypergraph reasoning for robust depression detection, advancing multimodal approaches.

Abstract: Depression represents a global mental health challenge requiring efficient and reliable automated detection methods. Current Transformer- or Graph Neural Networks (GNNs)-based multimodal depression detection methods face significant challenges in modeling individual differences and cross-modal temporal dependencies across diverse behavioral contexts. Therefore, we propose P$^3$HF (Personality-guided Public-Private Domain Disentangled Hypergraph-Former Network) with three key innovations: (1) personality-guided representation learning using LLMs to transform discrete individual features into contextual descriptions for personalized encoding; (2) Hypergraph-Former architecture modeling high-order cross-modal temporal relationships; (3) event-level domain disentanglement with contrastive learning for improved generalization across behavioral contexts. Experiments on MPDD-Young dataset show P$^3$HF achieves around 10\% improvement on accuracy and weighted F1 for binary and ternary depression classification task over existing methods. Extensive ablation studies validate the independent contribution of each architectural component, confirming that personality-guided representation learning and high-order hypergraph reasoning are both essential for generating robust, individual-aware depression-related representations. The code is released at https://github.com/hacilab/P3HF.

</details>


### [688] [Redundancy-optimized Multi-head Attention Networks for Multi-View Multi-Label Feature Selection](https://arxiv.org/abs/2511.12462)
*Yuzhou Liu,Jiarui Liu,Wanfu Gao*

Main category: cs.LG

TL;DR: This paper introduces the RMAN-MMFS method, an attention-based approach for feature selection addressing challenges in multi-view, multi-label data.


<details>
  <summary>Details</summary>
Motivation: The paper aims to better analyze complex interrelations of features, views, and labels in multi-view multi-label data while addressing issues of neglecting cross-view complementarity and feature redundancy.

Method: The RMAN-MMFS method uses multi-head attention networks, combining intra-view analysis using individual heads with cross-attention mechanisms to capture inter-view feature complementarity. Additionally, it incorporates static and dynamic redundancy terms to optimize feature compactness.

Result: Experiments on six real-world datasets show that RMAN-MMFS outperforms six existing methods in multi-view multi-label feature selection.

Conclusion: RMAN-MMFS demonstrates effectiveness in improving feature selection by addressing inter-view feature complementarity and redundancy, achieving robust results.

Abstract: Multi-view multi-label data offers richer perspectives for artificial intelligence, but simultaneously presents significant challenges for feature selection due to the inherent complexity of interrelations among features, views and labels. Attention mechanisms provide an effective way for analyzing these intricate relationships. They can compute importance weights for information by aggregating correlations between Query and Key matrices to focus on pertinent values. However, existing attention-based feature selection methods predominantly focus on intra-view relationships, neglecting the complementarity of inter-view features and the critical feature-label correlations. Moreover, they often fail to account for feature redundancy, potentially leading to suboptimal feature subsets. To overcome these limitations, we propose a novel method based on Redundancy-optimized Multi-head Attention Networks for Multi-view Multi-label Feature Selection (RMAN-MMFS). Specifically, we employ each individual attention head to model intra-view feature relationships and use the cross-attention mechanisms between different heads to capture inter-view feature complementarity. Furthermore, we design static and dynamic feature redundancy terms: the static term mitigates redundancy within each view, while the dynamic term explicitly models redundancy between unselected and selected features across the entire selection process, thereby promoting feature compactness. Comprehensive evaluations on six real-world datasets, compared against six multi-view multi-label feature selection methods, demonstrate the superior performance of the proposed method.

</details>


### [689] [Logarithmic Regret and Polynomial Scaling in Online Multi-step-ahead Prediction](https://arxiv.org/abs/2511.12467)
*Jiachen Qian,Yang Zheng*

Main category: cs.LG

TL;DR: The paper addresses online multi-step-ahead prediction for linear stochastic systems, suggesting an optimal prediction policy and proposing an algorithm with strong performance guarantees.


<details>
  <summary>Details</summary>
Motivation: The research aims to improve prediction techniques for linear stochastic systems by deriving optimal policies and understanding their performance under multi-step scenarios.

Method: The authors derived optimal parameterization using conditional distribution theory, proposed an online least-squares algorithm, and analyzed its regret compared to the optimal Kalman filter.

Result: The algorithm achieves logarithmic regret relative to the optimal Kalman filter and features a bound that grows polynomially concerning the prediction horizon.

Conclusion: This work evidences that the suggested approach is effective, with regret scaling logarithmically in steps but affected by the system’s eigenvalue properties.

Abstract: This letter studies the problem of online multi-step-ahead prediction for unknown linear stochastic systems. Using conditional distribution theory, we derive an optimal parameterization of the prediction policy as a linear function of future inputs, past inputs, and past outputs. Based on this characterization, we propose an online least-squares algorithm to learn the policy and analyze its regret relative to the optimal model-based predictor. We show that the online algorithm achieves logarithmic regret with respect to the optimal Kalman filter in the multi-step setting. Furthermore, with new proof techniques, we establish an almost-sure regret bound that does not rely on fixed failure probabilities for sufficiently large horizons $N$. Finally, our analysis also reveals that, while the regret remains logarithmic in $N$, its constant factor grows polynomially with the prediction horizon $H$, with the polynomial order set by the largest Jordan block of eigenvalue 1 in the system matrix.

</details>


### [690] [Diffusion Model Based Signal Recovery Under 1-Bit Quantization](https://arxiv.org/abs/2511.12471)
*Youming Chen,Zhaoqiang Liu*

Main category: cs.LG

TL;DR: Diff-OneBit utilizes diffusion models (DMs) to tackle 1-bit quantization challenges by employing a surrogate likelihood function for signal recovery, demonstrating superior performance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address difficulties in applying diffusion models (DMs) to 1-bit quantization tasks, which involve non-linear link functions that are non-differentiable or lack explicit characterization.

Method: Diff-OneBit incorporates a differentiable surrogate likelihood function to manage 1-bit quantization, allowing gradient-based iterations within a plug-and-play framework, decoupling the data-fidelity term from the diffusion prior and enabling pretrained DMs to act as denoisers.

Result: Extensive experiments on FFHQ, CelebA, and ImageNet datasets show that Diff-OneBit delivers high-quality image reconstructions and outperforms state-of-the-art methods in both reconstruction accuracy and computational efficiency.

Conclusion: Diff-OneBit effectively addresses the challenges of 1-bit quantization tasks with a novel DM-based approach, achieving superior image fidelity and efficiency in reconstruction tasks.

Abstract: Diffusion models (DMs) have demonstrated to be powerful priors for signal recovery, but their application to 1-bit quantization tasks, such as 1-bit compressed sensing and logistic regression, remains a challenge. This difficulty stems from the inherent non-linear link function in these tasks, which is either non-differentiable or lacks an explicit characterization. To tackle this issue, we introduce Diff-OneBit, which is a fast and effective DM-based approach for signal recovery under 1-bit quantization. Diff-OneBit addresses the challenge posed by non-differentiable or implicit links functions via leveraging a differentiable surrogate likelihood function to model 1-bit quantization, thereby enabling gradient based iterations. This function is integrated into a flexible plug-and-play framework that decouples the data-fidelity term from the diffusion prior, allowing any pretrained DM to act as a denoiser within the iterative reconstruction process. Extensive experiments on the FFHQ, CelebA and ImageNet datasets demonstrate that Diff-OneBit gives high-fidelity reconstructed images, outperforming state-of-the-art methods in both reconstruction quality and computational efficiency across 1-bit compressed sensing and logistic regression tasks.

</details>


### [691] [SculptDrug : A Spatial Condition-Aware Bayesian Flow Model for Structure-based Drug Design](https://arxiv.org/abs/2511.12489)
*Qingsong Zhong,Haomin Yu,Yan Lin,Wangmeng Shen,Long Zeng,Jilin Hu*

Main category: cs.LG

TL;DR: A novel generative model called SculptDrug addresses limitations in Structure-Based Drug Design by incorporating spatial condition awareness to refine spatial alignment, enforce protein surface constraints, and preserve molecular interactions.


<details>
  <summary>Details</summary>
Motivation: Existing generative models in SBDD struggle with spatial modeling fidelity, boundary conditions, and hierarchical structural constraints.

Method: SculptDrug adopts a Bayesian flow network framework, integrates a Boundary Awareness Block, and features a Hierarchical Encoder.

Result: Experimental evaluations show SculptDrug surpasses state-of-the-art baselines on the CrossDocked dataset.

Conclusion: SculptDrug effectively improves spatial condition-aware modeling for drug ligand generation, enhancing drug design precision and compatibility.

Abstract: Structure-Based drug design (SBDD) has emerged as a popular approach in drug discovery, leveraging three-dimensional protein structures to generate drug ligands. However, existing generative models encounter several key challenges: (1) incorporating boundary condition constraints, (2) integrating hierarchical structural conditions, and (3) ensuring spatial modeling fidelity. To address these limitations, we propose SculptDrug, a spatial condition-aware generative model based on Bayesian flow networks (BFNs). First, SculptDrug follows a BFN-based framework and employs a progressive denoising strategy to ensure spatial modeling fidelity, iteratively refining atom positions while enhancing local interactions for precise spatial alignment. Second, we introduce a Boundary Awareness Block that incorporates protein surface constraints into the generative process to ensure that generated ligands are geometrically compatible with the target protein. Third, we design a Hierarchical Encoder that captures global structural context while preserving fine-grained molecular interactions, ensuring overall consistency and accurate ligand-protein conformations. We evaluate SculptDrug on the CrossDocked dataset, and experimental results demonstrate that SculptDrug outperforms state-of-the-art baselines, highlighting the effectiveness of spatial condition-aware modeling.

</details>


### [692] [Uncover and Unlearn Nuisances: Agnostic Fully Test-Time Adaptation](https://arxiv.org/abs/2511.12491)
*Ponhvoan Srey,Yaxin Shi,Hangwei Qian,Jing Li,Ivor W. Tsang*

Main category: cs.LG

TL;DR: The paper introduces Agnostic Fully Test-Time Adaptation (AFTTA), a method to address domain shifts without source data access. It uses simulated nuisances and unlearning techniques to improve model generalization.


<details>
  <summary>Details</summary>
Motivation: The paper aims to tackle the challenge of domain shifts in machine learning when source data or model training protocols are unavailable, a problem not addressed by traditional methods.

Method: The authors propose the uncover-and-unlearn approach, simulating nuisance shifts and regularizing latent representations and label predictions using a mutual information-based criterion.

Result: The approach achieves superior performance compared to other methods in handling domain shifts across tasks involving corruption and style variations.

Conclusion: AFTTA effectively generalizes models to unpredictable target domains under FTTA constraints, as shown in comprehensive experiments.

Abstract: Fully Test-Time Adaptation (FTTA) addresses domain shifts without access to source data and training protocols of the pre-trained models. Traditional strategies that align source and target feature distributions are infeasible in FTTA due to the absence of training data and unpredictable target domains. In this work, we exploit a dual perspective on FTTA, and propose Agnostic FTTA (AFTTA) as a novel formulation that enables the usage of off-the-shelf domain transformations during test-time to enable direct generalization to unforeseeable target data. To address this, we develop an uncover-and-unlearn approach. First, we uncover potential unwanted shifts between source and target domains by simulating them through predefined mappings and consider them as nuisances. Then, during test-time prediction, the model is enforced to unlearn these nuisances by regularizing the consequent shifts in latent representations and label predictions. Specifically, a mutual information-based criterion is devised and applied to guide nuisances unlearning in the feature space and encourage confident and consistent prediction in label space. Our proposed approach explicitly addresses agnostic domain shifts, enabling superior model generalization under FTTA constraints. Extensive experiments on various tasks, involving corruption and style shifts, demonstrate that our method consistently outperforms existing approaches.

</details>


### [693] [Towards Better IncomLDL: We Are Unaware of Hidden Labels in Advance](https://arxiv.org/abs/2511.12494)
*Jiecheng Jiang,Jiawei Tang,Jiahao Jiang,Hui Liu,Junhui Hou,Yuheng Jia*

Main category: cs.LG

TL;DR: The paper introduces LDL with hidden labels (HidLDL) to address the unrealistic assumption in incomplete label distribution learning (IncomLDL) methods and proposes a novel approach for recovering complete label distributions.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the unrealistic assumption in IncomLDL methods where missing label descriptions are set to 0, and propose a more realistic paradigm for recovering label distributions where certain labels are hidden.

Method: The authors propose a novel constraint to capture proportional information of observed labels and leverage both local feature similarity and global low-rank structures to recover hidden labels. Additionally, they provide theoretical recovery bounds for their method.

Result: Extensive experiments on various datasets show that the proposed approach outperforms state-of-the-art LDL and IncomLDL methods in recovery and predictive tasks.

Conclusion: The proposed method successfully tackles the HidLDL problem and demonstrates superior performance in recovering complete label distributions from incomplete ones, proving its feasibility and effectiveness.

Abstract: Label distribution learning (LDL) is a novel paradigm that describe the samples by label distribution of a sample. However, acquiring LDL dataset is costly and time-consuming, which leads to the birth of incomplete label distribution learning (IncomLDL). All the previous IncomLDL methods set the description degrees of "missing" labels in an instance to 0, but remains those of other labels unchanged. This setting is unrealistic because when certain labels are missing, the degrees of the remaining labels will increase accordingly. We fix this unrealistic setting in IncomLDL and raise a new problem: LDL with hidden labels (HidLDL), which aims to recover a complete label distribution from a real-world incomplete label distribution where certain labels in an instance are omitted during annotation. To solve this challenging problem, we discover the significance of proportional information of the observed labels and capture it by an innovative constraint to utilize it during the optimization process. We simultaneously use local feature similarity and the global low-rank structure to reveal the mysterious veil of hidden labels. Moreover, we theoretically give the recovery bound of our method, proving the feasibility of our method in learning from hidden labels. Extensive recovery and predictive experiments on various datasets prove the superiority of our method to state-of-the-art LDL and IncomLDL methods.

</details>


### [694] [BSO: Binary Spiking Online Optimization Algorithm](https://arxiv.org/abs/2511.12502)
*Yu Liang,Yu Yang,Wenjie Wei,Ammar Belatreche,Shuai Wang,Malu Zhang,Yang Yang*

Main category: cs.LG

TL;DR: The paper proposes Binary Spiking Online (BSO) training, a memory-efficient algorithm for Binary Spiking Neural Networks (BSNNs), and its temporal-aware variant T-BSO, showing improved performance and convergence guarantees.


<details>
  <summary>Details</summary>
Motivation: To address the memory overhead and inefficiencies in training algorithms for Binary Spiking Neural Networks (BSNNs).

Method: Introduced Binary Spiking Online (BSO) optimization algorithm, a memory-saving online training method that eliminates the use of latent weights and leverages flip signals for weight updates. Also introduced T-BSO to enhance temporal-aware capabilities for adaptive thresholding.

Result: Both BSO and T-BSO demonstrated superior optimization performance compared to existing BSNN training methods and theoretically established convergence guarantees with regret bounds.

Conclusion: BSO and T-BSO offer memory-efficient and effective optimization solutions for BSNNs, making them attractive for resource-constrained environments.

Abstract: Binary Spiking Neural Networks (BSNNs) offer promising efficiency advantages for resource-constrained computing. However, their training algorithms often require substantial memory overhead due to latent weights storage and temporal processing requirements. To address this issue, we propose Binary Spiking Online (BSO) optimization algorithm, a novel online training algorithm that significantly reduces training memory. BSO directly updates weights through flip signals under the online training framework. These signals are triggered when the product of gradient momentum and weights exceeds a threshold, eliminating the need for latent weights during training. To enhance performance, we propose T-BSO, a temporal-aware variant that leverages the inherent temporal dynamics of BSNNs by capturing gradient information across time steps for adaptive threshold adjustment. Theoretical analysis establishes convergence guarantees for both BSO and T-BSO, with formal regret bounds characterizing their convergence rates. Extensive experiments demonstrate that both BSO and T-BSO achieve superior optimization performance compared to existing training methods for BSNNs. The codes are available at https://github.com/hamings1/BSO.

</details>


### [695] [Hierarchical Frequency-Decomposition Graph Neural Networks for Road Network Representation Learning](https://arxiv.org/abs/2511.12507)
*Jingtian Ma,Jingyuan Wang,Leong Hou U*

Main category: cs.LG

TL;DR: HiFiNet is a novel graph neural network designed for road networks, integrating spatial and spectral modeling to handle both global trends and localized variations effectively.


<details>
  <summary>Details</summary>
Motivation: Existing graph neural networks either overly smooth representations or fail to capture localized variations, limiting their capacity to model road networks with both global trends and local traffic fluctuations.

Method: HiFiNet employs a hierarchical frequency decomposition using virtual nodes, with a decomposition-updating-reconstruction framework supported by a topology-aware graph transformer to fuse low- and high-frequency signals.

Result: HiFiNet achieves superior performance and generalization in road network representation tasks, validated through experiments on real-world datasets across multiple downstream tasks.

Conclusion: HiFiNet successfully bridges spatial and spectral modeling gaps in road networks, providing robust and comprehensive representations for intelligent transportation applications.

Abstract: Road networks are critical infrastructures underpinning intelligent transportation systems and their related applications. Effective representation learning of road networks remains challenging due to the complex interplay between spatial structures and frequency characteristics in traffic patterns. Existing graph neural networks for modeling road networks predominantly fall into two paradigms: spatial-based methods that capture local topology but tend to over-smooth representations, and spectral-based methods that analyze global frequency components but often overlook localized variations. This spatial-spectral misalignment limits their modeling capacity for road networks exhibiting both coarse global trends and fine-grained local fluctuations. To bridge this gap, we propose HiFiNet, a novel hierarchical frequency-decomposition graph neural network that unifies spatial and spectral modeling. HiFiNet constructs a multi-level hierarchy of virtual nodes to enable localized frequency analysis, and employs a decomposition-updating-reconstruction framework with a topology-aware graph transformer to separately model and fuse low- and high-frequency signals. Theoretically justified and empirically validated on multiple real-world datasets across four downstream tasks, HiFiNet demonstrates superior performance and generalization ability in capturing effective road network representations.

</details>


### [696] [Spectral Bias Mitigation via xLSTM-PINN: Memory-Gated Representation Refinement for Physics-Informed Learning](https://arxiv.org/abs/2511.12512)
*Ze Tao,Darui Zhao,Fujun Liu,Ke Xu,Xiangsheng Hu*

Main category: cs.LG

TL;DR: The paper introduces xLSTM-PINN to address challenges like spectral bias, residual-data imbalance, and weak extrapolation in PDE-informed learning by integrating multiscale feature extraction and adaptive weighting, leading to superior performance in benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address challenges in physics-informed learning for PDEs, specifically spectral bias, residual-data imbalance, and weak extrapolation capabilities of current methods.

Method: The paper proposes xLSTM-PINN, which combines gated cross-scale memory, a staged frequency curriculum, and adaptive residual-data weighting for better feature extraction and spectral remodeling.

Result: The method achieves lower spectral and RMSE errors, enhanced resolvable frequency range, reduced high-frequency ripples, and generally improved performance over baseline PINNs in four benchmarks.

Conclusion: The xLSTM-PINN method effectively suppresses spectral bias, extends frequency learning capabilities, and enhances model accuracy, reproducibility, and transferability without altering automatic differentiation or physics losses.

Abstract: Physics-informed learning for PDEs is surging across scientific computing and industrial simulation, yet prevailing methods face spectral bias, residual-data imbalance, and weak extrapolation. We introduce a representation-level spectral remodeling xLSTM-PINN that combines gated-memory multiscale feature extraction with adaptive residual-data weighting to curb spectral bias and strengthen extrapolation. Across four benchmarks, we integrate gated cross-scale memory, a staged frequency curriculum, and adaptive residual reweighting, and verify with analytic references and extrapolation tests, achieving markedly lower spectral error and RMSE and a broader stable learning-rate window. Frequency-domain benchmarks show raised high-frequency kernel weights and a right-shifted resolvable bandwidth, shorter high-k error decay and time-to-threshold, and narrower error bands with lower MSE, RMSE, MAE, and MaxAE. Compared with the baseline PINN, we reduce MSE, RMSE, MAE, and MaxAE across all four benchmarks and deliver cleaner boundary transitions with attenuated high-frequency ripples in both frequency and field maps. This work suppresses spectral bias, widens the resolvable band and shortens the high-k time-to-threshold under the same budget, and without altering AD or physics losses improves accuracy, reproducibility, and transferability.

</details>


### [697] [Regret Guarantees for Linear Contextual Stochastic Shortest Path](https://arxiv.org/abs/2511.12534)
*Dor Polikar,Alon Cohen*

Main category: cs.LG

TL;DR: The paper tackles the problem of Contextual Stochastic Shortest Path (CSSP) in an unknown linear setting and introduces the LR-CSSP algorithm with established regret bounds.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenge of efficiently solving CSSP problems in unknown environments where context determines MDPs through an unexplored linear function.

Method: The authors propose LR-CSSP, an algorithm with established regret bounds, leveraging properties of the linear context mapping and ensuring termination of episodes within a reasonable time.

Result: LR-CSSP achieves regret bounds and demonstrates capability in handling continuous contexts while mitigating risks of prolonged or endless episodes.

Conclusion: The paper concludes that LR-CSSP is an effective solution for CSSP problems, minimizing regret and ensuring termination even in adversarial settings.

Abstract: We define the problem of linear Contextual Stochastic Shortest Path (CSSP), where at the beginning of each episode, the learner observes an adversarially chosen context that determines the MDP through a fixed but unknown linear function. The learner's objective is to reach a designated goal state with minimal expected cumulative loss, despite having no prior knowledge of the transition dynamics, loss functions, or the mapping from context to MDP. In this work, we propose LR-CSSP, an algorithm that achieves a regret bound of $\widetilde{O}(K^{2/3} d^{2/3} |S| |A|^{1/3} B_\star^2 T_\star \log (1/ δ))$, where $K$ is the number of episodes, $d$ is the context dimension, $S$ and $A$ are the sets of states and actions respectively, $B_\star$ bounds the optimal cumulative loss and $T_\star$, unknown to the learner, bounds the expected time for the optimal policy to reach the goal. In the case where all costs exceed $\ell_{\min}$, LR-CSSP attains a regret of $\widetilde O(\sqrt{K \cdot d^2 |S|^3 |A| B_\star^3 \log(1/δ)/\ell_{\min}})$. Unlike in contextual finite-horizon MDPs, where limited knowledge primarily leads to higher losses and regret, in the CSSP setting, insufficient knowledge can also prolong episodes and may even lead to non-terminating episodes. Our analysis reveals that LR-CSSP effectively handles continuous context spaces, while ensuring all episodes terminate within a reasonable number of time steps.

</details>


### [698] [CAO: Curvature-Adaptive Optimization via Periodic Low-Rank Hessian Sketching](https://arxiv.org/abs/2511.12548)
*Wenzhang Du*

Main category: cs.LG

TL;DR: The paper introduces a curvature-adaptive optimization method improving efficiency in sharp, anisotropic regions by combining low-rank Hessian subspace preconditioning with first-order methods. It demonstrates faster convergence on benchmarks like CIFAR-10/100.


<details>
  <summary>Details</summary>
Motivation: First-order optimization methods, though reliable, are inefficient in sharp, anisotropic regions. There is a need for methods that adapt to curvature, enhancing speed and convergence without compromising reliability.

Method: The proposed method combines periodic sketching of low-rank Hessian subspaces using Hessian-vector products, preconditioning gradients within the subspace, while leaving the orthogonal complement handled by a first-order approach.

Result: The method achieves a faster convergence rate, entering low-loss regions earlier and reaching predefined train loss thresholds 2.95x faster than Adam on CIFAR-100/ResNet-18, while maintaining competitive final test accuracy.

Conclusion: The curvature-adaptive method effectively enhances convergence for non-convex objectives, demonstrating robustness, simplicity, and insensitivity to the sketch rank (k). It provides a principled mechanism for curvature adaptation and releases reproducibility resources.

Abstract: First-order optimizers are reliable but slow in sharp, anisotropic regions. We study a curvature-adaptive method that periodically sketches a low-rank Hessian subspace via Hessian--vector products and preconditions gradients only in that subspace, leaving the orthogonal complement first-order. For L-smooth non-convex objectives, we recover the standard O(1/T) stationarity guarantee with a widened stable stepsize range; under a Polyak--Lojasiewicz (PL) condition with bounded residual curvature outside the sketch, the loss contracts at refresh steps. On CIFAR-10/100 with ResNet-18/34, the method enters the low-loss region substantially earlier: measured by epochs to a pre-declared train-loss threshold (0.75), it reaches the threshold 2.95x faster than Adam on CIFAR-100/ResNet-18, while matching final test accuracy. The approach is one-knob: performance is insensitive to the sketch rank k across {1,3,5}, and k=0 yields a principled curvature-free ablation. We release anonymized logs and scripts that regenerate all figures and tables.

</details>


### [699] [Training Instabilities Induce Flatness Bias in Gradient Descent](https://arxiv.org/abs/2511.12558)
*Lawrence Wang,Stephen J. Roberts*

Main category: cs.LG

TL;DR: This paper analyzes how training instabilities in gradient descent (GD), often avoided in classical approaches, can actually enhance generalization by promoting exploration and leading to flatter minima in the loss landscape.


<details>
  <summary>Details</summary>
Motivation: Classical stability thresholds determine stable regimes for gradient descent, but modern deep networks achieve better performance outside these stable regimes, prompting investigation into the role of training instabilities in deep learning.

Method: The authors study Rotational Polarity of Eigenvectors (RPE), showing how rotations of loss Hessian eigenvectors during instabilities drive parameters to flatter regions. They extend this analysis to stochastic GD and experiments with Adam.

Result: Training instabilities create an implicit bias in GD that promotes exploration and generalization by inducing the model toward flatter loss minima. In stochastic GD, the flattening effect outweighs minibatch noise, and restoring instabilities in Adam further improves generalization.

Conclusion: Instabilities in training, contrary to classical views, play a constructive role in improving generalization in deep learning by enabling exploration and leading to flatter minima. This framework redefines how instabilities are viewed in optimization strategies.

Abstract: Classical analyses of gradient descent (GD) define a stability threshold based on the largest eigenvalue of the loss Hessian, often termed sharpness. When the learning rate lies below this threshold, training is stable and the loss decreases monotonically. Yet, modern deep networks often achieve their best performance beyond this regime.
  We demonstrate that such instabilities induce an implicit bias in GD, driving parameters toward flatter regions of the loss landscape and thereby improving generalization. The key mechanism is the Rotational Polarity of Eigenvectors (RPE), a geometric phenomenon in which the leading eigenvectors of the Hessian rotate during training instabilities. These rotations, which increase with learning rates, promote exploration and provably lead to flatter minima.
  This theoretical framework extends to stochastic GD, where instability-driven flattening persists and its empirical effects outweigh minibatch noise. Finally, we show that restoring instabilities in Adam further improves generalization.
  Together, these results establish and understand the constructive role of training instabilities in deep learning.

</details>


### [700] [Linear time small coresets for k-mean clustering of segments with applications](https://arxiv.org/abs/2511.12564)
*David Denisov,Shlomi Dolev,Dan Felmdan,Michael Segal*

Main category: cs.LG

TL;DR: This paper addresses the $k$-means clustering problem by proposing a new coreset construction method for input segments, achieving efficient computation and scalability.


<details>
  <summary>Details</summary>
Motivation: To improve computational efficiency and scalability in the $k$-means clustering problem, particularly for input segments in high-dimensional data such as video tracking scenarios.

Method: The authors introduce a coreset construction approach that approximates cluster distances while handling arbitrary segments. The coreset size is $O(\log^2 n)$ and is computable in $O(nd)$ time.

Result: The method effectively produces coresets enabling efficient distributed and parallel computation for clustering tasks. Experiments show significant speedups with retained clustering accuracy, even in real-time applications.

Conclusion: The proposed coreset approach offers practical efficiency and theoretical assurances, proving advantageous for solving $k$-means with input segments in real-world applications.

Abstract: We study the $k$-means problem for a set $\mathcal{S} \subseteq \mathbb{R}^d$ of $n$ segments, aiming to find $k$ centers $X \subseteq \mathbb{R}^d$ that minimize
  $D(\mathcal{S},X) := \sum_{S \in \mathcal{S}} \min_{x \in X} D(S,x)$, where $D(S,x) := \int_{p \in S} |p - x| dp$
  measures the total distance from each point along a segment to a center. Variants of this problem include handling outliers, employing alternative distance functions such as M-estimators, weighting distances to achieve balanced clustering, or enforcing unique cluster assignments. For any $\varepsilon > 0$, an $\varepsilon$-coreset is a weighted subset $C \subseteq \mathbb{R}^d$ that approximates $D(\mathcal{S},X)$ within a factor of $1 \pm \varepsilon$ for any set of $k$ centers, enabling efficient streaming, distributed, or parallel computation. We propose the first coreset construction that provably handles arbitrary input segments. For constant $k$ and $\varepsilon$, it produces a coreset of size $O(\log^2 n)$ computable in $O(nd)$ time. Experiments, including a real-time video tracking application, demonstrate substantial speedups with minimal loss in clustering accuracy, confirming both the practical efficiency and theoretical guarantees of our method.

</details>


### [701] [Enhancing Machine Learning Model Efficiency through Quantization and Bit Depth Optimization: A Performance Analysis on Healthcare Data](https://arxiv.org/abs/2511.12568)
*Mitul Goswami,Romit Chatterjee*

Main category: cs.LG

TL;DR: The paper explores optimizing learning models by reducing data precision with quantization and bit-depth techniques, achieving faster computations with negligible accuracy loss.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of extended execution times in complex learning models, especially in medical applications.

Method: Quantization and bit-depth optimization applied to Logistic Regression models using medical datasets, downscaling data precision from float64 to float32 and int32.

Result: Significant reduction in execution time with slight accuracy decline, demonstrating effective optimization methods.

Conclusion: Optimization efficacy depends on parameter variations, and the study highlights the trade-off between speed and accuracy in model performance.

Abstract: This research aims to optimize intricate learning models by implementing quantization and bit-depth optimization techniques. The objective is to significantly cut time complexity while preserving model efficiency, thus addressing the challenge of extended execution times in intricate models. Two medical datasets were utilized as case studies to apply a Logistic Regression (LR) machine learning model. Using efficient quantization and bit depth optimization strategies the input data is downscaled from float64 to float32 and int32. The results demonstrated a significant reduction in time complexity, with only a minimal decrease in model accuracy post-optimization, showcasing the state-of-the-art optimization approach. This comprehensive study concludes that the impact of these optimization techniques varies depending on a set of parameters.

</details>


### [702] [LMM-IR: Large-Scale Netlist-Aware Multimodal Framework for Static IR-Drop Prediction](https://arxiv.org/abs/2511.12581)
*Kai Ma,Zhen Wang,Hongquan He,Qi Xu,Tinghuan Chen,Hao Geng*

Main category: cs.LG

TL;DR: The paper presents a multimodal approach for fast and accurate static IR drop prediction in chip design, leveraging a Large-scale Netlist Transformer (LNT) and 3D point cloud representations for efficient processing.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the time-consuming nature of static IR drop analysis and iterative analysis required for solving IR drop violations, aiming to accelerate the chip design process.

Method: The authors propose a novel multimodal approach employing a Large-scale Netlist Transformer (LNT) to process SPICE files and represent netlist topology as 3D point clouds. Data from multiple modalities are encoded into latent space for efficient static voltage drop predictions.

Result: The proposed algorithm achieved the best F1 score and lowest MAE among competitors in the ICCAD 2023 contest and state-of-the-art benchmarks.

Conclusion: The method demonstrates superior performance in IR drop prediction while processing large-scale netlists efficiently, contributing to faster chip design cycles.

Abstract: Static IR drop analysis is a fundamental and critical task in the field of chip design. Nevertheless, this process can be quite time-consuming, potentially requiring several hours. Moreover, addressing IR drop violations frequently demands iterative analysis, thereby causing the computational burden. Therefore, fast and accurate IR drop prediction is vital for reducing the overall time invested in chip design. In this paper, we firstly propose a novel multimodal approach that efficiently processes SPICE files through large-scale netlist transformer (LNT). Our key innovation is representing and processing netlist topology as 3D point cloud representations, enabling efficient handling of netlist with up to hundreds of thousands to millions nodes. All types of data, including netlist files and image data, are encoded into latent space as features and fed into the model for static voltage drop prediction. This enables the integration of data from multiple modalities for complementary predictions. Experimental results demonstrate that our proposed algorithm can achieve the best F1 score and the lowest MAE among the winning teams of the ICCAD 2023 contest and the state-of-the-art algorithms.

</details>


### [703] [Symmetry-Aware Graph Metanetwork Autoencoders: Model Merging through Parameter Canonicalization](https://arxiv.org/abs/2511.12601)
*Odysseas Boufalis,Jorge Carrasco-Pollo,Joshua Rosenthal,Eduardo Terres-Caballero,Alejandro García-Castellanos*

Main category: cs.LG

TL;DR: The paper introduces Scale Graph Metanetworks (ScaleGMNs) that account for both permutation and scaling symmetries in neural networks to align networks without solving combinatorial assignment problems explicitly.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in aligning neural network parameterizations exhibiting permutation and scaling symmetries by creating architectures that leverage these properties, facilitating smooth model interpolation.

Method: An autoencoder framework is designed using Scale Graph Metanetworks (ScaleGMNs) as invariant encoders to align neural networks under both permutation and scaling symmetries without explicit assignment problem computation.

Result: The method successfully aligns Implicit Neural Representations (INRs) and Convolutional Neural Networks (CNNs) in shared loss basins, enabling model merging and smooth interpolation while bypassing regions of high loss.

Conclusion: ScaleGMNs efficiently utilize inherent parameterization symmetries in neural networks for alignment and merging, demonstrating practical and computational benefits. Code is made available on GitHub.

Abstract: Neural network parameterizations exhibit inherent symmetries that yield multiple equivalent minima within the loss landscape. Scale Graph Metanetworks (ScaleGMNs) explicitly leverage these symmetries by proposing an architecture equivariant to both permutation and parameter scaling transformations. Previous work by Ainsworth et al. (2023) addressed permutation symmetries through a computationally intensive combinatorial assignment problem, demonstrating that leveraging permutation symmetries alone can map networks into a shared loss basin. In this work, we extend their approach by also incorporating scaling symmetries, presenting an autoencoder framework utilizing ScaleGMNs as invariant encoders. Experimental results demonstrate that our method aligns Implicit Neural Representations (INRs) and Convolutional Neural Networks (CNNs) under both permutation and scaling symmetries without explicitly solving the assignment problem. This approach ensures that similar networks naturally converge within the same basin, facilitating model merging, i.e., smooth linear interpolation while avoiding regions of high loss. The code is publicly available on our GitHub repository.

</details>


### [704] [PID-controlled Langevin Dynamics for Faster Sampling of Generative Models](https://arxiv.org/abs/2511.12603)
*Hongyi Chen,Jianhai Shu,Jingtao Ding,Yong Li,Xiao-Ping Zhang*

Main category: cs.LG

TL;DR: This paper introduces PID-controlled Langevin Dynamics (PIDLD), a faster sampling method using control-theoretic principles to improve efficiency and reduce iteration steps in Langevin dynamics.


<details>
  <summary>Details</summary>
Motivation: Traditional Langevin dynamics suffers from slow sampling due to numerous iterations required to converge. There is a need for a method that accelerates this process while ensuring high-quality samples.

Method: The proposed method, PIDLD, applies control-theoretic concepts by treating energy gradients as feedback signals. It incorporates historical gradients (integral term) and gradient trends (derivative term) to efficiently navigate energy landscapes. This adaptive stabilization reduces iteration steps without additional training or datasets.

Result: PIDLD significantly improves generation quality with fewer iterations across various tasks. Extensive experiments demonstrate its practicality and efficiency gains over traditional Langevin dynamics.

Conclusion: PIDLD makes Langevin-based generative models more efficient and practical, especially for applications where iteration speed matters. It is compatible with existing methods and requires no extra resources to implement.

Abstract: Langevin dynamics sampling suffers from extremely low generation speed, fundamentally limited by numerous fine-grained iterations to converge to the target distribution. We introduce PID-controlled Langevin Dynamics (PIDLD), a novel sampling acceleration algorithm that reinterprets the sampling process using control-theoretic principles. By treating energy gradients as feedback signals, PIDLD combines historical gradients (the integral term) and gradient trends (the derivative term) to efficiently traverse energy landscapes and adaptively stabilize, thereby significantly reducing the number of iterations required to produce high-quality samples. Our approach requires no additional training, datasets, or prior information, making it immediately integrable with any Langevin-based method. Extensive experiments across image generation and reasoning tasks demonstrate that PIDLD achieves higher quality with fewer steps, making Langevin-based generative models more practical for efficiency-critical applications. The implementation can be found at \href{https://github.com/tsinghua-fib-lab/PIDLD}{https://github.com/tsinghua-fib-lab/PIDLD}.

</details>


### [705] [FedTopo: Topology-Informed Representation Alignment in Federated Learning under Non-I.I.D. Conditions](https://arxiv.org/abs/2511.12628)
*Ke Hu,Liyao Xiang,Peng Tang,Weidong Qiu*

Main category: cs.LG

TL;DR: FedTopo introduces topological techniques to federated learning, improving accuracy and convergence in heterogeneous data settings.


<details>
  <summary>Details</summary>
Motivation: Traditional federated learning struggles with non-I.I.D. client data, leading to performance degradation in visual tasks due to representation divergence.

Method: FedTopo incorporates Topological-Guided Block Screening and Topological Embedding, coupled with Topological Alignment Loss, for coherent cross-client representations.

Result: FedTopo demonstrates improved convergence speed and accuracy in experiments with Fashion-MNIST, CIFAR-10, and CIFAR-100 datasets under challenging conditions.

Conclusion: FedTopo effectively leverages topology to align representations across clients, addressing challenges in federated learning with heterogeneous data.

Abstract: Current federated-learning models deteriorate under heterogeneous (non-I.I.D.) client data, as their feature representations diverge and pixel- or patch-level objectives fail to capture the global topology which is essential for high-dimensional visual tasks. We propose FedTopo, a framework that integrates Topological-Guided Block Screening (TGBS) and Topological Embedding (TE) to leverage topological information, yielding coherently aligned cross-client representations by Topological Alignment Loss (TAL). First, Topology-Guided Block Screening (TGBS) automatically selects the most topology-informative block, i.e., the one with maximal topological separability, whose persistence-based signatures best distinguish within- versus between-class pairs, ensuring that subsequent analysis focuses on topology-rich features. Next, this block yields a compact Topological Embedding, which quantifies the topological information for each client. Finally, a Topological Alignment Loss (TAL) guides clients to maintain topological consistency with the global model during optimization, reducing representation drift across rounds. Experiments on Fashion-MNIST, CIFAR-10, and CIFAR-100 under four non-I.I.D. partitions show that FedTopo accelerates convergence and improves accuracy over strong baselines.

</details>


### [706] [NFQ2.0: The CartPole Benchmark Revisited](https://arxiv.org/abs/2511.12644)
*Sascha Lange,Roland Hafner,Martin Riedmiller*

Main category: cs.LG

TL;DR: This paper revisits NFQ on CartPole and introduces NFQ2.0 for enhanced stability and reproducibility, emphasizing real-world industrial applications.


<details>
  <summary>Details</summary>
Motivation: Earlier NFQ struggled with reproducibility and practical application in real-world systems, leading to the need for improved stability and learning frameworks.

Method: A modernized variant, NFQ2.0, is developed with ablation studies and experiments on an industrial-grade CartPole system to investigate robustness and stability.

Result: NFQ2.0 demonstrated improved reproducibility, stability, and enhanced learning performance compared to the original NFQ.

Conclusion: NFQ2.0 offers valuable insights into reproducibility and deep reinforcement learning improvements for industrial applications.

Abstract: This article revisits the 20-year-old neural fitted Q-iteration (NFQ) algorithm on its classical CartPole benchmark. NFQ was a pioneering approach towards modern Deep Reinforcement Learning (Deep RL) in applying multi-layer neural networks to reinforcement learning for real-world control problems. We explore the algorithm's conceptual simplicity and its transition from online to batch learning, which contributed to its stability. Despite its initial success, NFQ required extensive tuning and was not easily reproducible on real-world control problems. We propose a modernized variant NFQ2.0 and apply it to the CartPole task, concentrating on a real-world system build from standard industrial components, to investigate and improve the learning process's repeatability and robustness. Through ablation studies, we highlight key design decisions and hyperparameters that enhance performance and stability of NFQ2.0 over the original variant. Finally, we demonstrate how our findings can assist practitioners in reproducing and improving results and applying deep reinforcement learning more effectively in industrial contexts.

</details>


### [707] [FLClear: Visually Verifiable Multi-Client Watermarking for Federated Learning](https://arxiv.org/abs/2511.12663)
*Chen Gu,Yingying Sun,Yifan She,Donghui Hu*

Main category: cs.LG

TL;DR: The paper introduces FLClear, a framework addressing issues in Federated Learning (FL) watermarking, ensuring secure and intuitive ownership verification.


<details>
  <summary>Details</summary>
Motivation: Existing FL watermarking methods face challenges such as potential collisions, insufficient security, and complex verification mechanisms, necessitating a robust alternative.

Method: FLClear employs a transposed model optimized with contrastive learning to effectively merge watermarking and main task goals, enabling clear and measurable verification.

Result: FLClear proves superior across datasets, aggregation schemes, and attack scenarios, showcasing its advancements over prior methods.

Conclusion: FLClear establishes itself as a reliable solution for preserving intellectual property rights in FL by ensuring secure, collision-free, and visually interpretable watermark verification.

Abstract: Federated learning (FL) enables multiple clients to collaboratively train a shared global model while preserving the privacy of their local data. Within this paradigm, the intellectual property rights (IPR) of client models are critical assets that must be protected. In practice, the central server responsible for maintaining the global model may maliciously manipulate the global model to erase client contributions or falsely claim sole ownership, thereby infringing on clients' IPR. Watermarking has emerged as a promising technique for asserting model ownership and protecting intellectual property. However, existing FL watermarking approaches remain limited, suffering from potential watermark collisions among clients, insufficient watermark security, and non-intuitive verification mechanisms. In this paper, we propose FLClear, a novel framework that simultaneously achieves collision-free watermark aggregation, enhanced watermark security, and visually interpretable ownership verification. Specifically, FLClear introduces a transposed model jointly optimized with contrastive learning to integrate the watermarking and main task objectives. During verification, the watermark is reconstructed from the transposed model and evaluated through both visual inspection and structural similarity metrics, enabling intuitive and quantitative ownership verification. Comprehensive experiments conducted over various datasets, aggregation schemes, and attack scenarios demonstrate the effectiveness of FLClear and confirm that it consistently outperforms state-of-the-art FL watermarking methods.

</details>


### [708] [Attention-Enhanced Convolutional Autoencoder and Structured Delay Embeddings for Weather Prediction](https://arxiv.org/abs/2511.12682)
*Amirpasha Hedayat,Karthik Duraisamy*

Main category: cs.LG

TL;DR: An efficient reduced-order modeling (ROM) framework for short-range weather prediction is introduced utilizing a ResNet-based encoder and a linear operator in the latent space for capturing dynamics, showing limitations in generalization for future predictions.


<details>
  <summary>Details</summary>
Motivation: To address the need for computationally efficient and accurate solutions for forecasting complex high-dimensional systems like weather, without relying extensively on computationally demanding AI-driven models.

Method: The paper uses ResNet-based convolutional autoencoder with block attention modules for dimensionality reduction, coupled with a linear operator on the time-delayed latent space to model dynamics.

Result: The method effectively predicts weather patterns within the training period using ERA5 reanalysis data but struggles with accuracy when generalizing to future states.

Conclusion: The study highlights the challenges in reduced-order modeling, especially projection errors, and recommends exploring hybrid models combining efficient ROMs with advanced AI for better long-term forecasting.

Abstract: Weather prediction is a quintessential problem involving the forecasting of a complex, nonlinear, and chaotic high-dimensional dynamical system. This work introduces an efficient reduced-order modeling (ROM) framework for short-range weather prediction and investigates fundamental questions in dimensionality reduction and reduced order modeling of such systems. Unlike recent AI-driven models, which require extensive computational resources, our framework prioritizes efficiency while achieving reasonable accuracy. Specifically, a ResNet-based convolutional autoencoder augmented by block attention modules is developed to reduce the dimensionality of high-dimensional weather data. Subsequently, a linear operator is learned in the time-delayed embedding of the latent space to efficiently capture the dynamics. Using the ERA5 reanalysis dataset, we demonstrate that this framework performs well in-distribution as evidenced by effectively predicting weather patterns within training data periods. We also identify important limitations in generalizing to future states, particularly in maintaining prediction accuracy beyond the training window. Our analysis reveals that weather systems exhibit strong temporal correlations that can be effectively captured through linear operations in an appropriately constructed embedding space, and that projection error rather than inference error is the main bottleneck. These findings shed light on some key challenges in reduced-order modeling of chaotic systems and point toward opportunities for hybrid approaches that combine efficient reduced-order models as baselines with more sophisticated AI architectures, particularly for applications in long-term climate modeling where computational efficiency is paramount.

</details>


### [709] [Beyond Fixed Tasks: Unsupervised Environment Design for Task-Level Pairs](https://arxiv.org/abs/2511.12706)
*Daniel Furelos-Blanco,Charles Pert,Frederik Kelbel,Alex F. Spies,Alessandra Russo,Michael Dennis*

Main category: cs.LG

TL;DR: ATLAS is a novel reinforcement learning method that jointly designs tasks and levels, overcoming unsolvability issues with random sampling. It generates challenging yet solvable task-level pairs.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address the difficulty in training agents to follow complex instructions in intricate environments, where random sampling often leads to unsolvable task-level pairs.

Method: ATLAS builds upon unsupervised environment design (UED) by dynamically generating task-level pairs aligned to create solvable yet challenging combinations. It uses mutations considering the structures of tasks and levels.

Result: ATLAS significantly surpasses random sampling methods for generating solvable task-level pairs and expedites learning for agents. It achieves improved convergence and higher policy performance.

Conclusion: The approach successfully aligns tasks and levels, offering a robust solution to enhance reinforcement learning training effectiveness and solving previously challenging task combinations.

Abstract: Training general agents to follow complex instructions (tasks) in intricate environments (levels) remains a core challenge in reinforcement learning. Random sampling of task-level pairs often produces unsolvable combinations, highlighting the need to co-design tasks and levels. While unsupervised environment design (UED) has proven effective at automatically designing level curricula, prior work has only considered a fixed task. We present ATLAS (Aligning Tasks and Levels for Autocurricula of Specifications), a novel method that generates joint autocurricula over tasks and levels. Our approach builds upon UED to automatically produce solvable yet challenging task-level pairs for policy training. To evaluate ATLAS and drive progress in the field, we introduce an evaluation suite that models tasks as reward machines in Minigrid levels. Experiments demonstrate that ATLAS vastly outperforms random sampling approaches, particularly when sampling solvable pairs is unlikely. We further show that mutations leveraging the structure of both tasks and levels accelerate convergence to performant policies.

</details>


### [710] [Adaptive Graph Rewiring to Mitigate Over-Squashing in Mesh-Based GNNs for Fluid Dynamics Simulations](https://arxiv.org/abs/2511.12709)
*Sangwoo Seo,Hyunsung Kim,Jiwan Kim,Chanyoung Park*

Main category: cs.LG

TL;DR: The paper presents AdaMeshNet, an adaptive graph rewiring method for mesh-based Graph Neural Networks (GNNs) applied to fluid dynamics simulation, addressing issues like over-squashing by dynamically rewiring graph edges.


<details>
  <summary>Details</summary>
Motivation: Existing mesh-refinement techniques and graph rewiring methods fail to accurately model distant physical interactions during fluid dynamics simulations due to assumptions of instantaneous interactions or neglecting particle distance.

Method: AdaMeshNet introduces an adaptive graph rewiring process during the message-passing phase in GNNs, utilizing a rewiring delay score calculated from shortest-path distances and velocity differences to dynamically add edges at appropriate layers.

Result: AdaMeshNet outperforms conventional rewiring methods in mesh-based fluid simulations, demonstrating improved prediction accuracy and better modeling of sequential physical interactions.

Conclusion: The proposed adaptive rewiring framework effectively addresses the limitation of conventional methods, making it a promising approach for improving GNN-based simulations of fluid dynamics.

Abstract: Mesh-based simulation using Graph Neural Networks (GNNs) has been recognized as a promising approach for modeling fluid dynamics. However, the mesh refinement techniques which allocate finer resolution to regions with steep gradients can induce the over-squashing problem in mesh-based GNNs, which prevents the capture of long-range physical interactions. Conventional graph rewiring methods attempt to alleviate this issue by adding new edges, but they typically complete all rewiring operations before applying them to the GNN. These approaches are physically unrealistic, as they assume instantaneous interactions between distant nodes and disregard the distance information between particles. To address these limitations, we propose a novel framework, called Adaptive Graph Rewiring in Mesh-Based Graph Neural Networks (AdaMeshNet), that introduces an adaptive rewiring process into the message-passing procedure to model the gradual propagation of physical interactions. Our method computes a rewiring delay score for bottleneck nodes in the mesh graph, based on the shortest-path distance and the velocity difference. Using this score, it dynamically selects the message-passing layer at which new edges are rewired, which can lead to adaptive rewiring in a mesh graph. Extensive experiments on mesh-based fluid simulations demonstrate that AdaMeshNet outperforms conventional rewiring methods, effectively modeling the sequential nature of physical interactions and enabling more accurate predictions.

</details>


### [711] [Oxytrees: Model Trees for Bipartite Learning](https://arxiv.org/abs/2511.12713)
*Pedro Ilídio,Felipe Kenji Nakano,Alireza Gharahighehi,Robbe D'hondt,Ricardo Cerri,Celine Vens*

Main category: cs.LG

TL;DR: Oxytrees algorithm makes bipartite learning faster and more efficient, with improved predictive performance in various settings.


<details>
  <summary>Details</summary>
Motivation: To address scalability and generalizability issues in existing bipartite learning methods.

Method: Developed Oxytrees, a proxy-based biclustering model combining compressed proxy matrices and Kronecker product kernel-based leaf models.

Result: Achieved up to 30x faster training times with competitive or superior performance compared to state-of-the-art approaches on 15 datasets.

Conclusion: Oxytrees successfully addressed performance and efficiency challenges in bipartite learning, with a user-friendly Python API for reproducibility.

Abstract: Bipartite learning is a machine learning task that aims to predict interactions between pairs of instances. It has been applied to various domains, including drug-target interactions, RNA-disease associations, and regulatory network inference. Despite being widely investigated, current methods still present drawbacks, as they are often designed for a specific application and thus do not generalize to other problems or present scalability issues. To address these challenges, we propose Oxytrees: proxy-based biclustering model trees. Oxytrees compress the interaction matrix into row- and column-wise proxy matrices, significantly reducing training time without compromising predictive performance. We also propose a new leaf-assignment algorithm that significantly reduces the time taken for prediction. Finally, Oxytrees employ linear models using the Kronecker product kernel in their leaves, resulting in shallower trees and thus even faster training. Using 15 datasets, we compared the predictive performance of ensembles of Oxytrees with that of the current state-of-the-art. We achieved up to 30-fold improvement in training times compared to state-of-the-art biclustering forests, while demonstrating competitive or superior performance in most evaluation settings, particularly in the inductive setting. Finally, we provide an intuitive Python API to access all datasets, methods and evaluation measures used in this work, thus enabling reproducible research in this field.

</details>


### [712] [On Robustness of Linear Classifiers to Targeted Data Poisoning](https://arxiv.org/abs/2511.12722)
*Nakshatra Gupta,Sumanth Prabhu,Supratik Chakraborty,R Venkatesh*

Main category: cs.LG

TL;DR: The paper investigates an adversarial data poisoning attack in machine learning, explores robustness computation challenges, and proposes efficient methods to approximate it.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of automatic and efficient evaluation of a training dataset’s robustness against targeted data poisoning attacks, which is otherwise difficult due to the large dataset size and theoretical complexity.

Method: A theoretical analysis proves the robustness computation problem as NP-Complete, focusing on linear classifiers and limited label manipulation. The authors introduce a novel method to estimate lower and upper robustness bounds and provide an efficient implementation.

Result: The proposed method efficiently computes robustness bounds on practical datasets, outperforms state-of-the-art techniques, and shows the impact of poisoning beyond identified thresholds.

Conclusion: The approach offers a computationally efficient way to evaluate dataset robustness against label poisoning attacks, effectively bridging theoretical and practical gaps in this domain.

Abstract: Data poisoning is a training-time attack that undermines the trustworthiness of learned models. In a targeted data poisoning attack, an adversary manipulates the training dataset to alter the classification of a targeted test point. Given the typically large size of training dataset, manual detection of poisoning is difficult. An alternative is to automatically measure a dataset's robustness against such an attack, which is the focus of this paper. We consider a threat model wherein an adversary can only perturb the labels of the training dataset, with knowledge limited to the hypothesis space of the victim's model. In this setting, we prove that finding the robustness is an NP-Complete problem, even when hypotheses are linear classifiers. To overcome this, we present a technique that finds lower and upper bounds of robustness. Our implementation of the technique computes these bounds efficiently in practice for many publicly available datasets. We experimentally demonstrate the effectiveness of our approach. Specifically, a poisoning exceeding the identified robustness bounds significantly impacts test point classification. We are also able to compute these bounds in many more cases where state-of-the-art techniques fail.

</details>


### [713] [LAYA: Layer-wise Attention Aggregation for Interpretable Depth-Aware Neural Networks](https://arxiv.org/abs/2511.12723)
*Gennaro Vessio*

Main category: cs.LG

TL;DR: LAYA is a novel neural network output head that uses attention-based mechanisms to aggregate information dynamically from all intermediate layers, enhancing interpretability and accuracy in vision and language benchmarks.


<details>
  <summary>Details</summary>
Motivation: Standard deep neural networks typically rely solely on their final layer for predictions, discarding rich intermediate layer representations that could provide complementary information.

Method: This paper introduces LAYA, a layer-wise attention aggregator, that learns input-conditioned attention weights to combine features from various intermediate layers instead of relying only on the deepest embedding.

Result: Experiments across vision and language tasks demonstrate LAYA improves performance compared to traditional output heads, with relative accuracy gains of up to 1%, and offers explicit interpretability through layer-attribution scores.

Conclusion: LAYA provides better accuracy and interpretable layer contributions by leveraging intermediate representations dynamically, proving to be architecture-agnostic and effective in synthesizing predictions directly through the model's computation.

Abstract: Deep neural networks typically rely on the representation produced by their final hidden layer to make predictions, implicitly assuming that this single vector fully captures the semantics encoded across all preceding transformations. However, intermediate layers contain rich and complementary information -- ranging from low-level patterns to high-level abstractions -- that is often discarded when the decision head depends solely on the last representation. This paper revisits the role of the output layer and introduces LAYA (Layer-wise Attention Aggregator), a novel output head that dynamically aggregates internal representations through attention. Instead of projecting only the deepest embedding, LAYA learns input-conditioned attention weights over layer-wise features, yielding an interpretable and architecture-agnostic mechanism for synthesizing predictions. Experiments on vision and language benchmarks show that LAYA consistently matches or improves the performance of standard output heads, with relative gains of up to about one percentage point in accuracy, while providing explicit layer-attribution scores that reveal how different abstraction levels contribute to each decision. Crucially, these interpretability signals emerge directly from the model's computation, without any external post hoc explanations. The code to reproduce LAYA is publicly available at: https://github.com/gvessio/LAYA.

</details>


### [714] [Convolutional Model Trees](https://arxiv.org/abs/2511.12725)
*William Ward Armstrong*

Main category: cs.LG

TL;DR: The paper introduces a method for constructing model tree forests to fit image-based functions, incorporating techniques for handling distortions and improving accuracy, plus a smoothing approach for continuous differentiability.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address the challenges of fitting functions defined on images, specifically accounting for image distortions and improving model accuracy and smoothness.

Method: The method involves several steps: image down-sampling, determining hyperplanes for the tree, applying convolutions to manage distortions, creating forests of model trees, and implementing a smoothing framework for continuous differentiability.

Result: The paper presents a theoretical training process and demonstrates convergence, offering robust handling of image distortions and smooth fitting of functions.

Conclusion: The proposed approach effectively improves fitting accuracy, handles distortions, and provides a smoothed approximation, with proven convergence of the training process.

Abstract: A method for creating a forest of model trees to fit samples of a function defined on images is described in several steps: down-sampling the images, determining a tree's hyperplanes, applying convolutions to the hyperplanes to handle small distortions of training images, and creating forests of model trees to increase accuracy and achieve a smooth fit. A 1-to-1 correspondence among pixels of images, coefficients of hyperplanes and coefficients of leaf functions offers the possibility of dealing with larger distortions such as arbitrary rotations or changes of perspective. A theoretical method for smoothing forest outputs to produce a continuously differentiable approximation is described. Within that framework, a training procedure is proved to converge.

</details>


### [715] [Stabilizing Self-Consuming Diffusion Models with Latent Space Filtering](https://arxiv.org/abs/2511.12742)
*Zhongteng Cai,Yaxuan Wang,Yang Liu,Xueru Zhang*

Main category: cs.LG

TL;DR: Synthetic data can lead to model collapse through repeated usage in generative models. This paper introduces Latent Space Filtering (LSF) to prevent degradation in latent representations.


<details>
  <summary>Details</summary>
Motivation: Repeated use of synthetic data in training generative models causes instability and model collapse due to degradation in latent space structure.

Method: Latent Space Filtering (LSF) identifies and removes less realistic synthetic data by analyzing latent space dynamics, without increasing computational costs or requiring human annotation.

Result: LSF reduces model collapse, demonstrating superiority over existing baselines across various real-world datasets.

Conclusion: Latent Space Filtering is effective in mitigating model collapse, proving practical for avoiding instability with synthetic data usage.

Abstract: As synthetic data proliferates across the Internet, it is often reused to train successive generations of generative models. This creates a ``self-consuming loop" that can lead to training instability or \textit{model collapse}. Common strategies to address the issue -- such as accumulating historical training data or injecting fresh real data -- either increase computational cost or require expensive human annotation. In this paper, we empirically analyze the latent space dynamics of self-consuming diffusion models and observe that the low-dimensional structure of latent representations extracted from synthetic data degrade over generations. Based on this insight, we propose \textit{Latent Space Filtering} (LSF), a novel approach that mitigates model collapse by filtering out less realistic synthetic data from mixed datasets. Theoretically, we present a framework that connects latent space degradation to empirical observations. Experimentally, we show that LSF consistently outperforms existing baselines across multiple real-world datasets, effectively mitigating model collapse without increasing training cost or relying on human annotation.

</details>


### [716] [DIVIDE: A Framework for Learning from Independent Multi-Mechanism Data Using Deep Encoders and Gaussian Processes](https://arxiv.org/abs/2511.12745)
*Vivek Chawla,Boris Slautin,Utkarsh Pratiush,Dayakar Penumadu,Sergei Kalinin*

Main category: cs.LG

TL;DR: DIVIDE introduces a framework using deep encoders and a structured Gaussian Process to separate and study distinct generative mechanisms in datasets for better interpretability and prediction.


<details>
  <summary>Details</summary>
Motivation: Disentangling influences from independent mechanisms in scientific datasets can lead to enhanced interpretability and better predictions. However, their combined effects often obscure individual contributions, creating a challenge.

Method: DIVIDE integrates mechanism-specific deep encoders with a structured Gaussian Process within a joint latent space. This combination isolates mechanisms while the Gaussian Process handles their combined interaction and uncertainty.

Result: The framework successfully separated mechanisms, reproduced independent and scaled interactions, and stayed robust despite noise in tests with synthetic datasets, FerroSIM simulations, and experimental data.

Conclusion: DIVIDE enables interpretable and mechanism-aware predictions, is effective for disentangling contributing mechanisms in datasets, and shows promise for its application in multifunctional scientific data.

Abstract: Scientific datasets often arise from multiple independent mechanisms such as spatial, categorical or structural effects, whose combined influence obscures their individual contributions. We introduce DIVIDE, a framework that disentangles these influences by integrating mechanism-specific deep encoders with a structured Gaussian Process in a joint latent space. Disentanglement here refers to separating independently acting generative factors. The encoders isolate distinct mechanisms while the Gaussian Process captures their combined effect with calibrated uncertainty. The architecture supports structured priors, enabling interpretable and mechanism-aware prediction as well as efficient active learning. DIVIDE is demonstrated on synthetic datasets combining categorical image patches with nonlinear spatial fields, on FerroSIM spin lattice simulations of ferroelectric patterns, and on experimental PFM hysteresis loops from PbTiO3 films. Across benchmarks, DIVIDE separates mechanisms, reproduces additive and scaled interactions, and remains robust under noise. The framework extends naturally to multifunctional datasets where mechanical, electromagnetic or optical responses coexist.

</details>


### [717] [INC: An Indirect Neural Corrector for Auto-Regressive Hybrid PDE Solvers](https://arxiv.org/abs/2511.12764)
*Hao Wei,Aleksandra Franz,Bjoern List,Nils Thuerey*

Main category: cs.LG

TL;DR: The paper introduces the Indirect Neural Corrector (INC), a method integrating learned corrections into governing equations for simulating partial differential equations, leading to reduced errors and improved performance in long-term trajectory and chaotic regimes.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the significant autoregressive errors that arise from amplified perturbations during long-term rollouts, especially in chaotic systems, when learned corrections are applied directly to solver outputs.

Method: The proposed method, INC, integrates learned corrections indirectly into governing equations rather than applying direct updates to simulation states. This approach reduces error amplification with no architectural constraints, allowing seamless integration with diverse neural networks and numerical solvers.

Result: INC improves long-term trajectory accuracy by up to 158.7% and stabilizes solver blow-ups under aggressive coarsening. It also achieves significant computational speed-ups, particularly for 3D turbulence cases.

Conclusion: INC enables stable and highly efficient PDE emulations and accelerates scientific and engineering simulations while maintaining reliable physics constraints and guarantees.

Abstract: When simulating partial differential equations, hybrid solvers combine coarse numerical solvers with learned correctors. They promise accelerated simulations while adhering to physical constraints. However, as shown in our theoretical framework, directly applying learned corrections to solver outputs leads to significant autoregressive errors, which originate from amplified perturbations that accumulate during long-term rollouts, especially in chaotic regimes. To overcome this, we propose the Indirect Neural Corrector (\(\mathrm{INC}\)), which integrates learned corrections into the governing equations rather than applying direct state updates. Our key insight is that \(\mathrm{INC}\) reduces the error amplification on the order of \(Δt^{-1} + L\), where \(Δt\) is the timestep and $L$ the Lipschitz constant. At the same time, our framework poses no architectural requirements and integrates seamlessly with arbitrary neural networks and solvers. We test \(\mathrm{INC}\) in extensive benchmarks, covering numerous differentiable solvers, neural backbones, and test cases ranging from a 1D chaotic system to 3D turbulence. INC improves the long-term trajectory performance (\(R^2\)) by up to 158.7\%, stabilizes blowups under aggressive coarsening, and for complex 3D turbulence cases yields speed-ups of several orders of magnitude. INC thus enables stable, efficient PDE emulation with formal error reduction, paving the way for faster scientific and engineering simulations with reliable physics guarantees. Our source code is available at https://github.com/tum-pbs/INC

</details>


### [718] [MolEdit: Knowledge Editing for Multimodal Molecule Language Models](https://arxiv.org/abs/2511.12770)
*Zhenyu Lei,Patrick Soga,Yaochen Zhu,Yinhan He,Yushun Dong,Jundong Li*

Main category: cs.LG

TL;DR: The paper introduces MolEdit, a framework for editing molecule language models (MoLMs), addressing key challenges and introducing a benchmark for evaluation.


<details>
  <summary>Details</summary>
Motivation: The study aims to address inaccuracies in MoLMs caused by outdated or manipulated training data, ensuring reliable advancements in biomedicine and materials science.

Method: MolEdit combines a Multi-Expert Knowledge Adapter for targeted edits and an Expertise-Aware Editing Switcher to minimize interference with unrelated knowledge. A benchmark, MEBench, is introduced to evaluate its performance across critical dimensions.

Result: MolEdit shows up to 18.8% improvement in editing reliability and 12.0% better locality compared to baselines, demonstrating effectiveness and efficiency.

Conclusion: MolEdit is a promising tool for reliable editing in MoLMs, safeguarding accuracy while preserving irrelevant molecular knowledge for downstream applications.

Abstract: Understanding and continuously refining multimodal molecular knowledge is crucial for advancing biomedicine, chemistry, and materials science. Molecule language models (MoLMs) have become powerful tools in these domains, integrating structural representations (e.g., SMILES strings, molecular graphs) with rich contextual descriptions (e.g., physicochemical properties). However, MoLMs can encode and propagate inaccuracies due to outdated web-mined training corpora or malicious manipulation, jeopardizing downstream discovery pipelines. While knowledge editing has been explored for general-domain AI, its application to MoLMs remains uncharted, presenting unique challenges due to the multifaceted and interdependent nature of molecular knowledge. In this paper, we take the first step toward MoLM editing for two critical tasks: molecule-to-caption generation and caption-to-molecule generation. To address molecule-specific challenges, we propose MolEdit, a powerful framework that enables targeted modifications while preserving unrelated molecular knowledge. MolEdit combines a Multi-Expert Knowledge Adapter that routes edits to specialized experts for different molecular facets with an Expertise-Aware Editing Switcher that activates the adapters only when input closely matches the stored edits across all expertise, minimizing interference with unrelated knowledge. To systematically evaluate editing performance, we introduce MEBench, a comprehensive benchmark assessing multiple dimensions, including Reliability (accuracy of the editing), Locality (preservation of irrelevant knowledge), and Generality (robustness to reformed queries). Across extensive experiments on two popular MoLM backbones, MolEdit delivers up to 18.8% higher Reliability and 12.0% better Locality than baselines while maintaining efficiency. The code is available at: https://github.com/LzyFischer/MolEdit.

</details>


### [719] [Scalable Multi-Objective and Meta Reinforcement Learning via Gradient Estimation](https://arxiv.org/abs/2511.12779)
*Zhenshuo Zhang,Minxuan Duan,Youran Ye,Hongyang R. Zhang*

Main category: cs.LG

TL;DR: The paper proposes PolicyGradEx, an efficient algorithm for clustering multiple objectives in RL into groups, optimizing multitask learning with significant performance enhancements.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiency of learning a single policy for a large number of objectives in RL, common in applications like robotics and language models.

Method: Introduce a two-stage approach: meta-training with multitask learning followed by fine-tuning to adapt meta-policy to subsets of objectives, leveraging affinity scores for clustering.

Result: PolicyGradEx notably outperforms baselines by 16% on average, achieves significant speedup (up to 26x), and demonstrates robust clustering improvements using loss-based metrics.

Conclusion: PolicyGradEx offers a scalable solution for efficient multitask learning in RL, validated by experiments, and provides insights into generalization errors in neural networks.

Abstract: We study the problem of efficiently estimating policies that simultaneously optimize multiple objectives in reinforcement learning (RL). Given $n$ objectives (or tasks), we seek the optimal partition of these objectives into $k \ll n$ groups, where each group comprises related objectives that can be trained together. This problem arises in applications such as robotics, control, and preference optimization in language models, where learning a single policy for all $n$ objectives is suboptimal as $n$ grows. We introduce a two-stage procedure -- meta-training followed by fine-tuning -- to address this problem. We first learn a meta-policy for all objectives using multitask learning. Then, we adapt the meta-policy to multiple randomly sampled subsets of objectives. The adaptation step leverages a first-order approximation property of well-trained policy networks, which is empirically verified to be accurate within a $2\%$ error margin across various RL environments. The resulting algorithm, PolicyGradEx, efficiently estimates an aggregate task-affinity score matrix given a policy evaluation algorithm. Based on the estimated affinity score matrix, we cluster the $n$ objectives into $k$ groups by maximizing the intra-cluster affinity scores. Experiments on three robotic control and the Meta-World benchmarks demonstrate that our approach outperforms state-of-the-art baselines by $16\%$ on average, while delivering up to $26\times$ faster speedup relative to performing full training to obtain the clusters. Ablation studies validate each component of our approach. For instance, compared with random grouping and gradient-similarity-based grouping, our loss-based clustering yields an improvement of $19\%$. Finally, we analyze the generalization error of policy networks by measuring the Hessian trace of the loss surface, which gives non-vacuous measures relative to the observed generalization errors.

</details>


### [720] [Optimal Look-back Horizon for Time Series Forecasting in Federated Learning](https://arxiv.org/abs/2511.12791)
*Dahao Tang,Nan Yang,Yanli Li,Zhiyu Zhu,Zhibo Jin,Dong Yuan*

Main category: cs.LG

TL;DR: This study focuses on adaptive look-back horizon selection for federated time series forecasting, introducing a new framework and synthetic data generator to address decentralized, heterogeneous, and non-independent data.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the challenge of selecting an appropriate look-back horizon in time series forecasting, particularly within federated learning environments characterized by decentralized and heterogeneous data.

Method: The authors propose a synthetic data generator to capture key temporal structures in client data and derive a framework using intrinsic space representation to analyze forecasting loss through a Bayesian and approximation error perspective.

Result: The analysis reveals that increasing the look-back horizon improves pattern identifiability but also increases approximation errors, highlighting the need for an optimal horizon where irreducible loss saturates.

Conclusion: The paper provides a theoretical basis for adaptive horizon selection in federated time series forecasting, optimizing data efficiency while balancing forecasting errors.

Abstract: Selecting an appropriate look-back horizon remains a fundamental challenge in time series forecasting (TSF), particularly in the federated learning scenarios where data is decentralized, heterogeneous, and often non-independent. While recent work has explored horizon selection by preserving forecasting-relevant information in an intrinsic space, these approaches are primarily restricted to centralized and independently distributed settings. This paper presents a principled framework for adaptive horizon selection in federated time series forecasting through an intrinsic space formulation. We introduce a synthetic data generator (SDG) that captures essential temporal structures in client data, including autoregressive dependencies, seasonality, and trend, while incorporating client-specific heterogeneity. Building on this model, we define a transformation that maps time series windows into an intrinsic representation space with well-defined geometric and statistical properties. We then derive a decomposition of the forecasting loss into a Bayesian term, which reflects irreducible uncertainty, and an approximation term, which accounts for finite-sample effects and limited model capacity. Our analysis shows that while increasing the look-back horizon improves the identifiability of deterministic patterns, it also increases approximation error due to higher model complexity and reduced sample efficiency. We prove that the total forecasting loss is minimized at the smallest horizon where the irreducible loss starts to saturate, while the approximation loss continues to rise. This work provides a rigorous theoretical foundation for adaptive horizon selection for time series forecasting in federated learning.

</details>


### [721] [Genomic Next-Token Predictors are In-Context Learners](https://arxiv.org/abs/2511.12797)
*Nathan Breslow,Aayush Mishra,Mahler Revsine,Michael C. Schatz,Anqi Liu,Daniel Khashabi*

Main category: cs.LG

TL;DR: In-context learning (ICL) is shown to emerge in genomic models trained on large-scale predictive tasks, suggesting it is not unique to human language but arises from statistical structures in data.


<details>
  <summary>Details</summary>
Motivation: The study aims to investigate whether in-context learning (ICL), often attributed to human language's statistical properties, can arise from sequence domains apart from language, specifically genomic sequences.

Method: The authors compared ICL in genomic and linguistic models using the Evo2 genomic model, trained on next-nucleotide prediction, and experimentally examined pattern induction in symbolic reasoning tasks across both domains.

Result: Findings demonstrate that genomic models display log-linear gains in pattern induction with increased in-context demonstrations, mirroring behavior in language models.

Conclusion: ICL can emerge through large-scale predictive modeling over rich data, supporting a modality-agnostic understanding of in-context learning.

Abstract: In-context learning (ICL) -- the capacity of a model to infer and apply abstract patterns from examples provided within its input -- has been extensively studied in large language models trained for next-token prediction on human text. In fact, prior work often attributes this emergent behavior to distinctive statistical properties in human language. This raises a fundamental question: can ICL arise organically in other sequence domains purely through large-scale predictive training?
  To explore this, we turn to genomic sequences, an alternative symbolic domain rich in statistical structure. Specifically, we study the Evo2 genomic model, trained predominantly on next-nucleotide (A/T/C/G) prediction, at a scale comparable to mid-sized LLMs. We develop a controlled experimental framework comprising symbolic reasoning tasks instantiated in both linguistic and genomic forms, enabling direct comparison of ICL across genomic and linguistic models. Our results show that genomic models, like their linguistic counterparts, exhibit log-linear gains in pattern induction as the number of in-context demonstrations increases. To the best of our knowledge, this is the first evidence of organically emergent ICL in genomic sequences, supporting the hypothesis that ICL arises as a consequence of large-scale predictive modeling over rich data. These findings extend emergent meta-learning beyond language, pointing toward a unified, modality-agnostic view of in-context learning.

</details>


### [722] [The Alignment Game: A Theory of Long-Horizon Alignment Through Recursive Curation](https://arxiv.org/abs/2511.12804)
*Ali Falahati,Mohammad Mohammadi Amiri,Kate Larson,Lukasz Golab*

Main category: cs.LG

TL;DR: This paper examines the recursive retraining process in self-consuming generative models, focusing on long-term alignment effects with user preferences. It categorizes alignment outcomes and highlights fundamental limitations in diversity, symmetric influence, and initialization independence.


<details>
  <summary>Details</summary>
Motivation: To address how self-consuming generative models align with user preferences over time and formalize an understanding of recursive retraining impacts.

Method: A two-stage curation mechanism based on the Bradley-Terry (BT) model is used to study the interaction between model owners and public users in shaping model alignment outcomes.

Result: Three structural regimes of convergence are identified: consensus collapse, compromise, and asymmetric refinement. An impossibility theorem is proven, showing limitations in achieving simultaneous diversity preservation, symmetric influence, and independence from initialization.

Conclusion: Alignment in generative models is a dynamic equilibrium influenced by power asymmetries and past decisions, rather than a static target. Recursive retraining poses inherent trade-offs and challenges in alignment processes.

Abstract: In self-consuming generative models that train on their own outputs, alignment with user preferences becomes a recursive rather than one-time process. We provide the first formal foundation for analyzing the long-term effects of such recursive retraining on alignment. Under a two-stage curation mechanism based on the Bradley-Terry (BT) model, we model alignment as an interaction between two factions: the Model Owner, who filters which outputs should be learned by the model, and the Public User, who determines which outputs are ultimately shared and retained through interactions with the model. Our analysis reveals three structural convergence regimes depending on the degree of preference alignment: consensus collapse, compromise on shared optima, and asymmetric refinement. We prove a fundamental impossibility theorem: no recursive BT-based curation mechanism can simultaneously preserve diversity, ensure symmetric influence, and eliminate dependence on initialization. Framing the process as dynamic social choice, we show that alignment is not a static goal but an evolving equilibrium, shaped both by power asymmetries and path dependence.

</details>


### [723] [Expressive Temporal Specifications for Reward Monitoring](https://arxiv.org/abs/2511.12808)
*Omar Adalat,Francesco Belardinelli*

Main category: cs.LG

TL;DR: This paper introduces a method to create dense reward monitors using Linear Temporal Logic to improve efficiency in reinforcement learning training.


<details>
  <summary>Details</summary>
Motivation: Sparse rewards in reinforcement learning limit effective agent training, particularly for long-horizon decision-making.

Method: Utilizes quantitative Linear Temporal Logic on finite traces ($\text{LTL}_f[\mathcal{F}]$) to design dense reward monitors, providing real-time nuanced training feedback.

Result: Empirical analysis shows that the method consistently outperforms Boolean monitors in task completion and reduces the convergence time during training.

Conclusion: Using quantitative temporal logic enhances agent training by addressing sparse reward challenges, improving both task performance and efficiency.

Abstract: Specifying informative and dense reward functions remains a pivotal challenge in Reinforcement Learning, as it directly affects the efficiency of agent training. In this work, we harness the expressive power of quantitative Linear Temporal Logic on finite traces (($\text{LTL}_f[\mathcal{F}]$)) to synthesize reward monitors that generate a dense stream of rewards for runtime-observable state trajectories. By providing nuanced feedback during training, these monitors guide agents toward optimal behaviour and help mitigate the well-known issue of sparse rewards under long-horizon decision making, which arises under the Boolean semantics dominating the current literature. Our framework is algorithm-agnostic and only relies on a state labelling function, and naturally accommodates specifying non-Markovian properties. Empirical results show that our quantitative monitors consistently subsume and, depending on the environment, outperform Boolean monitors in maximizing a quantitative measure of task completion and in reducing convergence time.

</details>


### [724] [Assessing Automated Fact-Checking for Medical LLM Responses with Knowledge Graphs](https://arxiv.org/abs/2511.12817)
*Shasha Zhou,Mingyu Huang,Jack Cole,Charles Britton,Ming Yin,Jan Wolber,Ke Li*

Main category: cs.LG

TL;DR: The paper explores using medical knowledge graphs (KGs) for evaluating the factuality of large language model (LLM) responses in healthcare and proposes the FAITH framework for this purpose.


<details>
  <summary>Details</summary>
Motivation: To ensure the reliability and safety of LLMs in high-stakes healthcare scenarios by evaluating their factuality using structured methods like knowledge graphs.

Method: FAITH framework decomposes LLM responses into atomic claims, links them to medical KGs, and scores them based on evidence paths. Its effectiveness is evaluated using human subjective judgments.

Result: KG-based evaluation correlates more strongly with clinician evaluations, can distinguish between LLM capabilities, and is robust to textual variances while offering explainable scoring.

Conclusion: Medical knowledge graphs offer a promising route for automated factuality evaluations in healthcare, despite existing limitations, enhancing LLM reliability and transparency.

Abstract: The recent proliferation of large language models (LLMs) holds the potential to revolutionize healthcare, with strong capabilities in diverse medical tasks. Yet, deploying LLMs in high-stakes healthcare settings requires rigorous verification and validation to understand any potential harm. This paper investigates the reliability and viability of using medical knowledge graphs (KGs) for the automated factuality evaluation of LLM-generated responses. To ground this investigation, we introduce FAITH, a framework designed to systematically probe the strengths and limitations of this KG-based approach. FAITH operates without reference answers by decomposing responses into atomic claims, linking them to a medical KG, and scoring them based on evidence paths. Experiments on diverse medical tasks with human subjective evaluations demonstrate that KG-grounded evaluation achieves considerably higher correlations with clinician judgments and can effectively distinguish LLMs with varying capabilities. It is also robust to textual variances. The inherent explainability of its scoring can further help users understand and mitigate the limitations of current LLMs. We conclude that while limitations exist, leveraging KGs is a prominent direction for automated factuality assessment in healthcare.

</details>


### [725] [Catastrophic Forgetting in Kolmogorov-Arnold Networks](https://arxiv.org/abs/2511.12828)
*Mohammad Marufur Rahman,Guanchu Wang,Kaixiong Zhou,Minghan Chen,Fan Yang*

Main category: cs.LG

TL;DR: This paper investigates catastrophic forgetting in Kolmogorov-Arnold Networks (KANs) within the context of continual learning, offering a theoretical framework and introducing KAN-LoRA for efficient fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To examine the practical behavior and limitations of Kolmogorov-Arnold Networks (KANs) regarding catastrophic forgetting in continual learning.

Method: Empirical and theoretical analyses linking activation support overlap and data dimension to forgetting, combined with systematic experiments across synthetic and vision tasks. Additionally, the paper proposes KAN-LoRA for continual fine-tuning in language models.

Result: KANs show promising retention in low-dimensional settings but struggle in high-dimensional domains. KAN-LoRA demonstrates effectiveness in knowledge editing tasks.

Conclusion: KANs have potential but face limitations in high-dimensional applications; insights from this study guide improvements for continual learning systems.

Abstract: Catastrophic forgetting is a longstanding challenge in continual learning, where models lose knowledge from earlier tasks when learning new ones. While various mitigation strategies have been proposed for Multi-Layer Perceptrons (MLPs), recent architectural advances like Kolmogorov-Arnold Networks (KANs) have been suggested to offer intrinsic resistance to forgetting by leveraging localized spline-based activations. However, the practical behavior of KANs under continual learning remains unclear, and their limitations are not well understood. To address this, we present a comprehensive study of catastrophic forgetting in KANs and develop a theoretical framework that links forgetting to activation support overlap and intrinsic data dimension. We validate these analyses through systematic experiments on synthetic and vision tasks, measuring forgetting dynamics under varying model configurations and data complexity. Further, we introduce KAN-LoRA, a novel adapter design for parameter-efficient continual fine-tuning of language models, and evaluate its effectiveness in knowledge editing tasks. Our findings reveal that while KANs exhibit promising retention in low-dimensional algorithmic settings, they remain vulnerable to forgetting in high-dimensional domains such as image classification and language modeling. These results advance the understanding of KANs' strengths and limitations, offering practical insights for continual learning system design.

</details>


### [726] [An Evaluation of Representation Learning Methods in Particle Physics Foundation Models](https://arxiv.org/abs/2511.12829)
*Michael Chen,Raghav Kansal,Abhijith Gandrakota,Zichun Hao,Jennifer Ngadiuba,Maria Spiropulu*

Main category: cs.LG

TL;DR: The paper evaluates various representation learning methods for particle physics using a unified framework and introduces modifications to achieve state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to systematically evaluate learning objectives for particle physics and provide a reproducible standardized approach for the development of foundation models.

Method: The study incorporates a transformer-based encoder, standardized preprocessing, matched sampling, and consistent evaluation with comparison across different learning objectives.

Result: Generates reproducible baselines and introduces state-of-the-art supervised modifications for jet classification dataset evaluations.

Conclusion: This work lays the foundation for transparent, reproducible advancements in particle physics models and serves as a reference point for future innovations.

Abstract: We present a systematic evaluation of representation learning objectives for particle physics within a unified framework. Our study employs a shared transformer-based particle-cloud encoder with standardized preprocessing, matched sampling, and a consistent evaluation protocol on a jet classification dataset. We compare contrastive (supervised and self-supervised), masked particle modeling, and generative reconstruction objectives under a common training regimen. In addition, we introduce targeted supervised architectural modifications that achieve state-of-the-art performance on benchmark evaluations. This controlled comparison isolates the contributions of the learning objective, highlights their respective strengths and limitations, and provides reproducible baselines. We position this work as a reference point for the future development of foundation models in particle physics, enabling more transparent and robust progress across the community.

</details>


### [727] [Connectivity-Guided Sparsification of 2-FWL GNNs: Preserving Full Expressivity with Improved Efficiency](https://arxiv.org/abs/2511.12838)
*Rongqin Chen,Fan Mo,Pak Lon Ip,Shenghui Zhang,Dan Wu,Ye Li,Leong Hou U*

Main category: cs.LG

TL;DR: Co-Sparsify introduces a framework for efficient Higher-order Graph Neural Networks (HOGNNs) by eliminating unnecessary computations while maintaining full expressivity.


<details>
  <summary>Details</summary>
Motivation: Current HOGNNs achieve strong expressivity by considering 2- and 3-node interactions but incur high computational costs, necessitating methods to reduce this burden without sacrificing expressivity.

Method: The approach restricts 2-node message passing to connected components and limits 3-node interactions to biconnected components, based on the insight that biconnected components need higher-order modeling for expressivity.

Result: Co-Sparsify shows equivalent or better accuracy in substructure counting tasks and achieves state-of-the-art results on datasets such as ZINC and QM9.

Conclusion: The framework demonstrates that high expressivity and computational efficiency can coexist by leveraging topology-guided sparsification with theoretical guarantees.

Abstract: Higher-order Graph Neural Networks (HOGNNs) based on the 2-FWL test achieve superior expressivity by modeling 2- and 3-node interactions, but at $\mathcal{O}(n^3)$ computational cost. However, this computational burden is typically mitigated by existing efficiency methods at the cost of reduced expressivity. We propose \textbf{Co-Sparsify}, a connectivity-aware sparsification framework that eliminates \emph{provably redundant} computations while preserving full 2-FWL expressive power. Our key insight is that 3-node interactions are expressively necessary only within \emph{biconnected components} -- maximal subgraphs where every pair of nodes lies on a cycle. Outside these components, structural relationships can be fully captured via 2-node message passing or global readout, rendering higher-order modeling unnecessary. Co-Sparsify restricts 2-node message passing to connected components and 3-node interactions to biconnected ones, removing computation without approximation or sampling. We prove that Co-Sparsified GNNs are as expressive as the 2-FWL test. Empirically, on PPGN, Co-Sparsify matches or exceeds accuracy on synthetic substructure counting tasks and achieves state-of-the-art performance on real-world benchmarks (ZINC, QM9). This study demonstrates that high expressivity and scalability are not mutually exclusive: principled, topology-guided sparsification enables powerful, efficient GNNs with theoretical guarantees.

</details>


### [728] [RoS-Guard: Robust and Scalable Online Change Detection with Delay-Optimal Guarantees](https://arxiv.org/abs/2511.12846)
*Zelin Zhu,Yancheng Huang,Kai Yang*

Main category: cs.LG

TL;DR: The paper introduces RoS-Guard, a robust online change detection algorithm for linear systems with uncertainty, providing theoretical guarantees and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing online change detection methods are inefficient for large-scale systems and rely on unrealistic assumptions, such as precise system knowledge.

Method: RoS-Guard reformulates the OCD optimization problem, using neural unrolling and GPU acceleration for efficient parallel computation in uncertain linear systems.

Result: Experiments show that RoS-Guard achieves significant computational speedups and maintains performance guarantees for false alarm rate and detection delay.

Conclusion: RoS-Guard addresses the limitations of traditional OCD methods by improving both robustness and efficiency, making it suitable for large-scale applications.

Abstract: Online change detection (OCD) aims to rapidly identify change points in streaming data and is critical in applications such as power system monitoring, wireless network sensing, and financial anomaly detection. Existing OCD methods typically assume precise system knowledge, which is unrealistic due to estimation errors and environmental variations. Moreover, existing OCD methods often struggle with efficiency in large-scale systems. To overcome these challenges, we propose RoS-Guard, a robust and optimal OCD algorithm tailored for linear systems with uncertainty. Through a tight relaxation and reformulation of the OCD optimization problem, RoS-Guard employs neural unrolling to enable efficient parallel computation via GPU acceleration. The algorithm provides theoretical guarantees on performance, including expected false alarm rate and worst-case average detection delay. Extensive experiments validate the effectiveness of RoS-Guard and demonstrate significant computational speedup in large-scale system scenarios.

</details>


### [729] [From Black-Box to White-Box: Control-Theoretic Neural Network Interpretability](https://arxiv.org/abs/2511.12852)
*Jihoon Moon*

Main category: cs.LG

TL;DR: The paper introduces a method to interpret deep neural networks using control theory to analyze their activation patterns and internal dynamics.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks have state-of-the-art performance but are challenging to interpret in terms of inner mechanisms.

Method: The authors propose a method that linearizes neural networks' hidden activations, forming state space models for analysis using controllability, observability Gramians, and Hankel singular values.

Result: The framework identifies the importance of neurons and pathways based on their roles in input excitation and output influence. Illustrated with specific network examples, the analysis highlights activation saturation effects and mode shifts.

Conclusion: The method establishes neural networks as local white-box dynamical models, aiding interpretability and guiding pruning or constraints for enhanced analysis.

Abstract: Deep neural networks achieve state of the art performance but remain difficult to interpret mechanistically. In this work, we propose a control theoretic framework that treats a trained neural network as a nonlinear state space system and uses local linearization, controllability and observability Gramians, and Hankel singular values to analyze its internal computation. For a given input, we linearize the network around the corresponding hidden activation pattern and construct a state space model whose state consists of hidden neuron activations. The input state and state output Jacobians define local controllability and observability Gramians, from which we compute Hankel singular values and associated modes. These quantities provide a principled notion of neuron and pathway importance: controllability measures how easily each neuron can be excited by input perturbations, observability measures how strongly each neuron influences the output, and Hankel singular values rank internal modes that carry input output energy. We illustrate the framework on simple feedforward networks, including a 1 2 2 1 SwiGLU network and a 2 3 3 2 GELU network. By comparing different operating points, we show how activation saturation reduces controllability, shrinks the dominant Hankel singular value, and shifts the dominant internal mode to a different subset of neurons. The proposed method turns a neural network into a collection of local white box dynamical models and suggests which internal directions are natural candidates for pruning or constraints to improve interpretability.

</details>


### [730] [An approach of deep reinforcement learning for maximizing the net present value of stochastic projects](https://arxiv.org/abs/2511.12865)
*Wei Xu,Fan Yang,Qinyuan Cui,Zhi Chen*

Main category: cs.LG

TL;DR: The paper addresses project management with uncertain durations and cash flows, proposing a DDQN-based method to maximize expected NPV, outperforming other strategies.


<details>
  <summary>Details</summary>
Motivation: To optimize project planning in environments with stochastic durations and cash flows, improving NPV by handling cash inflows and outflows more effectively.

Method: The authors use a Markov Decision Process (MDP) framework and develop a Double Deep Q-Network (DDQN) approach for optimization.

Result: DDQN surpasses traditional approaches in managing uncertainties, showing higher computational efficiency, reliability, and adaptability in large or uncertain environments.

Conclusion: This approach significantly enhances expected NPV, while the DDQN’s architecture ensures stable and robust policy implementation in project optimization.

Abstract: This paper investigates a project with stochastic activity durations and cash flows under discrete scenarios, where activities must satisfy precedence constraints generating cash inflows and outflows. The objective is to maximize expected net present value (NPV) by accelerating inflows and deferring outflows. We formulate the problem as a discrete-time Markov Decision Process (MDP) and propose a Double Deep Q-Network (DDQN) approach. Comparative experiments demonstrate that DDQN outperforms traditional rigid and dynamic strategies, particularly in large-scale or highly uncertain environments, exhibiting superior computational capability, policy reliability, and adaptability. Ablation studies further reveal that the dual-network architecture mitigates overestimation of action values, while the target network substantially improves training convergence and robustness. These results indicate that DDQN not only achieves higher expected NPV in complex project optimization but also provides a reliable framework for stable and effective policy implementation.

</details>


### [731] [Method of Manufactured Learning for Solver-free Training of Neural Operators](https://arxiv.org/abs/2511.12890)
*Arth Sojitra,Omer San*

Main category: cs.LG

TL;DR: The paper introduces the Method of Manufactured Learning (MML), a solver-independent approach for training neural operators using analytically constructed datasets, avoiding reliance on expensive numerical simulations or experimental data.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the scalability limitations and high costs associated with generating large datasets from numerical solvers or experimental setups when training neural operators for approximating mappings between function spaces.

Method: The method involves using functional synthesis to generate smooth, analytical solutions from controlled spaces and deriving corresponding forcing fields through governing differential operators, bypassing solver-based data generation.

Result: MML demonstrated high spectral accuracy, low residual errors, and strong generalization in canonical benchmarks, such as heat, advection, Burgers, and diffusion-reaction equations.

Conclusion: The framework provides a scalable, solver-agnostic method to construct neural operators that adhere to governing physical laws with minimal reliance on costly data generation methods.

Abstract: Training neural operators to approximate mappings between infinite-dimensional function spaces often requires extensive datasets generated by either demanding experimental setups or computationally expensive numerical solvers. This dependence on solver-based data limits scalability and constrains exploration across physical systems. Here we introduce the Method of Manufactured Learning (MML), a solver-independent framework for training neural operators using analytically constructed, physics-consistent datasets. Inspired by the classical method of manufactured solutions, MML replaces numerical data generation with functional synthesis, i.e., smooth candidate solutions are sampled from controlled analytical spaces, and the corresponding forcing fields are derived by direct application of the governing differential operators. During inference, setting these forcing terms to zero restores the original governing equations, allowing the trained neural operator to emulate the true solution operator of the system. The framework is agnostic to network architecture and can be integrated with any operator learning paradigm. In this paper, we employ Fourier neural operator as a representative example. Across canonical benchmarks including heat, advection, Burgers, and diffusion-reaction equations. MML achieves high spectral accuracy, low residual errors, and strong generalization to unseen conditions. By reframing data generation as a process of analytical synthesis, MML offers a scalable, solver-agnostic pathway toward constructing physically grounded neural operators that retain fidelity to governing laws without reliance on expensive numerical simulations or costly experimental data for training.

</details>


### [732] [Functional Mean Flow in Hilbert Space](https://arxiv.org/abs/2511.12898)
*Zhiqi Li,Yuchen Sun,Greg Turk,Bo Zhu*

Main category: cs.LG

TL;DR: The paper proposes Functional Mean Flow (FMF), a one-step generative model in infinite-dimensional Hilbert space, and demonstrates its application across various functional data domains.


<details>
  <summary>Details</summary>
Motivation: To extend one-step generative modeling techniques to functional domains, addressing a gap in efficient generation of complex data like time series, images, PDEs, and 3D geometry.

Method: The Functional Mean Flow (FMF) framework is developed, including a theoretical basis for Functional Flow Matching, and practical tools for training and sampling. An $x_1$-prediction variant is introduced for improved stability.

Result: The FMF framework effectively supports a broad range of functional data generation applications, combining theoretical rigor with practical efficiency.

Conclusion: FMF demonstrates that one-step Flow Matching can be extended to complex functional domains with both theoretical validity and practical applicability.

Abstract: We present Functional Mean Flow (FMF) as a one-step generative model defined in infinite-dimensional Hilbert space. FMF extends the one-step Mean Flow framework to functional domains by providing a theoretical formulation for Functional Flow Matching and a practical implementation for efficient training and sampling. We also introduce an $x_1$-prediction variant that improves stability over the original $u$-prediction form. The resulting framework is a practical one-step Flow Matching method applicable to a wide range of functional data generation tasks such as time series, images, PDEs, and 3D geometry.

</details>


### [733] [Contrastive Entropy Bounds for Density and Conditional Density Decomposition](https://arxiv.org/abs/2511.12903)
*Bo Hu,Jose C. Principe*

Main category: cs.LG

TL;DR: The paper explores neural network feature interpretability using a Bayesian Gaussian approach, introducing a method to optimize feature representations via probabilistic bounds and geometric norms in a Hilbert space.


<details>
  <summary>Details</summary>
Motivation: To enhance understanding and interpretability of neural network feature structures, particularly in mixture density networks and autoencoders by leveraging probabilistic frameworks.

Method: Introduces a framework using Hilbert space decomposition and Gaussian operators to define training objectives, trace-based and nuclear norm-based divergences for different architectures, and a novel encoder-mixture-decoder framework.

Result: Findings include equivalence of autoencoder objectives to maximizing Gaussian operator trace, utility of nuclear norm for divergence in mixture density networks, and improved bounds and sample diversity using customized architectures.

Conclusion: Probabilistic methods and operator norms allow for optimized neural network learning objectives that enhance diversity, accuracy, and bound tightness across different applications, furthering interpretability and efficiency.

Abstract: This paper studies the interpretability of neural network features from a Bayesian Gaussian view, where optimizing a cost is reaching a probabilistic bound; learning a model approximates a density that makes the bound tight and the cost optimal, often with a Gaussian mixture density. The two examples are Mixture Density Networks (MDNs) using the bound for the marginal and autoencoders using the conditional bound. It is a known result, not only for autoencoders, that minimizing the error between inputs and outputs maximizes the dependence between inputs and the middle.
  We use Hilbert space and decomposition to address cases where a multiple-output network produces multiple centers defining a Gaussian mixture. Our first finding is that an autoencoder's objective is equivalent to maximizing the trace of a Gaussian operator, the sum of eigenvalues under bases orthonormal w.r.t. the data and model distributions. This suggests that, when a one-to-one correspondence as needed in autoencoders is unnecessary, we can instead maximize the nuclear norm of this operator, the sum of singular values, to maximize overall rank rather than trace. Thus the trace of a Gaussian operator can be used to train autoencoders, and its nuclear norm can be used as divergence to train MDNs.
  Our second test uses inner products and norms in a Hilbert space to define bounds and costs. Such bounds often have an extra norm compared to KL-based bounds, which increases sample diversity and prevents the trivial solution where a multiple-output network produces the same constant, at the cost of requiring a sample batch to estimate and optimize. We propose an encoder-mixture-decoder architecture whose decoder is multiple-output, producing multiple centers per sample, potentially tightening the bound. Assuming the data are small-variance Gaussian mixtures, this upper bound can be tracked and analyzed quantitatively.

</details>


### [734] [LinkedIn Profile Characteristics and Professional Success Indicators](https://arxiv.org/abs/2511.12905)
*Tania-Amanda Fredrick Eneye,Ashlesha Malla,Pawan Paudel*

Main category: cs.LG

TL;DR: The paper studies how LinkedIn profile features influence professional success through machine learning analysis of 62,000 profiles.


<details>
  <summary>Details</summary>
Motivation: To explore how characteristics of LinkedIn profiles relate to professional success and provide actionable strategies for career improvement.

Method: Utilized machine learning models on a dataset of 62,000 anonymized LinkedIn profiles to predict influential factors of success indicators such as promotions and follower growth.

Result: Promotions are found to be highly predictable using LinkedIn profile data, while follower growth is more complex and less predictable.

Conclusion: The study offers professionals insights on optimizing LinkedIn profiles to enhance career progression and success.

Abstract: This study explores the relationship between LinkedIn profile characteristics and professional success, focusing on the indicators of promotions, follower count, and career progression rate. By leveraging a dataset of over 62,000 anonymized LinkedIn profiles, we developed predictive models using machine learning techniques to identify the most influential factors driving professional success. Results indicate that while promotions are highly predictable, follower growth exhibits greater complexity. This research provides actionable insights for professionals seeking to optimize their LinkedIn presence and career strategies.

</details>


### [735] [AIF: Asynchronous Inference Framework for Cost-Effective Pre-Ranking](https://arxiv.org/abs/2511.12934)
*Zhi Kou,Xiang-Rong Sheng,Shuguang Han,Zhishan Zhao,Yueyao Cheng,Han Zhu,Jian Xu,Bo Zheng*

Main category: cs.LG

TL;DR: The paper introduces the Asynchronous Inference Framework (AIF), which decouples certain computations in recommendation systems to reduce latency and improve efficiency, and is successfully applied to Taobao's advertising system.


<details>
  <summary>Details</summary>
Motivation: To overcome inefficiencies in traditional pre-ranking models for recommendation systems, such as redundant computations and increased latency due to sequential operations.

Method: The authors present AIF, which separates interaction-independent computations (e.g., user-side and item-side) from real-time prediction processes, enabling asynchronous and parallel computation for better efficiency.

Result: AIF improves computational efficiency, reduces latency, allows better use of resources for more complex features and models, and achieves notable performance improvements with minimal cost increases.

Conclusion: The proposed AIF framework successfully enhances recommendation system performance and efficiency, and has been effectively deployed in a real-world system, Taobao's ad platform.

Abstract: In industrial recommendation systems, pre-ranking models based on deep neural networks (DNNs) commonly adopt a sequential execution framework: feature fetching and model forward computation are triggered only after receiving candidates from the upstream retrieval stage. This design introduces inherent bottlenecks, including redundant computations of identical users/items and increased latency due to strictly sequential operations, which jointly constrain the model's capacity and system efficiency. To address these limitations, we propose the Asynchronous Inference Framework (AIF), a cost-effective computational architecture that decouples interaction-independent components, those operating within a single user or item, from real-time prediction. AIF reorganizes the model inference process by performing user-side computations in parallel with the retrieval stage and conducting item-side computations in a nearline manner. This means that interaction-independent components are calculated just once and completed before the real-time prediction phase of the pre-ranking stage. As a result, AIF enhances computational efficiency and reduces latency, freeing up resources to significantly improve the feature set and model architecture of interaction-independent components. Moreover, we delve into model design within the AIF framework, employing approximated methods for interaction-dependent components in online real-time predictions. By co-designing both the framework and the model, our solution achieves notable performance gains without significantly increasing computational and latency costs. This has enabled the successful deployment of AIF in the Taobao display advertising system.

</details>


### [736] [APT: Affine Prototype-Timestamp For Time Series Forecasting Under Distribution Shift](https://arxiv.org/abs/2511.12945)
*Yujie Li,Zezhi Shao,Chengqing Yu,Yisong Fu,Tao Sun,Yongjun Xu,Fei Wang*

Main category: cs.LG

TL;DR: The paper introduces APT, a module that improves time series forecasting under distribution shift by dynamically generating global distribution-aware features, compatible with various backbones and normalization strategies.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning models face challenges in accurately forecasting time series under distribution shifts due to limitations in normalization methods and issues such as missing values and noise.

Method: APT utilizes timestamp-conditioned prototype learning to dynamically generate affine parameters, which are then used to modulate the input and output series, integrating global distribution-awareness with self-supervised learning.

Result: APT demonstrated improved forecasting performance under distribution shift across six benchmark datasets and different backbone-normalization setups in extensive experiments.

Conclusion: APT successfully addressed major challenges in distribution-shift forecasting scenarios by injecting global distribution features, offering a lightweight and effective solution compatible with diverse setups.

Abstract: Time series forecasting under distribution shift remains challenging, as existing deep learning models often rely on local statistical normalization (e.g., mean and variance) that fails to capture global distribution shift. Methods like RevIN and its variants attempt to decouple distribution and pattern but still struggle with missing values, noisy observations, and invalid channel-wise affine transformation. To address these limitations, we propose Affine Prototype Timestamp (APT), a lightweight and flexible plug-in module that injects global distribution features into the normalization-forecasting pipeline. By leveraging timestamp conditioned prototype learning, APT dynamically generates affine parameters that modulate both input and output series, enabling the backbone to learn from self-supervised, distribution-aware clustered instances. APT is compatible with arbitrary forecasting backbones and normalization strategies while introducing minimal computational overhead. Extensive experiments across six benchmark datasets and multiple backbone-normalization combinations demonstrate that APT significantly improves forecasting performance under distribution shift.

</details>


### [737] [A FEDformer-Based Hybrid Framework for Anomaly Detection and Risk Forecasting in Financial Time Series](https://arxiv.org/abs/2511.12951)
*Ziling Fan,Ruijia Liang,Yiwen Hu*

Main category: cs.LG

TL;DR: The paper introduces a FEDformer-Based Hybrid Framework that addresses limitations of traditional deep learning models in financial anomaly detection and risk forecasting, showing significant improvements in accuracy on financial datasets.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the detection of anomalies and early risk forecasting in financial markets, given the inadequacies of traditional models like LSTM and GRU in handling nonstationary financial data.

Method: The study introduces a hybrid model combining a Frequency Enhanced Decomposed Transformer (FEDformer), a residual-based anomaly detector, and a risk forecasting module. The model captures time-frequency dynamics and separates trends and seasonal components for better interpretability.

Result: The model outperforms benchmarks on datasets like S&P 500 and NASDAQ Composite (2000-2024), achieving 15.7% lower RMSE and 11.5% higher F1-score in anomaly detection.

Conclusion: The proposed framework successfully enhances accuracy in detecting financial volatility, providing a reliable system for early market crash prediction and risk management.

Abstract: Financial markets are inherently volatile and prone to sudden disruptions such as market crashes, flash collapses, and liquidity crises. Accurate anomaly detection and early risk forecasting in financial time series are therefore crucial for preventing systemic instability and supporting informed investment decisions. Traditional deep learning models, such as LSTM and GRU, often fail to capture long-term dependencies and complex periodic patterns in highly nonstationary financial data. To address this limitation, this study proposes a FEDformer-Based Hybrid Framework for Anomaly Detection and Risk Forecasting in Financial Time Series, which integrates the Frequency Enhanced Decomposed Transformer (FEDformer) with a residual-based anomaly detector and a risk forecasting head. The FEDformer module models temporal dynamics in both time and frequency domains, decomposing signals into trend and seasonal components for improved interpretability. The residual-based detector identifies abnormal fluctuations by analyzing prediction errors, while the risk head predicts potential financial distress using learned latent embeddings. Experiments conducted on the S&P 500, NASDAQ Composite, and Brent Crude Oil datasets (2000-2024) demonstrate the superiority of the proposed model over benchmark methods, achieving a 15.7 percent reduction in RMSE and an 11.5 percent improvement in F1-score for anomaly detection. These results confirm the effectiveness of the model in capturing financial volatility, enabling reliable early-warning systems for market crash prediction and risk management.

</details>


### [738] [Global Cross-Time Attention Fusion for Enhanced Solar Flare Prediction from Multivariate Time Series](https://arxiv.org/abs/2511.12955)
*Onur Vural,Shah Muhammad Hamdi,Soukaina Filali Boubrahimi*

Main category: cs.LG

TL;DR: The paper introduces a Global Cross-Time Attention Fusion (GCTAF) transformer-based model to enhance solar flare prediction despite class imbalance in data.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the difficulty in predicting intense but rare solar flare events, which pose significant risks to modern technological systems.

Method: The paper proposes GCTAF, a novel transformer-based model employing cross-attentive global tokens to capture global temporal patterns for predictive flare modeling.

Result: The GCTAF model effectively identifies intense flares and improves prediction performance when tested on benchmark solar flare datasets.

Conclusion: Refining transformer architectures with global attention mechanisms holds promise for better solar flare prediction, contributing to space weather research and technological system resilience.

Abstract: Multivariate time series classification is increasingly investigated in space weather research as a means to predict intense solar flare events, which can cause widespread disruptions across modern technological systems. Magnetic field measurements of solar active regions are converted into structured multivariate time series, enabling predictive modeling across segmented observation windows. However, the inherently imbalanced nature of solar flare occurrences, where intense flares are rare compared to minor flare events, presents a significant barrier to effective learning. To address this challenge, we propose a novel Global Cross-Time Attention Fusion (GCTAF) architecture, a transformer-based model to enhance long-range temporal modeling. Unlike traditional self-attention mechanisms that rely solely on local interactions within time series, GCTAF injects a set of learnable cross-attentive global tokens that summarize salient temporal patterns across the entire sequence. These tokens are refined through cross-attention with the input sequence and fused back into the temporal representation, enabling the model to identify globally significant, non-contiguous time points that are critical for flare prediction. This mechanism functions as a dynamic attention-driven temporal summarizer that augments the model's capacity to capture discriminative flare-related dynamics. We evaluate our approach on the benchmark solar flare dataset and show that GCTAF effectively detects intense flares and improves predictive performance, demonstrating that refining transformer-based architectures presents a high-potential alternative for solar flare prediction tasks.

</details>


### [739] [RAGPulse: An Open-Source RAG Workload Trace to Optimize RAG Serving Systems](https://arxiv.org/abs/2511.12979)
*Zhengchao Wang,Yitao Hu,Jianing Ye,Zhuxuan Chang,Jiazheng Yu,Youpeng Deng,Keqiu Li*

Main category: cs.LG

TL;DR: The paper introduces RAGPulse, an open-source dataset of real-world Retrieval-Augmented Generation (RAG) workload traces collected from a university-wide Q&A system. It aims to enable better performance optimization for RAG systems in real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing performance optimization strategies for RAG systems fail to address the unique, real-world challenges of multi-stage pipelines and workload characteristics, creating a gap between academic research and real-world application.

Method: The authors collected workload traces from a university Q&A system (serving over 40,000 users) and used a privacy-preserving, hash-based data format. They analyzed the dataset's characteristics, such as temporal locality and document access patterns.

Result: Real-world RAG systems exhibit significant temporal locality and highly skewed access patterns, enabling researchers to study optimizations like content-aware batching and retrieval caching.

Conclusion: RAGPulse provides researchers with actionable insights and resources for developing more efficient and reliable RAG systems, bridging the gap between academic research and deployment in real-world contexts.

Abstract: Retrieval-Augmented Generation (RAG) is a critical paradigm for building reliable, knowledge-intensive Large Language Model (LLM) applications. However, the multi-stage pipeline (retrieve, generate) and unique workload characteristics (e.g., knowledge dependency) of RAG systems pose significant challenges for serving performance optimization. Existing generic LLM inference traces fail to capture these RAG-specific dynamics, creating a significant performance gap between academic research and real-world deployment. To bridge this gap, this paper introduces RAGPulse, an open-source RAG workload trace dataset. This dataset was collected from an university-wide Q&A system serving that has served more than 40,000 students and faculties since April 2024. We detail RAGPulse's system architecture, its privacy-preserving hash-based data format, and provide an in-depth statistical analysis. Our analysis reveals that real-world RAG workloads exhibit significant temporal locality and a highly skewed hot document access pattern. RAGPulse provides a high-fidelity foundation for researchers to develop and validate novel optimization strategies for RAG systems, such as content-aware batching and retrieval caching, ultimately enhancing the efficiency and reliability of RAG services. The code is available at https://github.com/flashserve/RAGPulse.

</details>


### [740] [Angular Gradient Sign Method: Uncovering Vulnerabilities in Hyperbolic Networks](https://arxiv.org/abs/2511.12985)
*Minsoo Jo,Dongyoon Yang,Taesup Kim*

Main category: cs.LG

TL;DR: This paper introduces a geometry-aware adversarial attack tailored for hyperbolic networks, yielding higher fooling rates and deeper insights into hyperbolic embeddings.


<details>
  <summary>Details</summary>
Motivation: Recent advances in hyperbolic neural networks require reevaluation of adversarial attack strategies, as existing methods neglect the underlying hyperbolic structure.

Method: The proposed attack focuses perturbations using the gradient decomposition in hyperbolic space's tangent space, specifically targeting the angular component for semantically sensitive directions.

Result: Empirical results show that this attack achieves higher fooling rates on image classification, cross-modal retrieval, and network architectures compared to conventional attacks.

Conclusion: This study underscores the importance of utilizing geometry-aware adversarial strategies in curved representation spaces while providing a robust framework to attack hierarchical embeddings.

Abstract: Adversarial examples in neural networks have been extensively studied in Euclidean geometry, but recent advances in \textit{hyperbolic networks} call for a reevaluation of attack strategies in non-Euclidean geometries. Existing methods such as FGSM and PGD apply perturbations without regard to the underlying hyperbolic structure, potentially leading to inefficient or geometrically inconsistent attacks. In this work, we propose a novel adversarial attack that explicitly leverages the geometric properties of hyperbolic space. Specifically, we compute the gradient of the loss function in the tangent space of hyperbolic space, decompose it into a radial (depth) component and an angular (semantic) component, and apply perturbation derived solely from the angular direction. Our method generates adversarial examples by focusing perturbations in semantically sensitive directions encoded in angular movement within the hyperbolic geometry. Empirical results on image classification, cross-modal retrieval tasks and network architectures demonstrate that our attack achieves higher fooling rates than conventional adversarial attacks, while producing high-impact perturbations with deeper insights into vulnerabilities of hyperbolic embeddings. This work highlights the importance of geometry-aware adversarial strategies in curved representation spaces and provides a principled framework for attacking hierarchical embeddings.

</details>


### [741] [Learning Branching Policies for MILPs with Proximal Policy Optimization](https://arxiv.org/abs/2511.12986)
*Abdelouahed Ben Mhamed,Assia Kamal-Idrissi,Amal El Fallah Seghrouchni*

Main category: cs.LG

TL;DR: This paper introduces Tree-Gate Proximal Policy Optimization (TGPPO), a reinforcement learning-based approach to improve the efficiency of branch-and-bound methods for solving Mixed Integer Linear Programs (MILP).


<details>
  <summary>Details</summary>
Motivation: To address limitations of traditional branch-and-bound methods for solving MILP, especially generalization issues in existing learning-based approaches relying on imitation learning.

Method: Uses Proximal Policy Optimization (PPO), a reinforcement learning algorithm, combined with a parameterized state-space representation to train adaptive branching strategies for MILP solvers.

Result: TGPPO consistently outperforms existing policies by reducing the number of search tree nodes explored and improving p-Primal-Dual Integrals (PDI), particularly for out-of-distribution MILP instances.

Conclusion: Reinforcement learning, specifically TGPPO, is a promising technique for developing robust and efficient branching strategies in MILP solvers, demonstrating better generalization to diverse problem structures.

Abstract: Branch-and-Bound (B\&B) is the dominant exact solution method for Mixed Integer Linear Programs (MILP), yet its exponential time complexity poses significant challenges for large-scale instances. The growing capabilities of machine learning have spurred efforts to improve B\&B by learning data-driven branching policies. However, most existing approaches rely on Imitation Learning (IL), which tends to overfit to expert demonstrations and struggles to generalize to structurally diverse or unseen instances. In this work, we propose Tree-Gate Proximal Policy Optimization (TGPPO), a novel framework that employs Proximal Policy Optimization (PPO), a Reinforcement Learning (RL) algorithm, to train a branching policy aimed at improving generalization across heterogeneous MILP instances. Our approach builds on a parameterized state space representation that dynamically captures the evolving context of the search tree. Empirical evaluations show that TGPPO often outperforms existing learning-based policies in terms of reducing the number of nodes explored and improving p-Primal-Dual Integrals (PDI), particularly in out-of-distribution instances. These results highlight the potential of RL to develop robust and adaptable branching strategies for MILP solvers.

</details>


### [742] [Are Graph Transformers Necessary? Efficient Long-Range Message Passing with Fractal Nodes in MPNNs](https://arxiv.org/abs/2511.13010)
*Jeongwhan Choi,Seungjun Park,Sumin Park,Sung-Bae Cho,Noseong Park*

Main category: cs.LG

TL;DR: The paper introduces fractal nodes to enhance Graph Neural Networks (GNNs), addressing limitations in balancing local and global information, and improving long-range dependencies.


<details>
  <summary>Details</summary>
Motivation: GNNs struggle to balance local and global information, and while graph Transformers attempt to solve this, they often neglect locality and computational efficiency emphasized in MPNNs.

Method: The authors use fractal nodes, inspired by fractal structures in real-world networks, to aggregate subgraph-level features and enforce feature similarity to alleviate over-squashing and support long-range propagation.

Result: Experiments show improved expressive power, comparable or better performance to graph Transformers, and efficient long-range dependency handling in MPNNs.

Conclusion: Fractal nodes enhance MPNNs by improving computational efficiency, expressive capability, and long-range dependencies, bridging the gap between graph Transformers and traditional MPNNs.

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for learning on graph-structured data, but often struggle to balance local and global information. While graph Transformers aim to address this by enabling long-range interactions, they often overlook the inherent locality and efficiency of Message Passing Neural Networks (MPNNs). We propose a new concept called fractal nodes, inspired by the fractal structure observed in real-world networks. Our approach is based on the intuition that graph partitioning naturally induces fractal structure, where subgraphs often reflect the connectivity patterns of the full graph. Fractal nodes are designed to coexist with the original nodes and adaptively aggregate subgraph-level feature representations, thereby enforcing feature similarity within each subgraph. We show that fractal nodes alleviate the over-squashing problem by providing direct shortcut connections that enable long-range propagation of subgraph-level representations. Experiment results show that our method improves the expressive power of MPNNs and achieves comparable or better performance to graph Transformers while maintaining the computational efficiency of MPNN by improving the long-range dependencies of MPNN.

</details>


### [743] [The Good, The Bad, and The Hybrid: A Reward Structure Showdown in Reasoning Models Training](https://arxiv.org/abs/2511.13016)
*Subramanyam Sahoo*

Main category: cs.LG

TL;DR: This paper introduces a unified framework to study various reward structures for fine-tuning large language models on mathematical reasoning tasks, focusing on hybrid reward structures.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance reinforcement learning from human feedback (RLHF) for large language models (LLMs), improving their performance in mathematical reasoning tasks by designing better reward mechanisms.

Method: The paper uses Qwen3-4B with LoRA fine-tuning on GSM8K dataset, incorporates various reward formulations (correctness, perplexity, reasoning quality, consistency), and employs an adaptive hybrid reward scheduler transitioning between discrete and continuous signals.

Result: Results show hybrid reward structures improve convergence speed and training stability compared to hard or continuous rewards.

Conclusion: Hybrid reward structures are effective in improving RLHF for LLMs, offering insights into adaptive reward modeling for alignment in reasoning tasks.

Abstract: Reward design is central to reinforcement learning from human feedback (RLHF) and alignment research. In this work, we propose a unified framework to study hard, continuous, and hybrid reward structures for fine-tuning large language models (LLMs) on mathematical reasoning tasks. Using Qwen3-4B with LoRA fine-tuning on the GSM8K dataset, we formalize and empirically evaluate reward formulations that incorporate correctness, perplexity, reasoning quality, and consistency. We introduce an adaptive hybrid reward scheduler that transitions between discrete and continuous signals, balancing exploration and stability. Our results show that hybrid reward structures improve convergence speed and training stability over purely hard or continuous approaches, offering insights for alignment via adaptive reward modeling.

</details>


### [744] [The Final-Stage Bottleneck: A Systematic Dissection of the R-Learner for Network Causal Inference](https://arxiv.org/abs/2511.13018)
*Sairam S,Sara Girdhar,Shivam Soni*

Main category: cs.LG

TL;DR: The paper analyzes the R-Learner framework for estimating heterogeneous treatment effects on graphs and identifies key bottlenecks impacting its performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of applying R-Learner frameworks to network data, particularly in the presence of graph-based causal heterogeneity.

Method: Large-scale empirical analysis of the R-Learner framework using synthetic and semi-synthetic benchmarks, introducing a Graph R-Learner and studying inductive biases and bottlenecks.

Result: The Graph R-Learner excels with its graph-aware design in estimating causal effects, overcoming the "representation bottleneck." Quantified catastrophic failures in graph-blind approaches and linked nuisance bottlenecks to graph topology.

Conclusion: The inductive bias of the final-stage model in causal estimators is crucial for success in graph-based scenarios. The Graph R-Learner significantly advances causal inference methods on network data.

Abstract: The R-Learner is a powerful, theoretically-grounded framework for estimating heterogeneous treatment effects, prized for its robustness to nuisance model errors. However, its application to network data, where causal heterogeneity is often graph-dependent, presents a critical challenge to its core assumption of a well-specified final-stage model. In this paper, we conduct a large-scale empirical study to systematically dissect the R-Learner framework on graphs. We provide the first rigorous evidence that the primary driver of performance is the inductive bias of the final-stage CATE estimator, an effect that dominates the choice of nuisance models. Our central finding is the quantification of a catastrophic "representation bottleneck": we prove with overwhelming statistical significance (p < 0.001) that R-Learners with a graph-blind final stage fail completely (MSE > 4.0), even when paired with powerful GNN nuisance models. Conversely, our proposed end-to-end Graph R-Learner succeeds and significantly outperforms a strong, non-DML GNN T-Learner baseline. Furthermore, we identify and provide a mechanistic explanation for a subtle, topology-dependent "nuisance bottleneck," linking it to GNN over-squashing via a targeted "Hub-Periphery Trade-off" analysis. Our findings are validated across diverse synthetic and semi-synthetic benchmarks. We release our code as a reproducible benchmark to facilitate future research on this critical "final-stage bottleneck."

</details>


### [745] [Learning Time-Scale Invariant Population-Level Neural Representations](https://arxiv.org/abs/2511.13022)
*Eshani Patel,Yisong Yue,Geeling Chau*

Main category: cs.LG

TL;DR: This paper introduces Time-scale Augmented Pretraining (TSAP) to improve robustness and generalization of population-level neural models under differing preprocessing settings, particularly targeting time-scale mismatches.


<details>
  <summary>Details</summary>
Motivation: To develop foundation models for neural time series that support scalable, population-level representation learning for neuroscience applications such as brain-computer interfaces, while addressing sensitivity to time-scale mismatches in preprocessing.

Method: The paper examines the effect of time-scale mismatches on generalizations of neural models, identifies the lack of invariance in existing methods, and proposes Time-scale Augmented Pretraining (TSAP) to introduce time-scale robustness.

Result: TSAP improves the robustness of neural time series foundation models to time-scale mismatches across multiple decoding tasks, demonstrating enhanced generalization capabilities.

Conclusion: Handling preprocessing diversity, particularly time-scale mismatches, is critical for creating generalized and scalable neural time series foundation models, with TSAP serving as an effective solution.

Abstract: General-purpose foundation models for neural time series can help accelerate neuroscientific discoveries and enable applications such as brain computer interfaces (BCIs). A key component in scaling these models is population-level representation learning, which leverages information across channels to capture spatial as well as temporal structure. Population-level approaches have recently shown that such representations can be both efficient to learn on top of pretrained temporal encoders and produce useful representations for decoding a variety of downstream tasks. However, these models remain sensitive to mismatches in preprocessing, particularly on time-scales, between pretraining and downstream settings. We systematically examine how time-scale mismatches affects generalization and find that existing representations lack invariance. To address this, we introduce Time-scale Augmented Pretraining (TSAP), which consistently improves robustness to different time-scales across decoding tasks and builds invariance in the representation space. These results highlight handling preprocessing diversity as a key step toward building generalizable neural foundation models.

</details>


### [746] [SLMQuant:Benchmarking Small Language Model Quantization for Practical Deployment](https://arxiv.org/abs/2511.13023)
*Jiacheng Wang,Yejun Zeng,Jinyang Guo,Yuqing Ma,Aishan Liu,Xianglong Liu*

Main category: cs.LG

TL;DR: This paper explores the challenges of compressing Small Language Models (SLMs) for efficient deployment on edge devices and introduces SLMQuant, a benchmark to evaluate quantization techniques for SLMs.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the growing interest in SLMs as resource-efficient alternatives to LLMs, and the need to address efficiency gaps in SLM compression for deployment on edge devices.

Method: SLMQuant, a benchmark, systematically evaluates state-of-the-art quantization methods across various architectures and tasks to analyze their effectiveness in SLMs.

Result: Findings highlight disparities in quantization sensitivity between SLMs and LLMs, showing that direct application of LLM-focused methods leads to suboptimal outcomes for SLMs.

Conclusion: SLMQuant provides a foundational framework for efficient SLM deployment, delivering critical insights and design principles tailored to lightweight models for resource-constrained applications.

Abstract: Despite the growing interest in Small Language Models (SLMs) as resource-efficient alternatives to Large Language Models (LLMs), their deployment on edge devices remains challenging due to unresolved efficiency gaps in model compression. While quantization has proven effective for LLMs, its applicability to SLMs is significantly underexplored, with critical questions about differing quantization bottlenecks and efficiency profiles. This paper introduces SLMQuant, the first systematic benchmark for evaluating LLM compression techniques when applied to SLMs. Through comprehensive multi-track evaluations across diverse architectures and tasks, we analyze how state-of-the-art quantization methods perform on SLMs. Our findings reveal fundamental disparities between SLMs and LLMs in quantization sensitivity, demonstrating that direct transfer of LLM-optimized techniques leads to suboptimal results due to SLMs' unique architectural characteristics and training dynamics. We identify key factors governing effective SLM quantization and propose actionable design principles for SLM-tailored compression. SLMQuant establishes a foundational framework for advancing efficient SLM deployment on low-end devices in edge applications, and provides critical insights for deploying lightweight language models in resource-constrained scenarios.

</details>


### [747] [One-Step Generative Policies with Q-Learning: A Reformulation of MeanFlow](https://arxiv.org/abs/2511.13035)
*Zeyuan Wang,Da Li,Yulin Chen,Ye Shi,Liang Bai,Tianyuan Yu,Yanwei Fu*

Main category: cs.LG

TL;DR: The paper proposes a one-step generative policy, MeanFlowQL, for offline reinforcement learning, enabling efficient, expressive action modeling and stable learning.


<details>
  <summary>Details</summary>
Motivation: Current reinforcement learning methods struggle with fast inference in Gaussian policies and the complexity of multimodal action distributions in flow-based solutions, which often require cumbersome multi-stage training.

Method: The authors reformulate MeanFlow to integrate the velocity field and noise-to-action transformations into a single policy network, enabling one-step expressive policy learning compatible with Q-learning.

Result: MeanFlowQL achieves strong performance across 73 tasks on OGBench and D4RL benchmarks in offline and offline-to-online learning scenarios.

Conclusion: The proposed technique offers efficient, expressive, and stable learning in reinforcement scenarios, simplifying the training process through innovative policy reformulation.

Abstract: We introduce a one-step generative policy for offline reinforcement learning that maps noise directly to actions via a residual reformulation of MeanFlow, making it compatible with Q-learning. While one-step Gaussian policies enable fast inference, they struggle to capture complex, multimodal action distributions. Existing flow-based methods improve expressivity but typically rely on distillation and two-stage training when trained with Q-learning. To overcome these limitations, we propose to reformulate MeanFlow to enable direct noise-to-action generation by integrating the velocity field and noise-to-action transformation into a single policy network-eliminating the need for separate velocity estimation. We explore several reformulation variants and identify an effective residual formulation that supports expressive and stable policy learning. Our method offers three key advantages: 1) efficient one-step noise-to-action generation, 2) expressive modelling of multimodal action distributions, and 3) efficient and stable policy learning via Q-learning in a single-stage training setup. Extensive experiments on 73 tasks across the OGBench and D4RL benchmarks demonstrate that our method achieves strong performance in both offline and offline-to-online reinforcement learning settings. Code is available at https://github.com/HiccupRL/MeanFlowQL.

</details>


### [748] [Bi-View Embedding Fusion: A Hybrid Learning Approach for Knowledge Graph's Nodes Classification Addressing Problems with Limited Data](https://arxiv.org/abs/2511.13044)
*Rosario Napoli,Giovanni Lonia,Antonio Celesti,Massimo Villari,Maria Fazio*

Main category: cs.LG

TL;DR: The study introduces Bi-View, a hybrid Graph Embedding approach combining Node2Vec and GraphSAGE techniques to enhance Graph Machine Learning performance, particularly in scenarios with sparse or incomplete data.


<details>
  <summary>Details</summary>
Motivation: Overcoming the limitations of traditional ML methods that require large datasets and addressing the hidden information in Knowledge Graphs using Graph Machine Learning approaches.

Method: Bi-View combines Node2Vec embeddings for graph topology representation with GraphSAGE's neighbourhood aggregation. A fusion layer integrates the two methods to create dual-perspective embeddings.

Result: The proposed approach improves downstream task performance in data-sparse conditions, leveraging both topological and semantic features of Knowledge Graphs.

Conclusion: Bi-View facilitates more accurate and precise Graph Machine Learning models, enriching node features without relying on synthetic data.

Abstract: Traditional Machine Learning (ML) methods require large amounts of data to perform well, limiting their applicability in sparse or incomplete scenarios and forcing the usage of additional synthetic data to improve the model training. To overcome this challenge, the research community is looking more and more at Graph Machine Learning (GML) as it offers a powerful alternative by using relationships within data. However, this method also faces limitations, particularly when dealing with Knowledge Graphs (KGs), which can hide huge information due to their semantic nature. This study introduces Bi-View, a novel hybrid approach that increases the informative content of node features in KGs to generate enhanced Graph Embeddings (GEs) that are used to improve GML models without relying on additional synthetic data. The proposed work combines two complementary GE techniques: Node2Vec, which captures structural patterns through unsupervised random walks, and GraphSAGE, which aggregates neighbourhood information in a supervised way. Node2Vec embeddings are first computed to represent the graph topology, and node features are then enriched with centrality-based metrics, which are used as input for the GraphSAGE model. Moreover, a fusion layer combines the original Node2Vec embeddings with the GraphSAGE-influenced representations, resulting in a dual-perspective embedding space. Such a fusion captures both topological and semantic properties of the graph, enabling the model to exploit informative features that may exist in the dataset but that are not explicitly represented. Our approach improves downstream task performance, especially in scenarios with poor initial features, giving the basis for more accurate and precise KG-enanched GML models.

</details>


### [749] [Learning from the Undesirable: Robust Adaptation of Language Models without Forgetting](https://arxiv.org/abs/2511.13052)
*Yunhun Nam,Jaehyung Kim,Jongheon Jeong*

Main category: cs.LG

TL;DR: The paper introduces Learning-from-the-Undesirable (LfU), a regularization scheme to address the overfitting problem in language model fine-tuning under limited data conditions.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of supervised fine-tuning (SFT) in LM adaptation when faced with limited training data, which leads to overfitting or loss of broad capabilities.

Method: LfU employs representation-level consistency regularization by aligning the internal model representations with those after undesirable updates, promoting generalization using undesirable updates for data augmentation.

Result: LfU shows a 16.8% average improvement in math tasks over standard SFT and achieves enhanced robustness and reduced variability in outputs.

Conclusion: LfU effectively enhances generalization and adaptability in fine-tuning scenarios, preserving pretrained knowledge and minimizing the downsides of traditional SFT with limited data.

Abstract: Language models (LMs) are often adapted through supervised fine-tuning (SFT) to specialize their capabilities for downstream tasks. However, in typical scenarios where the fine-tuning data is limited, e.g., compared to pre-training, SFT can lead LMs to overfit, causing them to rely on spurious patterns within the target task or to compromise other broadly useful capabilities as a side effect of narrow specialization. In this paper, we propose Learning-from-the-Undesirable (LfU), a simple yet effective regularization scheme for SFT to mitigate overfitting issues when fine-tuning LMs with limited data. Specifically, we aim to regularize the fine-tuning process to favor solutions that are resilient to "undesirable" model updates, e.g., gradient ascent steps that steer the model toward undesirable behaviors. To this end, we propose a novel form of consistency regularization that directly aligns internal representations of the model with those after an undesirable update. By leveraging representation-level data augmentation through undesirable updates, LfU effectively promotes generalization under limited data. Our experiments on diverse LM downstream tasks show that LfU serves as an effective prior that enhances adaptability while preserving pretrained knowledge. For example, our LM from LfU achieves a 16.8% average improvement on math tasks compared to vanilla SFT on the same dataset, where the latter even leads to degraded performance on those tasks. Furthermore, LfU exhibits improved robustness to prompt variations, e.g., yielding a 92.1% lower standard deviation in output performances compared to SFT, highlighting its versatile effects.

</details>


### [750] [Latency and Ordering Effects in Online Decisions](https://arxiv.org/abs/2511.13060)
*Duo Yi*

Main category: cs.LG

TL;DR: This paper addresses online decision systems with delays and order-sensitive effects, proving a structured lower bound for excess loss and extending guarantees beyond traditional convex assumptions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve performance benchmarks for online decision systems that naturally face delayed feedback and noncommutative dynamics, aiming for more interpretable and operational performance bounds.

Method: The authors introduce a structured inequality for excess loss decomposition and derive penalties for latency, order-sensitivity, and their interactions. This approach is extended to prox-regular and weakly convex settings.

Result: The study formalizes excess loss bounds including penalties for latency and order-sensitivity, and provides actionable diagnostic tools (like 2×2 experiments) for practical real-world stress-testing and adjustments.

Conclusion: This research offers a unified framework to handle delays, order-sensitive dynamics, and convex approximation gaps in online systems, making it operationally deployable.

Abstract: Online decision systems routinely operate under delayed feedback and order-sensitive (noncommutative) dynamics: actions affect which observations arrive, and in what sequence. Taking a Bregman divergence $D_Φ$ as the loss benchmark, we prove that the excess benchmark loss admits a structured lower bound $L \ge L_{\mathrm{ideal}} + g_1(λ) + g_2(\varepsilon_\star) + g_{12}(λ,\varepsilon_\star) - D_{\mathrm{ncx}}$, where $g_1$ and $g_2$ are calibrated penalties for latency and order-sensitivity, $g_{12}$ captures their geometric interaction, and $D_{\mathrm{ncx}}\ge 0$ is a nonconvexity/approximation penalty that vanishes under convex Legendre assumptions. We extend this inequality to prox-regular and weakly convex settings, obtaining robust guarantees beyond the convex case. We also give an operational recipe for estimating and monitoring the four terms via simple $2\times 2$ randomized experiments and streaming diagnostics (effective sample size, clipping rate, interaction heatmaps). The framework packages heterogeneous latency, noncommutativity, and implementation-gap effects into a single interpretable lower-bound statement that can be stress-tested and tuned in real-world systems.

</details>


### [751] [Self-Adaptive Graph Mixture of Models](https://arxiv.org/abs/2511.13062)
*Mohit Meena,Yash Punjabi,Abhishek A,Vishal Sharma,Mahesh Chandran*

Main category: cs.LG

TL;DR: The paper introduces SAGMM, a framework that adaptively selects and combines GNN models for graph tasks, outperforming existing methods across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing GNN models have plateaued in performance, making it hard to select the most suitable model for a task.

Method: The paper proposes SAGMM, which uses diverse architectures with topology-aware gating and pruning mechanisms to adaptively assign experts per node.

Result: SAGMM consistently surpasses or matches leading GNN baselines across 16 datasets.

Conclusion: SAGMM offers an efficient and adaptive solution for diverse graph-related tasks, addressing performance and model selection challenges.

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for learning over graph-structured data, yet recent studies have shown that their performance gains are beginning to plateau. In many cases, well-established models such as GCN and GAT, when appropriately tuned, can match or even exceed the performance of more complex, state-of-the-art architectures. This trend highlights a key limitation in the current landscape: the difficulty of selecting the most suitable model for a given graph task or dataset. To address this, we propose Self-Adaptive Graph Mixture of Models (SAGMM), a modular and practical framework that learns to automatically select and combine the most appropriate GNN models from a diverse pool of architectures. Unlike prior mixture-of-experts approaches that rely on variations of a single base model, SAGMM leverages architectural diversity and a topology-aware attention gating mechanism to adaptively assign experts to each node based on the structure of the input graph. To improve efficiency, SAGMM includes a pruning mechanism that reduces the number of active experts during training and inference without compromising performance. We also explore a training-efficient variant in which expert models are pretrained and frozen, and only the gating and task-specific layers are trained. We evaluate SAGMM on 16 benchmark datasets covering node classification, graph classification, regression, and link prediction tasks, and demonstrate that it consistently outperforms or matches leading GNN baselines and prior mixture-based methods, offering a robust and adaptive solution for real-world graph learning.

</details>


### [752] [A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning](https://arxiv.org/abs/2511.13078)
*Liuyi Jin,Pasan Gunawardena,Amran Haroon,Runzhi Wang,Sangwoo Lee,Radu Stoleru,Michael Middleton,Zepeng Huo,Jeeeun Kim,Jason Moats*

Main category: cs.LG

TL;DR: EMSGlass is a smart-glasses system powered by EMSNet and EMSServe, optimized for EMS decision-making by processing multimodal data in real-time, achieving faster and more accurate results for up to five critical tasks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the high-pressure, high-cognitive load environments EMTs face by developing a real-time, AI-enabled system to support life-critical decision-making and operational efficiency.

Method: EMSNet integrates multimodal data, and EMSServe optimizes inference performance in EMS scenarios. Together, they process text, vital signs, and images to handle five tasks concurrently, supported by novel technical approaches like modality-aware model splitting and feature caching.

Result: EMSGlass demonstrated a 1.9x–11.7x speedup over traditional methods and achieved higher accuracy than unimodal baselines. A user study with professional EMTs confirmed its positive impact on decision-making and situational awareness.

Conclusion: The research showcases how EMSGlass enhances EMS workflows via multimodal intelligence, improving EMT efficiency. The study also highlights areas for improvement toward future AI-driven EMS systems.

Abstract: Emergency Medical Technicians (EMTs) operate in high-pressure environments, making rapid, life-critical decisions under heavy cognitive and operational loads. We present EMSGlass, a smart-glasses system powered by EMSNet, the first multimodal multitask model for Emergency Medical Services (EMS), and EMSServe, a low-latency multimodal serving framework tailored to EMS scenarios. EMSNet integrates text, vital signs, and scene images to construct a unified real-time understanding of EMS incidents. Trained on real-world multimodal EMS datasets, EMSNet simultaneously supports up to five critical EMS tasks with superior accuracy compared to state-of-the-art unimodal baselines. Built on top of PyTorch, EMSServe introduces a modality-aware model splitter and a feature caching mechanism, achieving adaptive and efficient inference across heterogeneous hardware while addressing the challenge of asynchronous modality arrival in the field. By optimizing multimodal inference execution in EMS scenarios, EMSServe achieves 1.9x -- 11.7x speedup over direct PyTorch multimodal inference. A user study evaluation with six professional EMTs demonstrates that EMSGlass enhances real-time situational awareness, decision-making speed, and operational efficiency through intuitive on-glass interaction. In addition, qualitative insights from the user study provide actionable directions for extending EMSGlass toward next-generation AI-enabled EMS systems, bridging multimodal intelligence with real-world emergency response workflows.

</details>


### [753] [Real-time prediction of breast cancer sites using deformation-aware graph neural network](https://arxiv.org/abs/2511.13082)
*Kyunghyun Lee,Yong-Min Shin,Minwoo Shin,Jihun Kim,Sunghwan Lim,Won-Yong Shin,Kyungho Yoon*

Main category: cs.LG

TL;DR: The paper proposes a graph neural network (GNN)-based model to predict real-time tumor displacement for MRI-guided breast cancer biopsy, achieving high accuracy and improved computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in indirect MRI-guided biopsy, which faces difficulties in creating accurate real-time deformable breast models, essential for enhanced precision in breast cancer diagnosis.

Method: The study developed a patient-specific finite element model using MR-derived breast and tumor structural data to simulate deformation, integrating a GNN that processes graph data for real-time tissue displacement prediction.

Result: Validation on phantom and real patient datasets showed high accuracy (displacement RMSE within 0.2mm, DSC of 0.977), real-time inference capability, and a 4,000x speed-up compared to conventional FE simulations.

Conclusion: The deformation-aware GNN model offers a practical and accurate solution for real-time breast tumor displacement prediction, enhancing the precision of indirect MRI-guided biopsies.

Abstract: Early diagnosis of breast cancer is crucial, enabling the establishment of appropriate treatment plans and markedly enhancing patient prognosis. While direct magnetic resonance imaging-guided biopsy demonstrates promising performance in detecting cancer lesions, its practical application is limited by prolonged procedure times and high costs. To overcome these issues, an indirect MRI-guided biopsy that allows the procedure to be performed outside of the MRI room has been proposed, but it still faces challenges in creating an accurate real-time deformable breast model. In our study, we tackled this issue by developing a graph neural network (GNN)-based model capable of accurately predicting deformed breast cancer sites in real time during biopsy procedures. An individual-specific finite element (FE) model was developed by incorporating magnetic resonance (MR) image-derived structural information of the breast and tumor to simulate deformation behaviors. A GNN model was then employed, designed to process surface displacement and distance-based graph data, enabling accurate prediction of overall tissue displacement, including the deformation of the tumor region. The model was validated using phantom and real patient datasets, achieving an accuracy within 0.2 millimeters (mm) for cancer node displacement (RMSE) and a dice similarity coefficient (DSC) of 0.977 for spatial overlap with actual cancerous regions. Additionally, the model enabled real-time inference and achieved a speed-up of over 4,000 times in computational cost compared to conventional FE simulations. The proposed deformation-aware GNN model offers a promising solution for real-time tumor displacement prediction in breast biopsy, with high accuracy and real-time capability. Its integration with clinical procedures could significantly enhance the precision and efficiency of breast cancer diagnosis.

</details>


### [754] [Transformer-Based Scalable Multi-Agent Reinforcement Learning for Networked Systems with Long-Range Interactions](https://arxiv.org/abs/2511.13103)
*Vidur Sinha,Muhammed Ustaomeroglu,Guannan Qu*

Main category: cs.LG

TL;DR: STACCA introduces a transformer-based framework for MARL, addressing long-range interaction and network generalization challenges.


<details>
  <summary>Details</summary>
Motivation: To overcome MARL challenges of limited ability to capture long-range dependencies and poor generalization across network topologies.

Method: The STACCA framework uses a Graph Transformer Critic for modeling long-range dependencies and a shared Graph Transformer Actor for a generalizable policy. It also incorporates a novel counterfactual advantage estimator for effective credit assignment.

Result: STACCA showed improved performance, scalability, and generalization in epidemic containment and rumor-spreading network control tasks.

Conclusion: Transformer-based MARL architectures like STACCA are effective in scalable, generalizable control of large-scale networks.

Abstract: Multi-agent reinforcement learning (MARL) has shown promise for large-scale network control, yet existing methods face two major limitations. First, they typically rely on assumptions leading to decay properties of local agent interactions, limiting their ability to capture long-range dependencies such as cascading power failures or epidemic outbreaks. Second, most approaches lack generalizability across network topologies, requiring retraining when applied to new graphs. We introduce STACCA (Shared Transformer Actor-Critic with Counterfactual Advantage), a unified transformer-based MARL framework that addresses both challenges. STACCA employs a centralized Graph Transformer Critic to model long-range dependencies and provide system-level feedback, while its shared Graph Transformer Actor learns a generalizable policy capable of adapting across diverse network structures. Further, to improve credit assignment during training, STACCA integrates a novel counterfactual advantage estimator that is compatible with state-value critic estimates. We evaluate STACCA on epidemic containment and rumor-spreading network control tasks, demonstrating improved performance, network generalization, and scalability. These results highlight the potential of transformer-based MARL architectures to achieve scalable and generalizable control in large-scale networked systems.

</details>


### [755] [Synthetic Forgetting without Access: A Few-shot Zero-glance Framework for Machine Unlearning](https://arxiv.org/abs/2511.13116)
*Qipeng Song,Nan Yang,Ziqi Xu,Yue Li,Wei Shao,Feng Xia*

Main category: cs.LG

TL;DR: GFOES framework enables effective machine unlearning under limited data access without original data.


<details>
  <summary>Details</summary>
Motivation: Address the impractical assumption of full training dataset access in most machine unlearning methods.

Method: Introduced GFOES framework with Generative Feedback Network and two-phase fine-tuning for synthesizing erasure samples.

Result: Demonstrated effective forgetting and performance preservation using only 5% of the original data across three datasets.

Conclusion: GFOES provides a scalable and privacy-preserving approach to machine unlearning in data-constrained scenarios.

Abstract: Machine unlearning aims to eliminate the influence of specific data from trained models to ensure privacy compliance. However, most existing methods assume full access to the original training dataset, which is often impractical. We address a more realistic yet challenging setting: few-shot zero-glance, where only a small subset of the retained data is available and the forget set is entirely inaccessible. We introduce GFOES, a novel framework comprising a Generative Feedback Network (GFN) and a two-phase fine-tuning procedure. GFN synthesises Optimal Erasure Samples (OES), which induce high loss on target classes, enabling the model to forget class-specific knowledge without access to the original forget data, while preserving performance on retained classes. The two-phase fine-tuning procedure enables aggressive forgetting in the first phase, followed by utility restoration in the second. Experiments on three image classification datasets demonstrate that GFOES achieves effective forgetting at both logit and representation levels, while maintaining strong performance using only 5% of the original data. Our framework offers a practical and scalable solution for privacy-preserving machine learning under data-constrained conditions.

</details>


### [756] [Departures: Distributional Transport for Single-Cell Perturbation Prediction with Neural Schrödinger Bridges](https://arxiv.org/abs/2511.13124)
*Changxi Chi,Yufei Huang,Jun Xia,Jiangbin Zheng,Yunfan Liu,Zelin Zang,Stan Z. Li*

Main category: cs.LG

TL;DR: This paper proposes a method to model unpaired single-cell perturbation data using Schrödinger Bridge approximations and achieves state-of-the-art performance on single-cell genetic and drug perturbation datasets.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the accuracy and scalability of predicting single-cell perturbation outcomes, which is crucial for advancing gene function analysis and drug development.

Method: The paper approximates Schrödinger Bridge through Minibatch-Optimal Transport pairing to avoid bidirectional modeling, facilitating direct alignment of control and perturbed single-cell population distributions under different conditions.

Result: Joint training of SB models for discrete and continuous gene states enables effective modeling. Experiments confirm its capability to capture single-cell heterogeneity and outperform existing methods.

Conclusion: This approach proves to be a scalable and accurate solution for understanding unpaired single-cell perturbation data, aiding biomedical research and translational applications.

Abstract: Predicting single-cell perturbation outcomes directly advances gene function analysis and facilitates drug candidate selection, making it a key driver of both basic and translational biomedical research. However, a major bottleneck in this task is the unpaired nature of single-cell data, as the same cell cannot be observed both before and after perturbation due to the destructive nature of sequencing. Although some neural generative transport models attempt to tackle unpaired single-cell perturbation data, they either lack explicit conditioning or depend on prior spaces for indirect distribution alignment, limiting precise perturbation modeling. In this work, we approximate Schrödinger Bridge (SB), which defines stochastic dynamic mappings recovering the entropy-regularized optimal transport (OT), to directly align the distributions of control and perturbed single-cell populations across different perturbation conditions. Unlike prior SB approximations that rely on bidirectional modeling to infer optimal source-target sample coupling, we leverage Minibatch-OT based pairing to avoid such bidirectional inference and the associated ill-posedness of defining the reverse process. This pairing directly guides bridge learning, yielding a scalable approximation to the SB. We approximate two SB models, one modeling discrete gene activation states and the other continuous expression distributions. Joint training enables accurate perturbation modeling and captures single-cell heterogeneity. Experiments on public genetic and drug perturbation datasets show that our model effectively captures heterogeneous single-cell responses and achieves state-of-the-art performance.

</details>


### [757] [Soft Conflict-Resolution Decision Transformer for Offline Multi-Task Reinforcement Learning](https://arxiv.org/abs/2511.13133)
*Shudong Wang,Xinfei Wang,Chenhao Zhang,Shanchen Pang,Haiyuan Gui,Wenhao Ji,Xiaojian Liao*

Main category: cs.LG

TL;DR: This paper proposes a method for addressing gradient conflicts in multi-task reinforcement learning using dynamic parameter adjustment and a task-specific sparsity strategy.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to improve multi-task reinforcement learning by addressing gradient conflicts across tasks, which hinder knowledge sharing and generalization.

Method: The method involves using dynamic masks adjusted by Fisher information to retain important parameters and suppress conflicting ones. It also employs a dynamic sparsity adjustment strategy with interquartile range and asymmetric cosine annealing schedules.

Result: The proposed method (SoCo-DT) demonstrated a performance improvement of 7.6% on MT50 tasks and 10.5% on suboptimal datasets compared to state-of-the-art methods.

Conclusion: SoCo-DT effectively mitigates gradient conflicts and improves multi-task reinforcement learning performance, making it a promising solution for knowledge sharing across tasks without stability issues.

Abstract: Multi-task reinforcement learning (MTRL) seeks to learn a unified policy for diverse tasks, but often suffers from gradient conflicts across tasks. Existing masking-based methods attempt to mitigate such conflicts by assigning task-specific parameter masks. However, our empirical study shows that coarse-grained binary masks have the problem of over-suppressing key conflicting parameters, hindering knowledge sharing across tasks. Moreover, different tasks exhibit varying conflict levels, yet existing methods use a one-size-fits-all fixed sparsity strategy to keep training stability and performance, which proves inadequate. These limitations hinder the model's generalization and learning efficiency.
  To address these issues, we propose SoCo-DT, a Soft Conflict-resolution method based by parameter importance. By leveraging Fisher information, mask values are dynamically adjusted to retain important parameters while suppressing conflicting ones. In addition, we introduce a dynamic sparsity adjustment strategy based on the Interquartile Range (IQR), which constructs task-specific thresholding schemes using the distribution of conflict and harmony scores during training. To enable adaptive sparsity evolution throughout training, we further incorporate an asymmetric cosine annealing schedule to continuously update the threshold. Experimental results on the Meta-World benchmark show that SoCo-DT outperforms the state-of-the-art method by 7.6% on MT50 and by 10.5% on the suboptimal dataset, demonstrating its effectiveness in mitigating gradient conflicts and improving overall multi-task performance.

</details>


### [758] [Personalized Federated Learning with Bidirectional Communication Compression via One-Bit Random Sketching](https://arxiv.org/abs/2511.13144)
*Jiacheng Cheng,Xu Zhang,Guanghui Qiu,Yifang Zhang,Yinchuan Li,Kaiyuan Feng*

Main category: cs.LG

TL;DR: pFed1BS is a personalized federated learning framework that reduces communication costs using one-bit random sketching and maintains competitive performance.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenges of communication overhead and client-side data heterogeneity in federated learning.

Method: The framework employs one-bit random sketching for communication compression, a sign-based regularizer for personalization, and the Fast Hadamard Transform to mitigate computational costs.

Result: Numerical simulations validate that pFed1BS significantly reduces communication costs while maintaining competitive performance.

Conclusion: The proposed pFed1BS offers an efficient solution to the challenges in federated learning by balancing personalization with reduced communication demands.

Abstract: Federated Learning (FL) enables collaborative training across decentralized data, but faces key challenges of bidirectional communication overhead and client-side data heterogeneity. To address communication costs while embracing data heterogeneity, we propose pFed1BS, a novel personalized federated learning framework that achieves extreme communication compression through one-bit random sketching. In personalized FL, the goal shifts from training a single global model to creating tailored models for each client. In our framework, clients transmit highly compressed one-bit sketches, and the server aggregates and broadcasts a global one-bit consensus. To enable effective personalization, we introduce a sign-based regularizer that guides local models to align with the global consensus while preserving local data characteristics. To mitigate the computational burden of random sketching, we employ the Fast Hadamard Transform for efficient projection. Theoretical analysis guarantees that our algorithm converges to a stationary neighborhood of the global potential function. Numerical simulations demonstrate that pFed1BS substantially reduces communication costs while achieving competitive performance compared to advanced communication-efficient FL algorithms.

</details>


### [759] [OTARo: Once Tuning for All Precisions toward Robust On-Device LLMs](https://arxiv.org/abs/2511.13147)
*Shaoyuan Chen,Zhixuan Chen,Dawei Yang,Zhihang Yuan,Qiang Wu*

Main category: cs.LG

TL;DR: OTARo enables flexible and robust quantization precision switching for on-device LLMs, improving adaptability and overcoming challenges of conventional quantization.


<details>
  <summary>Details</summary>
Motivation: Current quantization methods lack flexibility and robustness needed for varying on-device tasks, as they fail to support precision switching effectively.

Method: OTARo uses Shared Exponent Floating Point (SEFP) and introduces methods like Bit-Width Path Search (BPS) and Low-Precision Asynchronous Accumulation (LAA) to facilitate training and flexible quantization.

Result: OTARo demonstrates strong and consistent performance across all precisions in experiments with LLaMA models.

Conclusion: The proposed method overcomes limitations of conventional quantization, enabling adaptable and robust LLM deployment across variable precisions.

Abstract: Large Language Models (LLMs) fine-tuning techniques not only improve the adaptability to diverse downstream tasks, but also mitigate adverse effects of model quantization. Despite this, conventional quantization suffers from its structural limitation that hinders flexibility during the fine-tuning and deployment stages. Practical on-device tasks demand different quantization precisions (i.e. different bit-widths), e.g., understanding tasks tend to exhibit higher tolerance to reduced precision compared to generation tasks. Conventional quantization, typically relying on scaling factors that are incompatible across bit-widths, fails to support the on-device switching of precisions when confronted with complex real-world scenarios. To overcome the dilemma, we propose OTARo, a novel method that enables on-device LLMs to flexibly switch quantization precisions while maintaining performance robustness through once fine-tuning. OTARo introduces Shared Exponent Floating Point (SEFP), a distinct quantization mechanism, to produce different bit-widths through simple mantissa truncations of a single model. Moreover, to achieve bit-width robustness in downstream applications, OTARo performs a learning process toward losses induced by different bit-widths. The method involves two critical strategies: (1) Exploitation-Exploration Bit-Width Path Search (BPS), which iteratively updates the search path via a designed scoring mechanism; (2) Low-Precision Asynchronous Accumulation (LAA), which performs asynchronous gradient accumulations and delayed updates under low bit-widths. Experiments on popular LLMs, e.g., LLaMA3.2-1B, LLaMA3-8B, demonstrate that OTARo achieves consistently strong and robust performance for all precisions.

</details>


### [760] [Warm-starting active-set solvers using graph neural networks](https://arxiv.org/abs/2511.13174)
*Ella J. Schmidtobreick,Daniel Arnström,Paul Häusner,Jens Sjölund*

Main category: cs.LG

TL;DR: The paper introduces a graph neural network (GNN) approach to predict active sets in a dual active-set solver for quadratic programming (QP), significantly reducing computational iterations and enabling scalability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the computational cost associated with quadratic programming (QP) solvers, which limits their use in time-sensitive applications like real-time control.

Method: The authors propose using graph neural networks (GNNs) to model QPs as bipartite graphs, enabling them to predict optimal active sets to warm-start the dual active-set solver (DAQP).

Result: The GNN reduces the number of solver iterations compared to a cold start, performing comparably to a multilayer perceptron (MLP). It generalizes well to previously unseen dimensions, demonstrating scalability.

Conclusion: Structure-aware learning via GNNs can effectively accelerate QP solvers, potentially advancing real-time applications like model predictive control.

Abstract: Quadratic programming (QP) solvers are widely used in real-time control and optimization, but their computational cost often limits applicability in time-critical settings. We propose a learning-to-optimize approach using graph neural networks (GNNs) to predict active sets in the dual active-set solver DAQP. The method exploits the structural properties of QPs by representing them as bipartite graphs and learning to identify the optimal active set for efficiently warm-starting the solver. Across varying problem sizes, the GNN consistently reduces the number of solver iterations compared to cold-starting, while performance is comparable to a multilayer perceptron (MLP) baseline. Furthermore, a GNN trained on varying problem sizes generalizes effectively to unseen dimensions, demonstrating flexibility and scalability. These results highlight the potential of structure-aware learning to accelerate optimization in real-time applications such as model predictive control.

</details>


### [761] [Real-time distortion prediction in metallic additive manufacturing via a physics-informed neural operator approach](https://arxiv.org/abs/2511.13178)
*Mingxuan Tian,Haochen Mu,Donghong Ding,Mengjiao Li,Yuhan Ding,Jianping Zhao*

Main category: cs.LG

TL;DR: This paper introduces a Physics-informed Neural Operator (PINO) model for real-time distortion prediction in metal Additive Manufacturing, addressing limitations of numerical simulations and conventional machine learning methods.


<details>
  <summary>Details</summary>
Motivation: There is a growing need for real-time prediction of distortion fields to control defects in metal Additive Manufacturing, as digital twins and smart manufacturing advance.

Method: The study combines a Physics-informed Deep Operator Network-Recurrent Neural Network (PIDeepONet-RNN), which decouples thermo-mechanical responses and processes temperature history and distortion fields using physics-based constraints.

Result: The PINO model performed with high accuracy and time efficiency, achieving low absolute errors of 0.9733 mm and 0.2049 mm in z and y-directions, respectively, and effectively mapping thermal history to distortion fields.

Conclusion: The PINO model demonstrates potential for real-time, long-horizon prediction of distortion fields, supporting defect control in Additive Manufacturing and improving physical consistency in predictions.

Abstract: With the development of digital twins and smart manufacturing systems, there is an urgent need for real-time distortion field prediction to control defects in metal Additive Manufacturing (AM). However, numerical simulation methods suffer from high computational cost, long run-times that prevent real-time use, while conventional Machine learning (ML) models struggle to extract spatiotemporal features for long-horizon prediction and fail to decouple thermo-mechanical fields. This paper proposes a Physics-informed Neural Operator (PINO) to predict z and y-direction distortion for the future 15 s. Our method, Physics-informed Deep Operator Network-Recurrent Neural Network (PIDeepONet-RNN) employs trunk and branch network to process temperature history and encode distortion fields, respectively, enabling decoupling of thermo-mechanical responses. By incorporating the heat conduction equation as a soft constraint, the model ensures physical consistency and suppresses unphysical artifacts, thereby establishing a more physically consistent mapping between the thermal history and distortion. This is important because such a basis function, grounded in physical laws, provides a robust and interpretable foundation for predictions. The proposed models are trained and tested using datasets generated from experimentally validated Finite Element Method (FEM). Evaluation shows that the model achieves high accuracy, low error accumulation, time efficiency. The max absolute errors in the z and y-directions are as low as 0.9733 mm and 0.2049 mm, respectively. The error distribution shows high errors in the molten pool but low gradient norms in the deposited and key areas. The performance of PINO surrogate model highlights its potential for real-time long-horizon physics field prediction in controlling defects.

</details>


### [762] [Uncertainty-aware Physics-informed Neural Networks for Robust CARS-to-Raman Signal Reconstruction](https://arxiv.org/abs/2511.13185)
*Aishwarya Venkataramanan,Sai Karthikeya Vemuri,Adithya Ashok Chalain Valapil,Joachim Denzler*

Main category: cs.LG

TL;DR: This paper addresses the challenge of reconstructing true Raman spectra from CARS data by evaluating uncertainty quantification techniques and enhancing model reliability with physics-informed constraints.


<details>
  <summary>Details</summary>
Motivation: The effectiveness of CARS spectroscopy is compromised by non-resonant background interference, necessitating tools for accurate reconstruction of Raman spectra for reliable use in critical applications.

Method: Various uncertainty quantification techniques are evaluated alongside physics-informed constraints to enhance accuracy and reliability in CARS-to-Raman signal reconstruction.

Result: Incorporation of physics-informed constraints improved model calibration and reliability for Raman spectrum reconstruction compared to traditional methods.

Conclusion: Physics-informed constraints and uncertainty quantification techniques offer a promising direction for trustworthy and effective CARS data analysis, particularly for high-stakes applications.

Abstract: Coherent anti-Stokes Raman scattering (CARS) spectroscopy is a powerful and rapid technique widely used in medicine, material science, and chemical analyses. However, its effectiveness is hindered by the presence of a non-resonant background that interferes with and distorts the true Raman signal. Deep learning methods have been employed to reconstruct the true Raman spectrum from measured CARS data using labeled datasets. A more recent development integrates the domain knowledge of Kramers-Kronig relationships and smoothness constraints in the form of physics-informed loss functions. However, these deterministic models lack the ability to quantify uncertainty, an essential feature for reliable deployment in high-stakes scientific and biomedical applications. In this work, we evaluate and compare various uncertainty quantification (UQ) techniques within the context of CARS-to-Raman signal reconstruction. Furthermore, we demonstrate that incorporating physics-informed constraints into these models improves their calibration, offering a promising path toward more trustworthy CARS data analysis.

</details>


### [763] [DiffFP: Learning Behaviors from Scratch via Diffusion-based Fictitious Play](https://arxiv.org/abs/2511.13186)
*Akash Karthikeyan,Yash Vardhan Pant*

Main category: cs.LG

TL;DR: The paper proposes DiffFP, a framework leveraging diffusion policies for strategic adaptability in continuous-space games, achieving faster convergence and robust policies.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies in self-play reinforcement learning methods for continuous decision spaces, particularly in achieving Nash equilibrium and avoiding exploitation by unseen opponents.

Method: Development of DiffFP, a fictitious play framework using diffusion policies with generative modeling to approximate adaptive and diverse best-response strategies.

Result: DiffFP achieves $ε$-Nash equilibria in zero-sum games, showing faster convergence (up to 3x) and higher success rates (30x on average) in multi-agent environments compared to baseline methods.

Conclusion: DiffFP enhances adaptability, robustness, and convergence efficiency in competitive multi-agent settings, providing significant advancements over traditional RL-based approaches.

Abstract: Self-play reinforcement learning has demonstrated significant success in learning complex strategic and interactive behaviors in competitive multi-agent games. However, achieving such behaviors in continuous decision spaces remains challenging. Ensuring adaptability and generalization in self-play settings is critical for achieving competitive performance in dynamic multi-agent environments. These challenges often cause methods to converge slowly or fail to converge at all to a Nash equilibrium, making agents vulnerable to strategic exploitation by unseen opponents. To address these challenges, we propose DiffFP, a fictitious play (FP) framework that estimates the best response to unseen opponents while learning a robust and multimodal behavioral policy. Specifically, we approximate the best response using a diffusion policy that leverages generative modeling to learn adaptive and diverse strategies. Through empirical evaluation, we demonstrate that the proposed FP framework converges towards $ε$-Nash equilibria in continuous- space zero-sum games. We validate our method on complex multi-agent environments, including racing and multi-particle zero-sum games. Simulation results show that the learned policies are robust against diverse opponents and outperform baseline reinforcement learning policies. Our approach achieves up to 3$\times$ faster convergence and 30$\times$ higher success rates on average against RL-based baselines, demonstrating its robustness to opponent strategies and stability across training iterations

</details>


### [764] [ParaDySe: A Parallel-Strategy Switching Framework for Dynamic Sequence Lengths in Transformer](https://arxiv.org/abs/2511.13198)
*Zhixin Ou,Peng Liang,Jianchen Han,Baihui Liu,Linbo Qiao*

Main category: cs.LG

TL;DR: The paper introduces ParaDySe, an adaptive framework for optimizing dynamic sequence training in transformer-based language models, addressing issues like out-of-memory errors and inefficient parallelization.


<details>
  <summary>Details</summary>
Motivation: Training Transformer-based LLMs with varying sequence lengths faces inefficiencies from pre-defined static parallel strategies, leading to communication-parallel cancellation for short sequences and memory issues for long ones.

Method: ParaDySe employs modular function libraries, hybrid cost models for memory and time predictions, and an efficient heuristic algorithm to dynamically and optimally switch parallel strategies during training.

Result: Experimental results show that ParaDySe resolves out-of-memory and communication-parallel cancellation challenges in LLM training with long sequences, enabling efficient optimization.

Conclusion: ParaDySe effectively integrates long-sequence optimizations and provides a robust, dynamic solution for LLM training, improving performance and resource utilization.

Abstract: Dynamic sequences with varying lengths have been widely used in the training of Transformer-based large language models (LLMs). However, current training frameworks adopt a pre-defined static parallel strategy for these sequences, causing neither communication-parallelization cancellation on short sequences nor out-of-memory on long sequences. To mitigate these issues, we propose ParaDySe, a novel adaptive Parallel strategy switching framework for Dynamic Sequences. ParaDySe enables on-the-fly optimal strategy adoption according to the immediate input sequence. It first implements the modular function libraries for parallel strategies with unified tensor layout specifications, and then builds sequence-aware memory and time cost models with hybrid methods. Guided by cost models, ParaDySe selects optimal layer-wise strategies for dynamic sequences via an efficient heuristic algorithm. By integrating these techniques together, ParaDySe achieves seamless hot-switching of optimal strategies through its well-designed function libraries. We compare ParaDySe with baselines on representative LLMs under datasets with sequence lengths up to 624K. Experimental results indicate that ParaDySe addresses OOM and CPC bottlenecks in LLM training by systematically integrating long-sequence optimizations with existing frameworks.

</details>


### [765] [TokenSqueeze: Performance-Preserving Compression for Reasoning LLMs](https://arxiv.org/abs/2511.13223)
*Yuxiang Zhang,Zhengxu Yu,Weihang Pan,Zhongming Jin,Qiang Fu,Deng Cai,Binbin Lin,Jieping Ye*

Main category: cs.LG

TL;DR: TokenSqueeze reduces token usage by 50% on reasoning tasks while maintaining accuracy, using self-generated data for efficient performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the efficiency-accuracy tradeoff of reasoning LLMs which face increased token usage, memory consumption, and latency due to generating long chain-of-thought traces.

Method: TokenSqueeze selects self-generated reasoning samples adaptive to problem complexity, and introduces a linguistically refined distribution-aligned compression method, preserving logical paths and clarity.

Result: Experimental results showed TokenSqueeze achieved 50% token reduction while maintaining accuracy, especially on benchmarks like MATH500 using DeepSeek-R1-Distill-Qwen-7B.

Conclusion: TokenSqueeze presents an effective balance between reasoning efficiency and accuracy, using self-generated methods to optimize token usage, enabling practical LLM deployment without external datasets.

Abstract: Emerging reasoning LLMs such as OpenAI-o1 and DeepSeek-R1 have achieved strong performance on complex reasoning tasks by generating long chain-of-thought (CoT) traces. However, these long CoTs result in increased token usage, leading to higher inference latency and memory consumption. As a result, balancing accuracy and reasoning efficiency has become essential for deploying reasoning LLMs in practical applications. Existing long-to-short (Long2Short) methods aim to reduce inference length but often sacrifice accuracy, revealing a need for an approach that maintains performance while lowering token costs. To address this efficiency-accuracy tradeoff, we propose TokenSqueeze, a novel Long2Short method that condenses reasoning paths while preserving performance and relying exclusively on self-generated data. First, to prevent performance degradation caused by excessive compression of reasoning depth, we propose to select self-generated samples whose reasoning depth is adaptively matched to the complexity of the problem. To further optimize the linguistic expression without altering the underlying reasoning paths, we introduce a distribution-aligned linguistic refinement method that enhances the clarity and conciseness of the reasoning path while preserving its logical integrity. Comprehensive experimental results demonstrate the effectiveness of TokenSqueeze in reducing token usage while maintaining accuracy. Notably, DeepSeek-R1-Distill-Qwen-7B fine-tuned using our proposed method achieved a 50\% average token reduction while preserving accuracy on the MATH500 benchmark. TokenSqueeze exclusively utilizes the model's self-generated data, enabling efficient and high-fidelity reasoning without relying on manually curated short-answer datasets across diverse applications. Our code is available at https://github.com/zhangyx1122/TokenSqueeze.

</details>


### [766] [MorphBoost: Self-Organizing Universal Gradient Boosting with Adaptive Tree Morphing](https://arxiv.org/abs/2511.13234)
*Boris Kriuk*

Main category: cs.LG

TL;DR: MorphBoost introduces dynamic, self-adaptive tree structures to gradient boosting, showing state-of-the-art performance compared to existing models like XGBoost and LightGBM.


<details>
  <summary>Details</summary>
Motivation: Traditional gradient boosting models use static tree structures, limiting flexibility and adaptation to evolving gradient distributions during training.

Method: MorphBoost employs dynamically morphing tree structures and adaptive split functions influenced by gradient statistics and problem complexity. It includes innovations like morphing split criteria, automatic task-specific parameter configuration, vectorized tree predictions, and advanced optimization techniques.

Result: MorphBoost outperforms XGBoost by an average of 0.84%, achieving state-of-the-art results across 10 datasets, with a 40% win rate and the lowest variance among models tested.

Conclusion: Dynamic adaptation in MorphBoost leads to improved consistency, robustness, and competitiveness, particularly on advanced machine learning problems compared to traditional gradient boosting approaches.

Abstract: Traditional gradient boosting algorithms employ static tree structures with fixed splitting criteria that remain unchanged throughout training, limiting their ability to adapt to evolving gradient distributions and problem-specific characteristics across different learning stages. This work introduces MorphBoost, a new gradient boosting framework featuring self-organizing tree structures that dynamically morph their splitting behavior during training. The algorithm implements adaptive split functions that evolve based on accumulated gradient statistics and iteration-dependent learning pressures, enabling automatic adjustment to problem complexity. Key innovations include: (1) morphing split criterion combining gradient-based scores with information-theoretic metrics weighted by training progress; (2) automatic problem fingerprinting for intelligent parameter configuration across binary/multiclass/regression tasks; (3) vectorized tree prediction achieving significant computational speedups; (4) interaction-aware feature importance detecting multiplicative relationships; and (5) fast-mode optimization balancing speed and accuracy. Comprehensive benchmarking across 10 diverse datasets against competitive models (XGBoost, LightGBM, GradientBoosting, HistGradientBoosting, ensemble methods) demonstrates that MorphBoost achieves state-of-the-art performance, outperforming XGBoost by 0.84% on average. MorphBoost secured the overall winner position with 4/10 dataset wins (40% win rate) and 6/30 top-3 finishes (20%), while maintaining the lowest variance (σ=0.0948) and highest minimum accuracy across all models, revealing superior consistency and robustness. Performance analysis across difficulty levels shows competitive results on easy datasets while achieving notable improvements on advanced problems due to higher adaptation levels.

</details>


### [767] [Incoherent Beliefs & Inconsistent Actions in Large Language Models](https://arxiv.org/abs/2511.13240)
*Arka Pal,Teo Kitanovski,Arthur Liang,Akilesh Potti,Micah Goldblum*

Main category: cs.LG

TL;DR: This paper examines challenges in predicting large language models' (LLMs) performance in complex, real-world tasks requiring sequential interactions, belief updates, and decision-making.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the gap between LLM performance on static benchmarks versus dynamic, real-world environments that involve complexities such as belief updating and consistent decision-making based on those beliefs.

Method: The authors assessed LLMs on two critical benchmarks: their ability to coherently update beliefs after new evidence and their ability to take actions consistent with those updated beliefs. Specific tests, including a betting market, were utilized.

Result: The study reveals that LLMs exhibit significant inconsistencies, with up to a 30% discrepancy in belief updates and actions that contradict their internally held beliefs. This remains true even for high-performing and well-calibrated models, highlighting limitations in dynamic scenarios.

Conclusion: LLMs face challenges in belief coherence, action consistency, and generalizing to dynamic, real-world settings, even when strong performance is observed on static datasets.

Abstract: Real-world tasks and environments exhibit differences from the static datasets that large language models (LLMs) are typically evaluated on. Such tasks can involve sequential interaction, requiring coherent updating of beliefs in light of new evidence, and making appropriate decisions based on those beliefs. Predicting how LLMs will perform in such dynamic environments is important, but can be tricky to determine from measurements in static settings. In this work, we examine two critical components of LLM performance: the ability of LLMs to coherently update their beliefs, and the extent to which the actions they take are consistent with those beliefs. First, we find that LLMs are largely inconsistent in how they update their beliefs; models can exhibit up to a 30% average difference between the directly elicited posterior, and the correct update of their prior. Second, we find that LLMs also often take actions which are inconsistent with the beliefs they hold. On a betting market, for example, LLMs often do not even bet in the same direction as their internally held beliefs over the underlying outcomes. We also find they have moderate self-inconsistency in how they respond to challenges by users to given answers. Finally, we show that the above properties hold even for strong models that obtain high accuracy or that are well-calibrated on the tasks at hand. Our results highlight the difficulties of predicting LLM behavior in complex real-world settings.

</details>


### [768] [Uncovering and Mitigating Transient Blindness in Multimodal Model Editing](https://arxiv.org/abs/2511.13243)
*Xiaoqi Han,Ru Li,Ran Yi,Hongye Tan,Zhuomin Liang,Víctor Gutiérrez-Basulto,Jeff Z. Pan*

Main category: cs.LG

TL;DR: Multimodal Model Editing is evaluated using a new framework addressing transient blindness and offering locality-aware improvements.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address shortcomings in multimodal model editing evaluation, such as over-reliance on random or low-similarity inputs and overfitting during edits.

Method: The authors propose a locality evaluation framework with three dimensions and seven data types, alongside De-VQA for dynamic visual question answering evaluation.

Result: Their method reduces transient blindness and enhances locality by 17% compared to existing baselines.

Conclusion: The framework and proposed approach provide balanced multimodal model editing, resolving transient blindness issues and offering a thorough evaluation structure.

Abstract: Multimodal Model Editing (MMED) aims to correct erroneous knowledge in multimodal models. Existing evaluation methods, adapted from textual model editing, overstate success by relying on low-similarity or random inputs, obscure overfitting. We propose a comprehensive locality evaluation framework, covering three key dimensions: random-image locality, no-image locality, and consistent-image locality, operationalized through seven distinct data types, enabling a detailed and structured analysis of multimodal edits. We introduce De-VQA, a dynamic evaluation for visual question answering, uncovering a phenomenon we term transient blindness, overfitting to edit-similar text while ignoring visuals. Token analysis shows edits disproportionately affect textual tokens. We propose locality-aware adversarial losses to balance cross-modal representations. Empirical results demonstrate that our approach consistently outperforms existing baselines, reducing transient blindness and improving locality by 17% on average.

</details>


### [769] [Seek and You Shall Fold](https://arxiv.org/abs/2511.13244)
*Nadav Bojan Sellam,Meital Bojan,Paul Schanda,Alex Bronstein*

Main category: cs.LG

TL;DR: The paper introduces a novel framework that uses non-differentiable guidance to improve protein generative models by incorporating experimental data such as chemical shifts, pairwise distance constraints, and nuclear Overhauser effect restraints.


<details>
  <summary>Details</summary>
Motivation: Currently, the integration of experimental data into protein generative models is challenging because most predictors of experimental observables are non-differentiable, which prevents their use with gradient-based methods.

Method: The authors propose coupling a continuous diffusion-based generator with any black-box objective using a tailored genetic algorithm, enabling non-differentiable guidance.

Result: The framework effectively integrates various experimental data types, including chemical shifts for the first time, demonstrating feasibility, exposing weaknesses in existing predictors, and providing a versatile strategy for data-conditioned protein modeling.

Conclusion: The research showcases that chemical shift guided structure generation is achievable and highlights a general methodology for enhancing protein generative models by leveraging diverse experimental signals, paving the way for automated and broader protein modeling applications.

Abstract: Accurate protein structures are essential for understanding biological function, yet incorporating experimental data into protein generative models remains a major challenge. Most predictors of experimental observables are non-differentiable, making them incompatible with gradient-based conditional sampling. This is especially limiting in nuclear magnetic resonance, where rich data such as chemical shifts are hard to directly integrate into generative modeling. We introduce a framework for non-differentiable guidance of protein generative models, coupling a continuous diffusion-based generator with any black-box objective via a tailored genetic algorithm. We demonstrate its effectiveness across three modalities: pairwise distance constraints, nuclear Overhauser effect restraints, and for the first time chemical shifts. These results establish chemical shift guided structure generation as feasible, expose key weaknesses in current predictors, and showcase a general strategy for incorporating diverse experimental signals. Our work points toward automated, data-conditioned protein modeling beyond the limits of differentiability.

</details>


### [770] [Edge-aware baselines for ogbn-proteins in PyTorch Geometric: species-wise normalization, post-hoc calibration, and cost-accuracy trade-offs](https://arxiv.org/abs/2511.13250)
*Aleksandar Stanković,Dejan Lisica*

Main category: cs.LG

TL;DR: Edge-aware baselines for ogbn-proteins in PyTorch Geometric are studied, focusing on edge aggregation methods and normalization schemes, highlighting GraphSAGE with edge-to-node features.


<details>
  <summary>Details</summary>
Motivation: To establish reproducible, edge-aware baselines for ogbn-proteins, addressing key design decisions in practice like edge aggregation and edge usage in message passing.

Method: The study involves comparing aggregation methods like sum, mean, and max for edge evidence, and examining normalization techniques such as LayerNorm (LN), BatchNorm (BN), and Conditional LayerNorm (CLN). Various implementation choices were tested for their effect on accuracy and resource utilization.

Result: Sum-based edge aggregation consistently outperforms others. BN achieves the best AUC, while CLN provides competitive AUC with superior thresholded F1 scores. Further, per-label adjustments improve F1 and calibration with minimal AUC impact, complemented by light label-correlation smoothing.

Conclusion: GraphSAGE with sum-based edge-to-node features and BN emerges as the best performing combination. Reproducible artifacts and scripts are provided to standardize experimentation.

Abstract: We present reproducible, edge-aware baselines for ogbn-proteins in PyTorch Geometric (PyG). We study two system choices that dominate practice: (i) how 8-dimensional edge evidence is aggregated into node inputs, and (ii) how edges are used inside message passing. Our strongest baseline is GraphSAGE with sum-based edge-to-node features. We compare LayerNorm (LN), BatchNorm (BN), and a species-aware Conditional LayerNorm (CLN), and report compute cost (time, VRAM, parameters) together with accuracy (ROC-AUC) and decision quality. In our primary experimental setup (hidden size 512, 3 layers, 3 seeds), sum consistently beats mean and max; BN attains the best AUC, while CLN matches the AUC frontier with better thresholded F1. Finally, post-hoc per-label temperature scaling plus per-label thresholds substantially improves micro-F1 and expected calibration error (ECE) with negligible AUC change, and light label-correlation smoothing yields small additional gains. We release standardized artifacts and scripts used for all of the runs presented in the paper.

</details>


### [771] [Explainable RL Policies by Distilling to Locally-Specialized Linear Policies with Voronoi State Partitioning](https://arxiv.org/abs/2511.13322)
*Senne Deproost,Dennis Steckelmacher,Ann Nowé*

Main category: cs.LG

TL;DR: This paper addresses the lack of transparency in deep reinforcement learning controllers and proposes a knowledge distillation approach using Voronoi partitioning for explainable linear models.


<details>
  <summary>Details</summary>
Motivation: Deep reinforcement learning controllers are effective but lack transparency, which hinders trust and regulatory compliance.

Method: The authors propose a novel model-agnostic approach using Voronoi partitioning to divide the state space into regions for simplified, linear models.

Result: Experiments in gridworld and classic control tasks show the distilled linear models produce explainable policies with performance equal to or surpassing the original black-box controllers.

Conclusion: The study demonstrates that distillation into locally-specialized linear models not only provides transparency but also maintains or improves controller performance.

Abstract: Deep Reinforcement Learning is one of the state-of-the-art methods for producing near-optimal system controllers. However, deep RL algorithms train a deep neural network, that lacks transparency, which poses challenges when the controller has to meet regulations, or foster trust. To alleviate this, one could transfer the learned behaviour into a model that is human-readable by design using knowledge distilla- tion. Often this is done with a single model which mimics the original model on average but could struggle in more dynamic situations. A key challenge is that this simpler model should have the right balance be- tween flexibility and complexity or right balance between balance bias and accuracy. We propose a new model-agnostic method to divide the state space into regions where a simplified, human-understandable model can operate in. In this paper, we use Voronoi partitioning to find regions where linear models can achieve similar performance to the original con- troller. We evaluate our approach on a gridworld environment and a classic control task. We observe that our proposed distillation to locally- specialized linear models produces policies that are explainable and show that the distillation matches or even slightly outperforms the black-box policy they are distilled from.

</details>


### [772] [Tab-PET: Graph-Based Positional Encodings for Tabular Transformers](https://arxiv.org/abs/2511.13338)
*Yunze Leng,Rohan Ghosh,Mehul Motani*

Main category: cs.LG

TL;DR: The paper explores the use of positional encodings (PEs) to improve generalization performance in transformer models designed for tabular data and proposes a graph-based framework, Tab-PET, to integrate these encodings effectively.


<details>
  <summary>Details</summary>
Motivation: There is a need to enhance the performance of transformer models on tabular data, which lacks structural cues and positional information, unlike other data types such as vision and text.

Method: The authors propose Tab-PET, a framework that uses graph-based methodologies (association-based and causality-based) to compute positional encodings for tabular transformers.

Result: Experimental results across 50 classification and regression datasets show significant performance improvements when PEs are used, particularly with association-based graphs performing consistently better.

Conclusion: Positional encodings, derived from graph topology, can effectively address the challenges of generalization in tabular transformers, offering a novel method to enhance their performance.

Abstract: Supervised learning with tabular data presents unique challenges, including low data sizes, the absence of structural cues, and heterogeneous features spanning both categorical and continuous domains. Unlike vision and language tasks, where models can exploit inductive biases in the data, tabular data lacks inherent positional structure, hindering the effectiveness of self-attention mechanisms. While recent transformer-based models like TabTransformer, SAINT, and FT-Transformer (which we refer to as 3T) have shown promise on tabular data, they typically operate without leveraging structural cues such as positional encodings (PEs), as no prior structural information is usually available. In this work, we find both theoretically and empirically that structural cues, specifically PEs can be a useful tool to improve generalization performance for tabular transformers. We find that PEs impart the ability to reduce the effective rank (a form of intrinsic dimensionality) of the features, effectively simplifying the task by reducing the dimensionality of the problem, yielding improved generalization. To that end, we propose Tab-PET (PEs for Tabular Transformers), a graph-based framework for estimating and inculcating PEs into embeddings. Inspired by approaches that derive PEs from graph topology, we explore two paradigms for graph estimation: association-based and causality-based. We empirically demonstrate that graph-derived PEs significantly improve performance across 50 classification and regression datasets for 3T. Notably, association-based graphs consistently yield more stable and pronounced gains compared to causality-driven ones. Our work highlights an unexpected role of PEs in tabular transformers, revealing how they can be harnessed to improve generalization.

</details>


### [773] [Statistically Accurate and Robust Generative Prediction of Rock Discontinuities with A Tabular Foundation Model](https://arxiv.org/abs/2511.13339)
*Han Meng,Gang Mei,Hong Tian,Nengxiong Xu,Jianbing Peng*

Main category: cs.LG

TL;DR: A method using a foundation model effectively predicts rock mass discontinuities, better than current systems, even with sparse data.


<details>
  <summary>Details</summary>
Motivation: To improve the prediction of internal rock discontinuities, which are difficult to observe directly and critical for geotechnical safety.

Method: Employs a tabular foundation model with sample learning capabilities to statistically predict patterns in measured rock discontinuities from sparse surface data.

Result: The proposed approach outperforms statistical models and deep generative methods across ten diverse datasets, showing improved accuracy and robustness.

Conclusion: This method enhances the quantification of rock mass structures, contributing to safer and more reliable geotechnical designs.

Abstract: Rock discontinuities critically govern the mechanical behavior and stability of rock masses. Their internal distributions remain largely unobservable and are typically inferred from surface-exposed discontinuities using generative prediction approaches. However, surface-exposed observations are inherently sparse, and existing generative prediction approaches either fail to capture the underlying complex distribution patterns or lack robustness under data-sparse conditions. Here, we proposed a simple yet robust approach for statistically accurate generative prediction of rock discontinuities by utilizing a tabular foundation model. By leveraging the powerful sample learning capability of the foundation model specifically designed for small data, our approach can effectively capture the underlying complex distribution patterns within limited measured discontinuities. Comparative experiments on ten datasets with diverse scales and distribution patterns of discontinuities demonstrate superior accuracy and robustness over conventional statistical models and deep generative approaches. This work advances quantitative characterization of rock mass structures, supporting safer and more reliable data-driven geotechnical design.

</details>


### [774] [Dual-LoRA and Quality-Enhanced Pseudo Replay for Multimodal Continual Food Learning](https://arxiv.org/abs/2511.13351)
*Xinlan Wu,Bin Zhu,Feng Han,Pengkun Jiao,Jingjing Chen*

Main category: cs.LG

TL;DR: The paper introduces a method for continual learning in multimodal food analysis using a Dual-LoRA framework and improved pseudo replay strategies.


<details>
  <summary>Details</summary>
Motivation: Existing large multimodal models in food analysis are prone to catastrophic forgetting when learning new tasks, making retraining costly and inefficient.

Method: The continual learning framework employs a Dual-LoRA architecture with two types of low-rank adapters: specialized LoRA for task-specific knowledge and cooperative LoRA for shared knowledge across tasks, alongside a Quality-Enhanced Pseudo Replay strategy to improve replay data reliability.

Result: Experiments on the Uni-Food dataset demonstrate superior performance in preventing forgetting, proving the effectiveness of the proposed approach for complex food tasks.

Conclusion: This study presents a foundational continual learning method for multimodal food analysis, offering significant improvements in task acquisition without catastrophic forgetting.

Abstract: Food analysis has become increasingly critical for health-related tasks such as personalized nutrition and chronic disease prevention. However, existing large multimodal models (LMMs) in food analysis suffer from catastrophic forgetting when learning new tasks, requiring costly retraining from scratch. To address this, we propose a novel continual learning framework for multimodal food learning, integrating a Dual-LoRA architecture with Quality-Enhanced Pseudo Replay. We introduce two complementary low-rank adapters for each task: a specialized LoRA that learns task-specific knowledge with orthogonal constraints to previous tasks' subspaces, and a cooperative LoRA that consolidates shared knowledge across tasks via pseudo replay. To improve the reliability of replay data, our Quality-Enhanced Pseudo Replay strategy leverages self-consistency and semantic similarity to reduce hallucinations in generated samples. Experiments on the comprehensive Uni-Food dataset show superior performance in mitigating forgetting, representing the first effective continual learning approach for complex food tasks.

</details>


### [775] [A Novel Hierarchical Integration Method for Efficient Model Merging in Medical LLMs](https://arxiv.org/abs/2511.13373)
*Prakrit Timilsina,Anuj Nepal,Rajan Kadel,Robin Doss*

Main category: cs.LG

TL;DR: This paper assesses methods for integrating medical Large Language Models (LLMs), introducing a novel hierarchical approach that combines selective alignment and interpolation, achieving efficient knowledge consolidation in resource-constrained environments.


<details>
  <summary>Details</summary>
Motivation: To address challenges in distributed healthcare LLMs, including maintaining privacy, computational efficiency, and knowledge integration across institutions.

Method: Evaluating six parameter merging techniques, including Task Arithmetic, Linear Averaging, and their novel hierarchical method combining optimal transport alignment with cosine similarity-weighted interpolation.

Result: Architecturally compatible models perform well with simple methods like Task Arithmetic, achieving 45.80% accuracy on MedQA and outperforming more complex strategies.

Conclusion: Simple averaging methods provide a computationally efficient and robust baseline for distributed medical LLMs, supporting scalable and resource-efficient AI deployment.

Abstract: Large Language Models (LLMs) face significant challenges in distributed healthcare, including consolidating specialized domain knowledge across institutions while maintaining privacy, reducing computational overhead, and preventing catastrophic forgetting during model updates.This paper presents a systematic evaluation of six parameter-space merging techniques applied to two architecturally compatible medical LLMs derived from the Mistral-7B base model. We introduce a novel hierarchical method that combines selective Optimal Transport (OT) alignment for attention layers with cosine similarity-weighted interpolation, designed to address permutation variance while minimizing computational overhead for edge deployment scenarios. Our study evaluates Task Arithmetic, Linear Averaging, DARE-TIES, DELLA, Breadcrumbs, and our Hierarchical approach across five medical benchmarks. Results demonstrate that architecturally compatible models benefit significantly from simple averaging methods, with Task Arithmetic achieving 45.80% accuracy on MedQA, outperforming complex pruning-based approaches. These findings offer critical insights for the deployment of distributed medical AI in resource-constrained IoT environments, where computational efficiency and model compatibility are paramount. Our work establishes that for architecturally compatible models, simple averaging provides a robust and computationally efficient baseline for knowledge consolidation, offering a pragmatic path forward for scalable medical AI systems.

</details>


### [776] [Finding Kissing Numbers with Game-theoretic Reinforcement Learning](https://arxiv.org/abs/2511.13391)
*Chengdong Ma,Théo Tao Zhaowei,Pengyu Li,Minghao Liu,Haojun Chen,Zihao Mao,Yuan Cheng,Yuan Qi,Yaodong Yang*

Main category: cs.LG

TL;DR: The paper addresses the long-standing Kissing Number Problem, modeling it as a two-player matrix completion game using a game-theoretic reinforcement learning system (PackingStar), which has achieved breakthrough results in high-dimensional spaces.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome the challenges in solving the Kissing Number Problem, especially in high-dimensional spaces where traditional methods face combinatorial complexity and scalability issues.

Method: The problem is modeled as a two-player matrix completion game. PackingStar uses game-theoretic reinforcement learning, with matrix entries representing pairwise cosines of sphere center vectors in a cooperative dynamic to maximize matrix size and kissing number.

Result: PackingStar surpasses human-known records in dimensions 25-31, reproduces configurations from previous dimensions, and discovers thousands of new structures. It achieves breakthrough results in dimensions like 13, providing geometrically significant results.

Conclusion: AI, specifically PackingStar, showcases its utility in exploring high-dimensional spaces, surpassing human limitations and offering new directions in solving the Kissing Number Problem and related geometry challenges.

Abstract: Since Isaac Newton first studied the Kissing Number Problem in 1694, determining the maximal number of non-overlapping spheres around a central sphere has remained a fundamental challenge. This problem represents the local analogue of Hilbert's 18th problem on sphere packing, bridging geometry, number theory, and information theory. Although significant progress has been made through lattices and codes, the irregularities of high-dimensional geometry and exponentially growing combinatorial complexity beyond 8 dimensions, which exceeds the complexity of Go game, limit the scalability of existing methods. Here we model this problem as a two-player matrix completion game and train the game-theoretic reinforcement learning system, PackingStar, to efficiently explore high-dimensional spaces. The matrix entries represent pairwise cosines of sphere center vectors; one player fills entries while another corrects suboptimal ones, jointly maximizing the matrix size, corresponding to the kissing number. This cooperative dynamics substantially improves sample quality, making the extremely large spaces tractable. PackingStar reproduces previous configurations and surpasses all human-known records from dimensions 25 to 31, with the configuration in 25 dimensions geometrically corresponding to the Leech lattice and suggesting possible optimality. It achieves the first breakthrough beyond rational structures from 1971 in 13 dimensions and discovers over 6000 new structures in 14 and other dimensions. These results demonstrate AI's power to explore high-dimensional spaces beyond human intuition and open new pathways for the Kissing Number Problem and broader geometry problems.

</details>


### [777] [PAST: A Primary-Auxiliary Spatio-Temporal Network for Traffic Time Series Imputation](https://arxiv.org/abs/2511.13414)
*Hanwen Hu,Zimo Wen,Shiyou Qian,Jian Co*

Main category: cs.LG

TL;DR: The paper introduces a novel Primary-Auxiliary Spatio-Temporal network (PAST) to address complex traffic time series imputation problems under various missing data conditions, achieving superior accuracy compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Improve traffic time series imputation accuracy under challenging conditions such as random, fiber, and block missing data.

Method: The PAST model categorizes patterns into primary (internal relationships) and auxiliary (external factors). It uses a graph-integrated module and a cross-gated module to extract these patterns and trains them with an ensemble self-supervised framework.

Result: The PAST model significantly outperforms seven state-of-the-art methods, showing improvements of up to 26.2% in RMSE and 31.6% in MAE across three datasets and 27 missing data conditions.

Conclusion: PAST effectively handles diverse missing data scenarios by leveraging internal and external patterns, proving its advantage in traffic time series imputation challenges.

Abstract: Traffic time series imputation is crucial for the safety and reliability of intelligent transportation systems, while diverse types of missing data, including random, fiber, and block missing make the imputation task challenging. Existing models often focus on disentangling and separately modeling spatial and temporal patterns based on relationships between data points. However, these approaches struggle to adapt to the random missing positions, and fail to learn long-term and large-scale dependencies, which are essential in extensive missing conditions. In this paper, patterns are categorized into two types to handle various missing data conditions: primary patterns, which originate from internal relationships between data points, and auxiliary patterns, influenced by external factors like timestamps and node attributes. Accordingly, we propose the Primary-Auxiliary Spatio-Temporal network (PAST). It comprises a graph-integrated module (GIM) and a cross-gated module (CGM). GIM captures primary patterns via dynamic graphs with interval-aware dropout and multi-order convolutions, and CGM extracts auxiliary patterns through bidirectional gating on embedded external features. The two modules interact via shared hidden vectors and are trained under an ensemble self-supervised framework. Experiments on three datasets under 27 missing data conditions demonstrate that the imputation accuracy of PAST outperforms seven state-of-the-art baselines by up to 26.2% in RMSE and 31.6% in MAE.

</details>


### [778] [MMWSTM-ADRAN+: A Novel Hybrid Deep Learning Architecture for Enhanced Climate Time Series Forecasting and Extreme Event Prediction](https://arxiv.org/abs/2511.13419)
*Shaheen Mohammed Saleh Ahmed,Hakan Hakan Guneyli*

Main category: cs.LG

TL;DR: The paper introduces MMWSTM-ADRAN+, a dual-stream deep learning model for accurately predicting extreme air temperature events using a regime-aware dynamics model and anomaly-sensitive attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: The study aims to tackle the challenge of accurately forecasting short-term extreme air temperature events, which is crucial for operational climate risk management.

Method: The method features the MMWSTM stream utilizing BiLSTM for weather regime detection and ADRAN stream integrating BiGRUs, multi-head attention, and anomaly amplification. A fusion gate adapts contributions from both streams. Optimization includes ExtremeWeatherLoss focusing on extreme temperatures and time-series data augmentation.

Result: The MMWSTM-ADRAN+ model enhances accurate predictions of daily maximum temperatures and their extremes, outperforming traditional approaches in sensitivity to extreme events.

Conclusion: MMWSTM-ADRAN+ represents a significant advancement in predicting extreme temperature events, offering improved capacity to capture rare climate patterns essential for risk management and planning.

Abstract: Accurate short-range prediction of extreme air temperature events remains a fundamental challenge in operational climate-risk management. We present Multi-Modal Weather State Transition Model with Anomaly-Driven Recurrent Attention Network Plus (MMWSTM-ADRAN+), a dual-stream deep learning architecture that couples a regime-aware dynamics model with an anomaly-focused attention mechanism to forecast daily maximum temperature and its extremes. The first stream, MMWSTM, combines bidirectional Long Short-Term Memory (BiLSTM) units with a learnable Markov state transition matrix to capture synoptic-scale weather regime changes. The second stream, ADRAN, integrates bidirectional Gated Recurrent Units (BiGRUs), multi-head self-attention, and a novel anomaly amplification layer to enhance sensitivity to low-probability signals. A lightweight attentive fusion gate adaptively determines the contribution of each stream to the final prediction. Model optimization employs a custom ExtremeWeatherLoss function that up-weights errors on the upper 5% and lower 5% of the temperature distribution, and a time-series data augmentation suite (jittering, scaling, time/magnitude warping) that effectively quadruples the training data

</details>


### [779] [Discovering Operational Patterns Using Image-Based Convolutional Clustering and Composite Evaluation: A Case Study in Foundry Melting Processes](https://arxiv.org/abs/2511.13444)
*Zhipeng Ma,Bo Nørregaard Jørgensen,Zheng Grace Ma*

Main category: cs.LG

TL;DR: The paper introduces a novel unsupervised framework for clustering univariate time-series data using image-based convolutional techniques and composite evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: Challenges like lack of labels, high variability, and operational noise in industrial time-series data hinder effective pattern extraction using conventional methods.

Method: A new framework transforms time-series data into grayscale matrix representations for feature extraction via deep convolutional autoencoder, refines clustering using a two-stage strategy, and evaluates performance using a composite score (S_eva).

Result: The method successfully identifies seven operational patterns in furnace melting operations, outperforming classical and deep clustering methods in accuracy, robustness, and explainability.

Conclusion: This framework overcomes challenges in unsupervised time-series analysis and offers a robust, generalizable solution for diagnostics and energy optimization in industrial systems.

Abstract: Industrial process monitoring increasingly relies on sensor-generated time-series data, yet the lack of labels, high variability, and operational noise make it difficult to extract meaningful patterns using conventional methods. Existing clustering techniques either rely on fixed distance metrics or deep models designed for static data, limiting their ability to handle dynamic, unstructured industrial sequences. Addressing this gap, this paper proposes a novel framework for unsupervised discovery of operational modes in univariate time-series data using image-based convolutional clustering with composite internal evaluation. The proposed framework improves upon existing approaches in three ways: (1) raw time-series sequences are transformed into grayscale matrix representations via overlapping sliding windows, allowing effective feature extraction using a deep convolutional autoencoder; (2) the framework integrates both soft and hard clustering outputs and refines the selection through a two-stage strategy; and (3) clustering performance is objectively evaluated by a newly developed composite score, S_eva, which combines normalized Silhouette, Calinski-Harabasz, and Davies-Bouldin indices. Applied to over 3900 furnace melting operations from a Nordic foundry, the method identifies seven explainable operational patterns, revealing significant differences in energy consumption, thermal dynamics, and production duration. Compared to classical and deep clustering baselines, the proposed approach achieves superior overall performance, greater robustness, and domain-aligned explainability. The framework addresses key challenges in unsupervised time-series analysis, such as sequence irregularity, overlapping modes, and metric inconsistency, and provides a generalizable solution for data-driven diagnostics and energy optimization in industrial systems.

</details>


### [780] [Artificial Intelligence-Enabled Spirometry for Early Detection of Right Heart Failure](https://arxiv.org/abs/2511.13457)
*Bin Liu,Qinghao Zhao,Yuxi Zhou,Zhejun Sun,Kaijie Lei,Deyun Zhang,Shijia Geng,Shenda Hong*

Main category: cs.LG

TL;DR: This study creates a method to detect early right heart failure (RHF) in lung disease patients using spirogram data and demographic information, achieving strong predictive performance.


<details>
  <summary>Details</summary>
Motivation: RHF, linked to high morbidity and mortality, is often caused by lung disease increasing RV load, stressing the need for early identification of affected patients.

Method: The approach uses self-supervised representation learning paired with spirogram embedding and demographic data fitted into a CatBoost classification model.

Result: The model achieved an AUROC score of 0.7501 overall, with even higher performance in high-risk subgroups like chronic kidney disease (AUROC 0.8194) and valvular heart disease (AUROC 0.8413).

Conclusion: The study proposes a method integrating spirogram and demographic data for early RHF detection, showing promise for better clinical risk prediction.

Abstract: Right heart failure (RHF) is a disease characterized by abnormalities in the structure or function of the right ventricle (RV), which is associated with high morbidity and mortality. Lung disease often causes increased right ventricular load, leading to RHF. Therefore, it is very important to screen out patients with cor pulmonale who develop RHF from people with underlying lung diseases. In this work, we propose a self-supervised representation learning method to early detecting RHF from patients with cor pulmonale, which uses spirogram time series to predict patients with RHF at an early stage. The proposed model is divided into two stages. The first stage is the self-supervised representation learning-based spirogram embedding (SLSE) network training process, where the encoder of the Variational autoencoder (VAE-encoder) learns a robust low-dimensional representation of the spirogram time series from the data-augmented unlabeled data. Second, this low-dimensional representation is fused with demographic information and fed into a CatBoost classifier for the downstream RHF prediction task. Trained and tested on a carefully selected subset of 26,617 individuals from the UK Biobank, our model achieved an AUROC of 0.7501 in detecting RHF, demonstrating strong population-level distinction ability. We further evaluated the model on high-risk clinical subgroups, achieving AUROC values of 0.8194 on a test set of 74 patients with chronic kidney disease (CKD) and 0.8413 on a set of 64 patients with valvular heart disease (VHD). These results highlight the model's potential utility in predicting RHF among clinically elevated-risk populations. In conclusion, this study presents a self-supervised representation learning approach combining spirogram time series and demographic data, demonstrating promising potential for early RHF detection in clinical practice.

</details>


### [781] [Multi-task GINN-LP for Multi-target Symbolic Regression](https://arxiv.org/abs/2511.13463)
*Hussein Rajabu,Lijun Qian,Xishuang Dong*

Main category: cs.LG

TL;DR: This paper introduces MTRGINN-LP, merging multi-task learning and symbolic regression to handle multi-target outputs effectively and interpretably.


<details>
  <summary>Details</summary>
Motivation: Symbolic Regression techniques struggle with generalization beyond scientific datasets and are mostly limited to single-output problems. Multi-target problems with interdependent outputs are common in real-world applications.

Method: The proposed MTRGINN-LP incorporates GINN-LP into a multi-task deep learning framework, using a shared backbone with power-term approximator blocks and task-specific output layers to capture dependencies and preserve interpretability.

Result: Validation on applications like energy efficiency prediction and sustainable agriculture shows competitive predictive performance combined with interpretability.

Conclusion: MTRGINN-LP successfully extends symbolic regression to multi-target real-world tasks, addressing limitations and improving broader applicability.

Abstract: In the area of explainable artificial intelligence, Symbolic Regression (SR) has emerged as a promising approach by discovering interpretable mathematical expressions that fit data. However, SR faces two main challenges: most methods are evaluated on scientific datasets with well-understood relationships, limiting generalization, and SR primarily targets single-output regression, whereas many real-world problems involve multi-target outputs with interdependent variables. To address these issues, we propose multi-task regression GINN-LP (MTRGINN-LP), an interpretable neural network for multi-target symbolic regression. By integrating GINN-LP with a multi-task deep learning, the model combines a shared backbone including multiple power-term approximator blocks with task-specific output layers, capturing inter-target dependencies while preserving interpretability. We validate multi-task GINN-LP on practical multi-target applications, including energy efficiency prediction and sustainable agriculture. Experimental results demonstrate competitive predictive performance alongside high interpretability, effectively extending symbolic regression to broader real-world multi-output tasks.

</details>


### [782] [GREAT: Generalizable Representation Enhancement via Auxiliary Transformations for Zero-Shot Environmental Prediction](https://arxiv.org/abs/2511.13469)
*Shiyuan Luo,Chonghao Qiu,Runlong Yu,Yiqun Xie,Xiaowei Jia*

Main category: cs.LG

TL;DR: This paper presents GREAT, an innovative framework addressing challenges in environmental modeling by enabling improved generalization in unseen regions using guided data augmentation.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the critical challenge of predicting ecosystem dynamics in unmonitored regions, affected by limited observation data and spatial heterogeneity.

Method: The proposed framework, GREAT, uses auxiliary transformations at multiple neural network layers for dataset augmentation. It incorporates a novel bi-level training process to preserve invariant environmental patterns in the augmented data.

Result: Experiments on stream temperature prediction across diverse watersheds in the eastern U.S. demonstrate GREAT's superior performance in zero-shot predictions compared to other methods.

Conclusion: GREAT offers a practical and effective solution for enhancing environmental modeling in situations where data collection is geographically limited or infeasible, advancing zero-shot generalization in the field.

Abstract: Environmental modeling faces critical challenges in predicting ecosystem dynamics across unmonitored regions due to limited and geographically imbalanced observation data. This challenge is compounded by spatial heterogeneity, causing models to learn spurious patterns that fit only local data. Unlike conventional domain generalization, environmental modeling must preserve invariant physical relationships and temporal coherence during augmentation. In this paper, we introduce Generalizable Representation Enhancement via Auxiliary Transformations (GREAT), a framework that effectively augments available datasets to improve predictions in completely unseen regions. GREAT guides the augmentation process to ensure that the original governing processes can be recovered from the augmented data, and the inclusion of the augmented data leads to improved model generalization. Specifically, GREAT learns transformation functions at multiple layers of neural networks to augment both raw environmental features and temporal influence. They are refined through a novel bi-level training process that constrains augmented data to preserve key patterns of the original source data. We demonstrate GREAT's effectiveness on stream temperature prediction across six ecologically diverse watersheds in the eastern U.S., each containing multiple stream segments. Experimental results show that GREAT significantly outperforms existing methods in zero-shot scenarios. This work provides a practical solution for environmental applications where comprehensive monitoring is infeasible.

</details>


### [783] [Quantum Machine Learning via Contrastive Training](https://arxiv.org/abs/2511.13497)
*Liudmila A. Zhukas,Vivian Ni Zhang,Qiang Miao,Qingfeng Wang,Marko Cetina,Jungsang Kim,Lawrence Carin,Christopher Monroe*

Main category: cs.LG

TL;DR: The paper introduces self-supervised pretraining for quantum machine learning (QML) to address labeled data scarcity issues, demonstrating improved accuracy and efficiency using a programmable quantum computer.


<details>
  <summary>Details</summary>
Motivation: QML faces challenges due to a lack of labeled data, which becomes more significant as models grow in scale and complexity. The authors aim to reduce dependence on labeled data for training.

Method: They employ self-supervised learning with in situ contrastive pretraining on a trapped-ion quantum computer, creating quantum representations from unlabeled data and fine-tuning them for image classification.

Result: The approach significantly improves test accuracy and reduces variability compared to models with random initialization, particularly in scenarios with limited labeled data. Learned representations also generalize beyond pretraining samples.

Conclusion: The study presents a label-efficient quantum representation learning method, with significant application potential for quantum datasets and compatibility with larger classical datasets.

Abstract: Quantum machine learning (QML) has attracted growing interest with the rapid parallel advances in large-scale classical machine learning and quantum technologies. Similar to classical machine learning, QML models also face challenges arising from the scarcity of labeled data, particularly as their scale and complexity increase. Here, we introduce self-supervised pretraining of quantum representations that reduces reliance on labeled data by learning invariances from unlabeled examples. We implement this paradigm on a programmable trapped-ion quantum computer, encoding images as quantum states. In situ contrastive pretraining on hardware yields a representation that, when fine-tuned, classifies image families with higher mean test accuracy and lower run-to-run variability than models trained from random initialization. Performance improvement is especially significant in regimes with limited labeled training data. We show that the learned invariances generalize beyond the pretraining image samples. Unlike prior work, our pipeline derives similarity from measured quantum overlaps and executes all training and classification stages on hardware. These results establish a label-efficient route to quantum representation learning, with direct relevance to quantum-native datasets and a clear path to larger classical inputs.

</details>


### [784] [Naga: Vedic Encoding for Deep State Space Models](https://arxiv.org/abs/2511.13510)
*Melanie Schaller,Nick Janssen,Bodo Rosenhahn*

Main category: cs.LG

TL;DR: The paper introduces Naga, a deep State Space Model inspired by Vedic mathematics, for improved long-term time series forecasting.


<details>
  <summary>Details</summary>
Motivation: To enhance long-term time series forecasting by leveraging structured concepts inspired by Vedic mathematics.

Method: Naga processes time series bidirectionally and combines forward and reverse sequences using Hadamard interaction for encoding temporal dependencies.

Result: Naga outperformed 28 state-of-the-art models across several benchmarks with improved computational efficiency.

Conclusion: Structured Vedic-inspired decomposition offers an effective and interpretable approach for long-range sequence modeling.

Abstract: This paper presents Naga, a deep State Space Model (SSM) encoding approach inspired by structural concepts from Vedic mathematics. The proposed method introduces a bidirectional representation for time series by jointly processing forward and time-reversed input sequences. These representations are then combined through an element-wise (Hadamard) interaction, resulting in a Vedic-inspired encoding that enhances the model's ability to capture temporal dependencies across distant time steps. We evaluate Naga on multiple long-term time series forecasting (LTSF) benchmarks, including ETTh1, ETTh2, ETTm1, ETTm2, Weather, Traffic, and ILI. The experimental results show that Naga outperforms 28 current state of the art models and demonstrates improved efficiency compared to existing deep SSM-based approaches. The findings suggest that incorporating structured, Vedic-inspired decomposition can provide an interpretable and computationally efficient alternative for long-range sequence modeling.

</details>


### [785] [A Quantum Tensor Network-Based Viewpoint for Modeling and Analysis of Time Series Data](https://arxiv.org/abs/2511.13514)
*Pragatheeswaran Vipulananthan,Kamal Premaratne,Dilip Sarkar,Manohar N. Murthi*

Main category: cs.LG

TL;DR: The paper proposes a quantum physics-based method for uncertainty quantification in machine learning using a tensor network-inspired model to improve interpretability and performance.


<details>
  <summary>Details</summary>
Motivation: Neural networks lack interpretability, while probabilistic white-box models have performance issues, requiring a solution that combines accuracy and interpretability.

Method: The authors map kernel mean embeddings of time series data to a reproducing kernel Hilbert space (RKHS) and construct a quantum tensor network-based spin chain. Using Schrödinger equations and perturbation theory, uncertainty is quantified.

Result: The quantum physics-based approach demonstrates superior performance in change point detection and time series clustering compared to existing white-box models.

Conclusion: The proposed methodology successfully addresses the trade-off between interpretability and performance in uncertainty quantification for machine learning tasks.

Abstract: Accurate uncertainty quantification is a critical challenge in machine learning. While neural networks are highly versatile and capable of learning complex patterns, they often lack interpretability due to their ``black box'' nature. On the other hand, probabilistic ``white box'' models, though interpretable, often suffer from a significant performance gap when compared to neural networks. To address this, we propose a novel quantum physics-based ``white box'' method that offers both accurate uncertainty quantification and enhanced interpretability. By mapping the kernel mean embedding (KME) of a time series data vector to a reproducing kernel Hilbert space (RKHS), we construct a tensor network-inspired 1D spin chain Hamiltonian, with the KME as one of its eigen-functions or eigen-modes. We then solve the associated Schr{ö}dinger equation and apply perturbation theory to quantify uncertainty, thereby improving the interpretability of tasks performed with the quantum tensor network-based model. We demonstrate the effectiveness of this methodology, compared to state-of-the-art ``white box" models, in change point detection and time series clustering, providing insights into the uncertainties associated with decision-making throughout the process.

</details>


### [786] [Mitigating Spurious Correlations in Patch-wise Tumor Classification on High-Resolution Multimodal Images](https://arxiv.org/abs/2511.13527)
*Ihab Asaad,Maha Shadaydeh,Joachim Denzler*

Main category: cs.LG

TL;DR: The paper addresses the issues in patch-wise binary classification for tumor detection in high-resolution images due to spurious correlations, proposing GERNE as a debiasing approach to improve performance, particularly in critical minority cases.


<details>
  <summary>Details</summary>
Motivation: The motivation is to reduce the complexity, annotation costs, and inefficiencies in pixel-wise segmentation by using patch-wise classification while addressing the challenges posed by spurious correlations in high-resolution tumor detection tasks.

Method: The authors propose applying GERNE, a debiasing strategy tuned to maximize worst-group accuracy (WGA), to mitigate spurious correlations between patch composition and labels.

Result: Using GERNE improves the worst-group accuracy by approximately 7% compared to ERM, especially in challenging cases involving minority groups with atypical tissue-to-background ratios.

Conclusion: Spurious correlation-aware learning, exemplified by GERNE, significantly enhances the effectiveness and fairness of patch-wise binary classification tasks, especially in critical minority cases.

Abstract: Patch-wise multi-label classification provides an efficient alternative to full pixel-wise segmentation on high-resolution images, particularly when the objective is to determine the presence or absence of target objects within a patch rather than their precise spatial extent. This formulation substantially reduces annotation cost, simplifies training, and allows flexible patch sizing aligned with the desired level of decision granularity. In this work, we focus on a special case, patch-wise binary classification, applied to the detection of a single class of interest (tumor) on high-resolution multimodal nonlinear microscopy images. We show that, although this simplified formulation enables efficient model development, it can introduce spurious correlations between patch composition and labels: tumor patches tend to contain larger tissue regions, whereas non-tumor patches often consist mostly of background with small tissue areas. We further quantify the bias in model predictions caused by this spurious correlation, and propose to use a debiasing strategy to mitigate its effect. Specifically, we apply GERNE, a debiasing method that can be adapted to maximize worst-group accuracy (WGA). Our results show an improvement in WGA by approximately 7% compared to ERM for two different thresholds used to binarize the spurious feature. This enhancement boosts model performance on critical minority cases, such as tumor patches with small tissues and non-tumor patches with large tissues, and underscores the importance of spurious correlation-aware learning in patch-wise classification problems.

</details>


### [787] [Fairness-Aware Graph Representation Learning with Limited Demographic Information](https://arxiv.org/abs/2511.13540)
*Zichong Wang,Zhipeng Yin,Liping Yang,Jun Zhuang,Rui Yu,Qingzhao Kong,Wenbin Zhang*

Main category: cs.LG

TL;DR: The paper introduces FairGLite, a fair graph learning framework designed to mitigate bias with limited demographic information.


<details>
  <summary>Details</summary>
Motivation: The need for fairness and bias mitigation in Graph Neural Networks, especially under constraints like limited access to demographic data.

Method: A mechanism to generate proxies for demographic information, enforcing consistent node embeddings across groups, adaptive confidence strategy for dynamic node contributions, and theoretical guarantees on fairness metrics.

Result: FairGLite improves bias mitigation while maintaining model utility, as validated through experiments on multiple datasets.

Conclusion: FairGLite is effective in reducing bias in graph learning frameworks while upholding fairness guarantees and preserving utility.

Abstract: Ensuring fairness in Graph Neural Networks is fundamental to promoting trustworthy and socially responsible machine learning systems. In response, numerous fair graph learning methods have been proposed in recent years. However, most of them assume full access to demographic information, a requirement rarely met in practice due to privacy, legal, or regulatory restrictions. To this end, this paper introduces a novel fair graph learning framework that mitigates bias in graph learning under limited demographic information. Specifically, we propose a mechanism guided by partial demographic data to generate proxies for demographic information and design a strategy that enforces consistent node embeddings across demographic groups. In addition, we develop an adaptive confidence strategy that dynamically adjusts each node's contribution to fairness and utility based on prediction confidence. We further provide theoretical analysis demonstrating that our framework, FairGLite, achieves provable upper bounds on group fairness metrics, offering formal guarantees for bias mitigation. Through extensive experiments on multiple datasets and fair graph learning frameworks, we demonstrate the framework's effectiveness in both mitigating bias and maintaining model utility.

</details>


### [788] [Graph Out-of-Distribution Detection via Test-Time Calibration with Dual Dynamic Dictionaries](https://arxiv.org/abs/2511.13541)
*Yue Hou,Ruomei Liu,Yingke Su,Junran Wu,Ke Xu*

Main category: cs.LG

TL;DR: The paper introduces BaCa, a method for graph OOD detection without the need for ground-truth OOD samples or fine-tuning. It implements dual dynamic dictionaries for adaptive calibration, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Traditional graph OOD detection methods struggle due to limited representation of distributional boundaries and the neglect of graph's latent multi-factor structure.

Method: BaCa calibrates OOD scores using dynamically updated dictionaries and a mix-up strategy with only test samples. It uses graphons to generate boundary-aware discriminatory topologies.

Result: BaCa demonstrated superior OOD detection performance compared to state-of-the-art methods in experiments with real-world datasets.

Conclusion: The proposed BaCa method effectively addresses OOD detection challenges by introducing adaptive, boundary-aware calibration strategies, outperforming existing methods.

Abstract: A key challenge in graph out-of-distribution (OOD) detection lies in the absence of ground-truth OOD samples during training. Existing methods are typically optimized to capture features within the in-distribution (ID) data and calculate OOD scores, which often limits pre-trained models from representing distributional boundaries, leading to unreliable OOD detection. Moreover, the latent structure of graph data is often governed by multiple underlying factors, which remains less explored. To address these challenges, we propose a novel test-time graph OOD detection method, termed BaCa, that calibrates OOD scores using dual dynamically updated dictionaries without requiring fine-tuning the pre-trained model. Specifically, BaCa estimates graphons and applies a mix-up strategy solely with test samples to generate diverse boundary-aware discriminative topologies, eliminating the need for exposing auxiliary datasets as outliers. We construct dual dynamic dictionaries via priority queues and attention mechanisms to adaptively capture latent ID and OOD representations, which are then utilized for boundary-aware OOD score calibration. To the best of our knowledge, extensive experiments on real-world datasets show that BaCa significantly outperforms existing state-of-the-art methods in OOD detection.

</details>


### [789] [RAC-DMVC: Reliability-Aware Contrastive Deep Multi-View Clustering under Multi-Source Noise](https://arxiv.org/abs/2511.13561)
*Shihao Dong,Yue Liu,Xiaotong Zhou,Yuhui Zheng,Huiying Xu,Xinzhong Zhu*

Main category: cs.LG

TL;DR: The paper proposes a novel framework, RAC-DMVC, for robust multi-view clustering in noisy environments using reliability-aware techniques and achieves superior performance under various noise levels.


<details>
  <summary>Details</summary>
Motivation: Enhancing the applicability of multi-view clustering in real-world scenarios by addressing challenges from multi-source noises (missing noise and observation noise).

Method: Proposes RAC-DMVC framework featuring reliability graphs, cross-view reconstruction, reliability-aware noise contrastive learning, dual-attention imputation, and self-supervised cluster distillation.

Result: Experiments on five benchmark datasets show RAC-DMVC outperforms existing methods across multiple metrics and maintains robust performance under varying noise levels.

Conclusion: RAC-DMVC effectively addresses multi-source noise challenges and boosts clustering accuracy, demonstrating its suitability for real-world applications.

Abstract: Multi-view clustering (MVC), which aims to separate the multi-view data into distinct clusters in an unsupervised manner, is a fundamental yet challenging task. To enhance its applicability in real-world scenarios, this paper addresses a more challenging task: MVC under multi-source noises, including missing noise and observation noise. To this end, we propose a novel framework, Reliability-Aware Contrastive Deep Multi-View Clustering (RAC-DMVC), which constructs a reliability graph to guide robust representation learning under noisy environments. Specifically, to address observation noise, we introduce a cross-view reconstruction to enhances robustness at the data level, and a reliability-aware noise contrastive learning to mitigates bias in positive and negative pairs selection caused by noisy representations. To handle missing noise, we design a dual-attention imputation to capture shared information across views while preserving view-specific features. In addition, a self-supervised cluster distillation module further refines the learned representations and improves the clustering performance. Extensive experiments on five benchmark datasets demonstrate that RAC-DMVC outperforms SOTA methods on multiple evaluation metrics and maintains excellent performance under varying ratios of noise.

</details>


### [790] [Batch Acquisition Function Evaluations and Decouple Optimizer Updates for Faster Bayesian Optimization](https://arxiv.org/abs/2511.13625)
*Kaichi Irie,Shuhei Watanabe,Masaki Onishi*

Main category: cs.LG

TL;DR: The paper addresses inefficiencies in acquisition function optimization for Bayesian optimization and proposes a method using coroutines to improve computational speed.


<details>
  <summary>Details</summary>
Motivation: Bayesian optimization struggles with computational bottlenecks in acquisition function optimization because of non-convexity, requiring multi-start optimization with quasi-Newton methods.

Method: The authors propose decoupling quasi-Newton updates using a coroutine while batching acquisition function calls.

Result: Their approach shows identical theoretical convergence to sequential optimization while significantly reducing computational time.

Conclusion: The proposed method improves computational efficiency in Bayesian optimization, overcoming limitations of the existing approach in BoTorch.

Abstract: Bayesian optimization (BO) efficiently finds high-performing parameters by maximizing an acquisition function, which models the promise of parameters. A major computational bottleneck arises in acquisition function optimization, where multi-start optimization (MSO) with quasi-Newton (QN) methods is required due to the non-convexity of the acquisition function. BoTorch, a widely used BO library, currently optimizes the summed acquisition function over multiple points, leading to the speedup of MSO owing to PyTorch batching. Nevertheless, this paper empirically demonstrates the suboptimality of this approach in terms of off-diagonal approximation errors in the inverse Hessian of a QN method, slowing down its convergence. To address this problem, we propose to decouple QN updates using a coroutine while batching the acquisition function calls. Our approach not only yields the theoretically identical convergence to the sequential MSO but also drastically reduces the wall-clock time compared to the previous approaches.

</details>


### [791] [Towards Multimodal Representation Learning in Paediatric Kidney Disease](https://arxiv.org/abs/2511.13637)
*Ana Durica,John Booth,Ivana Drobnjak*

Main category: cs.LG

TL;DR: This study uses EHR data and recurrent neural networks to predict abnormal kidney function in children.


<details>
  <summary>Details</summary>
Motivation: To monitor and predict renal function in paediatric patients using continuous data.

Method: A recurrent neural model integrates longitudinal lab data and demographics to predict abnormal serum creatinine levels within 30 days.

Result: Temporal models identified useful patterns in paediatric kidney data, demonstrating feasibility for predictive modelling.

Conclusion: The study proves the potential of temporal modelling in paediatric cases, paving the way for multimodal and detailed extensions.

Abstract: Paediatric kidney disease varies widely in its presentation and progression, which calls for continuous monitoring of renal function. Using electronic health records collected between 2019 and 2025 at Great Ormond Street Hospital, a leading UK paediatric hospital, we explored a temporal modelling approach that integrates longitudinal laboratory sequences with demographic information. A recurrent neural model trained on these data was used to predict whether a child would record an abnormal serum creatinine value within the following thirty days. Framed as a pilot study, this work provides an initial demonstration that simple temporal representations can capture useful patterns in routine paediatric data and lays the groundwork for future multimodal extensions using additional clinical signals and more detailed renal outcomes.

</details>


### [792] [Data Value in the Age of Scaling: Understanding LLM Scaling Dynamics Under Real-Synthetic Data Mixtures](https://arxiv.org/abs/2511.13640)
*Haohui Wang,Jingyuan Qi,Jianpeng Chen,Jun Wu,Lifu Huang,Lecheng Zheng,Kevin Choi,Balaji Veeramani,Edward Bowen,Alison Hu,Tyler Cody,Dawei Zhou*

Main category: cs.LG

TL;DR: The paper discovers scaling behaviors in large language models with mixed real-synthetic data, introduces a theoretical generalization bound, and proposes a computationally efficient data valuation method outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Address systematic discrepancies in mixed real-synthetic datasets used for training large language models and improve their evaluation and utility.

Method: Derive an LLM generalization bound for real-synthetic mixtures and propose an efficient data valuation method validated through experiments across multiple tasks.

Result: The proposed data valuation method surpasses existing baselines on four tasks with lower computational cost.

Conclusion: The findings enhance the understanding of synthetic data's impact on LLM performance and provide practical tools like a scalable data valuation method for mixed datasets.

Abstract: The rapid progress of large language models (LLMs) is fueled by the growing reliance on datasets that blend real and synthetic data. While synthetic data offers scalability and cost-efficiency, it often introduces systematic distributional discrepancies, particularly underrepresenting long-tail knowledge due to truncation effects from data generation mechanisms like top-p sampling, temperature scaling, and finite sampling. These discrepancies pose fundamental challenges in characterizing and evaluating the utility of mixed real-synthetic datasets. In this paper, we identify a three-phase scaling behavior characterized by two breakpoints that reflect transitions in model behavior across learning head and tail knowledge. We further derive an LLM generalization bound designed for real and synthetic mixtures, revealing several key factors that govern their generalization performance. Building on our theoretical findings, we propose an effective yet efficient data valuation method that scales to large-scale datasets. Comprehensive experiments across four tasks, including image classification, sentiment classification, instruction following, and complex reasoning, demonstrate that our method surpasses state-of-the-art baselines in data valuation with significantly low computational cost.

</details>


### [793] [FuseSampleAgg: Fused Neighbor Sampling and Aggregation for Mini-batch GNNs](https://arxiv.org/abs/2511.13645)
*Aleksandar Stanković*

Main category: cs.LG

TL;DR: FuseSampleAgg is a CUDA operator that combines neighbor sampling and mean aggregation for GraphSAGE models in one pass, significantly reducing memory traffic and step times.


<details>
  <summary>Details</summary>
Motivation: High memory traffic, overhead, and inefficiency in traditional GraphSAGE implementations create opportunities for optimization in graph processing frameworks.

Method: FuseSampleAgg fuses neighbor sampling and mean aggregation steps utilizing saved index replay to avoid block materialization and extra kernel launches, while preserving GraphSAGE semantics.

Result: Experiments show speedups of up to 51x (ogbn-products), 4x (Reddit), and 3.3x (ogbn-arxiv), and reductions in peak GPU memory usage up to 100x, 36x, and 3.5x respectively.

Conclusion: FuseSampleAgg offers dramatic efficiency improvements for GraphSAGE workloads and supports PyTorch frameworks with reproducibility via provided scripts and code.

Abstract: We present FuseSampleAgg, a CUDA operator that fuses neighbor sampling and mean aggregation into a single pass for one and two hop GraphSAGE. By eliminating block materialization and extra kernel launches, FuseSampleAgg reduces memory traffic and overhead while preserving GraphSAGE mean semantics via saved index replay. Across the Reddit, ogbn-arxiv, and ogbn-products benchmarks (batch size 1024, automatic mixed precision enabled), we observe step time speedups up to 51x on ogbn-products, about 4x on Reddit with fanouts 10-10 and 15-10, and about 3.3x on ogbn-arxiv at larger fanouts, with peak GPU memory reductions up to 100x, 36x, and about 3.5x, respectively. The operator is deterministic, integrates with standard PyTorch optimizers, and ships with scripts that reproduce all tables and figures from CSV logs. Code and scripts are available at https://github.com/SV25-22/FuseSampleAgg.

</details>


### [794] [Weight-sparse transformers have interpretable circuits](https://arxiv.org/abs/2511.13653)
*Leo Gao,Achyuta Rajaram,Jacob Coxon,Soham V. Govande,Bowen Baker,Dan Mossing*

Main category: cs.LG

TL;DR: The paper proposes a method to improve the interpretability of language models by training them with sparse weights, isolating task-critical circuits, and analyzing them to achieve unprecedented human understanding.


<details>
  <summary>Details</summary>
Motivation: The motivation is to create human-understandable circuits in language models to enhance mechanistic interpretability and understand model decision-making processes.

Method: The authors constrain model weights to be sparse, prune them to isolate circuits for specific tasks, and analyze the resulting interpretable structures. They also explore scaling and attempt to adapt the approach to dense models.

Result: The approach yields circuits with interpretable connections between neurons and residual channels representing natural concepts. Sparse weights enhance understandability but limit capability. Scaling sparse models is challenging beyond a certain size.

Conclusion: The method advances interpretability in language models but faces challenges with scaling sparse models further. It provides rigorous validation of the circuits and suggests preliminary steps for explaining dense models.

Abstract: Finding human-understandable circuits in language models is a central goal of the field of mechanistic interpretability. We train models to have more understandable circuits by constraining most of their weights to be zeros, so that each neuron only has a few connections. To recover fine-grained circuits underlying each of several hand-crafted tasks, we prune the models to isolate the part responsible for the task. These circuits often contain neurons and residual channels that correspond to natural concepts, with a small number of straightforwardly interpretable connections between them. We study how these models scale and find that making weights sparser trades off capability for interpretability, and scaling model size improves the capability-interpretability frontier. However, scaling sparse models beyond tens of millions of nonzero parameters while preserving interpretability remains a challenge. In addition to training weight-sparse models de novo, we show preliminary results suggesting our method can also be adapted to explain existing dense models. Our work produces circuits that achieve an unprecedented level of human understandability and validates them with considerable rigor.

</details>


### [795] [Tuning for Two Adversaries: Enhancing the Robustness Against Transfer and Query-Based Attacks using Hyperparameter Tuning](https://arxiv.org/abs/2511.13654)
*Pascal Zimmer,Ghassan Karame*

Main category: cs.LG

TL;DR: The paper analyzes how optimization hyperparameters affect robustness against transfer-based and query-based attacks and proposes methods to jointly enhance robustness for both.


<details>
  <summary>Details</summary>
Motivation: To understand and exploit the role of optimization hyperparameters in improving robustness against adversarial attacks.

Method: The study includes theoretical analysis and experiments over centralized, ensemble, and distributed training setups, examining the effects of varying hyperparameters like learning rate, momentum, etc.

Result: Decreasing the learning rate significantly increases robustness against transfer-based attacks, while increasing it enhances robustness against query-based attacks. Distributed models gain the most from hyperparameter tuning.

Conclusion: Strategic tuning of optimization hyperparameters helps achieve a balance in robustness against different types of attacks, especially benefiting distributed learning setups.

Abstract: In this paper, we present the first detailed analysis of how optimization hyperparameters -- such as learning rate, weight decay, momentum, and batch size -- influence robustness against both transfer-based and query-based attacks. Supported by theory and experiments, our study spans a variety of practical deployment settings, including centralized training, ensemble learning, and distributed training. We uncover a striking dichotomy: for transfer-based attacks, decreasing the learning rate significantly enhances robustness by up to $64\%$. In contrast, for query-based attacks, increasing the learning rate consistently leads to improved robustness by up to $28\%$ across various settings and data distributions. Leveraging these findings, we explore -- for the first time -- the optimization hyperparameter design space to jointly enhance robustness against both transfer-based and query-based attacks. Our results reveal that distributed models benefit the most from hyperparameter tuning, achieving a remarkable tradeoff by simultaneously mitigating both attack types more effectively than other training setups.

</details>


### [796] [Cross-Learning from Scarce Data via Multi-Task Constrained Optimization](https://arxiv.org/abs/2511.13680)
*Leopoldo Agorio,Juan Cerviño,Miguel Calvo-Fullana,Alejandro Ribeiro,Juan Andrés Bazerque*

Main category: cs.LG

TL;DR: This paper presents a cross-learning framework to address data scarcity in supervised learning by jointly estimating deterministic parameters across tasks.


<details>
  <summary>Details</summary>
Motivation: To solve the challenge of poor model generalization due to limited data, which prevents accurate learning and parameter inference.

Method: The proposed approach formulates joint estimation as a constrained optimization problem, allowing shared similarity across multiple tasks while still accounting for task-specific differences.

Result: Theoretical guarantees are demonstrated for Gaussian data, and empirical validation is provided in real-world applications like image classification and disease propagation.

Conclusion: Cross-learning improves parameter inference, enabling knowledge transfer and robustness in cases of limited data availability.

Abstract: A learning task, understood as the problem of fitting a parametric model from supervised data, fundamentally requires the dataset to be large enough to be representative of the underlying distribution of the source. When data is limited, the learned models fail generalize to cases not seen during training. This paper introduces a multi-task \emph{cross-learning} framework to overcome data scarcity by jointly estimating \emph{deterministic} parameters across multiple, related tasks. We formulate this joint estimation as a constrained optimization problem, where the constraints dictate the resulting similarity between the parameters of the different models, allowing the estimated parameters to differ across tasks while still combining information from multiple data sources. This framework enables knowledge transfer from tasks with abundant data to those with scarce data, leading to more accurate and reliable parameter estimates, providing a solution for scenarios where parameter inference from limited data is critical. We provide theoretical guarantees in a controlled framework with Gaussian data, and show the efficiency of our cross-learning method in applications with real data including image classification and propagation of infectious diseases.

</details>


### [797] [Protein Secondary Structure Prediction Using 3D Graphs and Relation-Aware Message Passing Transformers](https://arxiv.org/abs/2511.13685)
*Disha Varshney,Samarth Garg,Sarthak Tyagi,Deeksha Varshney,Nayan Deep,Asif Ekbal*

Main category: cs.LG

TL;DR: This paper proposes a model, SSRGNet, combining Graph Neural Networks and Language Models for predicting protein secondary structures using spatial information and achieving improved F1-scores.


<details>
  <summary>Details</summary>
Motivation: The paper aims to fill the gap of using 3D structural data directly for protein secondary structure prediction, as it is pivotal for understanding protein functions, activities, and relationships.

Method: The study combines Graph Neural Networks (GNNs) and Language Models (LMs), leveraging protein residue graphs and pre-trained protein language models with message-passing mechanisms like GCN and R-GCN to integrate spatial and sequential data for prediction.

Result: The SSRGNet model surpasses baseline methods in F1-scores on a dataset provided by NetSurfP-2.0 for predicting secondary structures in 3-and 8-states.

Conclusion: Using 3D protein structural data alongside sequence data significantly enhances secondary structure prediction, emphasizing the model's ability to produce superior insights into protein spatial interconnections.

Abstract: In this study, we tackle the challenging task of predicting secondary structures from protein primary sequences, a pivotal initial stride towards predicting tertiary structures, while yielding crucial insights into protein activity, relationships, and functions. Existing methods often utilize extensive sets of unlabeled amino acid sequences. However, these approaches neither explicitly capture nor harness the accessible protein 3D structural data, which is recognized as a decisive factor in dictating protein functions. To address this, we utilize protein residue graphs and introduce various forms of sequential or structural connections to capture enhanced spatial information. We adeptly combine Graph Neural Networks (GNNs) and Language Models (LMs), specifically utilizing a pre-trained transformer-based protein language model to encode amino acid sequences and employing message-passing mechanisms like GCN and R-GCN to capture geometric characteristics of protein structures. Employing convolution within a specific node's nearby region, including relations, we stack multiple convolutional layers to efficiently learn combined insights from the protein's spatial graph, revealing intricate interconnections and dependencies in its structural arrangement. To assess our model's performance, we employed the training dataset provided by NetSurfP-2.0, which outlines secondary structure in 3-and 8-states. Extensive experiments show that our proposed model, SSRGNet surpasses the baseline on f1-scores.

</details>


### [798] [Learning stochasticity: a nonparametric framework for intrinsic noise estimation](https://arxiv.org/abs/2511.13701)
*Gianluigi Pillonetto,Alberto Giaretta,Mauro Bisiacco*

Main category: cs.LG

TL;DR: This paper introduces Trine, a novel nonparametric framework for inferring state-dependent intrinsic noise from time-series data, validated on biological and ecological systems.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of incomplete knowledge, stochastic effects, and ineffective parametric models in studying dynamical systems, particularly in biological and ecological contexts.

Method: The authors propose Trine, a kernel-based framework with a three-stage algorithm that combines analytically solvable subproblems and a structured kernel architecture for capturing noise-driven fluctuations and smooth state-dependent changes.

Result: Trine demonstrates strong performance in identifying hidden dynamics across various benchmark problems, matching the performance of an idealized observer. It was successfully validated on biological and ecological systems.

Conclusion: Trine provides a new, powerful approach for studying intrinsic noise in dynamical systems, offering insights that were previously inaccessible due to parametric model limitations.

Abstract: Understanding the principles that govern dynamical systems is a central challenge across many scientific domains, including biology and ecology. Incomplete knowledge of nonlinear interactions and stochastic effects often renders bottom-up modeling approaches ineffective, motivating the development of methods that can discover governing equations directly from data. In such contexts, parametric models often struggle without strong prior knowledge, especially when estimating intrinsic noise. Nonetheless, incorporating stochastic effects is often essential for understanding the dynamic behavior of complex systems such as gene regulatory networks and signaling pathways. To address these challenges, we introduce Trine (Three-phase Regression for INtrinsic noisE), a nonparametric, kernel-based framework that infers state-dependent intrinsic noise from time-series data. Trine features a three-stage algorithm that com- bines analytically solvable subproblems with a structured kernel architecture that captures both abrupt noise-driven fluctuations and smooth, state-dependent changes in variance. We validate Trine on biological and ecological systems, demonstrating its ability to uncover hidden dynamics without relying on predefined parametric assumptions. Across several benchmark problems, Trine achieves performance comparable to that of an oracle. Biologically, this oracle can be viewed as an idealized observer capable of directly tracking the random fluctuations in molecular concentrations or reaction events within a cell. The Trine framework thus opens new avenues for understanding how intrinsic noise affects the behavior of complex systems.

</details>


### [799] [ST-ProC: A Graph-Prototypical Framework for Robust Semi-Supervised Travel Mode Identification](https://arxiv.org/abs/2511.13702)
*Luyao Niu,Nuoxian Huang*

Main category: cs.LG

TL;DR: The paper introduces ST-ProC, a novel framework for travel mode identification from GPS data, focusing on overcoming label scarcity and improving performance using multi-objective semi-supervised learning techniques.


<details>
  <summary>Details</summary>
Motivation: The research aims to tackle the issue of label scarcity in travel mode identification from GPS trajectories, essential for urban planning and intelligence but challenging due to high annotation costs.

Method: ST-ProC integrates a graph-prototypical core with SSL support, incorporating techniques like graph regularization, prototypical anchoring, margin-aware pseudo-labeling, contrastive loss, and teacher-student consistency in its framework.

Result: ST-ProC significantly outperforms all baselines, achieving a 21.5% performance improvement over top methods such as FixMatch in sparse-label real-world settings.

Conclusion: The proposed ST-ProC framework enhances travel mode identification accuracy and robustness in scenarios with limited labeled data, offering a promising solution to semi-supervised learning challenges in urban intelligence applications.

Abstract: Travel mode identification (TMI) from GPS trajectories is critical for urban intelligence, but is hampered by the high cost of annotation, leading to severe label scarcity. Prevailing semi-supervised learning (SSL) methods are ill-suited for this task, as they suffer from catastrophic confirmation bias and ignore the intrinsic data manifold. We propose ST-ProC, a novel graph-prototypical multi-objective SSL framework to address these limitations. Our framework synergizes a graph-prototypical core with foundational SSL Support. The core exploits the data manifold via graph regularization, prototypical anchoring, and a novel, margin-aware pseudo-labeling strategy to actively reject noise. This core is supported and stabilized by foundational contrastive and teacher-student consistency losses, ensuring high-quality representations and robust optimization. ST-ProC outperforms all baselines by a significant margin, demonstrating its efficacy in real-world sparse-label settings, with a performance boost of 21.5% over state-of-the-art methods like FixMatch.

</details>


### [800] [Rare Genomic Subtype Discovery from RNA-seq via Autoencoder Embeddings and Stability-Aware Clustering](https://arxiv.org/abs/2511.13705)
*Alaa Mezghiche*

Main category: cs.LG

TL;DR: This paper uses autoencoder and clustering methods on RNA-seq data to identify rare and reproducible genomic subtypes, uncovering a potential new KIRC subtype.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the problem of discovering rare genomic subtypes in high-dimensional RNA-seq data, moving beyond standard classifications to enable better understanding and biomedical insights.

Method: The authors use an autoencoder to reduce the dimensionality of RNA-seq data, followed by k-means clustering and stability analysis. They apply this method to pan-cancer and within-cancer datasets, focusing particularly on KIRC.

Result: The analysis identifies that pan-cancer clustering aligns strongly with tissue of origin. For KIRC, they discover a rare (6.85%), but stable, cluster with distinct genomic markers.

Conclusion: The paper concludes that while pan-cancer clustering is dominated by tissue-specific factors, a stability-aware clustering approach within a single cancer type reveals rare but meaningful genomic subtypes, as demonstrated in KIRC.

Abstract: Unsupervised learning on high-dimensional RNA-seq data can reveal molecular subtypes beyond standard labels. We combine an autoencoder-based representation with clustering and stability analysis to search for rare but reproducible genomic subtypes. On the UCI "Gene Expression Cancer RNA-Seq" dataset (801 samples, 20,531 genes; BRCA, COAD, KIRC, LUAD, PRAD), a pan-cancer analysis shows clusters aligning almost perfectly with tissue of origin (Cramer's V = 0.887), serving as a negative control. We therefore reframe the problem within KIRC (n = 146): we select the top 2,000 highly variable genes, standardize them, train a feed-forward autoencoder (128-dimensional latent space), and run k-means for k = 2-10. While global indices favor small k, scanning k with a pre-specified discovery rule (rare < 10 percent and stable with Jaccard >= 0.60 across 20 seeds after Hungarian alignment) yields a simple solution at k = 5 (silhouette = 0.129, DBI = 2.045) with a rare cluster C0 (6.85 percent of patients) that is highly stable (Jaccard = 0.787). Cluster-vs-rest differential expression (Welch's t-test, Benjamini-Hochberg FDR) identifies coherent markers. Overall, pan-cancer clustering is dominated by tissue of origin, whereas a stability-aware within-cancer approach reveals a rare, reproducible KIRC subtype.

</details>


### [801] [From Black Box to Insight: Explainable AI for Extreme Event Preparedness](https://arxiv.org/abs/2511.13712)
*Kiana Vu,İsmet Selçuk Özer,Phung Lai,Zheng Wu,Thilanka Munasinghe,Jennifer Wei*

Main category: cs.LG

TL;DR: This paper focuses on using explainable AI to improve wildfire forecasting and enhance decision-making strategies.


<details>
  <summary>Details</summary>
Motivation: The urgency of accurate forecasting for extreme events like wildfires is driven by accelerating climate change and its impacts. AI shows potential, but its lack of explainability limits trust and real-world application.

Method: The study employs various AI models and SHAP to analyze features, decision pathways, and biases in wildfire prediction. Visualizations are used to contextualize feature importance and temporal/geospatial patterns.

Result: Explainable AI enhances model transparency, guides decision-making for experts, and offers actionable insights by clarifying decision pathways and biases. Visual aids boost interpretability.

Conclusion: For effective climate resilience and disaster management planning, AI models must be accurate, interpretable, trustworthy, and easily usable by policymakers and practitioners.

Abstract: As climate change accelerates the frequency and severity of extreme events such as wildfires, the need for accurate, explainable, and actionable forecasting becomes increasingly urgent. While artificial intelligence (AI) models have shown promise in predicting such events, their adoption in real-world decision-making remains limited due to their black-box nature, which limits trust, explainability, and operational readiness. This paper investigates the role of explainable AI (XAI) in bridging the gap between predictive accuracy and actionable insight for extreme event forecasting. Using wildfire prediction as a case study, we evaluate various AI models and employ SHapley Additive exPlanations (SHAP) to uncover key features, decision pathways, and potential biases in model behavior. Our analysis demonstrates how XAI not only clarifies model reasoning but also supports critical decision-making by domain experts and response teams. In addition, we provide supporting visualizations that enhance the interpretability of XAI outputs by contextualizing feature importance and temporal patterns in seasonality and geospatial characteristics. This approach enhances the usability of AI explanations for practitioners and policymakers. Our findings highlight the need for AI systems that are not only accurate but also interpretable, accessible, and trustworthy, essential for effective use in disaster preparedness, risk mitigation, and climate resilience planning.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [802] [Phase-Coded Memory and Morphological Resonance: A Next-Generation Retrieval-Augmented Generator Architecture](https://arxiv.org/abs/2511.11848)
*Denis V. Saklakov*

Main category: cs.NE

TL;DR: This paper proposes a Retrieval-Augmented Generator (RAG) overcoming transformer limitations using phase-coded memory and semantic resonance, enabling unlimited context and reducing computational needs.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of transformer models, particularly context-length restrictions and inefficiencies in handling long input sequences.

Method: The three-tier architecture includes Morphological Mapper for semantic waveforms, Field Memory Layer using holographic traces for retrieval, and Non-Contextual Generator producing outputs via resonance patterns.

Result: The paper demonstrates reduced memory, computational overhead, and effective semantic retrieval without token-level dependencies.

Conclusion: The approach successfully eliminates context-length barriers in transformers, promising energy, storage, and time efficiencies with theoretical and experimental support.

Abstract: This paper introduces a cognitive Retrieval-Augmented Generator (RAG) architecture that transcends transformer context-length limitations through phase-coded memory and morphological-semantic resonance. Instead of token embeddings, the system encodes meaning as complex wave patterns with amplitude-phase structure. A three-tier design is presented: a Morphological Mapper that transforms inputs into semantic waveforms, a Field Memory Layer that stores knowledge as distributed holographic traces and retrieves it via phase interference, and a Non-Contextual Generator that produces coherent output guided by resonance rather than fixed context. This approach eliminates sequential token dependence, greatly reduces memory and computational overhead, and enables unlimited effective context through frequency-based semantic access. The paper outlines theoretical foundations, pseudocode implementation, and experimental evidence from related complex-valued neural models, emphasizing substantial energy, storage, and time savings.

</details>


### [803] [Benchmarking that Matters: Rethinking Benchmarking for Practical Impact](https://arxiv.org/abs/2511.12264)
*Anna V. Kononova,Niki van Stein,Olaf Mersmann,Thomas Bäck,Thomas Bartz-Beielstein,Tobias Glasmachers,Michael Hellwig,Sebastian Krey,Jakub Kůdela,Boris Naujoks,Leonard Papenmeier,Elena Raponi,Quentin Renau,Jeroen Rook,Lennart Schäpermeier,Diederick Vermetten,Daniela Zaharie*

Main category: cs.NE

TL;DR: Evolutionary Computation's current benchmarking tools poorly reflect real-world optimization challenges. The authors advocate for curated, real-world-inspired benchmarks and a coordinated, evolving benchmarking ecosystem.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the gap between current benchmarking practices in Evolutionary Computation and the practicality, complexity, and constraints of real-world optimization problems.

Method: The authors identify issues with current practices, propose improvements like real-world-inspired benchmarks, accessible feature spaces, and community-maintained performance databases, and emphasize fostering a living benchmarking ecosystem.

Result: Identification of key gaps in existing benchmarks and a proposal for a sustainable, community-driven framework for benchmarking evolutionary computation.

Conclusion: To align scientific progress with practical needs, the authors conclude that an evolving, community-supported benchmarking ecosystem is essential for advancing theoretical and real-world applications.

Abstract: Benchmarking has driven scientific progress in Evolutionary Computation, yet current practices fall short of real-world needs. Widely used synthetic suites such as BBOB and CEC isolate algorithmic phenomena but poorly reflect the structure, constraints, and information limitations of continuous and mixed-integer optimization problems in practice. This disconnect leads to the misuse of benchmarking suites for competitions, automated algorithm selection, and industrial decision-making, despite these suites being designed for different purposes.
  We identify key gaps in current benchmarking practices and tooling, including limited availability of real-world-inspired problems, missing high-level features, and challenges in multi-objective and noisy settings. We propose a vision centered on curated real-world-inspired benchmarks, practitioner-accessible feature spaces and community-maintained performance databases. Real progress requires coordinated effort: A living benchmarking ecosystem that evolves with real-world insights and supports both scientific understanding and industrial use.

</details>


### [804] [Random-Key Metaheuristic and Linearization for the Quadratic Multiple Constraints Variable-Sized Bin Packing Problem](https://arxiv.org/abs/2511.12367)
*Natalia A. Santos,Marlon Jeske,Antonio A. Chaves*

Main category: cs.NE

TL;DR: The paper presents enhanced solutions for a complex optimization problem called QMC-VSBPP, using a linearized model and a hybrid Ant Colony Optimization algorithm.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges of solving the QMC-VSBPP by improving upon the state-of-the-art methods and providing strong lower and upper bounds for benchmarking.

Method: The authors introduce a linearized mathematical model for solving the problem using exact solvers and a novel Ant Colony Optimization algorithm integrated with adaptive Q-learning and local search techniques.

Result: Their linearized model achieves tighter lower bounds, while their optimization algorithm matches or surpasses previous best-known solutions and sets new benchmarks for large-scale problems.

Conclusion: The study demonstrates the effectiveness of combining evolutionary algorithms with mathematical optimization, offering valuable reference values and methodological advancements for tackling complex packing problems.

Abstract: This paper addresses the Quadratic Multiple Constraints Variable-Sized Bin Packing Problem (QMC-VSBPP), a challenging combinatorial optimization problem that generalizes the classical bin packing by incorporating multiple capacity dimensions, heterogeneous bin types, and quadratic interaction costs between items. We propose two complementary methods that advance the current state-of-the-art. First, a linearized mathematical formulation is introduced to eliminate quadratic terms, enabling the use of exact solvers such as Gurobi to compute strong lower bounds - reported here for the first time for this problem. Second, we develop RKO-ACO, a continuous-domain Ant Colony Optimization algorithm within the Random-Key Optimization framework, enhanced with adaptive Q-learning parameter control and efficient local search. Extensive computational experiments on benchmark instances show that the proposed linearized model produces significantly tighter lower bounds than the original quadratic formulation, while RKO-ACO consistently matches or improves upon all best-known solutions in the literature, establishing new upper bounds for large-scale instances. These results provide new reference values for future studies and demonstrate the effectiveness of evolutionary and random-key metaheuristic approaches for solving complex quadratic packing problems. Source code and data available at https://github.com/nataliaalves03/RKO-ACO

</details>


### [805] [Evolving Prompts for Toxicity Search in Large Language Models](https://arxiv.org/abs/2511.12487)
*Onkar Shelar,Travis Desell*

Main category: cs.NE

TL;DR: The paper presents ToxSearch, a framework to identify and evolve adversarial prompts that elicit toxic content from language models, demonstrating insights into prompt evolution, cross-model toxicity transfer, and model safety improvements.


<details>
  <summary>Details</summary>
Motivation: Address the vulnerability of large language models to adversarial prompts eliciting toxic content, even after safety alignment.

Method: Developed ToxSearch, a black-box evolutionary system employing various prompt-modification techniques (e.g., lexical substitutions, back-translation) that evolve prompts based on fitness feedback from a moderation oracle.

Result: Identifies heterogeneous operator behavior, observes cross-model toxicity attenuation, and finds that smaller models exhibit stronger resistance to adversarial prompts.

Conclusion: Effective adversarial testing requires anticipating cross-model exposure rather than solely hardening individual models, as small perturbations can systematically identify model flaws.

Abstract: Large Language Models remain vulnerable to adversarial prompts that elicit toxic content even after safety alignment. We present ToxSearch, a black-box evolutionary framework that tests model safety by evolving prompts in a synchronous steady-state loop. The system employs a diverse set of operators, including lexical substitutions, negation, back-translation, paraphrasing, and two semantic crossover operators, while a moderation oracle provides fitness guidance. Operator-level analysis shows heterogeneous behavior: lexical substitutions offer the best yield-variance trade-off, semantic-similarity crossover acts as a precise low-throughput inserter, and global rewrites exhibit high variance with elevated refusal costs. Using elite prompts evolved on LLaMA 3.1 8B, we observe practically meaningful but attenuated cross-model transfer, with toxicity roughly halving on most targets, smaller LLaMA 3.2 variants showing the strongest resistance, and some cross-architecture models retaining higher toxicity. These results suggest that small, controllable perturbations are effective vehicles for systematic red-teaming and that defenses should anticipate cross-model reuse of adversarial prompts rather than focusing only on single-model hardening.

</details>


### [806] [On Counts and Densities of Homogeneous Bent Functions: An Evolutionary Approach](https://arxiv.org/abs/2511.12652)
*Claude Carlet,Marko Ðurasevic,Domagoj Jakobovic,Luca Mariot,Stjepan Picek,Alexandr Polujan*

Main category: cs.NE

TL;DR: This paper employs Evolutionary Algorithms to design homogeneous bent Boolean functions, examining their density and successfully constructing quadratic and cubic bent functions.


<details>
  <summary>Details</summary>
Motivation: To enhance cryptographic security, the authors aim to design Boolean functions with strong cryptographic properties such as maximal nonlinearity and specific algebraic characteristics.

Method: The paper used Evolutionary Algorithms (a form of metaheuristic optimization) to evolve homogeneous bent Boolean functions while introducing a density concept for these functions.

Result: The study successfully identified and designed quadratic and cubic bent functions across varying numbers of variables.

Conclusion: The proposed approach provides effective solutions for algorithmically designing cryptographically strong homogeneous bent Boolean functions, especially quadratic and cubic ones.

Abstract: Boolean functions with strong cryptographic properties, such as high nonlinearity and algebraic degree, are important for the security of stream and block ciphers. These functions can be designed using algebraic constructions or metaheuristics. This paper examines the use of Evolutionary Algorithms (EAs) to evolve homogeneous bent Boolean functions, that is, functions whose algebraic normal form contains only monomials of the same degree and that are maximally nonlinear. We introduce the notion of density of homogeneous bent functions, facilitating the algorithmic design that results in finding quadratic and cubic bent functions in different numbers of variables.

</details>


### [807] [DS-ATGO: Dual-Stage Synergistic Learning via Forward Adaptive Threshold and Backward Gradient Optimization for Spiking Neural Networks](https://arxiv.org/abs/2511.13050)
*Jiaqiang Jiang,Wenfeng Xu,Jing Fan,Rui Yan*

Main category: cs.NE

TL;DR: The paper introduces a dual-stage synergistic learning algorithm enhancing spiking neural networks (SNNs) through adaptive thresholds and dynamic surrogate gradient adjustments, addressing gradient signal issues and achieving significant performance boosts.


<details>
  <summary>Details</summary>
Motivation: To enhance the efficiency and effectiveness of brain-inspired spiking neural networks (SNNs) by addressing training challenges related to imbalanced spike firing and gradient signal loss.

Method: The paper proposes a dual-stage synergistic learning algorithm comprising forward adaptive thresholding (to balance neuronal activity) and backward dynamic surrogate gradient adjustment (to enhance gradient estimation).

Result: The proposed method significantly improves SNN performance by enabling stable spike proportions across timesteps and layers and increasing the availability of gradients in deeper layers.

Conclusion: The dual-stage synergistic learning approach effectively tackles gradient signal degradation and spike imbalance issues, improving SNNs' spatio-temporal learning and performance.

Abstract: Brain-inspired spiking neural networks (SNNs) are recognized as a promising avenue for achieving efficient, low-energy neuromorphic computing. Direct training of SNNs typically relies on surrogate gradient (SG) learning to estimate derivatives of non-differentiable spiking activity. However, during training, the distribution of neuronal membrane potentials varies across timesteps and progressively deviates toward both sides of the firing threshold. When the firing threshold and SG remain fixed, this may lead to imbalanced spike firing and diminished gradient signals, preventing SNNs from performing well. To address these issues, we propose a novel dual-stage synergistic learning algorithm that achieves forward adaptive thresholding and backward dynamic SG. In forward propagation, we adaptively adjust thresholds based on the distribution of membrane potential dynamics (MPD) at each timestep, which enriches neuronal diversity and effectively balances firing rates across timesteps and layers. In backward propagation, drawing from the underlying association between MPD, threshold, and SG, we dynamically optimize SG to enhance gradient estimation through spatio-temporal alignment, effectively mitigating gradient information loss. Experimental results demonstrate that our method achieves significant performance improvements. Moreover, it allows neurons to fire stable proportions of spikes at each timestep and increases the proportion of neurons that obtain gradients in deeper layers.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [808] [Large-scale Multigrid with Adaptive Galerkin Coarsening](https://arxiv.org/abs/2511.13109)
*Fabian Böhm,Nils Kohl,Harald Köstler,Ulrich Rüde*

Main category: cs.PF

TL;DR: The paper presents a robust and memory-efficient adaptive coarse-grid correction scheme for matrix-free geometric multigrid methods targeting PDEs with highly variable coefficients.


<details>
  <summary>Details</summary>
Motivation: To enhance multigrid methods for solving PDEs, particularly under challenging conditions like strongly varying coefficients.

Method: The approach combines uniform geometric coarsening with a selective use of Galerkin coarse-grid approximation in regions with large coefficient gradients, while using lightweight coarse approximations elsewhere to minimize memory usage.

Result: The proposed method efficiently solves generalized Stokes equations with extremely large problems (e.g., $10^{10}$ degrees of freedom), significant viscosity jumps, and high parallel processing scales.

Conclusion: The method provides a scalable, robust, and memory-efficient solution for PDE problems with varying coefficients, showcasing its strength in high-complexity scenarios and large-scale computational settings.

Abstract: We propose a robust, adaptive coarse-grid correction scheme for matrix-free geometric multigrid targeting PDEs with strongly varying coefficients. The method combines uniform geometric coarsening of the underlying grid with heterogeneous coarse-grid operators: Galerkin coarse grid approximation is applied locally in regions with large coefficient gradients, while lightweight, direct coarse grid approximation is used elsewhere. This selective application ensures that local Galerkin operators are computed and stored only where necessary, minimizing memory requirements while maintaining robust convergence. We demonstrate the method on a suite of sinker benchmark problems for the generalized Stokes equation, including grid-aligned and unaligned viscosity jumps, smoothly varying viscosity functions with large gradients, and different viscosity evaluation techniques. We analytically quantify the solver's memory consumption and demonstrate its efficiency by solving Stokes problems with $10^{10}$ degrees of freedom, viscosity jumps of $10^{6}$ magnitude, and more than 100{,}000 parallel processes.

</details>


### [809] [Evaluation of Domain-Specific Architectures for General-Purpose Applications in Apple Silicon](https://arxiv.org/abs/2511.13450)
*Álvaro Corrochano López,Carlos García Sánchez*

Main category: cs.PF

TL;DR: The paper explores whether specialized accelerators like Apple's Neural Engine (ANE) can replicate the role of GPUs in general-purpose HPC applications, focusing on performance and energy efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in investigating the potential of domain-specific accelerators, particularly Apple's Neural Engine (ANE), in general-purpose high-performance computing (HPC) tasks beyond AI workloads.

Method: The study involves evaluating the Apple Neural Engine (ANE) on classic HPC algorithms such as GEMM, Jacobi, and Multigrid methods across Apple's M1 and M4 architectures, comparing performance and energy efficiency.

Result: The ANE achieves competitive processing performance (up to 3.8 TFlops on M4-Pro) and significantly better energy efficiency compared to GPUs (e.g., 5.2 Watts on ANE vs. 24 Watts on GPUs in M4 for GEMM tasks).

Conclusion: Domain-specific accelerators like Apple's ANE are competitive in performance and highly energy-efficient, demonstrating their viability for general-purpose HPC applications if appropriately adapted.

Abstract: The rise of AI and its growing computational demands have driven the integration of domain-specific accelerators (such as GPUs, TPUs, and NPUs) across the entire computing infrastructure. Following the precedent set by the GPGPU which popularized GPUs for general-purpose tasks, this research asks whether this phenomenon can be replicated with specialized accelerators like NPUs in new contexts. This paper evaluates the potential of the Apple Neural Engine (ANE) designed for high energy efficiency in Machine Learning workloads, in the context of general-purpose HPC applications. We evaluate the performance and energy consumption of classic HPC algorithms such as GEMM, Jacobi or Multigrid methods on Apple's ANE across the M1 and the latest M4 architectures. Results confirm that, when algorithms are properly adapted, the ANE achieves competitive performance (up to 3.8 TFlops on the M4-Pro, comparable to the GPU's 4.7 TFlops on the same SoC for GEMM operation) while demonstrating significantly superior energy efficiency (e.g., GEMM consumes 5.2 Watts on the ANE versus 24 Watts on GPU counterpart in M4 architectures).

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [810] [Modular GPU Programming with Typed Perspectives](https://arxiv.org/abs/2511.11939)
*Manya Bansal,Daniel Sainati,Joseph W. Cutler,Saman Amarasinghe,Jonathan Ragan-Kelley*

Main category: cs.PL

TL;DR: This paper introduces Prism, a new GPU programming language designed for modularity and control over collective operations, ensuring high performance and safety.


<details>
  <summary>Details</summary>
Motivation: Current GPU programming lacks modularity and often leads to errors due to the need to balance control over individual and collective thread operations. This paper aims to address this challenge.

Method: The authors propose Prism, incorporating typed perspectives to represent thread control granularity at the type level. They implement a compiler and establish a theoretical foundation using the Bundl calculus.

Result: Prism facilitates writing modular GPU code confidently, while achieving performance comparable or superior to state-of-the-art GPU kernels.

Conclusion: Prism enhances safety and modularity in GPU programming without compromising performance, solving a key tension in thread operation management.

Abstract: To achieve peak performance on modern GPUs, one must balance two frames of mind: issuing instructions to individual threads to control their behavior, while simultaneously tracking the convergence of many threads acting in concert to perform collective operations like Tensor Core instructions. The tension between these two mindsets makes modular programming error prone. Functions that encapsulate collective operations, despite being called per-thread, must be executed cooperatively by groups of threads.
  In this work, we introduce Prism, a new GPU language that restores modularity while still giving programmers the low-level control over collective operations necessary for high performance. Our core idea is typed perspectives, which materialize, at the type level, the granularity at which the programmer is controlling the behavior of threads. We describe the design of Prism, implement a compiler for it, and lay its theoretical foundations in a core calculus called Bundl. We implement state-of-the-art GPU kernels in Prism and find that it offers programmers the safety guarantees needed to confidently write modular code without sacrificing performance.

</details>


### [811] [The Search for Constrained Random Generators](https://arxiv.org/abs/2511.12253)
*Harrison Goldstein,Hila Peleg,Cassia Torczon,Daniel Sainati,Leonidas Lampropoulos,Benjamin C. Pierce*

Main category: cs.PL

TL;DR: The paper introduces a novel method for constrained random generation in property-based testing using deductive program synthesis, presenting rules and implementation in the Lean theorem prover.


<details>
  <summary>Details</summary>
Motivation: Property-based testing relies on generating random values that meet specific predicates, which is challenging due to sparseness and constraints on valid test cases.

Method: The approach presents synthesis rules leveraging denotational semantics of generators, handling recursive predicates through catamorphisms and anamorphisms for simplicity and expressiveness.

Result: An implementation named Palamedes is developed as an extensible library within the Lean theorem prover, integrating synthesis algorithms with standard proof-search tactics.

Conclusion: This method simplifies recursive function synthesis in testing, improves expressiveness, and explores integration within proof automation systems like Lean.

Abstract: Among the biggest challenges in property-based testing (PBT) is the constrained random generation problem: given a predicate on program values, randomly sample from the set of all values satisfying that predicate, and only those values. Efficient solutions to this problem are critical, since the executable specifications used by PBT often have preconditions that input values must satisfy in order to be valid test cases, and satisfying values are often sparsely distributed.
  We propose a novel approach to this problem using ideas from deductive program synthesis. We present a set of synthesis rules, based on a denotational semantics of generators, that give rise to an automatic procedure for synthesizing correct generators. Our system handles recursive predicates by rewriting them as catamorphisms and then matching with appropriate anamorphisms; this is theoretically simpler than other approaches to synthesis for recursive functions, yet still extremely expressive.
  Our implementation, Palamedes, is an extensible library for the Lean theorem prover. The synthesis algorithm itself is built on standard proof-search tactics, reducing implementation burden and allowing the algorithm to benefit from further advances in Lean proof automation.

</details>


### [812] [Equivalence Checking of ML GPU Kernels](https://arxiv.org/abs/2511.12638)
*Kshitij Dubey,Benjamin Driscoll,Anjiang Wei,Neeraj Kayal,Rahul Sharma,Alex Aiken*

Main category: cs.PL

TL;DR: The paper introduces VOLTA, an equivalence checker to verify the correctness of GPU kernels optimized by different sources, including hand, LLMs, and compilers.


<details>
  <summary>Details</summary>
Motivation: The need arises due to extensive GPU kernel optimization efforts driven by the increasing use of deep learning and LLMs, while lacking formal guarantees about correctness.

Method: The authors developed VOLTA, an equivalence checker designed to ensure soundness and completeness for a specific class of GPU kernels, verifying operations such as convolutions and matrix multiplications.

Result: VOLTA proved capable of verifying a range of ML computations, showing its ability to formally verify correctness in GPU kernel optimization.

Conclusion: VOLTA facilitates formal verification of GPU kernels optimized by various methods, enhancing reliability in high-performance ML computations.

Abstract: With the rapid progress of deep learning and large language models (LLMs), companies now spend enormous sums executing GPU kernels. These kernels have, therefore, become prime targets for aggressive optimization. Recent efforts increasingly leverage LLMs to generate GPU kernels, but make no formal guarantees about the generated kernels. We present the first equivalence checker for GPU kernels and use it to formally verify the correctness of machine learning (ML) kernels optimized by hand, by LLMs, and by compilers. We show that our equivalence checker is sound and, for a well-defined class of GPU kernels which includes the programs of interest, complete. Our implementation, VOLTA, can verify ML computations such as convolutions, matrix multiplications, and various attention mechanisms.

</details>


### [813] [Cost-Driven Synthesis of Sound Abstract Interpreters](https://arxiv.org/abs/2511.13663)
*Qiuhan Gu,Avaljot Singh,Gagandeep Singh*

Main category: cs.PL

TL;DR: The paper presents a framework using LLMs to synthesize sound abstract interpreters, addressing a challenge in neural network verification.


<details>
  <summary>Details</summary>
Motivation: To reduce the difficulty in constructing sound abstract interpreters by leveraging modern LLMs for automatic synthesis.

Method: The authors formulate synthesis as a constrained optimization problem, introduce a mathematically grounded cost function for measuring unsoundness, and design a unified framework combining LLM-based generation with syntactic/semantic validation and feedback.

Result: The framework matches handcrafted transformer quality and discovers high-precision sound transformers for nonlinear operators absent in current literature.

Conclusion: Leveraging LLMs with guided frameworks can advance the construction of sound abstract interpreters, achieving high precision and solving complex problems in neural network verification.

Abstract: Constructing abstract interpreters that provide global soundness guarantees remains a major obstacle in abstract interpretation. We investigate whether modern LLMs can reduce this burden by leveraging them to synthesize sound, non-trivial abstract interpreters across multiple abstract domains in the setting of neural network verification. We formulate synthesis as a constrained optimization problem and introduce a novel mathematically grounded cost function for measuring unsoundness under strict syntactic and semantic constraints. Based on this formulation, we develop a unified framework that unifies LLM-based generation with syntactic and semantic validation and a quantitative cost-guided feedback mechanism. Empirical results demonstrate that our framework not only matches the quality of handcrafted transformers, but more importantly, discovers sound, high-precision transformers for complex nonlinear operators that are absent from existing literature.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [814] [Hierarchical Federated Graph Attention Networks for Scalable and Resilient UAV Collision Avoidance](https://arxiv.org/abs/2511.11616)
*Rathin Chandra Shit,Sharmila Subudhi*

Main category: cs.RO

TL;DR: The paper proposes a hierarchical framework to balance real-time performance, adversarial resiliency, and privacy preservation in large-scale multi-UAV systems.


<details>
  <summary>Details</summary>
Motivation: Current multi-UAV collision avoidance systems suffer from high computational complexity and lack Byzantine fault tolerance, necessitating an innovative approach.

Method: A three-layer architecture is introduced: (1) local layer using dense graph attention for immediate collision avoidance, (2) regional layer with sparse attention and asynchronous federated learning, and (3) global layer using a Hashgraph-inspired protocol with adaptive differential privacy.

Result: The framework achieves scalable performance for 500 UAVs with a collision rate below 2% and decision latencies under 50ms, while maintaining Byzantine fault tolerance of up to f<n/3.

Conclusion: This approach demonstrates a scalable, fault-tolerant, and privacy-preserving collision avoidance solution for large UAV swarms.

Abstract: The real-time performance, adversarial resiliency, and privacy preservation are the most important metrics that need to be balanced to practice collision avoidance in large-scale multi-UAV (Unmanned Aerial Vehicle) systems. Current frameworks tend to prescribe monolithic solutions that are not only prohibitively computationally complex with a scaling cost of $O(n^2)$ but simply do not offer Byzantine fault tolerance. The proposed hierarchical framework presented in this paper tries to eliminate such trade-offs by stratifying a three-layered architecture. We spread the intelligence into three layers: an immediate collision avoiding local layer running on dense graph attention with latency of $<10 ms$, a regional layer using sparse attention with $O(nk)$ computational complexity and asynchronous federated learning with coordinate-wise trimmed mean aggregation, and lastly, a global layer using a lightweight Hashgraph-inspired protocol. We have proposed an adaptive differential privacy mechanism, wherein the noise level $(ε\in [0.1, 1.0])$ is dynamically reduced based on an evaluation of the measured real-time threat that in turn maximized the privacy-utility tradeoff. Through the use of Distributed Hash Table (DHT)-based lightweight audit logging instead of heavyweight blockchain consensus, the median cost of getting a $95^{th}$ percentile decision within 50ms is observed across all tested swarm sizes. This architecture provides a scalable scenario of 500 UAVs with a collision rate of $< 2.0\%$ and the Byzantine fault tolerance of $f < n/3$.

</details>


### [815] [Tactile Data Recording System for Clothing with Motion-Controlled Robotic Sliding](https://arxiv.org/abs/2511.11634)
*Michikuni Eguchi,Takekazu Kitagishi,Yuichi Hiroi,Takefumi Hiraki*

Main category: cs.RO

TL;DR: The paper presents a robotic system for systematic tactile data collection from garments to study wearer comfort.


<details>
  <summary>Details</summary>
Motivation: Understanding physical properties that make clothing comfortable through tactile sensation.

Method: A robotic arm-based system with a simulated fingertip collects motion-labeled tactile data of garments.

Result: Machine learning analysis revealed improved accuracy using motion-related parameters for tactile data.

Conclusion: The system provides a scalable, non-destructive way to study and characterize tactile sensations of fabrics.

Abstract: The tactile sensation of clothing is critical to wearer comfort. To reveal physical properties that make clothing comfortable, systematic collection of tactile data during sliding motion is required. We propose a robotic arm-based system for collecting tactile data from intact garments. The system performs stroking measurements with a simulated fingertip while precisely controlling speed and direction, enabling creation of motion-labeled, multimodal tactile databases. Machine learning evaluation showed that including motion-related parameters improved identification accuracy for audio and acceleration data, demonstrating the efficacy of motion-related labels for characterizing clothing tactile sensation. This system provides a scalable, non-destructive method for capturing tactile data of clothing, contributing to future studies on fabric perception and reproduction.

</details>


### [816] [Image-based Morphological Characterization of Filamentous Biological Structures with Non-constant Curvature Shape Feature](https://arxiv.org/abs/2511.11639)
*Jie Fan,Francesco Visentin,Barbara Mazzolai,Emanuela Del Dottore*

Main category: cs.RO

TL;DR: The study introduces an image-based method using a 3D geometrical model to analyze shape changes in plant tendrils, showing responsiveness to mechano-stimulation.


<details>
  <summary>Details</summary>
Motivation: To understand the relationship between tendril shape changes, stimuli, and contact points, and address the challenges in analyzing temporal changes in climbing plant biomechanics.

Method: The proposed method uses a 3D Piece-Wise Clothoid-based model to reconstruct tendril shapes after mechanical rubbing, offering an image-based geometric analysis.

Result: The method achieved high accuracy (R2 > 0.99) and demonstrated the tendril's higher responsiveness in the apical segment.

Conclusion: This approach provides reliable biomechanical analysis of plant tendrils, offering insights for bio-inspired robotics design and advancements in plant biomechanics studies.

Abstract: Tendrils coil their shape to anchor the plant to supporting structures, allowing vertical growth toward light. Although climbing plants have been studied for a long time, extracting information regarding the relationship between the temporal shape change, the event that triggers it, and the contact location is still challenging. To help build this relation, we propose an image-based method by which it is possible to analyze shape changes over time in tendrils when mechano-stimulated in different portions of their body. We employ a geometric approach using a 3D Piece-Wise Clothoid-based model to reconstruct the configuration taken by a tendril after mechanical rubbing. The reconstruction shows high robustness and reliability with an accuracy of R2 > 0.99. This method demonstrates distinct advantages over deep learning-based approaches, including reduced data requirements, lower computational costs, and interpretability. Our analysis reveals higher responsiveness in the apical segment of tendrils, which might correspond to higher sensitivity and tissue flexibility in that region of the organs. Our study provides a methodology for gaining new insights into plant biomechanics and offers a foundation for designing and developing novel intelligent robotic systems inspired by climbing plants.

</details>


### [817] [ExpertAD: Enhancing Autonomous Driving Systems with Mixture of Experts](https://arxiv.org/abs/2511.11740)
*Haowen Jiang,Xinyu Huang,You Lu,Dingji Wang,Yuheng Cao,Chaofeng Sha,Bihuan Chen,Keyu Chen,Xin Peng*

Main category: cs.RO

TL;DR: The study introduces ExpertAD, which leverages Mixture of Experts (MoE) architecture to enhance autonomous driving systems by improving perception, reducing task interference, and minimizing latency.


<details>
  <summary>Details</summary>
Motivation: To address challenges in end-to-end autonomous driving systems, such as ambiguous semantics, task interference, and high inference latency, which compromise safety and decision-making.

Method: The paper proposes ExpertAD, an advanced framework incorporating a Perception Adapter (PA) for contextually relevant perception and a Mixture of Sparse Experts (MoSE) for minimizing task interference during prediction.

Result: ExpertAD achieves up to a 20% reduction in collision rates and a 25% reduction in inference latency compared to existing methods. It also demonstrates strong performance in rare driving scenarios and generalization to new environments.

Conclusion: ExpertAD enhances task performance and reduces latency in autonomous driving, making it suitable for complex, real-world traffic scenarios.

Abstract: Recent advancements in end-to-end autonomous driving systems (ADSs) underscore their potential for perception and planning capabilities. However, challenges remain. Complex driving scenarios contain rich semantic information, yet ambiguous or noisy semantics can compromise decision reliability, while interference between multiple driving tasks may hinder optimal planning. Furthermore, prolonged inference latency slows decision-making, increasing the risk of unsafe driving behaviors. To address these challenges, we propose ExpertAD, a novel framework that enhances the performance of ADS with Mixture of Experts (MoE) architecture. We introduce a Perception Adapter (PA) to amplify task-critical features, ensuring contextually relevant scene understanding, and a Mixture of Sparse Experts (MoSE) to minimize task interference during prediction, allowing for effective and efficient planning. Our experiments show that ExpertAD reduces average collision rates by up to 20% and inference latency by 25% compared to prior methods. We further evaluate its multi-skill planning capabilities in rare scenarios (e.g., accidents, yielding to emergency vehicles) and demonstrate strong generalization to unseen urban environments. Additionally, we present a case study that illustrates its decision-making process in complex driving scenarios.

</details>


### [818] [Large Language Models and 3D Vision for Intelligent Robotic Perception and Autonomy: A Review](https://arxiv.org/abs/2511.11777)
*Vinit Mehta,Charu Sharma,Karthick Thiyagarajan*

Main category: cs.RO

TL;DR: The paper discusses the integration of Large Language Models (LLMs) with 3D vision for robotic sensing, covering methodologies, applications, and challenges.


<details>
  <summary>Details</summary>
Motivation: To explore how combining LLMs and 3D vision can enhance robotic perception and interaction, enabling machines to better understand and operate in complex environments.

Method: A comprehensive review of foundational principles, current methodologies, key advancements, and multimodal LLM applications integrating 3D data, along with benchmarking and evaluation strategies.

Result: The study highlights state-of-the-art techniques in areas like scene understanding, text-to-3D generation, and language-guided manipulation, alongside multimodal integrations for enhanced robotic decision-making.

Conclusion: Integrating LLMs with 3D vision can revolutionize robotic sensing by bridging linguistic intelligence and spatial perception, but further work is needed on adaptive models, cross-modal alignment, and real-time processing for fully autonomous systems.

Abstract: With the rapid advancement of artificial intelligence and robotics, the integration of Large Language Models (LLMs) with 3D vision is emerging as a transformative approach to enhancing robotic sensing technologies. This convergence enables machines to perceive, reason and interact with complex environments through natural language and spatial understanding, bridging the gap between linguistic intelligence and spatial perception. This review provides a comprehensive analysis of state-of-the-art methodologies, applications and challenges at the intersection of LLMs and 3D vision, with a focus on next-generation robotic sensing technologies. We first introduce the foundational principles of LLMs and 3D data representations, followed by an in-depth examination of 3D sensing technologies critical for robotics. The review then explores key advancements in scene understanding, text-to-3D generation, object grounding and embodied agents, highlighting cutting-edge techniques such as zero-shot 3D segmentation, dynamic scene synthesis and language-guided manipulation. Furthermore, we discuss multimodal LLMs that integrate 3D data with touch, auditory and thermal inputs, enhancing environmental comprehension and robotic decision-making. To support future research, we catalog benchmark datasets and evaluation metrics tailored for 3D-language and vision tasks. Finally, we identify key challenges and future research directions, including adaptive model architectures, enhanced cross-modal alignment and real-time processing capabilities, which pave the way for more intelligent, context-aware and autonomous robotic sensing systems.

</details>


### [819] [LAVQA: A Latency-Aware Visual Question Answering Framework for Shared Autonomy in Self-Driving Vehicles](https://arxiv.org/abs/2511.11840)
*Shuangyu Xie,Kaiyuan Chen,Wenjing Chen,Chengyuan Qian,Christian Juette,Liu Ren,Dezhen Song,Ken Goldberg*

Main category: cs.RO

TL;DR: LAVQA is a shared autonomy framework that addresses latency issues in self-driving vehicles by integrating Visual Question Answering and risk visualization, significantly reducing collision rates.


<details>
  <summary>Details</summary>
Motivation: High uncertainty in autonomous driving may require human guidance for safety, making the combination of human and machine essential to ensure optimal responses.

Method: The paper introduces LAVQA, which uses latency-aware visual queries combined with a Latency-Induced Collision Map to dynamically visualize safety regions affected by latency.

Result: Simulations in CARLA demonstrate that LAVQA reduces collision rates by over 8x compared to systems that do not account for latency.

Conclusion: LAVQA is an effective solution for improving the safety of autonomous systems in high latency conditions by enabling enhanced shared autonomy with human operators.

Abstract: When uncertainty is high, self-driving vehicles may halt for safety and benefit from the access to remote human operators who can provide high-level guidance. This paradigm, known as {shared autonomy}, enables autonomous vehicle and remote human operators to jointly formulate appropriate responses. To address critical decision timing with variable latency due to wireless network delays and human response time, we present LAVQA, a latency-aware shared autonomy framework that integrates Visual Question Answering (VQA) and spatiotemporal risk visualization. LAVQA augments visual queries with Latency-Induced COllision Map (LICOM), a dynamically evolving map that represents both temporal latency and spatial uncertainty. It enables remote operator to observe as the vehicle safety regions vary over time in the presence of dynamic obstacles and delayed responses. Closed-loop simulations in CARLA, the de-facto standard for autonomous vehicle simulator, suggest that that LAVQA can reduce collision rates by over 8x compared to latency-agnostic baselines.

</details>


### [820] [Autonomous Underwater Cognitive System for Adaptive Navigation: A SLAM-Integrated Cognitive Architecture](https://arxiv.org/abs/2511.11845)
*K. A. I. N Jayarathne,R. M. N. M. Rathnayaka,D. P. S. S. Peiris*

Main category: cs.RO

TL;DR: The paper introduces an advanced underwater cognitive system leveraging SLAM and Soar architecture for better navigation in challenging oceanic conditions.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in deep-sea exploration such as disorientation, communication loss, and navigation failures.

Method: The proposed system integrates multi-sensor data with cognitive reasoning modules, incorporating semantic understanding, adaptive sensor management, and memory-based learning.

Result: The system reduces false loop closures and improves map consistency while demonstrating an effective perception-cognition-action-learning loop.

Conclusion: The architecture enhances safety, reliability, and autonomy in underwater exploration, serving as a foundation for advanced submersible cognitive systems.

Abstract: Deep-sea exploration poses significant challenges, including disorientation, communication loss, and navigational failures in dynamic underwater environments. This paper presents an Autonomous Underwater Cognitive System (AUCS) that integrates Simultaneous Localization and Mapping (SLAM) with a Soar-based cognitive architecture to enable adaptive navigation in complex oceanic conditions. The system fuses multi-sensor data from SONAR, LiDAR, IMU, and DVL with cognitive reasoning modules for perception, attention, planning, and learning. Unlike conventional SLAM systems, AUCS incorporates semantic understanding, adaptive sensor management, and memory-based learning to differentiate between dynamic and static objects, reducing false loop closures and enhancing long-term map consistency. The proposed architecture demonstrates a complete perception-cognition-action-learning loop, allowing autonomous underwater vehicles to sense, reason, and adapt intelligently. This work lays a foundation for next-generation cognitive submersible systems, improving safety, reliability, and autonomy in deep-sea exploration.

</details>


### [821] [MATT-Diff: Multimodal Active Target Tracking by Diffusion Policy](https://arxiv.org/abs/2511.11931)
*Saida Liu,Nikolay Atanasov,Shumon Koga*

Main category: cs.RO

TL;DR: This paper introduces MATT-Diff, a diffusion model-based control policy for multi-target tracking, handling exploration, tracking, and reacquisition across varied target behaviors.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in multi-target tracking, specifically balancing exploration for lost targets and tracking for uncertain detected ones, without relying on prior knowledge of targets.

Method: A diffusion policy trained on data from three expert planners, employing a vision transformer for map tokenization and attention mechanisms for handling multiple target estimates modeled as Gaussian densities.

Result: MATT-Diff outperformed expert and behavior cloning baselines in tracking targets moving in diverse patterns, demonstrating its effectiveness.

Conclusion: MATT-Diff advances the state of multi-target tracking by effectively leveraging diffusion models for multi-modal planning and decision-making.

Abstract: This paper proposes MATT-Diff: Multi-Modal Active Target Tracking by Diffusion Policy, a control policy that captures multiple behavioral modes - exploration, dedicated tracking, and target reacquisition - for active multi-target tracking. The policy enables agent control without prior knowledge of target numbers, states, or dynamics. Effective target tracking demands balancing exploration for undetected or lost targets with following the motion of detected but uncertain ones. We generate a demonstration dataset from three expert planners including frontier-based exploration, an uncertainty-based hybrid planner switching between frontier-based exploration and RRT* tracking based on target uncertainty, and a time-based hybrid planner switching between exploration and tracking based on target detection time. We design a control policy utilizing a vision transformer for egocentric map tokenization and an attention mechanism to integrate variable target estimates represented by Gaussian densities. Trained as a diffusion model, the policy learns to generate multi-modal action sequences through a denoising process. Evaluations demonstrate MATT-Diff's superior tracking performance against expert and behavior cloning baselines across multiple target motions, empirically validating its advantages in target tracking.

</details>


### [822] [Characterization and Evaluation of Screw-Based Locomotion Across Aquatic, Granular, and Transitional Media](https://arxiv.org/abs/2511.11958)
*Derek Chen,Zoe Samuels,Lizzie Peiros,Sujaan Mukherjee,Michael C. Yip*

Main category: cs.RO

TL;DR: The paper explores screw-based propulsion for amphibious mobility, studying its performance in sand and water under various conditions.


<details>
  <summary>Details</summary>
Motivation: To enhance screw-based propulsion systems for amphibious mobility by addressing challenges in optimizing locomotion across diverse terrains.

Method: Systematic analysis of various screw configurations in dry sand, wet sand, saturated sand, and water, inspired by heat sink design principles.

Result: Identified dominant parameters impacting screw performance; provided insights for categorizing performance based on media-specific conditions.

Conclusion: Design recommendations and adaptive locomotion strategies were formulated to improve screw-based propulsion systems for versatile applications.

Abstract: Screw-based propulsion systems offer promising capabilities for amphibious mobility, yet face significant challenges in optimizing locomotion across water, granular materials, and transitional environments. This study presents a systematic investigation into the locomotion performance of various screw configurations in media such as dry sand, wet sand, saturated sand, and water. Through a principles-first approach to analyze screw performance, it was found that certain parameters are dominant in their impact on performance. Depending on the media, derived parameters inspired from optimizing heat sink design help categorize performance within the dominant design parameters. Our results provide specific insights into screw shell design and adaptive locomotion strategies to enhance the performance of screw-based propulsion systems for versatile amphibious applications.

</details>


### [823] [Bootstrapped LLM Semantics for Context-Aware Path Planning](https://arxiv.org/abs/2511.11967)
*Mani Amani,Behrad Beheshti,Reza Akhavian*

Main category: cs.RO

TL;DR: The paper introduces a framework that combines large language models (LLMs) with classical planning to enable robots to safely and efficiently execute tasks in human-centric spaces by utilizing semantic sensing and Bayesian decision-making.


<details>
  <summary>Details</summary>
Motivation: Robots often lack a way to incorporate semantic context or natural language inputs to safely and efficiently navigate and complete tasks in human environments.

Method: The authors turn a large language model (LLM) into a stochastic semantic sensor, leveraging Bayesian bootstrap to assess risks and integrating these into classical path planning for adaptable navigation in semantically rich environments.

Result: The framework performs successfully across simulated environments and a digital twin, showing adaptability in robot movement based on natural language instructions and contextual cues.

Conclusion: The method demonstrates how LLMs can guide robot planning by incorporating natural language inputs and contextual information to make safer and more efficient decisions in human-centric spaces.

Abstract: Prompting robots with natural language (NL) has largely been studied as what task to execute (goal selection, skill sequencing) rather than how to execute that task safely and efficiently in semantically rich, human-centric spaces. We address this gap with a framework that turns a large language model (LLM) into a stochastic semantic sensor whose outputs modulate a classical planner. Given a prompt and a semantic map, we draw multiple LLM "danger" judgments and apply a Bayesian bootstrap to approximate a posterior over per-class risk. Using statistics from the posterior, we create a potential cost to formulate a path planning problem. Across simulated environments and a BIM-backed digital twin, our method adapts how the robot moves in response to explicit prompts and implicit contextual information. We present qualitative and quantitative results.

</details>


### [824] [ARCSnake V2: An Amphibious Multi-Domain Screw-Propelled Snake-Like Robot](https://arxiv.org/abs/2511.11970)
*Sara Wickenhiser,Lizzie Peiros,Calvin Joyce,Peter Gavrilrov,Sujaan Mukherjee,Syler Sylvester,Junrong Zhou,Mandy Cheung,Jason Lim,Florian Richter,Michael C. Yip*

Main category: cs.RO

TL;DR: The paper introduces ARCSnake V2, an amphibious robotic system capable of versatile locomotion across land, granular, and aquatic terrains.


<details>
  <summary>Details</summary>
Motivation: To address challenges faced by conventional robots in extreme and diverse terrains, especially for exploration, search and rescue, and monitoring.

Method: The design integrates Archimedean screw propulsion with snake-like mobility, featuring water-sealing, buoyancy control, teleoperation, and multiple locomotion modes.

Result: ARCSnake V2 demonstrated effective multi-terrain locomotion, underwater maneuverability, and robust communication through extensive experiments.

Conclusion: ARCSnake V2 serves as a versatile platform for challenging environments, enabling exploration and rescue missions in multi-domain conditions.

Abstract: Robotic exploration in extreme environments such as caves, oceans, and planetary surfaces pose significant challenges, particularly in locomotion across diverse terrains. Conventional wheeled or legged robots often struggle in these contexts due to surface variability. This paper presents ARCSnake V2, an amphibious, screw propelled, snake like robot designed for teleoperated or autonomous locomotion across land, granular media, and aquatic environments. ARCSnake V2 combines the high mobility of hyper redundant snake robots with the terrain versatility of Archimedean screw propulsion. Key contributions include a water sealed mechanical design with serially linked screw and joint actuation, an integrated buoyancy control system, and teleoperation via a kinematically matched handheld controller. The robots design and control architecture enable multiple locomotion modes screwing, wheeling, and sidewinding with smooth transitions between them. Extensive experiments validate its underwater maneuverability, communication robustness, and force regulated actuation. These capabilities position ARCSnake V2 as a versatile platform for exploration, search and rescue, and environmental monitoring in multi domain settings.

</details>


### [825] [SBAMP: Sampling Based Adaptive Motion Planning](https://arxiv.org/abs/2511.12022)
*Anh-Quan Pham,Kabir Ram Puri,Shreyas Raorane*

Main category: cs.RO

TL;DR: The paper presents the Sampling-Based Adaptive Motion Planning (SBAMP) framework, which combines RRT* and SEDS-based controllers to achieve adaptive, collision-free navigation without relying on pre-trained datasets.


<details>
  <summary>Details</summary>
Motivation: Robotic systems need to navigate dynamic environments in real time, but current methods either lack adaptability or require pre-trained data that limits their generalization.

Method: SBAMP integrates RRT* for global planning and SEDS-based controllers for real-time trajectory adjustment, ensuring stability through Lyapunov guarantees.

Result: Experiments using simulated environments and RoboRacer hardware demonstrated SBAMP's effectiveness in dynamic obstacle handling, perturbation recovery, and adaptation to sharp turns.

Conclusion: SBAMP provides a scalable and reliable solution for navigating dynamic, unstructured environments, balancing adaptability and path optimality.

Abstract: Autonomous robotic systems must navigate complex, dynamic environments in real time, often facing unpredictable obstacles and rapidly changing conditions. Traditional sampling-based methods, such as RRT*, excel at generating collision-free paths but struggle to adapt to sudden changes without extensive replanning. Conversely, learning-based dynamical systems, such as the Stable Estimator of Dynamical Systems (SEDS), offer smooth, adaptive trajectory tracking but typically rely on pre-collected demonstration data, limiting their generalization to novel scenarios. This paper introduces Sampling-Based Adaptive Motion Planning (SBAMP), a novel framework that overcomes these limitations by integrating RRT* for global path planning with a SEDS-based local controller for continuous, adaptive trajectory adjustment. Our approach requires no pre-trained datasets and ensures smooth transitions between planned waypoints, maintaining stability through Lyapunov-based guarantees. We validate SBAMP in both simulated environments and real hardware using the RoboRacer platform, demonstrating superior performance in dynamic obstacle scenarios, rapid recovery from perturbations, and robust handling of sharp turns. Experimental results highlight SBAMP's ability to adapt in real time without sacrificing global path optimality, providing a scalable solution for dynamic, unstructured environments.

</details>


### [826] [Decoupled Action Head: Confining Task Knowledge to Conditioning Layers](https://arxiv.org/abs/2511.12101)
*Jian Zhou,Sihao Lin,Shuai Fu,Qi WU*

Main category: cs.RO

TL;DR: The paper proposes a decoupled training approach for behavior cloning in robotic manipulation, addressing data scarcity and model inefficiencies while introducing a compact model, DP-MLP, for faster training.


<details>
  <summary>Details</summary>
Motivation: To address limitations in data scarcity, generalization, and inefficient training in behavior cloning methods, particularly Diffusion Policy.

Method: The method involves decoupling training by pretraining an action generator on observation-free data and adapting it to tasks via feature modulation. This design confines task-specific knowledge to the conditioning components and introduces DP-MLP for better efficiency.

Result: Experiments show the approach's feasibility in both in-distribution and out-of-distribution scenarios. It improves training efficiency with DP-C achieving up to 41% speedup, and the DP-MLP model reduces parameters significantly while gaining up to 89.1% faster training.

Conclusion: Decoupling improves efficiency in robotic manipulation, limits the role of large action backbone networks, and supports the use of lighter models like DP-MLP without compromising performance.

Abstract: Behavior Cloning (BC) is a data-driven supervised learning approach that has gained increasing attention with the success of scaling laws in language and vision domains. Among its implementations in robotic manipulation, Diffusion Policy (DP), with its two variants DP-CNN (DP-C) and DP-Transformer (DP-T), is one of the most effective and widely adopted models, demonstrating the advantages of predicting continuous action sequences. However, both DP and other BC methods remain constrained by the scarcity of paired training data, and the internal mechanisms underlying DP's effectiveness remain insufficiently understood, leading to limited generalization and a lack of principled design in model development. In this work, we propose a decoupled training recipe that leverages nearly cost-free kinematics-generated trajectories as observation-free data to pretrain a general action head (action generator). The pretrained action head is then frozen and adapted to novel tasks through feature modulation. Our experiments demonstrate the feasibility of this approach in both in-distribution and out-of-distribution scenarios. As an additional benefit, decoupling improves training efficiency; for instance, DP-C achieves up to a 41% speedup. Furthermore, the confinement of task-specific knowledge to the conditioning components under decoupling, combined with the near-identical performance of DP-C in both normal and decoupled training, indicates that the action generation backbone plays a limited role in robotic manipulation. Motivated by this observation, we introduce DP-MLP, which replaces the 244M-parameter U-Net backbone of DP-C with only 4M parameters of simple MLP blocks, achieving a 83.9% faster training speed under normal training and 89.1% under decoupling.

</details>


### [827] [Towards Obstacle-Avoiding Control of Planar Snake Robots Exploring Neuro-Evolution of Augmenting Topologies](https://arxiv.org/abs/2511.12148)
*Advik Sinha,Akshay Arjun,Abhijit Das,Joyjit Mukherjee*

Main category: cs.RO

TL;DR: The paper introduces a resource-efficient method for obstacle-avoiding control of a planar snake robot using NEAT, achieving superior results with reduced computational load.


<details>
  <summary>Details</summary>
Motivation: To develop a computationally efficient method for controlling a snake robot in densely cluttered environments while avoiding obstacles.

Method: The NEAT algorithm is used to dynamically generate gait parameters for snake robot locomotion. The algorithm employs input data from sensors and LiDAR, optimizing a reward function for controlling the snake robot's motion.

Result: The method is computationally efficient, performs better than existing methods, and achieves similar outcomes compared to recent CBRL approaches with significantly lower computational requirements.

Conclusion: The proposed NEAT-based framework demonstrates effective and efficient control of a planar snake robot in complex environments, verified through PyBullet simulations.

Abstract: This work aims to develop a resource-efficient solution for obstacle-avoiding tracking control of a planar snake robot in a densely cluttered environment with obstacles. Particularly, Neuro-Evolution of Augmenting Topologies (NEAT) has been employed to generate dynamic gait parameters for the serpenoid gait function, which is implemented on the joint angles of the snake robot, thus controlling the robot on a desired dynamic path. NEAT is a single neural-network based evolutionary algorithm that is known to work extremely well when the input layer is of significantly higher dimension and the output layer is of a smaller size. For the planar snake robot, the input layer consists of the joint angles, link positions, head link position as well as obstacle positions in the vicinity. However, the output layer consists of only the frequency and offset angle of the serpenoid gait that control the speed and heading of the robot, respectively. Obstacle data from a LiDAR and the robot data from various sensors, along with the location of the end goal and time, are employed to parametrize a reward function that is maximized over iterations by selective propagation of superior neural networks. The implementation and experimental results showcase that the proposed approach is computationally efficient, especially for large environments with many obstacles. The proposed framework has been verified through a physics engine simulation study on PyBullet. The approach shows superior results to existing state-of-the-art methodologies and comparable results to the very recent CBRL approach with significantly lower computational overhead. The video of the simulation can be found here: https://sites.google.com/view/neatsnakerobot

</details>


### [828] [Game-Theoretic Safe Multi-Agent Motion Planning with Reachability Analysis for Dynamic and Uncertain Environments (Extended Version)](https://arxiv.org/abs/2511.12160)
*Wenbin Mai,Minghui Liwang,Xinlei Yi,Xiaoyu Xia,Seyyedali Hosseinalipour,Xianbin Wang*

Main category: cs.RO

TL;DR: This paper addresses motion planning challenges for multi-agent systems in complex environments by proposing a Reachability-Enhanced Dynamic Potential Game framework for safe, scalable, and decentralized coordination.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address persistent challenges in ensuring safe, robust, and scalable motion planning for multi-agent systems in dynamic and uncertain environments.

Method: They developed a Reachability-Enhanced Dynamic Potential Game framework, with a Neighborhood-Dominated iterative Best Response scheme for decentralized execution and a Multi-Agent Forward Reachable Set to enforce safety under uncertainty.

Result: The framework was validated through simulations and real-world experiments in both 2D and 3D environments, demonstrating its effectiveness across varied scenarios.

Conclusion: The paper concludes that the proposed RE-DPG framework successfully ensures effective, scalable, and robust coordination for multi-agent systems in dynamic and uncertain environments.

Abstract: Ensuring safe, robust, and scalable motion planning for multi-agent systems in dynamic and uncertain environments is a persistent challenge, driven by complex inter-agent interactions, stochastic disturbances, and model uncertainties. To overcome these challenges, particularly the computational complexity of coupled decision-making and the need for proactive safety guarantees, we propose a Reachability-Enhanced Dynamic Potential Game (RE-DPG) framework, which integrates game-theoretic coordination into reachability analysis. This approach formulates multi-agent coordination as a dynamic potential game, where the Nash equilibrium (NE) defines optimal control strategies across agents. To enable scalability and decentralized execution, we develop a Neighborhood-Dominated iterative Best Response (ND-iBR) scheme, built upon an iterated $\varepsilon$-BR (i$\varepsilon$-BR) process that guarantees finite-step convergence to an $\varepsilon$-NE. This allows agents to compute strategies based on local interactions while ensuring theoretical convergence guarantees. Furthermore, to ensure safety under uncertainty, we integrate a Multi-Agent Forward Reachable Set (MA-FRS) mechanism into the cost function, explicitly modeling uncertainty propagation and enforcing collision avoidance constraints. Through both simulations and real-world experiments in 2D and 3D environments, we validate the effectiveness of RE-DPG across diverse operational scenarios.

</details>


### [829] [Variable Impedance Control for Floating-Base Supernumerary Robotic Leg in Walking Assistance](https://arxiv.org/abs/2511.12184)
*Jun Huo,Kehan Xu,Chengyao Li,Yu Cao,Jie Zuo,Xinxing Chen,Jian Huang*

Main category: cs.RO

TL;DR: The study focuses on improving safety and adaptability in human-robot systems using a novel control method for a supernumerary robotic leg (SRL) system.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in addressing safety challenges in human-robot systems, especially in managing internal and external disturbances for floating-base robots like the SRL.

Method: The paper develops a hybrid position/force impedance controller formulated for dynamic torque input and introduces a variable impedance control (VIC) method, which dynamically adjusts parameters to improve safety and adaptability.

Result: Simulations and experiments confirm that the system provides smooth transitions in flexible states and robust support in rigid states, demonstrating its ability to adapt to external force disturbances.

Conclusion: This work significantly enhances the safety, stability, and adaptability of human-robot systems, offering a viable solution for gait variation management and improved interaction resilience.

Abstract: In human-robot systems, ensuring safety during force control in the presence of both internal and external disturbances is crucial. As a typical loosely coupled floating-base robot system, the supernumerary robotic leg (SRL) system is particularly susceptible to strong internal disturbances. To address the challenge posed by floating base, we investigated the dynamics model of the loosely coupled SRL and designed a hybrid position/force impedance controller to fit dynamic torque input. An efficient variable impedance control (VIC) method is developed to enhance human-robot interaction, particularly in scenarios involving external force disturbances. By dynamically adjusting impedance parameters, VIC improves the dynamic switching between rigidity and flexibility, so that it can adapt to unknown environmental disturbances in different states. An efficient real-time stability guaranteed impedance parameters generating network is specifically designed for the proposed SRL, to achieve shock mitigation and high rigidity supporting. Simulations and experiments validate the system's effectiveness, demonstrating its ability to maintain smooth signal transitions in flexible states while providing strong support forces in rigid states. This approach provides a practical solution for accommodating individual gait variations in interaction, and significantly advances the safety and adaptability of human-robot systems.

</details>


### [830] [Innovative Design of Multi-functional Supernumerary Robotic Limbs with Ellipsoid Workspace Optimization](https://arxiv.org/abs/2511.12186)
*Jun Huo,Jian Huang,Jie Zuo,Bo Yang,Zhongzheng Fu,Xi Li,Samer Mohammed*

Main category: cs.RO

TL;DR: The paper introduces a multi-objective optimization (MOO) design theory for supernumerary robotic limbs (SRLs) that integrates workspace characteristics, force, and physical limitations using a novel firefly algorithm. Results show improved grasp success and reduced muscle activity during tasks, benefiting SRL functionality.


<details>
  <summary>Details</summary>
Motivation: To develop a unified theoretical framework for designing versatile and efficient supernumerary robotic limbs (SRLs) for both rehabilitation and functional enhancement.

Method: The paper proposes a multi-objective optimization (MOO) design framework that incorporates geometric vector quantification using an ellipsoid for workspace representation and evaluates factors like braced force, mass, and inertia. It also introduces a firefly algorithm to efficiently solve the optimization problem.

Result: The optimized SRL design improved grasp success by 7.2% and reduced muscle activity during walking and sit-to-stand tasks by 12.7% and 25.1%, respectively, in experiments involving healthy participants and hemiplegia patients.

Conclusion: The proposed multi-objective optimization design theory and firefly algorithm provide an effective approach for designing multi-functional SRLs, balancing functional and ergonomic demands.

Abstract: Supernumerary robotic limbs (SRLs) offer substantial potential in both the rehabilitation of hemiplegic patients and the enhancement of functional capabilities for healthy individuals. Designing a general-purpose SRL device is inherently challenging, particularly when developing a unified theoretical framework that meets the diverse functional requirements of both upper and lower limbs. In this paper, we propose a multi-objective optimization (MOO) design theory that integrates grasping workspace similarity, walking workspace similarity, braced force for sit-to-stand (STS) movements, and overall mass and inertia. A geometric vector quantification method is developed using an ellipsoid to represent the workspace, aiming to reduce computational complexity and address quantification challenges. The ellipsoid envelope transforms workspace points into ellipsoid attributes, providing a parametric description of the workspace. Furthermore, the STS static braced force assesses the effectiveness of force transmission. The overall mass and inertia restricts excessive link length. To facilitate rapid and stable convergence of the model to high-dimensional irregular Pareto fronts, we introduce a multi-subpopulation correction firefly algorithm. This algorithm incorporates a strategy involving attractive and repulsive domains to effectively handle the MOO task. The optimized solution is utilized to redesign the prototype for experimentation to meet specified requirements. Six healthy participants and two hemiplegia patients participated in real experiments. Compared to the pre-optimization results, the average grasp success rate improved by 7.2%, while the muscle activity during walking and STS tasks decreased by an average of 12.7% and 25.1%, respectively. The proposed design theory offers an efficient option for the design of multi-functional SRL mechanisms.

</details>


### [831] [Locally Optimal Solutions to Constraint Displacement Problems via Path-Obstacle Overlaps](https://arxiv.org/abs/2511.12203)
*Antony Thomas,Fulvio Mastrogiovanni,Marco Baglietto*

Main category: cs.RO

TL;DR: The paper proposes a two-stage approach for solving constraint displacement problems in robotic path planning.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of enabling feasible paths for robots by displacing constraints or obstacles.

Method: The approach consists of two stages: computing a trajectory through obstacles and displacing them to make the trajectory feasible.

Result: The proposed method successfully demonstrates its effectiveness on two classes of constraint displacement problems.

Conclusion: The unified approach provides locally optimal obstacle displacements, ensuring collision-free paths for robotic navigation.

Abstract: We present a unified approach for constraint displacement problems in which a robot finds a feasible path by displacing constraints or obstacles. To this end, we propose a two stage process that returns locally optimal obstacle displacements to enable a feasible path for the robot. The first stage proceeds by computing a trajectory through the obstacles while minimizing an appropriate objective function. In the second stage, these obstacles are displaced to make the computed robot trajectory feasible, that is, collision-free. Several examples are provided that successfully demonstrate our approach on two distinct classes of constraint displacement problems.

</details>


### [832] [SocialNav-Map: Dynamic Mapping with Human Trajectory Prediction for Zero-Shot Social Navigation](https://arxiv.org/abs/2511.12232)
*Lingfeng Zhang,Erjia Xiao,Xiaoshuai Hao,Haoxiang Fu,Zeying Gong,Long Chen,Xiaojun Liang,Renjing Xu,Hangjun Ye,Wenbo Ding*

Main category: cs.RO

TL;DR: The paper introduces the SocialNav-Map, a zero-shot social navigation framework that enables autonomous robots to navigate dense crowds without specific environment training, outperforming current RL-based strategies.


<details>
  <summary>Details</summary>
Motivation: Current RL-based approaches for robot social navigation demand excessive training hours and fail to generalize well to unfamiliar environments, hindering practical real-world use.

Method: SocialNav-Map integrates dynamic human trajectory prediction with occupancy mapping. It predicts human movements using history and orientation-based methods, processes them into a dynamic occupancy map, and utilizes this for safe robot navigation.

Result: SocialNav-Map shows superior performance in Social-HM3D and Social-MP3D datasets, reducing human collision rates by over 10%, while eliminating the necessity of training in new environments. It also outperforms state-of-the-art RL-based methods.

Conclusion: SocialNav-Map represents a significant advance in autonomous social navigation, enabling safe and efficient robot interactions without extensive training or fine-tuning, and is readily applicable to real-world diverse environments.

Abstract: Social navigation in densely populated dynamic environments poses a significant challenge for autonomous mobile robots, requiring advanced strategies for safe interaction. Existing reinforcement learning (RL)-based methods require over 2000+ hours of extensive training and often struggle to generalize to unfamiliar environments without additional fine-tuning, limiting their practical application in real-world scenarios. To address these limitations, we propose SocialNav-Map, a novel zero-shot social navigation framework that combines dynamic human trajectory prediction with occupancy mapping, enabling safe and efficient navigation without the need for environment-specific training. Specifically, SocialNav-Map first transforms the task goal position into the constructed map coordinate system. Subsequently, it creates a dynamic occupancy map that incorporates predicted human movements as dynamic obstacles. The framework employs two complementary methods for human trajectory prediction: history prediction and orientation prediction. By integrating these predicted trajectories into the occupancy map, the robot can proactively avoid potential collisions with humans while efficiently navigating to its destination. Extensive experiments on the Social-HM3D and Social-MP3D datasets demonstrate that SocialNav-Map significantly outperforms state-of-the-art (SOTA) RL-based methods, which require 2,396 GPU hours of training. Notably, it reduces human collision rates by over 10% without necessitating any training in novel environments. By eliminating the need for environment-specific training, SocialNav-Map achieves superior navigation performance, paving the way for the deployment of social navigation systems in real-world environments characterized by diverse human behaviors. The code is available at: https://github.com/linglingxiansen/SocialNav-Map.

</details>


### [833] [Intermittent Rendezvous Plans with Mixed Integer Linear Program for Large-Scale Multi-Robot Exploration](https://arxiv.org/abs/2511.12237)
*Alysson Ribeiro da Silva,Luiz Chaimowicz*

Main category: cs.RO

TL;DR: The paper addresses multi-robot exploration under communication constraints by proposing a model for rendezvous planning to handle unknown environments, producing promising simulation results.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance multi-robot exploration systems in constrained environments (e.g., underwater) where communication is limited, and pre-planned approaches can't adapt to uncertain conditions.

Method: The authors propose a Mixed-Integer Linear Program (MILP) for generating rendezvous plans, and they develop the Rendezvous Tracking for Unknown Scenarios (RTUS) policy for robots to execute these plans under uncertain conditions.

Result: The method was tested in large-scale simulated environments using Gazebo. Results show that the robots can follow the generated plans effectively and accomplish tasks with high efficiency.

Conclusion: The proposed approach demonstrates adaptability and efficiency in multi-robot systems operating under communication constraints, with an open-source implementation made available for further use.

Abstract: Multi-Robot Exploration (MRE) systems with communication constraints have proven efficient in accomplishing a variety of tasks, including search-and-rescue, stealth, and military operations. While some works focus on opportunistic approaches for efficiency, others concentrate on pre-planned trajectories or scheduling for increased interpretability. However, scheduling usually requires knowledge of the environment beforehand, which prevents its deployment in several domains due to related uncertainties (e.g., underwater exploration). In our previous work, we proposed an intermittent communications framework for MRE under communication constraints that uses scheduled rendezvous events to mitigate such limitations. However, the system was unable to generate optimal plans and had no mechanisms to follow the plan considering realistic trajectories, which is not suited for real-world deployments. In this work, we further investigate the problem by formulating the Multi-Robot Exploration with Communication Constraints and Intermittent Connectivity (MRE-CCIC) problem. We propose a Mixed-Integer Linear Program (MILP) formulation to generate rendezvous plans and a policy to follow them based on the Rendezvous Tracking for Unknown Scenarios (RTUS) mechanism. The RTUS is a simple rule to allow robots to follow the assigned plan, considering unknown conditions. Finally, we evaluated our method in a large-scale environment configured in Gazebo simulations. The results suggest that our method can follow the plan promptly and accomplish the task efficiently. We provide an open-source implementation of both the MILP plan generator and the large-scale MRE-CCIC.

</details>


### [834] [SAC-MoE: Reinforcement Learning with Mixture-of-Experts for Control of Hybrid Dynamical Systems with Uncertainty](https://arxiv.org/abs/2511.12361)
*Leroy D'Souza,Akash Karthikeyan,Yash Vardhan Pant,Sebastian Fischmeister*

Main category: cs.RO

TL;DR: The paper addresses hybrid dynamical systems' control with unobservable parameters and events using SAC-MoE, a novel RL method with a Mixture-of-Experts model and curriculum training for robust generalization.


<details>
  <summary>Details</summary>
Motivation: Model-based control methods struggle with hybrid dynamics uncertainty, while traditional RL approaches poorly handle abrupt mode switches in systems, motivating the need for a hybrid solution.

Method: The authors propose SAC-MoE, which combines Soft Actor-Critic (SAC) with a Mixture-of-Experts model for adaptive decision-making. A curriculum-based training algorithm supports better data collection and generalization.

Result: Simulation tests in tasks like hybrid autonomous racing and legged locomotion show that SAC-MoE outperforms traditional methods significantly, achieving up to 6x better zero-shot generalization.

Conclusion: SAC-MoE, with its interpretable and adaptable design, effectively handles hybrid dynamics and achieves superior performance in challenging scenarios, showcasing its real-world application potential.

Abstract: Hybrid dynamical systems result from the interaction of continuous-variable dynamics with discrete events and encompass various systems such as legged robots, vehicles and aircrafts. Challenges arise when the system's modes are characterized by unobservable (latent) parameters and the events that cause system dynamics to switch between different modes are also unobservable. Model-based control approaches typically do not account for such uncertainty in the hybrid dynamics, while standard model-free RL methods fail to account for abrupt mode switches, leading to poor generalization.
  To overcome this, we propose SAC-MoE which models the actor of the Soft Actor-Critic (SAC) framework as a Mixture-of-Experts (MoE) with a learned router that adaptively selects among learned experts. To further improve robustness, we develop a curriculum-based training algorithm to prioritize data collection in challenging settings, allowing better generalization to unseen modes and switching locations. Simulation studies in hybrid autonomous racing and legged locomotion tasks show that SAC-MoE outperforms baselines (up to 6x) in zero-shot generalization to unseen environments. Our curriculum strategy consistently improves performance across all evaluated policies. Qualitative analysis shows that the interpretable MoE router activates different experts for distinct latent modes.

</details>


### [835] [Multilaminate piezoelectric PVDF actuators to enhance performance of soft micro robots](https://arxiv.org/abs/2511.12380)
*Nicholas Gunter,Heiko Kabutz,Kaushik Jayaram*

Main category: cs.RO

TL;DR: This paper investigates multilayer PVDF actuators for enhancing soft microrobotics, focusing on balancing force, compliance, and bandwidth.


<details>
  <summary>Details</summary>
Motivation: The study aims to improve the performance of soft microrobotic systems by using PVDF actuators that bridge the gap between high-force, brittle PZT stacks and compliant yet low-bandwidth polymer actuators.

Method: The authors developed and characterized multilayer PVDF actuators with parallel voltage distribution, analyzed the impact of layer thickness and number, validated a first principles model, and integrated the actuators into a translating microrobot.

Result: The actuators demonstrated >3 mm free deflection, >20 mN blocked force, and >=500 Hz bandwidth at voltages as low as 150 volts, and were successfully applied in a microrobot for robust locomotion.

Conclusion: Multilayer PVDF actuators show promise as an effective, intermediate solution for soft robotics, offering a blend of high performance and operational versatility.

Abstract: Multilayer piezoelectric polyvinylidene fluoride (PVDF) actuators are a promising approach to enhance performance of soft microrobotic systems. In this work, we develop and characterize multilayer PVDF actuators with parallel voltage distribution across each layer, bridging a unique design space between brittle high-force PZT stacks and compliant but lower-bandwidth soft polymer actuators. We show the effects of layer thickness and number of layers in actuator performance and their agreement with a first principles model. By varying these parameters, we demonstrate actuators capable of >3 mm of free deflection, >20 mN of blocked force, and >=500 Hz, while operating at voltages as low as 150 volts. To illustrate their potential for robotic integration, we integrate our actuators into a planar, translating microrobot that leverages resonance to achieve locomotion with robustness to large perturbations.

</details>


### [836] [Evaluating Model-Agnostic Meta-Learning on MetaWorld ML10 Benchmark: Fast Adaptation in Robotic Manipulation Tasks](https://arxiv.org/abs/2511.12383)
*Sanjar Atamuradov*

Main category: cs.RO

TL;DR: This paper evaluates MAML-TRPO on robotic manipulation tasks, showcasing its potential for rapid task adaptation but also highlighting limitations in task generalization.


<details>
  <summary>Details</summary>
Motivation: The paper aims to enhance robotic systems' ability to quickly adapt to diverse tasks using minimal data, addressing real-world challenges in robotics.

Method: The authors use MAML-TRPO on the MetaWorld ML10 benchmark and analyze its efficiency in adapting to ten diverse robotic manipulation tasks with few-shot learning.

Result: MAML achieved one-shot adaptation, with final success rates of 21.0% on training tasks and 13.2% on test tasks, though showing gaps in generalization across tasks.

Conclusion: Gradient-based meta-learning shows promise for robotic manipulation but requires further advancements to handle task-aware adaptations and improve performance generalization.

Abstract: Meta-learning algorithms enable rapid adaptation to new tasks with minimal data, a critical capability for real-world robotic systems. This paper evaluates Model-Agnostic Meta-Learning (MAML) combined with Trust Region Policy Optimization (TRPO) on the MetaWorld ML10 benchmark, a challenging suite of ten diverse robotic manipulation tasks. We implement and analyze MAML-TRPO's ability to learn a universal initialization that facilitates few-shot adaptation across semantically different manipulation behaviors including pushing, picking, and drawer manipulation. Our experiments demonstrate that MAML achieves effective one-shot adaptation with clear performance improvements after a single gradient update, reaching final success rates of 21.0% on training tasks and 13.2% on held-out test tasks. However, we observe a generalization gap that emerges during meta-training, where performance on test tasks plateaus while training task performance continues to improve. Task-level analysis reveals high variance in adaptation effectiveness, with success rates ranging from 0% to 80% across different manipulation skills. These findings highlight both the promise and current limitations of gradient-based meta-learning for diverse robotic manipulation, and suggest directions for future work in task-aware adaptation and structured policy architectures.

</details>


### [837] [Learning Adaptive Neural Teleoperation for Humanoid Robots: From Inverse Kinematics to End-to-End Control](https://arxiv.org/abs/2511.12390)
*Sanjar Atamuradov*

Main category: cs.RO

TL;DR: The paper proposes a learning-based neural teleoperation framework for humanoid robots that replaces traditional methods with reinforcement learning. Achieved improved performance in smoothness, accuracy, and force handling for manipulation tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional humanoid robot teleoperation approaches based on inverse kinematics and PD controllers struggle with dynamic conditions and fail to adapt to users or handle external forces naturally.

Method: Developed a reinforcement learning framework that maps VR inputs directly to robot commands, trained policies using simulation data and fine-tuned with rewards for smooth trajectories and force handling.

Result: Experiments show the learned policies produce a 34% reduction in tracking error, 45% smoother motions, and better adaptability to force disturbances on humanoid manipulation tasks.

Conclusion: Learning-based teleoperation improves naturalness, robustness, and accuracy in controlling humanoid robots in diverse tasks, outperforming conventional methods.

Abstract: Virtual reality (VR) teleoperation has emerged as a promising approach for controlling humanoid robots in complex manipulation tasks. However, traditional teleoperation systems rely on inverse kinematics (IK) solvers and hand-tuned PD controllers, which struggle to handle external forces, adapt to different users, and produce natural motions under dynamic conditions. In this work, we propose a learning-based neural teleoperation framework that replaces the conventional IK+PD pipeline with learned policies trained via reinforcement learning. Our approach learns to directly map VR controller inputs to robot joint commands while implicitly handling force disturbances, producing smooth trajectories, and adapting to user preferences. We train our policies in simulation using demonstrations collected from IK-based teleoperation as initialization, then fine-tune them with force randomization and trajectory smoothness rewards. Experiments on the Unitree G1 humanoid robot demonstrate that our learned policies achieve 34% lower tracking error, 45% smoother motions, and superior force adaptation compared to the IK baseline, while maintaining real-time performance (50Hz control frequency). We validate our approach on manipulation tasks including object pick-and-place, door opening, and bimanual coordination. These results suggest that learning-based approaches can significantly improve the naturalness and robustness of humanoid teleoperation systems.

</details>


### [838] [RoboAfford++: A Generative AI-Enhanced Dataset for Multimodal Affordance Learning in Robotic Manipulation and Navigation](https://arxiv.org/abs/2511.12436)
*Xiaoshuai Hao,Yingbo Tang,Lingfeng Zhang,Yanbiao Ma,Yunfeng Diao,Ziyu Jia,Wenbo Ding,Hangjun Ye,Long Chen*

Main category: cs.RO

TL;DR: The paper introduces RoboAfford++, a dataset and benchmark for multimodal affordance learning to improve robotic manipulation and navigation.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of Vision-Language Models (VLMs) in fine-grained affordance reasoning for robotic interaction due to insufficient training annotations.

Method: Developed RoboAfford++, a generative AI-based dataset with nearly 870,000 images and 2 million QA annotations across object and spatial affordance tasks, and RoboAfford-Eval, a benchmark for evaluation.

Result: Fine-tuning VLMs on RoboAfford++ significantly improved their ability to predict actionable object and spatial affordances.

Conclusion: RoboAfford++ is effective in enhancing robotic affordance learning, bridging gaps in existing VLMs for manipulation and navigation.

Abstract: Robotic manipulation and navigation are fundamental capabilities of embodied intelligence, enabling effective robot interactions with the physical world. Achieving these capabilities requires a cohesive understanding of the environment, including object recognition to localize target objects, object affordances to identify potential interaction areas and spatial affordances to discern optimal areas for both object placement and robot movement. While Vision-Language Models (VLMs) excel at high-level task planning and scene understanding, they often struggle to infer actionable positions for physical interaction, such as functional grasping points and permissible placement regions. This limitation stems from the lack of fine-grained annotations for object and spatial affordances in their training datasets. To tackle this challenge, we introduce RoboAfford++, a generative AI-enhanced dataset for multimodal affordance learning for both robotic manipulation and navigation. Our dataset comprises 869,987 images paired with 2.0 million question answering (QA) annotations, covering three critical tasks: object affordance recognition to identify target objects based on attributes and spatial relationships, object affordance prediction to pinpoint functional parts for manipulation, and spatial affordance localization to identify free space for object placement and robot navigation. Complementing this dataset, we propose RoboAfford-Eval, a comprehensive benchmark for assessing affordance-aware prediction in real-world scenarios, featuring 338 meticulously annotated samples across the same three tasks. Extensive experimental results reveal the deficiencies of existing VLMs in affordance learning, while fine-tuning on the RoboAfford++ dataset significantly enhances their ability to reason about object and spatial affordances, validating the dataset's effectiveness.

</details>


### [839] [ClutterNav: Gradient-Guided Search for Efficient 3D Clutter Removal with Learned Costmaps](https://arxiv.org/abs/2511.12479)
*Navin Sriram Ravie,Keerthi Vasan M,Bijo Sebastian*

Main category: cs.RO

TL;DR: The paper presents ClutterNav, a framework for selecting the best object to remove in cluttered settings to access a target item while minimizing disruptions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address difficulties in retrieving objects from dense clutter while avoiding traditional rigid heuristics and improving generalizability.

Method: ClutterNav uses a reinforcement learning framework, with a removability critic trained from demonstrations and integrated gradients, to dynamically prioritize object removal decisions.

Result: ClutterNav showed strong performance in both simulations and real-world experiments, achieving efficient, real-time decision-making in partially observable environments.

Conclusion: ClutterNav enhances clutter removal tasks with human-like strategy, avoiding predefined rules and demonstrating practical utility for target retrieval.

Abstract: Dense clutter removal for target object retrieval presents a challenging problem, especially when targets are embedded deep within densely-packed configurations. It requires foresight to minimize overall changes to the clutter configuration while accessing target objects, avoiding stack destabilization and reducing the number of object removals required. Rule-based planners when applied to this problem, rely on rigid heuristics, leading to high computational overhead. End-to-end reinforcement learning approaches struggle with interpretability and generalizability over different conditions. To address these issues, we present ClutterNav, a novel decision-making framework that can identify the next best object to be removed so as to access a target object in a given clutter, while minimising stack disturbances. ClutterNav formulates the problem as a continuous reinforcement learning task, where each object removal dynamically updates the understanding of the scene. A removability critic, trained from demonstrations, estimates the cost of removing any given object based on geometric and spatial features. This learned cost is complemented by integrated gradients that assess how the presence or removal of surrounding objects influences the accessibility of the target. By dynamically prioritizing actions that balance immediate removability against long-term target exposure, ClutterNav achieves near human-like strategic sequencing, without predefined heuristics. The proposed approach is validated extensively in simulation and over real-world experiments. The results demonstrate real-time, occlusion-aware decision-making in partially observable environments.

</details>


### [840] [Botany Meets Robotics in Alpine Scree Monitoring](https://arxiv.org/abs/2511.12526)
*Davide De Benedittis,Giovanni Di Lorenzo,Franco Angelini,Barbara Valle,Marina Serena Borgatti,Paolo Remagnino,Marco Caccianiga,Manolo Garabini*

Main category: cs.RO

TL;DR: This paper introduces robotic assistance for monitoring scree habitats using the ANYmal C robot and deep learning techniques to improve data collection and species identification while addressing biodiversity loss.


<details>
  <summary>Details</summary>
Motivation: To combat biodiversity loss and environmental degradation by enhancing habitat monitoring, especially in scree habitats facing climate change threats.

Method: Deploying the ANYmal C legged robot in the Italian Alpine bio-region over two campaigns, combined with deep learning for plant species detection and classification.

Result: Robots efficiently navigated challenging terrains, boosting habitat monitoring efficiency and frequency while enhancing traditional data collection methods.

Conclusion: The study showcases the value of robotics in environmental science for efficient, sustainable habitat monitoring and preservation efforts.

Abstract: According to the European Union's Habitat Directive, habitat monitoring plays a critical role in response to the escalating problems posed by biodiversity loss and environmental degradation. Scree habitats, hosting unique and often endangered species, face severe threats from climate change due to their high-altitude nature. Traditionally, their monitoring has required highly skilled scientists to conduct extensive fieldwork in remote, potentially hazardous locations, making the process resource-intensive and time-consuming. This paper presents a novel approach for scree habitat monitoring using a legged robot to assist botanists in data collection and species identification. Specifically, we deployed the ANYmal C robot in the Italian Alpine bio-region in two field campaigns spanning two years and leveraged deep learning to detect and classify key plant species of interest. Our results demonstrate that agile legged robots can navigate challenging terrains and increase the frequency and efficiency of scree monitoring. When paired with traditional phytosociological surveys performed by botanists, this robotics-assisted protocol not only streamlines field operations but also enhances data acquisition, storage, and usage. The outcomes of this research contribute to the evolving landscape of robotics in environmental science, paving the way for a more comprehensive and sustainable approach to habitat monitoring and preservation.

</details>


### [841] [EcoFlight: Finding Low-Energy Paths Through Obstacles for Autonomous Sensing Drones](https://arxiv.org/abs/2511.12618)
*Jordan Leyva,Nahim J. Moran Vera,Yihan Xu,Adrien Durasno,Christopher U. Romero,Tendai Chimuka,Gabriel O. Huezo Ramirez,Ziqian Dong,Roberto Rojas-Cessa*

Main category: cs.RO

TL;DR: EcoFlight is a novel energy-efficient path-planning algorithm for drones focusing on 3D spaces with obstacles.


<details>
  <summary>Details</summary>
Motivation: Current UAV flight path planning often overlooks realistic obstacle scenarios and associated energy consumption.

Method: The researchers designed EcoFlight, which calculates energy-efficient routes in 3D, considering drone propulsion systems and flight dynamics.

Result: Simulations revealed EcoFlight achieves lower energy consumption compared to direct-flight and shortest-distance methods, especially in dense obstacle environments.

Conclusion: EcoFlight is effective in minimizing energy use during point-to-point drone operations by optimizing for both obstacle avoidance and flight speed.

Abstract: Obstacle avoidance path planning for uncrewed aerial vehicles (UAVs), or drones, is rarely addressed in most flight path planning schemes, despite obstacles being a realistic condition. Obstacle avoidance can also be energy-intensive, making it a critical factor in efficient point-to-point drone flights. To address these gaps, we propose EcoFlight, an energy-efficient pathfinding algorithm that determines the lowest-energy route in 3D space with obstacles. The algorithm models energy consumption based on the drone propulsion system and flight dynamics. We conduct extensive evaluations, comparing EcoFlight with direct-flight and shortest-distance schemes. The simulation results across various obstacle densities show that EcoFlight consistently finds paths with lower energy consumption than comparable algorithms, particularly in high-density environments. We also demonstrate that a suitable flying speed can further enhance energy savings.

</details>


### [842] [Task-Aware Morphology Optimization of Planar Manipulators via Reinforcement Learning](https://arxiv.org/abs/2511.12650)
*Arvind Kumar Mishra,Sohom Chakrabarty*

Main category: cs.RO

TL;DR: This research examines using reinforcement learning (RL) to optimize the morphology of planar robotic manipulators with Yoshikawa's manipulability index.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenge of optimizing robotic manipulator morphology efficiently, particularly for cases with no closed-form solutions where traditional methods like grid search are expensive.

Method: The authors validate RL's capability by using a known analytical solution (2R manipulator with equal link lengths and specific joint angles), comparing RL methods (SAC, DDPG, PPO) with grid search and black-box optimizers. The RL formulation is then applied to more complex path shapes (elliptical and rectangular) by expanding the action space.

Result: RL successfully recovered known optimal solutions and reliably solved non-analytical morphology optimization problems with fewer evaluations compared to grid or black-box methods.

Conclusion: Reinforcement learning is effective for both rediscovering analytical solutions and solving more complex morphology design problems, presenting a scalable alternative to traditional methods in robotic manipulator optimization.

Abstract: In this work, Yoshikawa's manipulability index is used to investigate reinforcement learning (RL) as a framework for morphology optimization in planar robotic manipulators. A 2R manipulator tracking a circular end-effector path is first examined because this case has a known analytical optimum: equal link lengths and the second joint orthogonal to the first. This serves as a validation step to test whether RL can rediscover the optimum using reward feedback alone, without access to the manipulability expression or the Jacobian. Three RL algorithms (SAC, DDPG, and PPO) are compared with grid search and black-box optimizers, with morphology represented by a single action parameter phi that maps to the link lengths. All methods converge to the analytical solution, showing that numerical recovery of the optimum is possible without supplying analytical structure.
  Most morphology design tasks have no closed-form solutions, and grid or heuristic search becomes expensive as dimensionality increases. RL is therefore explored as a scalable alternative. The formulation used for the circular path is extended to elliptical and rectangular paths by expanding the action space to the full morphology vector (L1, L2, theta2). In these non-analytical settings, RL continues to converge reliably, whereas grid and black-box methods require far larger evaluation budgets. These results indicate that RL is effective for both recovering known optima and solving morphology optimization problems without analytical solutions.

</details>


### [843] [Prompt-Driven Domain Adaptation for End-to-End Autonomous Driving via In-Context RL](https://arxiv.org/abs/2511.12755)
*Aleesha Khurram,Amir Moeini,Shangtong Zhang,Rohan Chandra*

Main category: cs.RO

TL;DR: This paper introduces a novel method for domain adaptation in autonomous driving, utilizing in-context reinforcement learning (ICRL) for safer and more efficient performance in adverse weather conditions.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address limitations in transferring driving policies trained in clear weather to adverse weather conditions without relying on impractical domain adaptation strategies, like extensive data collection or model retraining.

Method: They propose a Few-shot prompt-driven DA approach using ICRL, which leverages general trajectories observed during inference without model updates or additional data collection.

Result: Experimental results show that ICRL leads to better driving policies—enhancing safety, efficiency, and comfort under adverse weather scenarios in CARLA simulation.

Conclusion: ICRL expands the capabilities of prompt-driven domain adaptation methods to closed-loop autonomous driving, providing an effective solution for adapting to adverse weather conditions.

Abstract: Despite significant progress and advances in autonomous driving, many end-to-end systems still struggle with domain adaptation (DA), such as transferring a policy trained under clear weather to adverse weather conditions. Typical DA strategies in the literature include collecting additional data in the target domain or re-training the model, or both. Both these strategies quickly become impractical as we increase scale and complexity of driving. These limitations have encouraged investigation into few-shot and zero-shot prompt-driven DA at inference time involving LLMs and VLMs. These methods work by adding a few state-action trajectories during inference to the prompt (similar to in-context learning). However, there are two limitations of such an approach: $(i)$ prompt-driven DA methods are currently restricted to perception tasks such as detection and segmentation and $(ii)$ they require expert few-shot data. In this work, we present a new approach to inference-time few-shot prompt-driven DA for closed-loop autonomous driving in adverse weather condition using in-context reinforcement learning (ICRL). Similar to other prompt-driven DA methods, our approach does not require any updates to the model parameters nor does it require additional data collection in adversarial weather regime. Furthermore, our approach advances the state-of-the-art in prompt-driven DA by extending to closed driving using general trajectories observed during inference. Our experiments using the CARLA simulator show that ICRL results in safer, more efficient, and more comfortable driving policies in the target domain compared to state-of-the-art prompt-driven DA baselines.

</details>


### [844] [DR. Nav: Semantic-Geometric Representations for Proactive Dead-End Recovery and Navigation](https://arxiv.org/abs/2511.12778)
*Vignesh Rajagopal,Kasun Weerakoon Kulathun Mudiyanselage,Gershom Devake Seneviratne,Pon Aswin Sankaralingam,Mohamed Elnoor,Jing Liang,Rohan Chandra,Dinesh Manocha*

Main category: cs.RO

TL;DR: This paper introduces DR. Nav, a navigation system for robots in unstructured environments, which utilizes a semantic cost map with dead-end detection and recovery to improve safety and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of autonomous navigation in unmapped and unstructured environments, specifically focusing on handling dead-end detection and recovery in complex scenarios like corners and vegetation.

Method: DR. Nav employs cross-modal RGB-LiDAR fusion with attention filtering to estimate dead-end risks and recovery points. A semantic cost map, updated via Bayesian inference, integrates this data to improve navigation planning.

Result: DR. Nav significantly outperforms prior methods, achieving an 83.33% accuracy improvement in dead-end detection and a 52.4% reduction in navigation time, demonstrated in various indoor and outdoor scenarios.

Conclusion: Integrating dead-end risk awareness into navigation systems substantially enhances both efficiency and safety, making DR. Nav an effective solution for robots navigating complex, unstructured environments.

Abstract: We present DR. Nav (Dead-End Recovery-aware Navigation), a novel approach to autonomous navigation in scenarios where dead-end detection and recovery are critical, particularly in unstructured environments where robots must handle corners, vegetation occlusions, and blocked junctions. DR. Nav introduces a proactive strategy for navigation in unmapped environments without prior assumptions. Our method unifies dead-end prediction and recovery by generating a single, continuous, real-time semantic cost map. Specifically, DR. Nav leverages cross-modal RGB-LiDAR fusion with attention-based filtering to estimate per-cell dead-end likelihoods and recovery points, which are continuously updated through Bayesian inference to enhance robustness. Unlike prior mapping methods that only encode traversability, DR. Nav explicitly incorporates recovery-aware risk into the navigation cost map, enabling robots to anticipate unsafe regions and plan safer alternative trajectories. We evaluate DR. Nav across multiple dense indoor and outdoor scenarios and demonstrate an increase of 83.33% in accuracy in detection, a 52.4% reduction in time-to-goal (path efficiency), compared to state-of-the-art planners such as DWA, MPPI, and Nav2 DWB. Furthermore, the dead-end classifier functions

</details>


### [845] [ActiveGrasp: Information-Guided Active Grasping with Calibrated Energy-based Model](https://arxiv.org/abs/2511.12795)
*Boshu Lei,Wen Jiang,Kostas Daniilidis*

Main category: cs.RO

TL;DR: The paper tackles robotic grasping in cluttered environments by proposing a calibrated energy-based model for grasp pose generation, emphasizing effective view selection and multi-modality of grasp distribution.


<details>
  <summary>Details</summary>
Motivation: Grasping objects in cluttered environments is challenging due to the need for accurate grasp pose generation and effective view selection, where traditional approaches often ignore the structure of grasp poses or grasp distribution.

Method: The authors introduce a calibrated energy-based model for grasp pose generation, paired with an active view selection approach that uses information gain estimation from the grasp distribution. This model accounts for the SE(3) manifold structure and aligns the predicted grasp distribution with real-world success probability.

Result: The model demonstrated superior performance in both simulated and real-world environments, effectively grasping objects in cluttered settings using minimal view exploration compared to state-of-the-art techniques.

Conclusion: The proposed framework provides a robust approach for active robotic grasping in cluttered environments, bridging gaps in previous methods by incorporating a calibrated grasp pose model and efficient view selection mechanism. This can be used as a reproducible platform for future research, with source code to be released.

Abstract: Grasping in a densely cluttered environment is a challenging task for robots. Previous methods tried to solve this problem by actively gathering multiple views before grasp pose generation. However, they either overlooked the importance of the grasp distribution for information gain estimation or relied on the projection of the grasp distribution, which ignores the structure of grasp poses on the SE(3) manifold. To tackle these challenges, we propose a calibrated energy-based model for grasp pose generation and an active view selection method that estimates information gain from grasp distribution. Our energy-based model captures the multi-modality nature of grasp distribution on the SE(3) manifold. The energy level is calibrated to the success rate of grasps so that the predicted distribution aligns with the real distribution. The next best view is selected by estimating the information gain for grasp from the calibrated distribution conditioned on the reconstructed environment, which could efficiently drive the robot to explore affordable parts of the target object. Experiments on simulated environments and real robot setups demonstrate that our model could successfully grasp objects in a cluttered environment with limited view budgets compared to previous state-of-the-art models. Our simulated environment can serve as a reproducible platform for future research on active grasping. The source code of our paper will be made public when the paper is released to the public.

</details>


### [846] [Structured Imitation Learning of Interactive Policies through Inverse Games](https://arxiv.org/abs/2511.12848)
*Max M. Sun,Todd Murphey*

Main category: cs.RO

TL;DR: The paper introduces a framework combining imitation learning and game theory to improve interactive policy learning, showing promising preliminary results.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of training policies that can coordinate with humans in shared spaces without explicit communication due to the high behavioral complexity in multi-agent interactions.

Method: The framework involves two steps: learning individual behaviors through imitation learning and solving an inverse game problem to capture inter-agent interactions structurally.

Result: In a 5-agent synthetic social navigation task, the method outperformed non-interactive policies and achieved results comparable to ground truth interactive policies using only 50 demonstrations.

Conclusion: Structured imitation learning offers significant promise for interactive policy settings and multi-agent coordination tasks.

Abstract: Generative model-based imitation learning methods have recently achieved strong results in learning high-complexity motor skills from human demonstrations. However, imitation learning of interactive policies that coordinate with humans in shared spaces without explicit communication remains challenging, due to the significantly higher behavioral complexity in multi-agent interactions compared to non-interactive tasks. In this work, we introduce a structured imitation learning framework for interactive policies by combining generative single-agent policy learning with a flexible yet expressive game-theoretic structure. Our method explicitly separates learning into two steps: first, we learn individual behavioral patterns from multi-agent demonstrations using standard imitation learning; then, we structurally learn inter-agent dependencies by solving an inverse game problem. Preliminary results in a synthetic 5-agent social navigation task show that our method significantly improves non-interactive policies and performs comparably to the ground truth interactive policy using only 50 demonstrations. These results highlight the potential of structured imitation learning in interactive settings.

</details>


### [847] [Towards High-Consistency Embodied World Model with Multi-View Trajectory Videos](https://arxiv.org/abs/2511.12882)
*Taiyi Su,Jian Zhu,Yaxuan Li,Chong Ma,Zitai Huang,Yichen Zhu,Hanli Wang,Yi Xu*

Main category: cs.RO

TL;DR: MTV-World introduces a multi-view trajectory-video control system for precise robotic movement and interaction with the physical world.


<details>
  <summary>Details</summary>
Motivation: Existing embodied world models struggle to accurately map low-level actions to precise robotic movements, failing in realistic physical interactions.

Method: MTV-World utilizes trajectory videos transformed using camera parameters and Cartesian-space, employing multi-view frameworks to maintain spatial fidelity and predict future frames.

Result: The proposed model successfully improves robotic motion precision and object interaction accuracy, validated through extensive experiments in complex dual-arm scenarios.

Conclusion: MTV-World's multi-view approach ensures high consistency in physical interactions, offering advancements in embodied world modeling and evaluation methodologies.

Abstract: Embodied world models aim to predict and interact with the physical world through visual observations and actions. However, existing models struggle to accurately translate low-level actions (e.g., joint positions) into precise robotic movements in predicted frames, leading to inconsistencies with real-world physical interactions. To address these limitations, we propose MTV-World, an embodied world model that introduces Multi-view Trajectory-Video control for precise visuomotor prediction. Specifically, instead of directly using low-level actions for control, we employ trajectory videos obtained through camera intrinsic and extrinsic parameters and Cartesian-space transformation as control signals. However, projecting 3D raw actions onto 2D images inevitably causes a loss of spatial information, making a single view insufficient for accurate interaction modeling. To overcome this, we introduce a multi-view framework that compensates for spatial information loss and ensures high-consistency with physical world. MTV-World forecasts future frames based on multi-view trajectory videos as input and conditioning on an initial frame per view. Furthermore, to systematically evaluate both robotic motion precision and object interaction accuracy, we develop an auto-evaluation pipeline leveraging multimodal large models and referring video object segmentation models. To measure spatial consistency, we formulate it as an object location matching problem and adopt the Jaccard Index as the evaluation metric. Extensive experiments demonstrate that MTV-World achieves precise control execution and accurate physical interaction modeling in complex dual-arm scenarios.

</details>


### [848] [Air-Chamber Based Soft Six-Axis Force/Torque Sensor for Human-Robot Interaction](https://arxiv.org/abs/2511.12896)
*Jun Huo,Hongge Ru,Bo Yang,Xingjian Chen,Xi Li,Jian Huang*

Main category: cs.RO

TL;DR: The paper presents a soft six-axis force/torque sensor using air chambers and barometers, along with an effective decoupling method to ensure accurate measurements, achieving satisfactory performance with minimal error.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in accurately measuring forces in all degrees of freedom using six-axis sensors while minimizing cross-axis coupling, which decreases accuracy.

Method: The authors developed a soft six-axis force/torque sensor using silicone rubber air chambers and 16-channel barometers. A rigid-soft hierarchical decoupling method is introduced to handle the six-axis decoupling issue effectively.

Result: The sensor demonstrates a measuring range of 50 N force and 1 Nm torque. It achieves average deviations, repeatability, non-linearity, and hysteresis of 4.9%, 2.7%, 5.8%, and 6.7%, respectively.

Conclusion: The soft air-chamber sensor exhibits reliable and accurate sensing performance while maintaining flexibility, showcasing its potential for practical applications in force measurement.

Abstract: Soft multi-axis force/torque sensors provide safe and precise force interaction. Capturing the complete degree-of-freedom of force is imperative for accurate force measurement with six-axis force/torque sensors. However, cross-axis coupling can lead to calibration issues and decreased accuracy. In this instance, developing a soft and accurate six-axis sensor is a challenging task. In this paper, a soft air-chamber type six-axis force/torque sensor with 16-channel barometers is introduced, which housed in hyper-elastic air chambers made of silicone rubber. Additionally, an effective decoupling method is proposed, based on a rigid-soft hierarchical structure, which reduces the six-axis decoupling problem to two three-axis decoupling problems. Finite element model simulation and experiments demonstrate the compatibility of the proposed approach with reality. The prototype's sensing performance is quantitatively measured in terms of static load response, dynamic load response and dynamic response characteristic. It possesses a measuring range of 50 N force and 1 Nm torque, and the average deviation, repeatability, non-linearity and hysteresis are 4.9$\%$, 2.7$\%$, 5.8$\%$ and 6.7$\%$, respectively. The results indicate that the prototype exhibits satisfactory sensing performance while maintaining its softness due to the presence of soft air chambers.

</details>


### [849] [TOPP-DWR: Time-Optimal Path Parameterization of Differential-Driven Wheeled Robots Considering Piecewise-Constant Angular Velocity Constraints](https://arxiv.org/abs/2511.12910)
*Yong Li,Yujun Huang,Yi Chen,Hui Cheng*

Main category: cs.RO

TL;DR: The paper introduces a time-optimal path parameterization (TOPP) algorithm named TOPP-DWR for differential-driven wheeled robots, incorporating constraints often neglected by prior methods.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the limitations in time-optimal path parameterization (TOPP) for mobile robots, particularly the neglect of angular velocity and joint velocity constraints, which affect control performance in real-world applications.

Method: The authors propose TOPP-DWR, where the trajectory is represented using non-uniform B-splines. The optimization problem integrates multiple constraints uniformly as linear velocity constraints and reformulates it into second-order-cone programming (SOCP) using a slack variable.

Result: Quantitative experiments demonstrate that TOPP-DWR adheres to all constraints while achieving time optimality. Field experiments validate its practical effectiveness in autonomous navigation.

Conclusion: TOPP-DWR is a systematic and practical TOPP algorithm that outperforms existing methods by adhering to all relevant constraints and is validated for real-world applications.

Abstract: Differential-driven wheeled robots (DWR) represent the quintessential type of mobile robots and find extensive appli- cations across the robotic field. Most high-performance control approaches for DWR explicitly utilize the linear and angular velocities of the trajectory as control references. However, existing research on time-optimal path parameterization (TOPP) for mobile robots usually neglects the angular velocity and joint vel- ocity constraints, which can result in degraded control perfor- mance in practical applications. In this article, a systematic and practical TOPP algorithm named TOPP-DWR is proposed for DWR and other mobile robots. First, the non-uniform B-spline is adopted to represent the initial trajectory in the task space. Second, the piecewise-constant angular velocity, as well as joint velocity, linear velocity, and linear acceleration constraints, are incorporated into the TOPP problem. During the construction of the optimization problem, the aforementioned constraints are uniformly represented as linear velocity constraints. To boost the numerical computational efficiency, we introduce a slack variable to reformulate the problem into second-order-cone programming (SOCP). Subsequently, comparative experiments are conducted to validate the superiority of the proposed method. Quantitative performance indexes show that TOPP-DWR achieves TOPP while adhering to all constraints. Finally, field autonomous navigation experiments are carried out to validate the practicability of TOPP-DWR in real-world applications.

</details>


### [850] [DiffuDepGrasp: Diffusion-based Depth Noise Modeling Empowers Sim2Real Robotic Grasping](https://arxiv.org/abs/2511.12912)
*Yingting Zhou,Wenbo Cui,Weiheng Liu,Guixing Chen,Haoran Li,Dongbin Zhao*

Main category: cs.RO

TL;DR: The paper introduces DiffuDepGrasp, a zero-shot sim2real framework to transfer simulation-trained depth-based grasping policies to physical robots.


<details>
  <summary>Details</summary>
Motivation: Address the sim2real gap caused by sensor artifacts in depth maps, as well as challenges with data inefficiency and deployment complexity in robotic grasping tasks.

Method: Develop a framework with a Diffusion Depth Generator that uses two modules: the Diffusion Depth Module for simulating sensor noise efficiently, and the Noise Grafting Module for preserving geometric accuracy when applying noise.

Result: Achieved 95.7% grasping success rate on 12-object tasks with zero-shot transfer and generalization to unseen objects using raw depth inputs during deployment.

Conclusion: DiffuDepGrasp provides a data-efficient and deploy-efficient solution to sim2real transfer for robotic grasping by integrating noise realism and computational simplicity.

Abstract: Transferring the depth-based end-to-end policy trained in simulation to physical robots can yield an efficient and robust grasping policy, yet sensor artifacts in real depth maps like voids and noise establish a significant sim2real gap that critically impedes policy transfer. Training-time strategies like procedural noise injection or learned mappings suffer from data inefficiency due to unrealistic noise simulation, which is often ineffective for grasping tasks that require fine manipulation or dependency on paired datasets heavily. Furthermore, leveraging foundation models to reduce the sim2real gap via intermediate representations fails to mitigate the domain shift fully and adds computational overhead during deployment. This work confronts dual challenges of data inefficiency and deployment complexity. We propose DiffuDepGrasp, a deploy-efficient sim2real framework enabling zero-shot transfer through simulation-exclusive policy training. Its core innovation, the Diffusion Depth Generator, synthesizes geometrically pristine simulation depth with learned sensor-realistic noise via two synergistic modules. The first Diffusion Depth Module leverages temporal geometric priors to enable sample-efficient training of a conditional diffusion model that captures complex sensor noise distributions, while the second Noise Grafting Module preserves metric accuracy during perceptual artifact injection. With only raw depth inputs during deployment, DiffuDepGrasp eliminates computational overhead and achieves a 95.7% average success rate on 12-object grasping with zero-shot transfer and strong generalization to unseen objects.Project website: https://diffudepgrasp.github.io/.

</details>


### [851] [GUIDE: Gaussian Unified Instance Detection for Enhanced Obstacle Perception in Autonomous Driving](https://arxiv.org/abs/2511.12941)
*Chunyong Hu,Qi Luo,Jianyun Xu,Song Wang,Qiang Li,Sheng Yang*

Main category: cs.RO

TL;DR: The paper introduces GUIDE, a new autonomous driving system using 3D Gaussians for improved object detection, tracking, and occupancy prediction with better precision and efficiency.


<details>
  <summary>Details</summary>
Motivation: Traditional methods with 3D bounding boxes struggle to handle irregular object shapes in real-world scenarios. There’s a need for more accurate and computationally efficient solutions for object detection and tracking.

Method: GUIDE uses 3D Gaussians in a sparse representation framework with Gaussian-to-Voxel Splatting, enabling precise instance-level occupancy detection and tracking without high computational costs.

Result: Experimental validation on the nuScenes dataset shows that GUIDE achieves an occupancy mAP of 21.61, offering a 50% improvement over current methods, along with strong tracking capabilities.

Conclusion: GUIDE sets new performance benchmarks in autonomous perception by combining accuracy, tracking, and computational efficiency, making it well-suited for real-world autonomous driving environments.

Abstract: In the realm of autonomous driving, accurately detecting surrounding obstacles is crucial for effective decision-making. Traditional methods primarily rely on 3D bounding boxes to represent these obstacles, which often fail to capture the complexity of irregularly shaped, real-world objects. To overcome these limitations, we present GUIDE, a novel framework that utilizes 3D Gaussians for instance detection and occupancy prediction. Unlike conventional occupancy prediction methods, GUIDE also offers robust tracking capabilities. Our framework employs a sparse representation strategy, using Gaussian-to-Voxel Splatting to provide fine-grained, instance-level occupancy data without the computational demands associated with dense voxel grids. Experimental validation on the nuScenes dataset demonstrates GUIDE's performance, with an instance occupancy mAP of 21.61, marking a 50\% improvement over existing methods, alongside competitive tracking capabilities. GUIDE establishes a new benchmark in autonomous perception systems, effectively combining precision with computational efficiency to better address the complexities of real-world driving environments.

</details>


### [852] [SplatSearch: Instance Image Goal Navigation for Mobile Robots using 3D Gaussian Splatting and Diffusion Models](https://arxiv.org/abs/2511.12972)
*Siddarth Narasimhan,Matthew Lisondra,Haitong Wang,Goldie Nejat*

Main category: cs.RO

TL;DR: SplatSearch is a novel solution for mobile robots tackling the Instance Image Goal Navigation (IIN) problem by leveraging sparse 3D Gaussian Splatting reconstructions for effective target searching.


<details>
  <summary>Details</summary>
Motivation: The IIN problem involves robots identifying specific objects or people in unknown environments using only one reference image. Addressing challenges such as arbitrary reference image viewpoints and sparse reconstructions is crucial for improving navigation systems.

Method: The proposed method, SplatSearch, utilizes sparse-view 3D Gaussian Splatting (3DGS) for rendering viewpoints of candidate objects, multi-view diffusion models for completing rendered images, and a frontier exploration policy that blends visual and semantic contexts to prioritize the most relevant exploration frontiers.

Result: The method achieves improved Success Rate and shorter Success Path Length in both photorealistic home environments and real-world tests, outperforming state-of-the-art alternatives.

Conclusion: SplatSearch effectively addresses the IIN problem by combining innovative 3D reconstruction, image completion, and exploration policies, confirming its superiority through experimental validation and detailed ablation study.

Abstract: The Instance Image Goal Navigation (IIN) problem requires mobile robots deployed in unknown environments to search for specific objects or people of interest using only a single reference goal image of the target. This problem can be especially challenging when: 1) the reference image is captured from an arbitrary viewpoint, and 2) the robot must operate with sparse-view scene reconstructions. In this paper, we address the IIN problem, by introducing SplatSearch, a novel architecture that leverages sparse-view 3D Gaussian Splatting (3DGS) reconstructions. SplatSearch renders multiple viewpoints around candidate objects using a sparse online 3DGS map, and uses a multi-view diffusion model to complete missing regions of the rendered images, enabling robust feature matching against the goal image. A novel frontier exploration policy is introduced which uses visual context from the synthesized viewpoints with semantic context from the goal image to evaluate frontier locations, allowing the robot to prioritize frontiers that are semantically and visually relevant to the goal image. Extensive experiments in photorealistic home and real-world environments validate the higher performance of SplatSearch against current state-of-the-art methods in terms of Success Rate and Success Path Length. An ablation study confirms the design choices of SplatSearch.

</details>


### [853] [CUTE-Planner: Confidence-aware Uneven Terrain Exploration Planner](https://arxiv.org/abs/2511.12984)
*Miryeong Park,Dongjin Cho,Sanghyun Kim,Younggun Cho*

Main category: cs.RO

TL;DR: The paper proposes a robotic exploration framework addressing terrain navigation and mapping under uncertainty, achieving significant improvements in exploration safety and reliability.


<details>
  <summary>Details</summary>
Motivation: Current planetary exploration systems struggle with high uncertainty in mapping and navigation, particularly in complex terrains, impacting exploration safety and mission success.

Method: The proposed method integrates safe path generation, adaptive confidence updates, and confidence-aware exploration strategies, utilizing Kalman-based elevation estimation and incorporating these into a Graph-Based exploration Planner.

Result: In simulated lunar experiments, the method achieved a 69% reduction in uncertainty within low-confidence terrains and ensured a 100% mission success rate compared to the baseline's 0%.

Conclusion: By accounting for elevation uncertainty and strategically prioritizing exploration of critical terrains, the framework significantly enhances safety and reliability in planetary exploration.

Abstract: Planetary exploration robots must navigate uneven terrain while building reliable maps for space missions. However, most existing methods incorporate traversability constraints but may not handle high uncertainty in elevation estimates near complex features like craters, do not consider exploration strategies for uncertainty reduction, and typically fail to address how elevation uncertainty affects navigation safety and map quality. To address the problems, we propose a framework integrating safe path generation, adaptive confidence updates, and confidence-aware exploration strategies. Using Kalman-based elevation estimation, our approach generates terrain traversability and confidence scores, then incorporates them into Graph-Based exploration Planner (GBP) to prioritize exploration of traversable low-confidence regions. We evaluate our framework through simulated lunar experiments using a novel low-confidence region ratio metric, achieving 69% uncertainty reduction compared to baseline GBP. In terms of mission success rate, our method achieves 100% while baseline GBP achieves 0%, demonstrating improvements in exploration safety and map reliability.

</details>


### [854] [APP: A* Post-Processing Algorithm for Robots with Bidirectional Shortcut and Path Perturbation](https://arxiv.org/abs/2511.13042)
*Yong Li,Hui Cheng*

Main category: cs.RO

TL;DR: The paper introduces a post-processing algorithm (APP) for improving A* and graph-search-based planner paths by reducing zig-zag patterns, unnecessary heading changes, and shortening paths.


<details>
  <summary>Details</summary>
Motivation: Graph-search-based planners like A* generate paths that are often not optimal and contain artificial zig-zag patterns, especially in open spaces, which do not align with intuitive human navigation preferences.

Method: APP implements a bidirectional vertices reduction algorithm for better path asymmetry handling and a shortcut strategy for efficient path shortening. Additionally, an iterative path perturbation algorithm is used to reduce unnecessary heading changes and smooth paths.

Result: Experiments showed that APP achieves shorter, smoother paths with fewer unnecessary heading changes and faster planning time compared to existing methods.

Conclusion: APP significantly improves over traditional graph-search-based planners, making the paths more practical and intuitive, and has demonstrated utility in real-world navigation scenarios.

Abstract: Paths generated by A* and other graph-search-based planners are widely used in the robotic field. Due to the restricted node-expansion directions, the resulting paths are usually not the shortest. Besides, unnecessary heading changes, or zig-zag patterns, exist even when no obstacle is nearby, which is inconsistent with the human intuition that the path segments should be straight in wide-open space due to the absence of obstacles. This article puts forward a general and systematic post-processing algorithm for A* and other graph-search-based planners. The A* post-processing algorithm, called APP, is developed based on the costmap, which is widely used in commercial service robots. First, a bidirectional vertices reduction algorithm is proposed to tackle the asymm- etry of the path and the environments. During the forward and backward vertices reduction, a thorough shortcut strategy is put forward to improve the path-shortening performance and avoid unnecessary heading changes. Second, an iterative path perturbation algorithm is adopted to locally reduce the number of unnecessary heading changes and improve the path smooth- ness. Comparative experiments are then carried out to validate the superiority of the proposed method. Quantitative performance indexes show that APP outperforms the existing methods in planning time, path length as well as the number of unnecessary heading changes. Finally, field navigation experiments are carried out to verify the practicability of APP.

</details>


### [855] [Unidirectional-Road-Network-Based Global Path Planning for Cleaning Robots in Semi-Structured Environments](https://arxiv.org/abs/2511.13048)
*Yong Li,Hui Cheng*

Main category: cs.RO

TL;DR: This paper introduces an improved global path planning method for cleaning robots in semi-structured environments by balancing path length and traffic rule constraints.


<details>
  <summary>Details</summary>
Motivation: The need to enhance navigation performance in semi-structured environments while minimizing collision risks and overlong paths for cleaning robots.

Method: The approach includes building a unidirectional road network for traffic constraints, allowing selective road crossing, and introducing a two-layer potential map for planning in complex intersections.

Result: Experimental validations show the method balances path length and road network consistency better than existing methods.

Conclusion: This work offers a systematic path planning strategy that optimizes navigation efficiency while adhering to traffic rules in semi-structured environments.

Abstract: Practical global path planning is critical for commercializing cleaning robots working in semi-structured environments. In the literature, global path planning methods for free space usually focus on path length and neglect the traffic rule constraints of the environments, which leads to high-frequency re-planning and increases collision risks. In contrast, those for structured environments are developed mainly by strictly complying with the road network representing the traffic rule constraints, which may result in an overlong path that hinders the overall navigation efficiency. This article proposes a general and systematic approach to improve global path planning performance in semi-structured environments. A unidirectional road network is built to represent the traffic constraints in semi-structured environments and a hybrid strategy is proposed to achieve a guaranteed planning result.Cutting across the road at the starting and the goal points are allowed to achieve a shorter path. Especially, a two-layer potential map is proposed to achieve a guaranteed performance when the starting and the goal points are in complex intersections. Comparative experiments are carried out to validate the effectiveness of the proposed method. Quantitative experimental results show that, compared with the state-of-art, the proposed method guarantees a much better balance between path length and the consistency with the road network.

</details>


### [856] [Orientation-Free Neural Network-Based Bias Estimation for Low-Cost Stationary Accelerometers](https://arxiv.org/abs/2511.13071)
*Michal Levin,Itzik Klein*

Main category: cs.RO

TL;DR: The paper introduces a model-free learning-based calibration method for accelerometers, eliminating complex orientation-dependent procedures.


<details>
  <summary>Details</summary>
Motivation: To address performance degradation in low-cost accelerometers caused by bias errors and the challenges of traditional orientation-dependent calibration methods.

Method: A model-free learning-based approach calibrates accelerometer bias under stationary conditions without requiring sensor orientation knowledge or sensor rotation.

Result: Experimental validation shows the proposed method achieves over 52% error reduction compared to traditional techniques on a 13.39-hour dataset from six accelerometers.

Conclusion: The method significantly advances calibration accuracy in orientation-free scenarios, improving reliability of low-cost inertial sensors and eliminating leveled calibration requirements.

Abstract: Low-cost micro-electromechanical accelerometers are widely used in navigation, robotics, and consumer devices for motion sensing and position estimation. However, their performance is often degraded by bias errors. To eliminate deterministic bias terms a calibration procedure is applied under stationary conditions. It requires accelerom- eter leveling or complex orientation-dependent calibration procedures. To overcome those requirements, in this paper we present a model-free learning-based calibration method that estimates accelerometer bias under stationary conditions, without requiring knowledge of the sensor orientation and without the need to rotate the sensors. The proposed approach provides a fast, practical, and scalable solution suitable for rapid field deployment. Experimental validation on a 13.39-hour dataset collected from six accelerometers shows that the proposed method consistently achieves error levels more than 52% lower than traditional techniques. On a broader scale, this work contributes to the advancement of accurate calibration methods in orientation-free scenarios. As a consequence, it improves the reliability of low-cost inertial sensors in diverse scientific and industrial applications and eliminates the need for leveled calibration.

</details>


### [857] [ResAlignNet: A Data-Driven Approach for INS/DVL Alignment](https://arxiv.org/abs/2511.13096)
*Guy Damari,Itzik Klein*

Main category: cs.RO

TL;DR: The paper introduces ResAlignNet, a deep learning method for aligning sensor systems on underwater vehicles without external aids or motion pattern requirements, reducing alignment time and improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing sensor alignment methods for underwater vehicles face issues like slow convergence, reliance on specific motion patterns, and dependence on external sensors, limiting operational efficiency.

Method: The proposed ResAlignNet uses a 1D ResNet-18 deep neural network architecture, trained on synthetic data to address the sensor alignment problem in underwater vehicles. The method operates in-situ without external aiding sensors.

Result: Experiments with a Snapir underwater vehicle demonstrate that ResAlignNet achieves alignment accuracy within 0.8° in just 25 seconds, a 65% reduction in convergence time compared to traditional methods.

Conclusion: ResAlignNet offers a significant step forward in underwater navigation by enabling fast, accurate, and sensor-agnostic alignment, eliminating reliance on pre-mission procedures and external aids, while providing broad operational scalability.

Abstract: Autonomous underwater vehicles rely on precise navigation systems that combine the inertial navigation system and the Doppler velocity log for successful missions in challenging environments where satellite navigation is unavailable. The effectiveness of this integration critically depends on accurate alignment between the sensor reference frames. Standard model-based alignment methods between these sensor systems suffer from lengthy convergence times, dependence on prescribed motion patterns, and reliance on external aiding sensors, significantly limiting operational flexibility. To address these limitations, this paper presents ResAlignNet, a data-driven approach using the 1D ResNet-18 architecture that transforms the alignment problem into deep neural network optimization, operating as an in-situ solution that requires only sensors on board without external positioning aids or complex vehicle maneuvers, while achieving rapid convergence in seconds. Additionally, the approach demonstrates the learning capabilities of Sim2Real transfer, enabling training in synthetic data while deploying in operational sensor measurements. Experimental validation using the Snapir autonomous underwater vehicle demonstrates that ResAlignNet achieves alignment accuracy within 0.8° using only 25 seconds of data collection, representing a 65\% reduction in convergence time compared to standard velocity-based methods. The trajectory-independent solution eliminates motion pattern requirements and enables immediate vehicle deployment without lengthy pre-mission procedures, advancing underwater navigation capabilities through robust sensor-agnostic alignment that scales across different operational scenarios and sensor specifications.

</details>


### [858] [Count Every Rotation and Every Rotation Counts: Exploring Drone Dynamics via Propeller Sensing](https://arxiv.org/abs/2511.13100)
*Xuecheng Chen,Jingao Xu,Wenhua Ding,Haoyang Wang,Xinyu Luo,Ruiyang Duan,Jialong Chen,Xueqian Wang,Yunhao Liu,Xinlei Chen*

Main category: cs.RO

TL;DR: This paper introduces a system, \sysname, for contactless sensing of airborne drones focusing on their propeller rotational speed using event cameras.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the growing need for effective drone sensing as drone-based applications become widespread, with a focus on improving tracking and inference through propeller dynamics.

Method: The method includes two components: 'Count Every Rotation' for real-time propeller speed estimation by reducing environmental noise sensitivity, and 'Every Rotation Counts' for utilizing speed to deduce drone dynamics.

Result: In real-world tests, \sysname achieves 3ms sensing latency, 0.23% error in speed estimation, 96.5% precision in drone flight command inference, and 22% improved tracking accuracy when combined with other modalities.

Conclusion: \sysname significantly enhances drone sensing and tracking capabilities through precise propeller rotation analysis, proving effective in real-world scenarios like drone delivery.

Abstract: As drone-based applications proliferate, paramount contactless sensing of airborne drones from the ground becomes indispensable. This work demonstrates concentrating on propeller rotational speed will substantially improve drone sensing performance and proposes an event-camera-based solution, \sysname. \sysname features two components: \textit{Count Every Rotation} achieves accurate, real-time propeller speed estimation by mitigating ultra-high sensitivity of event cameras to environmental noise. \textit{Every Rotation Counts} leverages these speeds to infer both internal and external drone dynamics. Extensive evaluations in real-world drone delivery scenarios show that \sysname achieves a sensing latency of 3$ms$ and a rotational speed estimation error of merely 0.23\%. Additionally, \sysname infers drone flight commands with 96.5\% precision and improves drone tracking accuracy by over 22\% when combined with other sensing modalities. \textit{ Demo: {\color{blue}https://eventpro25.github.io/EventPro/.} }

</details>


### [859] [Monolithic Units: Actuation, Sensing, and Simulation for Integrated Soft Robot Design](https://arxiv.org/abs/2511.13120)
*Trevor Exley,Anderson Brazil Nardin,Petr Trunin,Diana Cafiso,Lucia Beccai*

Main category: cs.RO

TL;DR: This paper presents the Monolithic Unit (MU), a unified actuator-lattice-sensor building block for soft robotics, with parametric design for reproducibility and optimizations for embedded sensing.


<details>
  <summary>Details</summary>
Motivation: To enhance soft robotic design by introducing a scalable, reproducible building block combining actuation, structure, and sensing components.

Method: A parametric design framework was developed to relate actuator chamber dimensions and lattice unit cell size. Finite element simulation optimized sensor placement, minimizing the impact on mechanical properties.

Result: Optimized MU models were experimentally validated to ensure mechanical performance preservation while embedded sensing capability was maintained. Scaled units and applications like a two-finger gripper were demonstrated.

Conclusion: The Monolithic Unit offers a scalable and reproducible platform for soft robotics, with integrated co-design rules and sensor placement optimization for enhanced functionality.

Abstract: This work introduces the Monolithic Unit (MU), an actuator-lattice-sensor building block for soft robotics. The MU integrates pneumatic actuation, a compliant lattice envelope, and candidate sites for optical waveguide sensing into a single printed body. In order to study reproducibility and scalability, a parametric design framework establishes deterministic rules linking actuator chamber dimensions to lattice unit cell size. Experimental homogenization of lattice specimens provides effective material properties for finite element simulation. Within this simulation environment, sensor placement is treated as a discrete optimization problem, where a finite set of candidate waveguide paths derived from lattice nodes is evaluated by introducing local stiffening, and the configuration minimizing deviation from baseline mechanical response is selected. Optimized models are fabricated and experimentally characterized, validating the preservation of mechanical performance while enabling embedded sensing. The workflow is further extended to scaled units and a two-finger gripper, demonstrating generality of the MU concept. This approach advances monolithic soft robotic design by combining reproducible co-design rules with simulation-informed sensor integration.

</details>


### [860] [Collision-Free Navigation of Mobile Robots via Quadtree-Based Model Predictive Control](https://arxiv.org/abs/2511.13188)
*Osama Al Sheikh Ali,Sotiris Koutsoftas,Ze Zhang,Knut Akesson,Emmanuel Dean*

Main category: cs.RO

TL;DR: An integrated navigation framework for Autonomous Mobile Robots (AMRs) streamlines environment representation, trajectory generation, and Model Predictive Control using a quadtree-based approach.


<details>
  <summary>Details</summary>
Motivation: To create a unified and efficient navigation system for AMRs by addressing challenges like collision-free movement and real-time decision-making across complex environments.

Method: Uses quadtree-based structured collision-free regions from occupancy maps, safe-area extraction, connectivity graph construction, B-spline smoothing, and a Model Predictive Control formulation for navigation.

Result: Experimental results show consistent success and better performance compared to baseline approaches in navigating complex environments.

Conclusion: The framework provides an efficient, reliable, and scalable system for AMR navigation, integrating key components for enhanced performance in real-world settings.

Abstract: This paper presents an integrated navigation framework for Autonomous Mobile Robots (AMRs) that unifies environment representation, trajectory generation, and Model Predictive Control (MPC). The proposed approach incorporates a quadtree-based method to generate structured, axis-aligned collision-free regions from occupancy maps. These regions serve as both a basis for developing safe corridors and as linear constraints within the MPC formulation, enabling efficient and reliable navigation without requiring direct obstacle encoding. The complete pipeline combines safe-area extraction, connectivity graph construction, trajectory generation, and B-spline smoothing into one coherent system. Experimental results demonstrate consistent success and superior performance compared to baseline approaches across complex environments.

</details>


### [861] [PIGEON: VLM-Driven Object Navigation via Points of Interest Selection](https://arxiv.org/abs/2511.13207)
*Cheng Peng,Zhenzhe Zhang,Cheng Chi,Xiaobao Wei,Yanhao Zhang,Heng Wang,Pengwei Wang,Zhongyuan Wang,Jing Liu,Shanghang Zhang*

Main category: cs.RO

TL;DR: The paper proposes PIGEON, a method leveraging a Visual-Language Model (VLM) for efficient object navigation in unknown environments, achieving state-of-the-art performance and enabling better semantic guidance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in object navigation where current methods lack decision foresight and face difficulties ensuring continuous and intelligent actions in unknown environments.

Method: The authors introduce PIGEON, which integrates semantic input through a VLM called PIGEON-VL. This method uses Points of Interest (PoI) during exploration, a high-frequency decision mechanism, and generates RLVR data for improved navigation performance.

Result: The proposed method shows state-of-the-art performance on standard object navigation benchmarks, with PoI-based decision-making and RLVR data enabling enhanced real-time navigation capabilities.

Conclusion: PIGEON successfully achieves intelligent navigation by balancing decision frequency with semantic foresight, demonstrating its ability for deep reasoning and efficient exploration in unknown environments.

Abstract: Navigating to a specified object in an unknown environment is a fundamental yet challenging capability of embodied intelligence. However, current methods struggle to balance decision frequency with intelligence, resulting in decisions lacking foresight or discontinuous actions. In this work, we propose PIGEON: Point of Interest Guided Exploration for Object Navigation with VLM, maintaining a lightweight and semantically aligned snapshot memory during exploration as semantic input for the exploration strategy. We use a large Visual-Language Model (VLM), named PIGEON-VL, to select Points of Interest (PoI) formed during exploration and then employ a lower-level planner for action output, increasing the decision frequency. Additionally, this PoI-based decision-making enables the generation of Reinforcement Learning with Verifiable Reward (RLVR) data suitable for simulators. Experiments on classic object navigation benchmarks demonstrate that our zero-shot transfer method achieves state-of-the-art performance, while RLVR further enhances the model's semantic guidance capabilities, enabling deep reasoning during real-time navigation.

</details>


### [862] [GaRLILEO: Gravity-aligned Radar-Leg-Inertial Enhanced Odometry](https://arxiv.org/abs/2511.13216)
*Chiyun Noh,Sangwoo Jung,Hanjun Kim,Yafei Hu,Laura Herlant,Ayoung Kim*

Main category: cs.RO

TL;DR: GaRLILEO is an odometry framework for legged robots that utilizes radar, leg kinematics, and inertial sensing to overcome traditional challenges like foot slippage and roll-pitch errors, offering state-of-the-art accuracy in vertical odometry.


<details>
  <summary>Details</summary>
Motivation: Accurate odometry is essential for legged robots navigating complex terrains, as traditional methods face challenges like vertical drift, roll-pitch imprecisions, and reliance on external sensors in sparse or repetitive environments.

Method: GaRLILEO introduces a gravity-aligned continuous-time radar-leg-inertial framework that decouples velocity from IMU data using ego-velocity splines and a novel gravity factor. It eliminates dependence on LiDAR or cameras.

Result: Evaluated on real-world indoor-outdoor trajectories, GaRLILEO achieved state-of-the-art accuracy in vertical odometry estimation, especially on challenging terrains like stairs and slopes.

Conclusion: GaRLILEO enhances vertical pose accuracy for legged robots, mitigating traditional challenges. This innovation fosters further research, supported by its open-source release.

Abstract: Deployment of legged robots for navigating challenging terrains (e.g., stairs, slopes, and unstructured environments) has gained increasing preference over wheel-based platforms. In such scenarios, accurate odometry estimation is a preliminary requirement for stable locomotion, localization, and mapping. Traditional proprioceptive approaches, which rely on leg kinematics sensor modalities and inertial sensing, suffer from irrepressible vertical drift caused by frequent contact impacts, foot slippage, and vibrations, particularly affected by inaccurate roll and pitch estimation. Existing methods incorporate exteroceptive sensors such as LiDAR or cameras. Further enhancement has been introduced by leveraging gravity vector estimation to add additional observations on roll and pitch, thereby increasing the accuracy of vertical pose estimation. However, these approaches tend to degrade in feature-sparse or repetitive scenes and are prone to errors from double-integrated IMU acceleration. To address these challenges, we propose GaRLILEO, a novel gravity-aligned continuous-time radar-leg-inertial odometry framework. GaRLILEO decouples velocity from the IMU by building a continuous-time ego-velocity spline from SoC radar Doppler and leg kinematics information, enabling seamless sensor fusion which mitigates odometry distortion. In addition, GaRLILEO can reliably capture accurate gravity vectors leveraging a novel soft S2-constrained gravity factor, improving vertical pose accuracy without relying on LiDAR or cameras. Evaluated on a self-collected real-world dataset with diverse indoor-outdoor trajectories, GaRLILEO demonstrates state-of-the-art accuracy, particularly in vertical odometry estimation on stairs and slopes. We open-source both our dataset and algorithm to foster further research in legged robot odometry and SLAM. https://garlileo.github.io/GaRLILEO

</details>


### [863] [EL3DD: Extended Latent 3D Diffusion for Language Conditioned Multitask Manipulation](https://arxiv.org/abs/2511.13312)
*Jonas Bode,Raphael Memmesheimer,Sven Behnke*

Main category: cs.RO

TL;DR: The paper explores using diffusion models in robots to integrate vision and text for precise task execution, achieving better performance in multitask manipulation.


<details>
  <summary>Details</summary>
Motivation: To enable general-purpose robots to interpret natural language and perform physical tasks in human environments through improved visuomotor policy frameworks.

Method: The method utilizes diffusion models combined with visual and textual inputs, reference demonstrations during training, and improves embeddings for multitask manipulation.

Result: The approach results in enhanced performance on CALVIN dataset for manipulation tasks, with higher success rates in sequential task execution.

Conclusion: Demonstrates the utility of diffusion models in robotic task execution and contributes to advancing multitask manipulation capabilities.

Abstract: Acting in human environments is a crucial capability for general-purpose robots, necessitating a robust understanding of natural language and its application to physical tasks. This paper seeks to harness the capabilities of diffusion models within a visuomotor policy framework that merges visual and textual inputs to generate precise robotic trajectories. By employing reference demonstrations during training, the model learns to execute manipulation tasks specified through textual commands within the robot's immediate environment. The proposed research aims to extend an existing model by leveraging improved embeddings, and adapting techniques from diffusion models for image generation. We evaluate our methods on the CALVIN dataset, proving enhanced performance on various manipulation tasks and an increased long-horizon success rate when multiple tasks are executed in sequence. Our approach reinforces the usefulness of diffusion models and contributes towards general multitask manipulation.

</details>


### [864] [ZeroDexGrasp: Zero-Shot Task-Oriented Dexterous Grasp Synthesis with Prompt-Based Multi-Stage Semantic Reasoning](https://arxiv.org/abs/2511.13327)
*Juntao Jian,Yi-Lin Wei,Chengjie Mou,Yuhao Lin,Xing Zhu,Yujun Shen,Wei-Shi Zheng,Ruizhen Hu*

Main category: cs.RO

TL;DR: ZeroDexGrasp integrates Multimodal Large Language Models and grasp refinement for zero-shot, human-like, task-oriented dexterous grasping that generalizes across objects and tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitation of existing methods that rely on costly, labeled data and lack generalization for diverse objects and tasks.

Method: Integration of prompt-based multi-stage semantic reasoning with task and object semantics, paired with contact-guided optimization to refine grasp poses.

Result: ZeroDexGrasp demonstrates high-quality zero-shot dexterous grasping across diverse unseen objects and complex tasks.

Conclusion: The framework advances robotic grasping towards generalizable and intelligent solutions, enabling human-like task-oriented executions.

Abstract: Task-oriented dexterous grasping holds broad application prospects in robotic manipulation and human-object interaction. However, most existing methods still struggle to generalize across diverse objects and task instructions, as they heavily rely on costly labeled data to ensure task-specific semantic alignment. In this study, we propose \textbf{ZeroDexGrasp}, a zero-shot task-oriented dexterous grasp synthesis framework integrating Multimodal Large Language Models with grasp refinement to generate human-like grasp poses that are well aligned with specific task objectives and object affordances. Specifically, ZeroDexGrasp employs prompt-based multi-stage semantic reasoning to infer initial grasp configurations and object contact information from task and object semantics, then exploits contact-guided grasp optimization to refine these poses for physical feasibility and task alignment. Experimental results demonstrate that ZeroDexGrasp enables high-quality zero-shot dexterous grasping on diverse unseen object categories and complex task requirements, advancing toward more generalizable and intelligent robotic grasping.

</details>


### [865] [Contact-Safe Reinforcement Learning with ProMP Reparameterization and Energy Awareness](https://arxiv.org/abs/2511.13459)
*Bingkun Huang,Yuhe Gong,Zewen Yang,Tianyu Ren,Luis Figueredo*

Main category: cs.RO

TL;DR: The paper addresses the limitations of traditional RL methods in robotic tasks by proposing an energy-aware framework combining PPO and movement primitives for contact-rich manipulations in 3D environments, achieving high performance and safety.


<details>
  <summary>Details</summary>
Motivation: To address limitations in current RL methods, especially MDP-based and episodic approaches, which often underperform in ensuring contact-safety and robustness in contact-rich task-space robotic manipulations.

Method: The proposed framework combines Proximal Policy Optimization (PPO) with movement primitives. It integrates an energy-aware Cartesian Impedance Controller objective to ensure energy safety and smooth task-space trajectory generation in a contact-rich environment.

Result: The presented framework outperforms existing RL methods in handling complex, contact-rich tasks on diverse surfaces in 3D environments, achieving superior success rates and ensuring energy-safe, smooth trajectories.

Conclusion: The proposed task-space, energy-safe RL framework showcases its potential for advanced robotic manipulations by prioritizing safety, trajectory consistency, and high task success performance.

Abstract: Reinforcement learning (RL) approaches based on Markov Decision Processes (MDPs) are predominantly applied in the robot joint space, often relying on limited task-specific information and partial awareness of the 3D environment. In contrast, episodic RL has demonstrated advantages over traditional MDP-based methods in terms of trajectory consistency, task awareness, and overall performance in complex robotic tasks. Moreover, traditional step-wise and episodic RL methods often neglect the contact-rich information inherent in task-space manipulation, especially considering the contact-safety and robustness. In this work, contact-rich manipulation tasks are tackled using a task-space, energy-safe framework, where reliable and safe task-space trajectories are generated through the combination of Proximal Policy Optimization (PPO) and movement primitives. Furthermore, an energy-aware Cartesian Impedance Controller objective is incorporated within the proposed framework to ensure safe interactions between the robot and the environment. Our experimental results demonstrate that the proposed framework outperforms existing methods in handling tasks on various types of surfaces in 3D environments, achieving high success rates as well as smooth trajectories and energy-safe interactions.

</details>


### [866] [Towards Affect-Adaptive Human-Robot Interaction: A Protocol for Multimodal Dataset Collection on Social Anxiety](https://arxiv.org/abs/2511.13530)
*Vesna Poprcova,Iulia Lefter,Matthias Wieser,Martijn Warnier,Frances Brazier*

Main category: cs.RO

TL;DR: The paper proposes a protocol for creating a multimodal dataset to study social anxiety during human-robot interactions, utilizing synchronized audio, video, and physiological data.


<details>
  <summary>Details</summary>
Motivation: To address the scarcity of multimodal datasets needed for examining social anxiety manifestations during human-robot interactions.

Method: The dataset includes at least 70 participants grouped by social anxiety levels, engaging in Wizard-of-Oz role-play scenarios with a social robot, collecting synchronized audio, video, and physiological data.

Result: This protocol supports research in social anxiety detection through multimodal data while incorporating contextual data to understand individual variability.

Conclusion: The proposed dataset will enable new insights into robust multimodal detection of social anxiety, aiding affect-adaptive human-robot interaction advancements.

Abstract: Social anxiety is a prevalent condition that affects interpersonal interactions and social functioning. Recent advances in artificial intelligence and social robotics offer new opportunities to examine social anxiety in the human-robot interaction context. Accurate detection of affective states and behaviours associated with social anxiety requires multimodal datasets, where each signal modality provides complementary insights into its manifestations. However, such datasets remain scarce, limiting progress in both research and applications. To address this, this paper presents a protocol for multimodal dataset collection designed to reflect social anxiety in a human-robot interaction context. The dataset will consist of synchronised audio, video, and physiological recordings acquired from at least 70 participants, grouped according to their level of social anxiety, as they engage in approximately 10-minute interactive Wizard-of-Oz role-play scenarios with the Furhat social robot under controlled experimental conditions. In addition to multimodal data, the dataset will be enriched with contextual data providing deeper insight into individual variability in social anxiety responses. This work can contribute to research on affect-adaptive human-robot interaction by providing support for robust multimodal detection of social anxiety.

</details>


### [867] [OpenRoboCare: A Multimodal Multi-Task Expert Demonstration Dataset for Robot Caregiving](https://arxiv.org/abs/2511.13707)
*Xiaoyu Liang,Ziang Liu,Kelvin Lin,Edward Gu,Ruolin Ye,Tam Nguyen,Cynthia Hsu,Zhanxin Wu,Xiaoman Yang,Christy Sum Yu Cheung,Harold Soh,Katherine Dimitropoulou,Tapomayukh Bhattacharjee*

Main category: cs.RO

TL;DR: OpenRoboCare introduces a multimodal caregiving dataset showcasing expert occupational therapist demonstrations to enhance robotic caregiving systems, addressing challenges like complex human-robot interactions and perception under occlusion.


<details>
  <summary>Details</summary>
Motivation: There is a need for a large-scale, diverse dataset capturing real-world caregiving routines to improve robotic caregiving systems.

Method: Data was collected from 21 occupational therapists performing 15 ADL tasks on manikins across five modalities: RGB-D video, pose tracking, eye-gaze tracking, task annotations, and tactile sensing.

Result: The dataset reveals rich multimodal insights into caregiving strategies and highlights challenges for current state-of-the-art robot perception and human activity recognition methods.

Conclusion: OpenRoboCare provides a valuable resource for advancing safe and adaptive assistive robots by focusing on expert caregiving principles and strategies.

Abstract: We present OpenRoboCare, a multimodal dataset for robot caregiving, capturing expert occupational therapist demonstrations of Activities of Daily Living (ADLs). Caregiving tasks involve complex physical human-robot interactions, requiring precise perception under occlusions, safe physical contact, and long-horizon planning. While recent advances in robot learning from demonstrations have shown promise, there is a lack of a large-scale, diverse, and expert-driven dataset that captures real-world caregiving routines. To address this gap, we collect data from 21 occupational therapists performing 15 ADL tasks on two manikins. The dataset spans five modalities: RGB-D video, pose tracking, eye-gaze tracking, task and action annotations, and tactile sensing, providing rich multimodal insights into caregiver movement, attention, force application, and task execution strategies. We further analyze expert caregiving principles and strategies, offering insights to improve robot efficiency and task feasibility. Additionally, our evaluations demonstrate that OpenRoboCare presents challenges for state-of-the-art robot perception and human activity recognition methods, both critical for developing safe and adaptive assistive robots, highlighting the value of our contribution. See our website for additional visualizations: https://emprise.cs.cornell.edu/robo-care/.

</details>


### [868] [From Power to Precision: Learning Fine-grained Dexterity for Multi-fingered Robotic Hands](https://arxiv.org/abs/2511.13710)
*Jianglong Ye,Lai Wei,Guangqi Jiang,Changwei Jing,Xueyan Zou,Xiaolong Wang*

Main category: cs.RO

TL;DR: This paper introduces a co-design framework combining hardware and control optimization for robotic hands to achieve both power and precision grasps, validated with robust sim-to-real performance.


<details>
  <summary>Details</summary>
Motivation: Current robotic hand designs struggle to seamlessly achieve both stable power grasps and precise, fine-grained manipulation, limiting their versatility in practical scenarios.

Method: The paper proposes a lightweight fingertip geometry modification represented as a contact plane, jointly optimized with corresponding control strategies. It uses a differentiable neural-physics surrogate model and dynamic control adaptation.

Result: Experiments demonstrate an 82.5% zero-shot success rate for sim-to-real precision grasping and a 93.3% success rate in real-world tasks, showcasing improved manipulation capabilities.

Conclusion: This framework enhances robotic hand dexterity, achieving both precision and power grasps without compromising performance, bridging a critical gap in robotic hand design.

Abstract: Human grasps can be roughly categorized into two types: power grasps and precision grasps. Precision grasping enables tool use and is believed to have influenced human evolution. Today's multi-fingered robotic hands are effective in power grasps, but for tasks requiring precision, parallel grippers are still more widely adopted. This contrast highlights a key limitation in current robotic hand design: the difficulty of achieving both stable power grasps and precise, fine-grained manipulation within a single, versatile system. In this work, we bridge this gap by jointly optimizing the control and hardware design of a multi-fingered dexterous hand, enabling both power and precision manipulation. Rather than redesigning the entire hand, we introduce a lightweight fingertip geometry modification, represent it as a contact plane, and jointly optimize its parameters along with the corresponding control. Our control strategy dynamically switches between power and precision manipulation and simplifies precision control into parallel thumb-index motions, which proves robust for sim-to-real transfer. On the design side, we leverage large-scale simulation to optimize the fingertip geometry using a differentiable neural-physics surrogate model. We validate our approach through extensive experiments in both sim-to-real and real-to-real settings. Our method achieves an 82.5% zero-shot success rate on unseen objects in sim-to-real precision grasping, and a 93.3% success rate in challenging real-world tasks involving bread pinching. These results demonstrate that our co-design framework can significantly enhance the fine-grained manipulation ability of multi-fingered hands without reducing their ability for power grasps. Our project page is at https://jianglongye.com/power-to-precision

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [869] [WITNESS: A lightweight and practical approach to fine-grained predictive mutation testing](https://arxiv.org/abs/2511.11999)
*Zeyu Lu,Peng Zhang,Chun Yong Chong,Shan Gao,Yibiao Yang,Yanhui Li,Lin Chen,Yuming Zhou*

Main category: cs.SE

TL;DR: WITNESS is a fine-grained predictive mutation testing approach addressing computational costs and limited applicability by using classical machine learning and supporting all types of mutants.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on deep learning, which is computationally expensive and limited to inside-method mutants, restricting real-world applicability.

Method: WITNESS uses lightweight classical machine learning models and incorporates features from both inside-method and outside-method mutants for prediction.

Result: WITNESS achieves state-of-the-art performance across scenarios, improves kill matrix prediction efficiency, and outperforms baselines in test case prioritization.

Conclusion: WITNESS is cost-effective, comprehensive, and interpretable, making predictive mutation testing more practical and efficient for real-world applications.

Abstract: Existing fine-grained predictive mutation testing studies predominantly rely on deep learning, which faces two critical limitations in practice: (1) Exorbitant computational costs. The deep learning models adopted in these studies demand significant computational resources for training and inference acceleration. This introduces high costs and undermines the cost-reduction goal of predictive mutation testing. (2) Constrained applicability. Although modern mutation testing tools generate mutants both inside and outside methods, current fine-grained predictive mutation testing approaches handle only inside-method mutants. As a result, they cannot predict outside-method mutants, limiting their applicability in real-world scenarios. We propose WITNESS, a new fine-grained predictive mutation testing approach. WITNESS adopts a twofold design: (1) With collected features from both inside-method and outside-method mutants, WITNESS is suitable for all generated mutants. (2) Instead of using computationally expensive deep learning, WITNESS employs lightweight classical machine learning models for training and prediction. This makes it more cost-effective and enabling straightforward explanations of the decision-making processes behind the adopted models. Evaluations on Defects4J projects show that WITNESS consistently achieves state-of-the-art predictive performance across different scenarios. Additionally, WITNESS significantly enhances the efficiency of kill matrix prediction. Post-hoc analysis reveals that features incorporating information from before and after the mutation are the most important among those used in WITNESS. Test case prioritization based on the predicted kill matrix shows that WITNESS delivers results much closer to those obtained by using the actual kill matrix, outperforming baseline approaches.

</details>


### [870] [A Code Smell Refactoring Approach using GNNs](https://arxiv.org/abs/2511.12069)
*HanYu Zhang,Tomoji Kishi*

Main category: cs.SE

TL;DR: The paper introduces a graph-based deep learning method for code smell refactoring, addressing challenges in existing approaches by employing graph neural networks and a semi-automated dataset generation technique.


<details>
  <summary>Details</summary>
Motivation: Existing methods of code smell refactoring often rely on manual heuristics or face data and model limitations in machine learning-based approaches. A more robust and scalable solution is required.

Method: The authors proposed a graph-based deep learning approach using class-level and method-level graphs. They used graph neural networks and semi-automated dataset creation for experiments focusing on refactoring code smells like long method, large class, and feature envy.

Result: The approach, implemented with GCN, GraphSAGE, and GAT architectures, demonstrated superior performance compared to traditional and advanced deep learning approaches.

Conclusion: The graph-based deep learning method provides effective and scalable solutions for code smell refactoring and could potentially overcome limitations of existing techniques.

Abstract: Code smell is a great challenge in software refactoring, which indicates latent design or implementation flaws that may degrade the software maintainability and evolution. Over the past decades, a variety of refactoring approaches have been proposed, which can be broadly classified into metrics-based, rule-based, and machine learning-based approaches. Recent years, deep learning-based approaches have also attracted widespread attention. However, existing techniques exhibit various limitations. Metrics- and rule-based approaches rely heavily on manually defined heuristics and thresholds, whereas deep learning-based approaches are often constrained by dataset availability and model design. In this study, we proposed a graph-based deep learning approach for code smell refactoring. Specifically, we designed two types of input graphs (class-level and method-level) and employed both graph classification and node classification tasks to address the refactoring of three representative code smells: long method, large class, and feature envy. In our experiment, we propose a semi-automated dataset generation approach that could generate a large-scale dataset with minimal manual effort. We implemented the proposed approach with three classical GNN (graph neural network) architectures: GCN, GraphSAGE, and GAT, and evaluated its performance against both traditional and state-of-the-art deep learning approaches. The results demonstrate that proposed approach achieves superior refactoring performance.

</details>


### [871] [Actionable Warning Is Not Enough: Recommending Valid Actionable Warnings with Weak Supervision](https://arxiv.org/abs/2511.12229)
*Zhipeng Xue,Zhipeng Gao,Tongtong Xu,Xing Hu,Xin Xia,Shanping Li*

Main category: cs.SE

TL;DR: Static analysis tools often have high false alarm rates, hindering their adoption. This study introduces ACWRecommender, a framework to distinguish actionable warnings likely to be real bugs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the precision of static analysis tools by effectively identifying actionable warnings that are real bugs, addressing the issue of high false alarm rates.

Method: The authors built a large actionable warning dataset by analyzing reversions from GitHub repositories. They proposed a two-stage framework (ACWRecommender) involving coarse-grained detection via UniXcoder and fine-grained reranking using weakly supervised learning.

Result: ACWRecommender demonstrated superior performance compared to baselines in terms of nDCG and MRR for recommending warnings. Among manually checked warnings, 27 were confirmed as real bugs by developers.

Conclusion: ACWRecommender effectively identifies actionable warnings with high probability of being real bugs, enhancing the practical use of static analysis tools for bug detection.

Abstract: The use of static analysis tools has gained increasing popularity among developers in the last few years. However, the widespread adoption of static analysis tools is hindered by their high false alarm rates. Previous studies have introduced the concept of actionable warnings and built a machine-learning method to distinguish actionable warnings from false alarms. However, according to our empirical observation, the current assumption used for actionable warning(s) collection is rather shaky and inaccurate, leading to a large number of invalid actionable warnings. To address this problem, in this study, we build the first large actionable warning dataset by mining 68,274 reversions from Top-500 GitHub C repositories, we then take one step further by assigning each actionable warning a weak label regarding its likelihood of being a real bug. Following that, we propose a two-stage framework called ACWRecommender to automatically recommend the actionable warnings with high probability to be real bugs (AWHB). Our approach warms up the pre-trained model UniXcoder by identifying actionable warnings task (coarse-grained detection stage) and rerank AWHB to the top by weakly supervised learning (fine-grained reranking stage). Experimental results show that our proposed model outperforms several baselines by a large margin in terms of nDCG and MRR for AWHB recommendation. Moreover, we ran our tool on 6 randomly selected projects and manually checked the top-ranked warnings from 2,197 reported warnings, we reported top-10 recommended warnings to developers, 27 of them were already confirmed by developers as real bugs. Developers can quickly find real bugs among the massive amount of reported warnings, which verifies the practical usage of our tool.

</details>


### [872] [Reflections on the design, applications and implementations of the normative specification language eFLINT](https://arxiv.org/abs/2511.12276)
*L. Thomas van Binsbergen,Christopher A. Esterhuyse,Tim Müller*

Main category: cs.SE

TL;DR: The paper discusses eFLINT, a domain-specific language designed to automate compliance checks related to software and legal requirements. It reflects on the language’s design, key applications, and requirements.


<details>
  <summary>Details</summary>
Motivation: With increasing software integration into societal practices and growing legal regulations, organizations face challenges in ensuring compliance. Automation is essential to meet these demands, given the complexities of legal interpretations, constant law revisions, and the need for cross-disciplinary expertise.

Method: The authors reflect on the design of the eFLINT language, which combines declarative and procedural features to reason about legal scenarios. They analyze the language’s goals, requirements, and applications, and the design choices made to address conflicting needs.

Result: eFLINT formalizes connections between legal and computational concepts, enabling automation of compliance checks before, during, and after software execution. Insights into the language’s evolution are shared, benefiting developers working on similar tools.

Conclusion: The paper demonstrates how eFLINT addresses the challenges of automated compliance and provides valuable reflections on its design. The analysis offers practical lessons for developing domain-specific languages in the field of legal-software engineering.

Abstract: Checking the compliance of software against laws, regulations and contracts is increasingly important and costly as the embedding of software into societal practices is getting more pervasive. Moreover, the digitalised services provided by governmental organisations and companies are governed by an increasing amount of laws and regulations, requiring highly adaptable compliance practices. A potential solution is to automate compliance using software. However, automating compliance is difficult for various reasons. Legal practices involve subjective processes such as interpretation and qualification. New laws and regulations come into effect regularly and laws and regulations, as well as their interpretations, are subjected to constant revision. In addition, computational reasoning with laws requires a cross-disciplinary process involving both legal and software expertise.
  This paper reflects on the domain-specific software language eFLINT developed to experiment with novel solutions. The language combines declarative and procedural elements to reason about situations and scenarios respectively, explicates and formalises connections between legal concepts and computational concepts, and is designed to automate compliance checks both before, during and after a software system runs. The various goals and applications areas for the language give rise to (conflicting) requirements. This paper reflects on the current design of the language by recalling various applications, the requirements they imposed, and subsequent design decisions. As such, this paper reports on results and insights of an investigation that can benefit language developers within the field of automated compliance.

</details>


### [873] [Reducing Hallucinations in LLM-Generated Code via Semantic Triangulation](https://arxiv.org/abs/2511.12288)
*Yihan Dai,Sijie Liang,Haotian Xu,Peichu Xie,Sergey Mechtaev*

Main category: cs.SE

TL;DR: This paper introduces semantic triangulation, a method aimed at improving the reliability of code generation by LLMs through problem transformations that allow better sample consensus and abstention.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle to reliably select correct code solutions when probabilities are low or there are multiple valid but non-equivalent solutions, and often fail to abstain when no correct solution exists in sampled outputs.

Method: The authors propose semantic triangulation, a method of altering problem semantics non-trivially while maintaining verifiable mappings between solutions pre- and post-transformation, enabling consistent and reliable verification.

Result: Semantic triangulation improved code generation reliability by 21% on benchmarks like LiveCodeBench and CodeElo, allowing accurate selection even at low sampling probabilities (0.14) and handling non-equivalent valid solutions effectively.

Conclusion: Semantic triangulation enhances code reliability and consensus formations in tasks with complex or varied correct solutions, outperforming current high-confidence probability threshold methods.

Abstract: When generating code from natural language prompts, an LLM samples programs from a probability distribution, many of which might be incorrect. Sample consensus techniques - such as majority voting or validation against generated tests or specifications - aim to identify a correct program in the sample or abstain if none is valid. However, existing methods often fail to select a correct solution when its sampling probability is low, or when the problem permits multiple valid but non-equivalent solutions. Additionally, they often fail to abstain when no correct solution is present in the sample. To overcome these limitations, we introduce semantic triangulation, which transforms a programming problem in a way that non-trivially alters its semantics while preserving an exact, verifiable mapping between solutions before and after transformation. We theoretically establish that verifying consistency across such problem transformations increases confidence that generated programs reflect accurate generalization rather than spurious statistical correlations, enabling more reliable sample consensus and abstention. On the LiveCodeBench and CodeElo benchmarks, using GPT-4o and DeepSeek-V3 models, semantic triangulation increases reliability of generated code by 21% compared to the method that selects only high-confidence solutions with the probability threshold 0.5, while being able to pinpoint correct solutions at sampling probabilities as low as 0.14. Apart from that, it is also the only approach to consistently form true consensus on tasks with multiple valid but non-equivalent solutions.

</details>


### [874] [ProofWright: Towards Agentic Formal Verification of CUDA](https://arxiv.org/abs/2511.12294)
*Bodhisatwa Chatterjee,Drew Zagieboylo,Sana Damani,Siva Hari,Christos Kozyrakis*

Main category: cs.SE

TL;DR: ProofWright integrates automated formal verification with LLM-based code generation, ensuring safer CUDA kernels without sacrificing productivity.


<details>
  <summary>Details</summary>
Motivation: To address the validation bottleneck caused by the unreliability of runtime testing and the scalability challenges of manual formal verification for LLM-generated CUDA kernels.

Method: Introduced ProofWright, a framework combining LLM code generation with automated formal verification to ensure end-to-end safety and correctness guarantees.

Result: ProofWright verified 74% of safety properties for CUDA kernels, identified subtle errors missed by standard testing, and achieved semantic equivalence for certain kernel types with minimal overhead.

Conclusion: Automated formal verification for LLM-generated GPU code is feasible, ensuring trustworthy high-performance code generation and maintaining developer productivity.

Abstract: Large Language Models (LLMs) are increasingly used to automatically generate optimized CUDA kernels, substantially improving developer productivity. However, despite rapid generation, these kernels often contain subtle correctness bugs and lack formal safety guarantees. Runtime testing is inherently unreliable - limited input coverage and reward hacking can mask incorrect behavior - while manual formal verification is reliable but cannot scale to match LLM output rates, creating a critical validation bottleneck.
  We present ProofWright, an agentic verification framework that bridges this gap by integrating automated formal verification with LLM-based code generation. ProofWright provides end-to-end guarantees of memory safety, thread safety, and semantic correctness for LLM-generated CUDA kernels. On KernelBench L1, ProofWright verifies safety properties for 74% of generated kernels, uncovers subtle correctness errors missed by conventional testing, and establishes semantic equivalence for a class of element-wise kernels. With a modest overhead of 3 minutes per kernel, ProofWright demonstrates that scalable, automated formal verification of LLM-generated GPU code is feasible - offering a path toward trustworthy high-performance code generation without sacrificing developer productivity.

</details>


### [875] [Enhancing LLM Code Generation Capabilities through Test-Driven Development and Code Interpreter](https://arxiv.org/abs/2511.12823)
*Sajed Jalil,Shuvo Saha,Hossain Mohammad Seym*

Main category: cs.SE

TL;DR: This paper focuses on democratizing access to effective code generation tools for Bengali users through a novel approach combining Test-Driven Development (TDD) and Code Interpreter (CI).


<details>
  <summary>Details</summary>
Motivation: Despite Bengali being widely spoken, it is underrepresented in LLM training for code generation. The paper seeks to bridge this gap and make tools accessible for users in resource-constrained markets.

Method: The authors propose a method that combines Test-Driven Development (TDD) and Code Interpreter (CI) techniques, relying on open-weight models without the need for fine-tuning.

Result: The approach achieves an overall accuracy of 85% and up to 98% accuracy with smaller models in the same family, showing competitive performance. Results are publicly available on GitHub.

Conclusion: This method demonstrates that effective code generation for Bengali prompts can be achieved even with smaller models, emphasizing accessibility for emerging markets without requiring fine-tuning.

Abstract: Over the past few years, improving LLM code generation capabilities has been a key focus in NLP research. Despite Bengali having 242 million native speakers worldwide, it receives little attention when it comes to training LLMs. More recently, various fine-tuning and augmented generation techniques have been employed to significantly enhance code generation performance. However, they require considerable expertise and resources to utilize effectively as an end user. The goal of our work is to democratize access to powerful code generation tools in resource-constrained emerging markets, enabling users to leverage them in their native language.
  We introduce a novel approach that combines Test-Driven Development (TDD) and Code Interpreter (CI), utilizing open-weight models, which improves the baseline accuracy for code generation with Bengali prompts and achieves an overall accuracy of 85%. Our approach requires no finetuning and proves that even the smallest models in the same family can attain up to 98% accuracy compared to the largest models. All of our results are publicly shared in GitHub for validation and reproducibility.

</details>


### [876] [High-level reasoning while low-level actuation in Cyber-Physical Systems: How efficient is it?](https://arxiv.org/abs/2511.12543)
*Burak Karaduman,Baris Tekin Tezel,Moharram Challenger*

Main category: cs.SE

TL;DR: This study compares execution and development times of various programming languages/frameworks for advanced industrial applications, focusing on real-time and intelligent behaviour.


<details>
  <summary>Details</summary>
Motivation: Engineers lack empirical evidence on selecting appropriate programming tools for complex industrial tasks.

Method: The study evaluates worst-case execution time and development effort across six programming technologies using developer-centred analysis.

Result: Findings reveal trade-offs between development workload and runtime efficiency influenced by abstraction and reasoning mechanisms.

Conclusion: Practical insights are provided for designing intelligent systems under real-time constraints, aiding tool selection and future research directions.

Abstract: The increasing complexity of industrial information-integration systems demands software technologies that enable intelligent behaviour, real-time response, and efficient development. Although many programming languages and frameworks exist, engineers still lack sufficient empirical evidence to guide the choice of tools for advanced industrial applications. This study addresses that need by measuring and comparing worst-case execution time (WCET) and development time across six languages and frameworks: C++, Java, Jade, Jason, and fuzzy Jason BDI with both loosely and tightly coupled integration. These technologies reflect a progression from procedural and object-oriented programming to agent-based frameworks capable of symbolic and fuzzy reasoning.
  Rather than relying on broad concepts such as paradigms or orientations, the study adopts a developer-centred approach grounded in measurable outcomes. The structured comparison examines how rising abstraction levels and reasoning capabilities affect both development effort and runtime behaviour. By analysing these dimensions, the study highlights concrete trade-offs between engineering workload and execution efficiency.
  The findings show how abstraction and reasoning mechanisms shape system performance and developer productivity, offering practical insight for designing intelligent, agent-based solutions that must operate under real-time constraints and complex decision-making requirements. Overall, the study contributes evidence-based guidance for selecting software technologies in industrial informatization, supporting improved integration efficiency, maintainability, and responsiveness, and laying groundwork for future research on the interplay between language features, development dynamics, and runtime behaviour in cyber-physical and smart manufacturing systems.

</details>


### [877] [Can Small GenAI Language Models Rival Large Language Models in Understanding Application Behavior?](https://arxiv.org/abs/2511.12576)
*Mohammad Meymani,Hamed Jelodar,Parisa Hamedi,Roozbeh Razavi-Far,Ali A. Ghorbani*

Main category: cs.SE

TL;DR: This paper evaluates small and large Generative AI models for malware detection and application behavior analysis, finding small models competitive in precision and recall while being resource-efficient.


<details>
  <summary>Details</summary>
Motivation: To determine the effectiveness and efficiency of small and large Generative AI models, particularly in the context of malware detection and application behavior analysis.

Method: Systematic evaluation of both small and large GenAI models through metrics like accuracy, precision, recall, and F1-score.

Result: Large models achieve higher accuracy, but small models retain competitive performance (precision and recall), proving their efficiency and scalability in resource-constrained environments.

Conclusion: Small GenAI models are effective complements to large models, offering balanced performance and resource efficiency for real-world application behavior analysis.

Abstract: Generative AI (GenAI) models, particularly large language models (LLMs), have transformed multiple domains, including natural language processing, software analysis, and code understanding. Their ability to analyze and generate code has enabled applications such as source code summarization, behavior analysis, and malware detection. In this study, we systematically evaluate the capabilities of both small and large GenAI language models in understanding application behavior, with a particular focus on malware detection as a representative task. While larger models generally achieve higher overall accuracy, our experiments show that small GenAI models maintain competitive precision and recall, offering substantial advantages in computational efficiency, faster inference, and deployment in resource-constrained environments. We provide a detailed comparison across metrics such as accuracy, precision, recall, and F1-score, highlighting each model's strengths, limitations, and operational feasibility. Our findings demonstrate that small GenAI models can effectively complement large ones, providing a practical balance between performance and resource efficiency in real-world application behavior analysis.

</details>


### [878] [LLM4SCREENLIT: Recommendations on Assessing the Performance of Large Language Models for Screening Literature in Systematic Reviews](https://arxiv.org/abs/2511.12635)
*Lech Madeyski,Barbara Kitchenham,Martin Shepperd*

Main category: cs.SE

TL;DR: The paper highlights challenges and proposes recommendations for evaluating large language models (LLMs) in identifying relevant literature for systematic reviews (SRs).


<details>
  <summary>Details</summary>
Motivation: The rapid release and adoption of LLMs for research purposes necessitate rigorous performance assessments to ensure reliability and accountability, particularly in SRs.

Method: The authors use a large-scale study as an example and review 27 related papers to identify issues with traditional performance metrics, extract good practices, and form recommendations.

Result: Key issues identified include an over-reliance on poor metrics like Accuracy, neglect of recall or lost evidence, inadequate reporting of confusion matrices, and lack of comprehensive designs. The paper also highlights good practices for evaluation.

Conclusion: The paper emphasizes using robust metrics such as Weighted MCC, reporting confusion matrices, accounting for unclassifiable outputs, adopting baseline comparisons, and grounding claims in a cost-benefit framework to improve SR evaluations.

Abstract: Context: Large language models (LLMs) are released faster than users' ability to evaluate them rigorously. When LLMs underpin research, such as identifying relevant literature for systematic reviews (SRs), robust empirical assessment is essential. Objective: We identify and discuss key challenges in assessing LLM performance for selecting relevant literature, identify good (evaluation) practices, and propose recommendations. Method: Using a recent large-scale study as an example, we identify problems with the use of traditional metrics for assessing the performance of Gen-AI tools for identifying relevant literature in SRs. We analyzed 27 additional papers investigating this issue, extracted the performance metrics, and found both good practices and widespread problems, especially with the use and reporting of performance metrics for SR screening. Results: Major weaknesses included: i) a failure to use metrics that are robust to imbalanced data and do not directly indicate whether results are better than chance, e.g., the use of Accuracy, ii) a failure to consider the impact of lost evidence when making claims concerning workload savings, and iii) pervasive failure to report the full confusion matrix (or performance metrics from which it can be reconstructed) which is essential for future meta-analyses. On the positive side, we extract good (evaluation) practices on which our recommendations for researchers and practitioners, as well as policymakers, are built. Conclusions: SR screening evaluations should prioritize lost evidence/recall alongside chance-anchored and cost-sensitive Weighted MCC (WMCC) metric, report complete confusion matrices, treat unclassifiable outputs as referred-back positives for assessment, adopt leakage-aware designs with non-LLM baselines and open artifacts, and ground conclusions in cost-benefit analysis where FNs carry higher penalties than FPs.

</details>


### [879] [Human-Centred Requirements Engineering for Critical Systems: Insights from Disaster Early Warning Applications](https://arxiv.org/abs/2511.12856)
*Anuradha Madugalla,Jixuan Dong,Kai Lyne Loi,Matthew Crossman,John Grundy*

Main category: cs.SE

TL;DR: The paper emphasizes integrating human-centric aspects into critical systems development, proposing guidelines and a process to enhance usability and accessibility, validated through a prototype and evaluations.


<details>
  <summary>Details</summary>
Motivation: The paper highlights the gap in addressing human and social contexts within critical systems development, as traditional approaches focus only on technical assurance rather than human-centric dependability.

Method: A literature review was conducted to derive design guidelines for vulnerable communities, yielding 62 requirements. These were operationalized in an adaptive early warning system prototype, validated through six interviews and eight cognitive walkthroughs.

Result: The evaluations showed that early inclusion of human-centric considerations improves usability and accessibility for all system users.

Conclusion: Human-centric considerations should be treated as a core aspect, not an optional feature, for creating safe and equitable critical systems.

Abstract: Critical systems, such as those used in healthcare, defence, and disaster management, demand rigorous requirements engineering to ensure safety and reliability. Yet, much of this rigour has traditionally focused on technical assurance, often overlooking the human and social contexts in which these systems operate. This paper argues that considering human-centric aspects is an essential dimension of dependability, and presents a human-centred RE process designed to integrate social responsibility into critical system development. Drawing from a literature review, we identified a set of guidelines for designing software for vulnerable communities and translated these into sixty-two functional and non-functional requirements. These requirements were operationalised through the design of an adaptive early warning system prototype, which was subsequently evaluated through six interviews and eight cognitive walkthroughs to validate their relevance and applicability. The findings demonstrate that human-centric requirements, when addressed early, enhance the usability and accessibility of systems for all users. The paper concludes by positioning human-centricity not as an ethical add-on but as a defining quality of safe and equitable critical systems.

</details>


### [880] [Agent READMEs: An Empirical Study of Context Files for Agentic Coding](https://arxiv.org/abs/2511.12884)
*Worawalan Chatlatanagulchai,Hao Li,Yutaro Kashiwa,Brittany Reid,Kundjanasith Thonglek,Pattara Leelaprute,Arnon Rungsawang,Bundit Manaskasemsak,Bram Adams,Ahmed E. Hassan,Hajimu Iida*

Main category: cs.SE

TL;DR: This paper investigates agent context files in coding tools, analyzing 2,303 files for structure, maintenance, and content, revealing their complexity and gaps in specifying non-functional requirements like security and performance.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand the structure, maintenance practices, and usage of agent context files, central to agentic coding tools, to improve their functionality and ensure safer, more efficient code generation.

Method: Authors conducted a large-scale empirical analysis of 2,303 agent context files from 1,925 repositories. They also performed a detailed content classification of 16 instruction types in these files.

Result: Findings show that agent context files are dynamic, complex artifacts evolving like configuration code. Developers mainly focus on functional context (e.g., build and implementation details) but often overlook non-functional requirements like security and performance.

Conclusion: The study highlights the need for better tooling and practices to address gaps in addressing non-functional requirements, ensuring agent-generated code is secure and efficient.

Abstract: Agentic coding tools receive goals written in natural language as input, break them down into specific tasks, and write or execute the actual code with minimal human intervention. Central to this process are agent context files ("READMEs for agents") that provide persistent, project-level instructions. In this paper, we conduct the first large-scale empirical study of 2,303 agent context files from 1,925 repositories to characterize their structure, maintenance, and content. We find that these files are not static documentation but complex, difficult-to-read artifacts that evolve like configuration code, maintained through frequent, small additions. Our content analysis of 16 instruction types shows that developers prioritize functional context, such as build and run commands (62.3%), implementation details (69.9%), and architecture (67.7%). We also identify a significant gap: non-functional requirements like security (14.5%) and performance (14.5%) are rarely specified. These findings indicate that while developers use context files to make agents functional, they provide few guardrails to ensure that agent-written code is secure or performant, highlighting the need for improved tooling and practices.

</details>


### [881] [Diffploit: Facilitating Cross-Version Exploit Migration for Open Source Library Vulnerabilities](https://arxiv.org/abs/2511.12950)
*Zirui Chen,Zhipeng Xue,Jiayuan Zhou,Xing Hu,Xin Xia,Xiaohu Yang*

Main category: cs.SE

TL;DR: The article introduces Diffploit, an exploit migration method tailored to address library vulnerability migration issues across versions. It achieves better results than current tools and identifies errors in existing vulnerability reports.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of adapting exploits to alternative versions of libraries due to changes in conditions and dynamic environments, which are difficult to handle manually or with existing tools.

Method: Proposes Diffploit, a diff-driven method using a Context Module to detect discrepancies and a Migration Module with iterative feedback, leveraging LLMs to adapt exploits effectively.

Result: In tests on 102 Java CVEs across 79 libraries, Diffploit migrated 84.2% of exploits, outperforming existing tools. It also identified inconsistencies in CVE reports and additional vulnerable versions.

Conclusion: Diffploit is an effective and innovative solution for exploit migration, outperforming current tools and providing critical insights for library vulnerability assessments.

Abstract: Exploits are commonly used to demonstrate the presence of library vulnerabilities and validate their impact across different versions. However, their direct application to alternative versions often fails due to breaking changes introduced during evolution. These failures stem from both changes in triggering conditions (e.g., API refactorings) and broken dynamic environments (e.g., build or runtime errors), which are challenging to interpret and adapt manually. Existing techniques primarily focus on code-level trace alignment through fuzzing, which is both time-consuming and insufficient for handling environment-level failures. Moreover, they often fall short when dealing with complicated triggering condition changes across versions. To overcome this, we propose Diffploit, an iterative, diff-driven exploit migration method structured around two key modules: the Context Module and the Migration Module. The Context Module dynamically constructs contexts derived from analyzing behavioral discrepancies between the target and reference versions, which capture the failure symptom and its related diff hunks. Leveraging these contexts, the Migration Module guides an LLM-based adaptation through an iterative feedback loop, balancing exploration of diff candidates and gradual refinement to resolve reproduction failures effectively. We evaluate Diffploit on a large-scale dataset containing 102 Java CVEs and 689 version-migration tasks across 79 libraries. Diffploit successfully migrates 84.2% exploits, outperforming the change-aware test repair tool TARGET by 52.0% and the rule-based tool in IDEA by 61.6%. Beyond technical effectiveness, Diffploit identifies 5 CVE reports with incorrect affected version ranges, three of which have been confirmed. It also discovers 111 unreported vulnerable versions in the GitHub Advisory Database.

</details>


### [882] [SmartPoC: Generating Executable and Validated PoCs for Smart Contract Bug Reports](https://arxiv.org/abs/2511.12993)
*Longfei Chen,Ruibin Yan,Taiyu Wong,Yiyang Chen,Chao Zhang*

Main category: cs.SE

TL;DR: SmartPoC is an automated framework that processes audit reports to generate validated Proof of Concept (PoC) test cases for smart contract vulnerabilities, addressing issues like noisy inputs and hallucinations in large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: Smart contracts are vulnerable to attacks, and automated as well as manual audits often lack reproducible and validated PoC tests, making verification resource-intensive. This paper aims to automate and streamline the creation of reliable test cases from textual audit reports.

Method: The authors developed SmartPoC, which refines audit report inputs, extracts bug-relevant functions, and utilizes LLMs to synthesize PoC test cases. It includes noise reduction, pre-/post-execution repair to ensure readiness, and incorporates differential verification to confirm test case exploitability.

Result: SmartPoC achieves 85.61% and 86.45% execution and validation rates on SmartBugs-Vul and FORGE-Vul benchmarks, respectively. It processed Etherscan reports and confirmed 236 real bugs out of 545 findings at a cost-effective rate of $0.03 per finding.

Conclusion: SmartPoC successfully automates the generation of reliable, executable PoC test cases from audit artifacts, reducing manual intervention and enhancing efficiency in smart contract vulnerability verification.

Abstract: Smart contracts are prone to vulnerabilities and are analyzed by experts as well as automated systems, such as static analysis and AI-assisted solutions. However, audit artifacts are heterogeneous and often lack reproducible, executable PoC tests suitable for automated validation, leading to costly, ad hoc manual verification. Large language models (LLMs) can be leveraged to turn audit reports into PoC test cases, but have three major challenges: noisy inputs, hallucinations, and missing runtime oracles. In this paper, we present SmartPoC, an automated framework that converts textual audit reports into executable, validated test cases. First, the input audit report is processed to reduce noise, and only bug-related functions are extracted and fed to LLMs as context. To curb hallucinations and ensure compile-and-run readiness, we leverage LLMs to synthesize PoC test cases with specially-designed pre-/post-execution repair. We further utilize differential verification as oracles to confirm exploitability of the PoC test cases. On the SmartBugs-Vul and FORGE-Vul benchmarks, SmartPoC generates executable, validated Foundry test cases for 85.61% and 86.45% of targets, respectively. Applied to the latest Etherscan verified-source corpus, SmartPoC confirms 236 real bugs out of 545 audit findings at a cost of only $0.03 per finding.

</details>


### [883] [Towards Requirements Engineering for GenAI-Enabled Software: Bridging Responsibility Gaps through Human Oversight Requirements](https://arxiv.org/abs/2511.13069)
*Zhenyu Mao,Jacky Keung,Yicheng Sun,Yifei Wang,Shuo Liu,Jialong Li*

Main category: cs.SE

TL;DR: This paper introduces a methodology addressing responsibility gaps in GenAI software by conceptualizing, identifying, and formalizing human oversight requirements. Results show its effectiveness over traditional methods.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity and adaptive nature of GenAI software exacerbate responsibility gaps in socio-technical systems, necessitating an improved framework to specify and manage human oversight requirements.

Method: The paper proposes a design methodology structured across three layers: a conceptualization layer (to define responsibility dimensions), a methodological layer (deductive pipeline for identifying responsibility gaps), and an artifact layer (a formalized Deductive Backbone Table).

Result: A user study demonstrated that this methodology improves oversight requirements identification compared to a baseline methodology, successfully addressing research gaps across six dimensions.

Conclusion: The presented methodology enhances the RE process for GenAI-enabled systems by systematically analyzing and mitigating responsibility gaps, contributing a structured framework to achieve clearer accountability.

Abstract: Context: Responsibility gaps, long-recognized challenges in socio-technical systems where accountability becomes diffuse or ambiguous, have become increasingly pronounced in GenAI-enabled software. The generative and adaptive nature complicates how human oversight and responsibility are specified, delegated, and traced. Existing requirements engineering (RE) approaches remain limited in addressing these phenomena, revealing conceptual, methodological, and artifact-level research gaps.. Objective: This study aims to analyze these research gaps in the context of GenAI-enabled software systems. It seeks to establish a coherent perspective for a systematic analysis of responsibility gaps from a human oversight requirements standpoint, encompassing how these responsibility gaps should be conceptualized, identified, and represented throughout the RE process. Methods: The proposed design methodology is structured across three analytical layers. At the conceptualization layer, it establishes a conceptual framing that defines the key elements of responsibility across the human and system dimensions and explains how potential responsibility gaps emerge from their interactions. At the methodological layer, it introduces a deductive pipeline for identifying responsibility gaps by analyzing interactions between these dimensions and deriving corresponding oversight requirements within established RE frameworks. At the artifact layer, it formalizes the results in a Deductive Backbone Table, a reusable representation that traces the reasoning path from responsibility gaps identification to human oversight requirements derivation. Results: A user study compared the proposed methodology with a baseline goal-oriented RE across two scenarios. Evaluation across six dimensions indicated clear improvements of the proposed methodology, confirming its effectiveness in addressing three research gaps.

</details>


### [884] [Examining the Usage of Generative AI Models in Student Learning Activities for Software Programming](https://arxiv.org/abs/2511.13271)
*Rufeng Chen,Shuaishuai Jiang,Jiyun Shen,AJung Moon,Lili Wei*

Main category: cs.SE

TL;DR: This study explores the influence of Generative AI tools like ChatGPT on knowledge gains and task performance for programming students, highlighting varying impacts based on proficiency levels.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the gap in research about how Generative AI impacts learning outcomes, especially knowledge gains, in programming education beyond task performance.

Method: A controlled experiment with 24 undergraduate students (beginner and intermediate programming levels) was conducted to analyze their interactions with ChatGPT while solving programming tasks.

Result: Beginners improved task performance with ChatGPT but often lacked knowledge gains, while intermediates used the tool more selectively. Over-reliance or minimal use hindered knowledge acquisition.

Conclusion: The study emphasizes the need for guidance in using GenAI tools as learning aids rather than task completion tools to ensure deeper understanding, particularly in computing education.

Abstract: The rise of Generative AI (GenAI) tools like ChatGPT has created new opportunities and challenges for computing education. Existing research has primarily focused on GenAI's ability to complete educational tasks and its impact on student performance, often overlooking its effects on knowledge gains. In this study, we investigate how GenAI assistance compares to conventional online resources in supporting knowledge gains across different proficiency levels. We conducted a controlled user experiment with 24 undergraduate students of two different levels of programming experience (beginner, intermediate) to examine how students interact with ChatGPT while solving programming tasks. We analyzed task performance, conceptual understanding, and interaction behaviors. Our findings reveal that generating complete solutions with GenAI significantly improves task performance, especially for beginners, but does not consistently result in knowledge gains. Importantly, usage strategies differ by experience: beginners tend to rely heavily on GenAI toward task completion often without knowledge gain in the process, while intermediates adopt more selective approaches. We find that both over-reliance and minimal use result in weaker knowledge gains overall. Based on our results, we call on students and educators to adopt GenAI as a learning rather than a problem solving tool. Our study highlights the urgent need for guidance when integrating GenAI into programming education to foster deeper understanding.

</details>


### [885] [SAINT: Service-level Integration Test Generation with Program Analysis and LLM-based Agents](https://arxiv.org/abs/2511.13305)
*Rangeet Pan,Raju Pavuluri,Ruikai Huang,Rahul Krishna,Tyler Stennett,Alessandro Orso,Saurabh SInha*

Main category: cs.SE

TL;DR: SAINT is proposed as an advanced testing approach combining static analysis and LLM-based agents for Java application testing, which improves upon traditional methods.


<details>
  <summary>Details</summary>
Motivation: Current service-level testing tools rely on fuzzing or OpenAPI specifications and lack efficacy in creating functional, scenario-driven tests, which is a challenge for enterprise codebases.

Method: SAINT merges static analysis with LLMs to construct endpoint models and operation dependency graphs. It uses LLM-based agents to design endpoint-specific and scenario-centered tests by employing a systematic agentic loop.

Result: SAINT was evaluated on eight Java applications, demonstrating superior performance in coverage, fault detection, and scenario generation. Developer feedback highlighted its utility.

Conclusion: Combining static analysis with LLM workflows results in effective, functional, and developer-friendly service-level test generation for enterprise applications.

Abstract: Enterprise applications are typically tested at multiple levels, with service-level testing playing an important role in validating application functionality. Existing service-level testing tools, especially for RESTful APIs, often employ fuzzing and/or depend on OpenAPI specifications which are not readily available in real-world enterprise codebases. Moreover, these tools are limited in their ability to generate functional tests that effectively exercise meaningful scenarios. In this work, we present SAINT, a novel white-box testing approach for service-level testing of enterprise Java applications. SAINT combines static analysis, large language models (LLMs), and LLM-based agents to automatically generate endpoint and scenario-based tests. The approach builds two key models: an endpoint model, capturing syntactic and semantic information about service endpoints, and an operation dependency graph, capturing inter-endpoint ordering constraints. SAINT then employs LLM-based agents to generate tests. Endpoint-focused tests aim to maximize code and database interaction coverage. Scenario-based tests are synthesized by extracting application use cases from code and refining them into executable tests via planning, action, and reflection phases of the agentic loop. We evaluated SAINT on eight Java applications, including a proprietary enterprise application. Our results illustrate the effectiveness of SAINT in coverage, fault detection, and scenario generation. Moreover, a developer survey provides strong endorsement of the scenario-based tests generated by SAINT. Overall, our work shows that combining static analysis with agentic LLM workflows enables more effective, functional, and developer-aligned service-level test generation.

</details>


### [886] [LinkXplore: A Framework for Affordable High-Quality Blockchain Data](https://arxiv.org/abs/2511.13318)
*Peihao Li*

Main category: cs.SE

TL;DR: LinkXplore is an open framework for economical and flexible on-chain data collection and management, addressing the high cost of blockchain data.


<details>
  <summary>Details</summary>
Motivation: The paper highlights the high expense of large-scale blockchain data collection, which hinders academic research and industrial applications, as well as the lack of modular frameworks for flexible integration with on-chain data analysis.

Method: LinkXplore is introduced as a solution, enabling analysis of raw blockchain data from RPC queries/streams instead of relying on costly data providers. This is achieved through a simple API and backend processing for modular integration.

Result: LinkXplore provides high-quality blockchain data efficiently and cost-effectively, serving as a practical tool for budget-conscious researchers and developers.

Conclusion: The framework significantly reduces the cost barrier in blockchain data access, promotes open research/development, and establishes a systematic framework for scalable on-chain data analysis.

Abstract: Blockchain technologies are rapidly transforming both academia and industry. However, large-scale blockchain data collection remains prohibitively expensive, as many RPC providers only offer enhanced APIs with high pricing tiers that are unsuitable for budget-constrained research or industrial-scale applications, which has significantly slowed down academic studies and product development. Moreover, there is a clear lack of a systematic framework that allows flexible integration of new modules for analyzing on-chain data.
  To address these challenges, we introduce LinkXplore, the first open framework for collecting and managing on-chain data. LinkXplore enables users to bypass costly blockchain data providers by directly analyzing raw data from RPC queries or streams, thereby offering high-quality blockchain data at a fraction of the cost. Through a simple API and backend processing logic, any type of chain data can be integrated into the framework. This makes it a practical alternative for both researchers and developers with limited budgets. Code and dataset used in this project are publicly available at https://github.com/Linkis-Project/LinkXplore

</details>


### [887] [An LLM-based Quantitative Framework for Evaluating High-Stealthy Backdoor Risks in OSS Supply Chains](https://arxiv.org/abs/2511.13341)
*Zihe Yan,Kai Luo,Haoyu Yang,Yang Yu,Zhuosheng Zhang,Guancheng Li*

Main category: cs.SE

TL;DR: The paper discusses challenges in securing open-source software supply chains, proposing a framework using large language models to assess backdoor risks in repositories and finding notable vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: Open-source software dependencies are prone to risks like unmaintained code, insufficient audits, and stealthy backdoor attacks, presenting challenges in ensuring security for complex systems.

Method: The authors designed a fine-grained framework that models backdoor attack stages, employs metrics for risk evaluation, and integrates large language models to semantically assess repository security beyond manual or static analysis.

Result: Evaluation on 66 Debian packages revealed diverse security risks in the open-source supply chain, demonstrating the effectiveness of their proposed framework.

Conclusion: The framework improves detection and risk assessment for backdoor attacks in software repositories, advocating for enhanced security measures in the open-source ecosystem.

Abstract: In modern software development workflows, the open-source software supply chain contributes significantly to efficient and convenient engineering practices. With increasing system complexity, using open-source software as third-party dependencies has become a common practice. However, the lack of maintenance for underlying dependencies and insufficient community auditing create challenges in ensuring source code security and the legitimacy of repository maintainers, especially under high-stealthy backdoor attacks exemplified by the XZ-Util incident. To address these problems, we propose a fine-grained project evaluation framework for backdoor risk assessment in open-source software. The framework models stealthy backdoor attacks from the viewpoint of the attacker and defines targeted metrics for each attack stage. In addition, to overcome the limitations of static analysis in assessing the reliability of repository maintenance activities such as irregular committer privilege escalation and limited participation in reviews, the framework uses large language models (LLMs) to conduct semantic evaluation of code repositories without relying on manually crafted patterns. The framework is evaluated on sixty six high-priority packages in the Debian ecosystem. The experimental results indicate that the current open-source software supply chain is exposed to various security risks.

</details>


### [888] [FLOWER: Flow-Oriented Entity-Relationship Tool](https://arxiv.org/abs/2511.13357)
*Dmitry Moskalev*

Main category: cs.SE

TL;DR: The paper introduces FLOWER, a tool designed to optimize entity relationship models across multiple databases by automating processing and visualization tasks. It significantly improves analysis, modeling, and data storytelling.


<details>
  <summary>Details</summary>
Motivation: To address limitations in manually constructing entity relationship models, which are influenced by human factors and often challenging when dealing with large synthetic or organic datasets.

Method: FLOWER employs dynamic sampling, robust data analysis techniques, and automated constraint detection to process, create, and visualize entity relationships for SQL databases on-the-fly.

Result: FLOWER outperforms standard methods like reservoir sampling, achieving enhancements in distribution representation (2.4x), constraint learning (2.6x), and a 2.15x speed acceleration. It also improves accuracy (1.19x) and reduces context load (1.86x) for data storytelling.

Conclusion: FLOWER is a novel, end-to-end tool that improves entity-relationship modeling and data analysis, ensuring scalability, applicability, and quality for various use cases across 23 languages and different hardware configurations.

Abstract: Exploring relationships across data sources is a crucial optimization for entities recognition. Since databases can store big amount of information with synthetic and organic data, serving all quantity of objects correctly is an important task to deal with. However, the decision of how to construct entity relationship model is associated with human factor. In this paper, we present flow-oriented entity-relationship tool. This is first and unique end-to-end solution that eliminates routine and resource-intensive problems of processing, creating and visualizing both of explicit and implicit dependencies for prominent SQL dialects on-the-fly. Once launched, FLOWER automatically detects built-in constraints and starting to create own correct and necessary one using dynamic sampling and robust data analysis techniques. This approach applies to improve entity-relationship model and data storytelling to better understand the foundation of data and get unseen insights from DB sources using SQL or natural language. Evaluated on state-of-the-art STATS benchmark, experiments show that FLOWER is superior to reservoir sampling by 2.4x for distribution representation and 2.6x for constraint learning with 2.15x acceleration. For data storytelling, our tool archives 1.19x for accuracy enhance with 1.86x context decrease compare to LLM. Presented tool is also support 23 languages and compatible with both of CPU and GPU. Those results show that FLOWER can manage with real-world data a way better to ensure with quality, scalability and applicability for different use-cases.

</details>


### [889] [BIOMERO 2.0: end-to-end FAIR infrastructure for bioimaging data import, analysis, and provenance](https://arxiv.org/abs/2511.13611)
*Torec T. Luik,Joost de Folter,Rodrigo Rosas-Bertolini,Eric A. J. Reits,Ron A. Hoebe,Przemek M. Krawczyk*

Main category: cs.SE

TL;DR: BIOMERO 2.0 transforms OMERO into a FAIR-compliant bioimaging platform by enhancing features like data import, analysis, and provenance tracking.


<details>
  <summary>Details</summary>
Motivation: To address challenges in achieving FAIR principles and provenance tracking in bioimaging analysis environments.

Method: Introduced BIOMERO 2.0, which integrates OMERO with a plugin and containerized components for data preprocessing, analysis, and workflow monitoring.

Result: BIOMERO 2.0 ensures real-time provenance tracking for imports and analyses, connecting data acquisition to analysis and sharing workflows.

Conclusion: BIOMERO 2.0 significantly advances OMERO's capabilities, providing a streamlined and FAIR-compliant platform for bioimaging analysis and workflow traceability.

Abstract: We present BIOMERO 2.0, a major evolution of the BIOMERO framework that transforms OMERO into a FAIR-compliant (findable, accessible, interoperable, and reusable), provenance-aware bioimaging platform. BIOMERO 2.0 integrates data import, preprocessing, analysis, and workflow monitoring through an OMERO.web plugin and containerized components. The importer subsystem facilitates in-place import using containerized preprocessing and metadata enrichment via forms, while the analyzer subsystem coordinates and tracks containerized analyses on high-performance computing systems via the BIOMERO Python library. All imports and analyses are recorded with parameters, versions, and results, ensuring real-time provenance accessible through integrated dashboards. This dual approach places OMERO at the heart of the bioimaging analysis process: the importer ensures provenance from image acquisition through preprocessing and import into OMERO, while the analyzer records it for downstream processing. These integrated layers enhance OMEROs FAIRification, supporting traceable, reusable workflows for image analysis that bridge the gap between data import, analysis, and sharing.

</details>


### [890] [Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?](https://arxiv.org/abs/2511.13646)
*Chunqiu Steven Xia,Zhe Wang,Yan Yang,Yuxiang Wei,Lingming Zhang*

Main category: cs.SE

TL;DR: Live-SWE-agent autonomously evolves itself during runtime to solve software problems, achieving leading solve rates on benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing software agents that require costly design and training while being suboptimal, this paper introduces an agent capable of evolving autonomously during runtime.

Method: Live-SWE-agent starts with a basic scaffold and uses runtime evolution to improve its own scaffold implementation while solving software tasks.

Result: Live-SWE-agent achieved a solve rate of 75.4% on SWE-bench Verified and 45.8% on SWE-Bench Pro benchmarks, outperforming other state-of-the-art software agents.

Conclusion: Live-SWE-agent demonstrates a novel approach to self-improvement during runtime, optimizing software problem-solving capabilities and outperforming competitive benchmarks.

Abstract: Large Language Models (LLMs) are reshaping almost all industries, including software engineering. In recent years, a number of LLM agents have been proposed to solve real-world software problems. Such software agents are typically equipped with a suite of coding tools and can autonomously decide the next actions to form complete trajectories to solve end-to-end software tasks. While promising, they typically require dedicated design and may still be suboptimal, since it can be extremely challenging and costly to exhaust the entire agent scaffold design space. Recognizing that software agents are inherently software themselves that can be further refined/modified, researchers have proposed a number of self-improving software agents recently, including the Darwin-Gödel Machine (DGM). Meanwhile, such self-improving agents require costly offline training on specific benchmarks and may not generalize well across different LLMs or benchmarks. In this paper, we propose Live-SWE-agent, the first live software agent that can autonomously and continuously evolve itself on-the-fly during runtime when solving real-world software problems. More specifically, Live-SWE-agent starts with the most basic agent scaffold with only access to bash tools (e.g., mini-SWE-agent), and autonomously evolves its own scaffold implementation while solving real-world software problems. Our evaluation on the widely studied SWE-bench Verified benchmark shows that Live-SWE-agent can achieve an impressive solve rate of 75.4% without test-time scaling, outperforming all existing open-source software agents and approaching the performance of the best proprietary solution. Moreover, Live-SWE-agent outperforms state-of-the-art manually crafted software agents on the recent SWE-Bench Pro benchmark, achieving the best-known solve rate of 45.8%.

</details>


### [891] [What's in a Software Engineering Job Posting?](https://arxiv.org/abs/2511.13656)
*Marvin Wyrich,Lloyd Montgomery*

Main category: cs.SE

TL;DR: The study analyzes 100 job postings to explore the non-technical expectations for software engineers, focusing on qualities like culture fit, personal growth, and interpersonal skills.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the evolving narrative in the Software Engineering field, highlighting that non-technical aspects are increasingly important in candidate evaluation.

Method: The study employed Thematic Analysis on 100 SE job postings to identify non-technical requirements.

Result: Findings show that employers value candidates who align with company purpose, cultural fit, personal growth pursuits, and strong interpersonal abilities.

Conclusion: The study underscores the significance of non-technical expectations in SE roles, providing insights for various stakeholders and documenting 2023 trends in job requirements.

Abstract: A well-rounded software engineer is often defined by technical prowess and the ability to deliver on complex projects. However, the narrative around the ideal Software Engineering (SE) candidate is evolving, suggesting that there is more to the story. This article explores the non-technical aspects emphasized in SE job postings, revealing the sociotechnical and organizational expectations of employers. Our Thematic Analysis of 100 job postings shows that employers seek candidates who align with their sense of purpose, fit within company culture, pursue personal and career growth, and excel in interpersonal interactions. This study contributes to ongoing discussions in the SE community about the evolving role and workplace context of software engineers beyond technical skills. By highlighting these expectations, we provide relevant insights for researchers, educators, practitioners, and recruiters. Additionally, our analysis offers a valuable snapshot of SE job postings in 2023, providing a scientific record of prevailing trends and expectations.

</details>


### [892] [Ontology-Driven Model-to-Model Transformation of Workflow Specifications](https://arxiv.org/abs/2511.13661)
*Francisco Abreu,Luís Cruz,Sérgio Guerreiro*

Main category: cs.SE

TL;DR: This paper addresses vendor lock-in by creating an ontology-driven pipeline that converts proprietary workflows to BPMN 2.0, improving interoperability and reusability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address vendor lock-in caused by proprietary workflow modeling languages, which hinder interoperability and reuse, and to ease the migration to open standards.

Method: The paper introduces a model-to-model (M2M) pipeline with three phases: semantic lifting of JSON to RDF/OWL, ontology alignment and reasoning, and BPMN generation using the Camunda Model API. The approach externalizes mapping knowledge into ontologies and declarative rules.

Result: The pipeline was instantiated for Instituto Superior Técnico (IST)'s Smart Forms & Smart Flow, achieving a 94.2% success rate in generating BPMN diagrams from real-world workflows. Failures occurred due to dynamic and time-based transitions not represented in static JSON.

Conclusion: The presented pipeline enables interoperability and reduces vendor dependency by systematically translating domain-specific workflows into BPMN. It provides quantitative and qualitative benefits and is adaptable to other proprietary languages.

Abstract: Proprietary workflow modeling languages such as Smart Forms & Smart Flow hamper interoperability and reuse because they lock process knowledge into closed formats. To address this vendor lock-in and ease migration to open standards, we introduce an ontology-driven model-to-model pipeline that systematically translates domain-specific workflow definitions to Business Process Model and Notation (BPMN) 2.0. The pipeline comprises three phases: RML-based semantic lifting of JSON to RDF/OWL, ontology alignment and reasoning, and BPMN generation via the Camunda Model API. By externalizing mapping knowledge into ontologies and declarative rules rather than code, the approach supports reusability across vendor-specific formats and preserves semantic traceability between source definitions and target BPMN models. We instantiated the pipeline for Instituto Superior Técnico (IST)'s Smart Forms & Smart Flow and implemented a converter that produces standard-compliant BPMN diagrams. Evaluation on a corpus of 69 real-world workflows produced 92 BPMN diagrams with a 94.2% success rate. Failures (5.81%) stemmed from dynamic behaviors and time-based transitions not explicit in the static JSON. Interviews with support and development teams indicated that the resulting diagrams provide a top-down view that improves comprehension, diagnosis and onboarding by exposing implicit control flow and linking tasks and forms back to their sources. The pipeline is generalizable to other proprietary workflow languages by adapting the ontology and mappings, enabling interoperability and reducing vendor dependency while supporting continuous integration and long-term maintainability. The presented case study demonstrates that ontology-driven M2M transformation can systematically bridge domain-specific workflows and standard notations, offering quantifiable performance and qualitative benefits for stakeholders.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [893] [A Stochastic Quantum Neural Network Model for Ai](https://arxiv.org/abs/2511.11609)
*Gautier-Edouard Filardo,Thibaut Heckmann*

Main category: q-bio.NC

TL;DR: This paper introduces a stochastic quantum neural network model inspired by biological neuronal processes, exploring quantum computing's potential application in AI and computational neuroscience.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the limitations of traditional artificial neural networks, particularly their inability to fully emulate the complexity and dynamics of the biological brain.

Method: The method involves mathematical formalization of stochastic quantum neural networks (QNNS), where qubits evolve following stochastic differential equations that reflect biological neural dynamics.

Result: A framework for stochastic quantum neural networks was introduced, highlighting key challenges such as decoherence and qubit stability.

Conclusion: Quantum computing may provide a novel tool for advancing AI and computational neuroscience, though substantial challenges must be addressed to realize its potential.

Abstract: Artificial intelligence (AI) has drawn significant inspiration from neuroscience to develop artificial neural network (ANN) models. However, these models remain constrained by the Von Neumann architecture and struggle to capture the complexity of the biological brain. Quantum computing, with its foundational principles of superposition, entanglement, and unitary evolution, offers a promising alternative approach to modeling neural dynamics. This paper explores the possibility of a neuro-quantum model of the brain by introducing a stochastic quantum approach that incorporates random fluctuations of neuronal processing within a quantum framework. We propose a mathematical formalization of stochastic quantum neural networks (QNNS), where qubits evolve according to stochastic differential equations inspired by biological neuronal processes. We also discuss challenges related to decoherence, qubit stability, and implications for AI and computational neuroscience.

</details>


### [894] [Predicting upcoming visual features during eye movements yields scene representations aligned with human visual cortex](https://arxiv.org/abs/2511.12715)
*Sushrut Thorat,Adrien Doerig,Alexander Kroner,Carmen Amme,Tim C. Kietzmann*

Main category: q-bio.NC

TL;DR: The paper develops Glimpse Prediction Networks (GPNs), showing they can self-supervisedly learn scene representations aligned with human brain activity.


<details>
  <summary>Details</summary>
Motivation: Understanding and modeling unified scene representations that align with natural human vision and brain activity.

Method: Developing and testing recurrent Glimpse Prediction Networks (GPNs) to predict feature embeddings of scene glimpses during active vision.

Result: GPNs successfully modeled spatial and semantic relations, outperformed semantic-based methods, and aligned strongly with human fMRI responses.

Conclusion: Next-glimpse prediction is a plausible self-supervised method for learning brain-aligned visual scene representations.

Abstract: Scenes are complex, yet structured collections of parts, including objects and surfaces, that exhibit spatial and semantic relations to one another. An effective visual system therefore needs unified scene representations that relate scene parts to their location and their co-occurrence. We hypothesize that this structure can be learned self-supervised from natural experience by exploiting the temporal regularities of active vision: each fixation reveals a locally-detailed glimpse that is statistically related to the previous one via co-occurrence and saccade-conditioned spatial regularities. We instantiate this idea with Glimpse Prediction Networks (GPNs) -- recurrent models trained to predict the feature embedding of the next glimpse along human-like scanpaths over natural scenes. GPNs successfully learn co-occurrence structure and, when given relative saccade location vectors, show sensitivity to spatial arrangement. Furthermore, recurrent variants of GPNs were able to integrate information across glimpses into a unified scene representation. Notably, these scene representations align strongly with human fMRI responses during natural-scene viewing across mid/high-level visual cortex. Critically, GPNs outperform architecture- and dataset-matched controls trained with explicit semantic objectives, and match or exceed strong modern vision baselines, leaving little unique variance for those alternatives. These results establish next-glimpse prediction during active vision as a biologically plausible, self-supervised route to brain-aligned scene representations learned from natural visual experience.

</details>


### [895] [Integrative Model for Interoception and Exteroception: predictive coding, points of modulation, and testable predictions](https://arxiv.org/abs/2511.13668)
*Pranjal Balar,Sundeep Kapila*

Main category: q-bio.NC

TL;DR: This paper introduces and validates a predictive coding model integrating interoception and exteroception, explaining their role in anxiety and PTSD.


<details>
  <summary>Details</summary>
Motivation: To provide a unified predictive coding framework synthesizing interoceptive and exteroceptive feedback mechanisms and explain their malfunction in anxiety and PTSD.

Method: The model simulates interoceptive-exteroceptive dynamics via computational algorithms and tests predictions using EEG-fMRI datasets as well as experimental behavioral paradigms.

Result: The model shows biologically plausible dynamics, explains dysregulated precision weighting in anxiety/PTSD, and is validated using human neuroimaging data and proposed tasks.

Conclusion: The predictive coding framework advances understanding of interoceptive-exteroceptive integration and offers testable hypotheses for future research into anxiety and PTSD therapies.

Abstract: Interoception and exteroception provide continuous feedback about the body and the environment, yet how they are dynamically integrated within a unified predictive coding framework has remained under-specified. This paper develops and empirically validates an integrative predictive coding model that treats interoceptive and exteroceptive inference as parallel hierarchical systems exchanging precision-weighted prediction errors. Within this framework, arbitration between the two streams is governed by relative precision weights (w) and integrated within the anterior insula (AIC) and anterior cingulate cortex (ACC). Computational simulations of the model reproduced biologically plausible dynamics: prediction errors decayed exponentially while arbitration weights self-normalized toward equilibrium (w = 0.5), demonstrating stable convergence and coherent integration. Simulated anxiety and PTSD profiles, characterized respectively by interoceptive and exteroceptive overweighting, yielded rigid, self-sustaining imbalances (w to 1 or w to 0) and slowed recalibration. Empirical application of the arbitration equation to published EEG-fMRI datasets further validated the model. The framework contributes a unifying account of how dysregulated precision weighting may underlie anxiety (overweighted interoception) and PTSD (underweighted interoception). Building on this validation, a proposed experimental paradigm is outlined to test the model's predictions in humans. It examines recalibration across anxiety, neutral, and PTSD groups following targeted interoceptive or exteroceptive therapies. Key predictions include identifiable neural markers of coherence, modulation of heartbeat-evoked potentials by vagal stimulation, and precision-sensitive behavioral signatures in interoceptive-exteroceptive congruency tasks.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [896] [Generalized Inequality-based Approach for Probabilistic WCET Estimation](https://arxiv.org/abs/2511.11682)
*Hayate Toba,Atsushi Yano,Takuya Azumi*

Main category: stat.ML

TL;DR: The paper proposes a method to refine probabilistic Worst-Case Execution Time (pWCET) estimation by reducing pessimism using saturating functions in Chebyshev's inequality.


<details>
  <summary>Details</summary>
Motivation: Accurate pWCET estimation is crucial for ensuring real-time application's performance in areas like autonomous driving and IoT systems, addressing limitations in existing methods based on EVT and inequality approaches.

Method: Incorporating saturating functions (arctangent and hyperbolic tangent) into Chebyshev's inequality to mitigate large outlier effects while maintaining mathematical soundness.

Result: The method provides safe and less pessimistic bounds for pWCET, demonstrated using synthetic and real-world data from an autonomous driving stack.

Conclusion: The proposed approach successfully reduces pessimism while retaining accuracy, making it highly applicable for timing correctness in real-time applications.

Abstract: Estimating the probabilistic Worst-Case Execution Time (pWCET) is essential for ensuring the timing correctness of real-time applications, such as in robot IoT systems and autonomous driving systems. While methods based on Extreme Value Theory (EVT) can provide tight bounds, they suffer from model uncertainty due to the need to decide where the upper tail of the distribution begins. Conversely, inequality-based approaches avoid this issue but can yield pessimistic results for heavy-tailed distributions. This paper proposes a method to reduce such pessimism by incorporating saturating functions (arctangent and hyperbolic tangent) into Chebyshev's inequality, which mitigates the influence of large outliers while preserving mathematical soundness. Evaluations on synthetic and real-world data from the Autoware autonomous driving stack demonstrate that the proposed method achieves safe and tighter bounds for such distributions.

</details>


### [897] [FreDN: Spectral Disentanglement for Time Series Forecasting via Learnable Frequency Decomposition](https://arxiv.org/abs/2511.11817)
*Zhongde An,Jinhong You,Jiyanglin Li,Yiming Tang,Wen Li,Heming Du,Shouguo Du*

Main category: stat.ML

TL;DR: This paper addresses spectral challenges in non-stationary time series forecasting by introducing FreDN, a method with frequency disentangling and efficient complex-valued operations.


<details>
  <summary>Details</summary>
Motivation: Spectral entanglement and computational challenges in complex-valued learning hinder effective forecasting in non-stationary time series.

Method: The paper proposes FreDN with a Frequency Disentangler to separate trends and periodicities in the frequency domain and a ReIm Block for efficient complex-valued computation.

Result: FreDN improves forecasting accuracy by up to 10% on benchmarks and reduces computational cost and model size by 50%.

Conclusion: FreDN effectively handles spectral entanglement, enhances forecasting accuracy, and reduces complexity, advancing frequency-domain forecasting methods.

Abstract: Time series forecasting is essential in a wide range of real world applications. Recently, frequency-domain methods have attracted increasing interest for their ability to capture global dependencies. However, when applied to non-stationary time series, these methods encounter the $\textit{spectral entanglement}$ and the computational burden of complex-valued learning. The $\textit{spectral entanglement}$ refers to the overlap of trends, periodicities, and noise across the spectrum due to $\textit{spectral leakage}$ and the presence of non-stationarity. However, existing decompositions are not suited to resolving spectral entanglement. To address this, we propose the Frequency Decomposition Network (FreDN), which introduces a learnable Frequency Disentangler module to separate trend and periodic components directly in the frequency domain. Furthermore, we propose a theoretically supported ReIm Block to reduce the complexity of complex-valued operations while maintaining performance. We also re-examine the frequency-domain loss function and provide new theoretical insights into its effectiveness. Extensive experiments on seven long-term forecasting benchmarks demonstrate that FreDN outperforms state-of-the-art methods by up to 10\%. Furthermore, compared with standard complex-valued architectures, our real-imaginary shared-parameter design reduces the parameter count and computational cost by at least 50\%.

</details>


### [898] [PCA recovery thresholds in low-rank matrix inference with sparse noise](https://arxiv.org/abs/2511.11927)
*Urte Adomaityte,Gabriele Sicuro,Pierpaolo Vivo*

Main category: stat.ML

TL;DR: The paper analyzes rank-one signal inference in high dimensions under sparse noise, identifying critical transitions and providing both analytical and numerical investigations.


<details>
  <summary>Details</summary>
Motivation: To understand and generalize the behavior of rank-one signal inference amidst sparse noise, addressing a gap in extending dense noise scenarios to sparse ones.

Method: Replica method from statistical physics, distributional equations, population dynamics algorithm, and numerical diagonalization of matrices.

Result: Derived analytical expressions for key properties (e.g., top eigenvalue, component density, overlap), identified critical signal strength transitions, and verified findings with numerical experiments.

Conclusion: The research extends BBP transition analysis to sparse noise with analytical and numerical alignment, recovering results for dense noise in high-connectivity limits.

Abstract: We study the high-dimensional inference of a rank-one signal corrupted by sparse noise. The noise is modelled as the adjacency matrix of a weighted undirected graph with finite average connectivity in the large size limit. Using the replica method from statistical physics, we analytically compute the typical value of the top eigenvalue, the top eigenvector component density, and the overlap between the signal vector and the top eigenvector. The solution is given in terms of recursive distributional equations for auxiliary probability density functions which can be efficiently solved using a population dynamics algorithm. Specialising the noise matrix to Poissonian and Random Regular degree distributions, the critical signal strength is analytically identified at which a transition happens for the recovery of the signal via the top eigenvector, thus generalising the celebrated BBP transition to the sparse noise case. In the large-connectivity limit, known results for dense noise are recovered. Analytical results are in agreement with numerical diagonalisation of large matrices.

</details>


### [899] [Bayesian--AI Fusion for Epidemiological Decision Making: Calibrated Risk, Honest Uncertainty, and Hyperparameter Intelligence](https://arxiv.org/abs/2511.11983)
*Debashis Chatterjee*

Main category: stat.ML

TL;DR: The paper introduces a unified Bayesian and AI framework for accurate disease risk prediction and model optimization in epidemiological analytics.


<details>
  <summary>Details</summary>
Motivation: Machine learning methods in epidemiological analysis often lack calibrated uncertainty, necessitating approaches that integrate principled uncertainty quantification and model optimization.

Method: The framework combines Bayesian logistic regression for calibrated disease risk prediction and Gaussian-process Bayesian optimization for hyperparameter tuning of Cox survival models.

Result: Simulations show improved calibration, prediction accuracy, and optimization performance in both low and high-dimensional settings.

Conclusion: The Bayesian approach supports reliable uncertainty quantification and enhances predictive and model selection processes in epidemiological analytics.

Abstract: Modern epidemiological analytics increasingly use machine learning models that offer strong prediction but often lack calibrated uncertainty. Bayesian methods provide principled uncertainty quantification, yet are viewed as difficult to integrate with contemporary AI workflows. This paper proposes a unified Bayesian and AI framework that combines Bayesian prediction with Bayesian hyperparameter optimization.
  We use Bayesian logistic regression to obtain calibrated individual-level disease risk and credible intervals on the Pima Indians Diabetes dataset. In parallel, we use Gaussian-process Bayesian optimization to tune penalized Cox survival models on the GBSG2 breast cancer cohort. This yields a two-layer system: a Bayesian predictive layer that represents risk as a posterior distribution, and a Bayesian optimization layer that treats model selection as inference over a black-box objective.
  Simulation studies in low- and high-dimensional regimes show that the Bayesian layer provides reliable coverage and improved calibration, while Bayesian shrinkage improves AUC, Brier score, and log-loss. Bayesian optimization consistently pushes survival models toward near-oracle concordance. Overall, Bayesian reasoning enhances both what we infer and how we search, enabling calibrated risk and principled hyperparameter intelligence for epidemiological decision making.

</details>


### [900] [PCA++: How Uniformity Induces Robustness to Background Noise in Contrastive Learning](https://arxiv.org/abs/2511.12278)
*Mingqi Wu,Qiang Sun,Yi Yang*

Main category: stat.ML

TL;DR: This paper introduces PCA++, a contrastive PCA that better recovers low-dimensional signal subspaces in high-dimensional settings with structured noise, outperforming standard PCA and PCA+.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve signal recovery from high-dimensional data with structured background noise, where standard PCA fails, by leveraging contrastive learning approaches.

Method: The authors develop PCA++, a method using hard uniformity-constrained contrastive PCA. They enforce identity covariance on projected features to suppress background interference and derive a closed-form solution via a generalized eigenproblem.

Result: PCA++ demonstrates robust and optimal signal recovery across high-noise, high-dimensional simulations, corrupted-MNIST, and single-cell transcriptomics datasets, outperforming baseline PCA methods.

Conclusion: The study highlights the critical role of uniformity in robust contrastive learning, showing its value in mitigating structured noise and reliably recovering invariant signal structure.

Abstract: High-dimensional data often contain low-dimensional signals obscured by structured background noise, which limits the effectiveness of standard PCA. Motivated by contrastive learning, we address the problem of recovering shared signal subspaces from positive pairs, paired observations sharing the same signal but differing in background. Our baseline, PCA+, uses alignment-only contrastive learning and succeeds when background variation is mild, but fails under strong noise or high-dimensional regimes. To address this, we introduce PCA++, a hard uniformity-constrained contrastive PCA that enforces identity covariance on projected features. PCA++ has a closed-form solution via a generalized eigenproblem, remains stable in high dimensions, and provably regularizes against background interference. We provide exact high-dimensional asymptotics in both fixed-aspect-ratio and growing-spike regimes, showing uniformity's role in robust signal recovery. Empirically, PCA++ outperforms standard PCA and alignment-only PCA+ on simulations, corrupted-MNIST, and single-cell transcriptomics, reliably recovering condition-invariant structure. More broadly, we clarify uniformity's role in contrastive learning, showing that explicit feature dispersion defends against structured noise and enhances robustness.

</details>


### [901] [Accelerated Distributional Temporal Difference Learning with Linear Function Approximation](https://arxiv.org/abs/2511.12688)
*Kaicheng Jin,Yang Peng,Jiansheng Yang,Zhihua Zhang*

Main category: stat.ML

TL;DR: The paper explores the statistical efficiency of distributional reinforcement learning algorithms with linear function approximation, providing new results for finite-sample rates and improved bounds.


<details>
  <summary>Details</summary>
Motivation: The study aims to enhance understanding and provide theoretical insights into the efficiency of distributional temporal difference learning in estimating return distributions in Markov decision processes, moving beyond tabular case analyses.

Method: The authors conduct a fine-grained analysis of the linear-categorical Bellman equation, incorporate variance reduction techniques, and propose new algorithms to establish sample complexity bounds independent of support size.

Result: The paper finds that learning the full distribution is as feasible as learning the expectation using distributional TD learning with linear function approximation.

Conclusion: The findings suggest distributional reinforcement learning with linear function approximation achieves statistical efficiency and offers an effective approach to model return distributions in Markov decision processes.

Abstract: In this paper, we study the finite-sample statistical rates of distributional temporal difference (TD) learning with linear function approximation. The purpose of distributional TD learning is to estimate the return distribution of a discounted Markov decision process for a given policy. Previous works on statistical analysis of distributional TD learning focus mainly on the tabular case. We first consider the linear function approximation setting and conduct a fine-grained analysis of the linear-categorical Bellman equation. Building on this analysis, we further incorporate variance reduction techniques in our new algorithms to establish tight sample complexity bounds independent of the support size $K$ when $K$ is large. Our theoretical results imply that, when employing distributional TD learning with linear function approximation, learning the full distribution of the return function from streaming data is no more difficult than learning its expectation. This work provide new insights into the statistical efficiency of distributional reinforcement learning algorithms.

</details>


### [902] [TSB-HB: A Hierarchical Bayesian Extension of the TSB Model for Intermittent Demand Forecasting](https://arxiv.org/abs/2511.12749)
*Zong-Han Bai,Po-Yen Chu*

Main category: stat.ML

TL;DR: This paper proposes TSB-HB, a hierarchical Bayesian extension of the classical TSB method, to address challenges in intermittent demand forecasting using generative modeling.


<details>
  <summary>Details</summary>
Motivation: Intermittent demand forecasting is challenging due to sparse data, cold-start issues, and obsolescence. Existing models like Croston or TSB lack the benefits of a generative framework, while deep learning models often require large datasets and lack interpretability.

Method: The paper introduces TSB-HB, which models demand using Beta-Binomial and Log-Normal distributions with hierarchical Bayesian priors, enabling partial pooling to stabilize estimates for sparse or cold-start series while preserving heterogeneity.

Result: On the UCI Online Retail and M5 datasets, TSB-HB outperforms classical models (e.g., Croston, SBA, TSB) in RMSE and RMSSE, offering calibrated probabilistic forecasts and better accuracy for intermittent demands.

Conclusion: TSB-HB combines a principled generative formulation with hierarchical shrinkage to deliver interpretable, scalable, and accurate probabilistic forecasts for intermittent demand outperforming existing models.

Abstract: Intermittent demand forecasting poses unique challenges due to sparse observations, cold-start items, and obsolescence. Classical models such as Croston, SBA, and the Teunter-Syntetos-Babai (TSB) method provide simple heuristics but lack a principled generative foundation. Deep learning models address these limitations but often require large datasets and sacrifice interpretability.
  We introduce TSB-HB, a hierarchical Bayesian extension of TSB. Demand occurrence is modeled with a Beta-Binomial distribution, while nonzero demand sizes follow a Log-Normal distribution. Crucially, hierarchical priors enable partial pooling across items, stabilizing estimates for sparse or cold-start series while preserving heterogeneity. This framework yields a fully generative and interpretable model that generalizes classical exponential smoothing.
  On the UCI Online Retail dataset, TSB-HB achieves lower RMSE and RMSSE than Croston, SBA, TSB, ADIDA, IMAPA, ARIMA and Theta, and on a subset of the M5 dataset it outperforms all classical baselines we evaluate. The model provides calibrated probabilistic forecasts and improved accuracy on intermittent and lumpy items by combining a generative formulation with hierarchical shrinkage, while remaining interpretable and scalable.

</details>


### [903] [Function-on-Function Bayesian Optimization](https://arxiv.org/abs/2511.12783)
*Jingru Huang,Haijie Xu,Manrui Jiang,Chen Zhang*

Main category: stat.ML

TL;DR: This paper introduces a novel Bayesian optimization framework called Function-on-Function Bayesian Optimization (FFBO) designed for cases where both inputs and outputs are functions.


<details>
  <summary>Details</summary>
Motivation: Current Bayesian optimization methods fail to handle scenarios with both function-valued inputs and outputs, a situation increasingly common due to advancements in sensing technologies.

Method: The authors propose FFBO, which uses a Function-on-Function Gaussian Process (FFGP) with a separable operator-valued kernel to model correlations in function space. A scalar UCB acquisition function is defined with a functional gradient ascent algorithm to optimize inputs.

Result: Experiments on both synthetic and real-world data show that FFBO outperforms existing Bayesian optimization approaches.

Conclusion: The FFBO framework effectively addresses a gap in Bayesian optimization research, achieving better performance in handling function-valued inputs and outputs.

Abstract: Bayesian optimization (BO) has been widely used to optimize expensive and gradient-free objective functions across various domains. However, existing BO methods have not addressed the objective where both inputs and outputs are functions, which increasingly arise in complex systems as advanced sensing technologies. To fill this gap, we propose a novel function-on-function Bayesian optimization (FFBO) framework. Specifically, we first introduce a function-on-function Gaussian process (FFGP) model with a separable operator-valued kernel to capture the correlations between function-valued inputs and outputs. Compared to existing Gaussian process models, FFGP is modeled directly in the function space. Based on FFGP, we define a scalar upper confidence bound (UCB) acquisition function using a weighted operator-based scalarization strategy. Then, a scalable functional gradient ascent algorithm (FGA) is developed to efficiently identify the optimal function-valued input. We further analyze the theoretical properties of the proposed method. Extensive experiments on synthetic and real-world data demonstrate the superior performance of FFBO over existing approaches.

</details>


### [904] [Benign Overfitting in Linear Classifiers with a Bias Term](https://arxiv.org/abs/2511.12840)
*Yuta Kondo*

Main category: stat.ML

TL;DR: This paper extends the study of benign overfitting to linear classification models with a bias term, proving its persistence and analyzing the impact of covariance structure constraints.


<details>
  <summary>Details</summary>
Motivation: To address the gap in understanding benign overfitting in more realistic models by including the bias term, which is standard in practice.

Method: Extends prior work by Hashimoto et al. (2025) to analyze linear models that incorporate bias terms and studies the influence on generalization behavior under label noise and data covariance constraints.

Result: Shows that benign overfitting persists with bias terms in linear models but introduces additional covariance structure constraints, especially under noisy labels.

Conclusion: This work provides a deeper understanding of benign overfitting by highlighting the bias term's significant influence on the conditions necessary for good generalization.

Abstract: Modern machine learning models with a large number of parameters often generalize well despite perfectly interpolating noisy training data - a phenomenon known as benign overfitting. A foundational explanation for this in linear classification was recently provided by Hashimoto et al. (2025). However, this analysis was limited to the setting of "homogeneous" models, which lack a bias (intercept) term - a standard component in practice. This work directly extends Hashimoto et al.'s results to the more realistic inhomogeneous case, which incorporates a bias term. Our analysis proves that benign overfitting persists in these more complex models. We find that the presence of the bias term introduces new constraints on the data's covariance structure required for generalization, an effect that is particularly pronounced when label noise is present. However, we show that in the isotropic case, these new constraints are dominated by the requirements inherited from the homogeneous model. This work provides a more complete picture of benign overfitting, revealing the non-trivial impact of the bias term on the conditions required for good generalization.

</details>


### [905] [Reconstruction of Manifold Distances from Noisy Observations](https://arxiv.org/abs/2511.13025)
*Charles Fefferman,Jonathan Marty,Kevin Ren*

Main category: stat.ML

TL;DR: The paper addresses the recovery of intrinsic manifold geometry from noisy pairwise distances and develops novel methods with improved accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to reconstruct geodesic distances and the geometry of a manifold when observations are noisy, overcoming limitations of past methods that needed strong assumptions on noise distribution.

Method: Developing a framework based on estimating the $L_2$-norms of expectation functions to form robust clusters and designing two algorithms for distance recovery under mild assumptions.

Result: Achieved geodesic distance recovery with additive error $O(ε\log(1/ε))$. The first algorithm has a sample complexity of $N \asymp ε^{-2d-2}\log(1/ε)$ and runtime $o(N^3)$. The second explores novel geometric approaches.

Conclusion: The findings highlight key geometric properties necessary for accurate distance recovery, enabling future extensions to more general metric probability spaces.

Abstract: We consider the problem of reconstructing the intrinsic geometry of a manifold from noisy pairwise distance observations. Specifically, let $M$ denote a diameter 1 d-dimensional manifold and $μ$ a probability measure on $M$ that is mutually absolutely continuous with the volume measure. Suppose $X_1,\dots,X_N$ are i.i.d. samples of $μ$ and we observe noisy-distance random variables $d'(X_j, X_k)$ that are related to the true geodesic distances $d(X_j,X_k)$. With mild assumptions on the distributions and independence of the noisy distances, we develop a new framework for recovering all distances between points in a sufficiently dense subsample of $M$. Our framework improves on previous work which assumed i.i.d. additive noise with known moments. Our method is based on a new way to estimate $L_2$-norms of certain expectation-functions $f_x(y)=\mathbb{E}d'(x,y)$ and use them to build robust clusters centered at points of our sample. Using a new geometric argument, we establish that, under mild geometric assumptions--bounded curvature and positive injectivity radius--these clusters allow one to recover the true distances between points in the sample up to an additive error of $O(\varepsilon \log \varepsilon^{-1})$. We develop two distinct algorithms for producing these clusters. The first achieves a sample complexity $N \asymp \varepsilon^{-2d-2}\log(1/\varepsilon)$ and runtime $o(N^3)$. The second introduces novel geometric ideas that warrant further investigation. In the presence of missing observations, we show that a quantitative lower bound on sampling probabilities suffices to modify the cluster construction in the first algorithm and extend all recovery guarantees. Our main technical result also elucidates which properties of a manifold are necessary for the distance recovery, which suggests further extension of our techniques to a broader class of metric probability spaces.

</details>


### [906] [Likelihood-guided Regularization in Attention Based Models](https://arxiv.org/abs/2511.13221)
*Mohamed Salem,Inyoung Kim*

Main category: stat.ML

TL;DR: The paper presents a novel Bayesian regularization framework for Vision Transformers (ViTs) to enhance generalization and prune redundant parameters, showing improved performance on vision datasets.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of Vision Transformers (ViTs) that require large-scale data and depend heavily on regularization to avoid overfitting.

Method: The paper proposes a likelihood-guided, variational Ising-based regularization framework using Bayesian sparsification to impose structured sparsity and adaptively search architecture during training.

Result: The framework improves generalization for sparse and complex data, offers principled uncertainty quantification, and enhances probability calibration and feature selection.

Conclusion: The proposed method effectively enhances transformer-based models with structured Bayesian sparsification, providing an alternative to conventional regularization techniques.

Abstract: The transformer architecture has demonstrated strong performance in classification tasks involving structured and high-dimensional data. However, its success often hinges on large- scale training data and careful regularization to prevent overfitting. In this paper, we intro- duce a novel likelihood-guided variational Ising-based regularization framework for Vision Transformers (ViTs), which simultaneously enhances model generalization and dynamically prunes redundant parameters. The proposed variational Ising-based regularization approach leverages Bayesian sparsification techniques to impose structured sparsity on model weights, allowing for adaptive architecture search during training. Unlike traditional dropout-based methods, which enforce fixed sparsity patterns, the variational Ising-based regularization method learns task-adaptive regularization, improving both efficiency and interpretability. We evaluate our approach on benchmark vision datasets, including MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100, demonstrating improved generalization under sparse, complex data and allowing for principled uncertainty quantification on both weights and selection parameters. Additionally, we show that the Ising regularizer leads to better-calibrated probability estimates and structured feature selection through uncertainty-aware attention mechanisms. Our results highlight the effectiveness of structured Bayesian sparsification in enhancing transformer-based architectures, offering a principled alternative to standard regularization techniques.

</details>


### [907] [The Shape of Data: Topology Meets Analytics. A Practical Introduction to Topological Analytics and the Stability Index (TSI) in Business](https://arxiv.org/abs/2511.13503)
*Ioannis Diamantis*

Main category: stat.ML

TL;DR: This paper introduces Topological Data Analysis (TDA) as a geometric approach for analyzing nonlinear and multi-scale business and economic data, providing a practical pipeline with case studies.


<details>
  <summary>Details</summary>
Motivation: Traditional linear tools struggle to analyze nonlinear, multi-scale data structures relevant to business and economics, necessitating robust alternatives like TDA.

Method: The authors utilize persistent homology and a reproducible TDA pipeline to analyze patterns in case studies of consumer behavior, equity markets, and foreign exchange dynamics. They introduce the Topological Stability Index (TSI) for measuring structural variability.

Result: TDA uncovers segmentation patterns and structural relationships in the datasets, outperforming classical statistical methods. TSI provides an interpretable indicator of variability within the data.

Conclusion: TDA offers practical tools and insights for business and economic analysts, with guidelines for implementation and visualization emphasizing its advantages over traditional methods.

Abstract: Modern business and economic datasets often exhibit nonlinear, multi-scale structures that traditional linear tools under-represent. Topological Data Analysis (TDA) offers a geometric lens for uncovering robust patterns, such as connected components, loops and voids, across scales. This paper provides an intuitive, figure-driven introduction to persistent homology and a practical, reproducible TDA pipeline for applied analysts. Through comparative case studies in consumer behavior, equity markets (SAX/eSAX vs.\ TDA) and foreign exchange dynamics, we demonstrate how topological features can reveal segmentation patterns and structural relationships beyond classical statistical methods. We discuss methodological choices regarding distance metrics, complex construction and interpretation, and we introduce the \textit{Topological Stability Index} (TSI), a simple yet interpretable indicator of structural variability derived from persistence lifetimes. We conclude with practical guidelines for TDA implementation, visualization and communication in business and economic analytics.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [908] [Social and Physical Attributes-Defined Trust Evaluation for Effective Collaborator Selection in Human-Device Coexistence Systems](https://arxiv.org/abs/2511.11578)
*Botao Zhu,Xianbin Wang*

Main category: cs.HC

TL;DR: This paper introduces a self-supervised learning method (HSLCCA) using hypergraphs to evaluate trustworthiness in human-device systems by integrating physical and social attributes.


<details>
  <summary>Details</summary>
Motivation: Accurate trust evaluation in human-device systems is crucial due to the complex interplay of physical and social attributes. Efficiently addressing high heterogeneity is challenging.

Method: A hypergraph self-supervised learning (HSLCCA) approach is proposed. It constructs a relationship hypergraph integrating spatial, device, and social attributes, augmented into two views for enhanced semantic learning. CCA is applied to refine embeddings.

Result: Experiments show the HSLCCA method performs better than baseline methods in identifying trusted devices.

Conclusion: The proposed HSLCCA method effectively integrates multifaceted attributes, significantly improving device trust evaluation.

Abstract: In human-device coexistence systems, collaborations among devices are determined by not only physical attributes such as network topology but also social attributes among human users. Consequently, trust evaluation of potential collaborators based on these multifaceted attributes becomes critical for ensuring the eventual outcome. However, due to the high heterogeneity and complexity of physical and social attributes, efficiently integrating them for accurate trust evaluation remains challenging. To overcome this difficulty, a canonical correlation analysis-enhanced hypergraph self-supervised learning (HSLCCA) method is proposed in this research. First, by treating all attributes as relationships among connected devices, a relationship hypergraph is constructed to comprehensively capture inter-device relationships across three dimensions: spatial attribute-related, device attribute-related, and social attribute-related. Next, a self-supervised learning framework is developed to integrate these multi-dimensional relationships and generate device embeddings enriched with relational semantics. In this learning framework, the relationship hypergraph is augmented into two distinct views to enhance semantic information. A parameter-sharing hypergraph neural network is then utilized to learn device embeddings from both views. To further enhance embedding quality, a CCA approach is applied, allowing the comparison of data between the two views. Finally, the trustworthiness of devices is calculated based on the learned device embeddings. Extensive experiments demonstrate that the proposed HSLCCA method significantly outperforms the baseline algorithm in effectively identifying trusted devices.

</details>


### [909] [Agent-Oriented Visual Programming for the Web of Things](https://arxiv.org/abs/2511.13158)
*Samuele Burattini,Alessandro Ricci,Simon Mayer,Danai Vachtsevanou,Jeremy Lemee,Andrei Ciortea,Angelo Croatti*

Main category: cs.HC

TL;DR: This paper proposes a visual programming approach to enable non-programmers to design and configure autonomous multi-agent systems using agent abstractions and a belief-desire-intention model.


<details>
  <summary>Details</summary>
Motivation: The motivation is to simplify programming for individuals with no prior experience but domain-specific knowledge, by using human-aligned agent abstractions that are more intuitive than traditional procedural programming methods.

Method: The authors developed a blocks-based visual programming environment for agents built on the JaCaMo platform, integrating the Web of Things for creating autonomous behavior with physical device mashups.

Result: A pilot user study showed that novice users could successfully use the developed environment to create multi-agent systems for basic automation tasks.

Conclusion: The paper concludes that visual programming systems leveraging agent-based abstractions can simplify programming for novices and promote end-user programming trends.

Abstract: In this paper we introduce and discuss an approach for multi-agent-oriented visual programming. This aims at enabling individuals without programming experience but with knowledge in specific target domains to design and (re)configure autonomous software. We argue that, compared to procedural programming, it should be simpler for users to create programs when agent abstractions are employed. The underlying rationale is that these abstractions, and specifically the belief-desire-intention architecture that is aligned with human practical reasoning, match more closely with people's everyday experience in interacting with other agents and artifacts in the real world. On top of this, we designed and implemented a visual programming system for agents that hides the technicalities of agent-oriented programming using a blocks-based visual development environment that is built on the JaCaMo platform. To further validate the proposed solution, we integrate the Web of Things (WoT) to let users create autonomous behaviour on top of physical mashups of devices, following the trends in industrial end-user programming. Finally, we report on a pilot user study where we verified that novice users are indeed able to make use of this development environment to create multi-agent systems to solve simple automation tasks.

</details>


### [910] [MedBuild AI: An Agent-Based Hybrid Intelligence Framework for Reshaping Agency in Healthcare Infrastructure Planning through Generative Design for Medical Architecture](https://arxiv.org/abs/2511.11587)
*Yiming Zhang,Yuejia Xu,Ziyao Wang,Xin Yan,Xiaosai Hao*

Main category: cs.HC

TL;DR: The paper introduces MedBuild AI, a framework using AI and expert systems to provide modular low-cost hospital designs for regions lacking infrastructure.


<details>
  <summary>Details</summary>
Motivation: There is a global disparity in healthcare infrastructure, with many communities lacking access to medical services. Current infrastructure planning is slow and insufficient.

Method: MedBuild AI integrates large language models and deterministic expert systems, providing a web-based platform to create modular, low-tech healthcare building designs through three specialized agents.

Result: MedBuild AI offers local interactive tools for health data collection, architectural programming, and modular design generation, accessible through satellite internet.

Conclusion: MedBuild AI promotes equitable and inclusive healthcare planning globally, empowering underserved communities with accessible design solutions.

Abstract: Globally, disparities in healthcare infrastructure remain stark, leaving countless communities without access to even basic services. Traditional infrastructure planning is often slow and inaccessible, and although many architects are actively delivering humanitarian and aid-driven hospital projects worldwide, these vital efforts still fall far short of the sheer scale and urgency of demand. This paper introduces MedBuild AI, a hybrid-intelligence framework that integrates large language models (LLMs) with deterministic expert systems to rebalance the early design and conceptual planning stages. As a web-based platform, it enables any region with satellite internet access to obtain guidance on modular, low-tech, low-cost medical building designs. The system operates through three agents: the first gathers local health intelligence via conversational interaction; the second translates this input into an architectural functional program through rule-based computation; and the third generates layouts and 3D models. By embedding computational negotiation into the design process, MedBuild AI fosters a reciprocal, inclusive, and equitable approach to healthcare planning, empowering communities and redefining agency in global healthcare architecture.

</details>


### [911] [Accepted with Minor Revisions: Value of AI-Assisted Scientific Writing](https://arxiv.org/abs/2511.12529)
*Sanchaita Hazra,Doeun Lee,Bodhisattwa Prasad Majumder,Sachin Kumar*

Main category: cs.HC

TL;DR: The paper explores the effectiveness of Large Language Models (LLMs) in assisting domain experts with scientific abstract writing through a randomized controlled trial.


<details>
  <summary>Details</summary>
Motivation: To understand the potential of LLMs in aiding precise and expert-driven scientific writing, addressing concerns of quality and editing behaviors when using AI tools.

Method: An incentivized randomized controlled trial with a conference setup, using authors and reviewers to analyze the editing and perception differences between AI-generated and human-written abstracts.

Result: Authors tended to make more edits on human-written abstracts, and source disclosure influenced the volume of edits. Despite this, reviewer decisions were unaffected by source but linked to the number of edits. Careful revisions to AI-generated content could lead to acceptability comparable to human-written abstracts.

Conclusion: AI-generated abstracts have potential for comparable quality with minimal revision, but perceptions of AI authorship heavily influence editing behaviors. Transparency regarding the source is crucial in collaborative scientific writing.

Abstract: Large Language Models have seen expanding application across domains, yet their effectiveness as assistive tools for scientific writing -- an endeavor requiring precision, multimodal synthesis, and domain expertise -- remains insufficiently understood. We examine the potential of LLMs to support domain experts in scientific writing, with a focus on abstract composition. We design an incentivized randomized controlled trial with a hypothetical conference setup where participants with relevant expertise are split into an author and reviewer pool. Inspired by methods in behavioral science, our novel incentive structure encourages authors to edit the provided abstracts to an acceptable quality for a peer-reviewed submission. Our 2x2 between-subject design expands into two dimensions: the implicit source of the provided abstract and the disclosure of it. We find authors make most edits when editing human-written abstracts compared to AI-generated abstracts without source attribution, often guided by higher perceived readability in AI generation. Upon disclosure of source information, the volume of edits converges in both source treatments. Reviewer decisions remain unaffected by the source of the abstract, but bear a significant correlation with the number of edits made. Careful stylistic edits, especially in the case of AI-generated abstracts, in the presence of source information, improve the chance of acceptance. We find that AI-generated abstracts hold potential to reach comparable levels of acceptability to human-written ones with minimal revision, and that perceptions of AI authorship, rather than objective quality, drive much of the observed editing behavior. Our findings reverberate the significance of source disclosure in collaborative scientific writing.

</details>


### [912] [Maximizing the efficiency of human feedback in AI alignment: a comparative analysis](https://arxiv.org/abs/2511.12796)
*Andreas Chouliaras,Dimitris Chatzopoulos*

Main category: cs.HC

TL;DR: The paper explores efficient sampling and evaluation strategies for preference modeling in RLHF under constrained annotation budgets, proposing the Swiss InfoGain method that achieves better performance and sample efficiency compared to popular methods.


<details>
  <summary>Details</summary>
Motivation: Current RLHF methods, such as random pair sampling with Bradley-Terry modeling, are limited in statistical efficiency and do not effectively handle constrained annotation budgets.

Method: The authors introduce the Swiss InfoGain method, inspired by game theory and mutual-information pairing, to use adaptive, resource-aware strategies in preference inference, leveraging a Swiss tournament system.

Result: Swiss InfoGain significantly outperforms traditional random pair sampling methods, showing improved efficiency in preference learning by reducing redundancy and enhancing robustness, especially under constrained budgets.

Conclusion: Adaptive strategies like Swiss InfoGain improve the alignment process in RLHF by balancing preference inference quality with human workload, making them essential for optimizing annotation efforts in constrained or resource-heavy settings.

Abstract: Reinforcement Learning from Human Feedback (RLHF) relies on preference modeling to align machine learning systems with human values, yet the popular approach of random pair sampling with Bradley-Terry modeling is statistically limited and inefficient under constrained annotation budgets. In this work, we explore alternative sampling and evaluation strategies for preference inference in RLHF, drawing inspiration from areas such as game theory, statistics, and social choice theory. Our best-performing method, Swiss InfoGain, employs a Swiss tournament system with a proxy mutual-information-gain pairing rule, which significantly outperforms all other methods in constrained annotation budgets while also being more sample-efficient. Even in high-resource settings, we can identify superior alternatives to the Bradley-Terry baseline. Our experiments demonstrate that adaptive, resource-aware strategies reduce redundancy, enhance robustness, and yield statistically significant improvements in preference learning, highlighting the importance of balancing alignment quality with human workload in RLHF pipelines.

</details>


### [913] [Enhancing XR Auditory Realism via Multimodal Scene-Aware Acoustic Rendering](https://arxiv.org/abs/2511.11930)
*Tianyu Xu,Jihan Li,Penghe Zu,Pranav Sahay,Maruchi Kim,Jack Obeng-Marnu,Farley Miller,Xun Qian,Katrina Passarella,Mahitha Rachumalla,Rajeev Nongpiur,D. Shin*

Main category: cs.HC

TL;DR: The paper introduces SAMOSA, an on-device XR system for rendering spatially accurate sound by dynamically adapting to physical environments using advanced acoustic calibration.


<details>
  <summary>Details</summary>
Motivation: Current XR audio rendering methods struggle with real-time adaptation to diverse physical scenes, causing sensory mismatch and reduced immersion.

Method: SAMOSA combines multimodal scene representation using room geometry, surface materials, and semantic acoustic context to enable efficient acoustic calibration and realistic sound synthesis.

Result: Technical and expert evaluations validate SAMOSA’s ability to accurately synthesize Room Impulse Responses (RIRs) and enhance XR auditory realism.

Conclusion: SAMOSA is effective in improving XR audio rendering realism by integrating dynamic environmental adaptation and multimodal scene analysis.

Abstract: In Extended Reality (XR), rendering sound that accurately simulates real-world acoustics is pivotal in creating lifelike and believable virtual experiences. However, existing XR spatial audio rendering methods often struggle with real-time adaptation to diverse physical scenes, causing a sensory mismatch between visual and auditory cues that disrupts user immersion. To address this, we introduce SAMOSA, a novel on-device system that renders spatially accurate sound by dynamically adapting to its physical environment. SAMOSA leverages a synergistic multimodal scene representation by fusing real-time estimations of room geometry, surface materials, and semantic-driven acoustic context. This rich representation then enables efficient acoustic calibration via scene priors, allowing the system to synthesize a highly realistic Room Impulse Response (RIR). We validate our system through technical evaluation using acoustic metrics for RIR synthesis across various room configurations and sound types, alongside an expert evaluation (N=12). Evaluation results demonstrate SAMOSA's feasibility and efficacy in enhancing XR auditory realism.

</details>


### [914] [Multi-Domain EEG Representation Learning with Orthogonal Mapping and Attention-based Fusion for Cognitive Load Classification](https://arxiv.org/abs/2511.12394)
*Prithila Angkan,Amin Jalali,Paul Hungler,Ali Etemad*

Main category: cs.HC

TL;DR: The paper proposes a method using EEG data for cognitive load classification by combining time and frequency domain representations, with validation showing improved outcomes over traditional techniques.


<details>
  <summary>Details</summary>
Motivation: Existing methods for cognitive load classification are limited by focusing only on either time or frequency domain representations, and often struggle with effective discrimination between cognitive states.

Method: The method uses convolutional encoders for time-domain representation and multi-spectral topography maps for frequency-domain representation. A multi-domain attention module integrates the representations, along with an orthogonal projection constraint to enhance class discrimination.

Result: Extensive experiments validate the model's effectiveness on two EEG datasets (CL-Drive, CLARE), with superior performance over single-domain methods. The approach is robust even under added noise.

Conclusion: The proposed multi-domain representation learning significantly enhances cognitive load classification, achieving better robustness, inter-class distinction, and clustering than previous solutions.

Abstract: We propose a new representation learning solution for the classification of cognitive load based on Electroencephalogram (EEG). Our method integrates both time and frequency domains by first passing the raw EEG signals through the convolutional encoder to obtain the time domain representations. Next, we measure the Power Spectral Density (PSD) for all five EEG frequency bands and generate the channel power values as 2D images referred to as multi-spectral topography maps. These multi-spectral topography maps are then fed to a separate encoder to obtain the representations in frequency domain. Our solution employs a multi-domain attention module that maps these domain-specific embeddings onto a shared embedding space to emphasize more on important inter-domain relationships to enhance the representations for cognitive load classification. Additionally, we incorporate an orthogonal projection constraint during the training of our method to effectively increase the inter-class distances while improving intra-class clustering. This enhancement allows efficient discrimination between different cognitive states and aids in better grouping of similar states within the feature space. We validate the effectiveness of our model through extensive experiments on two public EEG datasets, CL-Drive and CLARE for cognitive load classification. Our results demonstrate the superiority of our multi-domain approach over the traditional single-domain techniques. Moreover, we conduct ablation and sensitivity analyses to assess the impact of various components of our method. Finally, robustness experiments on different amounts of added noise demonstrate the stability of our method compared to other state-of-the-art solutions.

</details>


### [915] [Trust in Vision-Language Models: Insights from a Participatory User Workshop](https://arxiv.org/abs/2511.13458)
*Agnese Chiatti,Lara Piccolo,Sara Bernardini,Matteo Matteucci,Viola Schiaffonati*

Main category: cs.HC

TL;DR: This paper investigates user trust in Vision-Language Models (VLMs), focusing on the need for contextualized trust metrics and user-centered engagement strategies.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges of understanding user trust in Vision-Language Models, considering the increasing use of these systems and reliance on AI for experimental validation.

Method: A pilot workshop with prospective Vision-Language Model (VLM) users was conducted, adopting a user-centered approach to gather insights into trust metrics and engagement strategies.

Result: Preliminary findings provide insights into how trust in VLMs can be contextualized and shaped through user-VLM interaction.

Conclusion: The study emphasizes the importance of designing trust metrics and engagement strategies tailored to user interactions with Vision-Language Models for better understanding and reliable deployment.

Abstract: With the growing deployment of Vision-Language Models (VLMs), pre-trained on large image-text and video-text datasets, it is critical to equip users with the tools to discern when to trust these systems. However, examining how user trust in VLMs builds and evolves remains an open problem. This problem is exacerbated by the increasing reliance on AI models as judges for experimental validation, to bypass the cost and implications of running participatory design studies directly with users. Following a user-centred approach, this paper presents preliminary results from a workshop with prospective VLM users. Insights from this pilot workshop inform future studies aimed at contextualising trust metrics and strategies for participants' engagement to fit the case of user-VLM interaction.

</details>


### [916] [The Quick Red Fox gets the best Data Driven Classroom Interviews: A manual for an interview app and its associated methodology](https://arxiv.org/abs/2511.13466)
*Jaclyn Ocumpaugh,Luc Paquette,Ryan S. Baker,Amanda Barany,Jeff Ginger,Nathan Casano,Andres F. Zambrano,Xiner Liu,Zhanlan Wei,Yiqui Zhou,Qianhui Liu,Stephen Hutt,Alexandra M. A. Andres,Nidhi Nasiar,Camille Giordano,Martin van Velsen,Micheal Mogessi*

Main category: cs.HC

TL;DR: The paper introduces Data Driven Classroom Interviews (DDCIs), a method to enhance research on digital learning environments by conducting short and targeted interviews facilitated by an open-source app called Quick Red Fox (QRF).


<details>
  <summary>Details</summary>
Motivation: To enable researchers to better understand and contextualize students' interactions in digital learning environments without significantly interrupting their educational experience.

Method: DDCIs are implemented through the Quick Red Fox (QRF) app, which integrates with existing student modeling technologies to monitor behaviors and alert researchers to critical moments for interviews.

Result: The QRF app enhances researcher efficiency by optimizing the interview process to focus on key learning moments, helping to contextualize student digital interactions.

Conclusion: DDCIs, supported by the QRF tool, provide a practical and efficient way for researchers to gain insights into student learning behavior while minimizing disruptions, showcasing the potential for technology-driven educational research.

Abstract: Data Driven Classroom Interviews (DDCIs) are an interviewing technique that is facilitated by recent technological developments in the learning analytics community. DDCIs are short, targeted interviews that allow researchers to contextualize students' interactions with a digital learning environment (e.g., intelligent tutoring systems or educational games) while minimizing the amount of time that the researcher interrupts that learning experience, and focusing researcher time on the events they most want to focus on DDCIs are facilitated by a research tool called the Quick Red Fox (QRF)--an open-source server-client Android app that optimizes researcher time by directing interviewers to users that have just displayed an interesting behavior (previously defined by the research team). QRF integrates with existing student modeling technologies (e.g., behavior-sensing, affect-sensing, detection of self-regulated learning) to alert researchers to key moments in a learner's experience. This manual documents the tech while providing training on the processes involved in developing triggers and interview techniques; it also suggests methods of analyses.

</details>


### [917] [A Lexical Analysis of online Reviews on Human-AI Interactions](https://arxiv.org/abs/2511.13480)
*Parisa Arbab,Xiaowen Fang*

Main category: cs.HC

TL;DR: The study analyzes 55,968 user reviews to understand human-AI interaction and identifies key factors affecting these dynamics.


<details>
  <summary>Details</summary>
Motivation: To address gaps in understanding specific user concerns and challenges in human-AI interaction.

Method: Lexical analysis and factor analysis of 55,968 online reviews from platforms like G2.com, Producthunt.com, and Trustpilot.com.

Result: Initial factor analysis reveals key factors influencing human-AI interactions.

Conclusion: Insights derived aim to contribute to user-centric AI development and improved user experience.

Abstract: This study focuses on understanding the complex dynamics between humans and AI systems by analyzing user reviews. While previous research has explored various aspects of human-AI interaction, such as user perceptions and ethical considerations, there remains a gap in understanding the specific concerns and challenges users face. By using a lexical approach to analyze 55,968 online reviews from G2.com, Producthunt.com, and Trustpilot.com, this preliminary research aims to analyze human-AI interaction. Initial results from factor analysis reveal key factors influencing these interactions. The study aims to provide deeper insights into these factors through content analysis, contributing to the development of more user-centric AI systems. The findings are expected to enhance our understanding of human-AI interaction and inform future AI technology and user experience improvements.

</details>


### [918] [Person-AI Bidirectional Fit - A Proof-Of-Concept Case Study Of Augmented Human-Ai Symbiosis In Management Decision-Making Process](https://arxiv.org/abs/2511.13670)
*Agnieszka Bieńkowska,Jacek Małecki,Alexander Mathiesen-Ohman,Katarzyna Tworek*

Main category: cs.HC

TL;DR: The study introduces and verifies the Person-AI bidirectional fit, exploring its role in decision-making via a case study using a symbiotic AI system in hiring, achieving high alignment in certain scenarios.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to align human and AI decision-making processes to enhance accuracy, trust, and contextual sensitivity in managerial decisions.

Method: The research employs a case study approach, analyzing three decision pathways, including evaluations from humans, an augmented AI-human system, and a general-purpose language model, during a hiring process.

Result: The results show significant variance in human judgments, high P-AI fit between the symbiotic system and a CEO, and highlight advantages of augmented intelligence for trustworthy decision-making.

Conclusion: The study confirms the importance of P-AI fit in creating effective human-AI collaboration and demonstrates the potential of augmented systems like H3LIX-LAIZA in improving decision accuracy.

Abstract: This article develops the concept of Person-AI bidirectional fit, defined as the continuously evolving, context-sensitive alignment-primarily cognitive, but also emotional and behavioral-between a human decision-maker and an artificial intelligence system. Grounded in contingency theory and quality theory, the study examines the role of P-AI fit in managerial decision-making through a proof-of-concept case study involving a real hiring process for a Senior AI Lead. Three decision pathways are compared: (1) independent evaluations by a CEO, CTO, and CSO; (2) an evaluation produced by an augmented human-AI symbiotic intelligence system (H3LIX-LAIZA); and (3) an assessment generated by a general-purpose large language model. The results reveal substantial role-based divergence in human judgments, high alignment between H3LIX-LAIZA and the CEOs implicit decision model-including ethical disqualification of a high-risk candidate and a critical false-positive recommendation from the LLMr. The findings demonstrate that higher P-AI fit, exemplified by the CEO H3LIX-LAIZA relationship, functions as a mechanism linking augmented symbiotic intelligence to accurate, trustworthy, and context-sensitive decisions. The study provides an initial verification of the P-AI fit construct and a proof-of-concept for H3LIX-LAIZA as an augmented human-AI symbiotic intelligence system.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [919] [On the Excitability of Ultra-Low-Power CMOS Analog Spiking Neurons](https://arxiv.org/abs/2511.12753)
*Léopold Van Brandt,Grégoire Brandsteert,Denis Flandre*

Main category: cond-mat.stat-mech

TL;DR: The paper analyzes excitability in analog spiking neurons, focusing on excitation criteria such as membrane potential threshold.


<details>
  <summary>Details</summary>
Motivation: To investigate the excitability property of spiking neurons, which is essential for real-time event-based neuromorphic computing.

Method: The researchers used SPICE simulations and nonlinear dynamics analysis to establish excitation thresholds and study neuron behavior.

Result: Critical excitation criteria were identified, with membrane potential thresholds found to be intrinsic to the neuron and independent of the input stimulus.

Conclusion: The findings provide insight into neuron dynamics and highlight areas for further exploration, including intrinsic noise effects.

Abstract: The excitability property of spiking neurons describes their capability to output an action potential as a real-time response to an input synaptic excitation current and is central to the event-based neuromorphic computing paradigm. The spiking mechanism is analysed in a representative ultra-low-power analog neuron from the circuit literature. Relying on conventional SPICE simulations compatible with industrial transistor compact models, we establish a excitation criterion, quantified either in terms of critical supplied charge or membrane potential threshold. Only the latter is found intrinsic to the neuron, i.e. independent of the input stimulus. Rigorous analysis of the nonlinear neuron dynamics provides insight but still needs to be explored further, as well as the effect of the intrinsic noise.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [920] [Lightweight Hopfield Neural Networks for Bioacoustic Detection and Call Monitoring of Captive Primates](https://arxiv.org/abs/2511.11615)
*Wendy Lomas,Andrew Gascoyne,Colin Dubreuil,Stefano Vaglio,Liam Naughton*

Main category: cs.SD

TL;DR: Passive acoustic monitoring generates large datasets that require efficient processing. This paper introduces a lightweight associative memory AI model using Hopfield neural networks to detect endangered lemur vocalizations with high accuracy (0.94), broad applicability, and rapid processing capabilities.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in processing large acoustic datasets for environmental and wildlife monitoring, particularly focusing on automating this using sustainable and efficient AI models without relying on resource-heavy methods.

Method: The paper utilizes a Hopfield neural network architecture adapted to detect specific lemur vocalizations. Call signals are stored in the network to facilitate quick and accurate detection, with enhancements made by incorporating movement-related signals.

Result: The model achieves an accuracy of 0.94 and processes audio at a rapid rate of 340 classifications per second, equivalent to over 5.5 hours of audio data per minute on a standard laptop.

Conclusion: This lightweight and transparent AI model provides a sustainable solution for acoustic monitoring. It accelerates data-to-insight turnover and supports decision-making in wildlife and captive settings.

Abstract: Passive acoustic monitoring is a sustainable method of monitoring wildlife and environments that leads to the generation of large datasets and, currently, a processing backlog. Academic research into automating this process is focused on the application of resource intensive convolutional neural networks which require large pre-labelled datasets for training and lack flexibility in application. We present a viable alternative relevant in both wild and captive settings; a transparent, lightweight and fast-to-train associative memory AI model with Hopfield neural network (HNN) architecture. Adapted from a model developed to detect bat echolocation calls, this model monitors captive endangered black-and-white ruffed lemur Varecia variegata vocalisations. Lemur social calls of interest when monitoring welfare are stored in the HNN in order to detect other call instances across the larger acoustic dataset. We make significant model improvements by storing an additional signal caused by movement and achieve an overall accuracy of 0.94. The model can perform $340$ classifications per second, processing over 5.5 hours of audio data per minute, on a standard laptop running other applications. It has broad applicability and trains in milliseconds. Our lightweight solution reduces data-to-insight turnaround times and can accelerate decision making in both captive and wild settings.

</details>


### [921] [Real-Time Speech Enhancement via a Hybrid ViT: A Dual-Input Acoustic-Image Feature Fusion](https://arxiv.org/abs/2511.11825)
*Behnaz Bahmei,Siamak Arzanpour,Elina Birmingham*

Main category: cs.SD

TL;DR: The paper introduces a transformer-based framework for real-time noise suppression in speech, addressing challenges with non-stationary noise.


<details>
  <summary>Details</summary>
Motivation: The research was driven by the need to enhance speech quality and intelligibility in noisy environments, particularly addressing the shortcomings of existing methods with non-stationary noises.

Method: A hybrid ViT-based framework is proposed, utilizing dual-input acoustic-image feature fusion to model both temporal and spectral dependencies effectively.

Result: Experimental results show significant improvements in noise reduction, speech intelligibility, and quality metrics compared to noisy inputs, using multiple datasets for evaluation.

Conclusion: The framework demonstrates strong performance close to clean reference signals and is computationally efficient, making it suitable for deployment on embedded devices in real-world environments.

Abstract: Speech quality and intelligibility are significantly degraded in noisy environments. This paper presents a novel transformer-based learning framework to address the single-channel noise suppression problem for real-time applications. Although existing deep learning networks have shown remarkable improvements in handling stationary noise, their performance often diminishes in real-world environments characterized by non-stationary noise (e.g., dog barking, baby crying). The proposed dual-input acoustic-image feature fusion using a hybrid ViT framework effectively models both temporal and spectral dependencies in noisy signals. Designed for real-world audio environments, the proposed framework is computationally lightweight and suitable for implementation on embedded devices. To evaluate its effectiveness, four standard and commonly used quality measurements, namely PESQ, STOI, Seg SNR, and LLR, are utilized. Experimental results obtained using the Librispeech dataset as the clean speech source and the UrbanSound8K and Google Audioset datasets as the noise sources, demonstrate that the proposed method significantly improves noise reduction, speech intelligibility, and perceptual quality compared to the noisy input signal, achieving performance close to the clean reference.

</details>


### [922] [MF-Speech: Achieving Fine-Grained and Compositional Control in Speech Generation via Factor Disentanglement](https://arxiv.org/abs/2511.12074)
*Xinyue Yu,Youqing Fang,Pingyu Wu,Guoyang Ye,Wenbo Zhou,Weiming Zhang,Song Xiao*

Main category: cs.SD

TL;DR: The paper introduces MF-Speech, a framework designed to generate human speech with precise and independent control over speech content, timbre, and emotion, outperforming existing methods in quality and controllability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges of deep entanglement in speech factors and the lack of fine-grained control mechanisms in existing speech generation methods.

Method: MF-Speech comprises two components: MF-SpeechEncoder, which purifies and separates speech representations using multi-objective optimization, and MF-SpeechGenerator, which enables detailed control over these factors through dynamic fusion and Hierarchical Style Adaptive Normalization (HSAN).

Result: Experiments show significant improvements over the state-of-the-art, including lower word error rate (WER=4.67%), improved stylistic control metrics (SECS=0.5685, Corr=0.68), and higher subjective evaluation scores.

Conclusion: The study demonstrates that MF-Speech not only achieves superior performance in multi-factor compositional speech generation but also establishes a robust and transferable speech representation framework.

Abstract: Generating expressive and controllable human speech is one of the core goals of generative artificial intelligence, but its progress has long been constrained by two fundamental challenges: the deep entanglement of speech factors and the coarse granularity of existing control mechanisms. To overcome these challenges, we have proposed a novel framework called MF-Speech, which consists of two core components: MF-SpeechEncoder and MF-SpeechGenerator. MF-SpeechEncoder acts as a factor purifier, adopting a multi-objective optimization strategy to decompose the original speech signal into highly pure and independent representations of content, timbre, and emotion. Subsequently, MF-SpeechGenerator functions as a conductor, achieving precise, composable and fine-grained control over these factors through dynamic fusion and Hierarchical Style Adaptive Normalization (HSAN). Experiments demonstrate that in the highly challenging multi-factor compositional speech generation task, MF-Speech significantly outperforms current state-of-the-art methods, achieving a lower word error rate (WER=4.67%), superior style control (SECS=0.5685, Corr=0.68), and the highest subjective evaluation scores(nMOS=3.96, sMOS_emotion=3.86, sMOS_style=3.78). Furthermore, the learned discrete factors exhibit strong transferability, demonstrating their significant potential as a general-purpose speech representation.

</details>


### [923] [FoleyBench: A Benchmark For Video-to-Audio Models](https://arxiv.org/abs/2511.13219)
*Satvik Dixit,Koichi Saito,Zhi Zhong,Yuki Mitsufuji,Chris Donahue*

Main category: cs.SD

TL;DR: The paper introduces FoleyBench, a benchmark tailored for evaluating video-to-audio generation models specifically for Foley sound scenarios.


<details>
  <summary>Details</summary>
Motivation: Traditional evaluation datasets for video-to-audio generation lack effective audio-visual correspondence and focus on speech/music instead of Foley-style sound effects needed for synchronized sounds tied to screen actions.

Method: The authors created FoleyBench, a benchmark dataset with 5,000 triplets of video, ground-truth audio, and text captions, sourced from internet videos. It ensures causal link between audio and screen visuals, includes metadata, and a sound category taxonomy for Foley-style evaluation.

Result: FoleyBench demonstrates enhanced coverage of Foley-related sound categories compared to previous datasets. It provides fine-grained analysis tools such as complexity and synchronization attributes for model benchmarking.

Conclusion: FoleyBench fills the evaluation gap for Foley-style V2A methods and facilitates effective performance testing across multiple dimensions like alignment and quality on relevant sound categories.

Abstract: Video-to-audio generation (V2A) is of increasing importance in domains such as film post-production, AR/VR, and sound design, particularly for the creation of Foley sound effects synchronized with on-screen actions. Foley requires generating audio that is both semantically aligned with visible events and temporally aligned with their timing. Yet, there is a mismatch between evaluation and downstream applications due to the absence of a benchmark tailored to Foley-style scenarios. We find that 74% of videos from past evaluation datasets have poor audio-visual correspondence. Moreover, they are dominated by speech and music, domains that lie outside the use case for Foley. To address this gap, we introduce FoleyBench, the first large-scale benchmark explicitly designed for Foley-style V2A evaluation. FoleyBench contains 5,000 (video, ground-truth audio, text caption) triplets, each featuring visible sound sources with audio causally tied to on-screen events. The dataset is built using an automated, scalable pipeline applied to in-the-wild internet videos from YouTube-based and Vimeo-based sources. Compared to past datasets, we show that videos from FoleyBench have stronger coverage of sound categories from a taxonomy specifically designed for Foley sound. Each clip is further labeled with metadata capturing source complexity, UCS/AudioSet category, and video length, enabling fine-grained analysis of model performance and failure modes. We benchmark several state-of-the-art V2A models, evaluating them on audio quality, audio-video alignment, temporal synchronization, and audio-text consistency. Samples are available at: https://gclef-cmu.org/foleybench

</details>


### [924] [Spatial Blind Spot: Auditory Motion Perception Deficits in Audio LLMs](https://arxiv.org/abs/2511.13273)
*Zhe Sun,Yujun Cai,Jiayu Yao,Yiwei Wang*

Main category: cs.SD

TL;DR: Current Large Audio-Language Models (LALMs) interpret speech and sound tasks effectively but fail to perceive motion and spatial dynamics from binaural audio, as shown in the AMPBench benchmark.


<details>
  <summary>Details</summary>
Motivation: The paper aims to uncover and address the gap in motion perception within LALMs, which limits their auditory spatial reasoning applications.

Method: The study introduces AMPBench, a benchmark designed to evaluate motion perception by measuring the ability of LALMs to identify direction and trajectory of sound sources from binaural audio.

Result: Quantitative and qualitative assessments reveal that current models achieve less than 50% accuracy in recognizing motion patterns, showcasing their deficiencies.

Conclusion: There is a significant disparity between human and model auditory spatial reasoning, and AMPBench provides insights and tools to enhance motion perception in future LALMs.

Abstract: Large Audio-Language Models (LALMs) have recently shown impressive progress in speech recognition, audio captioning, and auditory question answering. Yet, whether these models can perceive spatial dynamics, particularly the motion of sound sources, remains unclear. In this work, we uncover a systematic motion perception deficit in current ALLMs. To investigate this issue, we introduce AMPBench, the first benchmark explicitly designed to evaluate auditory motion understanding. AMPBench introduces a controlled question-answering benchmark designed to evaluate whether Audio-Language Models (LALMs) can infer the direction and trajectory of moving sound sources from binaural audio. Comprehensive quantitative and qualitative analyses reveal that current models struggle to reliably recognize motion cues or distinguish directional patterns. The average accuracy remains below 50%, underscoring a fundamental limitation in auditory spatial reasoning. Our study highlights a fundamental gap between human and model auditory spatial reasoning, providing both a diagnostic tool and new insight for enhancing spatial cognition in future Audio-Language Models.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [925] [Asymptotic confidence bands for centered purely random forests](https://arxiv.org/abs/2511.13199)
*Natalie Neumeyer,Jan Rabe,Mathias Trabs*

Main category: math.ST

TL;DR: The paper introduces efficient confidence bands for purely random forests in nonparametric regression, achieving optimal minimax rates using Ehrenfest-centered forests.


<details>
  <summary>Details</summary>
Motivation: To address the suboptimal performance of uniformly centered purely random forests and improve upon existing methods for constructing confidence bands in multivariate nonparametric regression.

Method: The authors develop a new type of purely random forests called 'Ehrenfest centered purely random forests,' leveraging their interpretation as generalized U-Statistics, and use Gaussian approximation for the supremum of empirical processes to construct the confidence bands.

Result: Their approach applies to both uniformly and Ehrenfest centered random forests and achieves minimax optimal rates, validated through theoretical analysis and simulations.

Conclusion: Ehrenfest centered purely random forests overcome limitations of uniformly centered forests, providing a robust method for constructing optimal confidence bands in nonparametric regression settings.

Abstract: In a multivariate nonparametric regression setting we construct explicit asymptotic uniform confidence bands for centered purely random forests. Since the most popular example in this class of random forests, namely the uniformly centered purely random forests, is well known to suffer from suboptimal rates, we propose a new type of purely random forests, called the Ehrenfest centered purely random forests, which achieve minimax optimal rates. Our main confidence band theorem applies to both random forests. The proof is based on an interpretation of random forests as generalized U-Statistics together with a Gaussian approximation of the supremum of empirical processes. Our theoretical findings are illustrated in simulation examples.

</details>


### [926] [Nonparametric Estimation of Joint Entropy through Partitioned Sample-Spacing Method](https://arxiv.org/abs/2511.13602)
*Jungwoo Ho,Sangun Park,Soyeong Oh*

Main category: math.ST

TL;DR: This paper introduces a novel nonparametric method for estimating multivariate joint entropy using partitioned sample spacings (PSS), showing strong consistency and competitive performance.


<details>
  <summary>Details</summary>
Motivation: To address limitations of existing entropy estimation methods, particularly their performance and scalability in multivariate data scenarios.

Method: The authors propose using partitioned sample spacings, which partition multivariate sample spaces into localized cells and aggregate within-cell statistics to estimate joint entropy.

Result: The proposed PSS estimator outperforms k-nearest neighbor methods, competes with normalizing flow-based approaches, requires no additional training, and is scalable to moderately high dimensions (10 to 40).

Conclusion: PSS is a practical and effective alternative for multivariate entropy estimation, especially in machine learning tasks involving information theory, as it is robust to challenging distributions and efficient to implement.

Abstract: We propose a nonparametric estimator of multivariate joint entropy based on partitioned sample spacings (PSS). The method extends univariate spacing ideas to multivariate settings by partitioning the sample space into localized cells and aggregating within-cell statistics, with strong consistency guarantees under mild conditions. In benchmarks across diverse distributions, PSS consistently outperforms k-nearest neighbor estimators and achieves accuracy competitive with recent normalizing flow-based methods, while requiring no training or auxiliary density modeling. The estimator scales favorably in moderately high dimensions (d = 10 to 40) and shows particular robustness to correlated or skewed distributions. These properties position PSS as a practical alternative to normalizing flow-based approaches, with broad potential in information-theoretic machine learning applications.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [927] [Optimal Multi-Constrained Workflow Scheduling for Cyber-Physical Systems in the Edge-Cloud Continuum](https://arxiv.org/abs/2511.07466)
*Andreas Kouloumpris,Georgios L. Stavrinides,Maria K. Michael,Theocharis Theocharides*

Main category: cs.NI

TL;DR: The paper proposes an optimal scheduling approach for improving latency in edge-hub-cloud cyber-physical systems, demonstrating superior performance over existing heuristics.


<details>
  <summary>Details</summary>
Motivation: The heterogeneity and limitations of edge devices in computation, communication, energy, and sensing/actuating capabilities pose challenges in the edge-hub-cloud paradigm, necessitating effective solutions to optimize application workflows.

Method: The approach uses continuous-time mixed integer linear programming to schedule workflows optimally across heterogeneous edge, hub, and cloud systems, incorporating previously overlooked constraints.

Result: The proposed method achieves an average latency reduction of 13.54% in real-world cases and 33.03% in synthetic workflows, outperforming an enhanced heuristic model.

Conclusion: The paper establishes the efficacy and scalability of the proposed scheduling approach for reducing latency in diverse edge-hub-cloud cyber-physical systems.

Abstract: The emerging edge-hub-cloud paradigm has enabled the development of innovative latency-critical cyber-physical applications in the edge-cloud continuum. However, this paradigm poses multiple challenges due to the heterogeneity of the devices at the edge of the network, their limited computational, communication, and energy capacities, as well as their different sensing and actuating capabilities. To address these issues, we propose an optimal scheduling approach to minimize the overall latency of a workflow application in an edge-hub-cloud cyber-physical system. We consider multiple edge devices cooperating with a hub device and a cloud server. All devices feature heterogeneous multicore processors and various sensing, actuating, or other specialized capabilities. We present a comprehensive formulation based on continuous-time mixed integer linear programming, encapsulating multiple constraints often overlooked by existing approaches. We conduct a comparative experimental evaluation between our method and a well-established and effective scheduling heuristic, which we enhanced to consider the constraints of the specific problem. The results reveal that our technique outperforms the heuristic, achieving an average latency improvement of 13.54% in a relevant real-world use case, under varied system configurations. In addition, the results demonstrate the scalability of our method under synthetic workflows of varying sizes, attaining a 33.03% average latency decrease compared to the heuristic.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [928] [Improving Neutrino Oscillation Measurements through Event Classification](https://arxiv.org/abs/2511.11938)
*Sebastian A. R. Ellis,Daniel C. Hackett,Shirley Weishi Li,Pedro A. N. Machado,Karla Tame-Narvaez*

Main category: hep-ph

TL;DR: The study introduces a machine-learning-based technique to classify neutrino interaction types, enhancing neutrino energy reconstruction accuracy for future experiments like DUNE.


<details>
  <summary>Details</summary>
Motivation: Current neutrino energy reconstruction methods have large uncertainties due to limited modeling of neutrino-nucleus interactions, and standard approaches overlook variations in missing energy among interaction types.

Method: Machine-learning techniques classify neutrino events based on their interaction type (e.g., quasi-elastic scattering, resonance production) before reconstructing energy.

Result: The approach shows improved neutrino energy reconstruction accuracy and greater sensitivity in simulated DUNE $ν_μ$ disappearance analyses, demonstrating its robustness against mismodeling.

Conclusion: Integrating interaction type classification into reconstruction processes significantly improves precision and reduces systematics in next-generation experiments.

Abstract: Precise neutrino energy reconstruction is essential for next-generation long-baseline oscillation experiments, yet current methods remain limited by large uncertainties in neutrino-nucleus interaction modeling. Even so, it is well established that different interaction channels produce systematically varying amounts of missing energy and therefore yield different reconstruction performance--information that standard calorimetric approaches do not exploit. We introduce a strategy that incorporates this structure by classifying events according to their underlying interaction type prior to energy reconstruction. Using supervised machine-learning techniques trained on labeled generator events, we leverage intrinsic kinematic differences among quasi-elastic scattering, meson-exchange current, resonance production, and deep-inelastic scattering processes. A cross-generator testing framework demonstrates that this classification approach is robust to microphysics mismodeling and, when applied to a simulated DUNE $ν_μ$ disappearance analysis, yields improved accuracy and sensitivity. These results highlight a practical path toward reducing reconstruction-driven systematics in future oscillation measurements.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [929] [Asymptotic analysis of cooperative censoring policies in sensor networks](https://arxiv.org/abs/2511.13492)
*Jesus Fernandez-Bes,Rocío Arroyo-Valles,Jesús Cid-Sueiro*

Main category: cs.MA

TL;DR: This paper explores cooperative data censoring in multihop sensor networks to save energy by censoring less important messages using a theoretically optimal Markov Decision Process approach, proposing an approximated solution and demonstrating its superiority in efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address energy-saving challenges in battery-powered multihop sensor networks by prioritizing message importance and censoring less important data for efficient communication.

Method: The problem is modeled using a Markov Decision Process to derive theoretically optimal censoring policies. An approximated solution is proposed through constant-threshold rules, accompanied by a centralized algorithm to compute these thresholds.

Result: Experimental simulations confirm that cooperative censoring policies are energy-efficient and outperform non-cooperative schemes.

Conclusion: The study concludes that cooperative censoring policies are feasible, offer significant energy-saving benefits, and outperform traditional non-cooperative methods.

Abstract: The problem of cooperative data censoring in battery-powered multihop sensor networks is analyzed in this paper. We are interested in scenarios where nodes generate messages (which are related to the sensor measurements) that can be graded with some importance value. Less important messages can be censored in order to save energy for later communications. The problem is modeled using a joint Markov Decision Process of the whole network dynamics, and a theoretically optimal censoring policy, which maximizes a long-term reward, is found. Though the optimal censoring rules are computationally prohibitive, our analysis suggests that, under some conditions, they can be approximated by a finite collection of constant-threshold rules. A centralized algorithm for the computation of these thresholds is proposed. The experimental simulations show that cooperative censoring policies are energy-efficient, and outperform other non-cooperative schemes.

</details>


### [930] [MALBO: Optimizing LLM-Based Multi-Agent Teams via Multi-Objective Bayesian Optimization](https://arxiv.org/abs/2511.11788)
*Antonio Sabbatella*

Main category: cs.MA

TL;DR: The paper presents MALBO, a systematic framework using Bayesian Optimization to efficiently assign Large Language Models (LLMs) to roles in multi-agent systems, optimizing for a balance between task accuracy and cost.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of assigning LLMs to roles in multi-agent systems by overcoming issues such as vast search space, expensive evaluations, and balancing performance with cost.

Method: The method involves a multi-objective Bayesian Optimization strategy using independent Gaussian Process surrogate models to identify Pareto-optimal configurations, based on task accuracy and inference cost.

Result: The results show MALBO reduces configuration costs by up to 65.8% while maintaining performance, with the Bayesian phase achieving significant cost savings over a random search baseline.

Conclusion: MALBO offers a principled approach to create cost-effective and specialized multi-agent AI systems by optimizing LLM assignments with a data-driven framework.

Abstract: The optimal assignment of Large Language Models (LLMs) to specialized roles in multi-agent systems is a significant challenge, defined by a vast combinatorial search space, expensive black-box evaluations, and an inherent trade-off between performance and cost. Current optimization methods focus on single-agent settings and lack a principled framework for this multi-agent, multi-objective problem.
  This thesis introduces MALBO (Multi-Agent LLM Bayesian Optimization), a systematic framework designed to automate the efficient composition of LLM-based agent teams. We formalize the assignment challenge as a multi-objective optimization problem, aiming to identify the Pareto front of configurations between task accuracy and inference cost. The methodology employs multi-objective Bayesian Optimization (MOBO) with independent Gaussian Process surrogate models. By searching over a continuous feature-space representation of the LLMs, this approach performs a sample-efficient exploration guided by the expected hypervolume improvement.
  The primary contribution is a principled and automated methodology that yields a Pareto front of optimal team configurations. Our results demonstrate that the Bayesian optimization phase, compared to an initial random search, maintained a comparable average performance while reducing the average configuration cost by over 45%. Furthermore, MALBO identified specialized, heterogeneous teams that achieve cost reductions of up to 65.8% compared to homogeneous baselines, all while maintaining maximum performance. The framework thus provides a data-driven tool for deploying cost-effective and highly specialized multi-agent AI systems.

</details>


### [931] [From Single to Societal: Analyzing Persona-Induced Bias in Multi-Agent Interactions](https://arxiv.org/abs/2511.11789)
*Jiayi Li,Xiao Liu,Yansong Feng*

Main category: cs.MA

TL;DR: The paper investigates biases in multi-agent systems using large language models (LLMs), revealing that personas lead to trustworthiness and insistence biases and in-group favoritism.


<details>
  <summary>Details</summary>
Motivation: To explore whether and how personas in LLM-based multi-agent systems introduce biases in collaborative settings.

Method: The authors carried out controlled experiments on collaborative problem-solving and persuasion tasks, analyzing social traits such as trustworthiness and insistence across diverse settings.

Result: The study finds biases in trustworthiness and insistence linked to historically advantaged personas and identifies in-group favoritism among agents with similar personas.

Conclusion: The findings emphasize the persistence of persona-induced biases across LLMs and interactions, necessitating active efforts for bias awareness and mitigation in such systems.

Abstract: Large Language Model (LLM)-based multi-agent systems are increasingly used to simulate human interactions and solve collaborative tasks. A common practice is to assign agents with personas to encourage behavioral diversity. However, this raises a critical yet underexplored question: do personas introduce biases into multi-agent interactions? This paper presents a systematic investigation into persona-induced biases in multi-agent interactions, with a focus on social traits like trustworthiness (how an agent's opinion is received by others) and insistence (how strongly an agent advocates for its opinion). Through a series of controlled experiments in collaborative problem-solving and persuasion tasks, we reveal that (1) LLM-based agents exhibit biases in both trustworthiness and insistence, with personas from historically advantaged groups (e.g., men and White individuals) perceived as less trustworthy and demonstrating less insistence; and (2) agents exhibit significant in-group favoritism, showing a higher tendency to conform to others who share the same persona. These biases persist across various LLMs, group sizes, and numbers of interaction rounds, highlighting an urgent need for awareness and mitigation to ensure the fairness and reliability of multi-agent systems.

</details>


### [932] [Goal-Oriented Multi-Agent Reinforcement Learning for Decentralized Agent Teams](https://arxiv.org/abs/2511.11992)
*Hung Du,Hy Nguyen,Srikanth Thudumu,Rajesh Vasa,Kon Mouzakis*

Main category: cs.MA

TL;DR: The paper proposes a decentralized MARL framework focusing on goal-driven communication to enhance coordination among autonomous vehicles in complex environments.


<details>
  <summary>Details</summary>
Motivation: The need for effective coordination among autonomous vehicles operating in dynamic and unpredictable environments with communication and visibility constraints.

Method: Developing a decentralized MARL framework where agents selectively communicate relevant information based on their local goals and observations.

Result: The proposed framework improves task success rates, reduces time-to-goal, and demonstrates scalability with increasing agent population.

Conclusion: Decentralized, goal-driven MARL framework can effectively coordinate autonomous vehicles across diverse real-world operational domains.

Abstract: Connected and autonomous vehicles across land, water, and air must often operate in dynamic, unpredictable environments with limited communication, no centralized control, and partial observability. These real-world constraints pose significant challenges for coordination, particularly when vehicles pursue individual objectives. To address this, we propose a decentralized Multi-Agent Reinforcement Learning (MARL) framework that enables vehicles, acting as agents, to communicate selectively based on local goals and observations. This goal-aware communication strategy allows agents to share only relevant information, enhancing collaboration while respecting visibility limitations. We validate our approach in complex multi-agent navigation tasks featuring obstacles and dynamic agent populations. Results show that our method significantly improves task success rates and reduces time-to-goal compared to non-cooperative baselines. Moreover, task performance remains stable as the number of agents increases, demonstrating scalability. These findings highlight the potential of decentralized, goal-driven MARL to support effective coordination in realistic multi-vehicle systems operating across diverse domains.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [933] [Moving Pictures of Thought: Extracting Visual Knowledge in Charles S. Peirce's Manuscripts with Vision-Language Models](https://arxiv.org/abs/2511.13378)
*Carlo Teo Pedretti,Davide Picca,Dario Rodighiero*

Main category: cs.DL

TL;DR: The paper explores the use of Visual Language Models (VLMs) to analyze diagrams in Charles S. Peirce’s manuscripts, proposing a workflow for segmenting, interpreting, and integrating diagram knowledge into knowledge graphs.


<details>
  <summary>Details</summary>
Motivation: To address challenges in studying hybrid pages that combine textual and visual data, as seen in Charles S. Peirce’s manuscripts, and to better understand the role of diagrams in reasoning and explanation.

Method: The workflow involves segmenting page layouts, annotating segments, submitting diagram fragments to VLMs for interpretation using semiotic-based prompts, and integrating extracted information into knowledge graphs.

Result: The study demonstrated the feasibility of leveraging VLMs and semiotic frameworks to extract meaningful captions and knowledge about diagrams from complex manuscripts.

Conclusion: Visual Language Models and structured workflows can enhance the interpretation and integration of diagrammatic content in scholarly studies, providing new insights into hybrid documents like those of Charles S. Peirce.

Abstract: Diagrams are crucial yet underexplored tools in many disciplines, demonstrating the close connection between visual representation and scholarly reasoning. However, their iconic form poses obstacles to visual studies, intermedial analysis, and text-based digital workflows. In particular, Charles S. Peirce consistently advocated the use of diagrams as essential for reasoning and explanation. His manuscripts, often combining textual content with complex visual artifacts, provide a challenging case for studying documents involving heterogeneous materials. In this preliminary study, we investigate whether Visual Language Models (VLMs) can effectively help us identify and interpret such hybrid pages in context. First, we propose a workflow that (i) segments manuscript page layouts, (ii) reconnects each segment to IIIF-compliant annotations, and (iii) submits fragments containing diagrams to a VLM. In addition, by adopting Peirce's semiotic framework, we designed prompts to extract key knowledge about diagrams and produce concise captions. Finally, we integrated these captions into knowledge graphs, enabling structured representations of diagrammatic content within composite sources.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [934] [DataOps-driven CI/CD for analytics repositories](https://arxiv.org/abs/2511.12277)
*Dmytro Valiaiev*

Main category: eess.SY

TL;DR: The paper introduces a DataOps-based validation framework to enhance SQL governance through a Controls Scorecard and modular CI/CD pipeline.


<details>
  <summary>Details</summary>
Motivation: The unstructured use of SQL in data processing has led to inefficiencies and risks, emphasizing the need for a standardized approach to enforce data governance and quality.

Method: The authors developed a qualitative design for a framework based on a DataOps Controls Scorecard containing 12 testable controls, mapped to a CI/CD pipeline with five stages: Lint, Optimize, Parse, Validate, and Observe.

Result: The proposed framework ensures data quality and governance through automated checks, supporting a single source of truth SQL repository.

Conclusion: The framework provides a scalable, structured method for improving analytics governance, collaboration, and development transparency.

Abstract: The proliferation of SQL for data processing has often occurred without the rigor of traditional software development, leading to siloed efforts, logic replication, and increased risk. This ad-hoc approach hampers data governance and makes validation nearly impossible. Organizations are adopting DataOps, a methodology combining Agile, Lean, and DevOps principles to address these challenges to treat analytics pipelines as production systems. However, a standardized framework for implementing DataOps is lacking. This perspective proposes a qualitative design for a DataOps-aligned validation framework. It introduces a DataOps Controls Scorecard, derived from a multivocal literature review, which distills key concepts into twelve testable controls. These controls are then mapped to a modular, extensible CI/CD pipeline framework designed to govern a single source of truth (SOT) SQL repository. The framework consists of five stages: Lint, Optimize, Parse, Validate, and Observe, each containing specific, automated checks. A Requirements Traceability Matrix (RTM) demonstrates how each high-level control is enforced by concrete pipeline checks, ensuring qualitative completeness. This approach provides a structured mechanism for enhancing data quality, governance, and collaboration, allowing teams to scale analytics development with transparency and control.

</details>


### [935] [Target Defense against Sequentially Arriving Intruders: Algorithm for Agents with Dubins Dynamics](https://arxiv.org/abs/2511.12329)
*Arman Pourghorban,Dipankar Maity*

Main category: eess.SY

TL;DR: This paper studies the problem of a defender capturing incoming intruders with non-holonomic dynamics, analyzing the defender's efficiency and capturability using Dubins path and guarding arc concepts.


<details>
  <summary>Details</summary>
Motivation: The aim is to enhance understanding of capturing strategies in a dynamic scenario where defender and intruder dynamics are non-holonomic, addressing the need for systematic analysis of intruder-defender engagements in target defense scenarios.

Method: The paper divides intruder engagement into two phases—partial and full information—and uses Dubins path and guarding arc concepts to analyze the defender's ability to capture intruders. Numerical experiments via Monte-Carlo trials verify theoretical findings.

Result: The study identifies key factors influencing intruder capture percentages for finite and infinite intruder sequences and provides quantitative results validated through simulations.

Conclusion: The paper successfully establishes theoretical and numerical frameworks to evaluate defender efficiency in target defense problems, offering insights into optimizing capture strategies in dynamic engagements.

Abstract: We consider a variant of the target defense problem where a single defender is tasked to capture a sequence of incoming intruders. Both the defender and the intruders have non-holonomic dynamics. The intruders' objective is to breach the target perimeter without being captured by the defender, while the defender's goal is to capture as many intruders as possible. After one intruder breaches or is captured, the next appears randomly on a fixed circle surrounding the target. Therefore, the defender's final position in one game becomes its starting position for the next. We divide an intruder-defender engagement into two phases, partial information and full information, depending on the information available to the players. We address the capturability of an intruder by the defender using the notions of Dubins path and guarding arc. We quantify the percentage of capture for both finite and infinite sequences of incoming intruders. Finally, the theoretical results are verified through numerical examples using Monte-Carlo-type random trials of experiments.

</details>


### [936] [Density-Driven Multi-Agent Coordination for Efficient Farm Coverage and Management in Smart Agriculture](https://arxiv.org/abs/2511.12492)
*Sungjun Seo,Kooktae Lee*

Main category: eess.SY

TL;DR: This paper introduces a Density-Driven Optimal Control (D2OC) framework for multi-UAV agricultural spraying that improves task efficiency, chemical use, and operational sustainability.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies and limitations in UAV-based agricultural spraying—specifically chemical overuse, uniform resource allocation, and scalability issues—arising from growing farm scales.

Method: The paper integrates Optimal Transport theory with UAV dynamics using a Density-Driven Optimal Control (D2OC) framework for non-uniform, priority-based resource allocation in agricultural spraying.

Result: Simulation results show that D2OC surpasses traditional and Spectral Multiscale Coverage (SMC) methods in terms of coverage efficiency, reduced chemical use, and improved mission scalability.

Conclusion: The proposed D2OC framework presents an effective, scalable solution for smart agriculture, optimizing multi-UAV spraying while reducing environmental impact and resource waste.

Abstract: The growing scale of modern farms has increased the need for efficient and adaptive multi-agent coverage strategies for pest, weed, and disease management. Traditional methods such as manual inspection and blanket pesticide spraying often lead to excessive chemical use, resource waste, and environmental impact. While unmanned aerial vehicles (UAVs) offer a promising platform for precision agriculture through targeted spraying and improved operational efficiency, existing UAV-based approaches remain limited by battery life, payload capacity, and scalability, especially in large fields where single-UAV or uniformly distributed spraying is insufficient. Although multi-UAV coordination has been explored, many current frameworks still assume uniform spraying and do not account for infestation severity, UAV dynamics, non-uniform resource allocation, or energy-efficient coordination.
  To address these limitations, this paper proposes a Density-Driven Optimal Control (D2OC) framework that integrates Optimal Transport (OT) theory with multi-UAV coverage control for large-scale agricultural spraying. The method supports non-uniform, priority-aware resource allocation based on infestation intensity, reducing unnecessary chemical application. UAVs are modeled as a linear time-varying (LTV) system to capture variations in mass and inertia during spraying missions. The D2OC control law, derived using Lagrangian mechanics, enables efficient coordination, balanced workload distribution, and improved mission duration. Simulation results demonstrate that the proposed approach outperforms uniform spraying and Spectral Multiscale Coverage (SMC) in coverage efficiency, chemical reduction, and operational sustainability, providing a scalable solution for smart agriculture.

</details>


### [937] [Density-Driven Optimal Control for Non-Uniform Area Coverage in Decentralized Multi-Agent Systems Using Optimal Transport](https://arxiv.org/abs/2511.12756)
*Sungjun Seo,Kooktae Lee*

Main category: eess.SY

TL;DR: The paper introduces a framework, Density-Driven Optimal Control (D2OC), to address non-uniform area coverage challenges in multi-agent systems by integrating optimal transport theory with coverage control under realistic constraints.


<details>
  <summary>Details</summary>
Motivation: Existing strategies lack effectiveness in addressing non-uniform coverage challenges in multi-agent systems due to mission-dependent requirements and constraints like agent dynamics, operation time, and decentralized execution.

Method: The paper integrates optimal transport theory with multi-agent coverage control, solving constrained optimization problems and deriving closed-form solutions for agent trajectories. A decentralized mechanism ensures coordination without requiring global information.

Result: Simulation studies show improved non-uniform area coverage performance compared to existing methods, with preserved scalability and decentralized execution.

Conclusion: The D2OC framework offers a novel, optimal, and scalable methodology to address realistic non-uniform area coverage challenges in multi-agent systems.

Abstract: This paper addresses the fundamental problem of non-uniform area coverage in multi-agent systems, where different regions require varying levels of attention due to mission-dependent priorities. Existing uniform coverage strategies are insufficient for realistic applications, and many non-uniform approaches either lack optimality guarantees or fail to incorporate crucial real-world constraints such as agent dynamics, limited operation time, the number of agents, and decentralized execution.
  To resolve these limitations, we propose a novel framework called Density-Driven Optimal Control (D2OC). The central idea of D2OC is the integration of optimal transport theory with multi-agent coverage control, enabling each agent to continuously adjust its trajectory to match a mission-specific reference density map. The proposed formulation establishes optimality by solving a constrained optimization problem that explicitly incorporates physical and operational constraints. The resulting control input is analytically derived from the Lagrangian of the objective function, yielding closed-form optimal solutions for linear systems and a generalizable structure for nonlinear systems. Furthermore, a decentralized data-sharing mechanism is developed to coordinate agents without reliance on global information.
  Comprehensive simulation studies demonstrate that D2OC achieves significantly improved non-uniform area coverage performance compared to existing methods, while maintaining scalability and decentralized implementability.

</details>


### [938] [AI-Enhanced IoT Systems for Predictive Maintenance and Affordability Optimization in Smart Microgrids: A Digital Twin Approach](https://arxiv.org/abs/2511.12175)
*Koushik Ahmed Kushal,Florimond Gueniat*

Main category: eess.SY

TL;DR: The paper introduces an AI-powered IoT framework leveraging Digital Twin modeling to enhance predictive maintenance and cost optimization in smart microgrids, showing improved reliability and efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges like reliability issues and high operational costs in distributed microgrids by providing a more accurate, efficient, and scalable approach using Digital Twin technology.

Method: The method integrates real-time sensor data, machine learning for fault prediction, and operational analytics into a Digital Twin framework for synchronizing virtual and physical microgrid environments.

Result: The system achieves better predictive accuracy, reduces downtime, and lowers costs when compared to traditional microgrid management systems, as shown in experimental evaluations.

Conclusion: Digital Twin-based IoT architectures offer a scalable and effective solution for improving reliability, energy efficiency, and affordability in next-gen smart energy systems.

Abstract: This study presents an AI enhanced IoT framework for predictive maintenance and affordability optimization in smart microgrids using a Digital Twin modeling approach. The proposed system integrates real time sensor data, machine learning based fault prediction, and cost aware operational analytics to improve reliability and energy efficiency in distributed microgrid environments. By synchronizing physical microgrid components with a virtual Digital Twin, the framework enables early detection of component degradation, dynamic load management, and optimized maintenance scheduling. Experimental evaluations demonstrate improved predictive accuracy, reduced operational downtime, and measurable cost savings compared to baseline microgrid management methods. The findings highlight the potential of Digital Twin driven IoT architectures as a scalable solution for next generation intelligent and affordable energy systems.

</details>


### [939] [One Request, Multiple Experts: LLM Orchestrates Domain Specific Models via Adaptive Task Routing](https://arxiv.org/abs/2511.12484)
*Xu Yang,Chenhui Lin,Haotian Liu,Qi Wang,Yue Yang,Wenchuan Wu*

Main category: eess.SY

TL;DR: This paper introduces the ADN-Agent architecture, utilizing large language models (LLMs) to enhance operation in active distribution networks, unifying disparate domain-specific models.


<details>
  <summary>Details</summary>
Motivation: To address coordination challenges with distributed energy resources in active distribution networks due to their complexity and reliance on specialized models.

Method: Utilization of a general large language model (LLM) for adaptive intent recognition, task decomposition, and DSM invocation, alongside a novel communication mechanism and fine-tuned modeling pipeline.

Result: Experiments confirmed the superior performance of the proposed ADN-Agent system compared to existing approaches in coordination tasks.

Conclusion: ADN-Agent effectively integrates diverse domain-specific models, simplifying operations in active distribution networks and outperforming existing methodologies.

Abstract: With the integration of massive distributed energy resources and the widespread participation of novel market entities, the operation of active distribution networks (ADNs) is progressively evolving into a complex multi-scenario, multi-objective problem. Although expert engineers have developed numerous domain specific models (DSMs) to address distinct technical problems, mastering, integrating, and orchestrating these heterogeneous DSMs still entail considerable overhead for ADN operators. Therefore, an intelligent approach is urgently required to unify these DSMs and enable efficient coordination. To address this challenge, this paper proposes the ADN-Agent architecture, which leverages a general large language model (LLM) to coordinate multiple DSMs, enabling adaptive intent recognition, task decomposition, and DSM invocation. Within the ADN-Agent, we design a novel communication mechanism that provides a unified and flexible interface for diverse heterogeneous DSMs. Finally, for some language-intensive subtasks, we propose an automated training pipeline for fine-tuning small language models, thereby effectively enhancing the overall problem-solving capability of the system. Comprehensive comparisons and ablation experiments validate the efficacy of the proposed method and demonstrate that the ADN-Agent architecture outperforms existing LLM application paradigms.

</details>


### [940] [Data-driven Acceleration of MPC with Guarantees](https://arxiv.org/abs/2511.13588)
*Agustin Castellano,Shijie Pan,Enrique Mallada*

Main category: eess.SY

TL;DR: The paper introduces a data-driven approach for faster Model Predictive Control (MPC), replacing online optimization with a lookup-based policy for orders-of-magnitude speed gains.


<details>
  <summary>Details</summary>
Motivation: MPC can be too slow for real-time or low-latency applications, necessitating faster alternative control methods.

Method: The authors propose a data-driven framework which uses offline MPC solutions to construct a nonparametric, greedy policy based on an upper bound on the optimal cost-to-go.

Result: The proposed policy achieves speeds 100-1000 times faster than standard MPC, with only a small reduction in optimality.

Conclusion: The presented framework enables real-time control by significantly accelerating MPC while maintaining bounded performance levels, depending on data coverage.

Abstract: Model Predictive Control (MPC) is a powerful framework for optimal control but can be too slow for low-latency applications. We present a data-driven framework to accelerate MPC by replacing online optimization with a nonparametric policy constructed from offline MPC solutions. Our policy is greedy with respect to a constructed upper bound on the optimal cost-to-go, and can be implemented as a nonparametric lookup rule that is orders of magnitude faster than solving MPC online. Our analysis shows that under sufficient coverage condition of the offline data, the policy is recursively feasible and admits provable, bounded optimality gap. These conditions establish an explicit trade-off between the amount of data collected and the tightness of the bounds. Our experiments show that this policy is between 100 and 1000 times faster than standard MPC, with only a modest hit to optimality, showing potential for real-time control tasks.

</details>


### [941] [Physics-Informed Neural Networks for Nonlinear Output Regulation](https://arxiv.org/abs/2511.13595)
*Sebastiano Mengozzi,Giovanni B. Esposito,Michelangelo Bin,Andrea Acquaviva,Andrea Bartolini,Lorenzo Marconi*

Main category: eess.SY

TL;DR: This paper employs a physics-informed neural network (PINN) approach to solve nonlinear systems' output regulation problem by accurately approximating the zero-regulation-error manifold and feedforward input, enabling real-time inference and generalization across varying conditions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance solutions for nonlinear systems' output regulation problem, particularly in achieving perfect tracking or rejection while generalizing across diverse conditions.

Method: A physics-informed neural network approach is used, which approximates a zero-error manifold π(w) and feedforward input c(w) by minimizing residuals under boundary and feasibility conditions.

Result: The PINN-based solver reconstructs the zero-error manifold with high fidelity, generalizes across varying exosystem conditions, and demonstrates effective regulation performance in a helicopter synchronization task.

Conclusion: Learning-enabled solvers like the proposed PINN approach show significant potential for addressing nonlinear output regulation problems and are broadly applicable across systems admitting a solution.

Abstract: This work addresses the full-information output regulation problem for nonlinear systems, assuming the states of both the plant and the exosystem are known. In this setting, perfect tracking or rejection is achieved by constructing a zero-regulation-error manifold π(w) and a feedforward input c(w) that render such manifold invariant. The pair (π(w), c(w)) is characterized by the regulator equations, i.e., a system of PDEs with an algebraic constraint. We focus on accurately solving the regulator equations introducing a physics-informed neural network (PINN) approach that directly approximates π(w) and c(w) by minimizing the residuals under boundary and feasibility conditions, without requiring precomputed trajectories or labeled data. The learned operator maps exosystem states to steady state plant states and inputs, enables real-time inference and, critically, generalizes across families of the exosystem with varying initial conditions and parameters. The framework is validated on a regulation task that synchronizes a helicopter's vertical dynamics with a harmonically oscillating platform. The resulting PINN-based solver reconstructs the zero-error manifold with high fidelity and sustains regulation performance under exosystem variations, highlighting the potential of learning-enabled solvers for nonlinear output regulation. The proposed approach is broadly applicable to nonlinear systems that admit a solution to the output regulation problem.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [942] [Rapid Machine Learning-Driven Detection of Pesticides and Dyes Using Raman Spectroscopy](https://arxiv.org/abs/2511.12167)
*Quach Thi Thai Binh,Thuan Phuoc,Xuan Hai,Thang Bach Phan,Vu Thi Hanh Thu,Nguyen Tuan Hung*

Main category: cond-mat.mtrl-sci

TL;DR: This research presents a scalable deep learning-based Raman spectroscopy model (MLRaman) achieving high accuracy for detecting pesticides and dyes in food safety and environmental surveillance.


<details>
  <summary>Details</summary>
Motivation: Pesticides and synthetic dyes threaten food safety, human health, and sustainability, requiring improved detection methods.

Method: The study introduces a deep learning framework with ResNet-18 feature extraction combined with classifiers including XGBoost and SVM, supported by dimensionality reduction analyses.

Result: The MLRaman model achieved 97.4% accuracy, AUC of 1.0, and confirmed robust Raman spectral separability across 10 analytes.

Conclusion: The framework demonstrates potential for deployment in contaminant monitoring and real-time applications, benefiting food safety and environmental studies.

Abstract: The extensive use of pesticides and synthetic dyes poses critical threats to food safety, human health, and environmental sustainability, necessitating rapid and reliable detection methods. Raman spectroscopy offers molecularly specific fingerprints but suffers from spectral noise, fluorescence background, and band overlap, limiting its real-world applicability. Here, we propose a deep learning framework based on ResNet-18 feature extraction, combined with advanced classifiers, including XGBoost, SVM, and their hybrid integration, to detect pesticides and dyes from Raman spectroscopy, called MLRaman. The MLRaman with the CNN-XGBoost model achieved a predictive accuracy of 97.4% and a perfect AUC of 1.0, while it with the CNN-SVM model provided competitive results with robust class-wise discrimination. Dimensionality reduction analyses (PCA, t-SNE, UMAP) confirmed the separability of Raman embeddings across 10 analytes, including 7 pesticides and 3 dyes. Finally, we developed a user-friendly Streamlit application for real-time prediction, which successfully identified unseen Raman spectra from our independent experiments and also literature sources, underscoring strong generalization capacity. This study establishes a scalable, practical MLRaman model for multi-residue contaminant monitoring, with significant potential for deployment in food safety and environmental surveillance.

</details>


### [943] [Reinforcement Learning for Chemical Ordering in Alloy Nanoparticles](https://arxiv.org/abs/2511.12260)
*Jonas Elsborg,Arghya Bhowmik*

Main category: cond-mat.mtrl-sci

TL;DR: The paper approaches the optimization of element ordering in bimetallic alloy nanoparticles using reinforcement learning (RL) with geometric graph representations for global optimization.


<details>
  <summary>Details</summary>
Motivation: The motivation is to efficiently optimize atomic orderings in bimetallic alloy nanoparticles, which is a combinatorial challenge, using reinforcement learning techniques.

Method: An RL agent is trained on geometric graph representations of nanoparticles, performing composition-conserving atomic swaps to optimize their ordering.

Result: The RL agent can identify ground-state structures, handle different initial configurations robustly, and extrapolate to unseen nanoparticle sizes. However, the approach is less effective with multiple alloying elements.

Conclusion: The RL method demonstrates effectiveness in navigating nanoparticle ordering spaces and offers a transferable optimization strategy, potentially reducing repetitive search costs across varying compositions.

Abstract: We approach the search for optimal element ordering in bimetallic alloy nanoparticles (NPs) as a reinforcement learning (RL) problem, and have built an RL agent that learns to perform such global optimisation using the geometric graph representation of the NPs. To demonstrate the effectiveness, we train an RL agent to perform composition-conserving atomic swap actions on the icosahedral nanoparticle structure. Trained once on randomised $Ag_{X}Au_{309-X}$ compositions and orderings, the agent discovers previously established ground state structure. We show that this optimization is robust to differently ordered initialisations of the same NP compositions. We also demonstrate that a trained policy can extrapolate effectively to NPs of unseen size. However, the efficacy is limited when multiple alloying elements are involved. Our results demonstrate that RL with pre-trained equivariant graph encodings can navigate combinatorial ordering spaces at the nanoparticle scale, and offer a transferable optimisation strategy with the potential to generalise across composition and reduce repeated individual search cost.

</details>


### [944] [Revealing the dynamic responses of Pb under shock loading based on DFT-accuracy machine learning potential](https://arxiv.org/abs/2511.12995)
*Enze Hou,Xiaoyang Wang,Han Wang*

Main category: cond-mat.mtrl-sci

TL;DR: This study evaluates the dynamic mechanical behavior of Lead (Pb) under shock-wave loading, using advanced machine learning potentials for improved accuracy in simulations.


<details>
  <summary>Details</summary>
Motivation: Understanding the dynamic mechanical responses of Lead (Pb), including plastic deformation and phase transitions, under shock-wave loading remains poorly clarified due to limitations in experimental and simulation techniques.

Method: The paper employs newly developed machine learning-based interatomic potentials for Pb-Sn alloys in non-equilibrium molecular dynamics (NEMD) simulations to analyze shock responses under various orientations.

Result: The study finds that Pb demonstrates fast, reversible phase transitions and stacking fault evolution along the [001] orientation, and slow, irreversible plastic deformation along [011], with specific phase relationship dynamics differing from earlier findings.

Conclusion: Machine learning potentials offer accurate insights into atomic-scale mechanisms and microstructure evolution, improving theoretical understanding of Pb's behavior under extreme shock conditions.

Abstract: Lead (Pb) is a typical low-melting-point ductile metal and serves as an important model material in the study of dynamic responses. Under shock-wave loading, its dynamic mechanical behavior comprises two key phenomena: plastic deformation and shock induced phase transitions. The underlying mechanisms of these processes are still poorly understood. Revealing these mechanisms remains challenging for experimental approaches. Non-equilibrium molecular dynamics (NEMD) simulations are an alternative theoretical tool for studying dynamic responses, as they capture atomic-scale mechanisms such as defect evolution and deformation pathways. However, due to the limited accuracy of empirical interatomic potentials, the reliability of previous NEMD studies is questioned. Using our newly developed machine learning potential for Pb-Sn alloys, we revisited the microstructure evolution in response to shock loading under various shock orientations. The results reveal that shock loading along the [001] orientation of Pb exhibits a fast, reversible, and massive phase transition and stacking fault evolution. The behavior of Pb differs from previous studies by the absence of twinning during plastic deformation. Loading along the [011] orientation leads to slow, irreversible plastic deformation, and a localized FCC-BCC phase transition in the Pitsch orientation relationship. This study provides crucial theoretical insights into the dynamic mechanical response of Pb, offering a theoretical input for understanding the microstructure-performance relationship under extreme conditions.

</details>


<div id='cs.GL'></div>

# cs.GL [[Back]](#toc)

### [945] [LLM Architecture, Scaling Laws, and Economics: A Quick Summary](https://arxiv.org/abs/2511.11572)
*William H. Press*

Main category: cs.GL

TL;DR: A summary of the standard architecture of Large Language Models (LLMs) with QKV self-attention and scaling laws for computational and memory costs.


<details>
  <summary>Details</summary>
Motivation: The paper aims to provide a concise summary of LLM architecture and scaling laws as such information is not readily available in summary form.

Method: Discussing typical Transformer architecture, scaling laws, and rough cost estimates for current LLMs, including parameters and data aspects.

Result: It summarizes scaling laws, current computational and memory estimates, and discussions about DeepSeek's relevance as a special case.

Conclusion: The document successfully compiles essential knowledge on LLM architectures and scaling costs in one accessible summary format, without presenting new material.

Abstract: The current standard architecture of Large Language Models (LLMs) with QKV self-attention is briefly summarized, including the architecture of a typical Transformer. Scaling laws for compute (flops) and memory (parameters plus data) are given, along with their present (2025) rough cost estimates for the parameters of present LLMs of various scales, including discussion of whether DeepSeek should be viewed as a special case. Nothing here is new, but this material seems not otherwise readily available in summary form.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [946] [Deep Reinforcement Learning for Automated Stock Trading: An Ensemble Strategy](https://arxiv.org/abs/2511.12120)
*Hongyang Yang,Xiao-Yang Liu,Shan Zhong,Anwar Walid*

Main category: q-fin.TR

TL;DR: This paper proposes a deep reinforcement learning-based ensemble strategy for stock trading, outperforming individual algorithms and traditional benchmarks.


<details>
  <summary>Details</summary>
Motivation: Designing profitable stock trading strategies is challenging in a complex and dynamic market.

Method: The authors employ an ensemble of three actor-critic reinforcement learning algorithms (PPO, A2C, and DDPG) and use a load-on-demand process for data efficiency.

Result: Their strategy outperformed individual algorithms and baselines like the Dow Jones Industrial Average index and traditional minimum-variance portfolio, measured by the Sharpe ratio.

Conclusion: The ensemble approach offers a robust strategy for maximizing risk-adjusted returns in stock trading, merging the strengths of multiple reinforcement learning algorithms.

Abstract: Stock trading strategies play a critical role in investment. However, it is challenging to design a profitable strategy in a complex and dynamic stock market. In this paper, we propose an ensemble strategy that employs deep reinforcement schemes to learn a stock trading strategy by maximizing investment return. We train a deep reinforcement learning agent and obtain an ensemble trading strategy using three actor-critic based algorithms: Proximal Policy Optimization (PPO), Advantage Actor Critic (A2C), and Deep Deterministic Policy Gradient (DDPG). The ensemble strategy inherits and integrates the best features of the three algorithms, thereby robustly adjusting to different market situations. In order to avoid the large memory consumption in training networks with continuous action space, we employ a load-on-demand technique for processing very large data. We test our algorithms on the 30 Dow Jones stocks that have adequate liquidity. The performance of the trading agent with different reinforcement learning algorithms is evaluated and compared with both the Dow Jones Industrial Average index and the traditional min-variance portfolio allocation strategy. The proposed deep ensemble strategy is shown to outperform the three individual algorithms and two baselines in terms of the risk-adjusted return measured by the Sharpe ratio. This work is fully open-sourced at \href{https://github.com/AI4Finance-Foundation/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020}{GitHub}.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [947] [ProAV-DiT: A Projected Latent Diffusion Transformer for Efficient Synchronized Audio-Video Generation](https://arxiv.org/abs/2511.12072)
*Jiahui Sun,Weining Wang,Mingzhen Sun,Yirong Yang,Xinxin Zhu,Jing Liu*

Main category: cs.MM

TL;DR: ProAV-DiT is a novel method for generating synchronized audio-video content using advanced techniques like latent space projection and spatio-temporal modeling, achieving high fidelity and efficiency.


<details>
  <summary>Details</summary>
Motivation: Current audio-video generation faces challenges due to structural misalignments between modalities and high computational costs.

Method: ProAV-DiT preprocesses audio into video-like representations and employs a Multi-scale Dual-stream Spatio-Temporal Autoencoder (MDSA) for latent space projection. Multi-scale attention mechanisms are used to enhance temporal coherence, and a spatio-temporal diffusion Transformer is utilized for efficient and reliable generation.

Result: ProAV-DiT demonstrated superior generation quality and computational efficiency compared to existing methods in experiments.

Conclusion: The proposed method effectively aligns audio and video, improving synchronized audio-video generation while reducing computational demands.

Abstract: Sounding Video Generation (SVG) remains a challenging task due to the inherent structural misalignment between audio and video, as well as the high computational cost of multimodal data processing. In this paper, we introduce ProAV-DiT, a Projected Latent Diffusion Transformer designed for efficient and synchronized audio-video generation. To address structural inconsistencies, we preprocess raw audio into video-like representations, aligning both the temporal and spatial dimensions between audio and video. At its core, ProAV-DiT adopts a Multi-scale Dual-stream Spatio-Temporal Autoencoder (MDSA), which projects both modalities into a unified latent space using orthogonal decomposition, enabling fine-grained spatiotemporal modeling and semantic alignment. To further enhance temporal coherence and modality-specific fusion, we introduce a multi-scale attention mechanism, which consists of multi-scale temporal self-attention and group cross-modal attention. Furthermore, we stack the 2D latents from MDSA into a unified 3D latent space, which is processed by a spatio-temporal diffusion Transformer. This design efficiently models spatiotemporal dependencies, enabling the generation of high-fidelity synchronized audio-video content while reducing computational overhead. Extensive experiments conducted on standard benchmarks demonstrate that ProAV-DiT outperforms existing methods in both generation quality and computational efficiency.

</details>


### [948] [SynthGuard: An Open Platform for Detecting AI-Generated Multimedia with Multimodal LLMs](https://arxiv.org/abs/2511.12404)
*Shail Desai,Aditya Pawar,Li Lin,Xin Wang,Shu Hu*

Main category: cs.MM

TL;DR: SynthGuard is an open, user-friendly platform designed to detect and analyze AI-generated multimedia using advanced technology and is accessible to the public.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the risks of misinformation, identity misuse, and public trust erosion caused by the indistinguishability of AI-generated media by offering more transparent and educational detection tools.

Method: SynthGuard integrates traditional detectors with multimodal large language models (MLLMs) to provide explainable inference, unified image and audio support, and an interactive forensic analysis interface.

Result: SynthGuard offers a comprehensive, open-source platform that detects AI-generated multimedia, addresses modality gaps, and involves users in the forensic process.

Conclusion: The SynthGuard platform is a significant step towards making forensic detection of AI-generated content more accessible, transparent, and reliable for various users, including the public and researchers.

Abstract: Artificial Intelligence (AI) has made it possible for anyone to create images, audio, and video with unprecedented ease, enriching education, communication, and creative expression. At the same time, the rapid rise of AI-generated media has introduced serious risks, including misinformation, identity misuse, and the erosion of public trust as synthetic content becomes increasingly indistinguishable from real media. Although deepfake detection has advanced, many existing tools remain closed-source, limited in modality, or lacking transparency and educational value, making it difficult for users to understand how detection decisions are made. To address these gaps, we introduce SynthGuard, an open, user-friendly platform for detecting and analyzing AI-generated multimedia using both traditional detectors and multimodal large language models (MLLMs). SynthGuard provides explainable inference, unified image and audio support, and an interactive interface designed to make forensic analysis accessible to researchers, educators, and the public. The SynthGuard platform is available at: https://in-engr-nova.it.purdue.edu/

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [949] [The Singularity Warfare: The metatheoretical Framework](https://arxiv.org/abs/2511.11674)
*Ridvan Bari Urcosta*

Main category: physics.soc-ph

TL;DR: The paper introduces a new concept, 'Singularity Warfare,' discussing technological advancements reshaping conflict.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the accelerating technological advancements, such as artificial intelligence and quantum mechanics, and their transformative impact on the nature of warfare.

Method: The paper merges theories from physics, philosophy, and futurology to create a metatheoretical framework for understanding modern combat dynamics.

Result: It proposes a conceptual framework focusing on cognitive-technological coherence as key to success in future conflicts, while aiming to disrupt an adversary's coherence.

Conclusion: Future battlefields will blend physical and abstract elements, requiring new strategies and combining imagination with algorithmic logic.

Abstract: This paper introduces the "Singularity Warfare" concept, arguing that the accelerating pace of technological revolution, driven by artificial intelligence and quantum mechanics, is fundamentally reshaping the nature of conflict. Moving beyond traditional "Newtonian" warfare and current military doctrines, this framework posits that future battlefields will be defined by a merger of physical and abstract domains, where human imagination and algorithmic logic become a unified, actionable reality. Victory will hinge on a unit's ability to maintain cognitive and technological "coherence" while creating "decoherence" in the adversary. The paper synthesizes theories from physics, philosophy, and futurology to provide a metatheoretical framework for understanding this paradigm shift.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [950] [A Review of Statistical and Machine Learning Approaches for Coral Bleaching Assessment](https://arxiv.org/abs/2511.12234)
*Soham Sarkar,Arnab Hazra*

Main category: stat.AP

TL;DR: The paper reviews statistical and machine learning methods used in coral bleaching assessment, emphasizing the importance of data-driven strategies for reef management.


<details>
  <summary>Details</summary>
Motivation: Coral bleaching threatens marine ecosystems, with environmental factors and rising sea temperatures identified as key contributors. A scarcity of stochastic modeling approaches motivates the need for a review to aid reef management.

Method: The review classifies statistical frameworks, such as regression models and resilience indicators, alongside machine learning methods like random forests and decision trees, highlighting their relevance in modeling coral bleaching.

Result: Statistical methods excel in analyzing environmental stressors, while machine learning approaches are effective at detecting nonlinear relationships and integrating diverse data. Knowledge gaps and potential future research directions are identified.

Conclusion: A synthesis of current models and methods offers direction for improving coral reef management, underscoring the need for more context-specific applications of data-driven strategies.

Abstract: Coral bleaching is a major concern for marine ecosystems; more than half of the world's coral reefs have either bleached or died over the past three decades. Increasing sea surface temperatures, along with various spatiotemporal environmental factors, are considered the primary reasons behind coral bleaching. The statistical and machine learning communities have focused on multiple aspects of the environment in detail. However, the literature on various stochastic modeling approaches for assessing coral bleaching is extremely scarce. Data-driven strategies are crucial for effective reef management, and this review article provides an overview of existing statistical and machine learning methods for assessing coral bleaching. Statistical frameworks, including simple regression models, generalized linear models, generalized additive models, Bayesian regression models, spatiotemporal models, and resilience indicators, such as Fisher's Information and Variance Index, are commonly used to explore how different environmental stressors influence coral bleaching. On the other hand, machine learning methods, including random forests, decision trees, support vector machines, and spatial operators, are more popular for detecting nonlinear relationships, analyzing high-dimensional data, and allowing integration of heterogeneous data from diverse sources. In addition to summarizing these models, we also discuss potential data-driven future research directions, with a focus on constructing statistical and machine learning models in specific contexts related to coral bleaching.

</details>


### [951] [Scalable Vision-Guided Crop Yield Estimation](https://arxiv.org/abs/2511.12999)
*Harrison H. Li,Medhanie Irgau,Nabil Janmohamed,Karen Solveig Rieckmann,David B. Lobell*

Main category: stat.AP

TL;DR: This paper introduces a method to improve crop yield estimates using less time-consuming field photos combined with crop cut data, achieving significant accuracy gains with minimal cost.


<details>
  <summary>Details</summary>
Motivation: The need for precise and efficient crop yield estimation is critical for agricultural monitoring, but existing methods like crop cuts are time-consuming and expensive.

Method: The study employs prediction-powered inference (PPI), combining computer vision models trained on field photos with a control function adjusted by spatial coordinates to recalibrate and enhance predictions.

Result: Using real-world data of nearly 20,000 crop cuts and photos in sub-Saharan Africa, the method significantly improves yield estimation accuracy (73% for rice and 12-23% for maize) and shortens confidence intervals.

Conclusion: Machine learning methods relying on low-cost visual data can enhance crop yield monitoring, supporting affordable area-based crop insurance and sustainable agriculture investments.

Abstract: Precise estimation and uncertainty quantification for average crop yields are critical for agricultural monitoring and decision making. Existing data collection methods, such as crop cuts in randomly sampled fields at harvest time, are relatively time-consuming. Thus, we propose an approach based on prediction-powered inference (PPI) to supplement these crop cuts with less time-consuming field photos. After training a computer vision model to predict the ground truth crop cut yields from the photos, we learn a ``control function" that recalibrates these predictions with the spatial coordinates of each field. This enables fields with photos but not crop cuts to be leveraged to improve the precision of zone-wide average yield estimates. Our control function is learned by training on a dataset of nearly 20,000 real crop cuts and photos of rice and maize fields in sub-Saharan Africa. To improve precision, we pool training observations across different zones within the same first-level subdivision of each country. Our final PPI-based point estimates of the average yield are provably asymptotically unbiased and cannot increase the asymptotic variance beyond that of the natural baseline estimator -- the sample average of the crop cuts -- as the number of fields grows. We also propose a novel bias-corrected and accelerated (BCa) bootstrap to construct accompanying confidence intervals. Even in zones with as few as 20 fields, the point estimates show significant empirical improvement over the baseline, increasing the effective sample size by as much as 73% for rice and by 12-23% for maize. The confidence intervals are accordingly shorter at minimal cost to empirical finite-sample coverage. This demonstrates the potential for relatively low-cost images to make area-based crop insurance more affordable and thus spur investment into sustainable agricultural practices.

</details>


### [952] [Stochastic Predictive Analytics for Stocks in the Newsvendor Problem](https://arxiv.org/abs/2511.12397)
*Pedro A. Pury*

Main category: stat.AP

TL;DR: This paper develops a stochastic model for inventory management, focusing on situations with limited historical data and dynamic demand.


<details>
  <summary>Details</summary>
Motivation: Inventory management often struggles due to lack of robust models that can adapt to unpredictable demand or sparse historical data.

Method: The research proposes a stochastic model that provides dynamic distribution predictions of inventory stock without relying on predefined demand distributions.

Result: The model was tested against real-world data from an electronic marketplace, showcasing its effectiveness in forecasting scenarios.

Conclusion: The proposed model is applicable for short-term predictions and practical scenarios like the Newsvendor problem, offering a flexible solution in inventory management.

Abstract: This work addresses a key challenge in inventory management by developing a stochastic model that describes the dynamic distribution of inventory stock over time without assuming a specific demand distribution. Our model provides a flexible and applicable solution for situations with limited historical data and short-term predictions, making it well-suited for the Newsvendor problem. We evaluate our model's performance using real-world data from a large electronic marketplace, demonstrating its effectiveness in a practical forecasting scenario.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [953] [DIGing--SGLD: Decentralized and Scalable Langevin Sampling over Time--Varying Networks](https://arxiv.org/abs/2511.12836)
*Waheed U. Bajwa,Mert Gurbuzbalaban,Mustafa Ali Kutbay,Lingjiong Zhu,Muhammad Zulqarnain*

Main category: math.OC

TL;DR: This paper introduces DIGing-SGLD, a decentralized algorithm for scalable Bayesian learning over time-varying networks. It provides finite-time non-asymptotic convergence guarantees and improves upon existing decentralized sampling methods.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing decentralized SGLD methods, which are restricted to static networks and exhibit bias due to network effects, and to enable efficient and bias-free sampling over time-varying networks.

Method: The proposed DIGing-SGLD algorithm integrates Langevin sampling with the gradient-tracking mechanism of the DIGing algorithm, designed for decentralized optimization over time-varying networks. It uses these mechanisms to achieve bias-free and scalable Bayesian sampling without a central coordinator.

Result: Theoretical analysis shows that DIGing-SGLD achieves geometric convergence to an $O(\sqrtη)$ neighborhood of the target distribution with rates matching the best-known centralized counterparts. Empirical validation demonstrates strong performance in Bayesian regression tasks under dynamic network conditions.

Conclusion: DIGing-SGLD successfully extends decentralized SGLD methods for scalable Bayesian learning over time-varying networks, with both theoretical guarantees and empirical validation of performance advantages.

Abstract: Sampling from a target distribution induced by training data is central to Bayesian learning, with Stochastic Gradient Langevin Dynamics (SGLD) serving as a key tool for scalable posterior sampling and decentralized variants enabling learning when data are distributed across a network of agents. This paper introduces DIGing-SGLD, a decentralized SGLD algorithm designed for scalable Bayesian learning in multi-agent systems operating over time-varying networks. Existing decentralized SGLD methods are restricted to static network topologies, and many exhibit steady-state sampling bias caused by network effects, even when full batches are used. DIGing-SGLD overcomes these limitations by integrating Langevin-based sampling with the gradient-tracking mechanism of the DIGing algorithm, originally developed for decentralized optimization over time-varying networks, thereby enabling efficient and bias-free sampling without a central coordinator. To our knowledge, we provide the first finite-time non-asymptotic Wasserstein convergence guarantees for decentralized SGLD-based sampling over time-varying networks, with explicit constants. Under standard strong convexity and smoothness assumptions, DIGing-SGLD achieves geometric convergence to an $O(\sqrtη)$ neighborhood of the target distribution, where $η$ is the stepsize, with dependence on the target accuracy matching the best-known rates for centralized and static-network SGLD algorithms using constant stepsize. Numerical experiments on Bayesian linear and logistic regression validate the theoretical results and demonstrate the strong empirical performance of DIGing-SGLD under dynamically evolving network conditions.

</details>


### [954] [A Computational Method for Solving the Stochastic Joint Replenishment Problem in High Dimensions](https://arxiv.org/abs/2511.11830)
*Barış Ata,Wouter van Eekelen,Yuan Zhong*

Main category: math.OC

TL;DR: The paper addresses high-dimensional stochastic joint replenishment problems, introduces a simulation-based computational method using deep neural networks for continuous-time impulse control, and verifies its effectiveness in inventory control with substantial computational feasibility.


<details>
  <summary>Details</summary>
Motivation: The paper aims to tackle the computational challenges associated with solving high-dimensional stochastic joint replenishment problems to design efficient inventory control policies.

Method: The authors approximate the discrete problem using a continuous-time impulse control framework, employing connections with backward stochastic differential equations (BSDEs) and stochastic target problems. They propose a novel deep neural network-based simulation method to solve this problem.

Result: For test problems, the proposed method matches or outperforms existing benchmarks and is computationally feasible for systems with up to 50 stock-keeping units (SKUs).

Conclusion: The simulation-based approach using deep neural networks demonstrates promising results in high-dimensional inventory control problems, offering a scalable and efficient solution.

Abstract: We consider a discrete-time formulation for a class of high-dimensional stochastic joint replenishment problems. First, we approximate the problem by a continuous-time impulse control problem. Exploiting connections among the impulse control problem, backward stochastic differential equations (BSDEs) with jumps, and the stochastic target problem, we develop a novel, simulation-based computational method that relies on deep neural networks to solve the impulse control problem. Based on that solution, we propose an implementable inventory control policy for the original (discrete-time) stochastic joint replenishment problem, and test it against the best available benchmarks in a series of test problems. For the problems studied thus far, our method matches or beats the best benchmark we could find, and it is computationally feasible up to at least 50 dimensions -- that is, 50 stock-keeping units (SKUs).

</details>


### [955] [DLMMPR:Deep Learning-based Measurement Matrix for Phase Retrieval](https://arxiv.org/abs/2511.12556)
*Jing Liu,Bing Guo,Ren Zhu*

Main category: math.OC

TL;DR: This paper introduces a deep learning-based algorithm (DLMMPR) that optimizes measurement matrix design for phase retrieval, achieving superior results in empirical tests compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to enhance phase retrieval's accuracy and robustness by integrating learning optimization into the measurement matrix design process.

Method: The authors propose the DLMMPR algorithm, which integrates a parameterized measurement matrix within a deep learning framework. It also incorporates subgradient descent and proximal mapping to improve recovery robustness.

Result: The proposed DLMMPR algorithm demonstrated significant improvements in PSNR and SSIM, outperforming established methods like DeepMMSE and PrComplex across various noise conditions.

Conclusion: DLMMPR effectively leverages deep learning for optimizing measurement matrices in phase retrieval, proving its efficacy and superiority through rigorous testing.

Abstract: This paper pioneers the integration of learning optimization into measurement matrix design for phase retrieval. We introduce the Deep Learning-based Measurement Matrix for Phase Retrieval (DLMMPR) algorithm, which parameterizes the measurement matrix within an end-to-end deep learning architecture. Synergistically augmented with subgradient descent and proximal mapping modules for robust recovery, DLMMPR's efficacy is decisively confirmed through comprehensive empirical validation across diverse noise regimes. Benchmarked against DeepMMSE and PrComplex, our method yields substantial gains in PSNR and SSIM, underscoring its superiority.

</details>


### [956] [Power Homotopy for Zeroth-Order Non-Convex Optimizations](https://arxiv.org/abs/2511.13592)
*Chen Xu*

Main category: math.OC

TL;DR: The paper introduces GS-PowerHP, a new zeroth-order optimization method for maximizing non-convex functions, achieving strong empirical and theoretical performance.


<details>
  <summary>Details</summary>
Motivation: Optimizing non-convex functions is crucial in fields like machine learning (e.g., black-box optimization), but existing zeroth-order methods often suffer from inefficiency and suboptimal convergence.

Method: They employ a power-transformed Gaussian-smoothed surrogate function and an incrementally decaying variance to enhance data efficiency and converge near the global maximizer.

Result: GS-PowerHP demonstrates theoretical convergence with $O(d^2 \varepsilon^{-2})$ complexity and ranks among the top methods in experiments, even excelling in high-dimensional black-box attacks.

Conclusion: The proposed approach effectively solves challenging non-convex problems and exhibits superior performance in both metrics and real-world applications.

Abstract: We introduce GS-PowerHP, a novel zeroth-order method for non-convex optimization problems of the form $\max_{x \in \mathbb{R}^d} f(x)$. Our approach leverages two key components: a power-transformed Gaussian-smoothed surrogate $F_{N,σ}(μ) = \mathbb{E}_{x\sim\mathcal{N}(μ,σ^2 I_d)}[e^{N f(x)}]$ whose stationary points cluster near the global maximizer $x^*$ of $f$ for sufficiently large $N$, and an incrementally decaying $σ$ for enhanced data efficiency. Under mild assumptions, we prove convergence in expectation to a small neighborhood of $x^*$ with the iteration complexity of $O(d^2 \varepsilon^{-2})$. Empirical results show our approach consistently ranks among the top three across a suite of competing algorithms. Its robustness is underscored by the final experiment on a substantially high-dimensional problem ($d=150,528$), where it achieved first place on least-likely targeted black-box attacks against images from ImageNet, surpassing all competing methods.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [957] [Finite-Horizon Quickest Change Detection Balancing Latency with False Alarm Probability](https://arxiv.org/abs/2511.12803)
*Yu-Han Huang,Venugopal V. Veeravalli*

Main category: cs.IT

TL;DR: The paper addresses a finite-horizon quickest change detection (QCD) problem to minimize detection delay (latency) in non-stationary environments while controlling false alarms.


<details>
  <summary>Details</summary>
Motivation: The motivation is to create effective change detection methods for non-stationary environments, focusing on balancing timely detection and managing false alarms within finite-horizon constraints.

Method: Derive a universal lower bound on latency, develop order-optimal change detectors for both known and unknown distributions, and extend to cases involving sub-Gaussian distributions with simulations to validate results.

Result: Order-optimal change detectors with proven theoretical performance bounds were developed for cases of known and unknown distributions, showing their effectiveness through simulations.

Conclusion: This study establishes foundational results for finite-horizon QCD and provides design methods for change detectors optimized under practical constraints of latency and false alarms.

Abstract: A finite-horizon variant of the quickest change detection (QCD) problem that is of relevance to learning in non-stationary environments is studied. The metric characterizing false alarms is the probability of a false alarm occurring before the horizon ends. The metric that characterizes the delay is \emph{latency}, which is the smallest value such that the probability that detection delay exceeds this value is upper bounded to a predetermined latency level. The objective is to minimize the latency (at a given latency level), while maintaining a low false alarm probability. Under the pre-specified latency and false alarm levels, a universal lower bound on the latency, which any change detection procedure needs to satisfy, is derived. Change detectors are then developed, which are order-optimal in terms of the horizon. The case where the pre- and post-change distributions are known is considered first, and then the results are generalized to the non-parametric case when they are unknown except that they are sub-Gaussian with different means. Simulations are provided to validate the theoretical results.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [958] [From Black Box to Bijection: Interpreting Machine Learning to Build a Zeta Map Algorithm](https://arxiv.org/abs/2511.12421)
*Xiaoyu Huang,Blake Jackson,Kyu-Hwan Lee*

Main category: math.CO

TL;DR: The paper proposes a machine learning-based workflow to discover combinatorial bijections and demonstrates it by deriving a new algorithmic description of the zeta map.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenge of constructing explicit combinatorial bijections, particularly in cases where patterns fail to emerge due to large-scale data.

Method: The method involves training a transformer model on paired Dyck paths and analyzing its attention patterns to derive new insights.

Result: A new algorithmic description of the zeta map, termed the 'Scaffolding Map,' is developed through the proposed machine learning workflow.

Conclusion: The study highlights the potential of machine learning, particularly transformer models, in advancing the discovery of combinatorial bijections when traditional methods become impractical.

Abstract: There is a large class of problems in algebraic combinatorics which can be distilled into the same challenge: construct an explicit combinatorial bijection. Traditionally, researchers have solved challenges like these by visually inspecting the data for patterns, formulating conjectures, and then proving them. But what is to be done if patterns fail to emerge until the data grows beyond human scale? In this paper, we propose a new workflow for discovering combinatorial bijections via machine learning. As a proof of concept, we train a transformer on paired Dyck paths and use its learned attention patterns to derive a new algorithmic description of the zeta map, which we call the \textit{Scaffolding Map}.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [959] [A Deep Learning Framework for Thyroid Nodule Segmentation and Malignancy Classification from Ultrasound Images](https://arxiv.org/abs/2511.11937)
*Omar Abdelrazik,Mohamed Elsayed,Noorul Wahab,Nasir Rajpoot,Adam Shephard*

Main category: eess.IV

TL;DR: The paper proposes a fully automated two-stage framework using deep learning for interpretability and high malignancy prediction of thyroid nodules detected via ultrasound.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the high inter-observer variability in ultrasound-based risk stratification of thyroid nodules while improving interpretability and prediction accuracy.

Method: The framework uses a TransUNet model for automatic thyroid nodule segmentation followed by a ResNet-18 classifier for malignancy prediction based on localised image regions.

Result: Using 5-fold cross-validation on 349 clinical images, the framework achieved an F1-score of 0.852, outperforming a Random Forest baseline (F1-score 0.829).

Conclusion: The automated DL pipeline is demonstrated to be more predictive than traditional methods and provides an interpretability-focused approach for thyroid nodule risk stratification.

Abstract: Ultrasound-based risk stratification of thyroid nodules is a critical clinical task, but it suffers from high inter-observer variability. While many deep learning (DL) models function as "black boxes," we propose a fully automated, two-stage framework for interpretable malignancy prediction. Our method achieves interpretability by forcing the model to focus only on clinically relevant regions. First, a TransUNet model automatically segments the thyroid nodule. The resulting mask is then used to create a region of interest around the nodule, and this localised image is fed directly into a ResNet-18 classifier. We evaluated our framework using 5-fold cross-validation on a clinical dataset of 349 images, where it achieved a high F1-score of 0.852 for predicting malignancy. To validate its performance, we compared it against a strong baseline using a Random Forest classifier with hand-crafted morphological features, which achieved an F1-score of 0.829. The superior performance of our DL framework suggests that the implicit visual features learned from the localised nodule are more predictive than explicit shape features alone. This is the first fully automated end-to-end pipeline for both detecting thyroid nodules on ultrasound images and predicting their malignancy.

</details>


### [960] [Recursive Threshold Median Filter and Autoencoder for Salt-and-Pepper Denoising: SSIM analysis of Images and Entropy Maps](https://arxiv.org/abs/2511.12212)
*Petr Boriskov,Kirill Rudkovskii,Andrei Velichko*

Main category: eess.IV

TL;DR: This paper evaluates methods for removing salt-and-pepper noise in images, introducing median filters (MF) and autoencoders (AE), complemented by a novel metric SSIMMap.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of efficiently removing salt-and-pepper noise from images, especially under strong noise scenarios, while proposing scalable and computationally viable solutions.

Method: Combine median filter (MF) with a recursive threshold algorithm and simple three-layer autoencoder (AE), introducing SSIMMap as a new metric for assessing denoising performance.

Result: Median filter (MF) robustly restores images even with strong noise levels, while simple AE performs better with low noise. Two scalable schemes, 2MF and MFs-AE, are proposed for low and high resolution scenarios.

Conclusion: MF is computationally efficient and preferable for resource-constrained platforms. SSIMMap provides valuable insights for blur assessment and tuning denoising parameters. AE requires prior denoising for effective restoration.

Abstract: This paper studies the removal of salt-and-pepper noise from images using median filter (MF) and simple three-layer autoencoder (AE) within recursive threshold algorithm. The performance of denoising is assessed with two metrics: the standard Structural Similarity Index SSIMImg of restored and clean images and a newly applied metric SSIMMap - the SSIM of entropy maps of these images computed via 2D Sample Entropy in sliding windows. We shown that SSIMMap is more sensitive to blur and local intensity transitions and complements SSIMImg. Experiments on low- and high-resolution grayscales images demonstrate that recursive threshold MF robustly restores images even under strong noise (50-60 %), whereas simple AE is only capable of restoring images with low levels of noise (<30 %). We propose two scalable schemes: (i) 2MF, which uses two MFs with different window sizes and a final thresholding step, effective for highlighting sharp local details at low resolution; and (ii) MFs-AE, which aggregates features from multiple MFs via an AE and is beneficial for restoring the overall scene structure at higher resolution. Owing to its simplicity and computational efficiency, MF remains preferable for deployment on resource-constrained platforms (edge/IoT), whereas AE underperforms without prior denoising. The results also validate the practical value of SSIMMap for objective blur assessment and denoising parameter tuning.

</details>


### [961] [Deep Unfolded BM3D: Unrolling Non-local Collaborative Filtering into a Trainable Neural Network](https://arxiv.org/abs/2511.12248)
*Kerem Basim,Mehmet Ozan Unal,Metin Ertas,Isa Yildirim*

Main category: eess.IV

TL;DR: The paper introduces DU-BM3D, a hybrid framework combining BM3D and U-Net for denoising, outperforming traditional methods in LDCT tasks.


<details>
  <summary>Details</summary>
Motivation: BM3D effectively exploits non-local self-similarity priors for denoising yet uses fixed parameters; U-Net lacks interpretability and generality across noise types. The study aims to resolve this limitation through a hybrid approach.

Method: BM3D is unrolled into a trainable framework by substituting its collaborative filtering with a learnable U-Net denoiser for end-to-end optimization.

Result: DU-BM3D achieves superior performance compared to classic BM3D and U-Net, showing higher PSNR and SSIM, especially in high-noise LDCT scenarios.

Conclusion: DU-BM3D successfully retains BM3D's structural priors while benefiting from U-Net's learning flexibility, making it effective for LDCT denoising.

Abstract: Block-Matching and 3D Filtering (BM3D) exploits non-local self-similarity priors for denoising but relies on fixed parameters. Deep models such as U-Net are more flexible but often lack interpretability and fail to generalize across noise regimes. In this study, we propose Deep Unfolded BM3D (DU-BM3D), a hybrid framework that unrolls BM3D into a trainable architecture by replacing its fixed collaborative filtering with a learnable U-Net denoiser. This preserves BM3D's non-local structural prior while enabling end-to-end optimization. We evaluate DU-BM3D on low-dose CT (LDCT) denoising and show that it outperforms classic BM3D and standalone U-Net across simulated LDCT at different noise levels, yielding higher PSNR and SSIM, especially in high-noise conditions.

</details>


### [962] [Slow - Motion Video Synthesis for Basketball Using Frame Interpolation](https://arxiv.org/abs/2511.11644)
*Jiantang Huang*

Main category: eess.IV

TL;DR: The paper introduces a system for generating high-quality slow-motion basketball footage by fine-tuning the RIFE network on a specialized dataset, achieving significant performance improvements over existing methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance viewer experience by addressing the limitations of standard basketball broadcast footage, which fails to adequately capture rapid plays like dunks and crossovers due to low frame rates.

Method: The team fine-tuned the RIFE network using a basketball-specific subset of the SportsSloMo dataset, employing human-aware random cropping for training, and evaluated performance with metrics like PSNR and SSIM.

Result: The fine-tuned RIFE achieved superior performance with a higher PSNR of 34.3 dB and SSIM of 0.949, surpassing both Super SloMo and the baseline RIFE, and demonstrated real-time capabilities with a lightweight Gradio interface.

Conclusion: Task-specific adaptation is critical for sports footage slow-motion synthesis, with the fine-tuned RIFE achieving an optimal balance between accuracy and computational speed, making it suitable for consumer applications.

Abstract: Basketball broadcast footage is traditionally captured at 30-60 fps, limiting viewers' ability to appreciate rapid plays such as dunks and crossovers. We present a real-time slow-motion synthesis system that produces high-quality basketball-specific interpolated frames by fine-tuning the recent Real-Time Intermediate Flow Estimation (RIFE) network on the SportsSloMo dataset. Our pipeline isolates the basketball subset of SportsSloMo, extracts training triplets, and fine-tunes RIFE with human-aware random cropping. We compare the resulting model against Super SloMo and the baseline RIFE model using Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity (SSIM) on held-out clips. The fine-tuned RIFE attains a mean PSNR of 34.3 dB and SSIM of 0.949, outperforming Super SloMo by 2.1 dB and the baseline RIFE by 1.3 dB. A lightweight Gradio interface demonstrates end-to-end 4x slow-motion generation on a single RTX 4070 Ti Super at approximately 30 fps. These results indicate that task-specific adaptation is crucial for sports slow-motion, and that RIFE provides an attractive accuracy-speed trade-off for consumer applications.

</details>


### [963] [Multimodal RGB-HSI Feature Fusion with Patient-Aware Incremental Heuristic Meta-Learning for Oral Lesion Classification](https://arxiv.org/abs/2511.12268)
*Rupam Mukherjee,Rajkumar Daniel,Soujanya Hazra,Shirin Dasgupta,Subhamoy Mandal*

Main category: eess.IV

TL;DR: The paper proposes an integrated classifier for oral lesions using RGB images, hyperspectral reconstruction, handcrafted descriptors, and demographic metadata to improve oral cancer detection in low-resource areas.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of early detection for oral cancer and potentially malignant disorders, particularly in low-resource settings where annotated medical data is limited.

Method: Developed a unified four-class oral lesion classifier combining deep learning-based RGB embeddings, hyperspectral imaging reconstruction, demographic metadata, and spectral-textural attributes. It also introduced an incremental heuristic meta-learner (IHML) for better classification reliability.

Result: The framework achieved a macro F1 score of 66.23% and an accuracy of 64.56% on an unseen patient dataset, indicating improved robustness and real-world applicability for oral lesion screening.

Conclusion: Integrating hyperspectral reconstruction and uncertainty-aware meta-learning enhances the effectiveness and reliability of oral lesion screening, offering a promising solution for low-resource settings.

Abstract: Early detection of oral cancer and potentially malignant disorders is challenging in low-resource settings due to limited annotated data. We present a unified four-class oral lesion classifier that integrates deep RGB embeddings, hyperspectral reconstruction, handcrafted spectral-textural descriptors, and demographic metadata. A pathologist-verified subset of oral cavity images was curated and processed using a fine-tuned ConvNeXt-v2 encoder, followed by RGB-to-HSI reconstruction into 31-band hyperspectral cubes. Haemoglobin-sensitive indices, texture features, and spectral-shape measures were extracted and fused with deep and clinical features. Multiple machine-learning models were assessed with patient-wise validation. We further introduce an incremental heuristic meta-learner (IHML) that combines calibrated base classifiers through probabilistic stacking and patient-level posterior smoothing. On an unseen patient split, the proposed framework achieved a macro F1 of 66.23% and an accuracy of 64.56%. Results demonstrate that hyperspectral reconstruction and uncertainty-aware meta-learning substantially improve robustness for real-world oral lesion screening.

</details>


### [964] [RAA-MIL: A Novel Framework for Classification of Oral Cytology](https://arxiv.org/abs/2511.12269)
*Rupam Mukherjee,Rajkumar Daniel,Soujanya Hazra,Shirin Dasgupta,Subhamoy Mandal*

Main category: eess.IV

TL;DR: This paper introduces a weakly supervised deep learning framework for diagnosing oral cytology WSIs, utilizing the Oral Cytology Dataset. The proposed RAA-MIL method improves diagnostic accuracy and F1-score over baseline methods.


<details>
  <summary>Details</summary>
Motivation: The manual examination of oral cytology WSIs is slow, subjective, and heavily dependent on expert pathologists, leading to potential inefficiencies in early detection of OSCC.

Method: The paper proposes a Region-Affinity Attention MIL (RAA-MIL) approach that models spatial relationships within cytology WSIs. Patient cases are represented as bags of cytology patches with weak labels assigned by expert pathologists.

Result: RAA-MIL achieves 72.7% accuracy and a weighted F1-score of 0.69 on an unseen test set, outperforming the baseline MIL model.

Conclusion: This study establishes the first patient-level weakly supervised benchmark for oral cytology, advancing AI-assisted digital pathology for reliable OSCC diagnosis.

Abstract: Cytology is a valuable tool for early detection of oral squamous cell carcinoma (OSCC). However, manual examination of cytology whole slide images (WSIs) is slow, subjective, and depends heavily on expert pathologists. To address this, we introduce the first weakly supervised deep learning framework for patient-level diagnosis of oral cytology whole slide images, leveraging the newly released Oral Cytology Dataset [1], which provides annotated cytology WSIs from ten medical centres across India. Each patient case is represented as a bag of cytology patches and assigned a diagnosis label (Healthy, Benign, Oral Potentially Malignant Disorders (OPMD), OSCC) by an in-house expert pathologist. These patient-level weak labels form a new extension to the dataset. We evaluate a baseline multiple-instance learning (MIL) model and a proposed Region-Affinity Attention MIL (RAA-MIL) that models spatial relationships between regions within each slide. The RAA-MIL achieves an average accuracy of 72.7%, weighted F1-score of 0.69 on an unseen test set, outperforming the baseline. This study establishes the first patient-level weakly supervised benchmark for oral cytology and moves toward reliable AI-assisted digital pathology.

</details>


### [965] [MTMed3D: A Multi-Task Transformer-Based Model for 3D Medical Imaging](https://arxiv.org/abs/2511.12373)
*Fan Li,Arun Iyengar,Lanyu Xu*

Main category: eess.IV

TL;DR: MTMed3D is a multi-task Transformer-based model for object detection, segmentation, and classification in 3D medical imaging, providing enhanced efficiency and reduced computational costs compared to single-task models.


<details>
  <summary>Details</summary>
Motivation: Single-task models in medical imaging don't leverage shared information across tasks, leading to inefficiencies in practical applications.

Method: The authors propose MTMed3D, using a Transformer encoder for multi-scale features and CNN-based decoders for task-specific outputs.

Result: MTMed3D achieves promising results across detection, segmentation, and classification tasks, surpassing prior works in detection and reducing computational costs.

Conclusion: MTMed3D demonstrates its potential for improving diagnostic processes by introducing a Transformer-based, multi-task learning framework in 3D medical imaging.

Abstract: In the field of medical imaging, AI-assisted techniques such as object detection, segmentation, and classification are widely employed to alleviate the workload of physicians and doctors. However, single-task models are predominantly used, overlooking the shared information across tasks. This oversight leads to inefficiencies in real-life applications. In this work, we propose MTMed3D, a novel end-to-end Multi-task Transformer-based model to address the limitations of single-task models by jointly performing 3D detection, segmentation, and classification in medical imaging. Our model uses a Transformer as the shared encoder to generate multi-scale features, followed by CNN-based task-specific decoders. The proposed framework was evaluated on the BraTS 2018 and 2019 datasets, achieving promising results across all three tasks, especially in detection, where our method achieves better results than prior works. Additionally, we compare our multi-task model with equivalent single-task variants trained separately. Our multi-task model significantly reduces computational costs and achieves faster inference speed while maintaining comparable performance to the single-task models, highlighting its efficiency advantage. To the best of our knowledge, this is the first work to leverage Transformers for multi-task learning that simultaneously covers detection, segmentation, and classification tasks in 3D medical imaging, presenting its potential to enhance diagnostic processes. The code is available at https://github.com/fanlimua/MTMed3D.git.

</details>


### [966] [DEMIST: \underline{DE}coupled \underline{M}ulti-stream latent d\underline{I}ffusion for Quantitative Myelin Map \underline{S}yn\underline{T}hesis](https://arxiv.org/abs/2511.12396)
*Jiacheng Wang,Hao Li,Xing Yao,Ahmad Toubasi,Taegan Vinarsky,Caroline Gheen,Joy Derwenskus,Chaoyang Jin,Richard Dortch,Junzhong Xu,Francesca Bagnato,Ipek Oguz*

Main category: eess.IV

TL;DR: This paper proposes a method called DEMIST to synthesize PSR maps from standard T1w and FLAIR images using a 3D latent diffusion model with advanced conditioning mechanisms.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of qMT imaging, which requires long scan times despite its utility for multiple sclerosis assessment by providing myelin-sensitive biomarkers.

Method: DEMIST synthesizes PSR maps through a 3D latent diffusion model utilizing separate autoencoders for PSR and anatomical images, and a conditional diffusion model trained in latent space with three complementary conditioning mechanisms: semantic, spatial, and adaptive. Edge-aware loss terms and alignment losses are also used.

Result: DEMIST outperforms baseline models like VAE, GAN, and diffusion models, producing sharper lesion boundaries and better quantitative agreement with ground truth. Evaluation was conducted with 163 scans from 99 subjects using 5-fold cross-validation.

Conclusion: The study demonstrates that DEMIST can effectively and accurately synthesize PSR maps from standard imaging, positioning it as a significant advancement for MS assessment.

Abstract: Quantitative magnetization transfer (qMT) imaging provides myelin-sensitive biomarkers, such as the pool size ratio (PSR), which is valuable for multiple sclerosis (MS) assessment. However, qMT requires specialized 20-30 minute scans. We propose DEMIST to synthesize PSR maps from standard T1w and FLAIR images using a 3D latent diffusion model with three complementary conditioning mechanisms. Our approach has two stages: first, we train separate autoencoders for PSR and anatomical images to learn aligned latent representations. Second, we train a conditional diffusion model in this latent space on top of a frozen diffusion foundation backbone. Conditioning is decoupled into: (i) \textbf{semantic} tokens via cross-attention, (ii) \textbf{spatial} per-scale residual hints via a 3D ControlNet branch, and (iii) \textbf{adaptive} LoRA-modulated attention. We include edge-aware loss terms to preserve lesion boundaries and alignment losses to maintain quantitative consistency, while keeping the number of trainable parameters low and retaining the inductive bias of the pretrained model. We evaluate on 163 scans from 99 subjects using 5-fold cross-validation. Our method outperforms VAE, GAN and diffusion baselines on multiple metrics, producing sharper boundaries and better quantitative agreement with ground truth. Our code is publicly available at https://github.com/MedICL-VU/MS-Synthesis-3DcLDM.

</details>


### [967] [Improving the Generalisation of Learned Reconstruction Frameworks](https://arxiv.org/abs/2511.12730)
*Emilien Valat,Ozan Öktem*

Main category: eess.IV

TL;DR: This paper proposes GLM, a neural network architecture using graph and grid convolutions for CT imaging, improving performance and efficiency compared to CNNs.


<details>
  <summary>Details</summary>
Motivation: Current CNNs struggle with generalization in CT imaging due to their grid-based convolutions not accounting for the sinogram's line manifold geometry.

Method: Introduce graph data structure for CT geometries and develop GLM architecture combining graph and grid convolutions.

Result: GLM surpasses CNNs in structural similarity and peak signal-to-noise ratio, while also being more resource-efficient and generalizing better to unseen CT geometries.

Conclusion: GLM effectively addresses the generalization challenge in CT imaging, offering superior performance, reduced resource usage, and robust handling of acquisition variations.

Abstract: Ensuring proper generalization is a critical challenge in applying data-driven methods for solving inverse problems in imaging, as neural networks reconstructing an image must perform well across varied datasets and acquisition geometries. In X-ray Computed Tomography (CT), convolutional neural networks (CNNs) are widely used to filter the projection data but are ill-suited for this task as they apply grid-based convolutions to the sinogram, which inherently lies on a line manifold, not a regular grid. The CNNs, unaware of the geometry, are implicitly tied to it and require an excessive amount of parameters as they must infer the relations between measurements from the data rather than from prior information.
  The contribution of this paper is twofold. First, we introduce a graph data structure to represent CT acquisition geometries and tomographic data, providing a detailed explanation of the graph's structure for circular, cone-beam geometries. Second, we propose GLM, a hybrid neural network architecture that leverages both graph and grid convolutions to process tomographic data.
  We demonstrate that GLM outperforms CNNs when performance is quantified in terms of structural similarity and peak signal-to-noise ratio, despite the fact that GLM uses only a fraction of the trainable parameters. Compared to CNNs, GLM also requires significantly less training time and memory, and its memory requirements scale better. Crucially, GLM demonstrates robust generalization to unseen variations in the acquisition geometry, like when training only on fully sampled CT data and then testing on sparse-view CT data.

</details>


### [968] [BrainNormalizer: Anatomy-Informed Pseudo-Healthy Brain Reconstruction from Tumor MRI via Edge-Guided ControlNet](https://arxiv.org/abs/2511.12853)
*Min Gu Kwak,Yeonju Lee,Hairong Wang,Jing Li*

Main category: eess.IV

TL;DR: BrainNormalizer reconstructs pseudo-healthy brain MRIs from tumorous scans using an anatomy-informed diffusion model, enabling anatomically plausible reconstructions without paired scans.


<details>
  <summary>Details</summary>
Motivation: Brain tumors disrupt brain tissue and architecture, complicating diagnosis and treatment. A method is needed to model a 'healthy' brain appearance in tumorous cases.

Method: BrainNormalizer trains a diffusion model in two stages: fine-tuning for inpainting and adding edge-map-guided ControlNet to incorporate anatomical contours.

Result: BrainNormalizer shows strong performance on the BraTS2020 dataset, offering anatomically realistic reconstructions in tumor-altered areas.

Conclusion: The model provides a tool for treatment planning by generating clinically reliable anatomical references and opens up research in counterfactual modeling and tumor-induced deformation analysis.

Abstract: Brain tumors are among the most clinically significant neurological diseases and remain a major cause of morbidity and mortality due to their aggressive growth and structural heterogeneity. As tumors expand, they induce substantial anatomical deformation that disrupts both local tissue organization and global brain architecture, complicating diagnosis, treatment planning, and surgical navigation. Yet a subject-specific reference of how the brain would appear without tumor-induced changes is fundamentally unobtainable in clinical practice. We present BrainNormalizer, an anatomy-informed diffusion framework that reconstructs pseudo-healthy MRIs directly from tumorous scans by conditioning the generative process on boundary cues extracted from the subject's own anatomy. This boundary-guided conditioning enables anatomically plausible pseudo-healthy reconstruction without requiring paired non-tumorous and tumorous scans. BrainNormalizer employs a two-stage training strategy. The pretrained diffusion model is first adapted through inpainting-based fine-tuning on tumorous and non-tumorous scans. Next, an edge-map-guided ControlNet branch is trained to inject fine-grained anatomical contours into the frozen decoder while preserving learned priors. During inference, a deliberate misalignment strategy pairs tumorous inputs with non-tumorous prompts and mirrored contralateral edge maps, leveraging hemispheric correspondence to guide reconstruction. On the BraTS2020 dataset, BrainNormalizer achieves strong quantitative performance and qualitatively produces anatomically plausible reconstructions in tumor-affected regions while retaining overall structural coherence. BrainNormalizer provides clinically reliable anatomical references for treatment planning and supports new research directions in counterfactual modeling and tumor-induced deformation analysis.

</details>


### [969] [Inertia-Informed Orientation Priors for Event-Based Optical Flow Estimation](https://arxiv.org/abs/2511.12961)
*Pritam P. Karmokar,William J. Beksi*

Main category: eess.IV

TL;DR: This paper proposes a biologically-inspired method to enhance event-based optical flow estimation by integrating visual and inertial motion cues using orientation maps.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges posed by the temporally dense but spatially sparse nature of events in event cameras, focusing on refining the non-convex contrast maximization method for optical flow estimation.

Method: The method introduces orientation maps derived from camera 3D velocities as priors to guide the contrast maximization process, providing directional guidance and constraining motion trajectories.

Result: The proposed approach demonstrates superior accuracy in optical flow estimation compared to state-of-the-art methods, validated on datasets MVSEC, DSEC, and ECD.

Conclusion: Integrating orientation maps with the CM framework improves robustness, convergence, and accuracy in event-based optical flow estimation.

Abstract: Event cameras, by virtue of their working principle, directly encode motion within a scene. Many learning-based and model-based methods exist that estimate event-based optical flow, however the temporally dense yet spatially sparse nature of events poses significant challenges. To address these issues, contrast maximization (CM) is a prominent model-based optimization methodology that estimates the motion trajectories of events within an event volume by optimally warping them. Since its introduction, the CM framework has undergone a series of refinements by the computer vision community. Nonetheless, it remains a highly non-convex optimization problem. In this paper, we introduce a novel biologically-inspired hybrid CM method for event-based optical flow estimation that couples visual and inertial motion cues. Concretely, we propose the use of orientation maps, derived from camera 3D velocities, as priors to guide the CM process. The orientation maps provide directional guidance and constrain the space of estimated motion trajectories. We show that this orientation-guided formulation leads to improved robustness and convergence in event-based optical flow estimation. The evaluation of our approach on the MVSEC, DSEC, and ECD datasets yields superior accuracy scores over the state of the art.

</details>


### [970] [A Multicollinearity-Aware Signal-Processing Framework for Cross-$β$ Identification via X-ray Scattering of Alzheimer's Tissue](https://arxiv.org/abs/2511.12451)
*Abdullah Al Bashit,Prakash Nepal,Lee Makowski*

Main category: eess.IV

TL;DR: This paper presents a framework for analyzing X-ray scattering profiles to automatically detect pathological cross-β inclusions, hallmarks of Alzheimer's disease, overcoming challenges of substrate contamination, correlations, and data limitations.


<details>
  <summary>Details</summary>
Motivation: The study aims to develop a systematic and automated method to detect cross-β structural inclusions in human brain tissue X-ray scattering data, addressing challenges like high sample correlations and limited experimental data.

Method: The framework comprises three stages: 1) classifying mica substrate and tissue regions using a Bayes-optimal classifier, 2) implementing a correlation pruning scheme to reduce redundancy while preserving discriminative information, and 3) training a compact neural network on the optimized feature set with a composite loss function.

Result: The optimized model achieved an F1-score of 84.30% with a reduced feature set (11 out of 211 features) and a small parameter count (174 trainable parameters).

Conclusion: This work demonstrates a robust, interpretable, and theory-driven classification strategy for data-limited scenarios involving complex, high-dimensional measurements, specifically in detecting cross-β structural inclusions from neurodegenerative tissue.

Abstract: X-ray scattering measurements of in situ human brain tissue encode structural signatures of pathological cross-$β$ inclusions, yet systematic exploitation of these data for automated detection remains challenging due to substrate contamination, strong inter-feature correlations, and limited sample sizes. This work develops a three-stage classification framework for identifying cross-$β$ structural inclusions-a hallmark of Alzheimer's disease-in X-ray scattering profiles of post-mortem human brain. Stage 1 employs a Bayes-optimal classifier to separate mica substrate from tissue regions on the basis of their distinct scattering signatures. Stage 2 introduces a multicollinearityaware, class-conditional correlation pruning scheme with formal guarantees on the induced Bayes risk and approximation error, thereby reducing redundancy while retaining class-discriminative information. Stage 3 trains a compact neural network on the pruned feature set to detect the presence or absence of cross-$β$ fibrillar ordering. The top-performing model, optimized with a composite loss combining Focal and Dice objectives, attains a test F1-score of 84.30% using 11 of 211 candidate features and 174 trainable parameters. The overall framework yields an interpretable, theory-grounded strategy for data-limited classification problems involving correlated, high-dimensional experimental measurements, exemplified here by X-ray scattering profiles of neurodegenerative tissue.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [971] [Decision and Gender Biases in Large Language Models: A Behavioral Economic Perspective](https://arxiv.org/abs/2511.12319)
*Luca Corazzini,Elisa Deriu,Marco Guerzoni*

Main category: econ.GN

TL;DR: Large language models exhibit human-like biases in decision-making in behavioral economics experiments.


<details>
  <summary>Details</summary>
Motivation: Understand if advanced LLMs display rational decision-making or reproduce human behavioral tendencies.

Method: Testing Google Gemma7B and Qwen in behavioral economics experiments (ultimatum and gambling games) with neutral and gender-conditioned prompts.

Result: Models showed moderate fairness concerns, mild loss aversion, and gender-conditioned differences compared to human benchmarks.

Conclusion: LLMs do not fully embody rational decision-making, displaying attenuated but persistent human-like biases.

Abstract: Large language models (LLMs) increasingly mediate economic and organisational processes, from automated customer support and recruitment to investment advice and policy analysis. These systems are often assumed to embody rational decision making free from human error; yet they are trained on human language corpora that may embed cognitive and social biases. This study investigates whether advanced LLMs behave as rational agents or whether they reproduce human behavioural tendencies when faced with classic decision problems. Using two canonical experiments in behavioural economics, the ultimatum game and a gambling game, we elicit decisions from two state of the art models, Google Gemma7B and Qwen, under neutral and gender conditioned prompts. We estimate parameters of inequity aversion and loss-aversion and compare them with human benchmarks. The models display attenuated but persistent deviations from rationality, including moderate fairness concerns, mild loss aversion, and subtle gender conditioned differences.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [972] [Tokenize Once, Recommend Anywhere: Unified Item Tokenization for Multi-domain LLM-based Recommendation](https://arxiv.org/abs/2511.12922)
*Yu Hou,Won-Yong Shin*

Main category: cs.IR

TL;DR: The paper presents UniTok, a framework for a unified item tokenization system that preserves semantics across various item domains and outperforms existing baselines.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of limited generalization and difficulties in preserving domain-specific semantic information when tokenizing items from different domains in large language model-based recommender systems.

Method: UniTok uses a mixture-of-experts (MoE) architecture and codebooks to project items into a unified latent space, with domain-specific and shared experts to capture domain-unique semantics and cross-domain common knowledge. A mutual information calibration mechanism ensures balanced semantic retention across domains.

Result: The proposed framework achieves up to 51.89% improvements over benchmarks, theoretically validates its design, and demonstrates generalizability across various domains without retraining.

Conclusion: UniTok is an effective, theoretically sound, and generalizable tokenization framework, addressing scalability and semantic preservation issues in multi-domain recommender systems.

Abstract: Large language model (LLM)-based recommender systems have achieved high-quality performance by bridging the discrepancy between the item space and the language space through item tokenization. However, existing item tokenization methods typically require training separate models for each item domain, limiting generalization. Moreover, the diverse distributions and semantics across item domains make it difficult to construct a unified tokenization that preserves domain-specific information. To address these challenges, we propose UniTok, a Unified item Tokenization framework that integrates our own mixture-of-experts (MoE) architecture with a series of codebooks to convert items into discrete tokens, enabling scalable tokenization while preserving semantic information across multiple item domains. Specifically, items from different domains are first projected into a unified latent space through a shared encoder. They are then routed to domain-specific experts to capture the unique semantics, while a shared expert, which is always active, encodes common knowledge transferable across domains. Additionally, to mitigate semantic imbalance across domains, we present a mutual information calibration mechanism, which guides the model towards retaining similar levels of semantic information for each domain. Comprehensive experiments on wide-ranging real-world datasets demonstrate that the proposed UniTok framework is (a) highly effective: achieving up to 51.89% improvements over strong benchmarks, (b) theoretically sound: showing the analytical validity of our architectural design and optimization; and (c) highly generalizable: demonstrating robust performance across diverse domains without requiring per-domain retraining, a capability not supported by existing baselines.

</details>


### [973] [Attention Grounded Enhancement for Visual Document Retrieval](https://arxiv.org/abs/2511.13415)
*Wanqing Cui,Wei Huang,Yazhi Guo,Yibo Hu,Meiguang Jin,Junfeng Ma,Keping Bi*

Main category: cs.IR

TL;DR: This paper introduces the AGREE framework to enhance visual document retrieval by aligning relevant document regions with queries using multimodal supervision.


<details>
  <summary>Details</summary>
Motivation: Existing visual document retrieval methods struggle to understand implicit semantic connections due to reliance on surface-level cues and coarse global relevance labels, limiting their ability to manage complex queries.

Method: The proposed AGREE framework uses cross-modal attention from multimodal large language models as local supervision during training to guide document region identification, combining these signals with global relevance labels.

Result: AGREE significantly outperforms baselines in retrieval performance, as shown on the ViDoRe V2 benchmark, achieving deeper alignment between queries and document regions.

Conclusion: AGREE enhances retrieval by improving semantic alignment and interpretability, addressing gaps in surface-level matching and enabling more accurate identification of relevant document regions.

Abstract: Visual document retrieval requires understanding heterogeneous and multi-modal content to satisfy information needs. Recent advances use screenshot-based document encoding with fine-grained late interaction, significantly improving retrieval performance. However, retrievers are still trained with coarse global relevance labels, without revealing which regions support the match. As a result, retrievers tend to rely on surface-level cues and struggle to capture implicit semantic connections, hindering their ability to handle non-extractive queries. To alleviate this problem, we propose a \textbf{A}ttention-\textbf{G}rounded \textbf{RE}triever \textbf{E}nhancement (AGREE) framework. AGREE leverages cross-modal attention from multimodal large language models as proxy local supervision to guide the identification of relevant document regions. During training, AGREE combines local signals with the global signals to jointly optimize the retriever, enabling it to learn not only whether documents match, but also which content drives relevance. Experiments on the challenging ViDoRe V2 benchmark show that AGREE significantly outperforms the global-supervision-only baseline. Quantitative and qualitative analyses further demonstrate that AGREE promotes deeper alignment between query terms and document regions, moving beyond surface-level matching toward more accurate and interpretable retrieval. Our code is available at: https://anonymous.4open.science/r/AGREE-2025.

</details>


### [974] [Exploring Multi-Table Retrieval Through Iterative Search](https://arxiv.org/abs/2511.13418)
*Allaa Boutaleb,Bernd Amann,Rafael Angarita,Hubert Naacke*

Main category: cs.IR

TL;DR: This paper develops a scalable and efficient iterative approach for multi-table retrieval in open-domain datalake question answering, achieving comparable performance to complex exact methods while being significantly faster.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of retrieving and composing joinable information from multiple tables in open-domain datalake question answering, as exact optimization methods are computationally heavy and simpler heuristics lack coherence.

Method: The paper introduces a general iterative search framework, presenting the Greedy Join-Aware Retrieval algorithm which balances relevance, coverage, and joinability during the retrieval process.

Result: Experiments on 5 NL2SQL benchmarks show that the iterative method achieves competitive performance with MIP-based approaches and is 4-400x faster depending on settings.

Conclusion: Iterative heuristics offer a practical, scalable solution for multi-table retrieval in datalake question answering, balancing semantic relevance and structural coherence efficiently.

Abstract: Open-domain question answering over datalakes requires retrieving and composing information from multiple tables, a challenging subtask that demands semantic relevance and structural coherence (e.g., joinability). While exact optimization methods like Mixed-Integer Programming (MIP) can ensure coherence, their computational complexity is often prohibitive. Conversely, simpler greedy heuristics that optimize for query coverage alone often fail to find these coherent, joinable sets. This paper frames multi-table retrieval as an iterative search process, arguing this approach offers advantages in scalability, interpretability, and flexibility. We propose a general framework and a concrete instantiation: a fast, effective Greedy Join-Aware Retrieval algorithm that holistically balances relevance, coverage, and joinability. Experiments across 5 NL2SQL benchmarks demonstrate that our iterative method achieves competitive retrieval performance compared to the MIP-based approach while being 4-400x faster depending on the benchmark and search space settings. This work highlights the potential of iterative heuristics for practical, scalable, and composition-aware retrieval.

</details>


### [975] [GroupRank: A Groupwise Reranking Paradigm Driven by Reinforcement Learning](https://arxiv.org/abs/2511.11653)
*Duolin Sun,Meixiu Long,Dan Yang,Yihan Jiao,Zhehao Tan,Jie Feng,Junjie Wang,Yue Shen,Peng Wei,Jian Wang,Jinjie Gu*

Main category: cs.IR

TL;DR: This paper introduces Groupwise, a reranking model combining the strengths of Pointwise and Listwise methods to improve retrieval systems.


<details>
  <summary>Details</summary>
Motivation: Existing reranking paradigms face limitations: Pointwise methods lack relative ranking context, while Listwise methods are inflexible and struggle with scalability.

Method: Groupwise reranking jointly considers queries and groups of candidate documents using GRPO training and synthetic data generation for enhanced performance.

Result: Experiments prove the method's effectiveness in reasoning-intensive retrieval tasks, validated by benchmarks BRIGHT and R2MED.

Conclusion: Groupwise reranking successfully integrates both context awareness and scalability, improving performance in RAG systems.

Abstract: Large Language Models have shown strong potential as rerankers to enhance the overall performance of RAG systems. However, existing reranking paradigms are constrained by a core theoretical and practical dilemma: Pointwise methods, while simple and highly flexible, evaluate documents independently, making them prone to the Ranking Myopia Trap, overlooking the relative importance between documents. In contrast, Listwise methods can perceive the global ranking context, but suffer from inherent List Rigidity, leading to severe scalability and flexibility issues when handling large candidate sets. To address these challenges, we propose Groupwise, a novel reranking paradigm. In this approach, the query and a group of candidate documents are jointly fed into the model, which performs within-group comparisons to assign individual relevance scores to each document. This design retains the flexibility of Pointwise methods while enabling the comparative capability of Listwise methods. We further adopt GRPO for model training, equipped with a heterogeneous reward function that integrates ranking metrics with a distributional reward aimed at aligning score distributions across groups. To overcome the bottleneck caused by the scarcity of high quality labeled data, we further propose an innovative pipeline for synthesizing high quality retrieval and ranking data. The resulting data can be leveraged not only for training the reranker but also for training the retriever. Extensive experiments validate the effectiveness of our approach. On two reasoning intensive retrieval benchmarks, BRIGHT and R2MED.

</details>


### [976] [A Multimodal Manufacturing Safety Chatbot: Knowledge Base Design, Benchmark Development, and Evaluation of Multiple RAG Approaches](https://arxiv.org/abs/2511.11847)
*Ryan Singh,Austin Hamilton,Amanda White,Michael Wise,Ibrahim Yousif,Arthur Carvalho,Zhe Shan,Reza Abrisham Baf,Mohammad Mayyas,Lora A. Cavuoto,Fadel M. Megahed*

Main category: cs.IR

TL;DR: This paper introduces a multimodal chatbot utilizing large language models for safety training in Industry 5.0 environments, achieving high accuracy, low latency, and low cost.


<details>
  <summary>Details</summary>
Motivation: Worker safety in modern manufacturing remains challenging, prompting the need for innovative human-centric solutions in Industry 5.0.

Method: Using design science research, the paper developed a chatbot grounded in regulatory and technical documentation, assessed through various RAG configurations and benchmarks.

Result: The chatbot achieved 86.66% accuracy, 10.04 seconds latency, and $0.005 cost per query, validated by experts and researchers.

Conclusion: The work offers an open-source safety training chatbot, a validated evaluation benchmark, and a methodology for AI-enabled safety tools in Industry 5.0.

Abstract: Ensuring worker safety remains a critical challenge in modern manufacturing environments. Industry 5.0 reorients the prevailing manufacturing paradigm toward more human-centric operations. Using a design science research methodology, we identify three essential requirements for next-generation safety training systems: high accuracy, low latency, and low cost. We introduce a multimodal chatbot powered by large language models that meets these design requirements. The chatbot uses retrieval-augmented generation to ground its responses in curated regulatory and technical documentation. To evaluate our solution, we developed a domain-specific benchmark of expert-validated question and answer pairs for three representative machines: a Bridgeport manual mill, a Haas TL-1 CNC lathe, and a Universal Robots UR5e collaborative robot. We tested 24 RAG configurations using a full-factorial design and assessed them with automated evaluations of correctness, latency, and cost. Our top 2 configurations were then evaluated by ten industry experts and academic researchers. Our results show that retrieval strategy and model configuration have a significant impact on performance. The top configuration (selected for chatbot deployment) achieved an accuracy of 86.66%, an average latency of 10.04 seconds, and an average cost of $0.005 per query. Overall, our work provides three contributions: an open-source, domain-grounded safety training chatbot; a validated benchmark for evaluating AI-assisted safety instruction; and a systematic methodology for designing and assessing AI-enabled instructional and immersive safety training systems for Industry 5.0 environments.

</details>


### [977] [The Environmental Impact of Ensemble Techniques in Recommender Systems](https://arxiv.org/abs/2511.11649)
*Jannik Nitschke*

Main category: cs.IR

TL;DR: This paper evaluates the environmental impact of ensemble techniques in recommender systems and their accuracy-energy trade-offs, analyzing 93 experiments with key results showcasing improved accuracy but higher energy consumption compared to single models.


<details>
  <summary>Details</summary>
Motivation: To assess the environmental impact of ensemble techniques in recommender systems and compare their trade-offs in accuracy and energy consumption to single optimized models.

Method: 93 experiments were conducted across two frameworks (Surprise and LensKit) using four datasets, evaluating four ensemble strategies, and measuring energy usage with a smart plug. Metrics such as RMSE, NDCG, and CO2 measurements were utilized.

Result: Ensemble methods improved accuracy (0.3–5.7%) but also increased energy usage substantially (19–2,549% more energy). Efficiency varied with strategy and dataset, with selective strategies performing better in the accuracy-energy trade-off.

Conclusion: Selective ensemble strategies, though offering accuracy improvements, come at significant energy and carbon costs, particularly in industrial-scale datasets, underscoring the need to prioritize sustainable algorithm selection.

Abstract: Ensemble techniques in recommender systems have demonstrated accuracy improvements of 10-30%, yet their environmental impact remains unmeasured. While deep learning recommendation algorithms can generate up to 3,297 kg CO2 per paper, ensemble methods have not been sufficiently evaluated for energy consumption. This thesis investigates how ensemble techniques influence environmental impact compared to single optimized models.
  We conducted 93 experiments across two frameworks (Surprise for rating prediction, LensKit for ranking) on four datasets spanning 100,000 to 7.8 million interactions. We evaluated four ensemble strategies (Average, Weighted, Stacking/Rank Fusion, Top Performers) against simple baselines and optimized single models, measuring energy consumption with a smart plug.
  Results revealed a non-linear accuracy-energy relationship. Ensemble methods achieved 0.3-5.7% accuracy improvements while consuming 19-2,549% more energy depending on dataset size and strategy. The Top Performers ensemble showed best efficiency: 0.96% RMSE improvement with 18.8% energy overhead on MovieLens-1M, and 5.7% NDCG improvement with 103% overhead on MovieLens-100K. Exhaustive averaging strategies consumed 88-270% more energy for comparable gains. On the largest dataset (Anime, 7.8M interactions), the Surprise ensemble consumed 2,005% more energy (0.21 Wh vs. 0.01 Wh) for 1.2% accuracy improvement, producing 53.8 mg CO2 versus 2.6 mg CO2 for the single model.
  This research provides one of the first systematic measurements of energy and carbon footprint for ensemble recommender systems, demonstrates that selective strategies offer superior efficiency over exhaustive averaging, and identifies scalability limitations at industrial scale. These findings enable informed decisions about sustainable algorithm selection in recommender systems.

</details>


### [978] [Dimension vs. Precision: A Comparative Analysis of Autoencoders and Quantization for Efficient Vector Retrieval on BEIR SciFact](https://arxiv.org/abs/2511.13057)
*Satyanarayan Pati*

Main category: cs.IR

TL;DR: This paper evaluates strategies to compress dense retrieval embeddings for real-world deployment. Quantization (int8) proves most effective with minimal trade-offs.


<details>
  <summary>Details</summary>
Motivation: Dense retrieval’s high-dimensional embeddings cause storage and deployment challenges in practical applications.

Method: The study compares Dimensionality Reduction with Autoencoders (AE) and Precision Reduction via Quantization across various compression levels and retrieval metrics.

Result: Int8 quantization achieved 4x compression with minimal performance loss (~1–2% in nDCG@10), while Autoencoders showed more significant performance degradation at similar compression rates.

Conclusion: Int8 scalar quantization is the recommended strategy for efficient and deployable dense retrieval systems.

Abstract: Dense retrieval models have become a standard for state-of-the-art information retrieval. However, their high-dimensional, high-precision (float32) vector embeddings create significant storage and memory challenges for real-world deployment. To address this, we conduct a rigorous empirical study on the BEIR SciFact benchmark, evaluating the trade-offs between two primary compression strategies: (1) Dimensionality Reduction via deep Autoencoders (AE), reducing original 384-dim vectors to latent spaces from 384 down to 12, and (2) Precision Reduction via Quantization (float16, int8, and binary). We systematically compare each method by measuring the "performance loss" (or gain) relative to a float32 baseline across a full suite of retrieval metrics (NDCG, MAP, MRR, Recall, Precision) at various k cutoffs. Our results show that int8 scalar quantization provides the most effective "sweet spot," achieving a 4x compression with a negligible [~1-2%] drop in nDCG@10. In contrast, Autoencoders show a graceful degradation but suffer a more significant performance loss at equivalent 4x compression ratios (AE-96). binary quantization was found to be unsuitable for this task due to catastrophic performance drops. This work provides a practical guide for deploying efficient, high-performance retrieval systems.

</details>


### [979] [From Scaling to Structured Expressivity: Rethinking Transformers for CTR Prediction](https://arxiv.org/abs/2511.12081)
*Bencheng Yan,Yuejie Lei,Zhiyuan Zeng,Di Wang,Kaiyi Lin,Pengjie Wang,Jian Xu,Bo Zheng*

Main category: cs.IR

TL;DR: This paper introduces the Field-Aware Transformer (FAT) for click-through rate (CTR) prediction, designed to address the misalignment between Transformer architectures and CTR data characteristics, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: To solve the inefficiency and diminishing returns of current CTR prediction models that fail due to misalignment with CTR data's combinatorial demands.

Method: The paper proposes embedding field-based interaction priors into Transformer attention mechanisms through decomposed content alignment and cross-field modulation.

Result: FAT achieves power-law scaling in AUC, improves AUC by up to +0.51% over existing methods, and delivers +2.33% CTR and +0.66% RPM when deployed online.

Conclusion: Scaling effectiveness in recommendation systems comes from structured expressivity and architectural alignment with data semantics rather than mere model size increases.

Abstract: Despite massive investments in scale, deep models for click-through rate (CTR) prediction often exhibit rapidly diminishing returns - a stark contrast to the smooth, predictable gains seen in large language models. We identify the root cause as a structural misalignment: Transformers assume sequential compositionality, while CTR data demand combinatorial reasoning over high-cardinality semantic fields. Unstructured attention spreads capacity indiscriminately, amplifying noise under extreme sparsity and breaking scalable learning. To restore alignment, we introduce the Field-Aware Transformer (FAT), which embeds field-based interaction priors into attention through decomposed content alignment and cross-field modulation. This design ensures model complexity scales with the number of fields F, not the total vocabulary size n >> F, leading to tighter generalization and, critically, observed power-law scaling in AUC as model width increases. We present the first formal scaling law for CTR models, grounded in Rademacher complexity, that explains and predicts this behavior. On large-scale benchmarks, FAT improves AUC by up to +0.51% over state-of-the-art methods. Deployed online, it delivers +2.33% CTR and +0.66% RPM. Our work establishes that effective scaling in recommendation arises not from size, but from structured expressivity-architectural coherence with data semantics.

</details>


### [980] [Local Collaborative Filtering: A Collaborative Filtering Method that Utilizes Local Similarities among Users](https://arxiv.org/abs/2511.13166)
*Zhaoxin Shen,Dan Wu*

Main category: cs.IR

TL;DR: The paper introduces Local Collaborative Filtering (LCF), a new CF method leveraging local similarities and LLN for improved user behavior utilization.


<details>
  <summary>Details</summary>
Motivation: To improve recommender systems by better utilizing user behavior data through local similarities.

Method: Developed Local Collaborative Filtering (LCF) using local user similarities and integrated data based on the law of large numbers.

Result: Experiments on the Steam game dataset show that LCF meets real-world requirements.

Conclusion: LCF effectively applies local similarities and LLN for enhancing collaborative filtering systems.

Abstract: To leverage user behavior data from the Internet more effectively in recommender systems, this paper proposes a novel collaborative filtering (CF) method called Local Collaborative Filtering (LCF). LCF utilizes local similarities among users and integrates their data using the law of large numbers (LLN), thereby improving the utilization of user behavior data. Experiments are conducted on the Steam game dataset, and the results of LCF align with real-world needs.

</details>


### [981] [Uncovering Causal Drivers of Energy Efficiency for Industrial Process in Foundry via Time-Series Causal Inference](https://arxiv.org/abs/2511.13389)
*Zhipeng Ma,Bo Nørregaard Jørgensen,Zheng Grace Ma*

Main category: cs.IR

TL;DR: The paper develops a method combining clustering and causal inference to identify operational factors impacting energy efficiency in induction furnace melting, using data from a Danish foundry.


<details>
  <summary>Details</summary>
Motivation: Energy efficiency in industrial foundries is vital due to their high energy consumption and the limitations of correlation-based approaches to pinpoint causal relationships.

Method: The study integrates time-series clustering with the PCMCI+ algorithm to uncover causal relationships between process variables during different operational modes in melting cycles.

Result: Key drivers of efficiency include material weight, furnace temperature, and energy consumption, with cluster-specific analysis showing stable structures in efficient modes and feedback loops in inefficient ones.

Conclusion: The introduced methodology provides insights that can help operators optimize processes, improve energy efficiency, and reduce environmental impacts.

Abstract: Improving energy efficiency in industrial foundry processes is a critical challenge, as these operations are highly energy-intensive and marked by complex interdependencies among process variables. Correlation-based analyses often fail to distinguish true causal drivers from spurious associations, limiting their usefulness for decision-making. This paper applies a time-series causal inference framework to identify the operational factors that directly affect energy efficiency in induction furnace melting. Using production data from a Danish foundry, the study integrates time-series clustering to segment melting cycles into distinct operational modes with the PCMCI+ algorithm, a state-of-the-art causal discovery method, to uncover cause-effect relationships within each mode. Across clusters, robust causal relations among energy consumption, furnace temperature, and material weight define the core drivers of efficiency, while voltage consistently influences cooling water temperature with a delayed response. Cluster-specific differences further distinguish operational regimes: efficient clusters are characterized by stable causal structures, whereas inefficient ones exhibit reinforcing feedback loops and atypical dependencies. The contributions of this study are twofold. First, it introduces an integrated clustering-causal inference pipeline as a methodological innovation for analyzing energy-intensive processes. Second, it provides actionable insights that enable foundry operators to optimize performance, reduce energy consumption, and lower emissions.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [982] [TR-Gaussians: High-fidelity Real-time Rendering of Planar Transmission and Reflection with 3D Gaussian Splatting](https://arxiv.org/abs/2511.13009)
*Yong Liu,Keyang Ye,Tianjia Shao,Kun Zhou*

Main category: cs.GR

TL;DR: The paper introduces TR-Gaussians, a novel 3D-Gaussian-based representation for realistic rendering of planar transmission and reflection in indoor scenes.


<details>
  <summary>Details</summary>
Motivation: Improvements in rendering high-fidelity effects for planar transmission and reflection are needed for realism in indoor scene synthesis.

Method: The method combines 3D Gaussians with learnable reflection planes, blending transmission and reflection components through a Fresnel-based weighting scheme, supported by a multi-stage optimization framework.

Result: TR-Gaussians enable real-time novel view synthesis with high fidelity in scenes featuring transmission and reflection components, outperforming state-of-the-art solutions.

Conclusion: TR-Gaussians represent a significant advancement in rendering techniques for complex indoor scenes, introducing efficiency and realism in handling transmission and reflection.

Abstract: We propose Transmission-Reflection Gaussians (TR-Gaussians), a novel 3D-Gaussian-based representation for high-fidelity rendering of planar transmission and reflection, which are ubiquitous in indoor scenes. Our method combines 3D Gaussians with learnable reflection planes that explicitly model the glass planes with view-dependent reflectance strengths. Real scenes and transmission components are modeled by 3D Gaussians and the reflection components are modeled by the mirrored Gaussians with respect to the reflection plane. The transmission and reflection components are blended according to a Fresnel-based, view-dependent weighting scheme, allowing for faithful synthesis of complex appearance effects under varying viewpoints. To effectively optimize TR-Gaussians, we develop a multi-stage optimization framework incorporating color and geometry constraints and an opacity perturbation mechanism. Experiments on different datasets demonstrate that TR-Gaussians achieve real-time, high-fidelity novel view synthesis in scenes with planar transmission and reflection, and outperform state-of-the-art approaches both quantitatively and qualitatively.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [983] [Auto-encoder model for faster generation of effective one-body gravitational waveform approximations](https://arxiv.org/abs/2511.12642)
*Suyog Garg,Feng-Li Lin,Kipp Cannon*

Main category: gr-qc

TL;DR: This paper introduces a machine learning model using a conditional variational auto-encoder for rapid generation of gravitational wave signals with high accuracy, addressing computational challenges in parameter estimation for next-gen detectors.


<details>
  <summary>Details</summary>
Motivation: With next-generation gravitational wave detectors having enhanced sensitivity and higher event rates, computational demands for accurately estimating source parameters increase significantly, necessitating faster likelihood calculations.

Method: The paper uses a conditional variational auto-encoder model trained on $\sim10^5$ gravitational waveforms with defined parameter ranges for masses and spins, optimizing for fast and accurate waveform generation.

Result: The model achieves a median mismatch of $10^{-2}$ in tests, generates 100 waveforms in 0.1 seconds (significantly faster than existing implementations), and demonstrates strong performance in a more restricted parameter space.

Conclusion: This work demonstrates the potential of machine learning for improving the computational efficiency of gravitational waveform generation, laying groundwork towards production-ready frameworks for next-gen detector analysis.

Abstract: Upgrades to current gravitational wave detectors for the next observation run and upcoming third-generation observatories, like the Einstein telescope, are expected to have enormous improvements in detection sensitivities and compact object merger event rates. Estimation of source parameters for a wider parameter space that these detectable signals will lie in, will be a computational challenge. Thus, it is imperative to have methods to speed-up the likelihood calculations with theoretical waveform predictions, which can ultimately make the parameter estimation faster and aid in rapid multi-messenger follow-ups. Towards this end, we present a conditional variational auto-encoder model, based on the best performing architecture of Liao+2021, for faster generation of aligned-spin SEOBNRv4 inspiral-merger-ringdown waveforms. Our parameter space consists of four parameters, [$m_1$, $m_2$, $χ_1(z)$, $χ_2(z)$]. The masses are uniformly sampled in $[5,75]\,M_{\odot}$ with a mass ratio limit at $10\,M_{\odot}$, while the spins are uniform in $[-0.99,0.99]$. We train the model using $\sim10^5$ input waveforms data with a 70\%/10\% train/validation split, while 20\% data are reserved for testing. The median mismatch for the generated waveforms in the test dataset is $\sim10^{-2}$, with better performance in a restricted parameter space of $χ_{\rm eff}\in[-0.80,0.80]$. Our model is able to generate 100 waveforms in 0.1 second at an average speed of about 4.46 ms per waveform. This is 2-3 orders of magnitude faster than the native SEOBNRv4 implementation in lalsimulation. The latent sampling uncertainty of our model can be quantified with a mean mismatch deviation of $2\times10^{-1}$ for 1000 generations of the same waveform. Our work aims to be the first step towards developing a production-ready machine learning framework for the faster generation of gravitational waveform approximations.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [984] [QPU Micro-Kernels for Stencil Computation](https://arxiv.org/abs/2511.12617)
*Stefano Markidis,Luca Pennati,Marco Pasquale,Gilbert Netzer,Ivy Peng*

Main category: cs.ET

TL;DR: The paper introduces QPU micro-kernels that use shallow quantum circuits to solve PDEs by treating the QPU as a sampling accelerator for stencil node updates.


<details>
  <summary>Details</summary>
Motivation: To develop a practical approach for solving PDEs using quantum computing, which can leverage shallow circuits to address challenges encountered in monolithic quantum solvers.

Method: The authors propose QPU micro-kernels, which utilize shallow parameterized circuits with fixed resource footprints to perform stencil node updates and return samples. Two specific implementations, Bernoulli and branching micro-kernels, are presented, targeting specific stencil types.

Result: Experiments on noiseless simulators demonstrate accuracy improvement with increased samples, while tests on IBM's quantum hardware show that the Bernoulli micro-kernel yields lower errors compared to the branching implementation at similar shot budgets.

Conclusion: The QPU micro-kernel method is effective in solving PDEs with a shallow quantum circuit approach, balancing overheads and enabling parallelization while avoiding deep monolithic circuits.

Abstract: We introduce QPU micro-kernels: shallow quantum circuits that perform a stencil node update and return a Monte Carlo estimate from repeated measurements. We show how to use them to solve Partial Differential Equations (PDEs) explicitly discretized on a computational stencil. From this point of view, the QPU serves as a sampling accelerator. Each micro-kernel consumes only stencil inputs (neighbor values and coefficients), runs a shallow parameterized circuit, and reports the sample mean of a readout rule. The resource footprint in qubits and depth is fixed and independent of the global grid. This makes micro-kernels easy to orchestrate from a classical host and to parallelize across grid points. We present two realizations. The Bernoulli micro-kernel targets convex-sum stencils by encoding values as single-qubit probabilities with shot allocation proportional to stencil weights. The branching micro-kernel prepares a selector over stencil branches and applies addressed rotations to a single readout qubit. In contrast to monolithic quantum PDE solvers that encode the full space-time problem in one deep circuit, our approach keeps the classical time loop and offloads only local updates. Batching and in-circuit fusion amortize submission and readout overheads. We test and validate the QPU micro-kernel method on two PDEs commonly arising in scientific computing: the Heat and viscous Burgers' equations. On noiseless quantum circuit simulators, accuracy improves as the number of samples increases. On the IBM Brisbane quantum computer, single-step diffusion tests show lower errors for the Bernoulli realization than for branching at equal shot budgets, with QPU micro-kernel execution dominating the wall time.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [985] [Omics-scale polymer computational database transferable to real-world artificial intelligence applications](https://arxiv.org/abs/2511.11626)
*Ryo Yoshida,Yoshihiro Hayashi,Hidemine Furuya,Ryohei Hosoya,Kazuyoshi Kaneko,Hiroki Sugisawa,Yu Kaneko,Aiko Takahashi,Yoh Noguchi,Shun Nanjo,Keiko Shinoda,Tomu Hamakawa,Mitsuru Ohno,Takuya Kitamura,Misaki Yonekawa,Stephen Wu,Masato Ohnishi,Chang Liu,Teruki Tsurimoto,Arifin,Araki Wakiuchi,Kohei Noda,Junko Morikawa,Teruaki Hayakawa,Junichiro Shiomi,Masanobu Naito,Kazuya Shiratori,Tomoki Nagai,Norio Tomotsu,Hiroto Inoue,Ryuichi Sakashita,Masashi Ishii,Isao Kuwajima,Kenji Furuichi,Norihiko Hiroi,Yuki Takemoto,Takahiro Ohkuma,Keita Yamamoto,Naoya Kowatari,Masato Suzuki,Naoya Matsumoto,Seiryu Umetani,Hisaki Ikebata,Yasuyuki Shudo,Mayu Nagao,Shinya Kamada,Kazunori Kamio,Taichi Shomura,Kensaku Nakamura,Yudai Iwamizu,Atsutoshi Abe,Koki Yoshitomi,Yuki Horie,Katsuhiko Koike,Koichi Iwakabe,Shinya Gima,Kota Usui,Gikyo Usuki,Takuro Tsutsumi,Keitaro Matsuoka,Kazuki Sada,Masahiro Kitabata,Takuma Kikutsuji,Akitaka Kamauchi,Yusuke Iijima,Tsubasa Suzuki,Takenori Goda,Yuki Takabayashi,Kazuko Imai,Yuji Mochizuki,Hideo Doi,Koji Okuwaki,Hiroya Nitta,Taku Ozawa,Hitoshi Kamijima,Toshiaki Shintani,Takuma Mitamura,Massimiliano Zamengo,Yuitsu Sugami,Seiji Akiyama,Yoshinari Murakami,Atsushi Betto,Naoya Matsuo,Satoru Kagao,Tetsuya Kobayashi,Norie Matsubara,Shosei Kubo,Yuki Ishiyama,Yuri Ichioka,Mamoru Usami,Satoru Yoshizaki,Seigo Mizutani,Yosuke Hanawa,Shogo Kunieda,Mitsuru Yambe,Takeru Nakamura,Hiromori Murashima,Kenji Takahashi,Naoki Wada,Masahiro Kawano,Yosuke Harada,Takehiro Fujita,Erina Fujita,Ryoji Himeno,Hiori Kino,Kenji Fukumizu*

Main category: physics.chem-ph

TL;DR: This study introduces PolyOmics, a massive omics-scale computational database containing diverse physical properties for over 100,000 polymeric materials, enabling AI-driven polymer research.


<details>
  <summary>Details</summary>
Motivation: To address the lack of large-scale open polymer datasets crucial to advancing AI in polymer science, particularly given its vast and complex chemical space.

Method: Utilized fully automated molecular dynamics simulation pipelines, with the collaborative effort of 260 researchers across 48 institutions, to generate an omics-scale polymer database.

Result: The PolyOmics database enables robust machine learning model development, demonstrating improved accuracy and generalization for real-world applications through power-law scaling with database size.

Conclusion: PolyOmics establishes a foundational platform for AI-driven polymer science, revealing new opportunities in unexplored polymer material regions and emphasizing the importance of large-scale datasets.

Abstract: Developing large-scale foundational datasets is a critical milestone in advancing artificial intelligence (AI)-driven scientific innovation. However, unlike AI-mature fields such as natural language processing, materials science, particularly polymer research, has significantly lagged in developing extensive open datasets. This lag is primarily due to the high costs of polymer synthesis and property measurements, along with the vastness and complexity of the chemical space. This study presents PolyOmics, an omics-scale computational database generated through fully automated molecular dynamics simulation pipelines that provide diverse physical properties for over $10^5$ polymeric materials. The PolyOmics database is collaboratively developed by approximately 260 researchers from 48 institutions to bridge the gap between academia and industry. Machine learning models pretrained on PolyOmics can be efficiently fine-tuned for a wide range of real-world downstream tasks, even when only limited experimental data are available. Notably, the generalisation capability of these simulation-to-real transfer models improve significantly as the size of the PolyOmics database increases, exhibiting power-law scaling. The emergence of scaling laws supports the "more is better" principle, highlighting the significance of ultralarge-scale computational materials data for improving real-world prediction performance. This unprecedented omics-scale database reveals vast unexplored regions of polymer materials, providing a foundation for AI-driven polymer science.

</details>


### [986] [Socrates-Mol: Self-Oriented Cognitive Reasoning through Autonomous Trial-and-Error with Empirical-Bayesian Screening for Molecules](https://arxiv.org/abs/2511.11769)
*Xiangru Wang,Zekun Jiang,Heng Yang,Cheng Tan,Xingying Lan,Chunming Xu,Tianhang Zhou*

Main category: physics.chem-ph

TL;DR: Socrates-Mol enhances molecular property prediction using empirical Bayesian reasoning without fine-tuning. It improves scalability and accuracy for chemical engineering tasks.


<details>
  <summary>Details</summary>
Motivation: To address cold start problems in molecular property prediction and reduce deployment costs for industrial applications like solvent screening.

Method: Framework transforms language models into empirical Bayesian reasoners using context engineering and self-consistency mechanisms.

Result: Improved predictive accuracy in regression tasks with significant MAE reduction and R-squared improvements through self-consistency; limited gains in ranking tasks.

Conclusion: Socrates-Mol offers scalable molecular property prediction methods with task-adaptive capabilities, reducing costs while maintaining accuracy.

Abstract: Molecular property prediction is fundamental to chemical engineering applications such as solvent screening. We present Socrates-Mol, a framework that transforms language models into empirical Bayesian reasoners through context engineering, addressing cold start problems without model fine-tuning. The system implements a reflective-prediction cycle where initial outputs serve as priors, retrieved molecular cases provide evidence, and refined predictions form posteriors, extracting reusable chemical rules from sparse data. We introduce ranking tasks aligned with industrial screening priorities and employ cross-model self-consistency across five language models to reduce variance. Experiments on amine solvent LogP prediction reveal task-dependent patterns: regression achieves 72% MAE reduction and 112% R-squared improvement through self-consistency, while ranking tasks show limited gains due to systematic multi-model biases. The framework reduces deployment costs by over 70% compared to full fine-tuning, providing a scalable solution for molecular property prediction while elucidating the task-adaptive nature of self-consistency mechanisms.

</details>


### [987] [Chemistry-Enhanced Diffusion-Based Framework for Small-to-Large Molecular Conformation Generation](https://arxiv.org/abs/2511.12182)
*Yifei Zhu,Jiahui Zhang,Jiawei Peng,Mengge Li,Chao Xu,Zhenggang Lan*

Main category: physics.chem-ph

TL;DR: The paper presents StoL, a diffusion model-based framework for generating 3D molecular structures of large molecules from small-molecule data. It is efficient, knowledge-free, and scalable.


<details>
  <summary>Details</summary>
Motivation: The challenge of obtaining 3D conformations of large polyatomic molecules at the quantum chemistry level persists, requiring computationally intensive processes despite advancements in machine learning.

Method: StoL employs a diffusion model-based framework that decomposes molecules (from SMILES input) into fragments, generates their 3D structures using a diffusion model trained on small molecules, and reassembles them into larger structures.

Result: StoL successfully generates chemically valid and diverse 3D molecular conformations without requiring training data for large molecules, matching DFT-calculated structures with faster convergence.

Conclusion: The introduction of a fragment-based strategy allows StoL to efficiently and accurately predict complex molecular structures while being scalable, transferable, and independent of large-molecule training data.

Abstract: Obtaining 3D conformations of realistic polyatomic molecules at the quantum chemistry level remains challenging, and although recent machine learning advances offer promise, predicting large-molecule structures still requires substantial computational effort. Here, we introduce StoL, a diffusion model-based framework that enables rapid and knowledge-free generation of large molecular structures from small-molecule data. Remarkably, StoL assembles molecules in a LEGO-style fashion from scratch, without seeing the target molecules or any structures of comparable size during training. Given a SMILES input, it decomposes the molecule into chemically valid fragments, generates their 3D structures with a diffusion model trained on small molecules, and assembles them into diverse conformations. This fragment-based strategy eliminates the need for large-molecule training data while maintaining high scalability and transferability. By embedding chemical principles into key steps, StoL ensures faster convergence, chemically rational structures, and broad configurational coverage, as confirmed against DFT calculations.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [988] [Towards Mitigating Systematics in Large-Scale Surveys via Few-Shot Optimal Transport-Based Feature Alignment](https://arxiv.org/abs/2511.11787)
*Sultan Hassan,Sambatra Andrianomena,Benjamin D. Wandelt*

Main category: astro-ph.IM

TL;DR: The paper introduces a method to address systematics-induced distribution shifts using feature alignment between in-distribution (ID) and out-of-distribution (OOD) data through feature-alignment loss.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the challenge of systematics contaminating observables, which creates distribution shifts that make it hard for pre-trained models to label data in real-world conditions.

Method: The proposed method aligns learned features between ID and OOD samples using feature-alignment loss, tested experimentally with techniques like mean squared error and optimal transport on MNIST and neutral hydrogen maps.

Result: Optimal transport was particularly effective in aligning OOD features when there is limited data and unknown parity, addressing real-world conditions.

Conclusion: This approach proves successful at mitigating systematics-induced distribution shifts for better generalization in large-scale surveys.

Abstract: Systematics contaminate observables, leading to distribution shifts relative to theoretically simulated signals-posing a major challenge for using pre-trained models to label such observables. Since systematics are often poorly understood and difficult to model, removing them directly and entirely may not be feasible. To address this challenge, we propose a novel method that aligns learned features between in-distribution (ID) and out-of-distribution (OOD) samples by optimizing a feature-alignment loss on the representations extracted from a pre-trained ID model. We first experimentally validate the method on the MNIST dataset using possible alignment losses, including mean squared error and optimal transport, and subsequently apply it to large-scale maps of neutral hydrogen. Our results show that optimal transport is particularly effective at aligning OOD features when parity between ID and OOD samples is unknown, even with limited data-mimicking real-world conditions in extracting information from large-scale surveys. Our code is available at https://github.com/sultan-hassan/feature-alignment-for-OOD-generalization.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [989] [Protein Structure Tokenization via Geometric Byte Pair Encoding](https://arxiv.org/abs/2511.11758)
*Michael Sun,Weize Yuan,Gang Liu,Wojciech Matusik,Marinka Zitnik*

Main category: q-bio.QM

TL;DR: GeoBPE is a new protein structure tokenizer that improves proteomics modeling by transforming protein conformations into discrete representations using a hierarchical vocabulary based on geometric primitives, enabling better efficiency, generalization, and interpretability.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address the lack of effective protein structure tokenizers (PSTs) that can jointly handle sequence, structure, and function in proteins. Current methods lack interpretability, multi-scale control, and cross-architecture transferability.

Method: GeoBPE utilizes a geometry-grounded process to tokenize protein structures by combining clustering with k-medoids, quantization, and optimization of boundary angles. This processing creates a hierarchical vocabulary of discrete geometric representations while adhering to global constraints.

Result: GeoBPE achieves significant improvements in data compression (10x reduction in bits-per-residue), data efficiency (10x less training data), and generalization (test/train distortion ratios remain stable) while being adaptable to various architectures and outperforming leading PST methods in multiple tasks.

Conclusion: GeoBPE offers a significant advancement in protein modeling by providing interpretable, flexible, and efficient representations of protein structures that improve downstream tasks and connect functionality with structural representation.

Abstract: Protein structure is central to biological function, and enabling multimodal protein models requires joint reasoning over sequence, structure, and function. A key barrier is the lack of principled protein structure tokenizers (PSTs): existing approaches fix token size or rely on continuous vector codebooks, limiting interpretability, multi-scale control, and transfer across architectures. We introduce GeoBPE, a geometry-grounded PST that transforms continuous, noisy, multi-scale backbone conformations into discrete ``sentences'' of geometry while enforcing global constraints. Analogous to byte-pair encoding, GeoBPE generates a hierarchical vocabulary of geometric primitives by iteratively (i) clustering Geo-Pair occurrences with k-medoids to yield a resolution-controllable vocabulary; (ii) quantizing each Geo-Pair to its closest medoid prototype; and (iii) reducing drift through differentiable inverse kinematics that optimizes boundary glue angles under an $\mathrm{SE}(3)$ end-frame loss. GeoBPE offers compression ($>$10x reduction in bits-per-residue at similar distortion rate), data efficiency ($>$10x less training data), and generalization (maintains test/train distortion ratio of $1.0-1.1$). It is architecture-agnostic: (a) its hierarchical vocabulary provides a strong inductive bias for coarsening residue-level embeddings from large PLMs into motif- and protein-level representations, consistently outperforming leading PSTs across $12$ tasks and $24$ test splits; (b) paired with a transformer, GeoBPE supports unconditional backbone generation via language modeling; and (c) tokens align with CATH functional families and support expert-interpretable case studies, offering functional meaning absent in prior PSTs. Code is available at https://github.com/shiningsunnyday/PT-BPE/.

</details>


### [990] [Causal Inference, Biomarker Discovery, Graph Neural Network, Feature Selection](https://arxiv.org/abs/2511.13295)
*Chaowang Lan,Jingxin Wu,Yulong Yuan,Chuxun Liu,Huangyi Kang,Caihua Liu*

Main category: q-bio.QM

TL;DR: The paper introduces a novel Causal-GNN method that combines causal inference with graph neural networks for stable biomarker discovery, demonstrating improved predictive accuracy and stability across datasets.


<details>
  <summary>Details</summary>
Motivation: Current biomarker discovery methods lack robustness as they ignore gene regulatory relationships and conflate causation with correlation, leading to unstable results across datasets.

Method: The authors propose 'Causal-GNN', a hybrid method using causal effect estimation techniques and graph neural networks that incorporate gene regulatory networks for reliable biomarker discovery.

Result: The method exhibits high predictive accuracy across four datasets and independent classifiers, while identifying more stable biomarkers than traditional approaches.

Conclusion: Causal-GNN is a robust, biologically interpretable tool for biomarker discovery, with high potential for application in precision medicine and other medical fields.

Abstract: Biomarker discovery from high-throughput transcriptomic data is crucial for advancing precision medicine. However, existing methods often neglect gene-gene regulatory relationships and lack stability across datasets, leading to conflation of spurious correlations with genuine causal effects. To address these issues, we develop a causal graph neural network (Causal-GNN) method that integrates causal inference with multi-layer graph neural networks (GNNs). The key innovation is the incorporation of causal effect estimation for identifying stable biomarkers, coupled with a GNN-based propensity scoring mechanism that leverages cross-gene regulatory networks. Experimental results demonstrate that our method achieves consistently high predictive accuracy across four distinct datasets and four independent classifiers. Moreover, it enables the identification of more stable biomarkers compared to traditional methods. Our work provides a robust, efficient, and biologically interpretable tool for biomarker discovery, demonstrating strong potential for broad application across medical disciplines.

</details>


<div id='hep-ex'></div>

# hep-ex [[Back]](#toc)

### [991] [Knowledge is Overrated: A zero-knowledge machine learning and cryptographic hashing-based framework for verifiable, low latency inference at the LHC](https://arxiv.org/abs/2511.12592)
*Pratik Jawahar,Caterina Doglioni,Maurizio Pierini*

Main category: hep-ex

TL;DR: The paper proposes PHAZE, a framework using cryptographic techniques like hashing and zkML, to reduce latency for machine learning inference in LHC triggers.


<details>
  <summary>Details</summary>
Motivation: Latency constraints in LHC operations (40 MHz) limit the incorporation of modern machine learning models in trigger algorithms due to their slow inference speeds.

Method: PHAZE framework employs cryptographic techniques, including hashing and zero-knowledge ML, to enable early-exits and achieve low-latency inference while maintaining model performance.

Result: The proposed system is envisioned to potentially achieve nanosecond-order latencies along with additional benefits like anomaly detection.

Conclusion: PHAZE lays the groundwork for integrating AI in LHC triggers by overcoming latency challenges and adding features like anomaly detection and dynamic low-level triggering.

Abstract: Low latency event-selection (trigger) algorithms are essential components of Large Hadron Collider (LHC) operation. Modern machine learning (ML) models have shown great offline performance as classifiers and could improve trigger performance, thereby improving downstream physics analyses. However, inference on such large models does not satisfy the $40\text{MHz}$ online latency constraint at the LHC. In this work, we propose \texttt{PHAZE}, a novel framework built on cryptographic techniques like hashing and zero-knowledge machine learning (zkML) to achieve low latency inference, via a certifiable, early-exit mechanism from an arbitrarily large baseline model. We lay the foundations for such a framework to achieve nanosecond-order latency and discuss its inherent advantages, such as built-in anomaly detection, within the scope of LHC triggers, as well as its potential to enable a dynamic low-level trigger in the future.

</details>


### [992] [NuBench: An Open Benchmark for Deep Learning-Based Event Reconstruction in Neutrino Telescopes](https://arxiv.org/abs/2511.13111)
*Rasmus F. Orsoe,Stephan Meighen-Berger,Jeffrey Lazar,Jorge Prado,Ivan Mozun-Mateo,Aske Rosted,Philip Weigel,Arturo Llorente Anaya*

Main category: hep-ex

TL;DR: This paper introduces NuBench, an open benchmark with datasets for developing and comparing deep learning-based event reconstruction methods in neutrino telescopes.


<details>
  <summary>Details</summary>
Motivation: To address the lack of diverse, open-source datasets that hinders cross-experimental collaboration in neutrino telescope event reconstruction, and to take advantage of recent developments in deep learning techniques for solving such problems.

Method: NuBench provides seven simulated datasets containing ~130 million muon-neutrino interactions, representing different detector geometries. It enables comparison and development of deep learning-based event reconstruction methods.

Result: NuBench evaluates four reconstruction algorithms (ParticleNeT, DynEdge, GRIT, DeepIce) on five tasks: energy/direction reconstruction, topology classification, vertex prediction, and inelasticity estimation.

Conclusion: NuBench makes progress toward unifying and advancing event reconstruction research by offering valuable datasets for testing and enhancing machine-learning methods in diverse neutrino telescope configurations.

Abstract: Neutrino telescopes are large-scale detectors designed to observe Cherenkov radiation produced from neutrino interactions in water or ice. They exist to identify extraterrestrial neutrino sources and to probe fundamental questions pertaining to the elusive neutrino itself. A central challenge common across neutrino telescopes is to solve a series of inverse problems known as event reconstruction, which seeks to resolve properties of the incident neutrino, based on the detected Cherenkov light. In recent times, significant efforts have been made in adapting advances from deep learning research to event reconstruction, as such techniques provide several benefits over traditional methods. While a large degree of similarity in reconstruction needs and low-level data exists, cross-experimental collaboration has been hindered by a lack of diverse open-source datasets for comparing methods.
  We present NuBench, an open benchmark for deep learning-based event reconstruction in neutrino telescopes. NuBench comprises seven large-scale simulated datasets containing nearly 130 million charged- and neutral-current muon-neutrino interactions spanning 10 GeV to 100 TeV, generated across six detector geometries inspired by existing and proposed experiments. These datasets provide pulse- and event-level information suitable for developing and comparing machine-learning reconstruction methods in both water and ice environments. Using NuBench, we evaluate four reconstruction algorithms - ParticleNeT and DynEdge, both actively used within the KM3NeT and IceCube collaborations, respectively, along with GRIT and DeepIce - on up to five core tasks: energy and direction reconstruction, topology classification, interaction vertex prediction, and inelasticity estimation.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [993] [How Far Do SSL Speech Models Listen for Tone? Temporal Focus of Tone Representation under Low-resource Transfer](https://arxiv.org/abs/2511.12285)
*Minu Kim,Ji Sub Um,Hoirin Kim*

Main category: eess.AS

TL;DR: The study examines how self-supervised learning (SSL) speech models process lexical tones in Burmese, Thai, Lao, and Vietnamese. It finds that how SSL models handle tones depends on downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Many languages rely on lexical tones, but these tones are underexplored in SSL models, especially in languages beyond Mandarin. The study seeks to understand tone processing and transfer in low-resource languages.

Method: The authors analyze tone processing by SSL models using probes and gradient analyses, estimating tone cue durations and examining the impact of task-specific fine-tuning in different tonal languages.

Result: Tone cue spans are around 100 ms in Burmese and Thai, and 180 ms in Lao and Vietnamese. Fine-tuning for speech recognition aligns models with tone cues, while other tasks like prosody cause models to over-focus on longer spans.

Conclusion: Task-specific fine-tuning shapes how SSL models process tones, emphasizing the influence of downstream tasks on temporal modeling of lexical tones.

Abstract: Lexical tone is central to many languages but remains underexplored in self-supervised learning (SSL) speech models, especially beyond Mandarin. We study four languages with complex and diverse tone systems: Burmese, Thai, Lao, and Vietnamese, to examine how far such models listen for tone and how transfer operates in low-resource conditions. As a baseline reference, we estimate the temporal span of tone cues to be about 100 ms in Burmese and Thai, and about 180 ms in Lao and Vietnamese. Probes and gradient analyses on fine-tuned SSL models reveal that tone transfer varies by downstream task: automatic speech recognition fine-tuning aligns spans with language-specific tone cues, while prosody- and voice-related tasks bias the model toward overly long spans. These findings indicate that tone transfer is shaped by downstream task, highlighting task effects on temporal focus in tone modeling.

</details>


### [994] [VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing](https://arxiv.org/abs/2511.12347)
*Zhisheng Zheng,Puyuan Peng,Anuj Diwan,Cong Phuoc Huynh,Xiaohang Sun,Zhu Liu,Vimal Bhat,David Harwath*

Main category: eess.AS

TL;DR: VoiceCraft-X is an autoregressive model that unifies multilingual speech editing and zero-shot Text-to-Speech (TTS) for 11 languages through a combined approach of text and speech generation.


<details>
  <summary>Details</summary>
Motivation: Develop a tool capable of seamless multilingual speech editing and synthesis, addressing tasks like TTS and natural audio creation across diverse languages.

Method: Integrates Qwen3's phoneme-free processing, a token reordering mechanism, and time-aligned tokens for handling text and audio as a single task.

Result: Achieved high-quality, natural-sounding speech synthesis and editing in multiple languages, even with limited data per language.

Conclusion: Unified autoregressive approaches, like VoiceCraft-X, demonstrate significant potential for diverse real-world multilingual speech applications.

Abstract: We introduce VoiceCraft-X, an autoregressive neural codec language model which unifies multilingual speech editing and zero-shot Text-to-Speech (TTS) synthesis across 11 languages: English, Mandarin, Korean, Japanese, Spanish, French, German, Dutch, Italian, Portuguese, and Polish. VoiceCraft-X utilizes the Qwen3 large language model for phoneme-free cross-lingual text processing and a novel token reordering mechanism with time-aligned text and speech tokens to handle both tasks as a single sequence generation problem. The model generates high-quality, natural-sounding speech, seamlessly creating new audio or editing existing recordings within one framework. VoiceCraft-X shows robust performance in diverse linguistic settings, even with limited per-language data, underscoring the power of unified autoregressive approaches for advancing complex, real-world multilingual speech applications. Audio samples are available at https://zhishengzheng.com/voicecraft-x/.

</details>


### [995] [Systematic evaluation of time-frequency features for binaural sound source localization](https://arxiv.org/abs/2511.13487)
*Davoud Shariat Panah,Alessandro Ragano,Dan Barry,Jan Skoglund,Andrew Hines*

Main category: eess.AS

TL;DR: This paper analyzes various time-frequency features for binaural sound source localization (SSL), highlighting the importance of feature selection over model complexity.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of feature selection on binaural SSL performance and to optimize for diverse conditions.

Method: A systematic evaluation using a CNN model with combinations of amplitude-based and phase-based features, tested across in-domain and out-of-domain datasets.

Result: Optimal feature sets outperform complex models; ILD + IPD works well in-domain, while richer combinations (e.g., channel spectrograms + ILD/IPD) generalize better to diverse conditions.

Conclusion: Feature design is crucial for effective SSL; insights provided guide domain-specific and general-purpose applications.

Abstract: This study presents a systematic evaluation of time-frequency feature design for binaural sound source localization (SSL), focusing on how feature selection influences model performance across diverse conditions. We investigate the performance of a convolutional neural network (CNN) model using various combinations of amplitude-based features (magnitude spectrogram, interaural level difference - ILD) and phase-based features (phase spectrogram, interaural phase difference - IPD). Evaluations on in-domain and out-of-domain data with mismatched head-related transfer functions (HRTFs) reveal that carefully chosen feature combinations often outperform increases in model complexity. While two-feature sets such as ILD + IPD are sufficient for in-domain SSL, generalization to diverse content requires richer inputs combining channel spectrograms with both ILD and IPD. Using the optimal feature sets, our low-complexity CNN model achieves competitive performance. Our findings underscore the importance of feature design in binaural SSL and provide practical guidance for both domain-specific and general-purpose localization.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [996] [AI-Open-RAN for Non-Terrestrial Networks](https://arxiv.org/abs/2511.11947)
*Tri Nhu Do*

Main category: eess.SP

TL;DR: The paper presents a unified All-In-One Radio Access Network model for Non-Terrestrial Networks, combining open interfaces and AI capabilities for enhanced interoperability and performance in future telecommunications.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges of interoperability, flexibility, and intelligence in next-generation telecommunications, particularly focusing on Non-Terrestrial Networks.

Method: The authors provide an architecture overview for Open-RAN and AI-RAN, introduce an integrated AIO-RAN-NTN blueprint, and perform experiments using a 5G testbed and AI forecasting based on realistic data.

Result: Experimental findings reveal that mobility impacts the AIO-based standalone architecture’s performance, but AI-driven forecasting mitigates these limitations effectively.

Conclusion: The proposed AIO-RAN-NTN model enhances adaptability and performance in NTNs, with AI playing a crucial role in mitigating mobility-related sensitivity issues.

Abstract: In this paper, we propose the concept of AIO-RAN-NTN, a unified all-in-one Radio Access Network (RAN) for Non-Terrestrial Networks (NTNs), built on an open architecture that leverages open interfaces and artificial intelligence (AI)-based functionalities. This approach advances interoperability, flexibility, and intelligence in next-generation telecommunications. First, we provide a concise overview of the state-of-the-art architectures for Open-RAN and AI-RAN, highlighting key network functions and infrastructure elements. Next, we introduce our integrated AIO-RAN-NTN blueprint, emphasizing how internal and air interfaces from AIO-RAN and the 3rd Generation Partnership Project (3GPP) can be applied to emerging environments such as NTNs. To examine the impact of mobility on AIO-RAN, we implement a testbed transmission using the OpenAirInterface platform for a standalone (SA) New Radio (NR) 5G system. We then train an AI model on realistic data to forecast key performance indicators (KPIs). Our experiments demonstrate that the AIO-based SA architecture is sensitive to mobility, even at low speeds, but this limitation can be mitigated through AI-driven KPI forecasting.

</details>


### [997] [Temporal Micro-Doppler Spectrogram-based ViT Multiclass Target Classification](https://arxiv.org/abs/2511.11951)
*Nghia Thinh Nguyen,Tri Nhu Do*

Main category: eess.SP

TL;DR: The paper proposes the T-MDS-ViT, a transformer-based model for better classification of targets using radar micro-Doppler spectrograms.


<details>
  <summary>Details</summary>
Motivation: The need for efficient and accurate multiclass target classification using radar micro-Doppler spectrograms, especially addressing challenges like overlaps and occlusions.

Method: Designing a transformer architecture with stacked spatiotemporal data, cross-axis attention, mobility-aware constraints, and an explainable mechanism targeting key spectral regions.

Result: Improved classification accuracy, better data efficiency, and real-time deployability compared to CNN-based models.

Conclusion: T-MDS-ViT effectively models and classifies radar spectrogram data, proving superior in handling complex scenarios such as occlusions.

Abstract: In this paper, we propose a new Temporal MDS-Vision Transformer (T-MDS-ViT) for multiclass target classification using millimeter-wave FMCW radar micro-Doppler spectrograms. Specifically, we design a transformer-based architecture that processes stacked range-velocity-angle (RVA) spatiotemporal tensors via patch embeddings and cross-axis attention mechanisms to explicitly model the sequential nature of MDS data across multiple frames. The T-MDS-ViT exploits mobility-aware constraints in its attention layer correspondences to maintain separability under target overlaps and partial occlusions. Next, we apply an explainable mechanism to examine how the attention layers focus on characteristic high-energy regions of the MDS representations and their effect on class-specific kinematic features. We also demonstrate that our proposed framework is superior to existing CNN-based methods in terms of classification accuracy while achieving better data efficiency and real-time deployability.

</details>


### [998] [Informed Bootstrap Augmentation Improves EEG Decoding](https://arxiv.org/abs/2511.12073)
*Woojae Jeong,Wenhui Cui,Kleanthis Avramidis,Takfarinas Medani,Shrikanth Narayanan,Richard Leahy*

Main category: eess.SP

TL;DR: The paper introduces a weighted bootstrapping method for EEG data augmentation, improving decoding accuracy and emphasizing reliable trials.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of EEG decoding performance caused by noise and trial variability.

Method: Weighted bootstrapping using relative ERP differences for selecting and augmenting reliable trials.

Result: Decoding accuracy improved significantly, from 68.35% to 71.25%, using the proposed method.

Conclusion: Focusing on reliable trials enhances EEG representation quality and performance for complex paradigms.

Abstract: Electroencephalography (EEG) offers detailed access to neural dynamics but remains constrained by noise and trial-by-trial variability, limiting decoding performance in data-restricted or complex paradigms. Data augmentation is often employed to enhance feature representations, yet conventional uniform averaging overlooks differences in trial informativeness and can degrade representational quality. We introduce a weighted bootstrapping approach that prioritizes more reliable trials to generate higher-quality augmented samples. In a Sentence Evaluation paradigm, weights were computed from relative ERP differences and applied during probabilistic sampling and averaging. Across conditions, weighted bootstrapping improved decoding accuracy relative to unweighted (from 68.35% to 71.25% at best), demonstrating that emphasizing reliable trials strengthens representational quality. The results demonstrate that reliability-based augmentation yields more robust and discriminative EEG representations. The code is publicly available at https://github.com/lyricists/NeuroBootstrap.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [999] [Perturbing Best Responses in Zero-Sum Games](https://arxiv.org/abs/2511.12523)
*Adam Dziwoki,Rostislav Horcik*

Main category: cs.GT

TL;DR: The paper examines perturbations' impact on algorithms approximating Nash equilibria in zero-sum games, finding they can reduce iterations, including logarithmic improvements in some cases.


<details>
  <summary>Details</summary>
Motivation: To explore how perturbing utilities in best-response algorithms can improve efficiency in finding Nash equilibria for zero-sum games.

Method: The paper assumes perturbed utility computation before best response selection and analyzes its effect on Double Oracle and Fictitious Play algorithms.

Result: Perturbations reduce the number of iterations for Nash equilibrium algorithms, with logarithmic improvements in certain situations.

Conclusion: Utility perturbations can enhance computational efficiency, particularly when strategies have inner structures allowing efficient perturbations.

Abstract: This paper investigates the impact of perturbations on the best-response-based algorithms approximating Nash equilibria in zero-sum games, namely Double Oracle and Fictitious Play. More precisely, we assume that the oracle computing the best responses perturbs the utilities before selecting the best response. We show that using such an oracle reduces the number of iterations for both algorithms. For some cases, suitable perturbations ensure the expected number of iterations is logarithmic. Although the utility perturbation is computationally demanding as it requires iterating through all pure strategies, we demonstrate that one can efficiently perturb the utilities in games where pure strategies have further inner structure.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [1000] [Towards Quantum Software for Quantum Simulation](https://arxiv.org/abs/2511.13520)
*Maja Franz,Lukas Schmidbauer,Joshua Ammermann,Ina Schaefer,Wolfgang Mauerer*

Main category: quant-ph

TL;DR: The paper discusses the limitations in current quantum simulation tools and proposes a modular framework for scalable, cross-platform workflows.


<details>
  <summary>Details</summary>
Motivation: To address shortcomings in quantum simulation infrastructure, particularly the absence of general-purpose frameworks for scalable workflows.

Method: Advocates for a modular model-driven engineering (MDE) approach to provide flexible frameworks for various quantum simulation types.

Result: Illustrates the concept with a high-energy physics example, demonstrating its potential scalability and cross-platform adaptability.

Conclusion: The paper identifies gaps in quantum simulation and proposes solutions to advance quantum simulation tools towards broader applicability and automation.

Abstract: Quantum simulation is a leading candidate for demonstrating practical quantum advantage over classical computation, as it is believed to provide exponentially more compute power than any classical system. It offers new means of studying the behaviour of complex physical systems, for which conventionally software-intensive simulation codes based on numerical high-performance computing are used. Instead, quantum simulations map properties and characteristics of subject systems, for instance chemical molecules, onto quantum devices that then mimic the system under study.
  Currently, the use of these techniques is largely limited to fundamental science, as the overall approach remains tailored for specific problems: We lack infrastructure and modelling abstractions that are provided by the software engineering community for other computational domains.
  In this paper, we identify critical gaps in the quantum simulation software stack-particularly the absence of general-purpose frameworks for model specification, Hamiltonian construction, and hardware-aware mappings. We advocate for a modular model-driven engineering (MDE) approach that supports different types of quantum simulation (digital and analogue), and facilitates automation, performance evaluation, and reusability. Through an example from high-energy physics, we outline a vision for a quantum simulation framework capable of supporting scalable, cross-platform simulation workflows.

</details>


### [1001] [Reinforcement Learning for Charging Optimization of Inhomogeneous Dicke Quantum Batteries](https://arxiv.org/abs/2511.12176)
*Xiaobin Song,Siyuan Bai,Da-Wei Wang,Hanxiao Tao,Xizhe Wang,Rebing Wu,Benben Jiang*

Main category: quant-ph

TL;DR: This paper uses reinforcement learning to optimize charging policies for quantum batteries, showing how partial observability affects efficiency and highlighting the importance of second-order correlations.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of optimizing quantum battery charging under conditions of inhomogeneity and partial state observability.

Method: Reinforcement learning is applied to design optimal piecewise-constant charging policies for the inhomogeneous Dicke battery under different observability regimes.

Result: The study finds that access to full-state observations allows for near-optimal charging performance, while incorporating second-order correlations under partial observability significantly improves results. Nonmyopic strategies facilitate superior terminal outcomes.

Conclusion: This paper underscores the utility of reinforcement learning and second-order correlations in developing practical fast-charging protocols for quantum systems with limited observability.

Abstract: Charging optimization is a key challenge to the implementation of quantum batteries, particularly under inhomogeneity and partial observability. This paper employs reinforcement learning to optimize piecewise-constant charging policies for an inhomogeneous Dicke battery. We systematically compare policies across four observability regimes, from full-state access to experimentally accessible observables (energies of individual two-level systems (TLSs), first-order averages, and second-order correlations). Simulation results demonstrate that full observability yields near-optimal ergotropy with low variability, while under partial observability, access to only single-TLS energies or energies plus first-order averages lags behind the fully observed baseline. However, augmenting partial observations with second-order correlations recovers most of the gap, reaching 94%-98% of the full-state baseline. The learned schedules are nonmyopic, trading temporary plateaus or declines for superior terminal outcomes. These findings highlight a practical route to effective fast-charging protocols under realistic information constraints.

</details>


### [1002] [Quantum Optimization Algorithms](https://arxiv.org/abs/2511.12379)
*Jonas Stein,Maximilian Zorn,Leo Sünkel,Thomas Gabor*

Main category: quant-ph

TL;DR: The paper focuses on the Quantum Approximate Optimization Algorithm (QAOA) and its applications, including quantum circuit implementation, constraints using Grover mixers, and its generalization through the Variational Quantum Eigensolver (VQE).


<details>
  <summary>Details</summary>
Motivation: To investigate quantum optimization algorithms like QAOA and how they might provide significant computational speedups, especially in the context of industrially relevant problems.

Method: The paper discusses QAOA in detail, covering its Hamiltonian simulation techniques, parameter training, and implementation using tools such as Pennylane. It also explores adding constraints with Grover mixers and generalization to the VQE.

Result: A practical demonstration of QAOA is shown with an application to the Maximum Cut problem, along with techniques to extend QAOA’s functionality to constrained optimization and VQE.

Conclusion: QAOA shows promise for quantum optimization, especially on NISQ devices. Its extension to VQE broadens its applicability, though challenges like barren plateaus remain.

Abstract: Quantum optimization allows for up to exponential quantum speedups for specific, possibly industrially relevant problems. As the key algorithm in this field, we motivate and discuss the Quantum Approximate Optimization Algorithm (QAOA), which can be understood as a slightly generalized version of Quantum Annealing for gate-based quantum computers. We delve into the quantum circuit implementation of the QAOA, including Hamiltonian simulation techniques for higher-order Ising models, and discuss parameter training using the parameter shift rule. An example implementation with Pennylane source code demonstrates practical application for the Maximum Cut problem. Further, we show how constraints can be incorporated into the QAOA using Grover mixers, allowing to restrict the search space to strictly valid solutions for specific problems. Finally, we outline the Variational Quantum Eigensolver (VQE) as a generalization of the QAOA, highlighting its potential in the NISQ era and addressing challenges such as barren plateaus and ansatz design.

</details>


### [1003] [Limitations of Quantum Advantage in Unsupervised Machine Learning](https://arxiv.org/abs/2511.10709)
*Apoorva D. Patel*

Main category: quant-ph

TL;DR: The paper explores quantum extensions to machine learning models in unsupervised learning and identifies constraints on quantum advantage based on data and observable targets.


<details>
  <summary>Details</summary>
Motivation: The authors aim to investigate potential quantum advantages in unsupervised machine learning by replacing classical probability distributions with quantum density matrices.

Method: The paper discusses examples and scenarios to determine the constraints and conditions under which quantum density matrices provide advantages over classical models.

Result: It demonstrates that quantum advantages are conditional, depending on the input data and target observables.

Conclusion: Quantum capabilities in unsupervised learning are not universally superior, and their advantages are limited to specific cases, impacting data analysis and sensing applications.

Abstract: Machine learning models are used for pattern recognition analysis of big data, without direct human intervention. The task of unsupervised learning is to find the probability distribution that would best describe the available data, and then use it to make predictions for observables of interest. Classical models generally fit the data to Boltzmann distribution of Hamiltonians with a large number of tunable parameters. Quantum extensions of these models replace classical probability distributions with quantum density matrices. An advantage can be obtained only when features of density matrices that are absent in classical probability distributions are exploited. Such situations depend on the input data as well as the targeted observables. Explicit examples are discussed that bring out the constraints limiting possible quantum advantage. The problem-dependent extent of quantum advantage has implications for both data analysis and sensing applications.

</details>


### [1004] [Discovering autonomous quantum error correction via deep reinforcement learning](https://arxiv.org/abs/2511.12482)
*Yue Yin,Tailong Xiao,Xiaoyang Deng,Ming He,Jianping Fan,Guihua Zeng*

Main category: quant-ph

TL;DR: The paper proposes utilizing curriculum learning-enabled deep reinforcement learning to discover Bosonic codes for autonomous quantum error correction (AQEC), achieving superior performance against photon loss and noise.


<details>
  <summary>Details</summary>
Motivation: Standard quantum error correction methods relying on active measurements can introduce errors. Autonomous quantum error correction using engineered dissipation and drives in bosonic systems offers a solution but struggles to identify practical encodings due to stringent conditions.

Method: They employ deep reinforcement learning with curriculum learning to discover optimal Bosonic codes under approximate AQEC frameworks, resisting photon losses. Analytical solutions to approximate master equations accelerate the reinforcement learning training process.

Result: The trained agent identified optimal codewords in Fock states ($\ket{4}$ and $\ket{7}$) for resisting single-photon and double-photon losses, surpassing breakeven thresholds over extended periods, achieving state-of-the-art performance, and showing robustness against phase and amplitude damping noise.

Conclusion: The approach demonstrates the potential for curriculum learning-enabled deep reinforcement learning in finding optimal quantum error correction codes, particularly in early fault-tolerant quantum systems.

Abstract: Quantum error correction is essential for fault-tolerant quantum computing. However, standard methods relying on active measurements may introduce additional errors. Autonomous quantum error correction (AQEC) circumvents this by utilizing engineered dissipation and drives in bosonic systems, but identifying practical encoding remains challenging due to stringent Knill-Laflamme conditions. In this work, we utilize curriculum learning enabled deep reinforcement learning to discover Bosonic codes under approximate AQEC framework to resist both single-photon and double-photon losses. We present an analytical solution of solving the master equation under approximation conditions, which can significantly accelerate the training process of reinforcement learning. The agent first identifies an encoded subspace surpassing the breakeven point through rapid exploration within a constrained evolutionary time-frame, then strategically fine-tunes its policy to sustain this performance advantage over extended temporal horizons. We find that the two-phase trained agent can discover the optimal set of codewords, i.e., the Fock states $\ket{4}$ and $\ket{7}$ considering the effect of both single-photon and double-photon loss. We identify that the discovered code surpasses the breakeven threshold over a longer evolution time and achieve the state-of-art performance. We also analyze the robustness of the code against the phase damping and amplitude damping noise. Our work highlights the potential of curriculum learning enabled deep reinforcement learning in discovering the optimal quantum error correct code especially in early fault-tolerant quantum systems.

</details>


### [1005] [Taming Barren Plateaus in Arbitrary Parameterized Quantum Circuits Without Sacrificing Expressibility](https://arxiv.org/abs/2511.13408)
*Zhenyu Chen,Yuguo Shao,Zhengwei Liu,Zhaohui Wei*

Main category: quant-ph

TL;DR: The paper presents a method to address the "barren plateaus" issue in parameterized quantum circuits (PQCs) by introducing hardware-efficient quantum channels, achieving robust training and applicability on noisy quantum hardware.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of barren plateaus in PQC architectures, which hinder effective parameter optimization due to exponential loss concentration.

Method: The proposed method involves inserting layers of quantum channels in the PQC that require minimal hardware resources (one ancilla qubit and four gates), forming a modified PQC (MPQC) that avoids barren plateaus.

Result: The method eliminates barren plateaus in PQCs, enables trainability of all parameters under mild assumptions, and works effectively even with realistic noise. Numerical demonstrations show improved training up to 100 qubits and 2400 layers.

Conclusion: The study provides a practical solution to barren plateaus in PQCs, ensuring robust optimization and compatibility with current noisy quantum hardware.

Abstract: Quantum algorithms based on parameterized quantum circuits (PQCs) have enabled a wide range of applications on near-term quantum devices. However, existing PQC architectures face several challenges, among which the ``barren plateaus" phenomenon is particularly prominent. In such cases, the loss function concentrates exponentially with increasing system size, thereby hindering effective parameter optimization. To address this challenge, we propose a general and hardware-efficient method for eliminating barren plateaus in an arbitrary PQC. Specifically, our approach achieves this by inserting a layer of easily implementable quantum channels into the original PQC, each channel requiring only one ancilla qubit and four additional gates, yielding a modified PQC (MPQC) that is provably at least as expressive as the original PQC and, under mild assumptions, is guaranteed to be free from barren plateaus. Furthermore, by appropriately adjusting the structure of MPQCs, we rigorously prove that any parameter in the original PQC can be made trainable. Importantly, the absence of barren plateaus in MPQCs is robust against realistic noise, making our approach directly applicable to current noisy intermediate-scale quantum (NISQ) hardware. Numerically, we demonstrate the practicality of our method by modifying a commonly used PQC for thermal-state preparation. The results show that {barren plateaus are effectively eliminated} in this class of circuits with up to 100 qubits and 2400 layers, whereas the original ansatz suffers from severe gradient vanishing.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [1006] [Bregman geometry-aware split Gibbs sampling for Bayesian Poisson inverse problems](https://arxiv.org/abs/2511.12257)
*Elhadji Cisse Faye,Mame Diarra Fall,Nicolas Dobigeon,Eric Barat*

Main category: stat.CO

TL;DR: This paper introduces a Bayesian framework with a Monte Carlo sampling method tailored for Poisson inverse problems, addressing issues like non-Euclidean geometries and positivity constraints.


<details>
  <summary>Details</summary>
Motivation: To overcome difficulties in solving Poisson inverse problems arising from non-Lipschitz gradients and positivity constraints, and to improve reconstruction quality.

Method: The paper employs data augmentation with Bregman divergence and Burg entropy, leveraging Gibbs sampling and an advanced Hessian Riemannian Langevin Monte Carlo algorithm for sampling efficiency.

Result: The proposed method shows competitive reconstruction performance in tasks like denoising, deblurring, and PET imaging.

Conclusion: Bayesian modeling combined with efficient sampling algorithms can address Poisson inverse challenges and enhance image reconstruction quality.

Abstract: This paper proposes a novel Bayesian framework for solving Poisson inverse problems by devising a Monte Carlo sampling algorithm which accounts for the underlying non-Euclidean geometry. To address the challenges posed by the Poisson likelihood -- such as non-Lipschitz gradients and positivity constraints -- we derive a Bayesian model which leverages exact and asymptotically exact data augmentations. In particular, the augmented model incorporates two sets of splitting variables both derived through a Bregman divergence based on the Burg entropy. Interestingly the resulting augmented posterior distribution is characterized by conditional distributions which benefit from natural conjugacy properties and preserve the intrinsic geometry of the latent and splitting variables. This allows for efficient sampling via Gibbs steps, which can be performed explicitly for all conditionals, except the one incorporating the regularization potential. For this latter, we resort to a Hessian Riemannian Langevin Monte Carlo (HRLMC) algorithm which is well suited to handle priors with explicit or easily computable score functions. By operating on a mirror manifold, this Langevin step ensures that the sampling satisfies the positivity constraints and more accurately reflects the underlying problem structure. Performance results obtained on denoising, deblurring, and positron emission tomography (PET) experiments demonstrate that the method achieves competitive performance in terms of reconstruction quality compared to optimization- and sampling-based approaches.

</details>


<div id='q-bio.MN'></div>

# q-bio.MN [[Back]](#toc)

### [1007] [Practical Causal Evaluation Metrics for Biological Networks](https://arxiv.org/abs/2511.12805)
*Noriaki Sato,Marco Scutari,Shuichi Kawano,Rui Yamaguchi,Seiya Imoto*

Main category: q-bio.MN

TL;DR: The paper introduces sSID and weighted sSID metrics to improve the evaluation of inferred causal networks in systems biology by considering qualitative relationships and intervention effects.


<details>
  <summary>Details</summary>
Motivation: Existing metrics inadequately account for the qualitative nature of biological databases and intervention effects to evaluate causal networks effectively in systems biology.

Method: The authors developed sSID and weighted sSID metrics, which incorporate the sign and net effects of interventions, validated through simulations and real datasets.

Result: sSID identified a different optimal algorithm compared to standard metrics and helped select networks that performed better in clinical covariates classification using transcriptomic data.

Conclusion: The proposed sSID metric is a more biologically meaningful evaluation tool that identifies structurally correct and functionally relevant networks, offering practical utility in systems biology.

Abstract: Estimating causal networks from biological data is a critical step in systems biology. When evaluating the inferred network, assessing the networks based on their intervention effects is particularly important for downstream probabilistic reasoning and the identification of potential drug targets. In the context of gene regulatory network inference, biological databases are often used as reference sources. These databases typically describe relationships in a qualitative rather than quantitative manner. However, few evaluation metrics have been developed that take this qualitative nature into account. To address this, we developed a metric, the sign-augmented Structural Intervention Distance (sSID), and a weighted sSID that incorporates the net effects of the intervention. Through simulations and analyses of real transcriptomic datasets, we found that our proposed metrics could identify a different algorithm as optimal compared to conventional metrics, and the network selected by sSID had a superior performance in the classification task of clinical covariates using transcriptomic data. This suggests that sSID can distinguish networks that are structurally correct but functionally incorrect, highlighting its potential as a more biologically meaningful and practical evaluation metric.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1008] [SEE++: Evolving Snowpark Execution Environment for Modern Workloads](https://arxiv.org/abs/2511.12457)
*Gaurav Jain,Brandon Baker,Joe Yin,Chenwei Xie,Zihao Ye,Sidh Kulkarni,Sara Abdelrahman,Nova Qi,Urjeet Shrestha,Mike Halcrow,Dave Bailey,Yuxiong He*

Main category: cs.DB

TL;DR: Snowpark transitioned to gVisor for better sandboxing to meet complex workload needs in Snowflake, optimizing performance, architecture, and feature flexibility.


<details>
  <summary>Details</summary>
Motivation: To meet the increasing complexity and sophistication of Data Engineering and AI/ML workloads, and to enhance security and performance in Snowpark's execution environment.

Method: Transitioning from an in-house sandboxing solution to gVisor and incorporating specific optimizations to address functional and performance objectives.

Result: The upgraded sandboxing architecture improved extensibility, flexibility, and security for Snowpark workloads, highlighted through case studies.

Conclusion: The gVisor-based sandboxing approach significantly advanced Snowpark's capabilities for secure and efficient workload execution, preparing it for the next generation of complex requirements.

Abstract: Snowpark enables Data Engineering and AI/ML workloads to run directly within Snowflake by deploying a secure sandbox on virtual warehouse nodes. This Snowpark Execution Environment (SEE) allows users to execute arbitrary workloads in Python and other languages in a secure and performant manner. As adoption has grown, the diversity of workloads has introduced increasingly sophisticated needs for sandboxing. To address these evolving requirements, Snowpark transitioned its in-house sandboxing solution to gVisor, augmented with targeted optimizations. This paper describes both the functional and performance objectives that guided the upgrade, outlines the new sandbox architecture, and details the challenges encountered during the journey, along with the solutions developed to resolve them. Finally, we present case studies that highlight new features enabled by the upgraded architecture, demonstrating SEE's extensibility and flexibility in supporting the next generation of Snowpark workloads.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [1009] [Proceedings Seventh International Workshop on Formal Methods for Autonomous Systems](https://arxiv.org/abs/2511.13245)
*Matt Luckcuck,Maike Schwammberger,Mengwei Xu*

Main category: cs.LO

TL;DR: This volume contains papers from the Seventh Workshop on Formal Methods for Autonomous Systems (FMAS 2025), focusing on challenges in autonomous systems with contributions from various countries.


<details>
  <summary>Details</summary>
Motivation: To bring researchers together to discuss how formal methods can address challenges in autonomous systems, fostering a growing research community.

Method: The workshop featured submissions from across the globe and facilitated discussions within the community on formal methods applications in autonomous systems.

Result: A total of 16 submissions were received from researchers in multiple countries, highlighting the network's appreciation and growth potential.

Conclusion: FMAS continues to be a platform connecting established and new authors, contributing to the growth of a strong autonomous systems research community.

Abstract: This EPTCS volume contains the papers from the Seventh International Workshop on Formal Methods for Autonomous Systems (FMAS 2025), which was held between the 17th and 19th of November 2025. The goal of the FMAS workshop series is to bring together leading researchers who are using formal methods to tackle the unique challenges that autonomous systems present, so that they can publish and discuss their work with a growing community of researchers. FMAS 2025 was co-located with the 20th International Conference on integrated Formal Methods (iFM'25), hosted by Inria Paris, France at the Inria Paris Center. 
  In total, FMAS 2025 received 16 submissions from researchers at institutions in: Canada, China, France, Germany, Ireland, Italy, Japan, the Netherlands, Portugal, Sweden, the United States of America, and the United Kingdom. Though we received fewer submissions than last year, we are encouraged to see the submissions being sent from a wide range of countries. Submissions come from both past and new FMAS authors, which shows us that the existing community appreciates the network that FMAS has built over the past 7 years, while new authors also show the FMAS community's great potential of growth.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [1010] [Modeling X-ray photon pile-up with a normalizing flow](https://arxiv.org/abs/2511.11863)
*Ole König,Daniela Huppenkothen,Douglas Finkbeiner,Christian Kirsch,Jörn Wilms,Justina R. Yang,James F. Steiner,Juan Rafael Martínez-Galarza*

Main category: astro-ph.HE

TL;DR: The paper introduces a machine learning approach to analyze data from bright X-ray sources impacted by pile-up effects, enhancing exploration of archival eROSITA observations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges posed by pile-up effects in X-ray observatory data, which distort spectral measurements and limit the analysis of bright X-ray sources.

Method: The authors propose a simulation-based inference framework using normalizing flow in machine learning to estimate posterior distributions of physical source parameters from piled-up eROSITA data.

Result: The proposed method demonstrates improved posterior density estimation compared to traditional techniques, enabling the use of more data from archival eROSITA observations.

Conclusion: The machine learning-based approach proves effective in overcoming pile-up issues in X-ray data and holds promise for analyzing eROSITA archival data with model and calibration considerations.

Abstract: The dynamic range of imaging detectors flown on-board X-ray observatories often only covers a limited flux range of extrasolar X-ray sources. The analysis of bright X-ray sources is complicated by so-called pile-up, which results from high incident photon flux. This nonlinear effect distorts the measured spectrum, resulting in biases in the inferred physical parameters, and can even lead to a complete signal loss in extreme cases. Piled-up data are commonly discarded due to resulting intractability of the likelihood. As a result, a large number of archival observations remain underexplored. We present a machine learning solution to this problem, using a simulation-based inference framework that allows us to estimate posterior distributions of physical source parameters from piled-up eROSITA data. We show that a normalizing flow produces better-constrained posterior densities than traditional mitigation techniques, as more data can be leveraged. We consider model- and calibration-dependent uncertainties and the applicability of such an algorithm to real data in the eROSITA archive.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [1011] [MMDCP: A Distribution-free Approach to Outlier Detection and Classification with Coverage Guarantees and SCW-FDR Control](https://arxiv.org/abs/2511.12016)
*Youwu Lin,Xiaoyu Qian,Jinru Wu,Qi Liu,Pei Wang*

Main category: stat.ME

TL;DR: Modified Mahalanobis Distance Conformal Prediction (MMDCP) is introduced for multi-class classification and outlier detection under label shift, aiming for computational efficiency and reliable prediction sets.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for conformal prediction under label shift often suffer from computational inefficiency and overly conservative prediction sets, particularly in small samples.

Method: MMDCP combines class-specific distance measures with full conformal prediction to construct adaptive prediction sets while offering theoretical guarantees on convergence rates and false discovery control.

Result: Theoretical validation ensures valid coverage, control over class-wise and global false discovery rates, supported by simulations and real application studies.

Conclusion: MMDCP effectively addresses challenges in conformal prediction under label shift while maintaining computational efficiency and robust error control.

Abstract: We propose the Modified Mahalanobis Distance Conformal Prediction (MMDCP), a unified framework for multi-class classification and outlier detection under label shift, where the training and test distributions may differ. In such settings, many existing methods construct nonconformity scores based on empirical cumulative or density functions combined with data-splitting strategies. However, these approaches are often computationally expensive due to their heavy reliance on resampling procedures and tend to produce overly conservative prediction sets with unstable coverage, especially in small samples. To address these challenges, MMDCP combines class-specific distance measures with full conformal prediction to construct a score function, thereby producing adaptive prediction sets that effectively capture both inlier and outlier structures. Under mild regularity conditions, we establish convergence rates for the resulting sets and provide the first theoretical characterization of the gap between oracle and empirical conformal $p$-values, which ensures valid coverage and effective control of the class-wise false discovery rate (CW-FDR). We further introduce the Summarized Class-Wise FDR (SCW-FDR), a novel global error metric aggregating false discoveries across classes, and show that it can be effectively controlled within the MMDCP framework. Extensive simulations and two real-data applications support our theoretical findings and demonstrate the advantages of the proposed method.

</details>


### [1012] [Aggregating Conformal Prediction Sets via α-Allocation](https://arxiv.org/abs/2511.12065)
*Congbin Xu,Yue Yu,Haojie Ren,Zhaojun Wang,Changliang Zou*

Main category: stat.ME

TL;DR: Introduces COLA, a strategy for minimizing prediction set sizes while maintaining valid coverage in conformal prediction by optimal confidence level allocation.


<details>
  <summary>Details</summary>
Motivation: Despite conformal prediction's ability to provide valid prediction sets, reducing set size while maintaining desired coverage using multiple conformity scores remains an open challenge.

Method: Proposes the COLA framework and its variants (COLA-s, COLA-f, COLA-l) to aggregate multiple conformity scores through optimal confidence allocation, ensuring both finite-sample and asymptotic coverage.

Result: Experiments on synthetic and real-world data show that COLA produces significantly smaller prediction sets compared to previous methods while upholding coverage guarantees.

Conclusion: COLA offers a principled approach to reducing prediction set sizes in conformal prediction, achieving efficiency and maintaining predictive coverage through novel allocation strategies.

Abstract: Conformal prediction offers a distribution-free framework for constructing prediction sets with finite-sample coverage. Yet, efficiently leveraging multiple conformity scores to reduce prediction set size remains a major open challenge. Instead of selecting a single best score, this work introduces a principled aggregation strategy, COnfidence-Level Allocation (COLA), that optimally allocates confidence levels across multiple conformal prediction sets to minimize empirical set size while maintaining provable coverage. Two variants are further developed, COLA-s and COLA-f, which guarantee finite-sample marginal coverage via sample splitting and full conformalization, respectively. In addition, we develop COLA-l, an individualized allocation strategy that promotes local size efficiency while achieving asymptotic conditional coverage. Extensive experiments on synthetic and real-world datasets demonstrate that COLA achieves considerably smaller prediction sets than state-of-the-art baselines while maintaining valid coverage.

</details>


### [1013] [A Gentle Introduction to Conformal Time Series Forecasting](https://arxiv.org/abs/2511.13608)
*M. Stocker,W. Małgorzewicz,M. Fontana,S. Ben Taieb*

Main category: stat.ME

TL;DR: This paper reviews advances in conformal prediction for time series data, addressing nonexchangeable conditions and surveying methods to improve prediction validity.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of maintaining valid prediction intervals for time series data, where classical conformal prediction methods fail due to violations of exchangeability.

Method: Theoretical derivation of finite-sample guarantees under weak dependence, classification of methods to handle serial dependence, including reweighting data, dynamic updates, and adaptive tuning.

Result: A simulation study comparing techniques shows their performance in coverage, interval width, and computational cost, while identifying trade-offs and potential research directions.

Conclusion: Advances in conformal prediction help improve uncertainty quantification in nonexchangeable time series settings, with practical strategies and highlighted future research opportunities.

Abstract: Conformal prediction is a powerful post-hoc framework for uncertainty quantification that provides distribution-free coverage guarantees. However, these guarantees crucially rely on the assumption of exchangeability. This assumption is fundamentally violated in time series data, where temporal dependence and distributional shifts are pervasive. As a result, classical split-conformal methods may yield prediction intervals that fail to maintain nominal validity. This review unifies recent advances in conformal forecasting methods specifically designed to address nonexchangeable data. We first present a theoretical foundation, deriving finite-sample guarantees for split-conformal prediction under mild weak-dependence conditions. We then survey and classify state-of-the-art approaches that mitigate serial dependence by reweighting calibration data, dynamically updating residual distributions, or adaptively tuning target coverage levels in real time. Finally, we present a comprehensive simulation study that compares these techniques in terms of empirical coverage, interval width, and computational cost, highlighting practical trade-offs and open research directions.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [1014] [Bridging the Skills Gap: A Course Model for Modern Generative AI Education](https://arxiv.org/abs/2511.11757)
*Anya Bardach,Hamilton Murrah*

Main category: cs.CY

TL;DR: The paper investigates the impact of generative AI tools in education, particularly focusing on a course developed for Computer Science students on AI applications. Students found the course valuable, and recommendations for replication are provided.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the disconnect between the increasing importance of generative AI competency in industry and its lack of proper instruction in higher education, ensuring students are prepared for future markets.

Method: The paper utilized mixed-method surveys to evaluate the effectiveness of a course developed to teach Computer Science students about applications for generative AI tools.

Result: The surveys showed that students overwhelmingly found the course valuable and effective in enhancing their understanding and application of generative AI.

Conclusion: The study emphasizes the need to integrate generative AI applications into educational curricula across fields, offering CS students practical skills while providing a replicable course model for other institutions.

Abstract: Research on how the popularization of generative Artificial Intelligence (AI) tools impacts learning environments has led to hesitancy among educators to teach these tools in classrooms, creating two observed disconnects. Generative AI competency is increasingly valued in industry but not in higher education, and students are experimenting with generative AI without formal guidance. The authors argue students across fields must be taught to responsibly and expertly harness the potential of AI tools to ensure job market readiness and positive outcomes. Computer Science trajectories are particularly impacted, and while consistently top ranked U.S. Computer Science departments teach the mechanisms and frameworks underlying AI, few appear to offer courses on applications for existing generative AI tools. A course was developed at a private research university to teach undergraduate and graduate Computer Science students applications for generative AI tools in software development. Two mixed method surveys indicated students overwhelmingly found the course valuable and effective. Co-authored by the instructor and one of the graduate students, this paper explores the context, implementation, and impact of the course through data analysis and reflections from both perspectives. It additionally offers recommendations for replication in and beyond Computer Science departments. This is the extended version of this paper to include technical appendices.

</details>


### [1015] [Embedding Explainable AI in NHS Clinical Safety: The Explainability-Enabled Clinical Safety Framework (ECSF)](https://arxiv.org/abs/2511.11590)
*Robert Gigiu*

Main category: cs.CY

TL;DR: The paper introduces an Explainability-Enabled Clinical Safety Framework (ECSF) to improve AI's clinical safety assurance by integrating explainability into existing NHS software governance standards.


<details>
  <summary>Details</summary>
Motivation: To address the challenges posed by AI's probabilistic and adaptive behavior conflicting with deterministic clinical safety standards in the NHS while enhancing the governance around AI-specific transparency, interpretability, and model drift.

Method: The authors developed the ECSF by synthesizing clinical safety regulatory frameworks (such as DCB0129/0160) with principles from Good Machine Learning Practice, NHS AI Assurance, T.E.S.T. frameworks, and the EU AI Act. ECSF introduces five checkpoints and leverages techniques like SHAP and LIME for structured safety evidence.

Result: A comprehensive matrix was created linking AI regulatory principles with ECSF checkpoints and explainability techniques to augment clinical safety. ECSF effectively bridges gaps in AI-safety governance.

Conclusion: ECSF positions explainability as a vital part of clinical safety assurance, aligning NHS standards with AI frameworks and promoting safer integration of AI into healthcare workflows.

Abstract: Artificial intelligence (AI) is increasingly embedded in NHS workflows, but its probabilistic and adaptive behaviour conflicts with the deterministic assumptions underpinning existing clinical-safety standards. DCB0129 and DCB0160 provide strong governance for conventional software yet do not define how AI-specific transparency, interpretability, or model drift should be evidenced within Safety Cases, Hazard Logs, or post-market monitoring. This paper proposes an Explainability-Enabled Clinical Safety Framework (ECSF) that integrates explainability into the DCB0129/0160 lifecycle, enabling Clinical Safety Officers to use interpretability outputs as structured safety evidence without altering compliance pathways. A cross-regulatory synthesis mapped DCB clauses to principles from Good Machine Learning Practice, the NHS AI Assurance and T.E.S.T. frameworks, and the EU AI Act. The resulting matrix links regulatory clauses, principles, ECSF checkpoints, and suitable explainability outputs. ECSF introduces five checkpoints: global transparency for hazard identification, case-level interpretability for verification, clinician usability for evaluation, traceable decision pathways for risk control, and longitudinal interpretability monitoring for post-market surveillance. Techniques such as SHAP, LIME, Integrated Gradients, saliency mapping, and attention visualisation are mapped to corresponding DCB artefacts. ECSF reframes explainability as a core element of clinical-safety assurance, bridging deterministic risk governance with the probabilistic behaviour of AI and supporting alignment with GMLP, the EU AI Act, and NHS AI Assurance principles.

</details>


### [1016] [EduAgentQG: A Multi-Agent Workflow Framework for Personalized Question Generation](https://arxiv.org/abs/2511.11635)
*Rui Jia,Min Zhang,Fengrui Liu,Bo Jiang,Kun Kuang,Zhongxiang Dai*

Main category: cs.CY

TL;DR: EduAgentQG is a multi-agent framework for generating high-quality personalized questions, addressing issues of diversity and alignment with educational goals.


<details>
  <summary>Details</summary>
Motivation: To create high-quality, personalized, and diverse question banks for adaptive learning and reduce the manual workload involved in question design.

Method: Proposes a multi-agent collaborative framework (EduAgentQG) involving iterative feedback among five agents: Planner, Writer, Solver, Educator, and Checker.

Result: EduAgentQG generated questions with enhanced diversity, quality, and alignment with educational objectives, outperforming existing methods on mathematics datasets.

Conclusion: EduAgentQG advances automated question generation by using multi-agent collaboration, achieving diverse and high-quality questions suited for educational needs.

Abstract: High-quality personalized question banks are crucial for supporting adaptive learning and individualized assessment. Manually designing questions is time-consuming and often fails to meet diverse learning needs, making automated question generation a crucial approach to reduce teachers' workload and improve the scalability of educational resources. However, most existing question generation methods rely on single-agent or rule-based pipelines, which still produce questions with unstable quality, limited diversity, and insufficient alignment with educational goals. To address these challenges, we propose EduAgentQG, a multi-agent collaborative framework for generating high-quality and diverse personalized questions. The framework consists of five specialized agents and operates through an iterative feedback loop: the Planner generates structured design plans and multiple question directions to enhance diversity; the Writer produces candidate questions based on the plan and optimizes their quality and diversity using feedback from the Solver and Educator; the Solver and Educator perform binary scoring across multiple evaluation dimensions and feed the evaluation results back to the Writer; the Checker conducts final verification, including answer correctness and clarity, ensuring alignment with educational goals. Through this multi-agent collaboration and iterative feedback loop, EduAgentQG generates questions that are both high-quality and diverse, while maintaining consistency with educational objectives. Experiments on two mathematics question datasets demonstrate that EduAgentQG outperforms existing single-agent and multi-agent methods in terms of question diversity, goal consistency, and overall quality.

</details>


### [1017] [Automatic generation of DRI Statements](https://arxiv.org/abs/2511.11655)
*Maurice Flechtner*

Main category: cs.CY

TL;DR: This paper automates the Deliberative Reason Index (DRI) statement generation using NLP and large language models.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenges of complex and time-intensive manual DRI statement creation for assessment of deliberative processes.

Method: An automated approach combining advanced NLP techniques and large language models (LLMs) to create DRI statements for survey preparation.

Result: The study creates a systematic framework for automated statement generation and a replicable method for integrating AI into social science methodologies.

Conclusion: Automating DRI statement generation reduces human effort, enables better deliberative process assessments, and offers a model for AI integration in social sciences.

Abstract: Assessing the quality of group deliberation is essential for improving our understanding of deliberative processes. The Deliberative Reason Index (DRI) offers a sophisticated metric for evaluating group reasoning, but its implementation has been constrained by the complex and time-consuming process of statement generation. This thesis introduces an innovative, automated approach to DRI statement generation that leverages advanced natural language processing (NLP) and large language models (LLMs) to substantially reduce the human effort involved in survey preparation. Key contributions are a systematic framework for automated DRI statement generation and a methodological innovation that significantly lowers the barrier to conducting comprehensive deliberative process assessments. In addition, the findings provide a replicable template for integrating generative artificial intelligence into social science research methodologies.

</details>


### [1018] [Decision-Making Amid Information-Based Threats in Sociotechnical Systems: A Review](https://arxiv.org/abs/2511.11595)
*Aaron R. Allred,Erin E. Richardson,Sarah R. Bostrom,James Crum,Cara Spencer,Chad Tossell,Richard E. Niemeyer,Leanne Hirshfield,Allison P. A. Hayman*

Main category: cs.CY

TL;DR: This paper reviews information-based threats affecting decision-making, explores shared cognitive mechanisms behind these threats, and suggests future research directions for integrating findings.


<details>
  <summary>Details</summary>
Motivation: Understanding and mitigating vulnerabilities in decision-making arising from increasing reliance on technological systems for information exchange.

Method: A synthesis of insights from studies on information-based threats and human information processing to identify shared cognitive mechanisms and propose integrative research directions.

Result: The paper identifies shared cognitive mechanisms mediating susceptibility to information-based threats and provides an outline for aligning human-machine representations.

Conclusion: Integrating insights from information-based threat research and human cognitive studies is crucial for mitigating vulnerabilities and enhancing decision-making in complex technological environments.

Abstract: Technological systems increasingly mediate human information exchange, spanning interactions among humans as well as between humans and artificial agents. The unprecedented scale and reliance on information disseminated through these systems substantially expand the scope of information-based influence that can both enable and undermine sound decision-making. Consequently, understanding and protecting decision-making today faces growing challenges, as individuals and organizations must navigate evolving opportunities and information-based threats across varied domains and information environments. While these risks are widely recognized, research remains fragmented: work evaluating information-based threat phenomena has progressed largely in isolation from foundational studies of human information processing. In this review, we synthesize insights from both domains to identify shared cognitive mechanisms that mediate vulnerability to information-based threats and shape behavioral outcomes. Finally, we outline directions for future research aimed at integrating these perspectives, emphasizing the importance of such integration for mitigating human vulnerabilities and aligning human-machine representations.

</details>


### [1019] [Generative AI as a Linguistic Equalizer in Global Science](https://arxiv.org/abs/2511.11687)
*Dragan Filimonovic,Christian Rutzer,Jeffrey Macher,Rolf Weder*

Main category: cs.CY

TL;DR: This paper explores the impact of generative AI in reducing linguistic barriers in scientific communication, especially for non-native English speakers.


<details>
  <summary>Details</summary>
Motivation: To address the long-standing scientific inequity due to language barriers caused by the dominance of English in global science.

Method: Analyzes 5.65 million scientific articles from 2021 to 2024, comparing GenAI-assisted and non-assisted articles using SciBERT embeddings to track linguistic similarity over time.

Result: Finds significant and growing stylistic convergence with U.S.-based English writing in GenAI-assisted publications, especially for authors in non-English-speaking countries after ChatGPT’s release in 2022.

Conclusion: Generative AI has begun to reduce language barriers in research, supporting equitable scientific communication for non-native English authors.

Abstract: For decades, the dominance of English has created a substantial barrier in global science, disadvantaging non-native speakers. The recent rise of generative AI (GenAI) offers a potential technological response to this long-standing inequity. We provide the first large-scale evidence testing whether GenAI acts as a linguistic equalizer in global science. Drawing on 5.65 million scientific articles published from 2021 to 2024, we compare GenAI-assisted and non-assisted publications from authors in non-English-speaking countries. Using text embeddings derived from a pretrained large language model (SciBERT), we measure each publication's linguistic similarity to a benchmark of scientific writing from U.S.-based authors and track stylistic convergence over time. We find significant and growing convergence for GenAI-assisted publications after the release of ChatGPT in late 2022. The effect is strongest for domestic coauthor teams from countries linguistically distant from English. These findings provide large-scale evidence that GenAI is beginning to reshape global science communication by reducing language barriers in research.

</details>


### [1020] [Leveraging Large Language Models for Career Mobility Analysis: A Study of Gender, Race, and Job Change Using U.S. Online Resume Profiles](https://arxiv.org/abs/2511.12010)
*Palakorn Achananuparp,Connie Xu,Yao Lu,Xavier Jayaraj Siddarth Ashok,Ee-Peng Lim*

Main category: cs.CY

TL;DR: The study analyzes the career mobility of college-educated workers in the U.S., linking job changes with gender and racial disparities using AI methods.


<details>
  <summary>Details</summary>
Motivation: To understand how job changes influence upward career mobility and disparities associated with gender and race.

Method: The authors utilized large language models (LLMs) for occupation classification and conducted sensitivity analyses on career trajectories.

Result: Their findings show job changes promote upward mobility, but the benefits are lower for women and Black graduates.

Conclusion: Occupation changes, particularly inside firms, are highly impactful for upward mobility, though disparities persist across gender and race.

Abstract: We present a large-scale analysis of career mobility of college-educated U.S. workers using online resume profiles to investigate how gender, race, and job change options are associated with upward mobility. This study addresses key research questions of how the job changes affect their upward career mobility, and how the outcomes of upward career mobility differ by gender and race. We address data challenges -- such as missing demographic attributes, missing wage data, and noisy occupation labels -- through various data processing and Artificial Intelligence (AI) methods. In particular, we develop a large language models (LLMs) based occupation classification method known as FewSOC that achieves accuracy significantly higher than the original occupation labels in the resume dataset. Analysis of 228,710 career trajectories reveals that intra-firm occupation change has been found to facilitate upward mobility most strongly, followed by inter-firm occupation change and inter-firm lateral move. Women and Black college graduates experience significantly lower returns from job changes than men and White peers. Multilevel sensitivity analyses confirm that these disparities are robust to cluster-level heterogeneity and reveal additional intersectional patterns.

</details>


### [1021] [Demystify, Use, Reflect: Preparing students to be informed LLM-users](https://arxiv.org/abs/2511.11764)
*Nikitha Donekal Chandrashekar,Sehrish Basir Nizamani,Margaret Ellis,Naren Ramakrishnan*

Main category: cs.CY

TL;DR: This paper discusses the integration of Large Language Models (LLMs) into a post-CS1 computer science course, aiming to develop students' responsible and practical engagement with AI.


<details>
  <summary>Details</summary>
Motivation: To address the growing significance of AI and LLMs in various fields, the paper aims to equip students with technical and ethical skills for meaningful interaction with AI.

Method: The course was redesigned to include instruction on LLM mechanisms, ethical issues, demonstrations, and activities for reflection. Students are taught how to verify LLM outputs, use them in problem-solving, and disclose their use of LLM assistance.

Result: Data from pre and post student surveys showed improvements in students' technical understanding of LLMs, enhanced verification practices, and a more discerning and collaborative approach to LLM use.

Conclusion: This course model can serve as a template for preparing students across disciplines for an AI-integrated future.

Abstract: We transitioned our post-CS1 course that introduces various subfields of computer science so that it integrates Large Language Models (LLMs) in a structured, critical, and practical manner. It aims to help students develop the skills needed to engage meaningfully and responsibly with AI. The course now includes explicit instruction on how LLMs work, exposure to current tools, ethical issues, and activities that encourage student reflection on personal use of LLMs as well as the larger evolving landscape of AI-assisted programming. In class, we demonstrate the use and verification of LLM outputs, guide students in the use of LLMs as an ingredient in a larger problem-solving loop, and require students to disclose and acknowledge the nature and extent of LLM assistance. Throughout the course, we discuss risks and benefits of LLMs across CS subfields. In our first iteration of the course, we collected and analyzed data from students pre and post surveys. Student understanding of how LLMs work became more technical, and their verification and use of LLMs shifted to be more discerning and collaborative. These strategies can be used in other courses to prepare students for the AI-integrated future.

</details>


### [1022] [Scaling Equitable Reflection Assessment in Education via Large Language Models and Role-Based Feedback Agents](https://arxiv.org/abs/2511.11772)
*Chenyu Zhang,Xiaohang Luo*

Main category: cs.CY

TL;DR: This study demonstrates a system of multi-agent LLMs for creating equitable, expert-level formative feedback, aiming to improve accessibility and effectiveness in large-scale education.


<details>
  <summary>Details</summary>
Motivation: Implementing formative feedback equitably at scale is challenging due to resource constraints in large or low-resource courses, resulting in gaps where learners need support most.

Method: A theory-grounded multi-agent LLM system with roles (Evaluator, Equity Monitor, Metacognitive Coach, Aggregator, Reflexion Reviewer) processes learner reflections using shared rubrics, fairness checks, and bias-aware comments to provide concise feedback.

Result: The system achieved near expert-level rubric scoring agreement, and human graders rated AI comments as helpful, empathetic, and well-aligned with educational objectives.

Conclusion: Multi-agent LLM systems enable equitable and high-quality formative feedback at scale, promoting widespread educational equity, instructional capacity, and accessibility.

Abstract: Formative feedback is widely recognized as one of the most effective drivers of student learning, yet it remains difficult to implement equitably at scale. In large or low-resource courses, instructors often lack the time, staffing, and bandwidth required to review and respond to every student reflection, creating gaps in support precisely where learners would benefit most. This paper presents a theory-grounded system that uses five coordinated role-based LLM agents (Evaluator, Equity Monitor, Metacognitive Coach, Aggregator, and Reflexion Reviewer) to score learner reflections with a shared rubric and to generate short, bias-aware, learner-facing comments. The agents first produce structured rubric scores, then check for potentially biased or exclusionary language, add metacognitive prompts that invite students to think about their own thinking, and finally compose a concise feedback message of at most 120 words. The system includes simple fairness checks that compare scoring error across lower and higher scoring learners, enabling instructors to monitor and bound disparities in accuracy. We evaluate the pipeline in a 12-session AI literacy program with adult learners. In this setting, the system produces rubric scores that approach expert-level agreement, and trained graders rate the AI-generated comments as helpful, empathetic, and well aligned with instructional goals. Taken together, these results show that multi-agent LLM systems can deliver equitable, high-quality formative feedback at a scale and speed that would be impossible for human graders alone. More broadly, the work points toward a future where feedback-rich learning becomes feasible for any course size or context, advancing long-standing goals of equity, access, and instructional capacity in education.

</details>


### [1023] [Differences in the Moral Foundations of Large Language Models](https://arxiv.org/abs/2511.11790)
*Peter Kirgis*

Main category: cs.CY

TL;DR: The paper analyzes the ethical judgments of large language models (LLMs) using moral foundations theory, revealing different biases and moral approaches as models increase in capability.


<details>
  <summary>Details</summary>
Motivation: To uncover and evaluate the opaque nature of normative ethical judgments in large language models being deployed in critical societal domains.

Method: Conducted a synthetic experiment using moral foundations theory (MFT) to analyze ethical biases and values across a range of LLMs through descriptive statistical approaches.

Result: Found that LLMs exhibit moral foundations distinct from human baselines, with variances increasing with model capabilities.

Conclusion: Encourages using moral psychology frameworks for further LLM analysis and alignment, and calls for policymakers to consider moral foundations deeply in alignment decisions.

Abstract: Large language models are increasingly being used in critical domains of politics, business, and education, but the nature of their normative ethical judgment remains opaque. Alignment research has, to date, not sufficiently utilized perspectives and insights from the field of moral psychology to inform training and evaluation of frontier models. I perform a synthetic experiment on a wide range of models from most major model providers using Jonathan Haidt's influential moral foundations theory (MFT) to elicit diverse value judgments from LLMs. Using multiple descriptive statistical approaches, I document the bias and variance of large language model responses relative to a human baseline in the original survey. My results suggest that models rely on different moral foundations from one another and from a nationally representative human baseline, and these differences increase as model capabilities increase. This work seeks to spur further analysis of LLMs using MFT, including finetuning of open-source models, and greater deliberation by policymakers on the importance of moral foundations for LLM alignment.

</details>


### [1024] [Understanding the Representation of Older Adults in Motion Capture Locomotion Datasets](https://arxiv.org/abs/2511.11713)
*Yunkai Yu,Yingying Wang,Rong Zheng*

Main category: cs.CY

TL;DR: The paper evaluates the representation of older adults in motion capture (MoCap) datasets, revealing underrepresentation and low-quality depictions of old-style motions. It introduces metrics to assess the fidelity of aging-related walking motions.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the lack of representation and realistic data of older adults in motion capture (MoCap) datasets, which is vital for healthcare applications using human locomotion data.

Method: The study analyzed 41 publicly available datasets, identified datasets including older adult motions, and introduced metrics to quantitatively assess the fidelity of old-style walking motions.

Result: Findings showed older adults are underrepresented, and old-style motions often fail to capture aging characteristics. The study highlights inaccuracies in existing datasets.

Conclusion: There is a need for improved representation and more accurate data of older adults in motion datasets. The authors provide a methodology to evaluate the quality of old-style walking motions.

Abstract: The Internet of Things (IoT) sensors have been widely employed to capture human locomotions to enable applications such as activity recognition, human pose estimation, and fall detection. Motion capture (MoCap) systems are frequently used to generate ground truth annotations for human poses when training models with data from wearable or ambient sensors, and have been shown to be effective to synthesize data in these modalities. However, the representation of older adults, an increasingly important demographic in healthcare, in existing MoCap locomotion datasets has not been thoroughly examined. This work surveyed 41 publicly available datasets, identifying eight that include older adult motions and four that contain motions performed by younger actors annotated as old style. Older adults represent a small portion of participants overall, and few datasets provide full-body motion data for this group. To assess the fidelity of old-style walking motions, quantitative metrics are introduced, defining high fidelity as the ability to capture age-related differences relative to normative walking. Using gait parameters that are age-sensitive, robust to noise, and resilient to data scarcity, we found that old-style walking motions often exhibit overly controlled patterns and fail to faithfully characterize aging. These findings highlight the need for improved representation of older adults in motion datasets and establish a method to quantitatively evaluate the quality of old-style walking motions.

</details>


### [1025] [AI Fairness Beyond Complete Demographics: Current Achievements and Future Directions](https://arxiv.org/abs/2511.13525)
*Zichong Wang,Zhipeng Yin,Roland H. C. Yap,Wenbin Zhang*

Main category: cs.CY

TL;DR: The paper addresses fairness in AI with incomplete demographic information, proposes a taxonomy of fairness notions, and summarizes methods and research gaps.


<details>
  <summary>Details</summary>
Motivation: To tackle discriminatory outcomes in AI despite lacking complete demographic data due to legal and ethical constraints.

Method: Surveying existing methods and proposing a novel taxonomy of fairness notions under incomplete demographics.

Result: Clear distinctions and relationships among fairness notions are presented, along with summarized techniques and unresolved research questions.

Conclusion: Fairness in AI with incomplete demographic data remains a challenge, with the taxonomy and summarized methods providing a foundation for future research.

Abstract: Fairness in artificial intelligence (AI) has become a growing concern due to discriminatory outcomes in AI-based decision-making systems. While various methods have been proposed to mitigate bias, most rely on complete demographic information, an assumption often impractical due to legal constraints and the risk of reinforcing discrimination. This survey examines fairness in AI when demographics are incomplete, addressing the gap between traditional approaches and real-world challenges. We introduce a novel taxonomy of fairness notions in this setting, clarifying their relationships and distinctions. Additionally, we summarize existing techniques that promote fairness beyond complete demographics and highlight open research questions to encourage further progress in the field.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [1026] [Scalable learning of macroscopic stochastic dynamics](https://arxiv.org/abs/2511.12842)
*Mengyi Chen,Pengru Huang,Kostya S. Novoselov,Qianxiao Li*

Main category: physics.comp-ph

TL;DR: The paper introduces a machine-learning framework to derive macroscopic dynamics of stochastic microscopic systems using data from small-scale simulations rather than large, computationally expensive ones.


<details>
  <summary>Details</summary>
Motivation: Understanding material behavior at a macroscopic level traditionally relies on complex first-principles methods, which are computationally expensive, particularly for large-scale systems.

Method: The framework uses small-system simulations, generates training data through a partial evolution scheme, identifies relevant macroscopic closure variables, and trains models with a custom loss function. It also includes a hierarchical upsampling approach for efficient generation of large-system data.

Result: The methodology achieves accurate macroscopic modeling of diverse stochastic systems, such as stochastic PDEs, lattice spin systems, and NbMoTa alloys, demonstrating its robustness.

Conclusion: The proposed framework serves as an efficient alternative to traditional methods by leveraging small-scale simulations to capture macroscopic dynamics for large stochastic systems, promoting more feasible computations across different materials.

Abstract: Macroscopic dynamical descriptions of complex physical systems are crucial for understanding and controlling material behavior. With the growing availability of data and compute, machine learning has become a promising alternative to first-principles methods to build accurate macroscopic models from microscopic trajectory simulations. However, for spatially extended systems, direct simulations of sufficiently large microscopic systems that inform macroscopic behavior is prohibitive. In this work, we propose a framework that learns the macroscopic dynamics of large stochastic microscopic systems using only small-system simulations. Our framework employs a partial evolution scheme to generate training data pairs by evolving large-system snapshots within local patches. We subsequently identify the closure variables associated with the macroscopic observables and learn the macroscopic dynamics using a custom loss. Furthermore, we introduce a hierarchical upsampling scheme that enables efficient generation of large-system snapshots from small-system trajectory distributions. We empirically demonstrate the accuracy and robustness of our framework through a variety of stochastic spatially extended systems, including those described by stochastic partial differential equations, idealised lattice spin systems, and a more realistic NbMoTa alloy system.

</details>


### [1027] [Case study of a differentiable heterogeneous multiphysics solver for a nuclear fusion application](https://arxiv.org/abs/2511.13262)
*Jack B. Coughlin,Archis Joglekar,Jonathan Brodrick,Alexander Lavin*

Main category: physics.comp-ph

TL;DR: The paper focuses on integrating a gradient-based multiphysics framework for nuclear fusion simulations with JAX, ensuring compatibility with complex solvers like Gkeyll via the Tesseract software.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in implementing gradient-based workflows for heterogeneous multiphysics problems in nuclear fusion, particularly due to the complexity of integrating non-differentiable advanced solvers.

Method: A JAX-based auto-differentiable ODE solver is developed to compute plasma and circuit parameters. The Tesseract software bridges the gap between high-fidelity solvers, neural surrogates, and analytical models while maintaining differentiability using JAX.

Result: The approach ensures a multi-physics abstraction layer enabling seamless integration of complex solvers like Gkeyll and supports flexibility for prototyping different surrogate models or analytical approximations.

Conclusion: This case study demonstrates the feasibility and benefits of combining advanced multiphysics solvers and auto-differentiable frameworks for nuclear fusion simulations.

Abstract: This work presents a case study of a heterogeneous multiphysics solver from the nuclear fusion domain. At the macroscopic scale, an auto-differentiable ODE solver in JAX computes the evolution of the pulsed power circuit and bulk plasma parameters for a compressing Z Pinch. The ODE solver requires a closure for the impedance of the plasma load obtained via root-finding at every timestep, which we solve efficiently using gradient-based Newton iteration. However, incorporating non-differentiable production-grade plasma solvers like Gkeyll (a C/CUDA plasma simulation suite) into a gradient-based workflow is non-trivial. The ''Tesseract'' software addresses this challenge by providing a multi-physics differentiable abstraction layer made fully compatible with JAX (through the `tesseract_jax` adapter). This architecture ensures end-to-end differentiability while allowing seamless interchange between high-fidelity solvers (Gkeyll), neural surrogates, and analytical approximations for rapid, progressive prototyping.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [1028] [Preference Learning from Physics-Based Feedback: Tuning Language Models to Design BCC/B2 Superalloys](https://arxiv.org/abs/2511.12036)
*Satanu Ghosh,Collin Holgate,Neal R. Brodnik,Doug Downey,Samantha Daly,Tresa M. Pollock,Samuel Carton*

Main category: cs.CE

TL;DR: The paper presents a novel application of language models to design BCC/B2 superalloys using physics-grounded feedback and preference tuning.


<details>
  <summary>Details</summary>
Motivation: Develop an efficient language model-guided approach to design structural alloys that target extreme environmental applications.

Method: Utilize Direct Preference Optimization (DPO) with open-weight language models and thermodynamic phase calculations to optimize design objectives.

Result: Successfully demonstrate preference-tuning of language models for structural alloy design using scientifically grounded feedback.

Conclusion: The framework is general and extensible, encouraging data-driven intelligent exploration in physical sciences.

Abstract: We apply preference learning to the task of language model-guided design of novel structural alloys. In contrast to prior work that focuses on generating stable inorganic crystals, our approach targets the synthesizeability of a specific structural class: BCC/B2 superalloys, an underexplored family of materials with potential applications in extreme environments. Using three open-weight models (LLaMA-3.1, Gemma-2, and OLMo-2), we demonstrate that language models can be optimized for multiple design objectives using a single, unified reward signal through Direct Preference Optimization (DPO). Unlike prior approaches that rely on heuristic or human-in-the-loop feedback (costly), our reward signal is derived from thermodynamic phase calculations, offering a scientifically grounded criterion for model tuning. To our knowledge, this is the first demonstration of preference-tuning a language model using physics-grounded feedback for structural alloy design. The resulting framework is general and extensible, providing a path forward for intelligent design-space exploration across a range of physical science domains.

</details>


### [1029] [Physics-Informed Neural Network-based Reliability Analysis of Buried Pipelines](https://arxiv.org/abs/2511.11613)
*Pouya Taraghi,Yong Li,Samer Adeeb*

Main category: cs.CE

TL;DR: The paper introduces Physics-Informed Neural Network for Reliability Analysis (PINN-RA) to efficiently assess buried pipelines' reliability under ground movement, reducing computational demand.


<details>
  <summary>Details</summary>
Motivation: Buried pipelines in geohazard areas experience risk of failure due to ground movement, necessitating accurate and efficient reliability analysis.

Method: A PINN-based surrogate model is combined with Monte Carlo Simulation, solving parametric differential equations for pipeline-ground interactions.

Result: PINN-RA significantly reduces computational effort and accelerates reliability analysis of pipelines impacted by permanent ground movement.

Conclusion: The proposed method is an efficient and scalable tool, enabling quick reliability assessments for pipelines in geohazard-prone regions.

Abstract: Buried pipelines transporting oil and gas across geohazard-prone regions are exposed to potential ground movement, leading to the risk of significant strain demand and structural failure. Reliability analysis, which determines the probability of failure after accounting for pertinent uncertainties, is essential for ensuring the safety of pipeline systems. However, traditional reliability analysis methods involving computationally intensive numerical models, such as finite element simulations of pipeline subjected to ground movement, have limited applications; this is partly because stochastic sampling approaches require repeated simulations over a large number of samples for the uncertain variables when estimating low probabilities. This study introduces Physics-Informed Neural Network for Reliability Analysis (PINN-RA) for buried pipelines subjected to ground movement, which integrates PINN-based surrogate model with Monte Carlo Simulation (MCS) to achieve efficient reliability assessment. To enable its application under uncertain variables associated with soil properties and ground movement, the PINN-based surrogate model is extended to solve a parametric differential equation system, namely the governing equation of pipelines embedded in soil with different properties. The findings demonstrate that PINN-RA significantly reduces the computational effort required and thus accelerates reliability analysis. By eliminating the need for repetitive numerical evaluations of pipeline subjected to permanent ground movement, the proposed approach provides an efficient and scalable tool for pipeline reliability assessment, enabling rapid decision-making in geohazard-prone regions.

</details>


### [1030] [Making Evidence Actionable in Adaptive Learning Closing the Diagnostic Pedagogical Loop](https://arxiv.org/abs/2511.13542)
*Amirreza Mehrabi,Jason Wade Morphew,Breejha Quezada,N. Sanjay Rebello*

Main category: cs.CE

TL;DR: The paper proposes an instructor-governed adaptive learning feedback loop for precise and personalized microinterventions, using algorithms and safeguards that optimize efficiency, adequacy, and diversity.


<details>
  <summary>Details</summary>
Motivation: Adaptive learning systems often fail to provide timely or effective interventions, particularly in classroom-scale applications. This paper aims to address these limitations by refining the connection between assessment and microintervention strategies.

Method: The study formulates intervention assignment as a binary integer program with constraints for skill coverage, allowed time, difficulty windows, prerequisites, and diversity safeguards. Greedy and gradient-based computational solvers were employed depending on resource richness and latency.

Result: Simulations and real-world trials in an introductory physics class with 1204 students showed that both solvers closed skill gaps and limited watch times. Notably, gradient-based method minimized redundant content by 12% compared to greedy, while greedy achieved comparable performance in resource-scarce scenarios.

Conclusion: The paper introduces a scalable and equitable adaptive learning controller that ensures adequate coverage, localized missing content, efficient computation, and tailored interventions for diverse student needs in classroom environments.

Abstract: Adaptive learning often diagnoses precisely yet intervenes weakly, producing help that is mistimed or misaligned. This study presents evidence supporting an instructor-governed feedback loop that converts concept-level assessment evidence into vetted microinterventions. The adaptive learning algorithm includes three safeguards: adequacy as a hard guarantee of gap closure, attention as a budgeted limit for time and redundancy, and diversity as protection against overfitting to a single resource. We formulate intervention assignment as a binary integer program with constraints for coverage, time, difficulty windows derived from ability estimates, prerequisites encoded by a concept matrix, and anti-redundancy with diversity. Greedy selection serves low-richness and tight-latency settings, gradient-based relaxation serves rich repositories, and a hybrid switches along a richness-latency frontier. In simulation and in an introductory physics deployment with 1204 students, both solvers achieved full skill coverage for nearly all learners within bounded watch time. The gradient-based method reduced redundant coverage by about 12 percentage points relative to greedy and produced more consistent difficulty alignment, while greedy delivered comparable adequacy at lower computational cost in resource-scarce environments. Slack variables localized missing content and guided targeted curation, sustaining sufficiency across student subgroups. The result is a tractable and auditable controller that closes the diagnostic pedagogical loop and enables equitable, load-aware personalization at the classroom scale.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [1031] [eFPE: Design, Implementation, and Evaluation of a Lightweight Format-Preserving Encryption Algorithm for Embedded Systems](https://arxiv.org/abs/2511.12225)
*Nishant Vasantkumar Hegde,Suneesh Bare,K B Ramesh,Aamir Ibrahim*

Main category: cs.CR

TL;DR: The paper introduces eFPE, a lightweight format-preserving encryption method designed for resource-constrained systems, with secure and efficient PRF implementation.


<details>
  <summary>Details</summary>
Motivation: Embedded systems require secure encryption methods that preserve data formats while being lightweight enough to work within their limited resources.

Method: The authors propose eFPE, an 8-round Feistel cipher, using a novel lightweight pseudorandom function (PRF) based on AES-inspired operations for direct encryption of even-length decimal strings.

Result: When implemented on an ARM7TDMI microcontroller, eFPE exhibited minimal resource usage: 4.73 kB ROM and 1.34 kB RAM, with the core module consuming just 3.55 kB ROM and 116 B RAM.

Conclusion: eFPE is a suitable encryption solution for resource-constrained systems like financial terminals and IoT devices, providing secure encryption with low memory and processing requirements.

Abstract: Resource-constrained embedded systems demand secure yet lightweight data protection, particularly when data formats must be preserved. This paper introduces eFPE (Enhanced Format-Preserving Encryption), an 8-round Feistel cipher featuring a "novel lightweight Pseudorandom Function (PRF)" specifically designed for this domain. The PRF, architected with an efficient two-iteration structure of AES-inspired operations (byte-substitution, keyed XOR, and byte-rotation), underpins eFPE's ability to directly encrypt even-length decimal strings without padding or complex conversions, while aiming for IND-CCA2 security under standard assumptions. Implemented and evaluated on an ARM7TDMI LPC2148 microcontroller using Keil μVision 4, eFPE demonstrates the efficacy of its targeted design: a total firmware Read-Only Memory (ROM) footprint of 4.73 kB and Random Access Memory (RAM) usage of 1.34 kB. The core eFPE algorithm module itself is notably compact, requiring only 3.55 kB ROM and 116 B RAM. These characteristics make eFPE a distinct and highly suitable solution for applications like financial terminals, medical sensors, and industrial IoT devices where data format integrity, minimal resource footprint, and low operational latency are paramount.

</details>


### [1032] [VULPO: Context-Aware Vulnerability Detection via On-Policy LLM Optimization](https://arxiv.org/abs/2511.11896)
*Youpeng Li,Fuxun Yu,Xinda Wang*

Main category: cs.CR

TL;DR: This paper presents VULPO, a reinforcement learning framework that enhances vulnerability detection (VD) in open-source software through context-aware analysis, outperforming previous approaches significantly.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current VD techniques, which fail to adaptively analyze repository-level dependencies and rely on fixed inputs or static datasets, resulting in a lack of comprehensive contextual analysis.

Method: The authors propose VULPO, an on-policy LLM-based reinforcement learning framework. They also develop ContextVul, a dataset combining function-level samples with repository-level context. VULPO uses multi-dimensional reward structuring and adaptive reward scaling to address prediction correctness, vulnerability localization, semantic accuracy, and difficulty across cases.

Result: Experiments show that VULPO-4B improves F1 scores by 85% compared to leading alternatives like Qwen3-4B and achieves performance comparable to a much larger model, DeepSeek-R1-0528.

Conclusion: The VULPO framework demonstrates superior performance in context-aware VD, making it a scalable and effective solution for adaptive analysis of vulnerabilities in open-source software.

Abstract: The widespread reliance on open-source software dramatically increases the risk of vulnerability exploitation, underscoring the need for effective and scalable vulnerability detection (VD). Existing VD techniques, whether traditional machine learning-based or LLM-based approaches like prompt engineering, supervised fine-tuning, or off-policy preference optimization, remain fundamentally limited in their ability to perform context-aware analysis: They depend on fixed inputs or static preference datasets, cannot adaptively explore repository-level dependencies, and are constrained by function-level benchmarks that overlook critical vulnerability context.
  This paper introduces Vulnerability-Adaptive Policy Optimization (VULPO), an on-policy LLM reinforcement learning framework for context-aware VD. To support training and evaluation, we first construct ContextVul, a new dataset that augments high-quality function-level samples with lightweight method to extract repository-level context information. We then design multi-dimensional reward structuring that jointly captures prediction correctness, vulnerability localization accuracy, and the semantic relevance of vulnerability analysis, thereby guiding the model toward comprehensive contextual reasoning. To address the asymmetric difficulty of different vulnerability cases and mitigate reward hacking, VULPO incorporates label-level and sample-level difficulty-adaptive reward scaling, encouraging the model to explore challenging cases while maintaining balanced reward distribution. Extensive experiments demonstrate the superiority of our VULPO framework in context-aware VD: Our VULPO-4B substantially outperforms existing VD baselines based on prompt engineering and off-policy optimization, improving F1 by 85% over Qwen3-4B and achieving performance comparable to a 150x larger-scale model, DeepSeek-R1-0528.

</details>


### [1033] [Multi-Agent Collaborative Fuzzing with Continuous Reflection for Smart Contracts Vulnerability Detection](https://arxiv.org/abs/2511.12164)
*Jie Chen,Liangmin Wang*

Main category: cs.CR

TL;DR: SmartFuzz uses a novel reflective fuzzing approach integrated with large language models to improve smart contract vulnerability detection by better targeting vulnerable regions and reducing invalid transaction sequences.


<details>
  <summary>Details</summary>
Motivation: Existing fuzzers for smart contracts fail to detect sophisticated vulnerabilities that require specific transaction sequences, as they prioritize code coverage over vulnerability discovery and lack semantic understanding of stateful contracts.

Method: SmartFuzz introduces a Continuous Reflection Process (CRP) for self-evolving transaction sequence generation, a Reactive Collaborative Chain (RCC) for task orchestration, and a multi-agent collaborative team for refining transaction sequences globally and locally.

Result: SmartFuzz detects 5.8%-74.7% more vulnerabilities within a 30-minute testing window and reduces false negatives by up to 80%, outperforming current tools.

Conclusion: SmartFuzz significantly enhances smart contract fuzzing by leveraging reflective processes, collaborative methodologies, and LLM-driven agents for accurate and efficient vulnerability detection.

Abstract: Fuzzing is a widely used technique for detecting vulnerabilities in smart contracts, which generates transaction sequences to explore the execution paths of smart contracts. However, existing fuzzers are falling short in detecting sophisticated vulnerabilities that require specific attack transaction sequences with proper inputs to trigger, as they (i) prioritize code coverage over vulnerability discovery, wasting considerable effort on non-vulnerable code regions, and (ii) lack semantic understanding of stateful contracts, generating numerous invalid transaction sequences that cannot pass runtime execution.
  In this paper, we propose SmartFuzz, a novel collaborative reflective fuzzer for smart contract vulnerability detection. It employs large language model-driven agents as the fuzzing engine and continuously improves itself by learning and reflecting through interactions with the environment. Specifically, we first propose a new Continuous Reflection Process (CRP) for fuzzing smart contracts, which reforms the transaction sequence generation as a self-evolving process through continuous reflection on feedback from the runtime environment. Then, we present the Reactive Collaborative Chain (RCC) to orchestrate the fuzzing process into multiple sub-tasks based on the dependencies of transaction sequences. Furthermore, we design a multi-agent collaborative team, where each expert agent is guided by the RCC to jointly generate and refine transaction sequences from both global and local perspectives. We conduct extensive experiments to evaluate SmartFuzz's performance on real-world contracts and DApp projects. The results demonstrate that SmartFuzz outperforms existing state-of-the-art tools: (i) it detects 5.8\%-74.7\% more vulnerabilities within 30 minutes, and (ii) it reduces false negatives by up to 80\%.

</details>


### [1034] [Software Supply Chain Security of Web3](https://arxiv.org/abs/2511.12274)
*Martin Monperrus*

Main category: cs.CR

TL;DR: The paper focuses on Web3 software supply chain security issues, analyzing threats, and proposing mitigation strategies.


<details>
  <summary>Details</summary>
Motivation: The increasing reliance on dApps and smart contracts in Web3 ecosystems necessitates addressing unique security challenges arising from their software supply chains.

Method: Through threat landscape analysis, the authors identify security vulnerabilities in Web3 software systems and propose concrete mitigation strategies.

Result: The study provides insights into the specific security vulnerabilities affecting Web3 software supply chains.

Conclusion: Addressing these distinct challenges in Web3 software supply chains is crucial for enhancing blockchain system security and protecting high-stakes digital assets.

Abstract: Web3 applications, built on blockchain technology, manage billions of dollars in digital assets through decentralized applications (dApps) and smart contracts. These systems rely on complex, software supply chains that introduce significant security vulnerabilities. This paper examines the software supply chain security challenges unique to the Web3 ecosystem, where traditional Web2 software supply chain problems intersect with the immutable and high-stakes nature of blockchain technology. We analyze the threat landscape and propose mitigation strategies to strengthen the security posture of Web3 systems.

</details>


### [1035] [GenSIaC: Toward Security-Aware Infrastructure-as-Code Generation with Large Language Models](https://arxiv.org/abs/2511.12385)
*Yikun Li,Matteo Grella,Daniel Nahmias,Gal Engelberg,Dan Klein,Giancarlo Guizzardi,Thijs van Ede,Andrea Continella*

Main category: cs.CR

TL;DR: The paper explores enhancing LLMs to generate security-aware Infrastructure as Code (IaC) scripts to prevent misconfigurations and vulnerabilities, presenting a fine-tuning dataset called GenSIaC to improve performance.


<details>
  <summary>Details</summary>
Motivation: The complexity of modern cloud infrastructures has heightened the risk of security vulnerabilities in IaC scripts. This paper seeks to leverage LLMs to mitigate these risks by enabling secure IaC code generation.

Method: The study evaluates base LLMs' ability to detect security weaknesses in IaC scripts and introduces a fine-tuning dataset, GenSIaC, to enhance their performance. Fine-tuned LLMs are then instructed to generate more secure IaC code, followed by evaluation and exploration of its effectiveness and generalizability.

Result: The fine-tuned LLMs showed significant improvements, evident from an increase in F1-score from 0.303 to 0.858 in recognizing and preventing security vulnerabilities. Additional studies confirmed the generalizability and effectiveness of GenSIaC across LLMs and programming languages.

Conclusion: Fine-tuning LLMs using GenSIaC yields substantial advancements in generating secure IaC scripts, addressing critical vulnerabilities and paving the way for safer and more robust infrastructure management.

Abstract: In recent years, Infrastructure as Code (IaC) has emerged as a critical approach for managing and provisioning IT infrastructure through code and automation. IaC enables organizations to create scalable and consistent environments, effectively managing servers and development settings. However, the growing complexity of cloud infrastructures has led to an increased risk of misconfigurations and security vulnerabilities in IaC scripts. To address this problem, this paper investigates the potential of Large Language Models (LLMs) in generating security-aware IaC code, avoiding misconfigurations introduced by developers and administrators.
  While LLMs have made significant progress in natural language processing and code generation, their ability to generate secure IaC scripts remains unclear. This paper addresses two major problems: 1) the lack of understanding of security weaknesses in IaC scripts generated by LLMs, and 2) the absence of techniques for enhancing security in generating IaC code with LLMs.
  To assess the extent to which LLMs contain security knowledge, we first conduct a comprehensive evaluation of base LLMs in recognizing major IaC security weaknesses during the generation and inspection of IaC code. Then, we propose GenSIaC, an instruction fine-tuning dataset designed to improve LLMs' ability to recognize potential security weaknesses. Leveraging GenSIaC, we fine-tune LLMs and instruct models to generate security-aware IaC code. Our evaluation demonstrates that our models achieve substantially improved performance in recognizing and preventing IaC security misconfigurations, e.g., boosting the F1-score from 0.303 to 0.858. Additionally, we perform ablation studies and explore GenSIaC's generalizability to other LLMs and its cross-language capabilities.

</details>


### [1036] [InfoDecom: Decomposing Information for Defending against Privacy Leakage in Split Inference](https://arxiv.org/abs/2511.13365)
*Ruijun Deng,Zhihui Lu,Qiang Duan*

Main category: cs.CR

TL;DR: Split inference (SI) suffers from data reconstruction attacks (DRAs) compromising privacy. InfoDecom addresses this by removing redundant information and adding noise for better privacy-utility balance.


<details>
  <summary>Details</summary>
Motivation: The authors aim to protect user privacy in deep learning services by addressing vulnerabilities in split inference models, which are prone to data reconstruction attacks.

Method: InfoDecom separates redundant information in smashed data, removes it, and injects calibrated noise to maintain privacy with theoretical guarantees.

Result: Experiments show InfoDecom provides a better balance of utility and privacy compared to existing approaches.

Conclusion: InfoDecom successfully mitigates excessive perturbation issues in split inference by effectively removing redundant information and implementing calibrated noise, improving the privacy-utility trade-off.

Abstract: Split inference (SI) enables users to access deep learning (DL) services without directly transmitting raw data. However, recent studies reveal that data reconstruction attacks (DRAs) can recover the original inputs from the smashed data sent from the client to the server, leading to significant privacy leakage. While various defenses have been proposed, they often result in substantial utility degradation, particularly when the client-side model is shallow. We identify a key cause of this trade-off: existing defenses apply excessive perturbation to redundant information in the smashed data. To address this issue in computer vision tasks, we propose InfoDecom, a defense framework that first decomposes and removes redundant information and then injects noise calibrated to provide theoretically guaranteed privacy. Experiments demonstrate that InfoDecom achieves a superior utility-privacy trade-off compared to existing baselines. The code and the appendix are available at https://github.com/SASA-cloud/InfoDecom.

</details>


### [1037] [A Content-Preserving Secure Linguistic Steganography](https://arxiv.org/abs/2511.12565)
*Lingyun Xiang,Chengfu Ou,Xu He,Zhongliang Yang,Yuling Liu*

Main category: cs.CR

TL;DR: The paper introduces CLstega, a method for linguistic steganography that embeds secret messages without modifying the original cover text while achieving perfect security.


<details>
  <summary>Details</summary>
Motivation: Most existing methods cause subtle inconsistencies between normal and modified stego texts, leading to potential security risks.

Method: CLstega uses an augmented masking strategy for embedding positions and dynamic distribution coding based on fine-tuning masked language models (MLM) to transform distributions without altering the text.

Result: CLstega attained a 100% extraction success rate, outperforming other methods in security and balancing embedding capacity.

Conclusion: CLstega ensures flawless concealment of secret messages while preserving the cover text's integrity, offering a secure and effective communication solution.

Abstract: Existing linguistic steganography methods primarily rely on content transformations to conceal secret messages. However, they often cause subtle yet looking-innocent deviations between normal and stego texts, posing potential security risks in real-world applications. To address this challenge, we propose a content-preserving linguistic steganography paradigm for perfectly secure covert communication without modifying the cover text. Based on this paradigm, we introduce CLstega (\textit{C}ontent-preserving \textit{L}inguistic \textit{stega}nography), a novel method that embeds secret messages through controllable distribution transformation. CLstega first applies an augmented masking strategy to locate and mask embedding positions, where MLM(masked language model)-predicted probability distributions are easily adjustable for transformation. Subsequently, a dynamic distribution steganographic coding strategy is designed to encode secret messages by deriving target distributions from the original probability distributions. To achieve this transformation, CLstega elaborately selects target words for embedding positions as labels to construct a masked sentence dataset, which is used to fine-tune the original MLM, producing a target MLM capable of directly extracting secret messages from the cover text. This approach ensures perfect security of secret messages while fully preserving the integrity of the original cover text. Experimental results show that CLstega can achieve a 100\% extraction success rate, and outperforms existing methods in security, effectively balancing embedding capacity and security.

</details>


### [1038] [AutoMalDesc: Large-Scale Script Analysis for Cyber Threat Research](https://arxiv.org/abs/2511.13333)
*Alexandru-Mihai Apostu,Andrei Preda,Alexandra Daniela Damir,Diana Bolocan,Radu Tudor Ionescu,Ioana Croitoru,Mihaela Gaman*

Main category: cs.CR

TL;DR: The paper introduces AutoMalDesc, a scalable framework for generating high-quality, language-based explanations for malware detection without extensive manual data annotation, backed by significant evaluation and data sharing.


<details>
  <summary>Details</summary>
Motivation: Despite progress in malware detection, producing clear, natural language explanations for threat reports remains a significant challenge in cybersecurity research.

Method: The authors use a self-paced iterative learning pipeline trained on a small seed of expert-curated examples, generating and validating synthetic data to improve summaries and classification over iterations.

Result: Across 3,600 samples in five scripting languages, the framework shows statistically significant improvements in summary quality and classification accuracy, validated through quantitative and human/LLM-based qualitative metrics.

Conclusion: This method demonstrates sustained improvements in language-based threat explanation, and the authors provide a large, annotated dataset to support further research in this field.

Abstract: Generating thorough natural language explanations for threat detections remains an open problem in cybersecurity research, despite significant advances in automated malware detection systems. In this work, we present AutoMalDesc, an automated static analysis summarization framework that, following initial training on a small set of expert-curated examples, operates independently at scale. This approach leverages an iterative self-paced learning pipeline to progressively enhance output quality through synthetic data generation and validation cycles, eliminating the need for extensive manual data annotation. Evaluation across 3,600 diverse samples in five scripting languages demonstrates statistically significant improvements between iterations, showing consistent gains in both summary quality and classification accuracy. Our comprehensive validation approach combines quantitative metrics based on established malware labels with qualitative assessment from both human experts and LLM-based judges, confirming both technical precision and linguistic coherence of generated summaries. To facilitate reproducibility and advance research in this domain, we publish our complete dataset of more than 100K script samples, including annotated seed (0.9K) and test (3.6K) datasets, along with our methodology and evaluation framework.

</details>


### [1039] [ForgeDAN: An Evolutionary Framework for Jailbreaking Aligned Large Language Models](https://arxiv.org/abs/2511.13548)
*Siyang Cheng,Gaotian Liu,Rui Mei,Yilin Wang,Kejia Zhang,Kaishuo Wei,Yuqi Yu,Weiping Wen,Xiaojie Wu,Junhua Liu*

Main category: cs.CR

TL;DR: ForgeDAN is a new framework that generates diverse and effective jailbreak prompts for large language models (LLMs), outperforming previous methods.


<details>
  <summary>Details</summary>
Motivation: To address security risks in LLMs by improving jailbreak generation techniques, enhancing diversity, evaluation, and detection strategies.

Method: ForgeDAN uses multi-strategy textual perturbations, semantic fitness evaluation, and dual-dimensional jailbreak judgment to craft adversarial prompts.

Result: ForgeDAN achieves high success rates in jailbreaking LLMs while maintaining naturalness and stealth, surpassing current state-of-the-art approaches.

Conclusion: This paper introduces an advanced framework, ForgeDAN, which improves the generation of harmful prompts against LLMs, emphasizing security and effectiveness.

Abstract: The rapid adoption of large language models (LLMs) has brought both transformative applications and new security risks, including jailbreak attacks that bypass alignment safeguards to elicit harmful outputs. Existing automated jailbreak generation approaches e.g. AutoDAN, suffer from limited mutation diversity, shallow fitness evaluation, and fragile keyword-based detection. To address these limitations, we propose ForgeDAN, a novel evolutionary framework for generating semantically coherent and highly effective adversarial prompts against aligned LLMs. First, ForgeDAN introduces multi-strategy textual perturbations across \textit{character, word, and sentence-level} operations to enhance attack diversity; then we employ interpretable semantic fitness evaluation based on a text similarity model to guide the evolutionary process toward semantically relevant and harmful outputs; finally, ForgeDAN integrates dual-dimensional jailbreak judgment, leveraging an LLM-based classifier to jointly assess model compliance and output harmfulness, thereby reducing false positives and improving detection effectiveness. Our evaluation demonstrates ForgeDAN achieves high jailbreaking success rates while maintaining naturalness and stealth, outperforming existing SOTA solutions.

</details>


### [1040] [Can AI Models be Jailbroken to Phish Elderly Victims? An End-to-End Evaluation](https://arxiv.org/abs/2511.11759)
*Fred Heiding,Simon Lermen*

Main category: cs.CR

TL;DR: The paper showcases how attackers exploit AI safety failures for phishing targeting vulnerable individuals, particularly elderly victims.


<details>
  <summary>Details</summary>
Motivation: To demonstrate and analyze the risks posed by AI safety vulnerabilities that facilitate attacks on vulnerable populations.

Method: The authors evaluated safety guardrails of six LLMs across four attack types and conducted a study with 108 senior participants to test the effectiveness of AI-generated phishing.

Result: Critical safety failures were highlighted; AI-generated phishing successfully compromised 11% of elderly volunteers during the study.

Conclusion: Existing AI safety measures are insufficient, enabling sophisticated fraud tactics that pose a significant threat to vulnerable populations.

Abstract: We present an end-to-end demonstration of how attackers can exploit AI safety failures to harm vulnerable populations: from jailbreaking LLMs to generate phishing content, to deploying those messages against real targets, to successfully compromising elderly victims. We systematically evaluated safety guardrails across six frontier LLMs spanning four attack categories, revealing critical failures where several models exhibited near-complete susceptibility to certain attack vectors. In a human validation study with 108 senior volunteers, AI-generated phishing emails successfully compromised 11\% of participants. Our work uniquely demonstrates the complete attack pipeline targeting elderly populations, highlighting that current AI safety measures fail to protect those most vulnerable to fraud. Beyond generating phishing content, LLMs enable attackers to overcome language barriers and conduct multi-turn trust-building conversations at scale, fundamentally transforming fraud economics. While some providers report voluntary counter-abuse efforts, we argue these remain insufficient.

</details>


### [1041] [NegBLEURT Forest: Leveraging Inconsistencies for Detecting Jailbreak Attacks](https://arxiv.org/abs/2511.11784)
*Lama Sleem,Jerome Francois,Lujun Li,Nathan Foucher,Niccolo Gentile,Radu State*

Main category: cs.CR

TL;DR: The paper introduces a framework to detect jailbreak attacks targeting LLMs, using semantic consistency analysis and a negation-aware scoring approach, without requiring fine-tuning or manual threshold calibration.


<details>
  <summary>Details</summary>
Motivation: The motivation is to counter jailbreak attacks that bypass safety filters in LLMs, leading to harmful content generation, and overcome challenges in creating universal filtering rules due to context dependency.

Method: A negation-aware scoring approach leveraging semantic consistency analysis is proposed. The NegBLEURT Forest framework employs the Isolation Forest algorithm to detect anomalous LLM responses against expected safe behaviors.

Result: The detection framework achieves high accuracy consistently across different models and datasets, outperforming competing methods that are sensitive to variations.

Conclusion: NegBLEURT Forest offers a reliable, context-aware mechanism for identifying jailbreak attacks without extensive calibration or model adjustment, addressing a critical issue in LLM safety.

Abstract: Jailbreak attacks designed to bypass safety mechanisms pose a serious threat by prompting LLMs to generate harmful or inappropriate content, despite alignment with ethical guidelines. Crafting universal filtering rules remains difficult due to their inherent dependence on specific contexts. To address these challenges without relying on threshold calibration or model fine-tuning, this work introduces a semantic consistency analysis between successful and unsuccessful responses, demonstrating that a negation-aware scoring approach captures meaningful patterns. Building on this insight, a novel detection framework called NegBLEURT Forest is proposed to evaluate the degree of alignment between outputs elicited by adversarial prompts and expected safe behaviors. It identifies anomalous responses using the Isolation Forest algorithm, enabling reliable jailbreak detection. Experimental results show that the proposed method consistently achieves top-tier performance, ranking first or second in accuracy across diverse models using the crafted dataset, while competing approaches exhibit notable sensitivity to model and data variations.

</details>


### [1042] [Securing Generative AI in Healthcare: A Zero-Trust Architecture Powered by Confidential Computing on Google Cloud](https://arxiv.org/abs/2511.11836)
*Adaobi Amanna,Ishana Shinde*

Main category: cs.CR

TL;DR: The paper introduces Confidential Zero-Trust Framework (CZF), combining Zero-Trust and Confidential Computing to secure sensitive data during processing in healthcare AI systems.


<details>
  <summary>Details</summary>
Motivation: Address the security gap where patient data and proprietary AI models are vulnerable during active processing within healthcare AI systems.

Method: Proposes a novel Confidential Zero-Trust Framework (CZF) that integrates Zero-Trust Architecture with hardware-enforced data isolation using Confidential Computing, implemented on Google Cloud.

Result: Demonstrated the CZF’s ability to secure data-in-use through encrypted processing, cryptographic workload integrity, and enabling secure collaborations.

Conclusion: The CZF framework provides a robust, verifiable security model for adopting generative AI in healthcare, overcoming traditional security and compliance barriers.

Abstract: The integration of Generative Artificial Intelligence (GenAI) in healthcare is impeded by significant security challenges unaddressed by traditional frameworks, precisely the data-in-use gap where sensitive patient data and proprietary AI models are exposed during active processing. To address this, the paper proposes the Confidential Zero-Trust Framework (CZF), a novel security paradigm that synergistically combines Zero-Trust Architecture for granular access control with the hardware-enforced data isolation of Confidential Computing. We detailed a multi-tiered architectural blueprint for implementing the CZF on Google Cloud and analyzed its efficacy against real-world threats. The CZF provides a defense-in-depth architecture where data remains encrypted while in-use within a hardware-based Trusted Execution Environment (TEE). The framework's use of remote attestation offers cryptographic proof of workload integrity, transforming compliance from a procedural exercise into a verifiable technical fact and enabling secure, multi-party collaborations previously blocked by security and intellectual property concerns. By closing the data-in-use gap and enforcing Zero-Trust principles, the CZF provides a robust and verifiable framework that establishes the necessary foundation of trust to enable the responsible adoption of transformative AI technologies in healthcare.

</details>


### [1043] [BackWeak: Backdooring Knowledge Distillation Simply with Weak Triggers and Fine-tuning](https://arxiv.org/abs/2511.12046)
*Shanmin Wang,Dongdong Zhao*

Main category: cs.CR

TL;DR: The paper introduces BackWeak, a simple and stealthy backdoor attack method in Knowledge Distillation (KD).


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the security risks in KD, particularly backdoor attacks introduced by using third-party pre-trained teacher models, with a focus on reducing the complexity of attack methods while improving stealth.

Method: BackWeak utilizes stealthy weak triggers (imperceptible perturbations) and fine-tunes the teacher model with a very small learning rate, removing the need for surrogates and computationally intensive processes.

Result: The method effectively implants a backdoor with high success rates, transferring reliably to diverse student architectures in standard KD processes. It is proven to be efficient, stealthy, and less complex than prior methods.

Conclusion: BackWeak highlights the importance of focusing on stealthiness and minimal adversarial characteristics in KD backdoor attacks. It offers a simpler, efficient alternative to existing approaches while maintaining high attack success rates.

Abstract: Knowledge Distillation (KD) is essential for compressing large models, yet relying on pre-trained "teacher" models downloaded from third-party repositories introduces serious security risks -- most notably backdoor attacks. Existing KD backdoor methods are typically complex and computationally intensive: they employ surrogate student models and simulated distillation to guarantee transferability, and they construct triggers in a way similar to universal adversarial perturbations (UAPs), which being not stealthy in magnitude, inherently exhibit strong adversarial behavior. This work questions whether such complexity is necessary and constructs stealthy "weak" triggers -- imperceptible perturbations that have negligible adversarial effect. We propose BackWeak, a simple, surrogate-free attack paradigm. BackWeak shows that a powerful backdoor can be implanted by simply fine-tuning a benign teacher with a weak trigger using a very small learning rate. We demonstrate that this delicate fine-tuning is sufficient to embed a backdoor that reliably transfers to diverse student architectures during a victim's standard distillation process, yielding high attack success rates. Extensive empirical evaluations on multiple datasets, model architectures, and KD methods show that BackWeak is efficient, simpler, and often more stealthy than previous elaborate approaches. This work calls on researchers studying KD backdoor attacks to pay particular attention to the trigger's stealthiness and its potential adversarial characteristics.

</details>


### [1044] [Exploring AI in Steganography and Steganalysis: Trends, Clusters, and Sustainable Development Potential](https://arxiv.org/abs/2511.12052)
*Aditya Kumar Sahu,Chandan Kumar,Saksham Kumar,Serdar Solak*

Main category: cs.CR

TL;DR: The paper conducts a scientometric analysis of AI-driven steganography-based data hiding techniques, analyzing 654 articles from 2017-2023, and identifying thematic trends and alignment with sustainable development goals (SDGs).


<details>
  <summary>Details</summary>
Motivation: This study aims to assess the contribution and thematic trends of AI-driven steganography techniques, highlighting their global impact and alignment with societal goals, particularly SDGs.

Method: A thematic modelling approach is used to evaluate 654 articles published between 2017 and 2023, focusing on geographic analysis, thematic clusters, and alignment with SDGs.

Result: The study reveals 69% of articles are from Asian countries, identifies seven thematic clusters in AI-steganography research, and concludes that only 18 articles align with SDGs, showcasing limited integration of societal goals.

Conclusion: The paper highlights critical research trends, geographic dominance (China and India), and gaps in AI-steganography's alignment with SDGs, particularly emphasizing the need for better integration of societal impact goals.

Abstract: Steganography and steganalysis are strongly related subjects of information security. Over the past decade, many powerful and efficient artificial intelligence (AI) - driven techniques have been designed and presented during research into steganography as well as steganalysis. This study presents a scientometric analysis of AI-driven steganography-based data hiding techniques using a thematic modelling approach. A total of 654 articles within the time span of 2017 to 2023 have been considered. Experimental evaluation of the study reveals that 69% of published articles are from Asian countries. The China is on top (TP:312), followed by India (TP-114). The study mainly identifies seven thematic clusters: steganographic image data hiding, deep image steganalysis, neural watermark robustness, linguistic steganography models, speech steganalysis algorithms, covert communication networks, and video steganography techniques. The proposed study also assesses the scope of AI-steganography under the purview of sustainable development goals (SDGs) to present the interdisciplinary reciprocity between them. It has been observed that only 18 of the 654 articles are aligned with one of the SDGs, which shows that limited studies conducted in alignment with SDG goals. SDG9 which is Industry, Innovation, and Infrastructure is leading among 18 SDGs mapped articles. To the top of our insight, this study is the unique one to present a scientometric study on AI-driven steganography-based data hiding techniques. In the context of descriptive statistics, the study breaks down the underlying causes of observed trends, including the influence of DL developments, trends in East Asia and maturity of foundational methods. The work also stresses upon the critical gaps in societal alignment, particularly the SDGs, ultimately working on unveiling the field's global impact on AI security challenges.

</details>


### [1045] [Explainable Transformer-Based Email Phishing Classification with Adversarial Robustness](https://arxiv.org/abs/2511.12085)
*Sajad U P*

Main category: cs.CR

TL;DR: This research tackles AI-generated phishing attacks with a hybrid approach including DistilBERT with adversarial training, transparency enhancement via XAI techniques, and interpretable security narratives.


<details>
  <summary>Details</summary>
Motivation: Phishing attacks, especially AI-generated ones, challenge the resilience of phishing detection systems and require advanced methods for accurate classification and user understanding.

Method: A hybrid framework combining DistilBERT with Fast Gradient Method adversarial training for robustness, LIME for transparency, and Flan-T5-small for generating interpretive security narratives.

Result: Improved phishing classification, resistance to adversarial text perturbations, and enhanced transparency for user interpretations.

Conclusion: The framework achieves reliable phishing detection and interpretability, addressing both security challenges and user transparency effectively.

Abstract: Phishing and related cyber threats are becoming more varied and technologically advanced. Among these, email-based phishing remains the most dominant and persistent threat. These attacks exploit human vulnerabilities to disseminate malware or gain unauthorized access to sensitive information. Deep learning (DL) models, particularly transformer-based models, have significantly enhanced phishing mitigation through their contextual understanding of language. However, some recent threats, specifically Artificial Intelligence (AI)-generated phishing attacks, are reducing the overall system resilience of phishing detectors. In response, adversarial training has shown promise against AI-generated phishing threats. This study presents a hybrid approach that uses DistilBERT, a smaller, faster, and lighter version of the BERT transformer model for email classification. Robustness against text-based adversarial perturbations is reinforced using Fast Gradient Method (FGM) adversarial training. Furthermore, the framework integrates the LIME Explainable AI (XAI) technique to enhance the transparency of the DistilBERT architecture. The framework also uses the Flan-T5-small language model from Hugging Face to generate plain-language security narrative explanations for end-users. This combined approach ensures precise phishing classification while providing easily understandable justifications for the model's decisions.

</details>


### [1046] [AttackVLA: Benchmarking Adversarial and Backdoor Attacks on Vision-Language-Action Models](https://arxiv.org/abs/2511.12149)
*Jiayu Li,Yunhan Zhao,Xiang Zheng,Zonghuan Xu,Yige Li,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.CR

TL;DR: The paper proposes AttackVLA, a unified framework to evaluate attacks on Vision-Language-Action (VLA) models and introduces BackdoorVLA, a targeted backdoor attack achieving high success rates in specific tasks.


<details>
  <summary>Details</summary>
Motivation: To address the absence of a unified evaluation framework for identifying safety vulnerabilities in Vision-Language-Action (VLA) models due to architecture inconsistencies and lack of real-world validations for attacks.

Method: Developed AttackVLA, a standardized framework aligning with the VLA development lifecycle, implemented existing and adapted attacks, and introduced BackdoorVLA - a targeted backdoor attack capable of executing specific long-horizon action sequences.

Result: AttackVLA evaluated a wide range of attacks in both simulation and real-world settings. BackdoorVLA demonstrated impressive effectiveness, achieving an average targeted success rate of 58.4% and 100% on selected tasks.

Conclusion: The study highlights critical gaps in VLA attack evaluations, provides a dynamic framework to address these, and introduces a novel attack that reveals the potential risks of adversarial manipulation in robotics.

Abstract: Vision-Language-Action (VLA) models enable robots to interpret natural-language instructions and perform diverse tasks, yet their integration of perception, language, and control introduces new safety vulnerabilities. Despite growing interest in attacking such models, the effectiveness of existing techniques remains unclear due to the absence of a unified evaluation framework. One major issue is that differences in action tokenizers across VLA architectures hinder reproducibility and fair comparison. More importantly, most existing attacks have not been validated in real-world scenarios. To address these challenges, we propose AttackVLA, a unified framework that aligns with the VLA development lifecycle, covering data construction, model training, and inference. Within this framework, we implement a broad suite of attacks, including all existing attacks targeting VLAs and multiple adapted attacks originally developed for vision-language models, and evaluate them in both simulation and real-world settings. Our analysis of existing attacks reveals a critical gap: current methods tend to induce untargeted failures or static action states, leaving targeted attacks that drive VLAs to perform precise long-horizon action sequences largely unexplored. To fill this gap, we introduce BackdoorVLA, a targeted backdoor attack that compels a VLA to execute an attacker-specified long-horizon action sequence whenever a trigger is present. We evaluate BackdoorVLA in both simulated benchmarks and real-world robotic settings, achieving an average targeted success rate of 58.4% and reaching 100% on selected tasks. Our work provides a standardized framework for evaluating VLA vulnerabilities and demonstrates the potential for precise adversarial manipulation, motivating further research on securing VLA-based embodied systems.

</details>


### [1047] [SeedAIchemy: LLM-Driven Seed Corpus Generation for Fuzzing](https://arxiv.org/abs/2511.12448)
*Aidan Wen,Norah A. Alzahrani,Jingzhi Jiang,Andrew Joe,Karen Shieh,Andy Zhang,Basel Alomair,David Wagner*

Main category: cs.CR

TL;DR: SeedAIchemy is a tool that uses LLMs to automatically generate high-quality fuzzing corpora by collecting data from the internet.


<details>
  <summary>Details</summary>
Motivation: The paper aims to simplify and enhance the process of fuzzing implementation for developers.

Method: SeedAIchemy employs five modules, most of which leverage LLM workflows to create optimized search terms for collecting publicly available files online.

Result: Corpora generated by SeedAIchemy outperform naive corpora and achieve comparable performance to manually-curated ones.

Conclusion: SeedAIchemy automates and improves corpus generation for fuzzing, making the process more efficient and accessible for developers.

Abstract: We introduce SeedAIchemy, an automated LLM-driven corpus generation tool that makes it easier for developers to implement fuzzing effectively. SeedAIchemy consists of five modules which implement different approaches at collecting publicly available files from the internet. Four of the five modules use large language model (LLM) workflows to construct search terms designed to maximize corpus quality. Corpora generated by SeedAIchemy perform significantly better than a naive corpus and similarly to a manually-curated corpus on a diverse range of target programs and libraries.

</details>


### [1048] [Scalable Hierarchical AI-Blockchain Framework for Real-Time Anomaly Detection in Large-Scale Autonomous Vehicle Networks](https://arxiv.org/abs/2511.12648)
*Rathin Chandra Shit,Sharmila Subudhi*

Main category: cs.CR

TL;DR: HAVEN, a three-tier security architecture for autonomous vehicle networks, provides sub-10 ms threat detection, distributed coordination, and improved scalability and accuracy.


<details>
  <summary>Details</summary>
Motivation: To address challenges in autonomous vehicle security arising from complex sensor integration, real-time demands, and distributed attack surfaces.

Method: HAVEN employs a three-tier architecture using light ensemble anomaly detection on the edge, Byzantine-fault-tolerant federated learning, and blockchain mechanisms for secure coordination.

Result: HAVEN achieved sub-10 ms detection latency with 94% accuracy and 92% F1-score, validated fault tolerance with 20% compromised nodes, and reduced blockchain overhead.

Conclusion: HAVEN successfully balances real-time detection and distributed security coordination for autonomous vehicle networks, offering scalability and enhanced resilience.

Abstract: The security of autonomous vehicle networks is facing major challenges, owing to the complexity of sensor integration, real-time performance demands, and distributed communication protocols that expose vast attack surfaces around both individual and network-wide safety. Existing security schemes are unable to provide sub-10 ms (milliseconds) anomaly detection and distributed coordination of large-scale networks of vehicles within an acceptable safety/privacy framework. This paper introduces a three-tier hybrid security architecture HAVEN (Hierarchical Autonomous Vehicle Enhanced Network), which decouples real-time local threat detection and distributed coordination operations. It incorporates a light ensemble anomaly detection model on the edge (first layer), Byzantine-fault-tolerant federated learning to aggregate threat intelligence at a regional scale (middle layer), and selected blockchain mechanisms (top layer) to ensure critical security coordination. Extensive experimentation is done on a real-world autonomous driving dataset. Large-scale simulations with the number of vehicles ranging between 100 and 1000 and different attack types, such as sensor spoofing, jamming, and adversarial model poisoning, are conducted to test the scalability and resiliency of HAVEN. Experimental findings show sub-10 ms detection latency with an accuracy of 94% and F1-score of 92% across multimodal sensor data, Byzantine fault tolerance validated with 20\% compromised nodes, and a reduced blockchain storage overhead, guaranteeing sufficient differential privacy. The proposed framework overcomes the important trade-off between real-time safety obligation and distributed security coordination with novel three-tiered processing. The scalable architecture of HAVEN is shown to provide great improvement in detection accuracy as well as network resilience over other methods.

</details>


### [1049] [AI Bill of Materials and Beyond: Systematizing Security Assurance through the AI Risk Scanning (AIRS) Framework](https://arxiv.org/abs/2511.12668)
*Samuel Nathanson,Alexander Lee,Catherine Chen Kieffer,Jared Junkin,Jessica Ye,Amir Saeed,Melanie Lockhart,Russ Fink,Elisha Peterson,Lanier Watkins*

Main category: cs.CR

TL;DR: The paper introduces the AI Risk Scanning (AIRS) Framework to enhance AI assurance by combining threat modeling and automated evidence generation.


<details>
  <summary>Details</summary>
Motivation: AI assurance is fragmented, lacking verifiable, machine-readable evidence of model security.

Method: The AIRS Framework integrates threat modeling with three pilot studies to generate measurable assurance artifacts aligned with adversarial ML taxonomy, focusing on LLMs.

Result: A proof-of-concept on GPT-OSS-20B demonstrates safe loader policies, hash verification per shard, and runtime probes, highlighting gaps in existing AI assurance practices.

Conclusion: The AIRS Framework provides standardized and trustworthy AI risk documentation, extending SBOM practices to meet AI-specific needs through automated evidence generation.

Abstract: Assurance for artificial intelligence (AI) systems remains fragmented across software supply-chain security, adversarial machine learning, and governance documentation. Existing transparency mechanisms - including Model Cards, Datasheets, and Software Bills of Materials (SBOMs) - advance provenance reporting but rarely provide verifiable, machine-readable evidence of model security. This paper introduces the AI Risk Scanning (AIRS) Framework, a threat-model-based, evidence-generating framework designed to operationalize AI assurance. The AIRS Framework evolved through three progressive pilot studies - Smurf (AIBOM schema design), OPAL (operational validation), and Pilot C (AIRS) - that reframed AI documentation from descriptive disclosure toward measurable, evidence-bound verification. The framework aligns its assurance fields to the MITRE ATLAS adversarial ML taxonomy and automatically produces structured artifacts capturing model integrity, packaging and serialization safety, structural adapters, and runtime behaviors. Currently, the AIRS Framework is scoped to provide model-level assurances for LLMs, but it could be expanded to include other modalities and cover system-level threats (e.g. application-layer abuses, tool-calling). A proof-of-concept on a quantized GPT-OSS-20B model demonstrates enforcement of safe loader policies, per-shard hash verification, and contamination and backdoor probes executed under controlled runtime conditions. Comparative analysis with SBOM standards of SPDX 3.0 and CycloneDX 1.6 reveals alignment on identity and evaluation metadata, but identifies critical gaps in representing AI-specific assurance fields. The AIRS Framework thus extends SBOM practice to the AI domain by coupling threat modeling with automated, auditable evidence generation, providing a principled foundation for standardized, trustworthy, and machine-verifiable AI risk documentation.

</details>


### [1050] [Whose Narrative is it Anyway? A KV Cache Manipulation Attack](https://arxiv.org/abs/2511.12752)
*Mukkesh Ganesh,Kaushik Iyer,Arun Baalaaji Sankar Ananthan*

Main category: cs.CR

TL;DR: This paper introduces "History Swapping," a novel cache manipulation attack that alters the behavior of large language models, demonstrating the importance of securing KV caches.


<details>
  <summary>Details</summary>
Motivation: To explore the vulnerability of the KV cache in autoregressive LLMs and understand its critical role in encoding context, topic trajectory, and structural planning.

Method: A block-level attack ('History Swapping') overwrites the KV cache of an ongoing model generation with a precomputed cache to manipulate output, analyzed across 324 configurations of Qwen 3 models.

Result: The attack successfully alters topic trajectory and conversation structure under certain conditions, revealing three manipulation behaviors related to layer depth and overwriting magnitude.

Conclusion: KV cache serves as a potential vector for security exploitation, emphasizing the need for its robust protection in LLM frameworks.

Abstract: The Key Value(KV) cache is an important component for efficient inference in autoregressive Large Language Models (LLMs), but its role as a representation of the model's internal state makes it a potential target for integrity attacks. This paper introduces "History Swapping," a novel block-level attack that manipulates the KV cache to steer model generation without altering the user-facing prompt. The attack involves overwriting a contiguous segment of the active generation's cache with a precomputed cache from a different topic. We empirically evaluate this method across 324 configurations on the Qwen 3 family of models, analyzing the impact of timing, magnitude, and layer depth of the cache overwrite. Our findings reveal that only full-layer overwrites can successfully hijack the conversation's topic, leading to three distinct behaviors: immediate and persistent topic shift, partial recovery, or a delayed hijack. Furthermore, we observe that high-level structural plans are encoded early in the generation process and local discourse structure is maintained by the final layers of the model. This work demonstrates that the KV cache is a significant vector for security analysis, as it encodes not just context but also topic trajectory and structural planning, making it a powerful interface for manipulating model behavior.

</details>


### [1051] [Privacy-Preserving Federated Learning from Partial Decryption Verifiable Threshold Multi-Client Functional Encryption](https://arxiv.org/abs/2511.12936)
*Minjie Wang,Jinguang Han,Weizhi Meng*

Main category: cs.CR

TL;DR: The paper introduces VTSAFL, a Federated Learning protocol that enhances security against gradient leakage and poisoning attacks while improving computational and communication efficiency.


<details>
  <summary>Details</summary>
Motivation: To address gradient leakage and poisoning attacks in Federated Learning while ensuring verifiability of aggregation results and maintaining efficiency for IoT devices.

Method: Developed VTSAFL using a Partial Decryption Verifiable Threshold Multi-Client Function Encryption scheme, enabling verification of aggregation results and minimizing overhead.

Result: VTSAFL maintains accuracy on the MNIST dataset while reducing training time by over 40% and communication overhead by up to 50%.

Conclusion: VTSAFL proves effective in enhancing privacy, verifiability, efficiency, and scalability, making it suitable for resource-constrained IoT applications.

Abstract: In federated learning, multiple parties can cooperate to train the model without directly exchanging their own private data, but the gradient leakage problem still threatens the privacy security and model integrity. Although the existing scheme uses threshold cryptography to mitigate the inference attack, it can not guarantee the verifiability of the aggregation results, making the system vulnerable to the threat of poisoning attack. We construct a partial decryption verifiable threshold multi client function encryption scheme, and apply it to Federated learning to implement the federated learning verifiable threshold security aggregation protocol (VTSAFL). VTSAFL empowers clients to verify aggregation results, concurrently minimizing both computational and communication overhead. The size of the functional key and partial decryption results of the scheme are constant, which provides efficiency guarantee for large-scale deployment. The experimental results on MNIST dataset show that vtsafl can achieve the same accuracy as the existing scheme, while reducing the total training time by more than 40%, and reducing the communication overhead by up to 50%. This efficiency is critical for overcoming the resource constraints inherent in Internet of Things (IoT) devices.

</details>


### [1052] [Esim: EVM Bytecode Similarity Detection Based on Stable-Semantic Graph](https://arxiv.org/abs/2511.12971)
*Zhuo Chen,Gaoqiang Ji,Yiling He,Lei Wu,Yajin Zhou*

Main category: cs.CR

TL;DR: The paper proposes a novel bytecode similarity detection method for EVM, introducing a Stable-Semantic Graph (SSG) and its implementation, Esim, to improve accuracy compared to traditional methods and tools.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of DeFi has highlighted issues such as plagiarism and propagation of vulnerable code due to code reuse and limited open-source contributions. Detecting similarities in EVM bytecode accurately is crucial for improving security and addressing these problems.

Method: The authors developed Stable-Semantic Graphs (SSG) to represent EVM bytecode and implemented Esim, a tool leveraging heterogeneous graph neural networks to detect similarities. SSG captures relationships between stable instructions, addressing the limitations of traditional binary similarity detection methods.

Result: Esim achieved high similarity detection performance with an AUC of 96.3%, surpassing traditional methods. It also demonstrated superior results in a large-scale study of 2.6 million smart contracts across six chains, outperforming Etherscan in vulnerability detection.

Conclusion: SSG and Esim provide a highly accurate and practical solution for EVM bytecode similarity detection, addressing challenges arising from code reuse and compiler diversity in the DeFi space. This advancement enhances the security and reliability of the blockchain ecosystem.

Abstract: Decentralized finance (DeFi) is experiencing rapid expansion. However, prevalent code reuse and limited open-source contributions have introduced significant challenges to the blockchain ecosystem, including plagiarism and the propagation of vulnerable code. Consequently, an effective and accurate similarity detection method for EVM bytecode is urgently needed to identify similar contracts. Traditional binary similarity detection methods are typically based on instruction stream or control flow graph (CFG), which have limitations on EVM bytecode due to specific features like low-level EVM bytecode and heavily-reused basic blocks. Moreover, the highly-diverse Solidity Compiler (Solc) versions further complicate accurate similarity detection.
  Motivated by these challenges, we propose a novel EVM bytecode representation called Stable-Semantic Graph (SSG), which captures relationships between 'stable instructions' (special instructions identified by our study). Moreover, we implement a prototype, Esim, which embeds SSG into matrices for similarity detection using a heterogeneous graph neural network. Esim demonstrates high accuracy in SSG construction, achieving F1-scores of 100% for control flow and 95.16% for data flow, and its similarity detection performance reaches 96.3% AUC, surpassing traditional approaches. Our large-scale study, analyzing 2,675,573 smart contracts on six EVM-compatible chains over a one-year period, also demonstrates that Esim outperforms the SOTA tool Etherscan in vulnerability search.

</details>


### [1053] [SoK: The Last Line of Defense: On Backdoor Defense Evaluation](https://arxiv.org/abs/2511.13143)
*Gorka Abad,Marina Krček,Stefanos Koffas,Behrad Tajalli,Marco Arazzi,Roberto Riaño,Xiaoyun Xu,Zhuoran Liu,Antonino Nocera,Stjepan Picek*

Main category: cs.CR

TL;DR: This paper analyzes 183 studies on backdoor defenses, revealing inconsistencies in evaluation methodologies, and recommends standards to improve future evaluations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address inconsistencies in evaluation methodologies of backdoor defenses, which hinder fair comparisons and progress in mitigating threats against deep learning models.

Method: The method involved a comprehensive literature review and empirical evaluation of 183 research papers, as well as conducting over 3,000 experiments with different datasets, model architectures, defenses, and attack scenarios.

Result: The results highlight substantial variability in defense effectiveness, key gaps in evaluation practices, and critical inadequacies in current methodologies, providing actionable recommendations for improvement.

Conclusion: The study underscores the need for standardized, rigorous evaluation practices for backdoor defenses to enhance comparability, reliability, and real-world applicability.

Abstract: Backdoor attacks pose a significant threat to deep learning models by implanting hidden vulnerabilities that can be activated by malicious inputs. While numerous defenses have been proposed to mitigate these attacks, the heterogeneous landscape of evaluation methodologies hinders fair comparison between defenses. This work presents a systematic (meta-)analysis of backdoor defenses through a comprehensive literature review and empirical evaluation. We analyzed 183 backdoor defense papers published between 2018 and 2025 across major AI and security venues, examining the properties and evaluation methodologies of these defenses.
  Our analysis reveals significant inconsistencies in experimental setups, evaluation metrics, and threat model assumptions in the literature. Through extensive experiments involving three datasets (MNIST, CIFAR-100, ImageNet-1K), four model architectures (ResNet-18, VGG-19, ViT-B/16, DenseNet-121), 16 representative defenses, and five commonly used attacks, totaling over 3\,000 experiments, we demonstrate that defense effectiveness varies substantially across different evaluation setups. We identify critical gaps in current evaluation practices, including insufficient reporting of computational overhead and behavior under benign conditions, bias in hyperparameter selection, and incomplete experimentation. Based on our findings, we provide concrete challenges and well-motivated recommendations to standardize and improve future defense evaluations. Our work aims to equip researchers and industry practitioners with actionable insights for developing, assessing, and deploying defenses to different systems.

</details>


### [1054] [SafeGRPO: Self-Rewarded Multimodal Safety Alignment via Rule-Governed Policy Optimization](https://arxiv.org/abs/2511.12982)
*Xuankun Rong,Wenke Huang,Tingfeng Wang,Daiguo Zhou,Bo Du,Mang Ye*

Main category: cs.CR

TL;DR: The paper introduces SafeGRPO, a self-rewarded framework to improve multimodal safety alignment in MLLMs through structured reasoning and rule-governed reward construction.


<details>
  <summary>Details</summary>
Motivation: Current multimodal large language models (MLLMs) are fragile in ensuring safety due to text-image interactions that produce compositional risks, showing a need for reliable multimodal reasoning safety strategies.

Method: The authors propose SafeGRPO, which integrates rule-based reward construction into Group Relative Policy Optimization (GRPO) using a newly-developed SafeTag-VL-3K dataset with explicit safety tags for structured reasoning.

Result: SafeGRPO achieves better multimodal safety awareness, reasoning stability, and compositional robustness without diminishing the general capabilities of the models.

Conclusion: SafeGRPO effectively improves safety and alignment in MLLMs by enforcing interpretable and verifiable reasoning through rule-governed rewards, addressing compositional risks across diverse benchmarks.

Abstract: Multimodal large language models (MLLMs) have demonstrated impressive reasoning and instruction-following capabilities, yet their expanded modality space introduces new compositional safety risks that emerge from complex text-image interactions. Such cross-modal couplings can produce unsafe semantics even when individual inputs are benign, exposing the fragile safety awareness of current MLLMs. While recent works enhance safety by guiding models to reason about potential risks, unregulated reasoning traces may compromise alignment; although Group Relative Policy Optimization (GRPO) offers self-rewarded refinement without human supervision, it lacks verifiable signals for reasoning safety. To address this, we propose SafeGRPO a self-rewarded multimodal safety alignment framework that integrates rule-governed reward construction into GRPO, enabling interpretable and verifiable optimization of reasoning safety. Built upon the constructed SafeTag-VL-3K dataset with explicit visual, textual, and combined safety tags, SafeGRPO performs step-guided safety thinking to enforce structured reasoning and behavior alignment, substantially improving multimodal safety awareness, compositional robustness, and reasoning stability across diverse benchmarks without sacrificing general capabilities.

</details>


### [1055] [Whistledown: Combining User-Level Privacy with Conversational Coherence in LLMs](https://arxiv.org/abs/2511.13319)
*Chelsea McMurray,Hayder Tirmazi*

Main category: cs.CR

TL;DR: The paper introduces Whistledown, a privacy layer for securing user prompts sent to large language models (LLMs) through pseudonymization and differential privacy.


<details>
  <summary>Details</summary>
Motivation: Users and enterprises need secure methods to interact with LLMs due to the inclusion of potentially sensitive and personal information in prompts.

Method: Whistledown uses pseudonymization, local differential privacy (ε-LDP), and transformation caching to modify prompts for privacy without affecting conversational quality.

Result: Whistledown successfully provides privacy protection, has low computational overhead, and operates on both individual devices and enterprise infrastructure.

Conclusion: Whistledown offers an efficient and adaptable solution for enhancing user privacy while using LLMs, without requiring changes in existing LLM APIs.

Abstract: Users increasingly rely on large language models (LLMs) for personal, emotionally charged, and socially sensitive conversations. However, prompts sent to cloud-hosted models can contain personally identifiable information (PII) that users do not want logged, retained, or leaked. We observe this to be especially acute when users discuss friends, coworkers, or adversaries, i.e., when they spill the tea. Enterprises face the same challenge when they want to use LLMs for internal communication and decision-making.
  In this whitepaper, we present Whistledown, a best-effort privacy layer that modifies prompts before they are sent to the LLM. Whistledown combines pseudonymization and $ε$-local differential privacy ($ε$-LDP) with transformation caching to provide best-effort privacy protection without sacrificing conversational utility. Whistledown is designed to have low compute and memory overhead, allowing it to be deployed directly on a client's device in the case of individual users. For enterprise users, Whistledown is deployed centrally within a zero-trust gateway that runs on an enterprise's trusted infrastructure. Whistledown requires no changes to the existing APIs of popular LLM providers.

</details>


### [1056] [GRAPHTEXTACK: A Realistic Black-Box Node Injection Attack on LLM-Enhanced GNNs](https://arxiv.org/abs/2511.12423)
*Jiaji Ma,Puja Trivedi,Danai Koutra*

Main category: cs.CR

TL;DR: The study creates GRAPHTEXTACK, a black-box attack methodology that undermines LLM-enhanced GNN models by injecting nodes with malicious structures and semantics.


<details>
  <summary>Details</summary>
Motivation: To address vulnerabilities in LLM-enhanced GNNs, particularly the resilience of these models to traditional uni-modal attacks and the lack of realistic, multi-modal attack solutions.

Method: Developed a black-box, multi-modal node injection attack called GRAPHTEXTACK, utilizing an evolutionary optimization framework and a novel multi-objective fitness function.

Result: Demonstrated GRAPHTEXTACK's effectiveness through empirical validation on five datasets and two LLM-enhanced GNN models, outperforming 12 strong baselines.

Conclusion: GRAPHTEXTACK is the first practical, black-box, multi-modal attack strategy for targeting LLM-enhanced GNNs, highlighting vulnerabilities in current models.

Abstract: Text-attributed graphs (TAGs), which combine structural and textual node information, are ubiquitous across many domains. Recent work integrates Large Language Models (LLMs) with Graph Neural Networks (GNNs) to jointly model semantics and structure, resulting in more general and expressive models that achieve state-of-the-art performance on TAG benchmarks. However, this integration introduces dual vulnerabilities: GNNs are sensitive to structural perturbations, while LLM-derived features are vulnerable to prompt injection and adversarial phrasing. While existing adversarial attacks largely perturb structure or text independently, we find that uni-modal attacks cause only modest degradation in LLM-enhanced GNNs. Moreover, many existing attacks assume unrealistic capabilities, such as white-box access or direct modification of graph data. To address these gaps, we propose GRAPHTEXTACK, the first black-box, multi-modal{, poisoning} node injection attack for LLM-enhanced GNNs. GRAPHTEXTACK injects nodes with carefully crafted structure and semantics to degrade model performance, operating under a realistic threat model without relying on model internals or surrogate models. To navigate the combinatorial, non-differentiable search space of connectivity and feature assignments, GRAPHTEXTACK introduces a novel evolutionary optimization framework with a multi-objective fitness function that balances local prediction disruption and global graph influence. Extensive experiments on five datasets and two state-of-the-art LLM-enhanced GNN models show that GRAPHTEXTACK significantly outperforms 12 strong baselines.

</details>


### [1057] [Enhancing All-to-X Backdoor Attacks with Optimized Target Class Mapping](https://arxiv.org/abs/2511.13356)
*Lei Wang,Yulong Tian,Hao Han,Fengyuan Xu*

Main category: cs.CR

TL;DR: The paper addresses the severe threat of backdoor attacks in machine learning, focusing on All-to-X (A2X) attacks with multiple target classes and proposing a novel strategy to enhance their effectiveness.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore and improve the under-researched area of All-to-X (A2X) backdoor attacks, challenging misconceptions about their low success rates and demonstrating their robustness.

Method: The paper introduces an optimized strategy for A2X attacks, leveraging improved grouping and target class assignment mechanisms to enhance their success rates while maintaining robustness.

Result: The proposed method achieves significant success rate improvements of up to 28%, with average improvements of 6.7%, 16.4%, and 14.1% on CIFAR10, CIFAR100, and Tiny-ImageNet respectively.

Conclusion: The findings highlight the robustness and effectiveness of A2X attacks, emphasizing the need for further research in this domain to develop stronger defenses.

Abstract: Backdoor attacks pose severe threats to machine learning systems, prompting extensive research in this area. However, most existing work focuses on single-target All-to-One (A2O) attacks, overlooking the more complex All-to-X (A2X) attacks with multiple target classes, which are often assumed to have low attack success rates. In this paper, we first demonstrate that A2X attacks are robust against state-of-the-art defenses. We then propose a novel attack strategy that enhances the success rate of A2X attacks while maintaining robustness by optimizing grouping and target class assignment mechanisms. Our method improves the attack success rate by up to 28%, with average improvements of 6.7%, 16.4%, 14.1% on CIFAR10, CIFAR100, and Tiny-ImageNet, respectively. We anticipate that this study will raise awareness of A2X attacks and stimulate further research in this under-explored area. Our code is available at https://github.com/kazefjj/A2X-backdoor .

</details>


### [1058] [Adaptive Dual-Layer Web Application Firewall (ADL-WAF) Leveraging Machine Learning for Enhanced Anomaly and Threat Detection](https://arxiv.org/abs/2511.12643)
*Ahmed Sameh,Sahar Selim*

Main category: cs.CR

TL;DR: The paper proposes an Adaptive Dual-Layer Web Application Firewall (WAF) integrating two Machine Learning models for enhanced threat detection, achieving high accuracy and precision.


<details>
  <summary>Details</summary>
Motivation: Traditional WAFs struggle to distinguish malicious from legitimate traffic, leading to inadequate detection of cyber threats.

Method: The paper utilizes a two-layer Machine Learning approach: the first layer employs Decision Tree algorithms for anomaly detection; the second layer uses Support Vector Machines to classify anomalies into threats or benign cases.

Result: The proposed Adaptive Dual-Layer WAF achieves a detection accuracy of 99.88% and precision of 100% using five benchmark datasets.

Conclusion: Integrating Machine Learning techniques into WAFs significantly improves web application security through enhanced accuracy and reduced false positives.

Abstract: Web Application Firewalls are crucial for protecting web applications against a wide range of cyber threats. Traditional Web Application Firewalls often struggle to effectively distinguish between malicious and legitimate traffic, leading to limited efficacy in threat detection. To overcome these limitations, this paper proposes an Adaptive Dual-Layer WAF employing a two-layered Machine Learning model designed to enhance the accuracy of anomaly and threat detection. The first layer employs a Decision Tree (DT) algorithm to detect anomalies by identifying traffic deviations from established normal patterns. The second layer employs Support Vector Machine to classify these anomalies as either threat anomalies or benign anomalies. Our Adaptive Dual Layer WAF incorporates comprehensive data pre-processing and feature engineering techniques and has been thoroughly evaluated using five large benchmark datasets. Evaluation using these datasets shows that ADL WAF achieves a detection accuracy of 99.88% and a precision of 100%, significantly enhancing anomaly detection and reducing false positives. These findings suggest that integrating machine learning techniques into WAFs can substantially improve web application security by providing more accurate and efficient threat detection.

</details>


### [1059] [An Evaluation Framework for Network IDS/IPS Datasets: Leveraging MITRE ATT&CK and Industry Relevance Metrics](https://arxiv.org/abs/2511.12743)
*Adrita Rahman Tori,Khondokar Fida Hasan*

Main category: cs.CR

TL;DR: The paper develops a multi-dimensional framework for assessing datasets' suitability for machine learning and deep learning intrusion detection systems (IDS/IPS), incorporating the MITRE ATT&CK knowledge base and five evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation methods for IDS/IPS datasets focus on accuracy but neglect industry-specific threat relevance, compromising real-world application effectiveness.

Method: The authors propose integrating the MITRE ATT&CK framework, threat intelligence, natural language processing, and quantitative analysis to comprehensively assess datasets' suitability for specific industries.

Result: Application of the framework on nine datasets reveals gaps in addressing healthcare, energy, and finance threats. Some new datasets perform better (e.g., CIC-IoMT and CIC-UNSW-NB15) than others, like CICIoV-24.

Conclusion: This framework standardizes and improves dataset selection for more effective AI-based IDS/IPS deployment, validated through a real-world case study.

Abstract: The performance of Machine Learning (ML) and Deep Learning (DL)-based Intrusion Detection and Prevention Systems (IDS/IPS) is critically dependent on the relevance and quality of the datasets used for training and evaluation. However, current AI model evaluation practices for developing IDS/IPS focus predominantly on accuracy metrics, often overlooking whether datasets represent industry-specific threats. To address this gap, we introduce a novel multi-dimensional framework that integrates the MITRE ATT&CK knowledge base for threat intelligence and employs five complementary metrics that together provide a comprehensive assessment of dataset suitability. Methodologically, this framework combines threat intelligence, natural language processing, and quantitative analysis to assess the suitability of datasets for specific industry contexts. Applying this framework to nine publicly available IDS/IPS datasets reveals significant gaps in threat coverage, particularly in the healthcare, energy, and financial sectors. In particular, recent datasets (e.g., CIC-IoMT, CIC-UNSW-NB15) align better with sector-specific threats, whereas others, like CICIoV-24, underperform despite their recency. Our findings provide a standardized, interpretable approach for selecting datasets aligned with sector-specific operational requirements, ultimately enhancing the real-world effectiveness of AI-driven IDS/IPS deployments. The efficiency and practicality of the framework are validated through deployment in a real-world case study, underscoring its capacity to inform dataset selection and enhance the effectiveness of AI-driven IDS/IPS in operational environments.

</details>


### [1060] [Efficient Adversarial Malware Defense via Trust-Based Raw Override and Confidence-Adaptive Bit-Depth Reduction](https://arxiv.org/abs/2511.12827)
*Ayush Chaudhary,Sisir Doppalpudi*

Main category: cs.CR

TL;DR: This paper presents a novel framework to optimize malware detection systems for adversarial robustness and computational efficiency in big data environments.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiency and high computational costs of current adversarial defenses in production systems handling millions of samples daily.

Method: Combination of Trust-Raw Override (TRO) and Confidence-Adaptive Bit-Depth Reduction (CABDR) with adaptive confidence-based mechanisms to balance robustness and efficiency.

Result: Achieved 1.76x computational overhead (2.3x improvement over smoothing defenses), 91% clean accuracy, reduced attack success rates to 31-37%, throughput of 1.26 million samples/second across multiple configurations.

Conclusion: Efficient adversarial robustness optimization enables practical deployment of malware defenses in big data systems without excessive infrastructure challenges.

Abstract: The deployment of robust malware detection systems in big data environments requires careful consideration of both security effectiveness and computational efficiency. While recent advances in adversarial defenses have demonstrated strong robustness improvements, they often introduce computational overhead ranging from 4x to 22x, which presents significant challenges for production systems processing millions of samples daily. In this work, we propose a novel framework that combines Trust-Raw Override (TRO) with Confidence-Adaptive Bit-Depth Reduction (CABDR) to explicitly optimize the trade-off between adversarial robustness and computational efficiency. Our approach leverages adaptive confidence-based mechanisms to selectively apply defensive measures, achieving 1.76x computational overhead - a 2.3x improvement over state-of-the-art smoothing defenses. Through comprehensive evaluation on the EMBER v2 dataset comprising 800K samples, we demonstrate that our framework maintains 91 percent clean accuracy while reducing attack success rates to 31-37 percent across multiple attack types, with particularly strong performance against optimization-based attacks such as C and W (48.8 percent reduction). The framework achieves throughput of up to 1.26 million samples per second (measured on pre-extracted EMBER features with no runtime feature extraction), validated across 72 production configurations with statistical significance (5 independent runs, 95 percent confidence intervals, p less than 0.01). Our results suggest that practical adversarial robustness in production environments requires explicit optimization of the efficiency-robustness trade-off, providing a viable path for organizations to deploy robust defenses without prohibitive infrastructure costs.

</details>


### [1061] [Robust Client-Server Watermarking for Split Federated Learning](https://arxiv.org/abs/2511.13598)
*Jiaxiong Tang,Zhengchunmin Dai,Liantao Wu,Peng Sun,Honglong Chen,Zhenfu Cao*

Main category: cs.CR

TL;DR: The paper introduces RISE, a watermarking scheme for Split Federated Learning (SFL) that protects intellectual property for both clients and servers while ensuring model ownership verification.


<details>
  <summary>Details</summary>
Motivation: Existing SFL frameworks face intellectual property challenges due to joint contributions in model training, with no participant possessing the entire model, making traditional watermarking techniques ineffective.

Method: RISE employs an asymmetric watermarking design. The server embeds feature-based watermarks via loss regularization, while clients embed backdoor-based watermarks by injecting trigger samples into their datasets, ensuring separate but complementary protection.

Result: Experimental results demonstrate that RISE achieves over 95% watermark detection rate across standard datasets, with $p\text{-value} < 0.03$. The scheme is robust against common attacks and avoids interference between client and server watermarks.

Conclusion: RISE successfully addresses the intellectual property ambiguity in SFL, enabling mutual ownership verification for clients and servers through a robust co-embedding watermark strategy.

Abstract: Split Federated Learning (SFL) is renowned for its privacy-preserving nature and low computational overhead among decentralized machine learning paradigms. In this framework, clients employ lightweight models to process private data locally and transmit intermediate outputs to a powerful server for further computation. However, SFL is a double-edged sword: while it enables edge computing and enhances privacy, it also introduces intellectual property ambiguity as both clients and the server jointly contribute to training. Existing watermarking techniques fail to protect both sides since no single participant possesses the complete model. To address this, we propose RISE, a Robust model Intellectual property protection scheme using client-Server watermark Embedding for SFL. Specifically, RISE adopts an asymmetric client-server watermarking design: the server embeds feature-based watermarks through a loss regularization term, while clients embed backdoor-based watermarks by injecting predefined trigger samples into private datasets. This co-embedding strategy enables both clients and the server to verify model ownership. Experimental results on standard datasets and multiple network architectures show that RISE achieves over $95\%$ watermark detection rate ($p-value \lt 0.03$) across most settings. It exhibits no mutual interference between client- and server-side watermarks and remains robust against common removal attacks.

</details>
