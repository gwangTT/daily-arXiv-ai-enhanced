<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 12]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.CL](#cs.CL) [Total: 64]
- [cs.CV](#cs.CV) [Total: 103]
- [cs.DC](#cs.DC) [Total: 10]
- [cs.LG](#cs.LG) [Total: 95]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.RO](#cs.RO) [Total: 33]
- [cs.SE](#cs.SE) [Total: 15]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [stat.ML](#stat.ML) [Total: 7]
- [quant-ph](#quant-ph) [Total: 3]
- [stat.ME](#stat.ME) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [cs.NI](#cs.NI) [Total: 2]
- [eess.SP](#eess.SP) [Total: 5]
- [eess.IV](#eess.IV) [Total: 7]
- [cs.OS](#cs.OS) [Total: 2]
- [eess.SY](#eess.SY) [Total: 2]
- [cs.HC](#cs.HC) [Total: 3]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.SI](#cs.SI) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [cs.CR](#cs.CR) [Total: 7]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [cs.GR](#cs.GR) [Total: 4]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.CE](#cs.CE) [Total: 3]
- [q-bio.QM](#q-bio.QM) [Total: 3]
- [math.ST](#math.ST) [Total: 1]
- [eess.AS](#eess.AS) [Total: 3]
- [cs.CY](#cs.CY) [Total: 3]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.SD](#cs.SD) [Total: 6]
- [stat.AP](#stat.AP) [Total: 1]
- [cs.MA](#cs.MA) [Total: 3]
- [physics.med-ph](#physics.med-ph) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Robot-Gated Interactive Imitation Learning with Adaptive Intervention Mechanism](https://arxiv.org/abs/2506.09176)
*Haoyuan Cai,Zhenghao Peng,Bolei Zhou*

Main category: cs.AI

TL;DR: This paper introduces 'Adaptive Intervention Mechanism' (AIM), a robot-gated interactive imitation learning (IIL) algorithm aimed at reducing cognitive and monitoring demands on human supervisors in training autonomous agents.


<details>
  <summary>Details</summary>
Motivation: Current methods in IIL require significant human supervision and impose cognitive burdens due to frequent intervention demands, highlighting the need for a more efficient and adaptive approach.

Method: The proposed AIM algorithm incorporates a proxy Q-function that adapts its intervention requests based on the alignment of agent actions with the human expert. High Q-values signal deviations, prompting human intervention, while proficiency reduces intervention demands.

Result: In experiments, AIM reduced human intervention costs by 40% compared to baseline methods like Thrifty-DAgger, while improving learning efficiency and enabling the collection of higher-quality expert demonstrations in both continuous and discrete tasks.

Conclusion: AIM successfully minimizes human effort in interactive imitation learning without compromising safety, surpassing traditional methods in efficiency and data quality. This provides a valuable tool for advancing autonomous agent training.

Abstract: Interactive Imitation Learning (IIL) allows agents to acquire desired
behaviors through human interventions, but current methods impose high
cognitive demands on human supervisors. We propose the Adaptive Intervention
Mechanism (AIM), a novel robot-gated IIL algorithm that learns an adaptive
criterion for requesting human demonstrations. AIM utilizes a proxy Q-function
to mimic the human intervention rule and adjusts intervention requests based on
the alignment between agent and human actions. By assigning high Q-values when
the agent deviates from the expert and decreasing these values as the agent
becomes proficient, the proxy Q-function enables the agent to assess the
real-time alignment with the expert and request assistance when needed. Our
expert-in-the-loop experiments reveal that AIM significantly reduces expert
monitoring efforts in both continuous and discrete control tasks. Compared to
the uncertainty-based baseline Thrifty-DAgger, our method achieves a 40%
improvement in terms of human take-over cost and learning efficiency.
Furthermore, AIM effectively identifies safety-critical states for expert
assistance, thereby collecting higher-quality expert demonstrations and
reducing overall expert data and environment interactions needed. Code and demo
video are available at https://github.com/metadriverse/AIM.

</details>


### [2] [Comment on The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity](https://arxiv.org/abs/2506.09250)
*C. Opus,A. Lawsen*

Main category: cs.AI

TL;DR: The paper critiques Shojaee et al.'s 2025 findings on 'accuracy collapse' in Large Reasoning Models, attributing their results to experimental flaws rather than fundamental reasoning failures.


<details>
  <summary>Details</summary>
Motivation: To challenge the claims that LRMs fundamentally fail on complex planning puzzles and to identify experimental design flaws affecting their evaluation results.

Method: Conducted an analysis of Shojaee et al.'s experimental procedures, controlled for token limits and mathematically impossible scenarios, and ran alternative tests using generating functions.

Result: The analysis showed that previous failures reported in Tower of Hanoi tasks were due to token constraints and improper evaluation, rather than model incapabilities.

Conclusion: AI reasoning capabilities must be assessed with careful experimental design to avoid misrepresenting their performance.

Abstract: Shojaee et al. (2025) report that Large Reasoning Models (LRMs) exhibit
"accuracy collapse" on planning puzzles beyond certain complexity thresholds.
We demonstrate that their findings primarily reflect experimental design
limitations rather than fundamental reasoning failures. Our analysis reveals
three critical issues: (1) Tower of Hanoi experiments systematically exceed
model output token limits at reported failure points, with models explicitly
acknowledging these constraints in their outputs; (2) The authors' automated
evaluation framework fails to distinguish between reasoning failures and
practical constraints, leading to misclassification of model capabilities; (3)
Most concerningly, their River Crossing benchmarks include mathematically
impossible instances for N > 5 due to insufficient boat capacity, yet models
are scored as failures for not solving these unsolvable problems. When we
control for these experimental artifacts, by requesting generating functions
instead of exhaustive move lists, preliminary experiments across multiple
models indicate high accuracy on Tower of Hanoi instances previously reported
as complete failures. These findings highlight the importance of careful
experimental design when evaluating AI reasoning capabilities.

</details>


### [3] [Ming-Omni: A Unified Multimodal Model for Perception and Generation](https://arxiv.org/abs/2506.09344)
*Inclusion AI,Biao Gong,Cheng Zou,Chuanyang Zheng,Chunluan Zhou,Canxiang Yan,Chunxiang Jin,Chunjie Shen,Dandan Zheng,Fudong Wang,Furong Xu,GuangMing Yao,Jun Zhou,Jingdong Chen,Jianxin Sun,Jiajia Liu,Jianjiang Zhu,Jun Peng,Kaixiang Ji,Kaiyou Song,Kaimeng Ren,Libin Wang,Lixiang Ru,Lele Xie,Longhua Tan,Lyuxin Xue,Lan Wang,Mochen Bai,Ning Gao,Pei Chen,Qingpei Guo,Qinglong Zhang,Qiang Xu,Rui Liu,Ruijie Xiong,Sirui Gao,Tinghao Liu,Taisong Li,Weilong Chai,Xinyu Xiao,Xiaomei Wang,Xiaoxue Chen,Xiao Lu,Xiaoyu Li,Xingning Dong,Xuzheng Yu,Yi Yuan,Yuting Gao,Yunxiao Sun,Yipeng Chen,Yifei Wu,Yongjie Lyu,Ziping Ma,Zipeng Feng,Zhijiang Fang,Zhihao Qiu,Ziyuan Huang,Zhengyu He*

Main category: cs.AI

TL;DR: The paper introduces Ming-Omni, a unified multimodal model supporting images, text, audio, and video with advanced generation capabilities.


<details>
  <summary>Details</summary>
Motivation: To unify multimodal processing and generation in a single framework, avoiding the need for separate models or extensive fine-tuning.

Method: Dedicated modality-specific encoders and a MoE architecture with new modality-specific routers process and integrate multimodal inputs.

Result: Ming-Omni excels in tasks like speech generation, image generation, text-to-speech conversion, and image editing. It matches GPT-4o in modality support.

Conclusion: Ming-Omni demonstrates robust performance across multiple modalities, serves as an open-source resource, and promotes future research in unified multimodal modeling.

Abstract: We propose Ming-Omni, a unified multimodal model capable of processing
images, text, audio, and video, while demonstrating strong proficiency in both
speech and image generation. Ming-Omni employs dedicated encoders to extract
tokens from different modalities, which are then processed by Ling, an MoE
architecture equipped with newly proposed modality-specific routers. This
design enables a single model to efficiently process and fuse multimodal inputs
within a unified framework, thereby facilitating diverse tasks without
requiring separate models, task-specific fine-tuning, or structural redesign.
Importantly, Ming-Omni extends beyond conventional multimodal models by
supporting audio and image generation. This is achieved through the integration
of an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for
high-quality image generation, which also allow the model to engage in
context-aware chatting, perform text-to-speech conversion, and conduct
versatile image editing. Our experimental results showcase Ming-Omni offers a
powerful solution for unified perception and generation across all modalities.
Notably, our proposed Ming-Omni is the first open-source model we are aware of
to match GPT-4o in modality support, and we release all code and model weights
to encourage further research and development in the community.

</details>


### [4] [Beyond Nash Equilibrium: Bounded Rationality of LLMs and humans in Strategic Decision-making](https://arxiv.org/abs/2506.09390)
*Kehan Zheng,Jinfeng Zhou,Hongning Wang*

Main category: cs.AI

TL;DR: The study investigates how large language models (LLMs) deviate from full rationality in strategic games, comparing their behaviors to humans and highlighting similarities and limitations.


<details>
  <summary>Details</summary>
Motivation: To understand whether LLMs exhibit human-like bounded rationality in strategic decision-making scenarios, especially as their usage grows in these contexts.

Method: LLMs were evaluated using experimental paradigms adapted from behavioral game theory, specifically focusing on Rock-Paper-Scissors and the Prisoner's Dilemma games.

Result: LLMs reproduced familiar human heuristics but applied strategies more rigidly and showed weaker sensitivity to dynamic changes, indicating partial human-like bounded rationality.

Conclusion: Current LLMs demonstrate limited flexibility and context awareness, suggesting the need for new training methods to enhance adaptive strategic reasoning.

Abstract: Large language models are increasingly used in strategic decision-making
settings, yet evidence shows that, like humans, they often deviate from full
rationality. In this study, we compare LLMs and humans using experimental
paradigms directly adapted from behavioral game-theory research. We focus on
two well-studied strategic games, Rock-Paper-Scissors and the Prisoner's
Dilemma, which are well known for revealing systematic departures from rational
play in human subjects. By placing LLMs in identical experimental conditions,
we evaluate whether their behaviors exhibit the bounded rationality
characteristic of humans. Our findings show that LLMs reproduce familiar human
heuristics, such as outcome-based strategy switching and increased cooperation
when future interaction is possible, but they apply these rules more rigidly
and demonstrate weaker sensitivity to the dynamic changes in the game
environment. Model-level analyses reveal distinctive architectural signatures
in strategic behavior, and even reasoning models sometimes struggle to find
effective strategies in adaptive situations. These results indicate that
current LLMs capture only a partial form of human-like bounded rationality and
highlight the need for training methods that encourage flexible opponent
modeling and stronger context awareness.

</details>


### [5] [A Call for Collaborative Intelligence: Why Human-Agent Systems Should Precede AI Autonomy](https://arxiv.org/abs/2506.09420)
*Henry Peng Zou,Wei-Chieh Huang,Yaozu Wu,Chunyu Miao,Dongyuan Li,Aiwei Liu,Yue Zhou,Yankai Chen,Weizhi Zhang,Yangning Li,Liancheng Fang,Renhe Jiang,Philip S. Yu*

Main category: cs.AI

TL;DR: The paper advocates for AI systems working collaboratively with humans rather than pursuing fully autonomous AI agents.


<details>
  <summary>Details</summary>
Motivation: To address reliability, transparency, and human requirements issues in fully autonomous AI systems.

Method: Proposal of LLM-based Human-Agent Systems (LLM-HAS), where AI complements human guidance and control.

Result: Examples from healthcare, finance, and software development show that human-AI collaboration excels in handling complex tasks.

Conclusion: AI progress should focus on enhancing human capabilities through collaboration, not replacing human roles.

Abstract: Recent improvements in large language models (LLMs) have led many researchers
to focus on building fully autonomous AI agents. This position paper questions
whether this approach is the right path forward, as these autonomous systems
still have problems with reliability, transparency, and understanding the
actual requirements of human. We suggest a different approach: LLM-based
Human-Agent Systems (LLM-HAS), where AI works with humans rather than replacing
them. By keeping human involved to provide guidance, answer questions, and
maintain control, these systems can be more trustworthy and adaptable. Looking
at examples from healthcare, finance, and software development, we show how
human-AI teamwork can handle complex tasks better than AI working alone. We
also discuss the challenges of building these collaborative systems and offer
practical solutions. This paper argues that progress in AI should not be
measured by how independent systems become, but by how well they can work with
humans. The most promising future for AI is not in systems that take over human
roles, but in those that enhance human capabilities through meaningful
partnership.

</details>


### [6] [Fast Monte Carlo Tree Diffusion: 100x Speedup via Parallel Sparse Planning](https://arxiv.org/abs/2506.09498)
*Jaesik Yoon,Hyeonseo Cho,Yoshua Bengio,Sungjin Ahn*

Main category: cs.AI

TL;DR: Fast-MCTD significantly speeds up diffusion-based planning by optimizing tree search and rollout mechanisms, delivering up to 100x faster performance while maintaining or improving solution quality.


<details>
  <summary>Details</summary>
Motivation: While diffusion models promise strong trajectory planning, their limitations in extended reasoning tasks and computational inefficiency remain major challenges for practical deployment.

Method: The paper introduces Fast-MCTD, which enhances the Monte Carlo Tree Diffusion (MCTD) technique with two optimizations: Parallel MCTD for parallel rollouts and Sparse MCTD to reduce rollout lengths.

Result: Fast-MCTD achieves remarkable speed improvements—up to 100x faster than standard MCTD—while maintaining similar or better planning quality.

Conclusion: Fast-MCTD establishes itself as a practical, scalable alternative for trajectory planning, overcoming the computational bottlenecks of MCTD and even surpassing simpler models like Diffuser in certain scenarios.

Abstract: Diffusion models have recently emerged as a powerful approach for trajectory
planning. However, their inherently non-sequential nature limits their
effectiveness in long-horizon reasoning tasks at test time. The recently
proposed Monte Carlo Tree Diffusion (MCTD) offers a promising solution by
combining diffusion with tree-based search, achieving state-of-the-art
performance on complex planning problems. Despite its strengths, our analysis
shows that MCTD incurs substantial computational overhead due to the sequential
nature of tree search and the cost of iterative denoising. To address this, we
propose Fast-MCTD, a more efficient variant that preserves the strengths of
MCTD while significantly improving its speed and scalability. Fast-MCTD
integrates two techniques: Parallel MCTD, which enables parallel rollouts via
delayed tree updates and redundancy-aware selection; and Sparse MCTD, which
reduces rollout length through trajectory coarsening. Experiments show that
Fast-MCTD achieves up to 100x speedup over standard MCTD while maintaining or
improving planning performance. Remarkably, it even outperforms Diffuser in
inference speed on some tasks, despite Diffuser requiring no search and
yielding weaker solutions. These results position Fast-MCTD as a practical and
scalable solution for diffusion-based inference-time reasoning.

</details>


### [7] [DipLLM: Fine-Tuning LLM for Strategic Decision-making in Diplomacy](https://arxiv.org/abs/2506.09655)
*Kaixuan Xu,Jiajun Chai,Sicheng Li,Yuqian Fu,Yuanheng Zhu,Dongbin Zhao*

Main category: cs.AI

TL;DR: DipLLM introduces a fine-tuned Large Language Model (LLM) that efficiently tackles the complexities of the Diplomacy multiplayer game, surpassing previous methods while drastically reducing data requirements.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome the computational challenges associated with applying AI to the complex Diplomacy game, leveraging LLMs to reduce dependence on extensive game data for equilibrium search.

Method: DipLLM utilizes an autoregressive factorization framework to break down multi-unit action assignments into simpler unit-level decisions. It fine-tunes an LLM using significantly smaller data compared to the Cicero model, focusing on equilibrium policies.

Result: DipLLM outperforms the state-of-the-art Cicero model in Diplomacy while using only 1.5% of the training data required for the latter.

Conclusion: The study highlights the effectiveness of fine-tuned LLMs in simplifying AI-driven strategic decision-making in multiplayer games, presenting DipLLM as a promising alternative to traditional computationally intensive methods.

Abstract: Diplomacy is a complex multiplayer game that requires both cooperation and
competition, posing significant challenges for AI systems. Traditional methods
rely on equilibrium search to generate extensive game data for training, which
demands substantial computational resources. Large Language Models (LLMs) offer
a promising alternative, leveraging pre-trained knowledge to achieve strong
performance with relatively small-scale fine-tuning. However, applying LLMs to
Diplomacy remains challenging due to the exponential growth of possible action
combinations and the intricate strategic interactions among players. To address
this challenge, we propose DipLLM, a fine-tuned LLM-based agent that learns
equilibrium policies for Diplomacy. DipLLM employs an autoregressive
factorization framework to simplify the complex task of multi-unit action
assignment into a sequence of unit-level decisions. By defining an equilibrium
policy within this framework as the learning objective, we fine-tune the model
using only 1.5% of the data required by the state-of-the-art Cicero model,
surpassing its performance. Our results demonstrate the potential of fine-tuned
LLMs for tackling complex strategic decision-making in multiplayer games.

</details>


### [8] [Application-Driven Value Alignment in Agentic AI Systems: Survey and Perspectives](https://arxiv.org/abs/2506.09656)
*Wei Zeng,Hengshu Zhu,Chuan Qin,Han Wu,Yihang Cheng,Sirui Zhang,Xiaowei Jin,Yinuo Shen,Zhenxing Wang,Feimin Zhong,Hui Xiong*

Main category: cs.AI

TL;DR: This paper reviews value alignment in AI agent systems, focusing on hierarchical principles, application scenarios, evaluation methods, and future research directions.


<details>
  <summary>Details</summary>
Motivation: The evolution of AI and the shift towards complex multi-agent systems has increased situational and systemic risks, necessitating value alignment to ensure AI agents align with human values and societal norms.

Method: The paper categorizes value principles hierarchically, examines application scenarios, evaluates datasets and methods for value alignment, and explores value coordination in multi-agent systems.

Result: The review organizes value principles and application scenarios and assesses evaluation datasets and methods for robust value alignment in agentic AI.

Conclusion: A framework for addressing value alignment challenges in agentic AI is presented, alongside proposed research directions to advance the field.

Abstract: The ongoing evolution of AI paradigms has propelled AI research into the
Agentic AI stage. Consequently, the focus of research has shifted from single
agents and simple applications towards multi-agent autonomous decision-making
and task collaboration in complex environments. As Large Language Models (LLMs)
advance, their applications become more diverse and complex, leading to
increasingly situational and systemic risks. This has brought significant
attention to value alignment for AI agents, which aims to ensure that an
agent's goals, preferences, and behaviors align with human values and societal
norms. This paper reviews value alignment in agent systems within specific
application scenarios. It integrates the advancements in AI driven by large
models with the demands of social governance. Our review covers value
principles, agent system application scenarios, and agent value alignment
evaluation. Specifically, value principles are organized hierarchically from a
top-down perspective, encompassing macro, meso, and micro levels. Agent system
application scenarios are categorized and reviewed from a general-to-specific
viewpoint. Agent value alignment evaluation systematically examines datasets
for value alignment assessment and relevant value alignment methods.
Additionally, we delve into value coordination among multiple agents within
agent systems. Finally, we propose several potential research directions in
this field.

</details>


### [9] [Intent Factored Generation: Unleashing the Diversity in Your Language Model](https://arxiv.org/abs/2506.09659)
*Eltayeb Ahmed,Uljad Berdica,Martha Elliott,Danijela Horak,Jakob N. Foerster*

Main category: cs.AI

TL;DR: The paper introduces Intent Factored Generation (IFG) to enhance sample diversity in large language models while maintaining output quality, through a two-stage sampling process and temperature adjustment.


<details>
  <summary>Details</summary>
Motivation: Current methods for generating diverse responses from large language models often fail to provide meaningful diversity, particularly in reasoning tasks and conversational use cases.

Method: IFG uses a two-stage process: first, a semantically dense intent is generated at high temperature for diversity; then, the response is conditioned on both the prompt and intent at lower temperature for coherence.

Result: The method demonstrates improved performance metrics across reasoning, coding, and conversational tasks, as well as higher diversity in language modeling while maintaining generation quality.

Conclusion: IFG provides a simple yet effective approach to enhance diversity in large language model outputs, which can be easily integrated into existing frameworks for varied applications.

Abstract: Obtaining multiple meaningfully diverse, high quality samples from Large
Language Models for a fixed prompt remains an open challenge. Current methods
for increasing diversity often only operate at the token-level, paraphrasing
the same response. This is problematic because it leads to poor exploration on
reasoning problems and to unengaging, repetitive conversational agents. To
address this we propose Intent Factored Generation (IFG), factorising the
sampling process into two stages. First, we sample a semantically dense intent,
e.g., a summary or keywords. Second, we sample the final response conditioning
on both the original prompt and the intent from the first stage. This allows us
to use a higher temperature during the intent step to promote conceptual
diversity, and a lower temperature during the final generation to ensure the
outputs are coherent and self-consistent. Additionally, we find that prompting
the model to explicitly state its intent for each step of the chain-of-thought
before generating the step is beneficial for reasoning tasks. We demonstrate
our method's effectiveness across a diverse set of tasks. We show this method
improves both pass@k and Reinforcement Learning from Verifier Feedback on maths
and code tasks. For instruction-tuning, we combine IFG with Direct Preference
Optimisation to increase conversational diversity without sacrificing reward.
Finally, we achieve higher diversity while maintaining the quality of
generations on a general language modelling task, using a new dataset of reader
comments and news articles that we collect and open-source. In summary, we
present a simple method of increasing the sample diversity of LLMs while
maintaining performance. This method can be implemented by changing the prompt
and varying the temperature during generation, making it easy to integrate into
many algorithms for gains across various applications.

</details>


### [10] [How Do People Revise Inconsistent Beliefs? Examining Belief Revision in Humans with User Studies](https://arxiv.org/abs/2506.09977)
*Stylianos Loukas Vasileiou,Antonio Rago,Maria Vanina Martinez,William Yeoh*

Main category: cs.AI

TL;DR: This paper investigates human belief revision, emphasizing that people favor explanation-based over classical minimal changes when presented with inconsistencies.


<details>
  <summary>Details</summary>
Motivation: To understand how humans revise their beliefs in light of new information, thereby enabling AI systems to align better with human reasoning.

Method: The authors conducted three user studies to examine how people revise beliefs, focusing on scenarios with explanations for inconsistencies and comparing provided versus self-formulated explanations.

Result: Findings show a consistent human preference for explanation-based belief revisions, even if such processes are non-minimal and deviate from classical belief change frameworks.

Conclusion: AI systems should integrate explanation-based belief revision mechanisms to align better with human cognitive processes and reasoning patterns.

Abstract: Understanding how humans revise their beliefs in light of new information is
crucial for developing AI systems which can effectively model, and thus align
with, human reasoning. While theoretical belief revision frameworks rely on a
set of principles that establish how these operations are performed, empirical
evidence from cognitive psychology suggests that people may follow different
patterns when presented with conflicting information. In this paper, we present
three comprehensive user studies showing that people consistently prefer
explanation-based revisions, i.e., those which are guided by explanations, that
result in changes to their belief systems that are not necessarily captured by
classical belief change theory. Our experiments systematically investigate how
people revise their beliefs with explanations for inconsistencies, whether they
are provided with them or left to formulate them themselves, demonstrating a
robust preference for what may seem non-minimal revisions across different
types of scenarios. These findings have implications for AI systems designed to
model human reasoning or interact with humans, suggesting that such systems
should accommodate explanation-based, potentially non-minimal belief revision
operators to better align with human cognitive processes.

</details>


### [11] [V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning](https://arxiv.org/abs/2506.09985)
*Mido Assran,Adrien Bardes,David Fan,Quentin Garrido,Russell Howes,Mojtaba,Komeili,Matthew Muckley,Ammar Rizvi,Claire Roberts,Koustuv Sinha,Artem Zholus,Sergio Arnaud,Abha Gejji,Ada Martin,Francois Robert Hogan,Daniel Dugas,Piotr Bojanowski,Vasil Khalidov,Patrick Labatut,Francisco Massa,Marc Szafraniec,Kapil Krishnakumar,Yong Li,Xiaodong Ma,Sarath Chandar,Franziska Meier,Yann LeCun,Michael Rabbat,Nicolas Ballas*

Main category: cs.AI

TL;DR: The paper combines internet-scale video data with limited robot interaction data for self-supervised learning, achieving strong performance in motion understanding, video Q&A, and zero-shot robotic planning.


<details>
  <summary>Details</summary>
Motivation: To teach AI systems to understand and act in the physical world largely by observation, leveraging self-supervised learning and large datasets.

Method: The authors pre-train a joint-embedding-predictive model (V-JEPA 2) on over 1 million hours of internet video, align it with a language model, and fine-tune it as V-JEPA 2-AC for robotic tasks using 62 hours of interaction data.

Result: The model achieves state-of-the-art results in human action anticipation, video question-answering, and zero-shot robotic planning without task-specific training or task rewards.

Conclusion: Self-supervised learning from large-scale video data combined with minimal robot interaction data can yield world models capable of general-purpose planning and understanding in the physical world.

Abstract: A major challenge for modern AI is to learn to understand the world and learn
to act largely by observation. This paper explores a self-supervised approach
that combines internet-scale video data with a small amount of interaction data
(robot trajectories), to develop models capable of understanding, predicting,
and planning in the physical world. We first pre-train an action-free
joint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset
comprising over 1 million hours of internet video. V-JEPA 2 achieves strong
performance on motion understanding (77.3 top-1 accuracy on Something-Something
v2) and state-of-the-art performance on human action anticipation (39.7
recall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models.
Additionally, after aligning V-JEPA 2 with a large language model, we
demonstrate state-of-the-art performance on multiple video question-answering
tasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on
TempCompass). Finally, we show how self-supervised learning can be applied to
robotic planning tasks by post-training a latent action-conditioned world
model, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the
Droid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two different
labs and enable picking and placing of objects using planning with image goals.
Notably, this is achieved without collecting any data from the robots in these
environments, and without any task-specific training or reward. This work
demonstrates how self-supervised learning from web-scale data and a small
amount of robot interaction data can yield a world model capable of planning in
the physical world.

</details>


### [12] [Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering](https://arxiv.org/abs/2506.06905)
*Akash Gupta,Amos Storkey,Mirella Lapata*

Main category: cs.AI

TL;DR: The paper introduces a meta-learning approach to improve Large Multimodal Models' (LMMs) ability to handle tasks with minimal examples, outperforming existing in-context learning methods.


<details>
  <summary>Details</summary>
Motivation: In-context learning (ICL) in LMMs is inconsistent, particularly in smaller models, due to being overwhelmed by unnecessary image information that hampers task performance.

Method: The study introduces a meta-learning approach using soft prompts distilled from task-relevant image features, an attention-mapper module integrated with LLaVA v1.5, and few-shot test-time adaptation with minimal gradient steps.

Result: The proposed method surpasses traditional ICL and related prompt-tuning methods, notably enhancing performance on tasks like visual question answering even under image perturbations.

Conclusion: The proposed meta-learning approach offers a robust alternative to ICL, enabling LMMs to adapt effectively under low-data conditions and improving task-relevant reasoning capabilities.

Abstract: Large Multimodal Models (LMMs) often rely on in-context learning (ICL) to
perform new tasks with minimal supervision. However, ICL performance,
especially in smaller LMMs, is inconsistent and does not always improve
monotonically with increasing examples. We hypothesize that this occurs due to
the LMM being overwhelmed by additional information present in the image
embeddings, which is not required for the downstream task. To address this, we
propose a meta-learning approach that provides an alternative for inducing
few-shot capabilities in LMMs, using a fixed set of soft prompts that are
distilled from task-relevant image features and can be adapted at test time
using a few examples. To facilitate this distillation, we introduce an
attention-mapper module that can be easily integrated with the popular LLaVA
v1.5 architecture and is jointly learned with soft prompts, enabling task
adaptation in LMMs under low-data regimes with just a few gradient steps.
Evaluation on the VL-ICL Bench shows that our method consistently outperforms
ICL and related prompt-tuning approaches, even under image perturbations,
improving task induction and reasoning across visual question answering tasks.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [13] [Exploiting Control-flow Enforcement Technology for Sound and Precise Static Binary Disassembly](https://arxiv.org/abs/2506.09426)
*Brian Zhao,Yiwei Yang,Yusheng Zheng,Andi Quinn*

Main category: cs.AR

TL;DR: The paper presents TVA, a method to rewrite x86_64 binaries more efficiently by leveraging Intel's CET, improving performance and security while avoiding substantial overhead.


<details>
  <summary>Details</summary>
Motivation: Rewriting x86_64 binaries is challenging due to instruction complexity and indirect jumps while existing solutions incur high overhead and infrastructure costs.

Method: Introduces TVA, which utilizes Intel's CET to limit valid indirect jump targets, reducing disassembly complexity and ensuring soundness even on systems without CET support.

Result: TVA improves instrumentation time by up to 1.3x compared to existing methods when applied to standard benchmarks and real applications.

Conclusion: TVA is a practical and efficient tool for x86_64 binary analysis and rewriting, combining robust security measures with high performance.

Abstract: Rewriting x86_64 binaries-whether for security hardening, dynamic
instrumentation, or performance profiling is notoriously difficult due to
variable-length instructions, interleaved code and data, and indirect jumps to
arbitrary byte offsets. Existing solutions (e.g., "superset disassembly")
ensure soundness but incur significant overhead and produce large rewritten
binaries, especially for on-the-fly instrumentation. This paper addresses these
challenges by introducing the Time Variance Authority (TVA), which leverages
Intel's Control-Flow Enforcement Technology (CET). By recognizing endbr64 as
the only valid indirect jump target, TVA prunes spurious disassembly paths
while preserving soundness and emulates CET constraints on processors lacking
native CET support, effectively mitigating ROP/JOP exploits without new
hardware. We implement TVA by modernizing the Multiverse rewriter for 64-bit
Linux. Our evaluation on SPEC CPU2017 and real-world applications shows that
TVA-guided rewriting achieves up to 1.3x faster instrumentation time. These
results underscore TVA's feasibility as a high-performance, uprobes-free
alternative for robust x86_64 binary analysis and rewriting.

</details>


### [14] [FPGA-Based Multiplier with a New Approximate Full Adder for Error-Resilient Applications](https://arxiv.org/abs/2506.09596)
*Ali Ranjbar,Elham Esmaeili,Roghayeh Rafieisangari,Nabiollah Shiri*

Main category: cs.AR

TL;DR: The paper introduces a novel approximate adder-based multiplier to improve speed and efficiency for signal processing applications, demonstrating significant power and accuracy gains.


<details>
  <summary>Details</summary>
Motivation: The primary aim is to enhance the multiplication process in electronic devices, focusing on low power consumption, compact design, and high-speed performance which are critical for VLSI and signal processing.

Method: The proposed multiplier uses a novel approximate adder to add partial products simultaneously, implemented in VHDL, and synthesized on the Xilinx Spartan3 FPGA to validate its effectiveness.

Result: The multiplier shows a power saving of 56.09% and a power-delay product improvement of 73.02%. It also enhances PSNR by 30.58% and SSIM by 22.22% compared to existing methods, demonstrating accuracy and energy efficiency.

Conclusion: The novel multiplier offers substantial improvements in power efficiency, speed, and accuracy, proving its practicality for real-world signal-processing tasks.

Abstract: Electronic devices primarily aim to offer low power consumption, high speed,
and a compact area. The performance of very large-scale integration (VLSI)
devices is influenced by arithmetic operations, where multiplication is a
crucial operation. Therefore, a high-speed multiplier is essential for
developing any signal-processing module. Numerous multipliers have been
reviewed in existing literature, and their speed is largely determined by how
partial products (PPs) are accumulated. To enhance the speed of multiplication
beyond current methods, an approximate adder-based multiplier is introduced.
This approach allows for the simultaneous addition of PPs from two consecutive
bits using a novel approximate adder. The proposed multiplier is utilized in a
mean filter structure and implemented in ISE Design Suite 14.7 using VHDL and
synthesized on the Xilinx Spartan3-XC3S400 FPGA board. Compared to the
literature, the proposed multiplier achieves power and power-delay product
(PDP) improvements of 56.09% and 73.02%, respectively. The validity of the
expressed multiplier is demonstrated through the mean filter system. Results
show that it achieves power savings of 33.33%. Additionally, the proposed
multiplier provides more accurate results than other approximate multipliers by
expressing higher values of peak signal-to-noise ratio (PSNR), (30.58%), and
structural similarity index metric (SSIM), (22.22%), while power consumption is
in a low range.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [15] [LLM-as-a-qualitative-judge: automating error analysis in natural language generation](https://arxiv.org/abs/2506.09147)
*Nadezhda Chirkova,Tunde Oluwaseyi Ajayi,Seth Aycock,Zain Muhammad Mujahid,Vladana Perlić,Ekaterina Borisova,Markarit Vartampetian*

Main category: cs.CL

TL;DR: This paper proposes the use of large language models (LLMs) to qualitatively evaluate natural language generation (NLG) outputs by generating structured reports highlighting common issues.


<details>
  <summary>Details</summary>
Motivation: To provide developers with actionable insights on improving NLG systems, moving beyond solely quantitative evaluation methods involving numerical scoring.

Method: The method involves a two-step process: open-ended per-instance issue analysis to identify problems and clustering of these issues using a cumulative algorithm to create structured error reports.

Result: The LLM-as-a-qualitative-judge approach successfully recognizes issues in 2/3 cases and produces reports resembling those created by human annotators, validated with ~300 annotations from 12 NLG datasets.

Conclusion: LLM-as-a-qualitative-judge is effective in providing developers with meaningful insights and structured error analysis for improving NLG systems.

Abstract: Prompting large language models (LLMs) to evaluate generated text, known as
LLM-as-a-judge, has become a standard evaluation approach in natural language
generation (NLG), but is primarily used as a quantitative tool, i.e. with
numerical scores as main outputs. In this work, we propose
LLM-as-a-qualitative-judge, an LLM-based evaluation approach with the main
output being a structured report of common issue types in the NLG system
outputs. Our approach is targeted at providing developers with meaningful
insights on what improvements can be done to a given NLG system and consists of
two main steps, namely open-ended per-instance issue analysis and clustering of
the discovered issues using an intuitive cumulative algorithm. We also
introduce a strategy for evaluating the proposed approach, coupled with ~300
annotations of issues in instances from 12 NLG datasets. Our results show that
LLM-as-a-qualitative-judge correctly recognizes instance-specific issues in 2/3
cases and is capable of producing error type reports resembling the reports
composed by human annotators. Our code and data are publicly available at
https://github.com/tunde-ajayi/llm-as-a-qualitative-judge.

</details>


### [16] [PHRASED: Phrase Dictionary Biasing for Speech Translation](https://arxiv.org/abs/2506.09175)
*Peidong Wang,Jian Xue,Rui Zhao,Junkun Chen,Aswin Shanmugam Subramanian,Jinyu Li*

Main category: cs.CL

TL;DR: A phrase dictionary biasing method improves speech translation, enhancing model accuracy and phrase recall.


<details>
  <summary>Details</summary>
Motivation: Translation of phrases is difficult in speech translation tasks due to their rare occurrence in training data.

Method: Introduced a phrase dictionary biasing approach applied to transducer-based streaming models and multimodal large language models.

Result: Significant improvements: 21% better performance for streaming models and 85% better phrase recall for multimodal models.

Conclusion: Using external phrase dictionaries successfully enhances phrase translation in both streaming and multimodal speech translation models.

Abstract: Phrases are essential to understand the core concepts in conversations.
However, due to their rare occurrence in training data, correct translation of
phrases is challenging in speech translation tasks. In this paper, we propose a
phrase dictionary biasing method to leverage pairs of phrases mapping from the
source language to the target language. We apply the phrase dictionary biasing
method to two types of widely adopted models, a transducer-based streaming
speech translation model and a multimodal large language model. Experimental
results show that the phrase dictionary biasing method outperforms phrase list
biasing by 21% relatively for the streaming speech translation model. In
addition, phrase dictionary biasing enables multimodal large language models to
use external phrase information, achieving 85% relative improvement in phrase
recall.

</details>


### [17] [A Technique for Isolating Lexically-Independent Phonetic Dependencies in Generative CNNs](https://arxiv.org/abs/2506.09218)
*Bruno Ferenc Šegedin*

Main category: cs.CL

TL;DR: This study explores the ability of convolutional neural networks (CNNs) to generalize phonotactic rules from lexical learning, introducing a novel probing technique with reduced fully-connected layer size.


<details>
  <summary>Details</summary>
Motivation: To understand whether deep neural networks can learn phonotactic generalizations independent of lexical constraints and investigate the role of architectural modifications like reducing the FC bottleneck.

Method: Generative CNNs are trained on raw audio waveforms, with comparisons of performance using a fully-connected (FC) bottleneck reduced from 1024 channels to 8, and introduce a probing method bypassing the FC layer for analysis.

Result: Outputs generated bypassing the FC layer still exhibit phonotactic preferences learned during training, indicating convolutional layers can generalize phonetic dependencies dynamically beyond lexically-constrained configurations.

Conclusion: Convolutional networks, particularly under narrow FC bottlenecks, are capable of lexically-independent phonotactic generalizations, highlighting their adaptability in language learning tasks.

Abstract: The ability of deep neural networks (DNNs) to represent phonotactic
generalizations derived from lexical learning remains an open question. This
study (1) investigates the lexically-invariant generalization capacity of
generative convolutional neural networks (CNNs) trained on raw audio waveforms
of lexical items and (2) explores the consequences of shrinking the
fully-connected layer (FC) bottleneck from 1024 channels to 8 before training.
Ultimately, a novel technique for probing a model's lexically-independent
generalizations is proposed that works only under the narrow FC bottleneck:
generating audio outputs by bypassing the FC and inputting randomized feature
maps into the convolutional block. These outputs are equally biased by a
phonotactic restriction in training as are outputs generated with the FC. This
result shows that the convolutional layers can dynamically generalize phonetic
dependencies beyond lexically-constrained configurations learned by the FC.

</details>


### [18] [Extrapolation by Association: Length Generalization Transfer in Transformers](https://arxiv.org/abs/2506.09251)
*Ziyang Cai,Nayoung Lee,Avi Schwarzschild,Samet Oymak,Dimitris Papailiopoulos*

Main category: cs.CL

TL;DR: The paper explores how transformer models can generalize from shorter to longer inputs by transferring generalization capabilities across related tasks, demonstrating this in areas like arithmetic, string transformation, and maze navigation.


<details>
  <summary>Details</summary>
Motivation: To understand the mechanisms behind the impressive generalization capabilities of transformer models, specifically in the context of length generalization (extrapolating from shorter to longer inputs).

Method: The authors analyze "length generalization transfer" by training models on longer auxiliary tasks and examining their ability to generalize to longer inputs in separate target tasks, across disciplines like arithmetic and maze navigation. They also explore pretrained language models and attention head mechanisms.

Result: Length generalization was observed to transfer between related tasks, indicating that joint training on similar tasks leads to improved extrapolation. Pretrained language models showed similar transfer effects, suggesting reusable computational scaffolding. Attention head reuse was found to correlate with length generalization.

Conclusion: Transformer models exhibit transferable length generalization when tasks are related, aided by reusable computational structures and shared attention mechanisms, enhancing our understanding of generalization to out-of-distribution inputs.

Abstract: Transformer language models have demonstrated impressive generalization
capabilities in natural language domains, yet we lack a fine-grained
understanding of how such generalization arises. In this paper, we investigate
length generalization--the ability to extrapolate from shorter to longer
inputs--through the lens of \textit{task association}. We find that length
generalization can be \textit{transferred} across related tasks. That is,
training a model with a longer and related auxiliary task can lead it to
generalize to unseen and longer inputs from some other target task. We
demonstrate this length generalization transfer across diverse algorithmic
tasks, including arithmetic operations, string transformations, and maze
navigation. Our results show that transformer models can inherit generalization
capabilities from similar tasks when trained jointly. Moreover, we observe
similar transfer effects in pretrained language models, suggesting that
pretraining equips models with reusable computational scaffolding that
facilitates extrapolation in downstream settings. Finally, we provide initial
mechanistic evidence that length generalization transfer correlates with the
re-use of the same attention heads between the tasks. Together, our findings
deepen our understanding of how transformers generalize to out-of-distribution
inputs and highlight the compositional reuse of inductive structure across
tasks.

</details>


### [19] [Self-Anchored Attention Model for Sample-Efficient Classification of Prosocial Text Chat](https://arxiv.org/abs/2506.09259)
*Zhuofang Li,Rafal Kocielnik,Fereshteh Soltani,Penphob,Boonyarungsrit,Animashree Anandkumar,R. Michael Alvarez*

Main category: cs.CL

TL;DR: The paper focuses on detecting and classifying prosocial behaviors in online game chats, proposing the novel Self-Anchored Attention Model (SAAM) to overcome the lack of training data and showing its application in Call of Duty: Modern Warfare II.


<details>
  <summary>Details</summary>
Motivation: Shift the focus of online gaming moderation from solely filtering toxic behaviors to also identifying and promoting prosocial communication, which is crucial for healthy gaming environments.

Method: The authors used unsupervised discovery combined with expert analysis to identify prosocial behaviors and introduced the Self-Anchored Attention Model (SAAM), which uses the full training set as anchors to enhance performance in low-resource settings.

Result: SAAM delivered a 7.9% improvement over the best existing technique for classifying prosocial behaviors in game chats. This led to the creation of the first automated system suitable for low-resource, labeled-data environments.

Conclusion: The study provides a novel framework and practical application for detecting prosocial behaviors in gaming chats, enabling a shift in moderation practices from penalization to encouragement of positive interactions on online platforms.

Abstract: Millions of players engage daily in competitive online games, communicating
through in-game chat. Prior research has focused on detecting relatively small
volumes of toxic content using various Natural Language Processing (NLP)
techniques for the purpose of moderation. However, recent studies emphasize the
importance of detecting prosocial communication, which can be as crucial as
identifying toxic interactions. Recognizing prosocial behavior allows for its
analysis, rewarding, and promotion. Unlike toxicity, there are limited
datasets, models, and resources for identifying prosocial behaviors in
game-chat text. In this work, we employed unsupervised discovery combined with
game domain expert collaboration to identify and categorize prosocial player
behaviors from game chat. We further propose a novel Self-Anchored Attention
Model (SAAM) which gives 7.9% improvement compared to the best existing
technique. The approach utilizes the entire training set as "anchors" to help
improve model performance under the scarcity of training data. This approach
led to the development of the first automated system for classifying prosocial
behaviors in in-game chats, particularly given the low-resource settings where
large-scale labeled data is not available. Our methodology was applied to one
of the most popular online gaming titles - Call of Duty(R): Modern
Warfare(R)II, showcasing its effectiveness. This research is novel in applying
NLP techniques to discover and classify prosocial behaviors in player in-game
chat communication. It can help shift the focus of moderation from solely
penalizing toxicity to actively encouraging positive interactions on online
platforms.

</details>


### [20] [Did I Faithfully Say What I Thought? Bridging the Gap Between Neural Activity and Self-Explanations in Large Language Models](https://arxiv.org/abs/2506.09277)
*Milan Bhan,Jean-Noel Vittaut,Nicolas Chesneau,Sarath Chandar,Marie-Jeanne Lesot*

Main category: cs.CL

TL;DR: This paper introduces a framework to measure the faithfulness of explanations generated by large language models (self-NLE) by comparing them with interpretations of the model's hidden states.


<details>
  <summary>Details</summary>
Motivation: Large Language Models often produce self-explanations to justify their answers, but these explanations may not truthfully reflect the model's decision-making processes.

Method: The framework quantitatively measures faithfulness by comparing LLM-generated explanations with neural interpretations of the model's internal hidden states.

Result: The framework provides insights into the connection between self-NLE and the underlying decision-making processes in LLMs.

Conclusion: This approach enhances the understanding of self-NLE faithfulness and helps in creating more trustworthy explanations for LLM outputs.

Abstract: Large Language Models (LLM) have demonstrated the capability of generating
free text self Natural Language Explanation (self-NLE) to justify their
answers. Despite their logical appearance, self-NLE do not necessarily reflect
the LLM actual decision-making process, making such explanations unfaithful.
While existing methods for measuring self-NLE faithfulness mostly rely on
behavioral tests or computational block identification, none of them examines
the neural activity underlying the model's reasoning. This work introduces a
novel flexible framework for quantitatively measuring the faithfulness of
LLM-generated self-NLE by directly comparing the latter with interpretations of
the model's internal hidden states. The proposed framework is versatile and
provides deep insights into self-NLE faithfulness by establishing a direct
connection between self-NLE and model reasoning. This approach advances the
understanding of self-NLE faithfulness and provides building blocks for
generating more faithful self-NLE.

</details>


### [21] [$(RSA)^2$: A Rhetorical-Strategy-Aware Rational Speech Act Framework for Figurative Language Understanding](https://arxiv.org/abs/2506.09301)
*Cesare Spinoso-Di Piano,David Austin,Pablo Piantanida,Jackie Chi Kit Cheung*

Main category: cs.CL

TL;DR: The paper introduces a framework, $(RSA)^2$, for interpreting figurative language, improving upon Rational Speech Act (RSA) by incorporating rhetorical strategies. This approach, combined with LLMs, achieves strong performance on irony interpretation.


<details>
  <summary>Details</summary>
Motivation: Existing RSA implementations struggle with figurative language or require context-specific modeling of motivations for non-literal expressions. A more generalized approach is needed to handle rhetorical strategies.

Method: The $(RSA)^2$ framework enhances the RSA framework by incorporating rhetorical strategies to model non-literal language use. It avoids the need to explicitly model speaker motivations and works with LLMs.

Result: $(RSA)^2$ achieves human-compatible interpretations of figurative language and sets state-of-the-art performance on the ironic subset of the PragMega+ dataset.

Conclusion: $(RSA)^2$ offers a significant advancement in probabilistic pragmatics by effectively handling figurative language through rhetorical strategy modeling.

Abstract: Figurative language (e.g., irony, hyperbole, understatement) is ubiquitous in
human communication, resulting in utterances where the literal and the intended
meanings do not match. The Rational Speech Act (RSA) framework, which
explicitly models speaker intentions, is the most widespread theory of
probabilistic pragmatics, but existing implementations are either unable to
account for figurative expressions or require modeling the implicit motivations
for using figurative language (e.g., to express joy or annoyance) in a
setting-specific way. In this paper, we introduce the Rhetorical-Strategy-Aware
RSA $(RSA)^2$ framework which models figurative language use by considering a
speaker's employed rhetorical strategy. We show that $(RSA)^2$ enables
human-compatible interpretations of non-literal utterances without modeling a
speaker's motivations for being non-literal. Combined with LLMs, it achieves
state-of-the-art performance on the ironic split of PragMega+, a new irony
interpretation dataset introduced in this study.

</details>


### [22] [Alzheimer's Dementia Detection Using Perplexity from Paired Large Language Models](https://arxiv.org/abs/2506.09315)
*Yao Xiao,Heidi Christensen,Stefan Goetze*

Main category: cs.CL

TL;DR: The paper enhances Alzheimer’s dementia (AD) detection using a fine-tuned Mistral-7B large language model, achieving improved accuracy and interpretable results.


<details>
  <summary>Details</summary>
Motivation: The detection of Alzheimer’s dementia involves identifying cognitive decline, specifically its impact on language abilities, and improving upon existing methods using advanced language models.

Method: The authors extended the paired perplexity approach with a fine-tuned instruction-following Mistral-7B model and improved decision boundary interpretability.

Result: The approach achieved a 3.33% accuracy improvement over the best paired perplexity method and a 6.35% improvement compared to the top method from ADReSS 2020. Additionally, it demonstrated clearer decision boundaries and highlighted unique language patterns in AD speakers.

Conclusion: The study shows that fine-tuned LLMs can surpass current metrics in AD detection, offer interpretable results, and pave the way for novel approaches to data augmentation and model understanding.

Abstract: Alzheimer's dementia (AD) is a neurodegenerative disorder with cognitive
decline that commonly impacts language ability. This work extends the paired
perplexity approach to detecting AD by using a recent large language model
(LLM), the instruction-following version of Mistral-7B. We improve accuracy by
an average of 3.33% over the best current paired perplexity method and by 6.35%
over the top-ranked method from the ADReSS 2020 challenge benchmark. Our
further analysis demonstrates that the proposed approach can effectively detect
AD with a clear and interpretable decision boundary in contrast to other
methods that suffer from opaque decision-making processes. Finally, by
prompting the fine-tuned LLMs and comparing the model-generated responses to
human responses, we illustrate that the LLMs have learned the special language
patterns of AD speakers, which opens up possibilities for novel methods of
model interpretation and data augmentation.

</details>


### [23] [Towards Efficient and Effective Alignment of Large Language Models](https://arxiv.org/abs/2506.09329)
*Yuxin Jiang*

Main category: cs.CL

TL;DR: This thesis introduces innovative methods for improving LLM alignment through advanced data collection, training, and evaluation techniques. It emphasizes iterative refinement, automated instruction-tuning, efficient integration of new knowledge, and a new benchmark for assessing adherence to constraints.


<details>
  <summary>Details</summary>
Motivation: The motivation is to solve the critical challenge of effectively aligning large language models (LLMs) with human expectations in diverse tasks.

Method: The paper proposes three main methods: Lion for refining training data using adversarial distillation, WebR for automated instruction-tuning via raw web documents, and two training frameworks LTE (Learning to Edit) and BMC (Bridging and Modeling Correlations) for optimizing alignment.

Result: It achieves state-of-the-art zero-shot reasoning with the Lion framework, improves data scalability with WebR, and enhances alignment training effectiveness using LTE and BMC frameworks. It also identifies weaknesses in current LLMs in adhering to complex constraints using the FollowBench benchmark.

Conclusion: The study presents a comprehensive strategy to significantly improve LLM alignment by introducing novel approaches in data collection, training optimization, and evaluation methods, addressing key weaknesses in existing models and setting directions for further research.

Abstract: Large language models (LLMs) exhibit remarkable capabilities across diverse
tasks, yet aligning them efficiently and effectively with human expectations
remains a critical challenge. This thesis advances LLM alignment by introducing
novel methodologies in data collection, training, and evaluation. We first
address alignment data collection. Existing approaches rely heavily on manually
curated datasets or proprietary models. To overcome these limitations, we
propose Lion, an adversarial distillation framework that iteratively refines
training data by identifying and generating challenging instructions, enabling
state-of-the-art zero-shot reasoning. Additionally, we introduce Web
Reconstruction (WebR), a fully automated framework that synthesizes
instruction-tuning data directly from raw web documents, significantly
improving data diversity and scalability over existing synthetic data methods.
Next, we enhance alignment training through novel optimization techniques. We
develop Learning to Edit (LTE), a framework that enables LLMs to efficiently
integrate new knowledge while preserving existing information. LTE leverages
meta-learning to improve both real-time and batch knowledge updates.
Furthermore, we introduce Bridging and Modeling Correlations (BMC), a
refinement of Direct Preference Optimization (DPO) that explicitly captures
token-level correlations in preference data, leading to superior alignment
across QA and mathematical reasoning tasks. Finally, we tackle the challenge of
evaluating alignment. Existing benchmarks emphasize response quality but
overlook adherence to specific constraints. To bridge this gap, we introduce
FollowBench, a multi-level, fine-grained benchmark assessing LLMs' ability to
follow complex constraints across diverse instruction types. Our results expose
key weaknesses in current models' constraint adherence, offering insights for
future improvements.

</details>


### [24] [Multi-Agent Language Models: Advancing Cooperation, Coordination, and Adaptation](https://arxiv.org/abs/2506.09331)
*Arjun Vaithilingam Sudhakar*

Main category: cs.CL

TL;DR: The paper explores whether Large Language Models (LLMs) can understand and reason about others' intentions, termed as 'theory of mind,' in the context of multi-agent reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: The study seeks to understand if LLMs can infer and reason about the intentions of others, a crucial capability for effective collaboration and cooperative human-agent interactions.

Method: The authors investigate the theory of mind in LLMs by integrating them into cooperative multi-agent reinforcement learning (MARL). This framework enables agents to learn collaboration and social reasoning through repeated interactions.

Result: The research demonstrates that LLM-based agents can adapt and cooperate in multi-agent and hybrid human-AI systems, opening pathways for seamless interaction and collaboration.

Conclusion: This study advances LLM applications by showcasing their potential for fostering hybrid human-AI collaboration, emphasizing their role in cooperative and socially intelligent systems.

Abstract: Modern Large Language Models (LLMs) exhibit impressive zero-shot and few-shot
generalization capabilities across complex natural language tasks, enabling
their widespread use as virtual assistants for diverse applications such as
translation and summarization. Despite being trained solely on large corpora of
text without explicit supervision on author intent, LLMs appear to infer the
underlying meaning of textual interactions. This raises a fundamental question:
can LLMs model and reason about the intentions of others, i.e., do they possess
a form of theory of mind? Understanding other's intentions is crucial for
effective collaboration, which underpins human societal success and is
essential for cooperative interactions among multiple agents, including humans
and autonomous systems. In this work, we investigate the theory of mind in LLMs
through the lens of cooperative multi-agent reinforcement learning (MARL),
where agents learn to collaborate via repeated interactions, mirroring human
social reasoning. Our approach aims to enhance artificial agent's ability to
adapt and cooperate with both artificial and human partners. By leveraging
LLM-based agents capable of natural language interaction, we move towards
creating hybrid human-AI systems that can foster seamless collaboration, with
broad implications for the future of human-artificial interaction.

</details>


### [25] [RePO: Replay-Enhanced Policy Optimization](https://arxiv.org/abs/2506.09340)
*Siheng Li,Zhanhui Zhou,Wai Lam,Chao Yang,Chaochao Lu*

Main category: cs.CL

TL;DR: RePO introduces replay strategies for retrieving off-policy samples to optimize LLM performance, significantly improving efficiency and results over GRPO.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning methods like GRPO incur high computational costs and inefficiencies in optimizing large language models.

Method: The proposed RePO framework uses a replay buffer to retrieve diverse off-policy samples for broader policy optimization per prompt.

Result: RePO outperforms GRPO with average performance gains of 18.4 and 4.1 points on benchmarks, along with enhanced optimization efficiency.

Conclusion: RePO balances increased computational cost with substantially improved optimization efficacy and performance for large language models.

Abstract: Reinforcement learning (RL) is vital for optimizing large language models
(LLMs). Recent Group Relative Policy Optimization (GRPO) estimates advantages
using multiple on-policy outputs per prompt, leading to high computational
costs and low data efficiency. To address this, we introduce Replay-Enhanced
Policy Optimization (RePO), which leverages diverse replay strategies to
retrieve off-policy samples from a replay buffer, allowing policy optimization
based on a broader and more diverse set of samples for each prompt. Experiments
on five LLMs across seven mathematical reasoning benchmarks demonstrate that
RePO achieves absolute average performance gains of $18.4$ and $4.1$ points for
Qwen2.5-Math-1.5B and Qwen3-1.7B, respectively, compared to GRPO. Further
analysis indicates that RePO increases computational cost by $15\%$ while
raising the number of effective optimization steps by $48\%$ for Qwen3-1.7B,
with both on-policy and off-policy sample numbers set to $8$. The repository
can be accessed at https://github.com/SihengLi99/RePO.

</details>


### [26] [Latent Multi-Head Attention for Small Language Models](https://arxiv.org/abs/2506.09342)
*Sushant Mehta,Raj Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.CL

TL;DR: The paper introduces an efficiency-quality trade-off study for latent multi-head attention (MLA) in small GPT language models, revealing significant memory savings and computational speed-ups with minimal quality loss.


<details>
  <summary>Details</summary>
Motivation: To investigate if latent multi-head attention (MLA) can achieve a balance between computational efficiency and model quality in small language models, especially for memory-constrained deployments.

Method: Three architectural variants of GPT models (MHA, MLA, MLA+RoPE) were benchmarked on a synthetic dataset using experimental setups to measure memory usage, validation loss, inference speed, and evaluation scores.

Result: MLA+RoPE with half-rank dimensions achieved 45% memory savings and a 1.4x inference speedup at the cost of only a 0.3% increase in validation loss. RoPE significantly enhanced MLA's performance, improving validation metrics by up to 2% compared to vanilla configurations.

Conclusion: The study identifies MLA+RoPE as a Pareto-optimal design for small models, offering substantial memory and computational advantages without compromising model quality, making it highly suitable for deployment scenarios with resource constraints.

Abstract: We present the first comprehensive study of latent multi-head attention (MLA)
for small language models, revealing interesting efficiency-quality trade-offs.
Training 30M-parameter GPT models on 100,000 synthetic stories, we benchmark
three architectural variants: standard multi-head attention (MHA), MLA, and MLA
with rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE
with half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory
reduction while incurring only a 0.3% increase in validation loss (essentially
matching MHA quality)- a Pareto improvement for memory constrained deployment.
We further show that RoPE is crucial for MLA in small models: without it, MLA
underperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by
2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2
achieves a 1.4 times speedup over full-rank MLA while maintaining the memory
savings. GPT-4 evaluations corroborate perplexity results, with ours achieving
the highest quality scores (7.4/10) across grammar, creativity, and consistency
metrics. Code and models will be released upon acceptance.

</details>


### [27] [RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic Sampling](https://arxiv.org/abs/2506.08672)
*Yang Liu,Jiaqi Li,Zilong Zheng*

Main category: cs.CL

TL;DR: This paper introduces RuleReasoner, a method for effective rule-based reasoning using small reasoning models that outperforms large reasoning models in various benchmarks while improving computational efficiency.


<details>
  <summary>Details</summary>
Motivation: The challenge of rule-based reasoning in complex real-world scenarios coupled with limitations in generalization and resources for large reasoning models necessitates exploration of effective small reasoning models.

Method: RuleReasoner employs a novel domain-aware dynamic sampling strategy in reinforcement learning, updating sampling weights based on historical rewards to enable augmentation and adaptive online learning.

Result: RuleReasoner surpasses large reasoning models significantly in both in-distribution (ID) and out-of-distribution (OOD) benchmarks, achieving +4.1% on ID and +10.4% on OOD benchmarks, while maintaining higher computational efficiency.

Conclusion: RuleReasoner demonstrates that small reasoning models can excel in rule-based reasoning tasks with enhanced generalization and efficiency, setting a new standard for reasoning approaches.

Abstract: Rule-based reasoning has been acknowledged as one of the fundamental problems
in reasoning, while deviations in rule formats, types, and complexity in
real-world applications pose severe challenges. Recent studies have shown that
large reasoning models (LRMs) have remarkable reasoning capabilities, and their
performance is substantially enhanced by reinforcement learning (RL). However,
it remains an open question whether small reasoning models (SRMs) can learn
rule-based reasoning effectively with robust generalization across diverse
tasks and domains. To address this, we introduce Reinforced Rule-based
Reasoning, a.k.a. RuleReasoner, a simple yet effective method to conduct
rule-based reasoning via a wide collection of curated tasks and a novel
domain-aware dynamic sampling approach. Specifically, RuleReasoner resamples
each training batch by updating the sampling weights of different domains based
on historical rewards. This facilitates domain augmentation and flexible online
learning schedules for RL, obviating the need for pre-hoc human-engineered
mix-training recipes used in existing methods. Empirical evaluations on
in-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that
RuleReasoner outperforms frontier LRMs by a significant margin ($\Delta$4.1%
average points on eight ID tasks and $\Delta$10.4% average points on three OOD
tasks over OpenAI-o1). Notably, our approach also exhibits higher computational
efficiency compared to prior dynamic sampling methods for RL.

</details>


### [28] [OmniDRCA: Parallel Speech-Text Foundation Model via Dual-Resolution Speech Representations and Contrastive Alignment](https://arxiv.org/abs/2506.09349)
*Chao-Hong Tan,Qian Chen,Wen Wang,Chong Deng,Qinglin Zhang,Luyao Cheng,Hai Yu,Xin Zhang,Xiang Lv,Tianyu Zhao,Chong Zhang,Yukun Ma,Yafeng Chen,Hui Wang,Jiaqing Liu,Jieping Ye*

Main category: cs.CL

TL;DR: The paper introduces OmniDRCA, a joint speech-text foundation model boosting state-of-the-art performances with parallel modeling and cross-modal alignment.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing LLM-based speech generation methods that either lack modality awareness or struggle with integrated processing.

Method: Joint autoregressive modeling with dual-resolution speech representations and contrastive cross-modal alignment, allowing mutual processing of speech and text.

Result: OmniDRCA achieves state-of-the-art results in Spoken Question Answering tasks compared to similar foundation models and shows competitive performance with interleaved models.

Conclusion: OmniDRCA advances speech-text generative modeling and explores extensions to full-duplex conversational applications.

Abstract: Recent studies on end-to-end speech generation with large language models
(LLMs) have attracted significant community attention, with multiple works
extending text-based LLMs to generate discrete speech tokens. Existing
approaches primarily fall into two categories: (1) Methods that generate
discrete speech tokens independently without incorporating them into the LLM's
autoregressive process, resulting in text generation being unaware of
concurrent speech synthesis. (2) Models that generate interleaved or parallel
speech-text tokens through joint autoregressive modeling, enabling mutual
modality awareness during generation. This paper presents OmniDRCA, a parallel
speech-text foundation model based on joint autoregressive modeling, featuring
dual-resolution speech representations and contrastive cross-modal alignment.
Our approach processes speech and text representations in parallel while
enhancing audio comprehension through contrastive alignment. Experimental
results on Spoken Question Answering benchmarks demonstrate that OmniDRCA
establishes new state-of-the-art (SOTA) performance among parallel joint
speech-text modeling based foundation models, and achieves competitive
performance compared to interleaved models. Additionally, we explore the
potential of extending the framework to full-duplex conversational scenarios.

</details>


### [29] [DIVE into MoE: Diversity-Enhanced Reconstruction of Large Language Models from Dense into Mixture-of-Experts](https://arxiv.org/abs/2506.09351)
*Yuchen Feng,Bowen Shen,Naibin Gu,Jiaxuan Zhao,Peng Fu,Zheng Lin,Weiping Wang*

Main category: cs.CL

TL;DR: The paper introduces DIVE, a method to reconstruct dense LLMs into Mixture-of-Experts models while preserving diversity among experts and improving training efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing methods for reconstructing dense LLMs into MoE LLMs often fail to preserve expert diversity, leading to redundancy and reduced efficiency.

Method: The authors propose DIVE, which includes domain affinity mining, pruning-based expert reconstruction, and efficient retraining. The process involves pruning and reassembling FFN modules, followed by retraining on routers, experts, and normalization modules.

Result: DIVE, implemented on Llama-style LLMs, achieves high training efficiency with minimal accuracy loss and outperforms existing methods with the same parameter activation count.

Conclusion: The DIVE method provides an effective strategy for MoE LLM reconstruction, balancing efficiency and performance while addressing the limitations of current approaches.

Abstract: Large language models (LLMs) with the Mixture-of-Experts (MoE) architecture
achieve high cost-efficiency by selectively activating a subset of the
parameters. Despite the inference efficiency of MoE LLMs, the training of
extensive experts from scratch incurs substantial overhead, whereas
reconstructing a dense LLM into an MoE LLM significantly reduces the training
budget. However, existing reconstruction methods often overlook the diversity
among experts, leading to potential redundancy. In this paper, we come up with
the observation that a specific LLM exhibits notable diversity after being
pruned on different calibration datasets, based on which we present a
Diversity-Enhanced reconstruction method named DIVE. The recipe of DIVE
includes domain affinity mining, pruning-based expert reconstruction, and
efficient retraining. Specifically, the reconstruction includes pruning and
reassembly of the feed-forward network (FFN) module. After reconstruction, we
efficiently retrain the model on routers, experts and normalization modules. We
implement DIVE on Llama-style LLMs with open-source training corpora.
Experiments show that DIVE achieves training efficiency with minimal accuracy
trade-offs, outperforming existing pruning and MoE reconstruction methods with
the same number of activated parameters.

</details>


### [30] [Taming SQL Complexity: LLM-Based Equivalence Evaluation for Text-to-SQL](https://arxiv.org/abs/2506.09359)
*Qingyun Zeng,Simin Ma,Arash Niknafs,Ashish Basran,Carol Szabo*

Main category: cs.CL

TL;DR: The paper investigates using Large Language Models (LLMs) to evaluate if different SQL queries are semantically equivalent, addressing challenges like user ambiguity and multiple valid SQL outputs.


<details>
  <summary>Details</summary>
Motivation: To improve evaluation of Text-to-SQL (NL2SQL) systems by addressing the issue of semantic equivalence in SQL, which is difficult due to ambiguous queries and multiple correct outputs.

Method: The authors analyzed common SQL equivalence patterns and proposed using LLMs to determine both strict and practical (weak) semantic equivalence of SQL queries.

Result: The study identifies challenges in using LLMs for SQL equivalence evaluation and provides insights into frequent patterns of SQL equivalence and inequivalence.

Conclusion: Using LLMs to evaluate SQL queries holds promise but faces challenges in correctly determining semantic equivalence, particularly when user queries are ambiguous.

Abstract: The rise of Large Language Models (LLMs) has significantly advanced
Text-to-SQL (NL2SQL) systems, yet evaluating the semantic equivalence of
generated SQL remains a challenge, especially given ambiguous user queries and
multiple valid SQL interpretations. This paper explores using LLMs to assess
both semantic and a more practical "weak" semantic equivalence. We analyze
common patterns of SQL equivalence and inequivalence, discuss challenges in
LLM-based evaluation.

</details>


### [31] [ComfyUI-R1: Exploring Reasoning Models for Workflow Generation](https://arxiv.org/abs/2506.09790)
*Zhenran Xu,Yiyu Wang,Xue Yang,Longyue Wang,Weihua Luo,Kaifu Zhang,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: The paper introduces ComfyUI-R1, a reasoning model designed to automate workflow generation for AI content creation, achieving high accuracy in format validity and fidelity.


<details>
  <summary>Details</summary>
Motivation: The authors aim to simplify the creation of AI content workflows, which currently requires significant expertise to manage complex components.

Method: The paper uses a two-stage model training framework: (1) chain-of-thought fine-tuning for domain adaptation, and (2) reinforcement learning with fine-grained rewards to ensure reasoning capabilities and fidelity.

Result: ComfyUI-R1 achieves a 97% format validity rate and outperforms existing methods in pass rate, node-level, and graph-level F1 scores, excelling in synthesizing complex workflows.

Conclusion: The model demonstrates the effectiveness of long chain-of-thought reasoning and code-based workflow transformation, paving the way for more accessible and advanced AI art creation pipelines.

Abstract: AI-generated content has evolved from monolithic models to modular workflows,
particularly on platforms like ComfyUI, enabling customization in creative
pipelines. However, crafting effective workflows requires great expertise to
orchestrate numerous specialized components, presenting a steep learning curve
for users. To address this challenge, we introduce ComfyUI-R1, the first large
reasoning model for automated workflow generation. Starting with our curated
dataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning
data, including node selection, workflow planning, and code-level workflow
representation. ComfyUI-R1 is trained through a two-stage framework: (1) CoT
fine-tuning for cold start, adapting models to the ComfyUI domain; (2)
reinforcement learning for incentivizing reasoning capability, guided by a
fine-grained rule-metric hybrid reward, ensuring format validity, structural
integrity, and node-level fidelity. Experiments show that our 7B-parameter
model achieves a 97\% format validity rate, along with high pass rate,
node-level and graph-level F1 scores, significantly surpassing prior
state-of-the-art methods that employ leading closed-source models such as
GPT-4o and Claude series. Further analysis highlights the critical role of the
reasoning process and the advantage of transforming workflows into code.
Qualitative comparison reveals our strength in synthesizing intricate workflows
with diverse nodes, underscoring the potential of long CoT reasoning in AI art
creation.

</details>


### [32] [COGENT: A Curriculum-oriented Framework for Generating Grade-appropriate Educational Content](https://arxiv.org/abs/2506.09367)
*Zhengyuan Liu,Stella Xin Yin,Dion Hoe-Lian Goh,Nancy F. Chen*

Main category: cs.CL

TL;DR: The paper proposes COGENT, a framework using generative AI to create grade-appropriate educational content aligned with curriculum standards.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from existing generative AI shortcomings in producing educational content that meets curriculum standards and grade-specific readability, especially for STEM topics.

Method: COGENT integrates curriculum components (science concepts, core ideas, and learning objectives), readability controls, and a wonder-based engagement approach. It evaluates content quality using AI and human experts.

Result: Experimental results reveal COGENT generates grade-appropriate content that matches or outperforms human-created references.

Conclusion: COGENT provides an effective solution for producing scalable, adaptive, and high-quality educational materials aligned to curriculum needs.

Abstract: While Generative AI has demonstrated strong potential and versatility in
content generation, its application to educational contexts presents several
challenges. Models often fail to align with curriculum standards and maintain
grade-appropriate reading levels consistently. Furthermore, STEM education
poses additional challenges in balancing scientific explanations with everyday
language when introducing complex and abstract ideas and phenomena to younger
students. In this work, we propose COGENT, a curriculum-oriented framework for
generating grade-appropriate educational content. We incorporate three
curriculum components (science concepts, core ideas, and learning objectives),
control readability through length, vocabulary, and sentence complexity, and
adopt a ``wonder-based'' approach to increase student engagement and interest.
We conduct a multi-dimensional evaluation via both LLM-as-a-judge and human
expert analysis. Experimental results show that COGENT consistently produces
grade-appropriate passages that are comparable or superior to human references.
Our work establishes a viable approach for scaling adaptive and high-quality
learning resources.

</details>


### [33] [CoLMbo: Speaker Language Model for Descriptive Profiling](https://arxiv.org/abs/2506.09375)
*Massa Baali,Shuo Han,Syed Abdul Hannan,Purusottam Samal,Karanveer Singh,Soham Deshmukh,Rita Singh,Bhiksha Raj*

Main category: cs.CL

TL;DR: The paper proposes CoLMbo, a model combining speaker embedding and prompt-based conditioning to generate detailed speaker captions, including demographic traits.


<details>
  <summary>Details</summary>
Motivation: Traditional speaker recognition systems lack the ability to provide detailed, context-rich speaker attributes, such as demographics, in a structured way.

Method: The proposed system, CoLMbo, integrates a speaker encoder with prompt-based conditioning, enabling the generation of customized descriptions of speakers based on user-defined prompts.

Result: CoLMbo demonstrates versatility in adapting to new speaker characteristics in zero-shot scenarios and shows excellent performance across diverse datasets.

Conclusion: CoLMbo advances speaker recognition by enabling detailed speaker profiling and offering dynamic adaptability to unstructured tasks.

Abstract: Speaker recognition systems are often limited to classification tasks and
struggle to generate detailed speaker characteristics or provide context-rich
descriptions. These models primarily extract embeddings for speaker
identification but fail to capture demographic attributes such as dialect,
gender, and age in a structured manner. This paper introduces CoLMbo, a Speaker
Language Model (SLM) that addresses these limitations by integrating a speaker
encoder with prompt-based conditioning. This allows for the creation of
detailed captions based on speaker embeddings. CoLMbo utilizes user-defined
prompts to adapt dynamically to new speaker characteristics and provides
customized descriptions, including regional dialect variations and age-related
traits. This innovative approach not only enhances traditional speaker
profiling but also excels in zero-shot scenarios across diverse datasets,
marking a significant advancement in the field of speaker recognition.

</details>


### [34] [Binary classification for perceived quality of headlines and links on worldwide news websites, 2018-2024](https://arxiv.org/abs/2506.09381)
*Austin McCutcheon,Thiago E. A. de Oliveira,Aleksandr Zheleznov,Chris Brogly*

Main category: cs.CL

TL;DR: The study explores automatic differentiation of perceived news headline quality using machine learning models, with DistilBERT achieving the best accuracy (90.3%).


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the issue of low-quality news headlines/links spreading online by finding automated ways to differentiate between low and high-quality news.

Method: The researchers used 12 machine learning models on a balanced dataset of 57,544,214 news links/headlines labeled by expert consensus. They also extracted 115 linguistic features for performance evaluation.

Result: Traditional ensemble methods, especially the Bagging Classifier, performed strongly (88.1% accuracy and 88.3% F1 scores). Fine-tuned DistilBERT achieved the highest accuracy (90.3%), though it required more training time.

Conclusion: Both NLP-based traditional classifiers and deep learning models can effectively distinguish news quality, with trade-offs between performance and training time.

Abstract: The proliferation of online news enables potential widespread publication of
perceived low-quality news headlines/links. As a result, we investigated
whether it was possible to automatically distinguish perceived lower-quality
news headlines/links from perceived higher-quality headlines/links. We
evaluated twelve machine learning models on a binary, balanced dataset of
57,544,214 worldwide news website links/headings from 2018-2024 (28,772,107 per
class) with 115 extracted linguistic features. Binary labels for each text were
derived from scores based on expert consensus regarding the respective news
domain quality. Traditional ensemble methods, particularly the bagging
classifier, had strong performance (88.1% accuracy, 88.3% F1, 80/20 train/test
split). Fine-tuned DistilBERT achieved the highest accuracy (90.3%, 80/20
train/test split) but required more training time. The results suggest that
both NLP features with traditional classifiers and deep learning models can
effectively differentiate perceived news headline/link quality, with some
trade-off between predictive performance and train time.

</details>


### [35] [Comparing human and LLM politeness strategies in free production](https://arxiv.org/abs/2506.09391)
*Haoran Zhao,Robert D. Hawkins*

Main category: cs.CL

TL;DR: The paper studies how large language models (LLMs) handle politeness strategies in communication and compares their responses to human behavior.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore if large language models can replicate the nuanced politeness strategies humans use in communication, balancing social and informational goals.

Method: The authors compare human and LLM responses in structured and open-ended scenarios, focusing on whether LLMs employ context-sensitive politeness strategies effectively.

Result: Larger LLMs perform well in replicating human-like preferences for politeness strategies and are preferred by human evaluators in open-ended tasks. However, they tend to overuse negative politeness strategies even in contexts where positive ones are more appropriate.

Conclusion: While the models exhibit impressive politeness capabilities, their tendency to misapply strategies raises concerns about pragmatic alignment in AI systems.

Abstract: Polite speech poses a fundamental alignment challenge for large language
models (LLMs). Humans deploy a rich repertoire of linguistic strategies to
balance informational and social goals -- from positive approaches that build
rapport (compliments, expressions of interest) to negative strategies that
minimize imposition (hedging, indirectness). We investigate whether LLMs employ
a similarly context-sensitive repertoire by comparing human and LLM responses
in both constrained and open-ended production tasks. We find that larger models
($\ge$70B parameters) successfully replicate key preferences from the
computational pragmatics literature, and human evaluators surprisingly prefer
LLM-generated responses in open-ended contexts. However, further linguistic
analyses reveal that models disproportionately rely on negative politeness
strategies even in positive contexts, potentially leading to
misinterpretations. While modern LLMs demonstrate an impressive handle on
politeness strategies, these subtle differences raise important questions about
pragmatic alignment in AI systems.

</details>


### [36] [A Hierarchical Probabilistic Framework for Incremental Knowledge Tracing in Classroom Settings](https://arxiv.org/abs/2506.09393)
*Xinyi Gao,Qiucheng Wu,Yang Zhang,Xuechen Liu,Kaizhi Qian,Ying Xu,Shiyu Chang*

Main category: cs.CL

TL;DR: The paper introduces a novel method called KT$^2$, which uses a hierarchical tree structure to improve knowledge tracing in low-resource, online classroom scenarios.


<details>
  <summary>Details</summary>
Motivation: To address challenges in knowledge tracing caused by limited data and the need for online updates in classroom settings.

Method: Introduces KT$^2$, a probabilistic framework leveraging a hierarchical knowledge tree and a Hidden Markov Tree Model, utilizing an EM algorithm for student mastery estimation and incremental updates.

Result: KT$^2$ demonstrates better performance compared to strong existing approaches, particularly in online, low-data scenarios.

Conclusion: Leveraging hierarchical knowledge structures can significantly improve the accuracy and personalization of knowledge tracing in low-resource classroom environments.

Abstract: Knowledge tracing (KT) aims to estimate a student's evolving knowledge state
and predict their performance on new exercises based on performance history.
Many realistic classroom settings for KT are typically low-resource in data and
require online updates as students' exercise history grows, which creates
significant challenges for existing KT approaches. To restore strong
performance under low-resource conditions, we revisit the hierarchical
knowledge concept (KC) information, which is typically available in many
classroom settings and can provide strong prior when data are sparse. We
therefore propose Knowledge-Tree-based Knowledge Tracing (KT$^2$), a
probabilistic KT framework that models student understanding over a
tree-structured hierarchy of knowledge concepts using a Hidden Markov Tree
Model. KT$^2$ estimates student mastery via an EM algorithm and supports
personalized prediction through an incremental update mechanism as new
responses arrive. Our experiments show that KT$^2$ consistently outperforms
strong baselines in realistic online, low-resource settings.

</details>


### [37] [Token Constraint Decoding Improves Robustness on Question Answering for Large Language Models](https://arxiv.org/abs/2506.09408)
*Jui-Ming Yao,Hao-Yuan Chen,Zi-Xian Tang,Bing-Jia Tan,Sheng-Wei Peng,Bing-Cheng Xie,Shun-Feng Su*

Main category: cs.CL

TL;DR: This paper introduces Token Constraint Decoding (TCD), an algorithm to improve the robustness of Large Language Models (LLMs) against noisy inputs, enhancing their multiple-choice question answering capabilities.


<details>
  <summary>Details</summary>
Motivation: LLMs are highly performant in multiple-choice question answering tasks but remain vulnerable to slight input changes, necessitating a method to boost their robustness.

Method: The authors propose Token Constraint Decoding (TCD), an inference-time algorithm that aligns token-level predictions to address noise. Experiments also incorporate prompt engineering fixes and penalty adjustments.

Result: TCD, especially with prompt engineering, substantially restores model performance in noisy conditions, yielding up to +39% gains for weaker models like Gemma3 1B.

Conclusion: TCD is an effective, model-agnostic solution for improving LLM reliability in real-world scenarios, making them more suitable for safety-critical applications.

Abstract: Large Language Models (LLMs) have demonstrated impressive performance on
multiple-choice question answering (MCQA) benchmarks, yet they remain highly
vulnerable to minor input perturbations. In this paper, we introduce and
evaluate Token Constraint Decoding (TCD). This simple yet effective
inference-time algorithm enforces alignment between token-level predictions to
enhance robustness in noisy settings. Through extensive experiments on
CommonsenseQA, MMLU, and MMLU-Pro, we show that TCD, especially when paired
with prompt engineering (PE) fixes, significantly restores performance degraded
by input noise, yielding up to +39\% absolute gains for weaker models like
Gemma3 1B. Penalty sweep analyses further reveal that TCD implicitly
regularizes overconfident outputs, with different models requiring distinct
penalty schedules to maximize resilience. Our findings establish TCD as a
practical, model-agnostic approach for improving reasoning stability under
real-world imperfections and pave the way for more reliable deployment of LLMs
in safety-critical or user-facing applications.

</details>


### [38] [PGDA-KGQA: A Prompt-Guided Generative Framework with Multiple Data Augmentation Strategies for Knowledge Graph Question Answering](https://arxiv.org/abs/2506.09414)
*Xiujun Zhou,Pingjian Zhang,Deyou Tang*

Main category: cs.CL

TL;DR: The paper introduces PGDA-KGQA, a framework that uses prompt-guided data augmentation strategies with LLMs to boost Knowledge Graph Question Answering (KGQA) performance, particularly addressing issues in data diversity and multi-hop reasoning.


<details>
  <summary>Details</summary>
Motivation: The paper aims to tackle challenges in KGQA, such as the lack of diverse annotated data, limited multi-hop reasoning samples, and semantic distortions that hinder models' generalization and robustness.

Method: The proposed PGDA-KGQA framework employs a prompt-guided generative approach that uses LLMs to create extensive datasets by generating single-hop questions, rewriting semantically similar questions, and developing multi-hop questions through reverse path exploration, followed by a semantic parsing pipeline.

Result: The PGDA-KGQA framework demonstrated superior performance over state-of-the-art methods, achieving notable improvements in evaluation metrics (F1, Hits@1, and Accuracy) on well-known datasets like WebQSP and ComplexWebQuestions.

Conclusion: By introducing diverse data through innovative augmentation strategies and leveraging LLMs, PGDA-KGQA addresses key limitations in existing KGQA approaches, leading to enhanced reasoning abilities and robust performance in answering complex queries.

Abstract: Knowledge Graph Question Answering (KGQA) is a crucial task in natural
language processing that requires reasoning over knowledge graphs (KGs) to
answer natural language questions. Recent methods utilizing large language
models (LLMs) have shown remarkable semantic parsing capabilities but are
limited by the scarcity of diverse annotated data and multi-hop reasoning
samples. Traditional data augmentation approaches are focus mainly on
single-hop questions and prone to semantic distortion, while LLM-based methods
primarily address semantic distortion but usually neglect multi-hop reasoning,
thus limiting data diversity. The scarcity of multi-hop samples further weakens
models' generalization. To address these issues, we propose PGDA-KGQA, a
prompt-guided generative framework with multiple data augmentation strategies
for KGQA. At its core, PGDA-KGQA employs a unified prompt-design paradigm: by
crafting meticulously engineered prompts that integrate the provided textual
content, it leverages LLMs to generate large-scale (question, logical form)
pairs for model training. Specifically, PGDA-KGQA enriches its training set by:
(1) generating single-hop pseudo questions to improve the alignment of question
semantics with KG relations; (2) applying semantic-preserving question
rewriting to improve robustness against linguistic variations; (3) employing
answer-guided reverse path exploration to create realistic multi-hop questions.
By adopting an augment-generate-retrieve semantic parsing pipeline, PGDA-KGQA
utilizes the augmented data to enhance the accuracy of logical form generation
and thus improve answer retrieval performance. Experiments demonstrate that
outperforms state-of-the-art methods on standard KGQA datasets, achieving
improvements on WebQSP by 2.8%, 1.2%, and 3.1% and on ComplexWebQuestions by
1.8%, 1.1%, and 2.4% in F1, Hits@1, and Accuracy, respectively.

</details>


### [39] [Hidden in Plain Sight: Evaluation of the Deception Detection Capabilities of LLMs in Multimodal Settings](https://arxiv.org/abs/2506.09424)
*Md Messal Monem Miah,Adrita Anika,Xi Shi,Ruihong Huang*

Main category: cs.CL

TL;DR: The paper evaluates how Large Language Models (LLMs) and Large Multimodal Models (LMMs) detect deception across various datasets and modalities. Key approaches include zero-shot, few-shot, and auxiliary feature integration.


<details>
  <summary>Details</summary>
Motivation: Detecting deception is crucial in a digital world, but remains challenging. This paper investigates whether advanced AI models like LLMs and LMMs can effectively address this issue across diverse domains.

Method: The study examines LLMs and LMMs on distinct datasets using experimental setups such as zero-shot, few-shot, and in-context learning strategies. It also incorporates analysis of auxiliary features like gestures and video summaries.

Result: Fine-tuned LLMs achieve state-of-the-art performance in textual deception detection, whereas LMMs struggle with cross-modal cues. Prompting strategies like chain-of-thought reasoning and auxiliary features are explored for their contributions.

Conclusion: LLMs are promising for textual deception detection, but LMMs require further development for handling multimodal data. The study highlights both the potential and limitations of these models in practical applications.

Abstract: Detecting deception in an increasingly digital world is both a critical and
challenging task. In this study, we present a comprehensive evaluation of the
automated deception detection capabilities of Large Language Models (LLMs) and
Large Multimodal Models (LMMs) across diverse domains. We assess the
performance of both open-source and commercial LLMs on three distinct datasets:
real life trial interviews (RLTD), instructed deception in interpersonal
scenarios (MU3D), and deceptive reviews (OpSpam). We systematically analyze the
effectiveness of different experimental setups for deception detection,
including zero-shot and few-shot approaches with random or similarity-based
in-context example selection. Our results show that fine-tuned LLMs achieve
state-of-the-art performance on textual deception detection tasks, while LMMs
struggle to fully leverage cross-modal cues. Additionally, we analyze the
impact of auxiliary features, such as non-verbal gestures and video summaries,
and examine the effectiveness of different prompting strategies, including
direct label generation and chain-of-thought reasoning. Our findings provide
key insights into how LLMs process and interpret deceptive cues across
modalities, highlighting their potential and limitations in real-world
deception detection applications.

</details>


### [40] [Improved Supervised Fine-Tuning for Large Language Models to Mitigate Catastrophic Forgetting](https://arxiv.org/abs/2506.09428)
*Fei Ding,Baiqiao Wang*

Main category: cs.CL

TL;DR: The paper introduces a supervised fine-tuning (SFT) method to reduce catastrophic forgetting in large language models without accessing original pre-training data, using instruction reconstruction and multi-model screening.


<details>
  <summary>Details</summary>
Motivation: Current SFT methods improve task-specific performance at the cost of reducing generalization capabilities, and exacerbating catastrophic forgetting due to inaccessible pre-training data.

Method: The approach reconstructs the instruction-following distribution of the base model, applies multi-model screening to select optimal data, and mixes this with new data for supervised fine-tuning.

Result: Experiments confirm the method maintains general capabilities while enhancing task-specific performance.

Conclusion: The proposed method is effective and cost-efficient in addressing catastrophic forgetting during supervised fine-tuning of LLMs.

Abstract: Supervised Fine-Tuning (SFT), while enhancing large language models(LLMs)'
instruction-following capabilities and domain-specific task adaptability, often
diminishes their general capabilities. Moreover, due to the inaccessibility of
original pre-training data, catastrophic forgetting tends to be exacerbated
when third-party practitioners implement SFT on open-sourced models. To address
this challenge, we propose a novel, more cost-effective SFT method which could
effectively reduce the risk of catastrophic forgetting without access to
original SFT data. Our approach begins by reconstructing the likely SFT
instruction distribution of the base model, followed by a multi-model screening
process to select optimal data, which is then mixed with new data for SFT.
Experimental results demonstrate that our method preserves generalization
capabilities in general domains while improving task-specific performance.

</details>


### [41] [GigaChat Family: Efficient Russian Language Modeling Through Mixture of Experts Architecture](https://arxiv.org/abs/2506.09440)
*GigaChat team,Mamedov Valentin,Evgenii Kosarev,Gregory Leleytner,Ilya Shchuckin,Valeriy Berezovskiy,Daniil Smirnov,Dmitry Kozlov,Sergei Averkiev,Lukyanenko Ivan,Aleksandr Proshunin,Ainur Israfilova,Ivan Baskov,Artem Chervyakov,Emil Shakirov,Mikhail Kolesov,Daria Khomich,Darya Latortseva,Sergei Porkhun,Yury Fedorov,Oleg Kutuzov,Polina Kudriavtseva,Sofiia Soldatova,Kolodin Egor,Stanislav Pyatkin,Dzmitry Menshykh,Grafov Sergei,Eldar Damirov,Karlov Vladimir,Ruslan Gaitukiev,Arkadiy Shatenov,Alena Fenogenova,Nikita Savushkin,Fedor Minkin*

Main category: cs.CL

TL;DR: The paper introduces GigaChat, a set of large language models tailored for the Russian language, available in multiple formats and emphasizing accessibility for research and application.


<details>
  <summary>Details</summary>
Motivation: Limited development of Russian-specific LLMs due to computational constraints inspired the authors to create specialized models for advancing NLP research and applications in Russian.

Method: GigaChat models were developed in various sizes using detailed architectures, pre-training processes, and instructions tuning, evaluated on both Russian and English benchmarks, and made publicly available.

Result: GigaChat achieved strong performance on benchmarks, with capabilities comparable to multilingual analogs, and is accessible through API, Telegram bot, and Web interface.

Conclusion: The release of GigaChat models aims to foster research and industrial applications in Russian NLP while providing community accessibility through open-source resources.

Abstract: Generative large language models (LLMs) have become crucial for modern NLP
research and applications across various languages. However, the development of
foundational models specifically tailored to the Russian language has been
limited, primarily due to the significant computational resources required.
This paper introduces the GigaChat family of Russian LLMs, available in various
sizes, including base models and instruction-tuned versions. We provide a
detailed report on the model architecture, pre-training process, and
experiments to guide design choices. In addition, we evaluate their performance
on Russian and English benchmarks and compare GigaChat with multilingual
analogs. The paper presents a system demonstration of the top-performing models
accessible via an API, a Telegram bot, and a Web interface. Furthermore, we
have released three open GigaChat models in open-source
(https://huggingface.co/ai-sage), aiming to expand NLP research opportunities
and support the development of industrial solutions for the Russian language.

</details>


### [42] [UniToMBench: Integrating Perspective-Taking to Improve Theory of Mind in LLMs](https://arxiv.org/abs/2506.09450)
*Prameshwar Thiyagarajan,Vaishnavi Parimi,Shamant Sai,Soumil Garg,Zhangir Meirbek,Nitin Yarlagadda,Kevin Zhu,Chris Kim*

Main category: cs.CL

TL;DR: The paper introduces UniToMBench, a unified benchmark for measuring Theory of Mind (ToM) in large language models (LLMs), using diverse evaluation metrics and tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges LLMs face in accurately predicting human mental states and to systematically assess and improve their Theory of Mind (ToM) capabilities.

Method: UniToMBench integrates multi-interaction task designs, evolving story scenarios, and a dataset of over 1,000 hand-written scenarios. It uses perspective-taking methods and diverse metrics to evaluate LLMs’ social cognition abilities.

Result: LLMs like GPT-4o and GPT-4o Mini perform well (above 80% accuracy) in emotional and belief-related tasks but show variability in knowledge-based tasks.

Conclusion: While LLMs exhibit strengths in certain ToM tasks, their limitations in knowledge-based scenarios underscore the importance of UniToMBench for advancing their social cognition capabilities.

Abstract: Theory of Mind (ToM), the ability to understand the mental states of oneself
and others, remains a challenging area for large language models (LLMs), which
often fail to predict human mental states accurately. In this paper, we
introduce UniToMBench, a unified benchmark that integrates the strengths of
SimToM and TOMBENCH to systematically improve and assess ToM capabilities in
LLMs by integrating multi-interaction task designs and evolving story
scenarios. Supported by a custom dataset of over 1,000 hand-written scenarios,
UniToMBench combines perspective-taking techniques with diverse evaluation
metrics to better stimulate social cognition in LLMs. Through evaluation, we
observe that while models like GPT-4o and GPT-4o Mini show consistently high
accuracy in tasks involving emotional and belief-related scenarios, with
results usually above 80%, there is significant variability in their
performance across knowledge-based tasks. These results highlight both the
strengths and limitations of current LLMs in ToM-related tasks, underscoring
the value of UniToMBench as a comprehensive tool for future development. Our
code is publicly available here:
https://github.com/Shamant/unifiedtombenchmark.

</details>


### [43] [Towards Bridging the Reward-Generation Gap in Direct Alignment Algorithms](https://arxiv.org/abs/2506.09457)
*Zeguan Xiao,Yun Chen,Guanhua Chen*

Main category: cs.CL

TL;DR: The paper identifies a reward-generation gap in Direct Alignment Algorithms (DAAs) and introduces Prefix-Oriented Equal-length Training (POET) to resolve it, leading to improved aligned performance of language models.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the misalignment (reward-generation gap) between optimization objectives during training in Direct Alignment Algorithms (DAAs) and the actual language generation performance during inference.

Method: The paper proposes POET, a technique where both preferred and dispreferred response samples are truncated to equal lengths during training, ensuring the optimization process pays more attention to prefix tokens.

Result: POET improves performance of DAAs like DPO and SimPO, showing up to 15.6 points improvement on AlpacaEval 2 and overall better results on downstream tasks.

Conclusion: Addressing the reward-generation gap through prefix token-focused optimization improves DAA-based alignment, signaling a more efficient alternative to existing approaches like RLHF.

Abstract: Direct Alignment Algorithms (DAAs), such as Direct Preference Optimization
(DPO) and Simple Preference Optimization (SimPO), have emerged as efficient
alternatives to Reinforcement Learning from Human Feedback (RLHF) algorithms
for aligning large language models (LLMs) with human preferences. However, DAAs
suffer from a fundamental limitation we identify as the "reward-generation gap"
-- a misalignment between optimization objectives during training and actual
generation performance during inference. In this paper, we find a contributor
to the reward-generation gap is the mismatch between the inherent importance of
prefix tokens during the LLM generation process and how this importance is
reflected in the implicit reward functions of DAAs. To bridge the gap, we
introduce a simple yet effective approach called Prefix-Oriented Equal-length
Training (POET), which truncates both preferred and dispreferred responses to
match the shorter one's length. Training with POET, where both responses in
each sample are truncated to equal length, resulting in diverse truncated
lengths across samples, the optimization of DAAs objective is implicitly
constrained to converge across all positions, thus paying more attention to
prefix tokens than the standard DAAs. We conduct experiments with DPO and
SimPO, two representative DAAs, demonstrating that POET improves over their
standard implementations, achieving up to 15.6 points in AlpacaEval 2 and
overall improvements across downstream tasks. Our results highlight the
importance of addressing the misalignment between reward optimization and
generation performance in DAAs.

</details>


### [44] [Bridging Online Behavior and Clinical Insight: A Longitudinal LLM-based Study of Suicidality on YouTube Reveals Novel Digital Markers](https://arxiv.org/abs/2506.09495)
*Ilanit Sobol,Shir Lissak,Refael Tikochinski,Tal Nakash,Anat Brunstein Klomek,Eyal Fruchter,Roi Reichart*

Main category: cs.CL

TL;DR: The paper examines suicidal behavior on YouTube through a dataset of 181 channels from individuals with life-threatening attempts. It uses computational, hybrid, and expert-driven methods to uncover behavioral indicators and temporal trends.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to better understand suicidal behavior through new research approaches, especially using digital footprints from social media to gain insights into such behaviors.

Method: Three complementary approaches were used: a bottom-up computational approach utilizing LLM-based topic modeling, a hybrid approach involving expert reviews of LLM-derived topics, and a top-down expert-driven psychological assessment of suicide attempt narratives.

Result: Key findings include five suicide-associated topics from the bottom-up approach (two with temporal changes), with 'YouTube Engagement' being a notable platform-specific indicator missed by experts. Psychological assessment revealed distinct motivations for sharing suicide experiences, categorized as either 'Help Others' or 'Personal Recovery'.

Conclusion: The integrated approaches provide a nuanced understanding of suicidality, blending insights from digital behaviors with clinical perspectives, and highlighting the unique contributions of computational methods to identifying indicators beyond expert prediction.

Abstract: Suicide remains a leading cause of death in Western countries, underscoring
the need for new research approaches. As social media becomes central to daily
life, digital footprints offer valuable insight into suicidal behavior.
Focusing on individuals who attempted suicide while uploading videos to their
channels, we investigate: How do suicidal behaviors manifest on YouTube, and
how do they differ from expert knowledge? We applied complementary approaches:
computational bottom-up, hybrid, and expert-driven top-down, on a novel
longitudinal dataset of 181 YouTube channels from individuals with
life-threatening attempts, alongside 134 control channels. In the bottom-up
approach, we applied LLM-based topic modeling to identify behavioral
indicators. Of 166 topics, five were associated with suicide-attempt, with two
also showing temporal attempt-related changes ($p<.01$) - Mental Health
Struggles ($+0.08$)* and YouTube Engagement ($+0.1$)*. In the hybrid approach,
a clinical expert reviewed LLM-derived topics and flagged 19 as
suicide-related. However, none showed significant attempt-related temporal
effects beyond those identified bottom-up. Notably, YouTube Engagement, a
platform-specific indicator, was not flagged by the expert, underscoring the
value of bottom-up discovery. In the top-down approach, psychological
assessment of suicide attempt narratives revealed that the only significant
difference between individuals who attempted before and those attempted during
their upload period was the motivation to share this experience: the former
aimed to Help Others ($\beta=-1.69$, $p<.01$), while the latter framed it as
part of their Personal Recovery ($\beta=1.08$, $p<.01$). By integrating these
approaches, we offer a nuanced understanding of suicidality, bridging digital
behavior and clinical insights.
  * Within-group changes in relation to the suicide attempt.

</details>


### [45] [Give Me FP32 or Give Me Death? Challenges and Solutions for Reproducible Reasoning](https://arxiv.org/abs/2506.09501)
*Jiayi Yuan,Hao Li,Xinheng Ding,Wenya Xie,Yu-Jhe Li,Wentian Zhao,Kun Wan,Jing Shi,Xia Hu,Zirui Liu*

Main category: cs.CL

TL;DR: This paper investigates the reproducibility issues of Large Language Model (LLM) performance caused by numerical precision variations across hardware and system configurations.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the fragile reproducibility of LLM evaluations, which affects benchmark accuracy and may impede proper assessment of advancing AI models.

Method: It systematically examines reproducibility through controlled experiments across diverse hardware, software, precision settings, and introduces LayerCast—a pipeline using FP32 for numerical stability while maintaining memory efficiency.

Result: Numerical precision discrepancies were identified as the root cause of accuracy and output divergence. Experiments demonstrated up to 9% accuracy variation and significant output disparities within specific configurations.

Conclusion: Floating-point precision is critical for LLM reproducibility but often overlooked, and adopting higher computational precision (FP32) enhances stability without major memory trade-offs.

Abstract: Large Language Models (LLMs) are now integral across various domains and have
demonstrated impressive performance. Progress, however, rests on the premise
that benchmark scores are both accurate and reproducible. We demonstrate that
the reproducibility of LLM performance is fragile: changing system
configuration such as evaluation batch size, GPU count, and GPU version can
introduce significant difference in the generated responses. This issue is
especially pronounced in reasoning models, where minor rounding differences in
early tokens can cascade into divergent chains of thought, ultimately affecting
accuracy. For instance, under bfloat16 precision with greedy decoding, a
reasoning model like DeepSeek-R1-Distill-Qwen-7B can exhibit up to 9% variation
in accuracy and 9,000 tokens difference in response length due to differences
in GPU count, type, and evaluation batch size. We trace the root cause of this
variability to the non-associative nature of floating-point arithmetic under
limited numerical precision. This work presents the first systematic
investigation into how numerical precision affects reproducibility in LLM
inference. Through carefully controlled experiments across various hardware,
software, and precision settings, we quantify when and how model outputs
diverge. Our analysis reveals that floating-point precision -- while critical
for reproducibility -- is often neglected in evaluation practices. Inspired by
this, we develop a lightweight inference pipeline, dubbed LayerCast, that
stores weights in 16-bit precision but performs all computations in FP32,
balancing memory efficiency with numerical stability. Code is available at
https://github.com/nanomaoli/llm_reproducibility.

</details>


### [46] [TransXSSM: A Hybrid Transformer State Space Model with Unified Rotary Position Embedding](https://arxiv.org/abs/2506.09507)
*Bingheng Wu,Jingze Shi,Yifan Wu,Nan Tang,Yuyu Luo*

Main category: cs.CL

TL;DR: The paper introduces a hybrid model that effectively integrates Transformers and State Space Models (SSMs) using a unified rotary position embedding (RoPE) method for better positional encoding.


<details>
  <summary>Details</summary>
Motivation: To overcome the positional encoding incompatibility between Transformers and SSMs, which hinders their integration and optimal performance.

Method: The authors propose a unified rotary position embedding (RoPE) and develop a hybrid architecture named \model that combines Transformer and SSM layers using this unified encoding framework.

Result: The presented \model achieves training and inference speeds that are 42.3% and 29.5% faster, respectively, while outperforming standard Transformers by over 4% accuracy in language modeling benchmarks. It also demonstrates better scalability and performs more effectively in long-context modeling.

Conclusion: Unified positional encoding resolves positional incompatibility challenges in hybrid models, enabling efficient and high-performance applications for long-sequence modeling.

Abstract: Transformers exhibit proficiency in capturing long-range dependencies,
whereas State Space Models (SSMs) facilitate linear-time sequence modeling.
Notwithstanding their synergistic potential, the integration of these
architectures presents a significant challenge, primarily attributable to a
fundamental incongruity in their respective positional encoding mechanisms:
Transformers rely on explicit Rotary Position Embeddings (RoPE), while SSMs
leverage implicit positional representations via convolutions. This divergence
often precipitates discontinuities and suboptimal performance. To address this
impediment, we propose a unified rotary position embedding (\textbf{\ourRoPE})
methodology, thereby establishing a consistent positional encoding framework
for both self-attention and state-space components. Using this \ourRoPE, we
introduce \textbf{\model}, a hybrid architecture that coherently integrates the
Transformer and SSM layers under this unified positional encoding scheme. At a
4K sequence length, \model exhibits training and inference speeds that are
\textbf{42.3\% and 29.5\% faster}, respectively, relative to standard
Transformer models. It also delivers higher accuracy: under comparable
settings, it surpasses a Transformer baseline by over 4\% on language modeling
benchmarks. \model furthermore scales more effectively: \model-1.3B gains
\textbf{7.22\%} in average accuracy over its 320M version (versus about 6\%
gains for equivalent Transformers or SSMs). Our results show that unified
positional encoding resolves positional incompatibility in hybrid models,
enabling efficient, high-performance long-context modeling.

</details>


### [47] [ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning](https://arxiv.org/abs/2506.09513)
*Yu Sun,Xingyu Qian,Weiwen Xu,Hao Zhang,Chenghao Xiao,Long Li,Yu Rong,Wenbing Huang,Qifeng Bai,Tingyang Xu*

Main category: cs.CL

TL;DR: The paper introduces ReasonMed, a large medical reasoning dataset, and proposes best practices to train medical reasoning models using it, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the limitations of reasoning-based LLMs in knowledge-intensive medical question answering, a domain that demands high accuracy in reasoning.

Method: They create ReasonMed, a dataset of 370k examples through multi-agent verification and refinement, supplemented with an Error Refiner system to enhance reasoning paths and ensure quality.

Result: Their trained ReasonMed-7B model achieves significant performance improvement, surpassing sub-10B models and even larger models like LLaMA3.1-70B on PubMedQA.

Conclusion: Combining detailed CoT reasoning with concise summaries provides an effective strategy for training medical reasoning models, setting new benchmarks in the field.

Abstract: Though reasoning-based large language models (LLMs) have excelled in
mathematics and programming, their capabilities in knowledge-intensive medical
question answering remain underexplored. To address this, we introduce
ReasonMed, the largest medical reasoning dataset, comprising 370k high-quality
examples distilled from 1.7 million initial reasoning paths generated by
various LLMs. ReasonMed is constructed through a \textit{multi-agent
verification and refinement process}, where we design an \textit{Error Refiner}
to enhance the reasoning paths by identifying and correcting error-prone steps
flagged by a verifier. Leveraging ReasonMed, we systematically investigate best
practices for training medical reasoning models and find that combining
detailed Chain-of-Thought (CoT) reasoning with concise answer summaries yields
the most effective fine-tuning strategy. Based on this strategy, we train
ReasonMed-7B, which sets a new benchmark for sub-10B models, outperforming the
prior best by 4.17\% and even exceeding LLaMA3.1-70B on PubMedQA by 4.60\%.

</details>


### [48] [KG-Infused RAG: Augmenting Corpus-Based RAG with External Knowledge Graphs](https://arxiv.org/abs/2506.09542)
*Dingjun Wu,Yukun Yan,Zhenghao Liu,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: This paper proposes KG-Infused RAG, a framework that enhances Retrieval-Augmented Generation systems by integrating knowledge graphs, improving performance on QA benchmarks significantly.


<details>
  <summary>Details</summary>
Motivation: Existing RAG systems often depend on a single source of knowledge and lack mechanisms inspired by cognitive processes, limiting their accuracy and interpretability.

Method: KG-Infused RAG incorporates spreading activation through query expansion using knowledge graphs, combining retrieved KG facts with corpus passages. Preference learning is applied to enhance its performance.

Result: KG-Infused RAG outperforms vanilla RAG systems significantly on five QA datasets, achieving gains between 3.8% and 13.8%. It also improves performance in Self-RAG implementations.

Conclusion: KG-Infused RAG demonstrates its effectiveness as a versatile enhancement module, enabling more accurate and interpretable retrieval-augmented generation systems through multi-source knowledge integration.

Abstract: Retrieval-Augmented Generation (RAG) improves factual accuracy by grounding
responses in external knowledge. However, existing methods typically rely on a
single source, either unstructured text or structured knowledge. Moreover, they
lack cognitively inspired mechanisms for activating relevant knowledge. To
address these issues, we propose KG-Infused RAG, a framework that integrates
KGs into RAG systems to implement spreading activation, a cognitive process
that enables concept association and inference. KG-Infused RAG retrieves KG
facts, expands the query accordingly, and enhances generation by combining
corpus passages with structured facts, enabling interpretable, multi-source
retrieval grounded in semantic structure. We further improve KG-Infused RAG via
preference learning on sampled key stages in the pipeline. Experiments on five
QA benchmarks show that KG-Infused RAG consistently outperforms vanilla RAG (by
3.8% to 13.8%). Additionally, when integrated into Self-RAG, KG-Infused RAG
brings further performance gains, demonstrating its effectiveness and
versatility as a plug-and-play enhancement module for corpus-based RAG methods.

</details>


### [49] [MEDUSA: A Multimodal Deep Fusion Multi-Stage Training Framework for Speech Emotion Recognition in Naturalistic Conditions](https://arxiv.org/abs/2506.09556)
*Georgios Chatzichristodoulou,Despoina Kosmopoulou,Antonios Kritikos,Anastasia Poulopoulou,Efthymios Georgiou,Athanasios Katsamanis,Vassilis Katsouros,Alexandros Potamianos*

Main category: cs.CL

TL;DR: The paper introduces MEDUSA, a multimodal framework achieving state-of-the-art results in speech emotion recognition (SER) through a novel training pipeline and deep transformer fusion.


<details>
  <summary>Details</summary>
Motivation: SER is difficult due to subjective and imbalanced emotion representation in real-world scenarios.

Method: MEDUSA employs a four-stage training process: (1) training classifiers using a new cross-modal transformer extension (DeepSER), (2) regularization via Manifold MixUp, (3-4) optimizing a meta-classifier integrating ensemble predictions with multitask learning and balanced data sampling.

Result: MEDUSA achieved first place in Categorical Emotion Recognition in the Interspeech 2025 Challenge.

Conclusion: The proposed method effectively addresses challenges in SER by leveraging multimodal fusion, innovative training strategies, and soft target annotations, offering state-of-the-art performance in naturalistic emotion recognition.

Abstract: SER is a challenging task due to the subjective nature of human emotions and
their uneven representation under naturalistic conditions. We propose MEDUSA, a
multimodal framework with a four-stage training pipeline, which effectively
handles class imbalance and emotion ambiguity. The first two stages train an
ensemble of classifiers that utilize DeepSER, a novel extension of a deep
cross-modal transformer fusion mechanism from pretrained self-supervised
acoustic and linguistic representations. Manifold MixUp is employed for further
regularization. The last two stages optimize a trainable meta-classifier that
combines the ensemble predictions. Our training approach incorporates human
annotation scores as soft targets, coupled with balanced data sampling and
multitask learning. MEDUSA ranked 1st in Task 1: Categorical Emotion
Recognition in the Interspeech 2025: Speech Emotion Recognition in Naturalistic
Conditions Challenge.

</details>


### [50] [Gender Bias in English-to-Greek Machine Translation](https://arxiv.org/abs/2506.09558)
*Eleni Gkovedarou,Joke Daems,Luna De Bruyne*

Main category: cs.CL

TL;DR: This study investigates gender bias in machine translation for English-to-Greek with Google Translate, DeepL, and GPT-4o using a novel dataset, GendEL, and finds persistent bias despite some promising bias mitigation by GPT-4o.


<details>
  <summary>Details</summary>
Motivation: Rising concerns over machine translation systems reinforcing gender stereotypes and the need for more inclusive language.

Method: Analysis of gender bias in Google Translate, DeepL, and GPT-4o for English-to-Greek translations using a custom dataset, GendEL, comprising 240 sentences with gender-related ambiguities.

Result: DeepL and Google Translate show persistent gender biases; DeepL performs better on feminine gender-unambiguous cases. GPT-4o demonstrates potential in creating gender-inclusive translations but retains some biases.

Conclusion: Machine translation systems still struggle with gender-neutral translations, but GPT-4o offers a promising approach to bias mitigation while requiring further improvement.

Abstract: As the demand for inclusive language increases, concern has grown over the
susceptibility of machine translation (MT) systems to reinforce gender
stereotypes. This study investigates gender bias in two commercial MT systems,
Google Translate and DeepL, focusing on the understudied English-to-Greek
language pair. We address three aspects of gender bias: i) male bias, ii)
occupational stereotyping, and iii) errors in anti-stereotypical translations.
Additionally, we explore the potential of prompted GPT-4o as a bias mitigation
tool that provides both gender-explicit and gender-neutral alternatives when
necessary. To achieve this, we introduce GendEL, a manually crafted bilingual
dataset of 240 gender-ambiguous and unambiguous sentences that feature
stereotypical occupational nouns and adjectives. We find persistent gender bias
in translations by both MT systems; while they perform well in cases where
gender is explicitly defined, with DeepL outperforming both Google Translate
and GPT-4o in feminine gender-unambiguous sentences, they are far from
producing gender-inclusive or neutral translations when the gender is
unspecified. GPT-4o shows promise, generating appropriate gendered and neutral
alternatives for most ambiguous cases, though residual biases remain evident.

</details>


### [51] [Towards Open Foundation Language Model and Corpus for Macedonian: A Low-Resource Language](https://arxiv.org/abs/2506.09560)
*Stefan Krsteski,Matea Tashkovska,Borjan Sazdov,Hristijan Gjoreski,Branislav Gerazov*

Main category: cs.CL

TL;DR: This paper develops novel resources for advancing Large Language Models (LLMs) in Macedonian, a low-resource language, by creating a large corpus, an evaluation benchmark, and training an 8B-parameter model that shows superior performance.


<details>
  <summary>Details</summary>
Motivation: The adoption of LLMs is limited for low-resource languages like Macedonian, which restricts their application in certain regions. This study aims to address this gap.

Method: The authors gather a 40GB Macedonian corpus, a 106k-instance culturally grounded instruction dataset, and develop a benchmark evaluation suite. They train an 8B-parameter model and compare it against eight baseline models.

Result: The trained model outperforms all baseline models in the 8B parameter range across benchmarks, achieving comparable performance to larger models (up to 10x the size). Native speakers prefer this model for grammatical correctness and cultural appropriateness.

Conclusion: The study creates a foundation for advancing LLMs for Macedonian and similar underrepresented languages. Open-source datasets, code, and model weights are released for public use.

Abstract: The increase in technological adoption worldwide comes with demands for novel
tools to be used by the general population. Large Language Models (LLMs)
provide a great opportunity in this respect, but their capabilities remain
limited for low-resource languages, restricting applications in countries where
such languages are spoken. We create several resources to facilitate the
adoption of LLMs and to support research advancements for Macedonian. We
collect the largest Macedonian corpus to date, consisting of 40GB of textual
data and totaling 3.5B words. To support conversational applications, we
collect a 106k-instance instruction dataset, carefully built to be culturally
grounded. For evaluation, we construct a Macedonian evaluation suite covering
seven benchmarks. Finally, we train domestic-yak, a state-of-the-art
8B-parameter model, on our curated datasets and evaluate it against eight
baseline models using the newly constructed benchmark suite. Our model
outperforms all existing models in the 8B parameter range across all
benchmarks, and achieves performance comparable to models up to 10x larger.
Furthermore, a qualitative analysis with native speakers reveals that our model
is preferred over larger counterparts, receiving higher ratings for grammatical
correctness and cultural appropriateness. All datasets, code, and model weights
are openly released, setting a foundation for advancing LLMs in similarly
underrepresented languages. These resources are publicly available at
github.com/LVSTCK for source code, and at huggingface.co/LVSTCK for pretrained
model weights and data.

</details>


### [52] [From Symbolic to Neural and Back: Exploring Knowledge Graph-Large Language Model Synergies](https://arxiv.org/abs/2506.09566)
*Blaž Škrlj,Boshko Koloski,Senja Pollak,Nada Lavrač*

Main category: cs.CL

TL;DR: The paper explores how combining Knowledge Graphs (KGs) and Large Language Models (LLMs) improves reasoning, factual accuracy, and question answering, while enhancing KG-related tasks. It identifies gaps and proposes future research directions.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between structured knowledge from Knowledge Graphs and the reasoning capabilities of Large Language Models, enhancing their combined utility for intelligent systems.

Method: The authors analyzed existing approaches, categorizing them into KG-enhanced LLMs (to improve reasoning and reduce hallucinations) and LLM-augmented KGs (to improve KG tasks). They also highlighted research gaps and proposed future directions.

Result: The study identified critical gaps in integration methods and demonstrated mutual benefits of combining KGs and LLMs. Focus was placed on scalability, computational efficiency, and data quality.

Conclusion: Structured knowledge integration between KGs and LLMs can lead to intelligent systems capable of handling more complex, real-world tasks. Future research should focus on neuro-symbolic integration, dynamic KG updating, increased reliability, and ethical considerations.

Abstract: Integrating structured knowledge from Knowledge Graphs (KGs) into Large
Language Models (LLMs) enhances factual grounding and reasoning capabilities.
This survey paper systematically examines the synergy between KGs and LLMs,
categorizing existing approaches into two main groups: KG-enhanced LLMs, which
improve reasoning, reduce hallucinations, and enable complex question
answering; and LLM-augmented KGs, which facilitate KG construction, completion,
and querying. Through comprehensive analysis, we identify critical gaps and
highlight the mutual benefits of structured knowledge integration. Compared to
existing surveys, our study uniquely emphasizes scalability, computational
efficiency, and data quality. Finally, we propose future research directions,
including neuro-symbolic integration, dynamic KG updating, data reliability,
and ethical considerations, paving the way for intelligent systems capable of
managing more complex real-world knowledge tasks.

</details>


### [53] [Memorization in Language Models through the Lens of Intrinsic Dimension](https://arxiv.org/abs/2506.09591)
*Stefan Arnold*

Main category: cs.CL

TL;DR: This paper studies the role of Intrinsic Dimension (ID), a geometric measure of sequence complexity, in influencing memorization by Language Models (LMs). High-ID sequences are less prone to memorization compared to low-ID ones.


<details>
  <summary>Details</summary>
Motivation: Understanding the factors that affect language model memorization is essential to address concerns over privacy leakage and intellectual property disclosure.

Method: The authors investigate how the structural complexity, measured as Intrinsic Dimension (ID), interacts with memorization, particularly under varying contexts like overparameterized models and sparse exposure situations.

Result: The study finds that high-ID sequences suppress memorization by LMs when compared to low-ID sequences. This is especially evident in scenarios involving overparameterized models and sparse exposure.

Conclusion: The intrinsic structural complexity of sequences plays a crucial role in shaping memorization, illustrating the importance of considering scale, exposure, and complexity to control unintended memorization in language models.

Abstract: Language Models (LMs) are prone to memorizing parts of their data during
training and unintentionally emitting them at generation time, raising concerns
about privacy leakage and disclosure of intellectual property. While previous
research has identified properties such as context length, parameter size, and
duplication frequency, as key drivers of unintended memorization, little is
known about how the latent structure modulates this rate of memorization. We
investigate the role of Intrinsic Dimension (ID), a geometric proxy for the
structural complexity of a sequence in latent space, in modulating
memorization. Our findings suggest that ID acts as a suppressive signal for
memorization: compared to low-ID sequences, high-ID sequences are less likely
to be memorized, particularly in overparameterized models and under sparse
exposure. These findings highlight the interaction between scale, exposure, and
complexity in shaping memorization.

</details>


### [54] [Benchmarking Debiasing Methods for LLM-based Parameter Estimates](https://arxiv.org/abs/2506.09627)
*Nicolas Audinet de Pieuchon,Adel Daoud,Connor T. Jerzak,Moa Johansson,Richard Johansson*

Main category: cs.CL

TL;DR: The paper examines debiasing methods for large language models (LLMs) used in text annotations, aiming to mitigate biases introduced by inconsistent annotations compared to expert evaluations. It compares the performance of two methods, DSL and PPI, under practical sample sizes.


<details>
  <summary>Details</summary>
Motivation: Address the bias introduced by inconsistent LLM annotations compared to experts and its effect on estimating population parameters, motivating a need for improved debiasing techniques.

Method: Study the performance of DSL and PPI in finite sample settings, highlighting how number of expert annotations influences bias and efficiency. Conduct comparative analysis of both methods across various tasks.

Result: DSL generally outperforms PPI in bias reduction and efficiency in large datasets, but shows inconsistent performance across datasets. PPI remains steady but less effective in reducing bias.

Conclusion: There exists a bias-variance tradeoff in debiasing methods, necessitating the development of better metrics for assessing efficiency, especially in finite sample contexts.

Abstract: Large language models (LLMs) offer an inexpensive yet powerful way to
annotate text, but are often inconsistent when compared with experts. These
errors can bias downstream estimates of population parameters such as
regression coefficients and causal effects. To mitigate this bias, researchers
have developed debiasing methods such as Design-based Supervised Learning (DSL)
and Prediction-Powered Inference (PPI), which promise valid estimation by
combining LLM annotations with a limited number of expensive expert
annotations. Although these methods produce consistent estimates under
theoretical assumptions, it is unknown how they compare in finite samples of
sizes encountered in applied research. We make two contributions: First, we
study how each method's performance scales with the number of expert
annotations, highlighting regimes where LLM bias or limited expert labels
significantly affect results. Second, we compare DSL and PPI across a range of
tasks, finding that although both achieve low bias with large datasets, DSL
often outperforms PPI on bias reduction and empirical efficiency, but its
performance is less consistent across datasets. Our findings indicate that
there is a bias-variance tradeoff at the level of debiasing methods, calling
for more research on developing metrics for quantifying their efficiency in
finite samples.

</details>


### [55] [Modeling Probabilistic Reduction using Information Theory and Naive Discriminative Learning](https://arxiv.org/abs/2506.09641)
*Anna Stein,Kevin Tang*

Main category: cs.CL

TL;DR: This study compares NDL predictors with probabilistic predictors to model word duration, highlighting the superiority of N-gram models and the importance of combining information-theoretic metrics with discriminative learning.


<details>
  <summary>Details</summary>
Motivation: To assess the effectiveness of different models—NDL-based and probabilistic predictors—in capturing acoustic word duration and probabilistic reduction.

Method: Comparison of three different modeling approaches (NDL with information-theoretic formulas, traditional NDL predictors, and N-gram models) using the Buckeye corpus.

Result: The N-gram model outperformed both versions of the NDL model, with information-theoretic formulas enhancing NDL's performance compared to the traditional approach.

Conclusion: Integrating information-theoretic metrics with discriminative learning improves modeling, and broader aspects such as average contextual predictability should be considered for acoustic reduction studies.

Abstract: This study compares probabilistic predictors based on information theory with
Naive Discriminative Learning (NDL) predictors in modeling acoustic word
duration, focusing on probabilistic reduction. We examine three models using
the Buckeye corpus: one with NDL-derived predictors using information-theoretic
formulas, one with traditional NDL predictors, and one with N-gram
probabilistic predictors. Results show that the N-gram model outperforms both
NDL models, challenging the assumption that NDL is more effective due to its
cognitive motivation. However, incorporating information-theoretic formulas
into NDL improves model performance over the traditional model. This research
highlights a) the need to incorporate not only frequency and contextual
predictability but also average contextual predictability, and b) the
importance of combining information-theoretic metrics of predictability and
information derived from discriminative learning in modeling acoustic
reduction.

</details>


### [56] [Using Sign Language Production as Data Augmentation to enhance Sign Language Translation](https://arxiv.org/abs/2506.09643)
*Harry Walsh,Maksym Ivashechkin,Richard Bowden*

Main category: cs.CL

TL;DR: The paper presents techniques to augment sign language datasets for improving Sign Language Translation performance using advanced generative production methods like SignGAN and SignSplat.


<details>
  <summary>Details</summary>
Motivation: The study aims to address challenges in sign language datasets, which are typically low-resource compared to spoken languages, hindering the development of effective machine learning models for tasks like Sign Language Translation.

Method: Three techniques are employed: skeleton-based production, sign stitching, and generative models (SignGAN and SignSplat) to generate variations in signer appearance and motion.

Result: The methods augment sign language datasets and improve the translation models' performance by up to 19%.

Conclusion: The proposed techniques significantly enhance Sign Language Translation systems, paving the way for advanced systems in low-resource environments.

Abstract: Machine learning models fundamentally rely on large quantities of
high-quality data. Collecting the necessary data for these models can be
challenging due to cost, scarcity, and privacy restrictions. Signed languages
are visual languages used by the deaf community and are considered low-resource
languages. Sign language datasets are often orders of magnitude smaller than
their spoken language counterparts. Sign Language Production is the task of
generating sign language videos from spoken language sentences, while Sign
Language Translation is the reverse translation task. Here, we propose
leveraging recent advancements in Sign Language Production to augment existing
sign language datasets and enhance the performance of Sign Language Translation
models. For this, we utilize three techniques: a skeleton-based approach to
production, sign stitching, and two photo-realistic generative models, SignGAN
and SignSplat. We evaluate the effectiveness of these techniques in enhancing
the performance of Sign Language Translation models by generating variation in
the signer's appearance and the motion of the skeletal data. Our results
demonstrate that the proposed methods can effectively augment existing datasets
and enhance the performance of Sign Language Translation models by up to 19%,
paving the way for more robust and accurate Sign Language Translation systems,
even in resource-constrained environments.

</details>


### [57] [Learning Efficient and Generalizable Graph Retriever for Knowledge-Graph Question Answering](https://arxiv.org/abs/2506.09645)
*Tianjun Yao,Haoxuan Li,Zhiqiang Shen,Pan Li,Tongliang Liu,Kun Zhang*

Main category: cs.CL

TL;DR: The paper introduces RAPL, a new framework for efficient and effective graph retrieval in knowledge graph question answering (KGQA), emphasizing its ability to enhance reasoning capability and generalization in large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: The work addresses the limitations of existing LLMs, such as outdated data, hallucinations, and their reliance on unstructured text for retrieval, which reduces interpretability and reasoning when integrating knowledge graphs in KGQA.

Method: RAPL employs a three-pronged approach: (1) a two-stage labeling strategy using heuristic signals and parametric models for grounded supervision, (2) a graph transformation approach to enhance representation by capturing intra- and inter-triple interactions, and (3) a path-based reasoning mechanism enabling the use of structured rational information.

Result: RAPL achieves superior performance, outperforming state-of-the-art methods by 2.66%-20.34% in empirical evaluations. It significantly bridges gaps between smaller and larger LLM-based reasoners and performs well under cross-dataset settings.

Conclusion: RAPL shows promise in advancing retrieval-augmented frameworks by enhancing efficiency, effectiveness, and generalization in KGQA tasks, offering potential improvements for real-world applications with its structured reasoning and retrieval capabilities.

Abstract: Large Language Models (LLMs) have shown strong inductive reasoning ability
across various domains, but their reliability is hindered by the outdated
knowledge and hallucinations. Retrieval-Augmented Generation mitigates these
issues by grounding LLMs with external knowledge; however, most existing RAG
pipelines rely on unstructured text, limiting interpretability and structured
reasoning. Knowledge graphs, which represent facts as relational triples, offer
a more structured and compact alternative. Recent studies have explored
integrating knowledge graphs with LLMs for knowledge graph question answering
(KGQA), with a significant proportion adopting the retrieve-then-reasoning
paradigm. In this framework, graph-based retrievers have demonstrated strong
empirical performance, yet they still face challenges in generalization
ability. In this work, we propose RAPL, a novel framework for efficient and
effective graph retrieval in KGQA. RAPL addresses these limitations through
three aspects: (1) a two-stage labeling strategy that combines heuristic
signals with parametric models to provide causally grounded supervision; (2) a
model-agnostic graph transformation approach to capture both intra- and
inter-triple interactions, thereby enhancing representational capacity; and (3)
a path-based reasoning strategy that facilitates learning from the injected
rational knowledge, and supports downstream reasoner through structured inputs.
Empirically, RAPL outperforms state-of-the-art methods by $2.66\%-20.34\%$, and
significantly reduces the performance gap between smaller and more powerful
LLM-based reasoners, as well as the gap under cross-dataset settings,
highlighting its superior retrieval capability and generalizability. Codes are
available at: https://github.com/tianyao-aka/RAPL.

</details>


### [58] [Bridging the Gap Between Open-Source and Proprietary LLMs in Table QA](https://arxiv.org/abs/2506.09657)
*Nikolas Evkarpidi,Elena Tutubalina*

Main category: cs.CL

TL;DR: The paper discusses a system for tackling SemEval 2025 Task 8 QA challenges, achieving top-13 ranking among 38 teams with an 80% accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve question answering performance over tabular data by combining state-of-the-art NLP techniques and identifying remaining challenges in this domain.

Method: The approach implements text-to-SQL/code generation, a self-correction mechanism, and retrieval-augmented generation (RAG), integrated with an end-to-end (E2E) module, overseen by a large language model (LLM).

Result: The system achieves 80% accuracy, ranks within the top 13 in the competition, and outperforms open-source models while being competitive with proprietary LLMs.

Conclusion: The approach demonstrates a promising pipeline for QA over tabular data, but challenges persist which are revealed by ablation studies.

Abstract: This paper presents a system developed for SemEval 2025 Task 8: Question
Answering (QA) over tabular data. Our approach integrates several key
components: text-to-SQL and text-to-code generation modules, a self-correction
mechanism, and a retrieval-augmented generation (RAG). Additionally, it
includes an end-to-end (E2E) module, all orchestrated by a large language model
(LLM). Through ablation studies, we analyzed the effects of different parts of
our pipeline and identified the challenges that are still present in this
field. During the evaluation phase of the competition, our solution achieved an
accuracy of 80%, resulting in a top-13 ranking among the 38 participating
teams. Our pipeline demonstrates a significant improvement in accuracy for
open-source models and achieves a performance comparable to proprietary LLMs in
QA tasks over tables. The code is available at GitHub repository.

</details>


### [59] [Query-Level Uncertainty in Large Language Models](https://arxiv.org/abs/2506.09669)
*Lihu Chen,Gaël Varoquaux*

Main category: cs.CL

TL;DR: This paper introduces a training-free method called 'Internal Confidence' to detect Large Language Models' knowledge boundaries, improving adaptability and efficiency.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to enhance the ability of Large Language Models to identify the boundaries of their knowledge, enabling adaptive inference mechanisms like Retrieval-Augmented Generation and abstention methods for trustworthy AI.

Method: The authors propose a training-free method named 'Internal Confidence', leveraging self-evaluation across layers and tokens to detect if the model can address a query without generating tokens.

Result: Empirical results show that 'Internal Confidence' outperforms existing baselines in factual QA and mathematical reasoning tasks, demonstrating its ability to effectively detect knowledge boundaries.

Conclusion: The study confirms the utility of the proposed method for improving efficiency in RAG and model cascading, reducing inference costs while maintaining performance.

Abstract: It is important for Large Language Models to be aware of the boundary of
their knowledge, the mechanism of identifying known and unknown queries. This
type of awareness can help models perform adaptive inference, such as invoking
RAG, engaging in slow and deep thinking, or adopting the abstention mechanism,
which is beneficial to the development of efficient and trustworthy AI. In this
work, we propose a method to detect knowledge boundaries via Query-Level
Uncertainty, which aims to determine if the model is able to address a given
query without generating any tokens. To this end, we introduce a novel and
training-free method called \emph{Internal Confidence}, which leverages
self-evaluations across layers and tokens. Empirical results on both factual QA
and mathematical reasoning tasks demonstrate that our internal confidence can
outperform several baselines. Furthermore, we showcase that our proposed method
can be used for efficient RAG and model cascading, which is able to reduce
inference costs while maintaining performance.

</details>


### [60] [Is Fine-Tuning an Effective Solution? Reassessing Knowledge Editing for Unstructured Data](https://arxiv.org/abs/2506.09672)
*Hao Xiong,Chuanyuan Tan,Wenliang Chen*

Main category: cs.CL

TL;DR: The paper introduces solutions for Unstructured Knowledge Editing (UKE), creating datasets for evaluating 'locality' and improvements to fine-tuning methods.


<details>
  <summary>Details</summary>
Motivation: There are two main issues in UKE research: the lack of evaluation methods for 'locality' and the unexplained failure of fine-tuning (FT) methods.

Method: The authors constructed two datasets (UnKEBench-Loc and AKEW-Loc) for systematic locality evaluation and identified four factors affecting FT-based methods, proposing optimized training strategies.

Result: The optimized fine-tuning method (FT-UKE) surpassed existing state-of-the-art performance, especially excelling in batch editing scenarios.

Conclusion: With improved evaluation datasets and a proven optimal FT approach, this research advances methodologies for effective UKE in large language models.

Abstract: Unstructured Knowledge Editing (UKE) is crucial for updating the relevant
knowledge of large language models (LLMs). It focuses on unstructured inputs,
such as long or free-form texts, which are common forms of real-world
knowledge. Although previous studies have proposed effective methods and tested
them, some issues exist: (1) Lack of Locality evaluation for UKE, and (2)
Abnormal failure of fine-tuning (FT) based methods for UKE. To address these
issues, we first construct two datasets, UnKEBench-Loc and AKEW-Loc (CF), by
extending two existing UKE datasets with locality test data from the
unstructured and structured views. This enables a systematic evaluation of the
Locality of post-edited models. Furthermore, we identify four factors that may
affect the performance of FT-based methods. Based on these factors, we conduct
experiments to determine how the well-performing FT-based methods should be
trained for the UKE task, providing a training recipe for future research. Our
experimental results indicate that the FT-based method with the optimal setting
(FT-UKE) is surprisingly strong, outperforming the existing state-of-the-art
(SOTA). In batch editing scenarios, FT-UKE shows strong performance as well,
with its advantage over SOTA methods increasing as the batch size grows,
expanding the average metric lead from +6.78% to +10.80%

</details>


### [61] [Inv-Entropy: A Fully Probabilistic Framework for Uncertainty Quantification in Language Models](https://arxiv.org/abs/2506.09684)
*Haoyi Song,Ruihan Ji,Naichen Shi,Fan Lai,Raed Al Kontar*

Main category: cs.CL

TL;DR: The paper proposes a probabilistic framework for uncertainty quantification (UQ) in large language models (LLMs) through systematic input perturbations, introducing new methods and evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: To improve the reliability of large language models (LLMs) by addressing the limitations of existing heuristic-based uncertainty quantification methods with a probabilistic approach.

Method: A dual random walk perspective models semantic similarities using two Markov chains, paired with systematic input perturbation strategies. Introduced techniques include Inv-Entropy for uncertainty measurement, the GAAP algorithm for enhancing input diversity, and TSU metric for direct evaluation.

Result: Experimental results show that the proposed Inv-Entropy outperforms existing UQ methods and the framework demonstrates flexibility and enhanced uncertainty measurement capabilities.

Conclusion: The framework enhances UQ for LLMs, offering superior performance, flexibility in implementation, and novel ways to measure uncertainty effectively.

Abstract: Large language models (LLMs) have transformed natural language processing,
but their reliable deployment requires effective uncertainty quantification
(UQ). Existing UQ methods are often heuristic and lack a probabilistic
foundation. This paper begins by providing a theoretical justification for the
role of perturbations in UQ for LLMs. We then introduce a dual random walk
perspective, modeling input-output pairs as two Markov chains with transition
probabilities defined by semantic similarity. Building on this, we propose a
fully probabilistic framework based on an inverse model, which quantifies
uncertainty by evaluating the diversity of the input space conditioned on a
given output through systematic perturbations. Within this framework, we define
a new uncertainty measure, Inv-Entropy. A key strength of our framework is its
flexibility: it supports various definitions of uncertainty measures,
embeddings, perturbation strategies, and similarity metrics. We also propose
GAAP, a perturbation algorithm based on genetic algorithms, which enhances the
diversity of sampled inputs. In addition, we introduce a new evaluation metric,
Temperature Sensitivity of Uncertainty (TSU), which directly assesses
uncertainty without relying on correctness as a proxy. Extensive experiments
demonstrate that Inv-Entropy outperforms existing semantic UQ methods. The code
to reproduce the results can be found at
https://github.com/UMDataScienceLab/Uncertainty-Quantification-for-LLMs.

</details>


### [62] [Do LLMs Give Psychometrically Plausible Responses in Educational Assessments?](https://arxiv.org/abs/2506.09796)
*Andreas Säuberli,Diego Frassinelli,Barbara Plank*

Main category: cs.CL

TL;DR: This paper investigates whether instruction-tuned large language models (LLMs) can display human-like behavior when responding to educational assessment items, using psychometric frameworks.


<details>
  <summary>Details</summary>
Motivation: The study aims to determine whether LLMs can replicate human response behaviors to accelerate the development of educational assessments without relying on extensive human pilot studies.

Method: The researchers evaluated 18 instruction-tuned LLMs on multiple-choice test items in reading, U.S. history, and economics, using psychometric frameworks such as classical test theory and item response theory.

Result: Larger LLMs were found excessively confident, but their responses could mimic human likelihood better when calibrated with temperature scaling. LLMs showed better performance in reading comprehension compared to other subjects but overall correlations with human responses were weak.

Conclusion: LLMs currently cannot substitute for humans in piloting educational assessments, particularly in zero-shot settings.

Abstract: Knowing how test takers answer items in educational assessments is essential
for test development, to evaluate item quality, and to improve test validity.
However, this process usually requires extensive pilot studies with human
participants. If large language models (LLMs) exhibit human-like response
behavior to test items, this could open up the possibility of using them as
pilot participants to accelerate test development. In this paper, we evaluate
the human-likeness or psychometric plausibility of responses from 18
instruction-tuned LLMs with two publicly available datasets of multiple-choice
test items across three subjects: reading, U.S. history, and economics. Our
methodology builds on two theoretical frameworks from psychometrics which are
commonly used in educational assessment, classical test theory and item
response theory. The results show that while larger models are excessively
confident, their response distributions can be more human-like when calibrated
with temperature scaling. In addition, we find that LLMs tend to correlate
better with humans in reading comprehension items compared to other subjects.
However, the correlations are not very strong overall, indicating that LLMs
should not be used for piloting educational assessments in a zero-shot setting.

</details>


### [63] [CoRT: Code-integrated Reasoning within Thinking](https://arxiv.org/abs/2506.09820)
*Chengpeng Li,Zhengyang Tang,Ziniu Li,Mingfeng Xue,Keqin Bao,Tian Ding,Ruoyu Sun,Benyou Wang,Xiang Wang,Junyang Lin,Dayiheng Liu*

Main category: cs.CL

TL;DR: The paper presents CoRT, a framework for training large reasoning models (LRMs) to efficiently utilize code interpreters (CI) for mathematical reasoning, showing improvements in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Current large reasoning models struggle with efficiency and accuracy in handling complex mathematical operations. While code interpreters could address this, their integration is currently inefficient.

Method: The authors propose a post-training framework called CoRT using Hint-Engineering to synthesize reasoning data for better LRM-CI interaction. They manually crafted datasets, used supervised and reinforcement learning techniques for fine-tuning models of various scales.

Result: Models trained with Hint-Engineering show significant improvements (4-8%) on mathematical reasoning tasks and use 30-50% fewer tokens, proving both accuracy and efficiency gains.

Conclusion: The CoRT framework successfully enables LRMs to utilize CIs effectively, making them more competent in complex mathematical reasoning. The resources (models and code) are openly shared for further exploration.

Abstract: Large Reasoning Models (LRMs) like o1 and DeepSeek-R1 have shown remarkable
progress in natural language reasoning with long chain-of-thought (CoT), yet
they remain inefficient or inaccurate when handling complex mathematical
operations. Addressing these limitations through computational tools (e.g.,
computation libraries and symbolic solvers) is promising, but it introduces a
technical challenge: Code Interpreter (CI) brings external knowledge beyond the
model's internal text representations, thus the direct combination is not
efficient. This paper introduces CoRT, a post-training framework for teaching
LRMs to leverage CI effectively and efficiently. As a first step, we address
the data scarcity issue by synthesizing code-integrated reasoning data through
Hint-Engineering, which strategically inserts different hints at appropriate
positions to optimize LRM-CI interaction. We manually create 30 high-quality
samples, upon which we post-train models ranging from 1.5B to 32B parameters,
with supervised fine-tuning, rejection fine-tuning and reinforcement learning.
Our experimental results demonstrate that Hint-Engineering models achieve 4\%
and 8\% absolute improvements on DeepSeek-R1-Distill-Qwen-32B and
DeepSeek-R1-Distill-Qwen-1.5B respectively, across five challenging
mathematical reasoning datasets. Furthermore, Hint-Engineering models use about
30\% fewer tokens for the 32B model and 50\% fewer tokens for the 1.5B model
compared with the natural language models. The models and code are available at
https://github.com/ChengpengLi1003/CoRT.

</details>


### [64] [EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech Emotion Detection](https://arxiv.org/abs/2506.09827)
*Christoph Schuhmann,Robert Kaczmarczyk,Gollam Rabby,Felix Friedrich,Maurice Kraus,Kourosh Nadi,Huu Nguyen,Kristian Kersting,Sören Auer*

Main category: cs.CL

TL;DR: The paper introduces EmoNet-Voice, a resource for speech emotion recognition that includes a large dataset, fine-grained annotations, and new benchmarks, validated by psychology experts.


<details>
  <summary>Details</summary>
Motivation: Existing SER datasets have limitations in granularity, privacy, and reliance on acted emotions, necessitating a better resource for robust emotional understanding in AI systems.

Method: The authors developed synthetic audio snippets mimicking emotions using advanced voice generation and validated these emotions with psychology experts. They also created an emotion model benchmark with 40 fine-grained categories.

Result: EmoNet-Voice provides a large dataset (4,500+ hours) in multiple languages and with rigorous human annotations. Empathic Insight Voice models achieved strong alignment with human expert evaluations.

Conclusion: EmoNet-Voice and the Empathic Insight Voice models advance the field of speech emotion recognition, showing that high-arousal emotions are easier to detect than low-arousal ones.

Abstract: The advancement of text-to-speech and audio generation models necessitates
robust benchmarks for evaluating the emotional understanding capabilities of AI
systems. Current speech emotion recognition (SER) datasets often exhibit
limitations in emotional granularity, privacy concerns, or reliance on acted
portrayals. This paper introduces EmoNet-Voice, a new resource for speech
emotion detection, which includes EmoNet-Voice Big, a large-scale pre-training
dataset (featuring over 4,500 hours of speech across 11 voices, 40 emotions,
and 4 languages), and EmoNet-Voice Bench, a novel benchmark dataset with human
expert annotations. EmoNet-Voice is designed to evaluate SER models on a
fine-grained spectrum of 40 emotion categories with different levels of
intensities. Leveraging state-of-the-art voice generation, we curated synthetic
audio snippets simulating actors portraying scenes designed to evoke specific
emotions. Crucially, we conducted rigorous validation by psychology experts who
assigned perceived intensity labels. This synthetic, privacy-preserving
approach allows for the inclusion of sensitive emotional states often absent in
existing datasets. Lastly, we introduce Empathic Insight Voice models that set
a new standard in speech emotion recognition with high agreement with human
experts. Our evaluations across the current model landscape exhibit valuable
findings, such as high-arousal emotions like anger being much easier to detect
than low-arousal states like concentration.

</details>


### [65] [Error-Guided Pose Augmentation: Enhancing Rehabilitation Exercise Assessment through Targeted Data Generation](https://arxiv.org/abs/2506.09833)
*Omar Sherif,Ali Hamdi*

Main category: cs.CL

TL;DR: This paper proposes a novel method called Error-Guided Pose Augmentation (EGPA) to enhance automated rehabilitation assessment by simulating clinically relevant movement errors and combining it with an attention-based graph convolutional network.


<details>
  <summary>Details</summary>
Motivation: To address challenges in rehabilitation assessment, such as data imbalance and difficulty detecting subtle movement errors, particularly in home-based settings.

Method: The method involves generating synthetic skeleton data simulating movement errors and integrating this with an attention-based graph convolutional network.

Result: EGPA achieves up to a 27.6% reduction in mean absolute error and a 45.8% improvement in error classification accuracy. Visualizations show that it focuses on clinically significant aspects.

Conclusion: EGPA enhances the accuracy and interpretability of movement quality assessment, presenting a promising tool for both clinical and home-rehabilitation applications.

Abstract: Effective rehabilitation assessment is essential for monitoring patient
progress, particularly in home-based settings. Existing systems often face
challenges such as data imbalance and difficulty detecting subtle movement
errors. This paper introduces Error-Guided Pose Augmentation (EGPA), a method
that generates synthetic skeleton data by simulating clinically relevant
movement mistakes. Unlike standard augmentation techniques, EGPA targets
biomechanical errors observed in rehabilitation. Combined with an
attention-based graph convolutional network, EGPA improves performance across
multiple evaluation metrics. Experiments demonstrate reductions in mean
absolute error of up to 27.6 percent and gains in error classification accuracy
of 45.8 percent. Attention visualizations show that the model learns to focus
on clinically significant joints and movement phases, enhancing both accuracy
and interpretability. EGPA offers a promising approach for improving automated
movement quality assessment in both clinical and home-based rehabilitation
contexts.

</details>


### [66] [Dataset of News Articles with Provenance Metadata for Media Relevance Assessment](https://arxiv.org/abs/2506.09847)
*Tomas Peterka,Matyas Bohacek*

Main category: cs.CL

TL;DR: The paper addresses media manipulation through out-of-context and misattributed imagery by introducing a dataset of news articles with provenance-tagged images and analyzing Large Language Models performances on two tasks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address limitations in current methods for detecting out-of-context imagery in media manipulation, where existing approaches fail if the depicted content aligns somewhat with the text narrative.

Method: The authors introduce the News Media Provenance Dataset and design two specific tasks: location of origin relevance (LOR) and date and time of origin relevance (DTOR). Baseline results are evaluated on six large language models.

Result: Zero-shot performance on the LOR task was promising, but DTOR results were weaker, indicating room for improvement through specialized models.

Conclusion: The paper underscores the need for enhanced approaches to media manipulation detection, particularly for temporal and locational provenance, and opens avenues for developing specialized architectures.

Abstract: Out-of-context and misattributed imagery is the leading form of media
manipulation in today's misinformation and disinformation landscape. The
existing methods attempting to detect this practice often only consider whether
the semantics of the imagery corresponds to the text narrative, missing
manipulation so long as the depicted objects or scenes somewhat correspond to
the narrative at hand. To tackle this, we introduce News Media Provenance
Dataset, a dataset of news articles with provenance-tagged images. We formulate
two tasks on this dataset, location of origin relevance (LOR) and date and time
of origin relevance (DTOR), and present baseline results on six large language
models (LLMs). We identify that, while the zero-shot performance on LOR is
promising, the performance on DTOR hinders, leaving room for specialized
architectures and future work.

</details>


### [67] [Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning](https://arxiv.org/abs/2506.09853)
*Xiangning Yu,Zhuohan Wang,Linyi Yang,Haoxuan Li,Anjie Liu,Xiao Xue,Jun Wang,Mengyue Yang*

Main category: cs.CL

TL;DR: This paper proposes a causal framework to enhance Chain-of-Thought (CoT) prompting for Large Language Models (LLMs), addressing the challenges of sufficiency and necessity in reasoning steps.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address two challenges in CoT reasoning for LLMs: ensuring the generated inference steps are both sufficient (comprehensive and substantiating) and necessary (indispensable for sound answers).

Method: The authors introduce a causal framework leveraging the causal Probability of Sufficiency and Necessity, enabling automatic addition of missing steps and removal of redundant ones in reasoning processes.

Result: The framework significantly improves reasoning efficiency and reduces token usage across mathematical and commonsense reasoning benchmarks without sacrificing accuracy.

Conclusion: The proposed approach optimizes LLM reasoning performance by balancing efficiency, accuracy, and cost-effectiveness, offering a promising advancement in CoT prompting.

Abstract: Chain-of-Thought (CoT) prompting plays an indispensable role in endowing
large language models (LLMs) with complex reasoning capabilities. However, CoT
currently faces two fundamental challenges: (1) Sufficiency, which ensures that
the generated intermediate inference steps comprehensively cover and
substantiate the final conclusion; and (2) Necessity, which identifies the
inference steps that are truly indispensable for the soundness of the resulting
answer. We propose a causal framework that characterizes CoT reasoning through
the dual lenses of sufficiency and necessity. Incorporating causal Probability
of Sufficiency and Necessity allows us not only to determine which steps are
logically sufficient or necessary to the prediction outcome, but also to
quantify their actual influence on the final reasoning outcome under different
intervention scenarios, thereby enabling the automated addition of missing
steps and the pruning of redundant ones. Extensive experimental results on
various mathematical and commonsense reasoning benchmarks confirm substantial
improvements in reasoning efficiency and reduced token usage without
sacrificing accuracy. Our work provides a promising direction for improving LLM
reasoning performance and cost-effectiveness.

</details>


### [68] [Attention Head Embeddings with Trainable Deep Kernels for Hallucination Detection in LLMs](https://arxiv.org/abs/2506.09886)
*Rodion Oblovatny,Alexandra Bazarova,Alexey Zaytsev*

Main category: cs.CL

TL;DR: The paper introduces a new method to detect hallucinations in large language models (LLMs) using hidden-state distributions and deep learnable kernels.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of detecting hallucinations in LLMs without relying on external knowledge or auxiliary systems.

Method: Analyze probabilistic divergence between prompt and response hidden-state distributions, using distributional distances and deep learnable kernels for detection.

Result: The proposed method surpasses current baselines, achieving state-of-the-art benchmark performance and remains effective even without kernel training.

Conclusion: This approach provides a scalable, robust, and effective solution for detecting LLM hallucinations using intrinsic model properties.

Abstract: We present a novel approach for detecting hallucinations in large language
models (LLMs) by analyzing the probabilistic divergence between prompt and
response hidden-state distributions. Counterintuitively, we find that
hallucinated responses exhibit smaller deviations from their prompts compared
to grounded responses, suggesting that hallucinations often arise from
superficial rephrasing rather than substantive reasoning. Leveraging this
insight, we propose a model-intrinsic detection method that uses distributional
distances as principled hallucination scores, eliminating the need for external
knowledge or auxiliary models. To enhance sensitivity, we employ deep learnable
kernels that automatically adapt to capture nuanced geometric differences
between distributions. Our approach outperforms existing baselines,
demonstrating state-of-the-art performance on several benchmarks. The method
remains competitive even without kernel training, offering a robust, scalable
solution for hallucination detection.

</details>


### [69] [The Emergence of Abstract Thought in Large Language Models Beyond Any Language](https://arxiv.org/abs/2506.09890)
*Yuxin Chen,Yiran Zhao,Yang Zhang,An Zhang,Kenji Kawaguchi,Shafiq Joty,Junnan Li,Tat-Seng Chua,Michael Qizhe Shieh,Wenxuan Zhang*

Main category: cs.CL

TL;DR: The paper explores the multilingual capabilities of large language models (LLMs), revealing a critical language-agnostic parameter space that supports abstract thought and generalization across languages.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs exhibit strong multilingual performance and whether their thought processes are tied to specific languages or are language-agnostic.

Method: The authors analyze neurons in LLMs for language-related activities, categorize them as shared or exclusive across languages, and observe changes over time in neuron functionality. They also propose neuron-specific training strategies based on these findings.

Result: The study finds a compact, essential language-agnostic parameter set within LLMs, with shared neurons increasing and exclusive neurons diminishing as LLMs develop, enabling language-independent abstract thought.

Conclusion: LLMs achieve multilingual robustness through a core language-agnostic parameter space supported by shared neurons, representing a shift towards abstract, language-independent processing. The proposed neuron-specific training strategies are validated through experiments.

Abstract: As large language models (LLMs) continue to advance, their capacity to
function effectively across a diverse range of languages has shown marked
improvement. Preliminary studies observe that the hidden activations of LLMs
often resemble English, even when responding to non-English prompts. This has
led to the widespread assumption that LLMs may "think" in English. However,
more recent results showing strong multilingual performance, even surpassing
English performance on specific tasks in other languages, challenge this view.
In this work, we find that LLMs progressively develop a core language-agnostic
parameter space-a remarkably small subset of parameters whose deactivation
results in significant performance degradation across all languages. This
compact yet critical set of parameters underlies the model's ability to
generalize beyond individual languages, supporting the emergence of abstract
thought that is not tied to any specific linguistic system. Specifically, we
identify language-related neurons-those are consistently activated during the
processing of particular languages, and categorize them as either shared
(active across multiple languages) or exclusive (specific to one). As LLMs
undergo continued development over time, we observe a marked increase in both
the proportion and functional importance of shared neurons, while exclusive
neurons progressively diminish in influence. These shared neurons constitute
the backbone of the core language-agnostic parameter space, supporting the
emergence of abstract thought. Motivated by these insights, we propose
neuron-specific training strategies tailored to LLMs' language-agnostic levels
at different development stages. Experiments across diverse LLM families
support our approach.

</details>


### [70] [PersonaLens: A Benchmark for Personalization Evaluation in Conversational AI Assistants](https://arxiv.org/abs/2506.09902)
*Zheng Zhao,Clara Vania,Subhradeep Kayal,Naila Khan,Shay B. Cohen,Emine Yilmaz*

Main category: cs.CL

TL;DR: The paper introduces PersonaLens, a new benchmark for evaluating personalization in task-oriented conversational AI, highlighting variability in current AI assistants' personalization abilities.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks inadequately assess personalization in AI assistants due to their focus on chit-chat, limited tasks, or narrow domains.

Method: PersonaLens integrates diverse user profiles, preferences, histories, along with LLM-based agents for realistic dialogues and evaluation under the LLM-as-a-Judge paradigm.

Result: Experiments demonstrate variability in personalization capabilities among current LLM-based assistants, revealing limitations and areas for improvement.

Conclusion: PersonaLens offers a robust framework for advancing the understanding and development of personalized AI assistance in complex task-oriented scenarios.

Abstract: Large language models (LLMs) have advanced conversational AI assistants.
However, systematically evaluating how well these assistants apply
personalization--adapting to individual user preferences while completing
tasks--remains challenging. Existing personalization benchmarks focus on
chit-chat, non-conversational tasks, or narrow domains, failing to capture the
complexities of personalized task-oriented assistance. To address this, we
introduce PersonaLens, a comprehensive benchmark for evaluating personalization
in task-oriented AI assistants. Our benchmark features diverse user profiles
equipped with rich preferences and interaction histories, along with two
specialized LLM-based agents: a user agent that engages in realistic
task-oriented dialogues with AI assistants, and a judge agent that employs the
LLM-as-a-Judge paradigm to assess personalization, response quality, and task
success. Through extensive experiments with current LLM assistants across
diverse tasks, we reveal significant variability in their personalization
capabilities, providing crucial insights for advancing conversational AI
systems.

</details>


### [71] [Aspect-Based Opinion Summarization with Argumentation Schemes](https://arxiv.org/abs/2506.09917)
*Wendi Zhou,Ameer Saadat-Yazd,Nadin Kokciyan*

Main category: cs.CL

TL;DR: ASESUM is a novel system for opinion summarization in online reviews, designed to extract and summarize aspect-centric arguments with grounding and adaptability across domains.


<details>
  <summary>Details</summary>
Motivation: Customers face challenges in manually analyzing vast reviews for key opinions, necessitating automated summarization systems.

Method: ASESUM extracts aspect-centric arguments, evaluates salience and validity, and operates without pre-defined aspect sets, adaptable for differing domains.

Result: Experiments on real-world datasets show ASESUM outperforms comparative methods in capturing diverse and grounded perspectives.

Conclusion: ASESUM effectively provides grounded, aspect-centric summaries, addressing challenges in domain adaptability and opinion diversity.

Abstract: Reviews are valuable resources for customers making purchase decisions in
online shopping. However, it is impractical for customers to go over the vast
number of reviews and manually conclude the prominent opinions, which prompts
the need for automated opinion summarization systems. Previous approaches,
either extractive or abstractive, face challenges in automatically producing
grounded aspect-centric summaries. In this paper, we propose a novel
summarization system that not only captures predominant opinions from an aspect
perspective with supporting evidence, but also adapts to varying domains
without relying on a pre-defined set of aspects. Our proposed framework,
ASESUM, summarizes viewpoints relevant to the critical aspects of a product by
extracting aspect-centric arguments and measuring their salience and validity.
We conduct experiments on a real-world dataset to demonstrate the superiority
of our approach in capturing diverse perspectives of the original reviews
compared to new and existing methods.

</details>


### [72] [VerIF: Verification Engineering for Reinforcement Learning in Instruction Following](https://arxiv.org/abs/2506.09942)
*Hao Peng,Yunjia Qi,Xiaozhi Wang,Bin Xu,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: The paper presents VerIF, a hybrid verification method combining rule-based and LLM-based approaches, for reinforcement learning in instruction-following tasks. The approach achieves state-of-the-art results on benchmarks and generalizes well.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the underexplored challenge of verification in reinforcement learning for instruction-following tasks, improving the reliability and performance of trained large language models.

Method: The authors introduce VerIF, a method that integrates rule-based code verification with verification from a reasoning language model (like QwQ-32B). They also create VerInstruct, a dataset of approximately 22,000 instruction-following examples to support RL training with verification.

Result: Models trained with VerIF achieved state-of-the-art performance on instruction-following benchmarks, demonstrated strong generalization to unseen constraints, and maintained their general capabilities.

Conclusion: The proposed VerIF method can enhance reinforcement learning in large language models without compromising their general abilities, providing a scalable solution for instruction-following tasks and promoting further research via open datasets and codes.

Abstract: Reinforcement learning with verifiable rewards (RLVR) has become a key
technique for enhancing large language models (LLMs), with verification
engineering playing a central role. However, best practices for RL in
instruction following remain underexplored. In this work, we explore the
verification challenge in RL for instruction following and propose VerIF, a
verification method that combines rule-based code verification with LLM-based
verification from a large reasoning model (e.g., QwQ-32B). To support this
approach, we construct a high-quality instruction-following dataset,
VerInstruct, containing approximately 22,000 instances with associated
verification signals. We apply RL training with VerIF to two models, achieving
significant improvements across several representative instruction-following
benchmarks. The trained models reach state-of-the-art performance among models
of comparable size and generalize well to unseen constraints. We further
observe that their general capabilities remain unaffected, suggesting that RL
with VerIF can be integrated into existing RL recipes to enhance overall model
performance. We have released our datasets, codes, and models to facilitate
future research at https://github.com/THU-KEG/VerIF.

</details>


### [73] [Query-Focused Retrieval Heads Improve Long-Context Reasoning and Re-ranking](https://arxiv.org/abs/2506.09944)
*Wuwei Zhang,Fangcong Yin,Howard Yen,Danqi Chen,Xi Ye*

Main category: cs.CL

TL;DR: The paper introduces QRHEAD and QR-RETRIEVER to improve information retrieval in long-context language models, achieving significant performance advancements in reasoning and re-ranking tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of effective information retrieval from long contexts in language models, improving interpretability and performance using attention mechanisms.

Method: QRHEAD is identified by aggregating attention scores relative to an input query. QR-RETRIEVER leverages QRHEAD's accumulated attention mass for scoring and retrieving context parts, enabling better long-context reasoning.

Result: QRHEAD and QR-RETRIEVER improve performance in reasoning tasks like LongMemEval and CLIPPER by over 10% and outperform other models in re-ranking tasks on the BEIR benchmark.

Conclusion: The findings highlight QR-RETRIEVER's efficiency and effectiveness as a general-purpose retrieval tool while offering interpretability insights about attention mechanisms in LMs.

Abstract: Recent work has identified retrieval heads (Wu et al., 2025b), a subset of
attention heads responsible for retrieving salient information in long-context
language models (LMs), as measured by their copy-paste behavior in
Needle-in-a-Haystack tasks. In this paper, we introduce QRHEAD (Query-Focused
Retrieval Head), an improved set of attention heads that enhance retrieval from
long context. We identify QRHEAD by aggregating attention scores with respect
to the input query, using a handful of examples from real-world tasks (e.g.,
long-context QA). We further introduce QR- RETRIEVER, an efficient and
effective retriever that uses the accumulated attention mass of QRHEAD as
retrieval scores. We use QR- RETRIEVER for long-context reasoning by selecting
the most relevant parts with the highest retrieval scores. On multi-hop
reasoning tasks LongMemEval and CLIPPER, this yields over 10% performance gains
over full context and outperforms strong dense retrievers. We also evaluate
QRRETRIEVER as a re-ranker on the BEIR benchmark and find that it achieves
strong zero-shot performance, outperforming other LLM-based re-rankers such as
RankGPT. Further analysis shows that both the querycontext attention scoring
and task selection are crucial for identifying QRHEAD with strong downstream
utility. Overall, our work contributes a general-purpose retriever and offers
interpretability insights into the long-context capabilities of LMs.

</details>


### [74] [Resa: Transparent Reasoning Models via SAEs](https://arxiv.org/abs/2506.09967)
*Shangshang Wang,Julian Asilis,Ömer Faruk Akgül,Enes Burak Bilgin,Ollie Liu,Deqing Fu,Willie Neiswanger*

Main category: cs.CL

TL;DR: Resa introduces a cost-effective method using sparse autoencoder tuning to improve reasoning in language models.


<details>
  <summary>Details</summary>
Motivation: Develop cost-effective methods to enhance reasoning abilities in language models without extensive computational and financial resources.

Method: Sparse Autoencoder Tuning (SAE-Tuning), which trains an autoencoder to capture reasoning capabilities, followed by traditional fine-tuning on question-answer data.

Result: The method reduces training costs by over 2000x and training time by 450x while achieving strong reasoning performance comparable to extensively trained models.

Conclusion: SAE-Tuning efficiently elicits reasoning, yielding generalizable and modular capabilities, demonstrating scalability and strong performance validated by extensive tests.

Abstract: How cost-effectively can we elicit strong reasoning in language models by
leveraging their underlying representations? We answer this question with Resa,
a family of 1.5B reasoning models trained via a novel and efficient sparse
autoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to
capture reasoning abilities from a source model, and then uses the trained SAE
to guide a standard supervised fine-tuning process to elicit such abilities in
a target model, all using verified question-answer data without any reasoning
traces. Notably, when applied to certain base models before further RL
post-training, SAE-Tuning retains >97% of its RL-trained counterpart's
reasoning performance while reducing training costs by >2000x to roughly \$1
and training time by >450x to around 20 minutes. Furthermore, when applied to
lightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning
performance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only
around \$1 additional cost. Surprisingly, the reasoning abilities extracted via
SAEs are potentially both generalizable and modular. Generality means abilities
extracted from one dataset still elevate performance on a larger and
overlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math
can be attached to the R1-Distill model at test time, without any retraining,
and yield comparable gains. Extensive ablations validate these findings and all
artifacts are fully open-sourced.

</details>


### [75] [When Detection Fails: The Power of Fine-Tuned Models to Generate Human-Like Social Media Text](https://arxiv.org/abs/2506.09975)
*Hillary Dawkins,Kathleen C. Fraser,Svetlana Kiritchenko*

Main category: cs.CL

TL;DR: The paper addresses the challenge of detecting AI-generated social media content, especially posts created by fine-tuned large language models (LLMs), where detectability significantly drops, posing critical implications for detection methods.


<details>
  <summary>Details</summary>
Motivation: The authors aim to tackle the challenge of detecting AI-generated content on social media, motivated by the increasing risk of such content driving online influence campaigns to manipulate public opinion on controversial topics.

Method: A dataset of 505,159 AI-generated posts on 11 controversial topics was created using a mix of open-source, closed-source, and fine-tuned LLMs. Detection capabilities were evaluated under both ideal research conditions and realistic attack scenarios with secretive fine-tuned models. Human studies and ablation experiments were also conducted.

Result: The paper demonstrates that while detection techniques are effective under controlled conditions, their performance significantly degrades when dealing with fine-tuned, non-public LLMs. This vulnerability was confirmed through human evaluations and experiments.

Conclusion: Detection systems are currently ill-equipped to handle the realistic threat posed by fine-tuned LLMs in generating undetectable social media posts, raising concerns for all AI detection areas and necessitating more robust future solutions.

Abstract: Detecting AI-generated text is a difficult problem to begin with; detecting
AI-generated text on social media is made even more difficult due to the short
text length and informal, idiosyncratic language of the internet. It is
nonetheless important to tackle this problem, as social media represents a
significant attack vector in online influence campaigns, which may be bolstered
through the use of mass-produced AI-generated posts supporting (or opposing)
particular policies, decisions, or events. We approach this problem with the
mindset and resources of a reasonably sophisticated threat actor, and create a
dataset of 505,159 AI-generated social media posts from a combination of
open-source, closed-source, and fine-tuned LLMs, covering 11 different
controversial topics. We show that while the posts can be detected under
typical research assumptions about knowledge of and access to the generating
models, under the more realistic assumption that an attacker will not release
their fine-tuned model to the public, detectability drops dramatically. This
result is confirmed with a human study. Ablation experiments highlight the
vulnerability of various detection algorithms to fine-tuned LLMs. This result
has implications across all detection domains, since fine-tuning is a generally
applicable and realistic LLM use case.

</details>


### [76] [Step-by-step Instructions and a Simple Tabular Output Format Improve the Dependency Parsing Accuracy of LLMs](https://arxiv.org/abs/2506.09983)
*Hiroshi Matsuda,Chunpeng Ma,Masayuki Asahara*

Main category: cs.CL

TL;DR: The paper introduces a step-by-step instruction strategy that significantly improves dependency parsing accuracy using large language models.


<details>
  <summary>Details</summary>
Motivation: Standard prompting in large language models struggles with accuracy and structural validity in tasks like dependency parsing.

Method: The authors used a novel step-by-step strategy starting with part-of-speech tagging, followed by predicting syntactic heads and dependency labels, along with a simplified CoNLL-U style format. Multilingual fine-tuning was also employed.

Result: The method achieved state-of-the-art accuracy across 17 languages on Universal Dependencies datasets, with improved cross-language generalization.

Conclusion: Explicit reasoning steps in LLM-based parsing are effective, and the proposed approach provides a scalable and consistent alternative to traditional methods.

Abstract: Recent advances in large language models (LLMs) have enabled impressive
performance in various tasks. However, standard prompting often struggles to
produce structurally valid and accurate outputs, especially in dependency
parsing. We propose a novel step-by-step instruction strategy, where universal
part-of-speech tagging precedes the prediction of syntactic heads and
dependency labels, and a simplified CoNLL-U like output format, our method
achieves state-of-the-art accuracy on Universal Dependencies datasets across 17
languages without hallucination or contamination. We further show that
multilingual fine-tuning simultaneously improves cross-language generalization
performance. Our results highlight the effectiveness of explicit reasoning
steps in LLM-based parsing and offer a scalable, format-consistent alternative
to bracket-based approaches.

</details>


### [77] [Large Language Models for Toxic Language Detection in Low-Resource Balkan Languages](https://arxiv.org/abs/2506.09992)
*Amel Muminovic,Amela Kadric Muminovic*

Main category: cs.CL

TL;DR: This paper evaluates large language models in detecting toxic comments in Serbian, Croatian, and Bosnian. It shows that adding short context improves recall and F1 scores.


<details>
  <summary>Details</summary>
Motivation: To address the lack of toxic language moderation tools in Balkan languages, which are low-resource in terms of labeled datasets.

Method: Manually curated a dataset of 4,500 comments from YouTube and TikTok. Evaluated four language models under zero-shot and context-augmented scenarios, assessing performance metrics like precision, recall, F1 score, and accuracy.

Result: Context snippets improved recall by around 0.12 and F1 scores by up to 0.10 on average but increased false positives. Gemini in context-augmented mode achieved the best F1 score (0.82) and accuracy (0.82), while GPT-4.1 excelled in precision and minimizing false positives.

Conclusion: Adding minimal context enhances toxic language detection for resource-limited Balkan languages. Prompt design and threshold calibration can further optimize outcomes in such settings.

Abstract: Online toxic language causes real harm, especially in regions with limited
moderation tools. In this study, we evaluate how large language models handle
toxic comments in Serbian, Croatian, and Bosnian, languages with limited
labeled data. We built and manually labeled a dataset of 4,500 YouTube and
TikTok comments drawn from videos across diverse categories, including music,
politics, sports, modeling, influencer content, discussions of sexism, and
general topics. Four models (GPT-3.5 Turbo, GPT-4.1, Gemini 1.5 Pro, and Claude
3 Opus) were tested in two modes: zero-shot and context-augmented. We measured
precision, recall, F1 score, accuracy and false positive rates. Including a
short context snippet raised recall by about 0.12 on average and improved F1
score by up to 0.10, though it sometimes increased false positives. The best
balance came from Gemini in context-augmented mode, reaching an F1 score of
0.82 and accuracy of 0.82, while zero-shot GPT-4.1 led on precision and had the
lowest false alarms. We show how adding minimal context can improve toxic
language detection in low-resource settings and suggest practical strategies
such as improved prompt design and threshold calibration. These results show
that prompt design alone can yield meaningful gains in toxicity detection for
underserved Balkan language communities.

</details>


### [78] [From Judgment to Interference: Early Stopping LLM Harmful Outputs via Streaming Content Monitoring](https://arxiv.org/abs/2506.09996)
*Yang Li,Qiang Sheng,Yehan Yang,Xueyao Zhang,Juan Cao*

Main category: cs.CL

TL;DR: This paper addresses the issue of high latency in harmfulness detection for large language model (LLM) outputs by proposing a more efficient partial detection method.


<details>
  <summary>Details</summary>
Motivation: Current harmfulness detection strategies for LLM outputs, based on analyzing complete outputs, result in high latency. Partial detection has potential but faces challenges due to training-inference gaps when applying full-detection-trained moderators to incomplete outputs.

Method: The authors introduce 'FineHarm,' a dataset crafted for fine-grained annotations, alongside the 'Streaming Content Monitor' (SCM), which utilizes dual supervision (response- and token-level labels) to provide timely token-level harmfulness judgments during generation.

Result: SCM achieves a macro F1 score of 0.95+ comparable to full detection methods, while analyzing only the first 18% of tokens on average. It also serves as a pseudo-annotator for better safety alignment, outperforming methods like DPO in achieving higher harmlessness scores.

Conclusion: The proposed SCM offers efficient, accurate partial detection of harmful content while aiding safety alignment improvements, addressing both latency and performance issues in LLM moderation.

Abstract: Though safety alignment has been applied to most large language models
(LLMs), LLM service providers generally deploy a subsequent moderation as the
external safety guardrail in real-world products. Existing moderators mainly
practice a conventional full detection, which determines the harmfulness based
on the complete LLM output, causing high service latency. Recent works pay more
attention to partial detection where moderators oversee the generation midway
and early stop the output if harmfulness is detected, but they directly apply
moderators trained with the full detection paradigm to incomplete outputs,
introducing a training-inference gap that lowers the performance. In this
paper, we explore how to form a data-and-model solution that natively supports
partial detection. For the data, we construct FineHarm, a dataset consisting of
29K prompt-response pairs with fine-grained annotations to provide reasonable
supervision for token-level training. Then, we propose the streaming content
monitor, which is trained with dual supervision of response- and token-level
labels and can follow the output stream of LLM to make a timely judgment of
harmfulness. Experiments show that SCM gains 0.95+ in macro F1 score that is
comparable to full detection, by only seeing the first 18% of tokens in
responses on average. Moreover, the SCM can serve as a pseudo-harmfulness
annotator for improving safety alignment and lead to a higher harmlessness
score than DPO.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [79] [ReStNet: A Reusable & Stitchable Network for Dynamic Adaptation on IoT Devices](https://arxiv.org/abs/2506.09066)
*Maoyu Wang,Yao Lu,Jiaqi Nie,Zeyu Wang,Yun Lin,Qi Xuan,Guan Gui*

Main category: cs.CV

TL;DR: ReStNet dynamically stitches pre-trained models to create flexible hybrid networks suitable for diverse IoT devices.


<details>
  <summary>Details</summary>
Motivation: The paper addresses inefficiencies in deploying deep learning models across heterogeneous IoT platforms with varying resource constraints.

Method: ReStNet calculates layer-wise similarity using Centered Kernel Alignment (CKA) to identify stitching points, constructs hybrid models combining layers from different pre-trained networks, and fine-tunes only stitching layers for rapid adaptation.

Result: Experiments showcase ReStNet's ability to offer accuracy-efficiency trade-offs and lower training costs across benchmarks, supporting various model types.

Conclusion: ReStNet effectively leverages available resources, adapts to dynamic budgets, and supports flexible model combinations for practical IoT deployments.

Abstract: With the rapid development of deep learning, a growing number of pre-trained
models have been publicly available. However, deploying these fixed models in
real-world IoT applications is challenging because different devices possess
heterogeneous computational and memory resources, making it impossible to
deploy a single model across all platforms. Although traditional compression
methods, such as pruning, quantization, and knowledge distillation, can improve
efficiency, they become inflexible once applied and cannot adapt to changing
resource constraints. To address these issues, we propose ReStNet, a Reusable
and Stitchable Network that dynamically constructs a hybrid network by
stitching two pre-trained models together. Implementing ReStNet requires
addressing several key challenges, including how to select the optimal
stitching points, determine the stitching order of the two pre-trained models,
and choose an effective fine-tuning strategy. To systematically address these
challenges and adapt to varying resource constraints, ReStNet determines the
stitching point by calculating layer-wise similarity via Centered Kernel
Alignment (CKA). It then constructs the hybrid model by retaining early layers
from a larger-capacity model and appending deeper layers from a smaller one. To
facilitate efficient deployment, only the stitching layer is fine-tuned. This
design enables rapid adaptation to changing budgets while fully leveraging
available resources. Moreover, ReStNet supports both homogeneous (CNN-CNN,
Transformer-Transformer) and heterogeneous (CNN-Transformer) stitching,
allowing to combine different model families flexibly. Extensive experiments on
multiple benchmarks demonstrate that ReStNet achieve flexible
accuracy-efficiency trade-offs at runtime while significantly reducing training
cost.

</details>


### [80] [Enhancing the Safety of Medical Vision-Language Models by Synthetic Demonstrations](https://arxiv.org/abs/2506.09067)
*Zhiyu Xue,Reza Abbasi-Asl,Ramtin Pedarsani*

Main category: cs.CV

TL;DR: The paper introduces a strategy to safeguard Generative Medical Vision-Language Models (Med-VLMs) against harmful queries while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: To address the security vulnerabilities in Med-VLMs, which should reject harmful queries without overly compromising their ability to respond to benign clinical queries.

Method: The paper proposes an inference-time defense strategy using synthetic clinical demonstrations to defend against visual and textual jailbreak attacks.

Result: The experimental results show that their method enhances the security of Med-VLMs without significantly affecting their overall performance. A mixed demonstration strategy is also introduced to balance security and performance under limited resources.

Conclusion: The proposed approach effectively mitigates the risk of harmful queries in Med-VLMs and offers a well-balanced trade-off between safety and functionality.

Abstract: Generative medical vision-language models~(Med-VLMs) are primarily designed
to generate complex textual information~(e.g., diagnostic reports) from
multimodal inputs including vision modality~(e.g., medical images) and language
modality~(e.g., clinical queries). However, their security vulnerabilities
remain underexplored. Med-VLMs should be capable of rejecting harmful queries,
such as \textit{Provide detailed instructions for using this CT scan for
insurance fraud}. At the same time, addressing security concerns introduces the
risk of over-defense, where safety-enhancing mechanisms may degrade general
performance, causing Med-VLMs to reject benign clinical queries. In this paper,
we propose a novel inference-time defense strategy to mitigate harmful queries,
enabling defense against visual and textual jailbreak attacks. Using diverse
medical imaging datasets collected from nine modalities, we demonstrate that
our defense strategy based on synthetic clinical demonstrations enhances model
safety without significantly compromising performance. Additionally, we find
that increasing the demonstration budget alleviates the over-defense issue. We
then introduce a mixed demonstration strategy as a trade-off solution for
balancing security and performance under few-shot demonstration budget
constraints.

</details>


### [81] [BG-HOP: A Bimanual Generative Hand-Object Prior](https://arxiv.org/abs/2506.09068)
*Sriram Krishna,Sravan Chittupalli,Sungjae Park*

Main category: cs.CV

TL;DR: BG-HOP is a generative model to simulate bimanual hand-object interactions in 3D, leveraging single-hand priors.


<details>
  <summary>Details</summary>
Motivation: To address the scarcity of bimanual interaction data and develop a method to model these interactions effectively.

Method: Extend existing single-hand generative priors to jointly handle bimanual hand-object interactions.

Result: Preliminary results show the model can generate realistic bimanual interactions and synthesize grasps for objects.

Conclusion: BG-HOP offers a promising solution for simulating 3D bimanual hand-object interactions and makes their implementation open source.

Abstract: In this work, we present BG-HOP, a generative prior that seeks to model
bimanual hand-object interactions in 3D. We address the challenge of limited
bimanual interaction data by extending existing single-hand generative priors,
demonstrating preliminary results in capturing the joint distribution of hands
and objects. Our experiments showcase the model's capability to generate
bimanual interactions and synthesize grasps for given objects. We make code and
models publicly available.

</details>


### [82] [Segment Any Architectural Facades (SAAF):An automatic segmentation model for building facades, walls and windows based on multimodal semantics guidance](https://arxiv.org/abs/2506.09071)
*Peilin Li,Jun Yin,Jing Zhong,Ran Luo,Pengyu Zeng,Miao Zhang*

Main category: cs.CV

TL;DR: This paper introduces the SAAF model, a multimodal semantic-guided approach for automatically segmenting building facade walls and windows, achieving high accuracy and generalizability on various datasets.


<details>
  <summary>Details</summary>
Motivation: To enhance the efficiency of building information models and computer-aided design by addressing the challenge of automatically segmenting walls and windows in architectural facades.

Method: The study proposes SAAF, a model that combines multimodal semantic collaborative feature extraction with natural language processing to integrate text descriptions and image features. It uses an end-to-end training framework to autonomously map text descriptions to image segmentation.

Result: Extensive experiments show that SAAF outperforms existing methods in terms of the mIoU metric, demonstrating better precision and generalization across diverse facade datasets.

Conclusion: The SAAF model improves accuracy and generalization in wall and window segmentation, offering advancements in architectural computer vision and multimodal learning applications.

Abstract: In the context of the digital development of architecture, the automatic
segmentation of walls and windows is a key step in improving the efficiency of
building information models and computer-aided design. This study proposes an
automatic segmentation model for building facade walls and windows based on
multimodal semantic guidance, called Segment Any Architectural Facades (SAAF).
First, SAAF has a multimodal semantic collaborative feature extraction
mechanism. By combining natural language processing technology, it can fuse the
semantic information in text descriptions with image features, enhancing the
semantic understanding of building facade components. Second, we developed an
end-to-end training framework that enables the model to autonomously learn the
mapping relationship from text descriptions to image segmentation, reducing the
influence of manual intervention on the segmentation results and improving the
automation and robustness of the model. Finally, we conducted extensive
experiments on multiple facade datasets. The segmentation results of SAAF
outperformed existing methods in the mIoU metric, indicating that the SAAF
model can maintain high-precision segmentation ability when faced with diverse
datasets. Our model has made certain progress in improving the accuracy and
generalization ability of the wall and window segmentation task. It is expected
to provide a reference for the development of architectural computer vision
technology and also explore new ideas and technical paths for the application
of multimodal learning in the architectural field.

</details>


### [83] [VersaVid-R1: A Versatile Video Understanding and Reasoning Model from Question Answering to Captioning Tasks](https://arxiv.org/abs/2506.09079)
*Xinlong Chen,Yuanxing Zhang,Yushuo Guan,Bohan Zeng,Yang Shi,Sihan Yang,Pengfei Wan,Qiang Liu,Liang Wang,Tieniu Tan*

Main category: cs.CV

TL;DR: The paper introduces two datasets, DarkEventInfer and MixVidQA, to address video-based reasoning challenges and presents a new model, VersaVid-R1, achieving state-of-the-art performance in video reasoning and captioning tasks.


<details>
  <summary>Details</summary>
Motivation: Existing advancements in multimodal large language models have focused on image-based reasoning, but video-based reasoning lags behind due to a lack of effective datasets and methodologies.

Method: The paper proposes two datasets: DarkEventInfer (inference with masked event segments) and MixVidQA (reasoning in interleaved video sequences). It also develops VersaVid-R1, trained with reinforcement learning and diverse reward functions, under the Reason-Then-Respond paradigm.

Result: VersaVid-R1 demonstrates state-of-the-art performance in multiple tasks, including video understanding, cognitive reasoning, and captioning, across various benchmarks.

Conclusion: The proposed approach successfully advances video reasoning by introducing novel datasets and a versatile model, significantly outperforming prior methods across key benchmarks.

Abstract: Recent advancements in multimodal large language models have successfully
extended the Reason-Then-Respond paradigm to image-based reasoning, yet
video-based reasoning remains an underdeveloped frontier, primarily due to the
scarcity of high-quality reasoning-oriented data and effective training
methodologies. To bridge this gap, we introduce DarkEventInfer and MixVidQA,
two novel datasets specifically designed to stimulate the model's advanced
video understanding and reasoning abilities. DarkEventinfer presents videos
with masked event segments, requiring models to infer the obscured content
based on contextual video cues. MixVidQA, on the other hand, presents
interleaved video sequences composed of two distinct clips, challenging models
to isolate and reason about one while disregarding the other. Leveraging these
carefully curated training samples together with reinforcement learning guided
by diverse reward functions, we develop VersaVid-R1, the first versatile video
understanding and reasoning model under the Reason-Then-Respond paradigm
capable of handling multiple-choice and open-ended question answering, as well
as video captioning tasks. Extensive experiments demonstrate that VersaVid-R1
significantly outperforms existing models across a broad spectrum of
benchmarks, covering video general understanding, cognitive reasoning, and
captioning tasks.

</details>


### [84] [FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model Evaluation](https://arxiv.org/abs/2506.09081)
*Zheqi He,Yesheng Liu,Jing-shu Zheng,Xuejing Li,Richeng Xuan,Jin-Ge Yao,Xi Yang*

Main category: cs.CV

TL;DR: FlagEvalMM is an open-source framework for evaluating multimodal models across various vision-language tasks, with enhanced efficiency and flexibility.


<details>
  <summary>Details</summary>
Motivation: To address the need for a comprehensive, flexible, and efficient evaluation tool for multimodal models in vision-language tasks.

Method: FlagEvalMM decouples model inference from evaluation through an independent service, utilizes inference acceleration tools, and employs asynchronous data loading to enhance evaluation efficiency.

Result: The framework demonstrates accurate and efficient insights into model performance, as validated by extensive experiments.

Conclusion: FlagEvalMM is a valuable, publicly accessible tool that can help advance research in the field of multimodal AI.

Abstract: We present FlagEvalMM, an open-source evaluation framework designed to
comprehensively assess multimodal models across a diverse range of
vision-language understanding and generation tasks, such as visual question
answering, text-to-image/video generation, and image-text retrieval. We
decouple model inference from evaluation through an independent evaluation
service, thus enabling flexible resource allocation and seamless integration of
new tasks and models. Moreover, FlagEvalMM utilizes advanced inference
acceleration tools (e.g., vLLM, SGLang) and asynchronous data loading to
significantly enhance evaluation efficiency. Extensive experiments show that
FlagEvalMM offers accurate and efficient insights into model strengths and
limitations, making it a valuable tool for advancing multimodal research. The
framework is publicly accessible athttps://github.com/flageval-baai/FlagEvalMM.

</details>


### [85] [AVA-Bench: Atomic Visual Ability Benchmark for Vision Foundation Models](https://arxiv.org/abs/2506.09082)
*Zheda Mai,Arpita Chowdhury,Zihe Wang,Sooyoung Jeon,Lemeng Wang,Jiacheng Hou,Jihyung Kil,Wei-Lun Chao*

Main category: cs.CV

TL;DR: The paper introduces AVA-Bench, a benchmark designed to systematically evaluate Vision Foundation Models (VFMs) by isolating 14 fundamental visual abilities, enabling precise identification of model strengths and weaknesses.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation protocols for VFMs using Visual Question Answering benchmarks lack clarity due to mismatched instruction tuning data and task complexity.

Method: The authors propose AVA-Bench, a framework that disentangles 14 Atomic Visual Abilities and ensures alignment between training and testing distributions for precise performance assessment.

Result: AVA-Bench successfully identifies distinct 'ability fingerprints' of VFMs, facilitating informed model selection and efficient evaluation using smaller language models.

Conclusion: AVA-Bench enhances VFM evaluation by offering a transparent, systematic approach, laying a foundation for developing the next generation of vision models.

Abstract: The rise of vision foundation models (VFMs) calls for systematic evaluation.
A common approach pairs VFMs with large language models (LLMs) as
general-purpose heads, followed by evaluation on broad Visual Question
Answering (VQA) benchmarks. However, this protocol has two key blind spots: (i)
the instruction tuning data may not align with VQA test distributions, meaning
a wrong prediction can stem from such data mismatch rather than a VFM' visual
shortcomings; (ii) VQA benchmarks often require multiple visual abilities,
making it hard to tell whether errors stem from lacking all required abilities
or just a single critical one. To address these gaps, we introduce AVA-Bench,
the first benchmark that explicitly disentangles 14 Atomic Visual Abilities
(AVAs) -- foundational skills like localization, depth estimation, and spatial
understanding that collectively support complex visual reasoning tasks. By
decoupling AVAs and matching training and test distributions within each,
AVA-Bench pinpoints exactly where a VFM excels or falters. Applying AVA-Bench
to leading VFMs thus reveals distinctive "ability fingerprints," turning VFM
selection from educated guesswork into principled engineering. Notably, we find
that a 0.5B LLM yields similar VFM rankings as a 7B LLM while cutting GPU hours
by 8x, enabling more efficient evaluation. By offering a comprehensive and
transparent benchmark, we hope AVA-Bench lays the foundation for the next
generation of VFMs.

</details>


### [86] [BakuFlow: A Streamlining Semi-Automatic Label Generation Tool](https://arxiv.org/abs/2506.09083)
*Jerry Lin,Partick P. W. Chen*

Main category: cs.CV

TL;DR: The paper introduces BakuFlow, a semi-automatic labeling tool with innovative features aimed at reducing the workload and improving efficiency in labeling tasks for computer vision, especially for video data and dynamic datasets.


<details>
  <summary>Details</summary>
Motivation: Data labeling remains a significant challenge in computer vision due to its time-consuming and error-prone nature, especially for large-scale tasks.

Method: BakuFlow integrates tools like a live adjustable magnifier for corrections, an interactive data augmentation module, label propagation for video data, and a modified YOLOE framework for automatic labeling.

Result: BakuFlow enables streamlined, flexible, and scalable labeling of dynamic datasets, significantly accelerating labeling tasks and reducing manual workload.

Conclusion: The tool provides practical solutions for object detection and tracking, making it effective in industrial and computer vision applications, improving data annotation processes.

Abstract: Accurately labeling (or annotation) data is still a bottleneck in computer
vision, especially for large-scale tasks where manual labeling is
time-consuming and error-prone. While tools like LabelImg can handle the
labeling task, some of them still require annotators to manually label each
image. In this paper, we introduce BakuFlow, a streamlining semi-automatic
label generation tool. Key features include (1) a live adjustable magnifier for
pixel-precise manual corrections, improving user experience; (2) an interactive
data augmentation module to diversify training datasets; (3) label propagation
for rapidly copying labeled objects between consecutive frames, greatly
accelerating annotation of video data; and (4) an automatic labeling module
powered by a modified YOLOE framework. Unlike the original YOLOE, our extension
supports adding new object classes and any number of visual prompts per class
during annotation, enabling flexible and scalable labeling for dynamic,
real-world datasets. These innovations make BakuFlow especially effective for
object detection and tracking, substantially reducing labeling workload and
improving efficiency in practical computer vision and industrial scenarios.

</details>


### [87] [Bias Analysis in Unconditional Image Generative Models](https://arxiv.org/abs/2506.09106)
*Xiaofeng Zhang,Michelle Lin,Simon Lacoste-Julien,Aaron Courville,Yash Goyal*

Main category: cs.CV

TL;DR: This paper investigates how bias emerges in generative AI models and evaluates bias shifts between training and generated distributions, finding small attribute shifts and classifier sensitivity for spectrum-based attributes.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand representational harm and discriminatory outcomes from generative AI models, especially the mechanisms of bias emergence in unconditional generation.

Method: The authors trained unconditional image generative models and employed a bias evaluation framework to analyze the shifts in attributes between training data and generated images, focusing on classifier sensitivity.

Result: The experiments revealed small attribute shifts, with classifier sensitivity being significant for spectrum-based attributes rather than binary ones.

Conclusion: The findings emphasize the importance of improved labeling practices, scrutiny of evaluation frameworks, and consideration of the social complexity of attributes in bias evaluation for generative AI models.

Abstract: The widespread adoption of generative AI models has raised growing concerns
about representational harm and potential discriminatory outcomes. Yet, despite
growing literature on this topic, the mechanisms by which bias emerges -
especially in unconditional generation - remain disentangled. We define the
bias of an attribute as the difference between the probability of its presence
in the observed distribution and its expected proportion in an ideal reference
distribution. In our analysis, we train a set of unconditional image generative
models and adopt a commonly used bias evaluation framework to study bias shift
between training and generated distributions. Our experiments reveal that the
detected attribute shifts are small. We find that the attribute shifts are
sensitive to the attribute classifier used to label generated images in the
evaluation framework, particularly when its decision boundaries fall in
high-density regions. Our empirical analysis indicates that this classifier
sensitivity is often observed in attributes values that lie on a spectrum, as
opposed to exhibiting a binary nature. This highlights the need for more
representative labeling practices, understanding the shortcomings through
greater scrutiny of evaluation frameworks, and recognizing the socially complex
nature of attributes when evaluating bias.

</details>


### [88] [CAIRe: Cultural Attribution of Images by Retrieval-Augmented Evaluation](https://arxiv.org/abs/2506.09109)
*Arnav Yayavaram,Siddharth Yayavaram,Simran Khanuja,Michael Saxon,Graham Neubig*

Main category: cs.CV

TL;DR: The paper introduces CAIRe, a metric for evaluating cultural relevance in text-to-image (T2I) models, showing strong correlation with human judgment.


<details>
  <summary>Details</summary>
Motivation: To address biases in text-to-image models that impede equitable performance across cultures due to a lack of reliable measurement tools.

Method: CAIRe evaluates cultural relevance by grounding entities in images to a knowledge base and using factual information for independent cultural judgments.

Result: CAIRe outperforms baselines by 28% F1 on a culturally salient dataset and correlates strongly with human ratings, achieving Pearson's correlations of 0.56 and 0.66.

Conclusion: CAIRe is a reliable tool for assessing cultural relevance in images and aligns well with human judgments, aiding efforts to mitigate cultural biases in T2I models.

Abstract: As text-to-image models become increasingly prevalent, ensuring their
equitable performance across diverse cultural contexts is critical. Efforts to
mitigate cross-cultural biases have been hampered by trade-offs, including a
loss in performance, factual inaccuracies, or offensive outputs. Despite
widespread recognition of these challenges, an inability to reliably measure
these biases has stalled progress. To address this gap, we introduce CAIRe, a
novel evaluation metric that assesses the degree of cultural relevance of an
image, given a user-defined set of labels. Our framework grounds entities and
concepts in the image to a knowledge base and uses factual information to give
independent graded judgments for each culture label. On a manually curated
dataset of culturally salient but rare items built using language models, CAIRe
surpasses all baselines by 28% F1 points. Additionally, we construct two
datasets for culturally universal concept, one comprising of T2I-generated
outputs and another retrieved from naturally occurring data. CAIRe achieves
Pearson's correlations of 0.56 and 0.66 with human ratings on these sets, based
on a 5-point Likert scale of cultural relevance. This demonstrates its strong
alignment with human judgment across diverse image sources.

</details>


### [89] [Seedance 1.0: Exploring the Boundaries of Video Generation Models](https://arxiv.org/abs/2506.09113)
*Yu Gao,Haoyuan Guo,Tuyen Hoang,Weilin Huang,Lu Jiang,Fangyuan Kong,Huixia Li,Jiashi Li,Liang Li,Xiaojie Li,Xunsong Li,Yifu Li,Shanchuan Lin,Zhijie Lin,Jiawei Liu,Shu Liu,Xiaonan Nie,Zhiwu Qing,Yuxi Ren,Li Sun,Zhi Tian,Rui Wang,Sen Wang,Guoqiang Wei,Guohong Wu,Jie Wu,Ruiqi Xia,Fei Xiao,Xuefeng Xiao,Jiangqiao Yan,Ceyuan Yang,Jianchao Yang,Runkai Yang,Tao Yang,Yihang Yang,Zilyu Ye,Xuejiao Zeng,Yan Zeng,Heng Zhang,Yang Zhao,Xiaozheng Zheng,Peihao Zhu,Jiaxin Zou,Feilong Zuo*

Main category: cs.CV

TL;DR: Seedance 1.0 is an advanced diffusion-based video generation model addressing challenges in balancing prompt adherence, motion plausibility, and visual quality, offering significant quality and efficiency improvements.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address limitations in existing video generation models, specifically their struggles in combining high prompt accuracy, motion realism, and visual quality in generated videos.

Method: The authors propose a model integrating multi-source data curation with detailed captioning, an efficient architecture for multi-shot and multiple tasks, enhanced post-training with fine-tuning and reinforcement learning, and multi-stage distillation for significant speedups.

Result: Seedance 1.0 generates high-quality, visually coherent, and prompt-accurate 5-second, 1080p videos in 41.4 seconds on an NVIDIA L20 GPU, outperforming state-of-the-art models in speed and quality.

Conclusion: Seedance 1.0 demonstrates superior video generation performance across key dimensions like speed, quality, and narrative coherence, making significant advances in video-oriented foundational models.

Abstract: Notable breakthroughs in diffusion modeling have propelled rapid improvements
in video generation, yet current foundational model still face critical
challenges in simultaneously balancing prompt following, motion plausibility,
and visual quality. In this report, we introduce Seedance 1.0, a
high-performance and inference-efficient video foundation generation model that
integrates several core technical improvements: (i) multi-source data curation
augmented with precision and meaningful video captioning, enabling
comprehensive learning across diverse scenarios; (ii) an efficient architecture
design with proposed training paradigm, which allows for natively supporting
multi-shot generation and jointly learning of both text-to-video and
image-to-video tasks. (iii) carefully-optimized post-training approaches
leveraging fine-grained supervised fine-tuning, and video-specific RLHF with
multi-dimensional reward mechanisms for comprehensive performance improvements;
(iv) excellent model acceleration achieving ~10x inference speedup through
multi-stage distillation strategies and system-level optimizations. Seedance
1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds
(NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance
1.0 stands out with high-quality and fast video generation having superior
spatiotemporal fluidity with structural stability, precise instruction
adherence in complex multi-subject contexts, native multi-shot narrative
coherence with consistent subject representation.

</details>


### [90] [Cross-Frame Representation Alignment for Fine-Tuning Video Diffusion Models](https://arxiv.org/abs/2506.09229)
*Sungwon Hwang,Hyojin Jang,Kinam Kim,Minho Park,Jaegul choo*

Main category: cs.CV

TL;DR: This paper introduces a regularization technique, Cross-frame Representation Alignment (CREPA), for fine-tuning video diffusion models (VDMs), improving their visual fidelity and coherence across video frames.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenge of fine-tuning video diffusion models at the user level to retain specific attributes in training data while preserving cross-frame semantic coherence, a problem that remains underexplored.

Method: The method involves adapting a recent technique called Representation Alignment (REPA) for video models and introducing Cross-frame Representation Alignment (CREPA), which aligns hidden states of video frames with features from neighboring frames.

Result: CREPA significantly enhances visual fidelity and cross-frame semantic coherence when applied to large-scale video diffusion models like CogVideoX-5B and Hunyuan Video and demonstrates effectiveness across diverse datasets.

Conclusion: CREPA is an effective and broadly applicable regularization method for fine-tuning VDMs, addressing limitations of prior approaches and achieving better video consistency.

Abstract: Fine-tuning Video Diffusion Models (VDMs) at the user level to generate
videos that reflect specific attributes of training data presents notable
challenges, yet remains underexplored despite its practical importance.
Meanwhile, recent work such as Representation Alignment (REPA) has shown
promise in improving the convergence and quality of DiT-based image diffusion
models by aligning, or assimilating, its internal hidden states with external
pretrained visual features, suggesting its potential for VDM fine-tuning. In
this work, we first propose a straightforward adaptation of REPA for VDMs and
empirically show that, while effective for convergence, it is suboptimal in
preserving semantic consistency across frames. To address this limitation, we
introduce Cross-frame Representation Alignment (CREPA), a novel regularization
technique that aligns hidden states of a frame with external features from
neighboring frames. Empirical evaluations on large-scale VDMs, including
CogVideoX-5B and Hunyuan Video, demonstrate that CREPA improves both visual
fidelity and cross-frame semantic coherence when fine-tuned with
parameter-efficient methods such as LoRA. We further validate CREPA across
diverse datasets with varying attributes, confirming its broad applicability.
Project page: https://crepavideo.github.io

</details>


### [91] [PatchGuard: Adversarially Robust Anomaly Detection and Localization through Vision Transformers and Pseudo Anomalies](https://arxiv.org/abs/2506.09237)
*Mojtaba Nafez,Amirhossein Koochakian,Arad Maleki,Jafar Habibi,Mohammad Hossein Rohban*

Main category: cs.CV

TL;DR: PatchGuard is a Vision Transformer-based approach for anomaly detection and localization, robust against adversarial attacks, resulting in significantly improved performance metrics in both adversarial and non-adversarial settings.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of current anomaly detection and localization systems which are vulnerable to adversarial attacks due to reliance on normal, unlabeled samples.

Method: Integrate pseudo-anomalies and localization masks into a ViT-based architecture, employing adversarial training guided by a novel loss function to improve robustness.

Result: PatchGuard outperforms prior methods with a performance boost of 53.2% in anomaly detection and 68.5% in anomaly localization, tested across industrial and medical datasets.

Conclusion: PatchGuard proves its efficacy as a robust method against adversarial attacks while maintaining high accuracy in standard scenarios for anomaly detection and localization.

Abstract: Anomaly Detection (AD) and Anomaly Localization (AL) are crucial in fields
that demand high reliability, such as medical imaging and industrial
monitoring. However, current AD and AL approaches are often susceptible to
adversarial attacks due to limitations in training data, which typically
include only normal, unlabeled samples. This study introduces PatchGuard, an
adversarially robust AD and AL method that incorporates pseudo anomalies with
localization masks within a Vision Transformer (ViT)-based architecture to
address these vulnerabilities. We begin by examining the essential properties
of pseudo anomalies, and follow it by providing theoretical insights into the
attention mechanisms required to enhance the adversarial robustness of AD and
AL systems. We then present our approach, which leverages Foreground-Aware
Pseudo-Anomalies to overcome the deficiencies of previous anomaly-aware
methods. Our method incorporates these crafted pseudo-anomaly samples into a
ViT-based framework, with adversarial training guided by a novel loss function
designed to improve model robustness, as supported by our theoretical analysis.
Experimental results on well-established industrial and medical datasets
demonstrate that PatchGuard significantly outperforms previous methods in
adversarial settings, achieving performance gains of $53.2\%$ in AD and
$68.5\%$ in AL, while also maintaining competitive accuracy in non-adversarial
settings. The code repository is available at
https://github.com/rohban-lab/PatchGuard .

</details>


### [92] [UFM: A Simple Path towards Unified Dense Correspondence with Flow](https://arxiv.org/abs/2506.09278)
*Yuchen Zhang,Nikhil Keetha,Chenwei Lyu,Bhuvan Jhamb,Yutian Chen,Yuheng Qiu,Jay Karhade,Shreyas Jha,Yaoyu Hu,Deva Ramanan,Sebastian Scherer,Wenshan Wang*

Main category: cs.CV

TL;DR: The paper introduces Unified Flow & Matching (UFM), a transformer-based model trained to unify dense image correspondence across wide-baseline matching and optical flow estimation, achieving significant improvements in accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: The paper aims to unify methods for dense image correspondence across wide-baseline and optical flow scenarios, addressing historical separation despite their common goal of matching image content.

Method: UFM employs a simple transformer architecture that directly regresses flow vectors, simplifying training and improving performance through unified training data for pixels visible in both images.

Result: UFM outperforms existing methods by being 28% more accurate than Unimatch for optical flow and 62% less error-prone and 6.7x faster than RoMa for wide-baseline matching.

Conclusion: UFM demonstrates the capabilities of unified training to outperform specialized approaches, enabling efficient, general-purpose correspondence and paving the way for advancements in multi-modal and real-time tasks.

Abstract: Dense image correspondence is central to many applications, such as visual
odometry, 3D reconstruction, object association, and re-identification.
Historically, dense correspondence has been tackled separately for
wide-baseline scenarios and optical flow estimation, despite the common goal of
matching content between two images. In this paper, we develop a Unified Flow &
Matching model (UFM), which is trained on unified data for pixels that are
co-visible in both source and target images. UFM uses a simple, generic
transformer architecture that directly regresses the (u,v) flow. It is easier
to train and more accurate for large flows compared to the typical
coarse-to-fine cost volumes in prior work. UFM is 28% more accurate than
state-of-the-art flow methods (Unimatch), while also having 62% less error and
6.7x faster than dense wide-baseline matchers (RoMa). UFM is the first to
demonstrate that unified training can outperform specialized approaches across
both domains. This result enables fast, general-purpose correspondence and
opens new directions for multi-modal, long-range, and real-time correspondence
tasks.

</details>


### [93] [Lightweight Object Detection Using Quantized YOLOv4-Tiny for Emergency Response in Aerial Imagery](https://arxiv.org/abs/2506.09299)
*Sindhu Boddu,Arindam Mukherjee*

Main category: cs.CV

TL;DR: The paper introduces a lightweight, energy-efficient object detection solution using quantized YOLOv4-Tiny for aerial emergency scenarios.


<details>
  <summary>Details</summary>
Motivation: Address the lack of specialized drone-view emergency imagery datasets and create an efficient detection model for low-power emergency response.

Method: Deploy YOLOv4-Tiny, optimize with INT8 quantization, and train on a custom-curated emergency dataset with 10,820 images.

Result: Quantized YOLOv4-Tiny achieved comparable detection performance, reduced model size by 71%, and increased inference speed by 44%.

Conclusion: Quantized YOLOv4-Tiny is effective for real-time emergency detection on low-power edge devices, supported by the newly created drone emergency dataset.

Abstract: This paper presents a lightweight and energy-efficient object detection
solution for aerial imagery captured during emergency response situations. We
focus on deploying the YOLOv4-Tiny model, a compact convolutional neural
network, optimized through post-training quantization to INT8 precision. The
model is trained on a custom-curated aerial emergency dataset, consisting of
10,820 annotated images covering critical emergency scenarios. Unlike prior
works that rely on publicly available datasets, we created this dataset
ourselves due to the lack of publicly available drone-view emergency imagery,
making the dataset itself a key contribution of this work. The quantized model
is evaluated against YOLOv5-small across multiple metrics, including mean
Average Precision (mAP), F1 score, inference time, and model size. Experimental
results demonstrate that the quantized YOLOv4-Tiny achieves comparable
detection performance while reducing the model size from 22.5 MB to 6.4 MB and
improving inference speed by 44\%. With a 71\% reduction in model size and a
44\% increase in inference speed, the quantized YOLOv4-Tiny model proves highly
suitable for real-time emergency detection on low-power edge devices.

</details>


### [94] [Efficient Edge Deployment of Quantized YOLOv4-Tiny for Aerial Emergency Object Detection on Raspberry Pi 5](https://arxiv.org/abs/2506.09300)
*Sindhu Boddu,Arindam Mukherjee*

Main category: cs.CV

TL;DR: This study evaluates a quantized YOLOv4-Tiny model for real-time object detection on the Raspberry Pi 5.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of real-time object detection in aerial emergency imagery using resource-constrained edge devices.

Method: The researchers quantized the YOLOv4-Tiny model to INT8 using TensorFlow Lite post-training techniques and deployed it on Raspberry Pi 5 for performance evaluation.

Result: The model demonstrated 28.2 ms inference time per image, consumed 13.85 W power, maintained robust detection accuracy, and outperformed the FP32 counterpart in energy efficiency.

Conclusion: Low-power embedded AI systems are promising for real-time deployment in emergency response applications, combining efficiency and accuracy.

Abstract: This paper presents the deployment and performance evaluation of a quantized
YOLOv4-Tiny model for real-time object detection in aerial emergency imagery on
a resource-constrained edge device the Raspberry Pi 5. The YOLOv4-Tiny model
was quantized to INT8 precision using TensorFlow Lite post-training
quantization techniques and evaluated for detection speed, power consumption,
and thermal feasibility under embedded deployment conditions. The quantized
model achieved an inference time of 28.2 ms per image with an average power
consumption of 13.85 W, demonstrating a significant reduction in power usage
compared to its FP32 counterpart. Detection accuracy remained robust across key
emergency classes such as Ambulance, Police, Fire Engine, and Car Crash. These
results highlight the potential of low-power embedded AI systems for real-time
deployment in safety-critical emergency response applications.

</details>


### [95] [MSSDF: Modality-Shared Self-supervised Distillation for High-Resolution Multi-modal Remote Sensing Image Learning](https://arxiv.org/abs/2506.09327)
*Tong Wang,Guanzhou Chen,Xiaodong Zhang,Chenxi Liu,Jiaqi Wang,Xiaoliang Tan,Wenchao Guo,Qingyuan Yang,Kaiqi Zhang*

Main category: cs.CV

TL;DR: The paper introduces a multi-modal self-supervised learning framework to tackle the challenges in remote sensing image interpretation by utilizing RGB images, multi-spectral data, and DSMs. It showed superior performance across multiple remote sensing tasks compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work is the high cost and time required to acquire high-quality labeled remote sensing data. The study aims to address this limitation by creating a framework that uses unlabeled data for effective pretraining through multi-modal and self-supervised strategies.

Method: The framework integrates an information-aware adaptive masking strategy, a cross-modal masking mechanism, and multi-task self-supervised objectives to leverage correlations across modalities (RGB, multi-spectral, DSM) and unique features within each modality.

Result: The method was tested on 15 datasets across 26 tasks, achieving superior performance on various benchmarks. Notable results include high mIoU scores on Potsdam and Vaihingen semantic segmentation tasks and improved RMSE errors on the US3D depth estimation task.

Conclusion: The multi-modal self-supervised framework demonstrates its effectiveness and versatility for remote sensing applications, outperforming traditional pretraining methods in most scenarios while providing a more efficient approach to utilizing unlabeled data.

Abstract: Remote sensing image interpretation plays a critical role in environmental
monitoring, urban planning, and disaster assessment. However, acquiring
high-quality labeled data is often costly and time-consuming. To address this
challenge, we proposes a multi-modal self-supervised learning framework that
leverages high-resolution RGB images, multi-spectral data, and digital surface
models (DSM) for pre-training. By designing an information-aware adaptive
masking strategy, cross-modal masking mechanism, and multi-task self-supervised
objectives, the framework effectively captures both the correlations across
different modalities and the unique feature structures within each modality. We
evaluated the proposed method on multiple downstream tasks, covering typical
remote sensing applications such as scene classification, semantic
segmentation, change detection, object detection, and depth estimation.
Experiments are conducted on 15 remote sensing datasets, encompassing 26 tasks.
The results demonstrate that the proposed method outperforms existing
pretraining approaches in most tasks. Specifically, on the Potsdam and
Vaihingen semantic segmentation tasks, our method achieved mIoU scores of
78.30\% and 76.50\%, with only 50\% train-set. For the US3D depth estimation
task, the RMSE error is reduced to 0.182, and for the binary change detection
task in SECOND dataset, our method achieved mIoU scores of 47.51\%, surpassing
the second CS-MAE by 3 percentage points. Our pretrain code, checkpoints, and
HR-Pairs dataset can be found in https://github.com/CVEO/MSSDF.

</details>


### [96] [CheckManual: A New Challenge and Benchmark for Manual-based Appliance Manipulation](https://arxiv.org/abs/2506.09343)
*Yuxing Long,Jiyao Zhang,Mingjie Pan,Tianshu Wu,Taewhan Kim,Hao Dong*

Main category: cs.CV

TL;DR: The paper introduces CheckManual, the first benchmark for manual-based appliance manipulation, and proposes ManualPlan as a baseline model.


<details>
  <summary>Details</summary>
Motivation: Enable robots to effectively understand and utilize appliance manuals for accurate manipulation tasks, addressing the underutilization of manuals in robotics.

Method: Design a data generation pipeline to create appliance manuals and establish a benchmark with challenges, metrics, and environments for evaluating manipulation models.

Result: Introduced CheckManual benchmark, novel evaluation methods, and ManualPlan model as a baseline.

Conclusion: The work demonstrates the importance of manual-based understanding in robotics and provides a foundational benchmark for further research.

Abstract: Correct use of electrical appliances has significantly improved human life
quality. Unlike simple tools that can be manipulated with common sense,
different parts of electrical appliances have specific functions defined by
manufacturers. If we want the robot to heat bread by microwave, we should
enable them to review the microwave manual first. From the manual, it can learn
about component functions, interaction methods, and representative task steps
about appliances. However, previous manual-related works remain limited to
question-answering tasks while existing manipulation researchers ignore the
manual's important role and fail to comprehend multi-page manuals. In this
paper, we propose the first manual-based appliance manipulation benchmark
CheckManual. Specifically, we design a large model-assisted human-revised data
generation pipeline to create manuals based on CAD appliance models. With these
manuals, we establish novel manual-based manipulation challenges, metrics, and
simulator environments for model performance evaluation. Furthermore, we
propose the first manual-based manipulation planning model ManualPlan to set up
a group of baselines for the CheckManual benchmark.

</details>


### [97] [An Effective End-to-End Solution for Multimodal Action Recognition](https://arxiv.org/abs/2506.09345)
*Songping Wang,Xiantao Hu,Yueming Lyu,Caifeng Shan*

Main category: cs.CV

TL;DR: The paper addresses tri-modal action recognition challenges by improving data augmentation, using transfer learning, and combining advanced multimodal feature extraction. Their methods achieve state-of-the-art accuracy.


<details>
  <summary>Details</summary>
Motivation: To tackle challenges in tri-modal action recognition due to limited data and enhance the utilization of multimodal information.

Method: Used optimized data augmentation, pre-trained backbones with transfer learning, extracted multimodal spatial-temporal features using 2D CNNs and TSM, and applied prediction enhancement techniques like SWA, Ensemble, and TTA.

Result: Achieved Top-1 accuracy of 99% and Top-5 accuracy of 100% on the competition leaderboard.

Conclusion: The proposed comprehensive solution effectively leverages multimodal information and advanced learning techniques to outperform other approaches in tri-modal action recognition.

Abstract: Recently, multimodal tasks have strongly advanced the field of action
recognition with their rich multimodal information. However, due to the
scarcity of tri-modal data, research on tri-modal action recognition tasks
faces many challenges. To this end, we have proposed a comprehensive multimodal
action recognition solution that effectively utilizes multimodal information.
First, the existing data are transformed and expanded by optimizing data
enhancement techniques to enlarge the training scale. At the same time, more
RGB datasets are used to pre-train the backbone network, which is better
adapted to the new task by means of transfer learning. Secondly, multimodal
spatial features are extracted with the help of 2D CNNs and combined with the
Temporal Shift Module (TSM) to achieve multimodal spatial-temporal feature
extraction comparable to 3D CNNs and improve the computational efficiency. In
addition, common prediction enhancement methods, such as Stochastic Weight
Averaging (SWA), Ensemble and Test-Time augmentation (TTA), are used to
integrate the knowledge of models from different training periods of the same
architecture and different architectures, so as to predict the actions from
different perspectives and fully exploit the target information. Ultimately, we
achieved the Top-1 accuracy of 99% and the Top-5 accuracy of 100% on the
competition leaderboard, demonstrating the superiority of our solution.

</details>


### [98] [Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation](https://arxiv.org/abs/2506.09350)
*Shanchuan Lin,Ceyuan Yang,Hao He,Jianwen Jiang,Yuxi Ren,Xin Xia,Yang Zhao,Xuefeng Xiao,Lu Jiang*

Main category: cs.CV

TL;DR: The paper introduces an autoregressive adversarial post-training (AAPT) method to convert a latent video diffusion model into a real-time video generator, addressing computational challenges of prior models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the computational intensity of existing large-scale video generation models, which limits their real-time and interactive application potential.

Method: The method involves using autoregressive adversarial training to enable single neural function evaluations for each latent frame and allows real-time streaming and interactive control during generation.

Result: The 8B model achieves real-time 24fps video streaming generation at resolutions of 736x416 on a single H100 or 1280x720 on 8 H100 GPUs, achieving up to 1440 frames (a minute-long video).

Conclusion: The proposed approach transforms video generation into a real-time, interactive process while reducing error accumulation and maintaining computational efficiency.

Abstract: Existing large-scale video generation models are computationally intensive,
preventing adoption in real-time and interactive applications. In this work, we
propose autoregressive adversarial post-training (AAPT) to transform a
pre-trained latent video diffusion model into a real-time, interactive video
generator. Our model autoregressively generates a latent frame at a time using
a single neural function evaluation (1NFE). The model can stream the result to
the user in real time and receive interactive responses as controls to generate
the next latent frame. Unlike existing approaches, our method explores
adversarial training as an effective paradigm for autoregressive generation.
This not only allows us to design an architecture that is more efficient for
one-step generation while fully utilizing the KV cache, but also enables
training the model in a student-forcing manner that proves to be effective in
reducing error accumulation during long video generation. Our experiments
demonstrate that our 8B model achieves real-time, 24fps, streaming video
generation at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to
a minute long (1440 frames). Visit our research website at
https://seaweed-apt.com/2

</details>


### [99] [A new approach for image segmentation based on diffeomorphic registration and gradient fields](https://arxiv.org/abs/2506.09357)
*Junchao Zhou*

Main category: cs.CV

TL;DR: This paper introduces a novel variational framework for 2D image segmentation based on diffeomorphic transformations and shape analysis, avoiding the need for extensive training data.


<details>
  <summary>Details</summary>
Motivation: To develop an image segmentation approach that combines theoretical robustness with practical flexibility, without requiring abundant labeled training data like deep learning methods.

Method: The proposed method uses the LDDMM framework to model segmentation as a template curve deformation through diffeomorphic transformation, employing a loss function with varifold shape representation, and is GPU-accelerated via PyKeops.

Result: The approach provides accurate segmentation results while being theoretically grounded and avoiding dependency on large datasets.

Conclusion: The novel framework enables efficient, accurate, and data-efficient segmentation through a combination of variational principles and shape geometry.

Abstract: Image segmentation is a fundamental task in computer vision aimed at
delineating object boundaries within images. Traditional approaches, such as
edge detection and variational methods, have been widely explored, while recent
advances in deep learning have shown promising results but often require
extensive training data. In this work, we propose a novel variational framework
for 2D image segmentation that integrates concepts from shape analysis and
diffeomorphic transformations. Our method models segmentation as the
deformation of a template curve via a diffeomorphic transformation of the image
domain, using the Large Deformation Diffeomorphic Metric Mapping (LDDMM)
framework. The curve evolution is guided by a loss function that compares the
deformed curve to the image gradient field, formulated through the varifold
representation of geometric shapes. The approach is implemented in Python with
GPU acceleration using the PyKeops library. This framework allows for accurate
segmentation with a flexible and theoretically grounded methodology that does
not rely on large datasets.

</details>


### [100] [SAGE: Exploring the Boundaries of Unsafe Concept Domain with Semantic-Augment Erasing](https://arxiv.org/abs/2506.09363)
*Hongguang Zhu,Yunchao Wei,Mengyu Wang,Siyu Jiao,Yan Fang,Jiannan Huang,Yao Zhao*

Main category: cs.CV

TL;DR: The paper introduces SAGE, a method that improves safe text-to-image generation by addressing challenges in erasing unsafe concepts from diffusion models.


<details>
  <summary>Details</summary>
Motivation: Improve the safety and compliance of diffusion models by removing sensitive or unsafe concepts during generation.

Method: Semantic-augment erasing with cyclic self-check/self-erasure, along with a global-local retention mechanism to balance concept erasure and irrelevant data preservation without additional preprocessing.

Result: The proposed SAGE method demonstrated superior effectiveness in safe content generation compared to existing methods, as validated through extensive experiments.

Conclusion: SAGE provides an innovative approach to safely and efficiently erase sensitive information from diffusion models while retaining the accuracy and irrelevance of unaffected concepts.

Abstract: Diffusion models (DMs) have achieved significant progress in text-to-image
generation. However, the inevitable inclusion of sensitive information during
pre-training poses safety risks, such as unsafe content generation and
copyright infringement. Concept erasing finetunes weights to unlearn
undesirable concepts, and has emerged as a promising solution. However,
existing methods treat unsafe concept as a fixed word and repeatedly erase it,
trapping DMs in ``word concept abyss'', which prevents generalized
concept-related erasing. To escape this abyss, we introduce semantic-augment
erasing which transforms concept word erasure into concept domain erasure by
the cyclic self-check and self-erasure. It efficiently explores and unlearns
the boundary representation of concept domain through semantic spatial
relationships between original and training DMs, without requiring additional
preprocessed data. Meanwhile, to mitigate the retention degradation of
irrelevant concepts while erasing unsafe concepts, we further propose the
global-local collaborative retention mechanism that combines global semantic
relationship alignment with local predicted noise preservation, effectively
expanding the retentive receptive field for irrelevant concepts. We name our
method SAGE, and extensive experiments demonstrate the comprehensive
superiority of SAGE compared with other methods in the safe generation of DMs.
The code and weights will be open-sourced at
https://github.com/KevinLight831/SAGE.

</details>


### [101] [ScaleLSD: Scalable Deep Line Segment Detection Streamlined](https://arxiv.org/abs/2506.09369)
*Zeran Ke,Bin Tan,Xianwei Zheng,Yujun Shen,Tianfu Wu,Nan Xue*

Main category: cs.CV

TL;DR: This paper introduces ScaleLSD, a high-performing, scalable self-supervised learning framework for Line Segment Detection (LSD) in images, evaluated over zero-shot tasks with excellent results.


<details>
  <summary>Details</summary>
Motivation: The authors aim to develop a robust LSD model that works for various natural images and outperforms existing deep and non-deep LSD methods in line geometry characterization.

Method: The method involves scalable self-supervised learning, revisiting conventional LSD designs to create ScaleLSD, capable of analyzing over 10M unlabeled real-world images by detecting a higher number of line segments accurately.

Result: ScaleLSD achieves superior results compared to existing methods, including both deep and non-deep approaches, across tasks like geometry estimation, line segment matching, and 3D line mapping.

Conclusion: ScaleLSD is the first deep approach to comprehensively outperform non-deep LSD methods, demonstrating its versatility and scalability for robust image geometry analysis.

Abstract: This paper studies the problem of Line Segment Detection (LSD) for the
characterization of line geometry in images, with the aim of learning a
domain-agnostic robust LSD model that works well for any natural images. With
the focus of scalable self-supervised learning of LSD, we revisit and
streamline the fundamental designs of (deep and non-deep) LSD approaches to
have a high-performing and efficient LSD learner, dubbed as ScaleLSD, for the
curation of line geometry at scale from over 10M unlabeled real-world images.
Our ScaleLSD works very well to detect much more number of line segments from
any natural images even than the pioneered non-deep LSD approach, having a more
complete and accurate geometric characterization of images using line segments.
Experimentally, our proposed ScaleLSD is comprehensively testified under
zero-shot protocols in detection performance, single-view 3D geometry
estimation, two-view line segment matching, and multiview 3D line mapping, all
with excellent performance obtained. Based on the thorough evaluation, our
ScaleLSD is observed to be the first deep approach that outperforms the
pioneered non-deep LSD in all aspects we have tested, significantly expanding
and reinforcing the versatility of the line geometry of images. Code and Models
are available at https://github.com/ant-research/scalelsd

</details>


### [102] [UniForward: Unified 3D Scene and Semantic Field Reconstruction via Feed-Forward Gaussian Splatting from Only Sparse-View Images](https://arxiv.org/abs/2506.09378)
*Qijian Tian,Xin Tan,Jingyu Gong,Yuan Xie,Lizhuang Ma*

Main category: cs.CV

TL;DR: The paper presents UniForward, a feed-forward Gaussian Splatting model for real-time 3D scene and semantic field reconstruction from uncalibrated sparse-view images.


<details>
  <summary>Details</summary>
Motivation: To unify 3D scene and semantic field reconstruction for improved environmental understanding, while addressing challenges like embedding semantics into 3D representations and achieving generalizability and real-time performance without relying on camera parameters or ground truth depth.

Method: A feed-forward model, UniForward, predicts 3D Gaussians with anisotropic semantic features using a dual-branch decoupled decoder. It uses a novel loss-guided view sampler for training without the need for ground truth depth or masks. The model trains end-to-end using photometric and distillation losses.

Result: UniForward reconstructs high-quality 3D scenes and semantic fields in real-time, enabling view-consistent rendering and open-vocabulary segmentation masks. It achieves state-of-the-art performance in novel view synthesis and segmentation tasks.

Conclusion: The paper introduces a unified approach for 3D scene and semantic field reconstruction, achieving efficient, high-quality outputs from sparse-view images without requiring additional input like ground truth depth or camera calibration.

Abstract: We propose a feed-forward Gaussian Splatting model that unifies 3D scene and
semantic field reconstruction. Combining 3D scenes with semantic fields
facilitates the perception and understanding of the surrounding environment.
However, key challenges include embedding semantics into 3D representations,
achieving generalizable real-time reconstruction, and ensuring practical
applicability by using only images as input without camera parameters or ground
truth depth. To this end, we propose UniForward, a feed-forward model to
predict 3D Gaussians with anisotropic semantic features from only uncalibrated
and unposed sparse-view images. To enable the unified representation of the 3D
scene and semantic field, we embed semantic features into 3D Gaussians and
predict them through a dual-branch decoupled decoder. During training, we
propose a loss-guided view sampler to sample views from easy to hard,
eliminating the need for ground truth depth or masks required by previous
methods and stabilizing the training process. The whole model can be trained
end-to-end using a photometric loss and a distillation loss that leverages
semantic features from a pre-trained 2D semantic model. At the inference stage,
our UniForward can reconstruct 3D scenes and the corresponding semantic fields
in real time from only sparse-view images. The reconstructed 3D scenes achieve
high-quality rendering, and the reconstructed 3D semantic field enables the
rendering of view-consistent semantic features from arbitrary views, which can
be further decoded into dense segmentation masks in an open-vocabulary manner.
Experiments on novel view synthesis and novel view segmentation demonstrate
that our method achieves state-of-the-art performances for unifying 3D scene
and semantic field reconstruction.

</details>


### [103] [ReID5o: Achieving Omni Multi-modal Person Re-identification in a Single Model](https://arxiv.org/abs/2506.09385)
*Jialong Zuo,Yongtai Deng,Mengdan Tan,Rui Jin,Dongyue Wu,Nong Sang,Liang Pan,Changxin Gao*

Main category: cs.CV

TL;DR: The paper introduces OM-ReID, a comprehensive multi-modal person re-identification problem, along with ORBench, a high-quality dataset spanning five modalities (e.g., RGB, infrared, sketch). The ReID5o framework is also proposed, achieving strong performance via unified encoding and multi-expert routing.


<details>
  <summary>Details</summary>
Motivation: Current person ReID methods are constrained to specific modalities and fail to accommodate varying real-world multi-modal queries. This limitation hinders progress in person identification across diverse contexts.

Method: The study constructs ORBench, a rich dataset featuring five modalities, and introduces ReID5o—a robust multi-modal framework that utilizes unified encodings and a multi-expert routing mechanism for effective query processing.

Result: Experimental analyses on ORBench validate the superiority of the ReID5o model against alternative approaches, showcasing its ability to handle diverse modality queries with high performance.

Conclusion: The OM-ReID challenge and ORBench dataset set a new benchmark for multi-modal person re-identification research. ReID5o demonstrates feasibility and advancement in solving this problem by synergizing multiple modalities effectively.

Abstract: In real-word scenarios, person re-identification (ReID) expects to identify a
person-of-interest via the descriptive query, regardless of whether the query
is a single modality or a combination of multiple modalities. However, existing
methods and datasets remain constrained to limited modalities, failing to meet
this requirement. Therefore, we investigate a new challenging problem called
Omni Multi-modal Person Re-identification (OM-ReID), which aims to achieve
effective retrieval with varying multi-modal queries. To address dataset
scarcity, we construct ORBench, the first high-quality multi-modal dataset
comprising 1,000 unique identities across five modalities: RGB, infrared, color
pencil, sketch, and textual description. This dataset also has significant
superiority in terms of diversity, such as the painting perspectives and
textual information. It could serve as an ideal platform for follow-up
investigations in OM-ReID. Moreover, we propose ReID5o, a novel multi-modal
learning framework for person ReID. It enables synergistic fusion and
cross-modal alignment of arbitrary modality combinations in a single model,
with a unified encoding and multi-expert routing mechanism proposed. Extensive
experiments verify the advancement and practicality of our ORBench. A wide
range of possible models have been evaluated and compared on it, and our
proposed ReID5o model gives the best performance. The dataset and code will be
made publicly available at https://github.com/Zplusdragon/ReID5o_ORBench.

</details>


### [104] [Improving Out-of-Distribution Detection via Dynamic Covariance Calibration](https://arxiv.org/abs/2506.09399)
*Kaiyu Guo,Zijian Wang,Brian C. Lovell,Mahsa Baktashmotlagh*

Main category: cs.CV

TL;DR: The paper introduces a method to improve Out-of-Distribution (OOD) detection by dynamically adjusting the prior geometry of data in response to new inputs.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current subspace-based OOD detection methods, which struggle with geometry distortion due to ill-distributed samples, and to improve robustness and accuracy.

Method: The proposed method dynamically updates the prior covariance matrix with real-time input features, reduces covariance along specific directions, and constrains adjustments to ensure data characteristics are preserved.

Result: The method achieves significant improvements in OOD detection on multiple pre-trained models, including CIFAR and ImageNet-1k datasets.

Conclusion: Dynamic adaptation of prior information enhances the reliability of OOD detection, making AI systems more trustworthy and effective.

Abstract: Out-of-Distribution (OOD) detection is essential for the trustworthiness of
AI systems. Methods using prior information (i.e., subspace-based methods) have
shown effective performance by extracting information geometry to detect OOD
data with a more appropriate distance metric. However, these methods fail to
address the geometry distorted by ill-distributed samples, due to the
limitation of statically extracting information geometry from the training
distribution. In this paper, we argue that the influence of ill-distributed
samples can be corrected by dynamically adjusting the prior geometry in
response to new data. Based on this insight, we propose a novel approach that
dynamically updates the prior covariance matrix using real-time input features,
refining its information. Specifically, we reduce the covariance along the
direction of real-time input features and constrain adjustments to the residual
space, thus preserving essential data characteristics and avoiding effects on
unintended directions in the principal space. We evaluate our method on two
pre-trained models for the CIFAR dataset and five pre-trained models for
ImageNet-1k, including the self-supervised DINO model. Extensive experiments
demonstrate that our approach significantly enhances OOD detection across
various models. The code is released at https://github.com/workerbcd/ooddcc.

</details>


### [105] [SRPL-SFDA: SAM-Guided Reliable Pseudo-Labels for Source-Free Domain Adaptation in Medical Image Segmentation](https://arxiv.org/abs/2506.09403)
*Xinya Liu,Jianghao Wu,Tao Lu,Shaoting Zhang,Guotai Wang*

Main category: cs.CV

TL;DR: This paper introduces a Source-Free Domain Adaptation (SFDA) method leveraging a Segment Anything Model (SAM) to improve pseudo-label quality and segmentation performance on medical images. The method is evaluated for fetal brain and prostate datasets and shows competitive performance compared to supervised methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in Source-Free Domain Adaptation (SFDA) for medical image segmentation, especially overcoming the limited supervision due to unlabeled target domains while respecting privacy concerns regarding source-domain data.

Method: The proposed SRPL-SFDA method includes three components: Test-Time Tri-branch Intensity Enhancement (T3IE) for input adjustment to refine pseudo-labels, a pseudo-label selection module using Consistency of Multiple SAM Outputs (CMSO), and a reliability-aware training process combining reliable pseudo-labels with entropy minimization on unreliable areas.

Result: Experiments on fetal brain and prostate datasets show SRPL-SFDA significantly improves pseudo-label quality, enhances SFDA performance, and outperforms state-of-the-art SFDA methods, achieving performance nearing supervised training.

Conclusion: SRPL-SFDA proves to be an effective approach for domain adaptation in medical image segmentation by utilizing SAM-guided pseudo-label reliability, with practical benefits for SFDA while maintaining privacy in source-domain data access.

Abstract: Domain Adaptation (DA) is crucial for robust deployment of medical image
segmentation models when applied to new clinical centers with significant
domain shifts. Source-Free Domain Adaptation (SFDA) is appealing as it can deal
with privacy concerns and access constraints on source-domain data during
adaptation to target-domain data. However, SFDA faces challenges such as
insufficient supervision in the target domain with unlabeled images. In this
work, we propose a Segment Anything Model (SAM)-guided Reliable Pseudo-Labels
method for SFDA (SRPL-SFDA) with three key components: 1) Test-Time Tri-branch
Intensity Enhancement (T3IE) that not only improves quality of raw
pseudo-labels in the target domain, but also leads to SAM-compatible inputs
with three channels to better leverage SAM's zero-shot inference ability for
refining the pseudo-labels; 2) A reliable pseudo-label selection module that
rejects low-quality pseudo-labels based on Consistency of Multiple SAM Outputs
(CMSO) under input perturbations with T3IE; and 3) A reliability-aware training
procedure in the unlabeled target domain where reliable pseudo-labels are used
for supervision and unreliable parts are regularized by entropy minimization.
Experiments conducted on two multi-domain medical image segmentation datasets
for fetal brain and the prostate respectively demonstrate that: 1) SRPL-SFDA
effectively enhances pseudo-label quality in the unlabeled target domain, and
improves SFDA performance by leveraging the reliability-aware training; 2)
SRPL-SFDA outperformed state-of-the-art SFDA methods, and its performance is
close to that of supervised training in the target domain. The code of this
work is available online: https://github.com/HiLab-git/SRPL-SFDA.

</details>


### [106] [Synthetic Human Action Video Data Generation with Pose Transfer](https://arxiv.org/abs/2506.09411)
*Vaclav Knapp,Matyas Bohacek*

Main category: cs.CV

TL;DR: The paper presents a method to generate synthetic human action video data using 3D Gaussian avatar models for video understanding tasks, addressing issues of uncanny features in synthetic data.


<details>
  <summary>Details</summary>
Motivation: Synthetic data generation often lacks realism, which limits its training effectiveness in video understanding tasks such as sign language translation and gesture recognition.

Method: A pose transfer technique involving controllable 3D Gaussian avatar models is developed to generate synthetic human action videos.

Result: The proposed method boosts action recognition performance on Toyota Smarthome and NTU RGB+D datasets and enhances few-shot dataset scalability by improving representation and diversity.

Conclusion: The method effectively addresses limitations in synthetic data, improving training in video understanding tasks, and introduces an open-source dataset, RANDOM People, to facilitate further research.

Abstract: In video understanding tasks, particularly those involving human motion,
synthetic data generation often suffers from uncanny features, diminishing its
effectiveness for training. Tasks such as sign language translation, gesture
recognition, and human motion understanding in autonomous driving have thus
been unable to exploit the full potential of synthetic data. This paper
proposes a method for generating synthetic human action video data using pose
transfer (specifically, controllable 3D Gaussian avatar models). We evaluate
this method on the Toyota Smarthome and NTU RGB+D datasets and show that it
improves performance in action recognition tasks. Moreover, we demonstrate that
the method can effectively scale few-shot datasets, making up for groups
underrepresented in the real training data and adding diverse backgrounds. We
open-source the method along with RANDOM People, a dataset with videos and
avatars of novel human identities for pose transfer crowd-sourced from the
internet.

</details>


### [107] [Noise Conditional Variational Score Distillation](https://arxiv.org/abs/2506.09416)
*Xinyu Peng,Ziyang Zheng,Yaoming Wang,Han Li,Nuowen Kan,Wenrui Dai,Chenglin Li,Junni Zou,Hongkai Xiong*

Main category: cs.CV

TL;DR: The paper introduces a novel method called Noise Conditional Variational Score Distillation (NCVSD) to efficiently distill diffusion models into generative denoisers, enabling both fast generation and iterative refinement for improved sampling.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the limitations of existing generative models by improving their efficiency and flexibility, allowing fast generation, iterative refinement, and robust performance in tasks like image generation and inverse problem solving.

Method: The paper integrates insights about unconditional score functions into the Variational Score Distillation (VSD) framework, enabling the learning of generative denoisers that approximate denoising posterior distributions across different noise levels.

Result: Experiments demonstrate that NCVSD not only outperforms teacher diffusion models but also achieves competitive performance with larger consistency models and sets new benchmarks, particularly in LPIPS for inverse problem solving.

Conclusion: NCVSD successfully combines the benefits of diffusion models and consistency models into a flexible and efficient generative framework, offering high-quality samples and scalable test-time computation.

Abstract: We propose Noise Conditional Variational Score Distillation (NCVSD), a novel
method for distilling pretrained diffusion models into generative denoisers. We
achieve this by revealing that the unconditional score function implicitly
characterizes the score function of denoising posterior distributions. By
integrating this insight into the Variational Score Distillation (VSD)
framework, we enable scalable learning of generative denoisers capable of
approximating samples from the denoising posterior distribution across a wide
range of noise levels. The proposed generative denoisers exhibit desirable
properties that allow fast generation while preserve the benefit of iterative
refinement: (1) fast one-step generation through sampling from pure Gaussian
noise at high noise levels; (2) improved sample quality by scaling the
test-time compute with multi-step sampling; and (3) zero-shot probabilistic
inference for flexible and controllable sampling. We evaluate NCVSD through
extensive experiments, including class-conditional image generation and inverse
problem solving. By scaling the test-time compute, our method outperforms
teacher diffusion models and is on par with consistency models of larger sizes.
Additionally, with significantly fewer NFEs than diffusion-based methods, we
achieve record-breaking LPIPS on inverse problems.

</details>


### [108] [ODG: Occupancy Prediction Using Dual Gaussians](https://arxiv.org/abs/2506.09417)
*Yunxiao Shi,Yinhao Zhu,Shizhong Han,Jisoo Jeong,Amin Ansari,Hong Cai,Fatih Porikli*

Main category: cs.CV

TL;DR: The paper proposes a novel 3D occupancy prediction method called ODG, which combines Bird's Eye View (BEV) and sparse point representations to address shortcomings like information loss on small objects and inefficiency in capturing flat surfaces.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 3D occupancy in autonomous driving are computationally expensive or suffer from shortcomings like BEV losing information on small objects and sparse points being inefficient for large surfaces.

Method: The paper introduces ODG, a dual-branch framework combining query-based sparse points and BEV representations, with cross-attention between branches to enrich signals.

Result: ODG achieves superior performance on Occ3D-nuScenes and Occ3D-Waymo benchmarks while maintaining competitive inference speeds compared to previous efficient methods.

Conclusion: ODG effectively addresses the challenges in 3D occupancy prediction, providing a balanced approach with improved accuracy and efficiency in autonomous driving applications.

Abstract: 3D occupancy provides fine-grained 3D geometry and semantics for scene
understanding which is critical for autonomous driving. Most existing methods,
however, carry high compute costs, requiring dense 3D feature volume and
cross-attention to effectively aggregate information. More recent works have
adopted Bird's Eye View (BEV) or sparse points as scene representation with
much reduced cost, but still suffer from their respective shortcomings. More
concretely, BEV struggles with small objects that often experience significant
information loss after being projected to the ground plane. On the other hand,
points can flexibly model little objects in 3D, but is inefficient at capturing
flat surfaces or large objects. To address these challenges, in this paper, we
present a novel 3D occupancy prediction approach, ODG, which combines BEV and
sparse points based representations. We propose a dual-branch design: a
query-based sparse points branch and a BEV branch. The 3D information learned
in the sparse points branch is shared with the BEV stream via cross-attention,
which enriches the weakened signals of difficult objects on the BEV plane. The
outputs of both branches are finally fused to generate predicted 3D occupancy.
We conduct extensive experiments on the Occ3D-nuScenes and Occ3D-Waymo
benchmarks that demonstrate the superiority of our proposed ODG. Moreover, ODG
also delivers competitive inference speed when compared to the latest efficient
approaches.

</details>


### [109] [A High-Quality Dataset and Reliable Evaluation for Interleaved Image-Text Generation](https://arxiv.org/abs/2506.09427)
*Yukang Feng,Jianwen Sun,Chuanhao Li,Zizhen Li,Jiaxin Ai,Fanrui Zhang,Yifan Chang,Sizhuo Zhou,Shenglin Zhang,Yu Dai,Kaipeng Zhang*

Main category: cs.CV

TL;DR: The paper introduces InterSyn, a large-scale multimodal dataset created using the SEIR method, to improve tightly interleaved image-text outputs in LMMs. It also presents SynJudge, an automatic evaluation model for assessing such multimodal outputs.


<details>
  <summary>Details</summary>
Motivation: Traditional LMMs struggle with generating tightly interleaved image-text outputs due to limitations in existing datasets. This paper aims to address the gap by creating a more robust dataset and evaluation tool.

Method: The authors developed the Self-Evaluation with Iterative Refinement (SEIR) method to construct the InterSyn dataset. They also created SynJudge to automatically evaluate multimodal outputs along four specific dimensions.

Result: Experimental results demonstrate that the SEIR method significantly improves dataset quality. Additionally, LMMs trained on InterSyn show uniform performance improvements across various evaluation metrics.

Conclusion: The introduction of InterSyn and SynJudge addresses major gaps in LMM training and evaluation, enabling advancements in multimodal systems capable of tightly integrated image-text outputs.

Abstract: Recent advancements in Large Multimodal Models (LMMs) have significantly
improved multimodal understanding and generation. However, these models still
struggle to generate tightly interleaved image-text outputs, primarily due to
the limited scale, quality and instructional richness of current training
datasets. To address this, we introduce InterSyn, a large-scale multimodal
dataset constructed using our Self-Evaluation with Iterative Refinement (SEIR)
method. InterSyn features multi-turn, instruction-driven dialogues with tightly
interleaved imagetext responses, providing rich object diversity and rigorous
automated quality refinement, making it well-suited for training
next-generation instruction-following LMMs. Furthermore, to address the lack of
reliable evaluation tools capable of assessing interleaved multimodal outputs,
we introduce SynJudge, an automatic evaluation model designed to quantitatively
assess multimodal outputs along four dimensions: text content, image content,
image quality, and image-text synergy.
  Experimental studies show that the SEIR method leads to substantially higher
dataset quality compared to an otherwise identical process without refinement.
  Moreover, LMMs trained on InterSyn achieve uniform performance gains across
all evaluation metrics, confirming InterSyn's utility for advancing multimodal
systems.

</details>


### [110] [A Novel Lightweight Transformer with Edge-Aware Fusion for Remote Sensing Image Captioning](https://arxiv.org/abs/2506.09429)
*Swadhin Das,Divyansh Mundra,Priyanshu Dayal,Raksha Sharma*

Main category: cs.CV

TL;DR: The paper proposes a lightweight transformer model with edge-aware enhancement to improve remote sensing image captioning quality while reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the high computational costs and the lack of attention to fine-grained structural features in remote sensing image captioning models.

Method: The method involves a lightweight transformer architecture with dimensionality reduction, a distilled GPT-2 decoder, a knowledge distillation strategy, and an edge-aware enhancement strategy.

Result: The proposed method significantly outperforms state-of-the-art methods in caption quality for remote sensing images.

Conclusion: The lightweight and edge-aware architecture effectively balances computational efficiency and high-quality caption generation for remote sensing images.

Abstract: Transformer-based models have achieved strong performance in remote sensing
image captioning by capturing long-range dependencies and contextual
information. However, their practical deployment is hindered by high
computational costs, especially in multi-modal frameworks that employ separate
transformer-based encoders and decoders. In addition, existing remote sensing
image captioning models primarily focus on high-level semantic extraction while
often overlooking fine-grained structural features such as edges, contours, and
object boundaries. To address these challenges, a lightweight transformer
architecture is proposed by reducing the dimensionality of the encoder layers
and employing a distilled version of GPT-2 as the decoder. A knowledge
distillation strategy is used to transfer knowledge from a more complex teacher
model to improve the performance of the lightweight network. Furthermore, an
edge-aware enhancement strategy is incorporated to enhance image representation
and object boundary understanding, enabling the model to capture fine-grained
spatial details in remote sensing images. Experimental results demonstrate that
the proposed approach significantly improves caption quality compared to
state-of-the-art methods.

</details>


### [111] [TOGA: Temporally Grounded Open-Ended Video QA with Weak Supervision](https://arxiv.org/abs/2506.09445)
*Ayush Gupta,Anirban Roy,Rama Chellappa,Nathaniel D. Bastian,Alvaro Velasquez,Susmit Jha*

Main category: cs.CV

TL;DR: The paper introduces TOGA, a model for generating answers and temporal grounding in video question answering without manual temporal annotations.


<details>
  <summary>Details</summary>
Motivation: Enable video question answering with temporal grounding using weak supervision, avoiding reliance on temporal annotations.

Method: TOGA uses instruct-tuning for joint answer and grounding generation, leveraging pseudo labels and consistency constraints for weak supervision.

Result: TOGA outperforms state-of-the-art methods in video QA benchmarks including NExT-GQA, MSVD-QA, and ActivityNet-QA.

Conclusion: Joint generation of answers and temporal grounding enhances performance in video QA tasks, proving effective even with weak supervision.

Abstract: We address the problem of video question answering (video QA) with temporal
grounding in a weakly supervised setup, without any temporal annotations. Given
a video and a question, we generate an open-ended answer grounded with the
start and end time. For this task, we propose TOGA: a vision-language model for
Temporally Grounded Open-Ended Video QA with Weak Supervision. We instruct-tune
TOGA to jointly generate the answer and the temporal grounding. We operate in a
weakly supervised setup where the temporal grounding annotations are not
available. We generate pseudo labels for temporal grounding and ensure the
validity of these labels by imposing a consistency constraint between the
question of a grounding response and the response generated by a question
referring to the same temporal segment. We notice that jointly generating the
answers with the grounding improves performance on question answering as well
as grounding. We evaluate TOGA on grounded QA and open-ended QA tasks. For
grounded QA, we consider the NExT-GQA benchmark which is designed to evaluate
weakly supervised grounded question answering. For open-ended QA, we consider
the MSVD-QA and ActivityNet-QA benchmarks. We achieve state-of-the-art
performance for both tasks on these benchmarks.

</details>


### [112] [Harmonizing and Merging Source Models for CLIP-based Domain Generalization](https://arxiv.org/abs/2506.09446)
*Yuhe Ding,Jian Liang,Bo Jiang,Zi Wang,Aihua Zheng,Bin Luo*

Main category: cs.CV

TL;DR: The paper introduces HAM, a model merging framework for CLIP-based domain generalization, addressing conflicts in multi-source training, and achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To address the generalization challenges posed by sample and optimization conflicts in CLIP-based domain generalization across multiple source domains.

Method: The proposed HAM framework enriches source samples while avoiding conflicting ones, harmonizes update directions during training, and employs a redundancy-aware model merging method to integrate knowledge across source models.

Result: HAM leads to a final model with superior generalization capabilities and outperforms existing methods across five widely used benchmark datasets.

Conclusion: HAM effectively mitigates conflicts inherent in multi-source domain generalization, achieving improved generalization performance and setting new state-of-the-art benchmarks.

Abstract: CLIP-based domain generalization aims to improve model generalization to
unseen domains by leveraging the powerful zero-shot classification capabilities
of CLIP and multiple source datasets. Existing methods typically train a single
model across multiple source domains to capture domain-shared information.
However, this paradigm inherently suffers from two types of conflicts: 1)
sample conflicts, arising from noisy samples and extreme domain shifts among
sources; and 2) optimization conflicts, stemming from competition and
trade-offs during multi-source training. Both hinder the generalization and
lead to suboptimal solutions. Recent studies have shown that model merging can
effectively mitigate the competition of multi-objective optimization and
improve generalization performance. Inspired by these findings, we propose
Harmonizing and Merging (HAM), a novel source model merging framework for
CLIP-based domain generalization. During the training process of the source
models, HAM enriches the source samples without conflicting samples, and
harmonizes the update directions of all models. Then, a redundancy-aware
historical model merging method is introduced to effectively integrate
knowledge across all source models. HAM comprehensively consolidates source
domain information while enabling mutual enhancement among source models,
ultimately yielding a final model with optimal generalization capabilities.
Extensive experiments on five widely used benchmark datasets demonstrate the
effectiveness of our approach, achieving state-of-the-art performance.

</details>


### [113] [Evidential Deep Learning with Spectral-Spatial Uncertainty Disentanglement for Open-Set Hyperspectral Domain Generalization](https://arxiv.org/abs/2506.09460)
*Amirreza Khoshbakht,Erchan Aptoula*

Main category: cs.CV

TL;DR: The paper proposes an open-set domain generalization framework for hyperspectral image classification, combining spectral-spatial feature extraction, uncertainty measurement, and reliable open-set decisions to handle unknown classes without target domain training.


<details>
  <summary>Details</summary>
Motivation: Current domain adaptation methods require access to target domain data during training, making them ineffective for scenarios with unknown classes and domain shifts, thereby reducing classification performance.

Method: The framework integrates four key components: Spectrum-Invariant Frequency Disentanglement (SIFD) for domain-agnostic features, Dual-Channel Residual Network (DCRN) for spectral-spatial learning, Evidential Deep Learning (EDL) for uncertainty quantification, and Spectral-Spatial Uncertainty Disentanglement (SSUD) for handling open-set classification.

Result: Experiments on three cross-scene hyperspectral tasks demonstrate that the proposed approach achieves performance comparable to leading domain adaptation methods, without requiring training data from the target domain.

Conclusion: The framework successfully addresses the challenges of open-set domain generalization and hyperspectral image classification by effectively handling unknown classes and domain shifts, and its implementation will be publicly available upon acceptance.

Abstract: Open-set domain generalization(OSDG) for hyperspectral image classification
presents significant challenges due to the presence of unknown classes in
target domains and the need for models to generalize across multiple unseen
domains without target-specific adaptation. Existing domain adaptation methods
assume access to target domain data during training and fail to address the
fundamental issue of domain shift when unknown classes are present, leading to
negative transfer and reduced classification performance. To address these
limitations, we propose a novel open-set domain generalization framework that
combines four key components: Spectrum-Invariant Frequency Disentanglement
(SIFD) for domain-agnostic feature extraction, Dual-Channel Residual Network
(DCRN) for robust spectral-spatial feature learning, Evidential Deep Learning
(EDL) for uncertainty quantification, and Spectral-Spatial Uncertainty
Disentanglement (SSUD) for reliable open-set classification. The SIFD module
extracts domain-invariant spectral features in the frequency domain through
attention-weighted frequency analysis and domain-agnostic regularization, while
DCRN captures complementary spectral and spatial information via parallel
pathways with adaptive fusion. EDL provides principled uncertainty estimation
using Dirichlet distributions, enabling the SSUD module to make reliable
open-set decisions through uncertainty-aware pathway weighting and adaptive
rejection thresholding. Experimental results on three cross-scene hyperspectral
classification tasks show that our approach achieves performance comparable to
state-of-the-art domain adaptation methods while requiring no access to the
target domain during training. The implementation will be made available at
https://github.com/amir-khb/SSUDOSDG upon acceptance.

</details>


### [114] [Optimizing Cooperative Multi-Object Tracking using Graph Signal Processing](https://arxiv.org/abs/2506.09469)
*Maria Damanaki,Nikos Piperigkos,Alexandros Gkillas,Aris S. Lalos*

Main category: cs.CV

TL;DR: The paper presents a Cooperative Multi-Object Tracking (MOT) framework to improve object tracking accuracy by fusing data from multiple vehicles using a graph topology-aware optimization approach.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the limitations of single-agent MOT caused by occlusions and sensor failures, emphasizing the need for multi-agent data integration to enhance environmental understanding in autonomous driving systems.

Method: The paper introduces a graph topology-aware optimization method that uses Graph Laplacian processing to refine and combine bounding box data from multiple vehicles. It associates these refined detections with tracked objects across two stages.

Result: The proposed framework was extensively evaluated on the V2V4Real dataset, showing significant improvements in localization and tracking accuracies compared to baseline and state-of-the-art methods like DMSTrack and V2V4Real.

Conclusion: The Cooperative MOT framework successfully fuses data from multiple agents, demonstrating enhanced tracking accuracies and showcasing the potential of collaborative perception in autonomous systems.

Abstract: Multi-Object Tracking (MOT) plays a crucial role in autonomous driving
systems, as it lays the foundations for advanced perception and precise path
planning modules. Nonetheless, single agent based MOT lacks in sensing
surroundings due to occlusions, sensors failures, etc. Hence, the integration
of multiagent information is essential for comprehensive understanding of the
environment. This paper proposes a novel Cooperative MOT framework for tracking
objects in 3D LiDAR scene by formulating and solving a graph topology-aware
optimization problem so as to fuse information coming from multiple vehicles.
By exploiting a fully connected graph topology defined by the detected bounding
boxes, we employ the Graph Laplacian processing optimization technique to
smooth the position error of bounding boxes and effectively combine them. In
that manner, we reveal and leverage inherent coherences of diverse multi-agent
detections, and associate the refined bounding boxes to tracked objects at two
stages, optimizing localization and tracking accuracies. An extensive
evaluation study has been conducted, using the real-world V2V4Real dataset,
where the proposed method significantly outperforms the baseline frameworks,
including the state-of-the-art deep-learning DMSTrack and V2V4Real, in various
testing sequences.

</details>


### [115] [Provoking Multi-modal Few-Shot LVLM via Exploration-Exploitation In-Context Learning](https://arxiv.org/abs/2506.09473)
*Cheng Chen,Yunpeng Zhai,Yifan Zhao,Jinyang Gao,Bolin Ding,Jia Li*

Main category: cs.CV

TL;DR: The paper introduces a reinforcement learning-based framework for adaptive demonstration selection in in-context learning for Large Vision-Language Models (LVLMs), addressing limitations of heuristic strategies and redundancy in existing approaches.


<details>
  <summary>Details</summary>
Motivation: Improve in-context learning by addressing shortcomings in current demonstration selection methods such as heuristic reliance and redundancy.

Method: A reinforcement learning framework is developed to adaptively and holistically select task-specific demonstrations for LVLMs by leveraging exploration and exploitation strategies.

Result: Experiments show the framework's superior performance on four Visual Question-Answering datasets, demonstrating improved generalization for few-shot LVLMs.

Conclusion: The proposed framework enhances LVLMs by enabling self-optimization and effective demonstration selection, significantly improving task performance.

Abstract: In-context learning (ICL), a predominant trend in instruction learning, aims
at enhancing the performance of large language models by providing clear task
guidance and examples, improving their capability in task understanding and
execution. This paper investigates ICL on Large Vision-Language Models (LVLMs)
and explores the policies of multi-modal demonstration selection. Existing
research efforts in ICL face significant challenges: First, they rely on
pre-defined demonstrations or heuristic selecting strategies based on human
intuition, which are usually inadequate for covering diverse task requirements,
leading to sub-optimal solutions; Second, individually selecting each
demonstration fails in modeling the interactions between them, resulting in
information redundancy. Unlike these prevailing efforts, we propose a new
exploration-exploitation reinforcement learning framework, which explores
policies to fuse multi-modal information and adaptively select adequate
demonstrations as an integrated whole. The framework allows LVLMs to optimize
themselves by continually refining their demonstrations through
self-exploration, enabling the ability to autonomously identify and generate
the most effective selection policies for in-context learning. Experimental
results verify the superior performance of our approach on four Visual
Question-Answering (VQA) datasets, demonstrating its effectiveness in enhancing
the generalization capability of few-shot LVLMs.

</details>


### [116] [Urban1960SatSeg: Unsupervised Semantic Segmentation of Mid-20$^{th}$ century Urban Landscapes with Satellite Imageries](https://arxiv.org/abs/2506.09476)
*Tianxiang Hao,Lixian Zhang,Yingjia Zhang,Mengxuan Chen,Jinxiao Zhang,Haohuan Fu*

Main category: cs.CV

TL;DR: This paper introduces Urban1960SatBench, a unique segmentation dataset based on historical satellite imagery, and Urban1960SatUSM, an unsupervised semantic segmentation framework tailored for this data.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges like quality degradation and annotation scarcity in historical satellite imagery, enabling semantic segmentation to understand long-term urban transformation.

Method: The authors developed Urban1960SatBench, an annotated dataset covering mid-20th century urban areas, and Urban1960SatUSM, a novel unsupervised segmentation model employing self-supervised learning with a confidence-aware mechanism and focal-confidence loss.

Result: Urban1960SatUSM outperformed existing unsupervised segmentation methods on historical urban scenes, achieving significantly better segmentation results under challenging conditions.

Conclusion: The work provides valuable tools and methodologies for studying historical urban development, opening up new avenues for quantitative research on long-term urban changes using computer vision.

Abstract: Historical satellite imagery, such as mid-20$^{th}$ century Keyhole data,
offers rare insights into understanding early urban development and long-term
transformation. However, severe quality degradation (e.g., distortion,
misalignment, and spectral scarcity) and annotation absence have long hindered
semantic segmentation on such historical RS imagery. To bridge this gap and
enhance understanding of urban development, we introduce
$\textbf{Urban1960SatBench}$, an annotated segmentation dataset based on
historical satellite imagery with the earliest observation time among all
existing segmentation datasets, along with a benchmark framework for
unsupervised segmentation tasks, $\textbf{Urban1960SatUSM}$. First,
$\textbf{Urban1960SatBench}$ serves as a novel, expertly annotated semantic
segmentation dataset built on mid-20$^{th}$ century Keyhole imagery, covering
1,240 km$^2$ and key urban classes (buildings, roads, farmland, water). As the
earliest segmentation dataset of its kind, it provides a pioneering benchmark
for historical urban understanding. Second,
$\textbf{Urban1960SatUSM}$(Unsupervised Segmentation Model) is a novel
unsupervised semantic segmentation framework for historical RS imagery. It
employs a confidence-aware alignment mechanism and focal-confidence loss based
on a self-supervised learning architecture, which generates robust
pseudo-labels and adaptively prioritizes prediction difficulty and label
reliability to improve unsupervised segmentation on noisy historical data
without manual supervision. Experiments show Urban1960SatUSM significantly
outperforms existing unsupervised segmentation methods on Urban1960SatSeg for
segmenting historical urban scenes, promising in paving the way for
quantitative studies of long-term urban change using modern computer vision.
Our benchmark and supplementary material are available at
https://github.com/Tianxiang-Hao/Urban1960SatSeg.

</details>


### [117] [TinySplat: Feedforward Approach for Generating Compact 3D Scene Representation](https://arxiv.org/abs/2506.09479)
*Zetian Song,Jiaye Fu,Jiaqi Zhang,Xiaohan Lu,Chuanmin Jia,Siwei Ma,Wen Gao*

Main category: cs.CV

TL;DR: TinySplat introduces a feedforward method to make 3D Gaussian Splatting (3DGS) compact, achieving over 100x compression with minimal loss in quality.


<details>
  <summary>Details</summary>
Motivation: Address the high storage cost and redundancy in the feedforward 3D Gaussian Splatting representation for 3D reconstruction.

Method: TinySplat employs a training-free compression framework with three key components: View-Projection Transformation to reduce geometric redundancy, Visibility-Aware Basis Reduction to tackle perceptual redundancy, and spatial redundancy reduction using a video codec.

Result: TinySplat achieves over 100x compression for 3D Gaussian data generated by feedforward methods, comparable quality with only 6% storage size, 25% encoding time, and 1% decoding time.

Conclusion: TinySplat presents a feasible and efficient solution for compact 3D scene representation with significant storage savings and speed improvements, without compromising quality.

Abstract: The recent development of feedforward 3D Gaussian Splatting (3DGS) presents a
new paradigm to reconstruct 3D scenes. Using neural networks trained on
large-scale multi-view datasets, it can directly infer 3DGS representations
from sparse input views. Although the feedforward approach achieves high
reconstruction speed, it still suffers from the substantial storage cost of 3D
Gaussians. Existing 3DGS compression methods relying on scene-wise optimization
are not applicable due to architectural incompatibilities. To overcome this
limitation, we propose TinySplat, a complete feedforward approach for
generating compact 3D scene representations. Built upon standard feedforward
3DGS methods, TinySplat integrates a training-free compression framework that
systematically eliminates key sources of redundancy. Specifically, we introduce
View-Projection Transformation (VPT) to reduce geometric redundancy by
projecting geometric parameters into a more compact space. We further present
Visibility-Aware Basis Reduction (VABR), which mitigates perceptual redundancy
by aligning feature energy along dominant viewing directions via basis
transformation. Lastly, spatial redundancy is addressed through an
off-the-shelf video codec. Comprehensive experimental results on multiple
benchmark datasets demonstrate that TinySplat achieves over 100x compression
for 3D Gaussian data generated by feedforward methods. Compared to the
state-of-the-art compression approach, we achieve comparable quality with only
6% of the storage size. Meanwhile, our compression framework requires only 25%
of the encoding time and 1% of the decoding time.

</details>


### [118] [Marrying Autoregressive Transformer and Diffusion with Multi-Reference Autoregression](https://arxiv.org/abs/2506.09482)
*Dingcheng Zhen,Qian Qiao,Tan Yu,Kangxi Wu,Ziwei Zhang,Siyuan Liu,Shunshun Yin,Ming Tao*

Main category: cs.CV

TL;DR: TransDiff combines Autoregressive Transformer and diffusion models for image generation, achieving state-of-the-art performance on benchmarks like ImageNet.


<details>
  <summary>Details</summary>
Motivation: The paper aims to integrate the strengths of AR Transformers and diffusion models to significantly enhance performance in image generation tasks.

Method: TransDiff encodes labels and images into semantic features using a fusion framework and employs a diffusion model for image sample distribution. It also introduces Multi-Reference Autoregression (MRAR) to improve diversity and quality in generated images.

Result: TransDiff achieves superior performance with an FID of 1.61, IS of 293.4, and faster inference compared to state-of-the-art models. With MRAR, it further reduces FID to 1.42.

Conclusion: TransDiff sets a new benchmark in image generation, showcasing improved generation quality and inference efficiency, and introduces innovative techniques like MRAR for enhanced representation learning.

Abstract: We introduce TransDiff, the first image generation model that marries
Autoregressive (AR) Transformer with diffusion models. In this joint modeling
framework, TransDiff encodes labels and images into high-level semantic
features and employs a diffusion model to estimate the distribution of image
samples. On the ImageNet 256x256 benchmark, TransDiff significantly outperforms
other image generation models based on standalone AR Transformer or diffusion
models. Specifically, TransDiff achieves a Fr\'echet Inception Distance (FID)
of 1.61 and an Inception Score (IS) of 293.4, and further provides x2 faster
inference latency compared to state-of-the-art methods based on AR Transformer
and x112 faster inference compared to diffusion-only models. Furthermore,
building on the TransDiff model, we introduce a novel image generation paradigm
called Multi-Reference Autoregression (MRAR), which performs autoregressive
generation by predicting the next image. MRAR enables the model to reference
multiple previously generated images, thereby facilitating the learning of more
diverse representations and improving the quality of generated images in
subsequent iterations. By applying MRAR, the performance of TransDiff is
improved, with the FID reduced from 1.61 to 1.42. We expect TransDiff to open
up a new frontier in the field of image generation.

</details>


### [119] [HopaDIFF: Holistic-Partial Aware Fourier Conditioned Diffusion for Referring Human Action Segmentation in Multi-Person Scenarios](https://arxiv.org/abs/2506.09650)
*Kunyu Peng,Junchao Huang,Xiangsheng Huang,Di Wen,Junwei Zheng,Yufan Chen,Kailun Yang,Jiamin Wu,Chongqing Hao,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: The paper introduces a new task in video understanding, Referring Human Action Segmentation (RHAS), that focuses on textual descriptions for segmentation in multi-person scenarios and presents a dataset and novel framework named HopaDIFF.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on single-person action segmentation or scenarios with fixed action sequences, ignoring multi-person contexts where actions need to be tied to specific people referenced by textual descriptions.

Method: The authors created RHAS133, a dataset with 33 hours of video and textual descriptions, and proposed HopaDIFF, a diffusion framework using cross-input gate attentional xLSTM for reasoning and Fourier-based conditioning for fine-grained control.

Result: HopaDIFF demonstrates state-of-the-art performance in multiple evaluation settings, outperforming existing methods in action segmentation.

Conclusion: The paper pioneers a new domain of video understanding by introducing RHAS133 and HopaDIFF, achieving significant advancements in multi-person action segmentation guided by textual references.

Abstract: Action segmentation is a core challenge in high-level video understanding,
aiming to partition untrimmed videos into segments and assign each a label from
a predefined action set. Existing methods primarily address single-person
activities with fixed action sequences, overlooking multi-person scenarios. In
this work, we pioneer textual reference-guided human action segmentation in
multi-person settings, where a textual description specifies the target person
for segmentation. We introduce the first dataset for Referring Human Action
Segmentation, i.e., RHAS133, built from 133 movies and annotated with 137
fine-grained actions with 33h video data, together with textual descriptions
for this new task. Benchmarking existing action recognition methods on RHAS133
using VLM-based feature extractors reveals limited performance and poor
aggregation of visual cues for the target person. To address this, we propose a
holistic-partial aware Fourier-conditioned diffusion framework, i.e., HopaDIFF,
leveraging a novel cross-input gate attentional xLSTM to enhance
holistic-partial long-range reasoning and a novel Fourier condition to
introduce more fine-grained control to improve the action segmentation
generation. HopaDIFF achieves state-of-the-art results on RHAS133 in diverse
evaluation settings. The code is available at
https://github.com/KPeng9510/HopaDIFF.git.

</details>


### [120] [Generalized Gaussian Entropy Model for Point Cloud Attribute Compression with Dynamic Likelihood Intervals](https://arxiv.org/abs/2506.09510)
*Changhao Peng,Yuqi Ye,Wei Gao*

Main category: cs.CV

TL;DR: This paper introduces a generalized Gaussian entropy model and a dynamic likelihood interval adjustment method to improve point cloud attribute compression.


<details>
  <summary>Details</summary>
Motivation: Current methods in point cloud attribute compression leave unutilized information in entropy parameters provided by neural networks, which can be optimized for better probability estimation.

Method: The authors propose a generalized Gaussian entropy model incorporating a shape parameter for better tail shape control and introduce a Mean Error Discriminator (MED) to dynamically adjust likelihood intervals during arithmetic coding.

Result: The proposed method significantly enhances rate-distortion performance in point cloud attribute compression on three VAE-based models and demonstrates applicability to other compression tasks such as image and video compression.

Conclusion: By leveraging improved entropy modeling and dynamic coding techniques, this study provides a more accurate and adaptable framework for data compression, benefiting various tasks beyond point cloud attribute compression.

Abstract: Gaussian and Laplacian entropy models are proved effective in learned point
cloud attribute compression, as they assist in arithmetic coding of latents.
However, we demonstrate through experiments that there is still unutilized
information in entropy parameters estimated by neural networks in current
methods, which can be used for more accurate probability estimation. Thus we
introduce generalized Gaussian entropy model, which controls the tail shape
through shape parameter to more accurately estimate the probability of latents.
Meanwhile, to the best of our knowledge, existing methods use fixed likelihood
intervals for each integer during arithmetic coding, which limits model
performance. We propose Mean Error Discriminator (MED) to determine whether the
entropy parameter estimation is accurate and then dynamically adjust likelihood
intervals. Experiments show that our method significantly improves
rate-distortion (RD) performance on three VAE-based models for point cloud
attribute compression, and our method can be applied to other compression
tasks, such as image and video compression.

</details>


### [121] [Hierarchical Image Matching for UAV Absolute Visual Localization via Semantic and Structural Constraints](https://arxiv.org/abs/2506.09748)
*Xiangkai Zhang,Xiang Zhou,Mao Chen,Yuchen Lu,Xu Yang,Zhiyong Liu*

Main category: cs.CV

TL;DR: The paper proposes a method for UAV absolute localization using semantic-aware coarse matching and lightweight fine-grained matching, aimed at overcoming challenges in GNSS-denied environments.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of UAV localization when GNSS signals are unavailable and improve the accuracy of vision-based methods that face cross-source discrepancies and temporal variations.

Method: The paper introduces hierarchical cross-source image matching, combining semantic-aware region-level matching with lightweight pixel-level fine-grained matching, integrated into a UAV localization pipeline.

Result: Experimental results on public datasets and a new CS-UAV dataset demonstrate improved accuracy and robustness in challenging conditions.

Conclusion: The hierarchical cross-source image matching method is effective for UAV localization in GNSS-denied scenarios, providing superior performance compared to existing approaches.

Abstract: Absolute localization, aiming to determine an agent's location with respect
to a global reference, is crucial for unmanned aerial vehicles (UAVs) in
various applications, but it becomes challenging when global navigation
satellite system (GNSS) signals are unavailable. Vision-based absolute
localization methods, which locate the current view of the UAV in a reference
satellite map to estimate its position, have become popular in GNSS-denied
scenarios. However, existing methods mostly rely on traditional and low-level
image matching, suffering from difficulties due to significant differences
introduced by cross-source discrepancies and temporal variations. To overcome
these limitations, in this paper, we introduce a hierarchical cross-source
image matching method designed for UAV absolute localization, which integrates
a semantic-aware and structure-constrained coarse matching module with a
lightweight fine-grained matching module. Specifically, in the coarse matching
module, semantic features derived from a vision foundation model first
establish region-level correspondences under semantic and structural
constraints. Then, the fine-grained matching module is applied to extract fine
features and establish pixel-level correspondences. Building upon this, a UAV
absolute visual localization pipeline is constructed without any reliance on
relative localization techniques, mainly by employing an image retrieval module
before the proposed hierarchical image matching modules. Experimental
evaluations on public benchmark datasets and a newly introduced CS-UAV dataset
demonstrate superior accuracy and robustness of the proposed method under
various challenging conditions, confirming its effectiveness.

</details>


### [122] [HAIF-GS: Hierarchical and Induced Flow-Guided Gaussian Splatting for Dynamic Scene](https://arxiv.org/abs/2506.09518)
*Jianing Chen,Zehao Li,Yujun Cai,Hao Jiang,Chengxuan Qian,Juyuan Kang,Shuqin Gao,Honglong Zhao,Tianlu Mao,Yucheng Zhang*

Main category: cs.CV

TL;DR: This paper introduces HAIF-GS, a framework to improve dynamic 3D reconstruction from monocular videos, addressing issues like redundant updates, insufficient motion supervision, and non-rigid deformations.


<details>
  <summary>Details</summary>
Motivation: Dynamic 3D scene reconstruction from monocular videos suffers from issues such as inefficient motion representation and inconsistent temporal modeling, hindering its quality and efficiency.

Method: The proposed HAIF-GS framework uses anchor-based deformation mechanisms: an Anchor Filter for identifying motion-relevant regions, a self-supervised Induced Flow-Guided Deformation module for motion induction without explicit labels, and a Hierarchical Anchor Propagation mechanism to capture fine-grained deformations.

Result: HAIF-GS significantly improves rendering quality, temporal coherence, and reconstruction efficiency when compared to existing methods, validated via experiments on synthetic and real-world benchmarks.

Conclusion: By addressing motion structuring and consistency, HAIF-GS offers a robust solution for dynamically reconstructing 3D scenes, overcoming limitations in prior methods.

Abstract: Reconstructing dynamic 3D scenes from monocular videos remains a fundamental
challenge in 3D vision. While 3D Gaussian Splatting (3DGS) achieves real-time
rendering in static settings, extending it to dynamic scenes is challenging due
to the difficulty of learning structured and temporally consistent motion
representations. This challenge often manifests as three limitations in
existing methods: redundant Gaussian updates, insufficient motion supervision,
and weak modeling of complex non-rigid deformations. These issues collectively
hinder coherent and efficient dynamic reconstruction. To address these
limitations, we propose HAIF-GS, a unified framework that enables structured
and consistent dynamic modeling through sparse anchor-driven deformation. It
first identifies motion-relevant regions via an Anchor Filter to suppresses
redundant updates in static areas. A self-supervised Induced Flow-Guided
Deformation module induces anchor motion using multi-frame feature aggregation,
eliminating the need for explicit flow labels. To further handle fine-grained
deformations, a Hierarchical Anchor Propagation mechanism increases anchor
resolution based on motion complexity and propagates multi-level
transformations. Extensive experiments on synthetic and real-world benchmarks
validate that HAIF-GS significantly outperforms prior dynamic 3DGS methods in
rendering quality, temporal coherence, and reconstruction efficiency.

</details>


### [123] [OctoNav: Towards Generalist Embodied Navigation](https://arxiv.org/abs/2506.09839)
*Chen Gao,Liankai Jin,Xingyu Peng,Jiazhao Zhang,Yue Deng,Annan Li,He Wang,Si Liu*

Main category: cs.CV

TL;DR: This paper introduces OctoNav-Bench and OctoNav-R1 to advance generalist embodied navigation agents by unifying diverse instructions and modalities, providing benchmarks and a novel training paradigm for improved reasoning and navigation.


<details>
  <summary>Details</summary>
Motivation: Navigation tasks in embodied AI are fragmented due to differing objectives, datasets, and methods. This work seeks to unify these tasks and create more capable generalist navigation agents.

Method: The authors developed OctoNav-Bench, a benchmark with diverse free-form instructions and Think-Before-Action datasets, and OctoNav-R1, a model trained through a three-stage Hybrid Training Paradigm leveraging elements like Action-/TBA-SFT and Nav-GPRO for reasoning and navigation abilities.

Result: OctoNav-R1 outperforms previous navigation methods by better handling multi-modal and multi-capability instructions in embodied navigation tasks.

Conclusion: Integrating a unified benchmark, diverse data, and a structured training approach significantly improves embodied navigation agents, paving the way for generalist AI in this domain.

Abstract: Embodied navigation stands as a foundation pillar within the broader pursuit
of embodied AI. However, previous navigation research is divided into different
tasks/capabilities, e.g., ObjNav, ImgNav and VLN, where they differ in task
objectives and modalities, making datasets and methods are designed
individually. In this work, we take steps toward generalist navigation agents,
which can follow free-form instructions that include arbitrary compounds of
multi-modal and multi-capability. To achieve this, we propose a large-scale
benchmark and corresponding method, termed OctoNav-Bench and OctoNav-R1.
Specifically, OctoNav-Bench features continuous environments and is constructed
via a designed annotation pipeline. We thoroughly craft instruction-trajectory
pairs, where instructions are diverse in free-form with arbitrary modality and
capability. Also, we construct a Think-Before-Action (TBA-CoT) dataset within
OctoNav-Bench to provide the thinking process behind actions. For OctoNav-R1,
we build it upon MLLMs and adapt it to a VLA-type model, which can produce
low-level actions solely based on 2D visual observations. Moreover, we design a
Hybrid Training Paradigm (HTP) that consists of three stages, i.e.,
Action-/TBA-SFT, Nav-GPRO, and Online RL stages. Each stage contains
specifically designed learning policies and rewards. Importantly, for TBA-SFT
and Nav-GRPO designs, we are inspired by the OpenAI-o1 and DeepSeek-R1, which
show impressive reasoning ability via thinking-before-answer. Thus, we aim to
investigate how to achieve thinking-before-action in the embodied navigation
field, to improve model's reasoning ability toward generalists. Specifically,
we propose TBA-SFT to utilize the TBA-CoT dataset to fine-tune the model as a
cold-start phrase and then leverage Nav-GPRO to improve its thinking ability.
Finally, OctoNav-R1 shows superior performance compared with previous methods.

</details>


### [124] [Revisit What You See: Disclose Language Prior in Vision Tokens for Efficient Guided Decoding of LVLMs](https://arxiv.org/abs/2506.09522)
*Beomsik Cho,Jaehyung Kim*

Main category: cs.CV

TL;DR: The paper introduces ReVisiT, a simple decoding method for large vision-language models (LVLMs) that improves visual grounding without additional training or complex procedures.


<details>
  <summary>Details</summary>
Motivation: Current LVLM decoding strategies often fail to incorporate visual information effectively into responses, leading to visually ungrounded outputs.

Method: ReVisiT guides text generation by leveraging vision tokens. It projects these tokens into the text distribution space, dynamically selects relevant ones at each decoding step via constrained divergence minimization, and refines output distributions accordingly.

Result: Experiments on three LVLM hallucination benchmarks show that ReVisiT enhances visual grounding, achieves competitive or superior results compared to state-of-the-art methods, and reduces computational costs by up to 2×.

Conclusion: ReVisiT is a simple, effective, and computationally efficient method to improve visual grounding in LVLMs, making it valuable for multimodal tasks.

Abstract: Large Vision-Language Models (LVLMs) have demonstrated remarkable performance
across various multimodal tasks by integrating visual perception with language
understanding. However, conventional decoding strategies of LVLMs often fail to
successfully utilize visual information, leading to visually ungrounded
responses. While various approaches have been proposed to address this
limitation, they typically require additional training, multi-step inference
procedures, or external model dependencies. This paper introduces ReVisiT, a
simple yet effective decoding method that references vision tokens to guide the
text generation process in LVLMs. Our approach leverages the semantic
information embedded within vision tokens by projecting them into the text
token distribution space, and dynamically selecting the most relevant vision
token at each decoding step through constrained divergence minimization. This
selected vision token is then used to refine the output distribution to better
incorporate visual semantics. Experiments on three LVLM hallucination
benchmarks with two recent LVLMs demonstrate that ReVisiT consistently enhances
visual grounding with minimal computational overhead. Moreover, our method
achieves competitive or superior results relative to state-of-the-art baselines
while reducing computational costs for up to $2\times$.

</details>


### [125] [ReSim: Reliable World Simulation for Autonomous Driving](https://arxiv.org/abs/2506.09981)
*Jiazhi Yang,Kashyap Chitta,Shenyuan Gao,Long Chen,Yuqian Shao,Xiaosong Jia,Hongyang Li,Andreas Geiger,Xiangyu Yue,Li Chen*

Main category: cs.CV

TL;DR: This paper introduces ReSim, a controllable simulation model for diverse driving scenarios, by training on both real and simulated non-expert driving data.


<details>
  <summary>Details</summary>
Motivation: Current driving world models struggle to simulate hazardous or rare non-expert behaviors since they are typically developed using real-world data mostly composed of safe expert actions.

Method: The authors combine real-world human driving data with non-expert data from a driving simulator, then train a diffusion-transformer video generator with conditioning strategies to enhance control and fidelity.

Result: ReSim achieves significant improvements: up to 44% higher visual fidelity, over 50% better controllability for expert and non-expert actions, and boosts policy evaluation performance by 2–25% on the NAVSIM benchmark.

Conclusion: ReSim is a robust and reliable simulation tool capable of generating diverse driving scenarios under various behaviors, enabling better policy evaluation and planning.

Abstract: How can we reliably simulate future driving scenarios under a wide range of
ego driving behaviors? Recent driving world models, developed exclusively on
real-world driving data composed mainly of safe expert trajectories, struggle
to follow hazardous or non-expert behaviors, which are rare in such data. This
limitation restricts their applicability to tasks such as policy evaluation. In
this work, we address this challenge by enriching real-world human
demonstrations with diverse non-expert data collected from a driving simulator
(e.g., CARLA), and building a controllable world model trained on this
heterogeneous corpus. Starting with a video generator featuring a diffusion
transformer architecture, we devise several strategies to effectively integrate
conditioning signals and improve prediction controllability and fidelity. The
resulting model, ReSim, enables Reliable Simulation of diverse open-world
driving scenarios under various actions, including hazardous non-expert ones.
To close the gap between high-fidelity simulation and applications that require
reward signals to judge different actions, we introduce a Video2Reward module
that estimates a reward from ReSim's simulated future. Our ReSim paradigm
achieves up to 44% higher visual fidelity, improves controllability for both
expert and non-expert actions by over 50%, and boosts planning and policy
selection performance on NAVSIM by 2% and 25%, respectively.

</details>


### [126] [Gaussian Herding across Pens: An Optimal Transport Perspective on Global Gaussian Reduction for 3DGS](https://arxiv.org/abs/2506.09534)
*Tao Wang,Mengyu Li,Geduo Zeng,Cheng Meng,Qiong Zhang*

Main category: cs.CV

TL;DR: The paper proposes a method to reduce the number of Gaussians used in 3D Gaussian Splatting, cutting down the count to 10% while maintaining rendering quality.


<details>
  <summary>Details</summary>
Motivation: Rendering with millions of Gaussian primitives in 3D Gaussian Splatting is memory-intensive and inefficient, and existing reduction techniques lack a guarantee on global quality fidelity.

Method: The authors introduce an optimal transport framework for global Gaussian mixture reduction, employing KD-tree partitioning for geometric simplification followed by fine-tuning attributes (color, opacity).

Result: The proposed method significantly reduces Gaussian counts (to 10%) with negligible loss in rendering quality while outperforming existing compaction methods on benchmarks.

Conclusion: The method offers an efficient, adaptable solution for lightweight neural rendering, applicable across different stages of 3D Gaussian Splatting pipelines.

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful technique for radiance
field rendering, but it typically requires millions of redundant Gaussian
primitives, overwhelming memory and rendering budgets. Existing compaction
approaches address this by pruning Gaussians based on heuristic importance
scores, without global fidelity guarantee. To bridge this gap, we propose a
novel optimal transport perspective that casts 3DGS compaction as global
Gaussian mixture reduction. Specifically, we first minimize the composite
transport divergence over a KD-tree partition to produce a compact geometric
representation, and then decouple appearance from geometry by fine-tuning color
and opacity attributes with far fewer Gaussian primitives. Experiments on
benchmark datasets show that our method (i) yields negligible loss in rendering
quality (PSNR, SSIM, LPIPS) compared to vanilla 3DGS with only 10% Gaussians;
and (ii) consistently outperforms state-of-the-art 3DGS compaction techniques.
Notably, our method is applicable to any stage of vanilla or accelerated 3DGS
pipelines, providing an efficient and agnostic pathway to lightweight neural
rendering.

</details>


### [127] [AngleRoCL: Angle-Robust Concept Learning for Physically View-Invariant T2I Adversarial Patches](https://arxiv.org/abs/2506.09538)
*Wenjun Ji,Yuxiang Fu,Luyang Ying,Deng-Ping Fan,Yuyi Wang,Ming-Ming Cheng,Ivor Tsang,Qing Guo*

Main category: cs.CV

TL;DR: This paper studies and addresses the angle robustness limitations of text-to-image (T2I) adversarial patches, proposing a new method called AngleRoCL for generating patches resistant to viewpoint variations.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-image adversarial patches fail to maintain high attack effectiveness from different angles, highlighting a critical robustness gap in real-world applications.

Method: The authors propose Angle-Robust Concept Learning (AngleRoCL), which learns a generalized concept (text embeddings) to generate patches inherently resilient to varying angles when used with T2I diffusion models.

Result: Extensive experiments demonstrate that AngleRoCL significantly improves the angle robustness of T2I adversarial patches, achieving over 50% improvement in attack effectiveness across multiple angles compared to baseline approaches.

Conclusion: AngleRoCL provides a practical solution for enhancing the angle robustness of adversarial patches, offering new insights into the interaction between textual prompts and physical properties in T2I-generated content.

Abstract: Cutting-edge works have demonstrated that text-to-image (T2I) diffusion
models can generate adversarial patches that mislead state-of-the-art object
detectors in the physical world, revealing detectors' vulnerabilities and
risks. However, these methods neglect the T2I patches' attack effectiveness
when observed from different views in the physical world (i.e., angle
robustness of the T2I adversarial patches). In this paper, we study the angle
robustness of T2I adversarial patches comprehensively, revealing their
angle-robust issues, demonstrating that texts affect the angle robustness of
generated patches significantly, and task-specific linguistic instructions fail
to enhance the angle robustness. Motivated by the studies, we introduce
Angle-Robust Concept Learning (AngleRoCL), a simple and flexible approach that
learns a generalizable concept (i.e., text embeddings in implementation)
representing the capability of generating angle-robust patches. The learned
concept can be incorporated into textual prompts and guides T2I models to
generate patches with their attack effectiveness inherently resistant to
viewpoint variations. Through extensive simulation and physical-world
experiments on five SOTA detectors across multiple views, we demonstrate that
AngleRoCL significantly enhances the angle robustness of T2I adversarial
patches compared to baseline methods. Our patches maintain high attack success
rates even under challenging viewing conditions, with over 50% average relative
improvement in attack effectiveness across multiple angles. This research
advances the understanding of physically angle-robust patches and provides
insights into the relationship between textual concepts and physical properties
in T2I-generated contents.

</details>


### [128] [3DGeoDet: General-purpose Geometry-aware Image-based 3D Object Detection](https://arxiv.org/abs/2506.09541)
*Yi Zhang,Yi Wang,Yawen Cui,Lap-Pui Chau*

Main category: cs.CV

TL;DR: 3DGeoDet is a new approach for 3D object detection in single- and multi-view RGB images that improves geometric awareness using depth-based voxel and TSDF representations, achieving state-of-the-art results on various datasets.


<details>
  <summary>Details</summary>
Motivation: Image-based 3D object detection struggles with the lack of 3D geometric cues, resulting in difficulties in mapping images to 3D spaces.

Method: The method employs predicted depth to create voxel occupancy and manipulate a voxelized 3D feature volume with occupancy attention. Additionally, an implicit TSDF representation is added to enhance 3D awareness. Both explicit and implicit representations are trained end-to-end without 3D signal supervision.

Result: 3DGeoDet achieved significant performance improvements: 9.3 mAP@0.5 on SUN RGB-D, 3.3 mAP@0.5 on ScanNetV2, and 0.19 AP3D@0.7 on KITTI, surpassing state-of-the-art image-based methods.

Conclusion: By leveraging predicted depth and combining explicit voxel-based and implicit TSDF-based geometric representations, 3DGeoDet establishes itself as an effective and general-purpose method for image-based 3D object detection.

Abstract: This paper proposes 3DGeoDet, a novel geometry-aware 3D object detection
approach that effectively handles single- and multi-view RGB images in indoor
and outdoor environments, showcasing its general-purpose applicability. The key
challenge for image-based 3D object detection tasks is the lack of 3D geometric
cues, which leads to ambiguity in establishing correspondences between images
and 3D representations. To tackle this problem, 3DGeoDet generates efficient 3D
geometric representations in both explicit and implicit manners based on
predicted depth information. Specifically, we utilize the predicted depth to
learn voxel occupancy and optimize the voxelized 3D feature volume explicitly
through the proposed voxel occupancy attention. To further enhance 3D
awareness, the feature volume is integrated with an implicit 3D representation,
the truncated signed distance function (TSDF). Without requiring supervision
from 3D signals, we significantly improve the model's comprehension of 3D
geometry by leveraging intermediate 3D representations and achieve end-to-end
training. Our approach surpasses the performance of state-of-the-art
image-based methods on both single- and multi-view benchmark datasets across
diverse environments, achieving a 9.3 mAP@0.5 improvement on the SUN RGB-D
dataset, a 3.3 mAP@0.5 improvement on the ScanNetV2 dataset, and a 0.19
AP3D@0.7 improvement on the KITTI dataset. The project page is available at:
https://cindy0725.github.io/3DGeoDet/.

</details>


### [129] [GLD-Road:A global-local decoding road network extraction model for remote sensing images](https://arxiv.org/abs/2506.09553)
*Ligao Deng,Yupeng Deng,Yu Meng,Jingbo Chen,Zhihao Xi,Diyou Liu,Qifeng Chu*

Main category: cs.CV

TL;DR: GLD-Road is a novel two-stage model for efficient and precise road network extraction, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Manual annotation of road networks is costly, and existing deep learning methods have trade-offs between accuracy and computational efficiency.

Method: The paper introduces GLD-Road, a hybrid model combining a global Connect Module for node detection and connection with iterative local refinement to fix broken roads.

Result: GLD-Road improves APLS by 1.9% (City-Scale) and 0.67% (SpaceNet3) while reducing retrieval time by 40% against global models and 92% against local models.

Conclusion: The proposed GLD-Road model effectively balances precision and efficiency, making it suitable for road network extraction tasks with superior performance to existing methods.

Abstract: Road networks are crucial for mapping, autonomous driving, and disaster
response. While manual annotation is costly, deep learning offers efficient
extraction. Current methods include postprocessing (prone to errors), global
parallel (fast but misses nodes), and local iterative (accurate but slow). We
propose GLD-Road, a two-stage model combining global efficiency and local
precision. First, it detects road nodes and connects them via a Connect Module.
Then, it iteratively refines broken roads using local searches, drastically
reducing computation. Experiments show GLD-Road outperforms state-of-the-art
methods, improving APLS by 1.9% (City-Scale) and 0.67% (SpaceNet3). It also
reduces retrieval time by 40% vs. Sat2Graph (global) and 92% vs. RNGDet++
(local). The experimental results are available at
https://github.com/ucas-dlg/GLD-Road.

</details>


### [130] [AD^2-Bench: A Hierarchical CoT Benchmark for MLLM in Autonomous Driving under Adverse Conditions](https://arxiv.org/abs/2506.09557)
*Zhaoyang Wei,Chenhui Qiang,Bowen Jiang,Xumeng Han,Xuehui Yu,Zhenjun Han*

Main category: cs.CV

TL;DR: The paper introduces AD^2-Bench, a Chain-of-Thought benchmark designed for evaluating Multi-Modal Large Models (MLLMs) in autonomous driving under adverse weather and complex traffic conditions.


<details>
  <summary>Details</summary>
Motivation: There is a lack of rigorous evaluation tools for Multi-Modal Large Models' decision-making capabilities in the challenging scenarios of autonomous driving, including adverse weather and complex traffic.

Method: The authors developed AD^2-Bench with over 5.4k high-quality CoT instances featuring fine-grained annotations and an evaluation framework to assess MLLMs accurately across diverse environments and visual prompts.

Result: The evaluation of state-of-the-art MLLMs using AD^2-Bench indicated an accuracy below 60%, showcasing the benchmark's difficulty and the challenges in advancing robust reasoning systems.

Conclusion: AD^2-Bench provides a crucial platform for standardized evaluation and research on improving MLLMs' structured reasoning in autonomous driving, making it a valuable resource for the field.

Abstract: Chain-of-Thought (CoT) reasoning has emerged as a powerful approach to
enhance the structured, multi-step decision-making capabilities of Multi-Modal
Large Models (MLLMs), is particularly crucial for autonomous driving with
adverse weather conditions and complex traffic environments. However, existing
benchmarks have largely overlooked the need for rigorous evaluation of CoT
processes in these specific and challenging scenarios. To address this critical
gap, we introduce AD^2-Bench, the first Chain-of-Thought benchmark specifically
designed for autonomous driving with adverse weather and complex scenes.
AD^2-Bench is meticulously constructed to fulfill three key criteria:
comprehensive data coverage across diverse adverse environments, fine-grained
annotations that support multi-step reasoning, and a dedicated evaluation
framework tailored for assessing CoT performance. The core contribution of
AD^2-Bench is its extensive collection of over 5.4k high-quality, manually
annotated CoT instances. Each intermediate reasoning step in these annotations
is treated as an atomic unit with explicit ground truth, enabling unprecedented
fine-grained analysis of MLLMs' inferential processes under text-level,
point-level, and region-level visual prompts. Our comprehensive evaluation of
state-of-the-art MLLMs on AD^2-Bench reveals accuracy below 60%, highlighting
the benchmark's difficulty and the need to advance robust, interpretable
end-to-end autonomous driving systems. AD^2-Bench thus provides a standardized
evaluation platform, driving research forward by improving MLLMs' reasoning in
autonomous driving, making it an invaluable resource.

</details>


### [131] [SemanticSplat: Feed-Forward 3D Scene Understanding with Language-Aware Gaussian Fields](https://arxiv.org/abs/2506.09565)
*Qijing Li,Jingxiang Sun,Liang An,Zhaoqi Su,Hongwen Zhang,Yebin Liu*

Main category: cs.CV

TL;DR: SemanticSplat is a new method for holistic 3D scene understanding that integrates geometry, appearance, and semantic modeling, overcoming limitations of prior approaches.


<details>
  <summary>Details</summary>
Motivation: Current methods for 3D scene understanding struggle to achieve holistic comprehension due to low-quality geometry reconstructions, noisy artifacts, or reliance on dense input views.

Method: SemanticSplat unifies 3D anisotropic Gaussian modeling with latent semantic attributes and employs a two-stage distillation framework using sparse-view images to reconstruct a multi-modal semantic feature field.

Result: SemanticSplat demonstrates improved results in 3D scene understanding tasks, including promptable and open-vocabulary segmentation, validated through experiments.

Conclusion: SemanticSplat offers an efficient and practical framework for semantic and holistic 3D scene reconstruction, enhancing accuracy and usability for robotics and augmented reality applications.

Abstract: Holistic 3D scene understanding, which jointly models geometry, appearance,
and semantics, is crucial for applications like augmented reality and robotic
interaction. Existing feed-forward 3D scene understanding methods (e.g., LSM)
are limited to extracting language-based semantics from scenes, failing to
achieve holistic scene comprehension. Additionally, they suffer from
low-quality geometry reconstruction and noisy artifacts. In contrast, per-scene
optimization methods rely on dense input views, which reduces practicality and
increases complexity during deployment. In this paper, we propose
SemanticSplat, a feed-forward semantic-aware 3D reconstruction method, which
unifies 3D Gaussians with latent semantic attributes for joint
geometry-appearance-semantics modeling. To predict the semantic anisotropic
Gaussians, SemanticSplat fuses diverse feature fields (e.g., LSeg, SAM) with a
cost volume representation that stores cross-view feature similarities,
enhancing coherent and accurate scene comprehension. Leveraging a two-stage
distillation framework, SemanticSplat reconstructs a holistic multi-modal
semantic feature field from sparse-view images. Experiments demonstrate the
effectiveness of our method for 3D scene understanding tasks like promptable
and open-vocabulary segmentation. Video results are available at
https://semanticsplat.github.io.

</details>


### [132] [Consistent Story Generation with Asymmetry Zigzag Sampling](https://arxiv.org/abs/2506.09612)
*Mingxiao LI,mang ning,Marie-Francine Moens*

Main category: cs.CV

TL;DR: The paper proposes a novel method, Zigzag Sampling, to address the challenge of maintaining subject consistency in text-to-image visual storytelling.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the issue of poor subject consistency in text-to-image generation across multiple images, a crucial requirement for coherent visual storytelling.

Method: The paper introduces Zigzag Sampling, a training-free sampling strategy that combines asymmetric prompting and visual sharing to ensure subject consistency.

Result: Experimental results showcase that the Zigzag Sampling method significantly outperforms existing techniques in terms of generating coherent and consistent visual stories.

Conclusion: The proposed Zigzag Sampling strategy is an effective and resource-efficient solution for improving subject consistency in text-to-image visual storytelling.

Abstract: Text-to-image generation models have made significant progress in producing
high-quality images from textual descriptions, yet they continue to struggle
with maintaining subject consistency across multiple images, a fundamental
requirement for visual storytelling. Existing methods attempt to address this
by either fine-tuning models on large-scale story visualization datasets, which
is resource-intensive, or by using training-free techniques that share
information across generations, which still yield limited success. In this
paper, we introduce a novel training-free sampling strategy called Zigzag
Sampling with Asymmetric Prompts and Visual Sharing to enhance subject
consistency in visual story generation. Our approach proposes a zigzag sampling
mechanism that alternates between asymmetric prompting to retain subject
characteristics, while a visual sharing module transfers visual cues across
generated images to %further enforce consistency. Experimental results, based
on both quantitative metrics and qualitative evaluations, demonstrate that our
method significantly outperforms previous approaches in generating coherent and
consistent visual stories. The code is available at
https://github.com/Mingxiao-Li/Asymmetry-Zigzag-StoryDiffusion.

</details>


### [133] [ECAM: A Contrastive Learning Approach to Avoid Environmental Collision in Trajectory Forecasting](https://arxiv.org/abs/2506.09626)
*Giacomo Rosin,Muhammad Rameez Ur Rahman,Sebastiano Vascon*

Main category: cs.CV

TL;DR: The paper introduces ECAM, a module enhancing human trajectory prediction models with environment-aware collision avoidance, significantly lowering collision rates.


<details>
  <summary>Details</summary>
Motivation: Existing trajectory forecasting models often fail to adequately consider environmental obstacles, leading to avoidable collisions.

Method: The paper proposes ECAM, a contrastive learning-based module, which can be seamlessly integrated into trajectory forecasting models to better account for environmental factors.

Result: Integration of ECAM into state-of-the-art models reduces collision rates by 40-50%, as demonstrated using the ETH/UCY dataset.

Conclusion: ECAM effectively improves collision avoidance in human trajectory prediction models and can be a valuable addition to existing solutions.

Abstract: Human trajectory forecasting is crucial in applications such as autonomous
driving, robotics and surveillance. Accurate forecasting requires models to
consider various factors, including social interactions, multi-modal
predictions, pedestrian intention and environmental context. While existing
methods account for these factors, they often overlook the impact of the
environment, which leads to collisions with obstacles. This paper introduces
ECAM (Environmental Collision Avoidance Module), a contrastive learning-based
module to enhance collision avoidance ability with the environment. The
proposed module can be integrated into existing trajectory forecasting models,
improving their ability to generate collision-free predictions. We evaluate our
method on the ETH/UCY dataset and quantitatively and qualitatively demonstrate
its collision avoidance capabilities. Our experiments show that
state-of-the-art methods significantly reduce (-40/50%) the collision rate when
integrated with the proposed module. The code is available at
https://github.com/CVML-CFU/ECAM.

</details>


### [134] [HSENet: Hybrid Spatial Encoding Network for 3D Medical Vision-Language Understanding](https://arxiv.org/abs/2506.09634)
*Yanzhao Shi,Xiaodan Zhang,Junzhong Ji,Haoning Jiang,Chengxin Zheng,Yinong Wang,Liangqiong Qu*

Main category: cs.CV

TL;DR: The paper introduces HSENet, a framework improving 3D CT diagnosis by leveraging dual-3D vision encoders and a Spatial Packer for enhanced vision-language understanding.


<details>
  <summary>Details</summary>
Motivation: Existing large language models primarily focus on 2D medical images, which limits their capacity to fully capture complex 3D anatomical structures, leading to diagnostic errors.

Method: HSENet uses dual-3D vision encoders to extract both global and fine-grained 3D imagery details, combined with a spatial packer for compact representation of high-resolution regions, aligned with diagnostic reports.

Result: HSENet achieves state-of-the-art results across several tasks: 39.85% R@100 in 3D retrieval, 24.01% BLEU-4 score in medical report generation, and 73.60% major class accuracy in visual question answering.

Conclusion: HSENet effectively enhances the interpretation of 3D medical images for diagnostic purposes, addressing limitations in current methods while establishing new benchmarks for vision-language tasks.

Abstract: Automated 3D CT diagnosis empowers clinicians to make timely, evidence-based
decisions by enhancing diagnostic accuracy and workflow efficiency. While
multimodal large language models (MLLMs) exhibit promising performance in
visual-language understanding, existing methods mainly focus on 2D medical
images, which fundamentally limits their ability to capture complex 3D
anatomical structures. This limitation often leads to misinterpretation of
subtle pathologies and causes diagnostic hallucinations. In this paper, we
present Hybrid Spatial Encoding Network (HSENet), a framework that exploits
enriched 3D medical visual cues by effective visual perception and projection
for accurate and robust vision-language understanding. Specifically, HSENet
employs dual-3D vision encoders to perceive both global volumetric contexts and
fine-grained anatomical details, which are pre-trained by dual-stage alignment
with diagnostic reports. Furthermore, we propose Spatial Packer, an efficient
multimodal projector that condenses high-resolution 3D spatial regions into a
compact set of informative visual tokens via centroid-based compression. By
assigning spatial packers with dual-3D vision encoders, HSENet can seamlessly
perceive and transfer hybrid visual representations to LLM's semantic space,
facilitating accurate diagnostic text generation. Experimental results
demonstrate that our method achieves state-of-the-art performance in 3D
language-visual retrieval (39.85% of R@100, +5.96% gain), 3D medical report
generation (24.01% of BLEU-4, +8.01% gain), and 3D visual question answering
(73.60% of Major Class Accuracy, +1.99% gain), confirming its effectiveness.
Our code is available at https://github.com/YanzhaoShi/HSENet.

</details>


### [135] [DGAE: Diffusion-Guided Autoencoder for Efficient Latent Representation Learning](https://arxiv.org/abs/2506.09644)
*Dongxu Liu,Yuang Peng,Haomiao Tang,Yuwei Chen,Chunrui Han,Zheng Ge,Daxin Jiang,Mingxue Liao*

Main category: cs.CV

TL;DR: The paper introduces DGAE, a diffusion model-guided autoencoder, that addresses challenges in high spatial compression rates while reducing latent space size and maintaining state-of-the-art performance on image generation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the training instability caused by GANs in autoencoders under high compression ratios, while achieving efficient representation with a minimized latent space.

Method: DGAE uses a diffusion model in the decoder to guide the recovery of signals not fully captured by latent representations, improving spatial compression and mitigating performance issues.

Result: DGAE achieves state-of-the-art performance with a 2x smaller latent space and supports faster convergence of diffusion models in tasks such as ImageNet-1K image generation.

Conclusion: The proposed DGAE framework enhances the decoder's expressiveness, manages challenges of high spatial compression, and exemplifies competitive compact representations for image generative tasks.

Abstract: Autoencoders empower state-of-the-art image and video generative models by
compressing pixels into a latent space through visual tokenization. Although
recent advances have alleviated the performance degradation of autoencoders
under high compression ratios, addressing the training instability caused by
GAN remains an open challenge. While improving spatial compression, we also aim
to minimize the latent space dimensionality, enabling more efficient and
compact representations. To tackle these challenges, we focus on improving the
decoder's expressiveness. Concretely, we propose DGAE, which employs a
diffusion model to guide the decoder in recovering informative signals that are
not fully decoded from the latent representation. With this design, DGAE
effectively mitigates the performance degradation under high spatial
compression rates. At the same time, DGAE achieves state-of-the-art performance
with a 2x smaller latent space. When integrated with Diffusion Models, DGAE
demonstrates competitive performance on image generation for ImageNet-1K and
shows that this compact latent representation facilitates faster convergence of
the diffusion model.

</details>


### [136] [Self-Supervised Multi-Part Articulated Objects Modeling via Deformable Gaussian Splatting and Progressive Primitive Segmentation](https://arxiv.org/abs/2506.09663)
*Haowen Wang,Xiaoping Yuan,Zhao Jin,Zhen Zhao,Zhengping Che,Yousong Xue,Jin Tian,Yakun Huang,Jian Tang*

Main category: cs.CV

TL;DR: The paper proposes DeGSS, a framework for representing articulated objects as deformable 3D Gaussian fields, achieving unsupervised motion and part segmentation.


<details>
  <summary>Details</summary>
Motivation: Accurately modeling the 3D geometry and motion of articulated objects without human annotations, which current methods struggle to achieve for objects with multiple movable parts.

Method: Introduced DeGSS, encoding articulated objects as deformable 3D Gaussian fields, modeling interaction states as smooth deformations, and enabling unsupervised part segmentation and motion modeling.

Result: DeGSS showed superior accuracy and stability compared to existing methods on an enlarged synthetic benchmark and a new real-to-sim dataset, RS-Art.

Conclusion: DeGSS outperforms the state-of-the-art by providing a compact and unsupervised representation that enhances 3D modeling of articulated objects across geometry, appearance, and motion.

Abstract: Articulated objects are ubiquitous in everyday life, and accurate 3D
representations of their geometry and motion are critical for numerous
applications. However, in the absence of human annotation, existing approaches
still struggle to build a unified representation for objects that contain
multiple movable parts. We introduce DeGSS, a unified framework that encodes
articulated objects as deformable 3D Gaussian fields, embedding geometry,
appearance, and motion in one compact representation. Each interaction state is
modeled as a smooth deformation of a shared field, and the resulting
deformation trajectories guide a progressive coarse-to-fine part segmentation
that identifies distinct rigid components, all in an unsupervised manner. The
refined field provides a spatially continuous, fully decoupled description of
every part, supporting part-level reconstruction and precise modeling of their
kinematic relationships. To evaluate generalization and realism, we enlarge the
synthetic PartNet-Mobility benchmark and release RS-Art, a real-to-sim dataset
that pairs RGB captures with accurately reverse-engineered 3D models. Extensive
experiments demonstrate that our method outperforms existing methods in both
accuracy and stability.

</details>


### [137] [CINeMA: Conditional Implicit Neural Multi-Modal Atlas for a Spatio-Temporal Representation of the Perinatal Brain](https://arxiv.org/abs/2506.09668)
*Maik Dannecker,Vasiliki Sideri-Lampretsa,Sophie Starck,Angeline Mihailov,Mathieu Milh,Nadine Girard,Guillaume Auzias,Daniel Rueckert*

Main category: cs.CV

TL;DR: This paper introduces CINeMA, a novel framework for creating high-resolution brain atlases suitable for analyzing neurodevelopment in low-data settings.


<details>
  <summary>Details</summary>
Motivation: The study aims to address challenges in creating brain atlases for pathological cases where data is limited, especially during critical neurodevelopment stages.

Method: CINeMA constructs brain atlases in latent space, bypassing traditional image registration, enabling flexible conditioning on anatomical features, and reducing processing time from days to minutes.

Result: The method surpasses state-of-the-art techniques in accuracy, efficiency, and versatility, supporting tasks like tissue segmentation and age prediction.

Conclusion: CINeMA provides an advanced tool for brain research, enabling better modeling, synthetic data creation, and data augmentation, especially in combination with pathologies.

Abstract: Magnetic resonance imaging of fetal and neonatal brains reveals rapid
neurodevelopment marked by substantial anatomical changes unfolding within
days. Studying this critical stage of the developing human brain, therefore,
requires accurate brain models-referred to as atlases-of high spatial and
temporal resolution. To meet these demands, established traditional atlases and
recently proposed deep learning-based methods rely on large and comprehensive
datasets. This poses a major challenge for studying brains in the presence of
pathologies for which data remains scarce. We address this limitation with
CINeMA (Conditional Implicit Neural Multi-Modal Atlas), a novel framework for
creating high-resolution, spatio-temporal, multimodal brain atlases, suitable
for low-data settings. Unlike established methods, CINeMA operates in latent
space, avoiding compute-intensive image registration and reducing atlas
construction times from days to minutes. Furthermore, it enables flexible
conditioning on anatomical features including GA, birth age, and pathologies
like ventriculomegaly (VM) and agenesis of the corpus callosum (ACC). CINeMA
supports downstream tasks such as tissue segmentation and age prediction
whereas its generative properties enable synthetic data creation and
anatomically informed data augmentation. Surpassing state-of-the-art methods in
accuracy, efficiency, and versatility, CINeMA represents a powerful tool for
advancing brain research. We release the code and atlases at
https://github.com/m-dannecker/CINeMA.

</details>


### [138] [Reasoning Models Are More Easily Gaslighted Than You Think](https://arxiv.org/abs/2506.09677)
*Bin Zhu,Hailong Yin,Jingjing Chen,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: The paper evaluates the robustness of reasoning models against manipulative user inputs and introduces a new benchmark to test their susceptibility to gaslighting, revealing weaknesses in belief persistence.


<details>
  <summary>Details</summary>
Motivation: To address the lack of understanding on how reasoning models handle misleading user inputs and assess their robustness against manipulative scenarios.

Method: The authors systematically evaluated three state-of-the-art reasoning models across three benchmarks and introduced a new diagnostic benchmark, GaslightingBench-R, which is curated to probe susceptibility to gaslighting negation prompts.

Result: The evaluation demonstrated significant accuracy drops (25-29% on current benchmarks and over 53% on GaslightingBench-R) under manipulative conditions, revealing major vulnerabilities.

Conclusion: Reasoning models struggle to maintain correct answers under gaslighting prompts, exposing limitations in their ability to persist beliefs despite step-by-step reasoning mechanisms.

Abstract: Recent advances in reasoning-centric models promise improved robustness
through mechanisms such as chain-of-thought prompting and test-time scaling.
However, their ability to withstand misleading user input remains
underexplored. In this paper, we conduct a systematic evaluation of three
state-of-the-art reasoning models, i.e., OpenAI's o4-mini, Claude-3.7-Sonnet
and Gemini-2.5-Flash, across three multimodal benchmarks: MMMU, MathVista, and
CharXiv. Our evaluation reveals significant accuracy drops (25-29% on average)
following gaslighting negation prompts, indicating that even top-tier reasoning
models struggle to preserve correct answers under manipulative user feedback.
Built upon the insights of the evaluation and to further probe this
vulnerability, we introduce GaslightingBench-R, a new diagnostic benchmark
specifically designed to evaluate reasoning models' susceptibility to defend
their belief under gaslighting negation prompt. Constructed by filtering and
curating 1,025 challenging samples from the existing benchmarks,
GaslightingBench-R induces even more dramatic failures, with accuracy drops
exceeding 53% on average. Our findings reveal fundamental limitations in the
robustness of reasoning models, highlighting the gap between step-by-step
reasoning and belief persistence.

</details>


### [139] [Adding simple structure at inference improves Vision-Language Compositionality](https://arxiv.org/abs/2506.09691)
*Imanol Miranda,Ander Salaberria,Eneko Agirre,Gorka Azkune*

Main category: cs.CV

TL;DR: The paper introduces an inference-time technique to improve Vision-Language Models (VLMs) regarding compositionality in image-text retrieval tasks without requiring additional training.


<details>
  <summary>Details</summary>
Motivation: Existing dual encoder models like CLIP exhibit limitations in compositional understanding, hindering image-text retrieval performance. The paper aims to overcome this issue using inference-time methods.

Method: The authors propose dividing images into crops, extracting text segments capturing attributes and relations, aligning image crops with text segments via VLMs, and computing final similarity scores by aggregating individual matches.

Result: The proposed approach consistently enhances VLMs’ performance across both controlled and natural datasets, with remarkable gains particularly in attribute-object binding.

Conclusion: Inference-time techniques, specifically image cropping, significantly boost VLM performance while pointing to further areas for improvement in compositional retrieval tasks.

Abstract: Dual encoder Vision-Language Models (VLM) such as CLIP are widely used for
image-text retrieval tasks. However, those models struggle with
compositionality, showing a bag-of-words-like behavior that limits their
retrieval performance. Many different training approaches have been proposed to
improve the vision-language compositionality capabilities of those models. In
comparison, inference-time techniques have received little attention. In this
paper, we propose to add simple structure at inference, where, given an image
and a caption: i) we divide the image into different smaller crops, ii) we
extract text segments, capturing objects, attributes and relations, iii) using
a VLM, we find the image crops that better align with text segments obtaining
matches, and iv) we compute the final image-text similarity aggregating the
individual similarities of the matches. Based on various popular dual encoder
VLMs, we evaluate our approach in controlled and natural datasets for VL
compositionality. We find that our approach consistently improves the
performance of evaluated VLMs without any training, which shows the potential
of inference-time techniques. The results are especially good for
attribute-object binding as shown in the controlled dataset. As a result of an
extensive analysis: i) we show that processing image crops is actually
essential for the observed gains in performance, and ii) we identify specific
areas to further improve inference-time approaches.

</details>


### [140] [Towards Practical Alzheimer's Disease Diagnosis: A Lightweight and Interpretable Spiking Neural Model](https://arxiv.org/abs/2506.09695)
*Changwei Wu,Yifei Chen,Yuxin Du,Jinying Zong,Jie Dong,Mingxuan Liu,Yong Peng,Jin Fan,Feiwei Qin,Changmiao Wang*

Main category: cs.CV

TL;DR: This paper introduces FasterSNN, a hybrid spiking neural network designed for early Alzheimer's diagnosis using 3D MRI, which is both efficient and accurate.


<details>
  <summary>Details</summary>
Motivation: The diagnosis of Alzheimer's Disease at the mild cognitive impairment stage is crucial but often challenging due to subjective assessments and the costs of imaging. Current AI-based methods are energy-intensive, making them impractical for resource-limited applications.

Method: The authors developed FasterSNN, a hybrid spiking neural network architecture combining biologically-inspired LIF neurons with region-adaptive convolution and multi-scale spiking attention for sparse and efficient MRI processing.

Result: FasterSNN was evaluated on benchmark datasets and showed competitive diagnostic performance combined with enhanced efficiency and training stability.

Conclusion: FasterSNN offers a promising low-power and effective solution for Alzheimer's diagnostics, potentially enabling scalable screening in real-world settings.

Abstract: Early diagnosis of Alzheimer's Disease (AD), especially at the mild cognitive
impairment (MCI) stage, is vital yet hindered by subjective assessments and the
high cost of multimodal imaging modalities. Although deep learning methods
offer automated alternatives, their energy inefficiency and computational
demands limit real-world deployment, particularly in resource-constrained
settings. As a brain-inspired paradigm, spiking neural networks (SNNs) are
inherently well-suited for modeling the sparse, event-driven patterns of neural
degeneration in AD, offering a promising foundation for interpretable and
low-power medical diagnostics. However, existing SNNs often suffer from weak
expressiveness and unstable training, which restrict their effectiveness in
complex medical tasks. To address these limitations, we propose FasterSNN, a
hybrid neural architecture that integrates biologically inspired LIF neurons
with region-adaptive convolution and multi-scale spiking attention. This design
enables sparse, efficient processing of 3D MRI while preserving diagnostic
accuracy. Experiments on benchmark datasets demonstrate that FasterSNN achieves
competitive performance with substantially improved efficiency and stability,
supporting its potential for practical AD screening. Our source code is
available at https://github.com/wuchangw/FasterSNN.

</details>


### [141] [CHIP: A multi-sensor dataset for 6D pose estimation of chairs in industrial settings](https://arxiv.org/abs/2506.09699)
*Mattia Nardon,Mikel Mujika Agirre,Ander González Tomé,Daniel Sedano Algarabel,Josep Rueda Collell,Ana Paola Caro,Andrea Caraffa,Fabio Poiesi,Paul Ian Chippendale,Davide Boscaini*

Main category: cs.CV

TL;DR: This paper introduces the CHIP dataset for evaluating 6D pose estimation of chairs in real-world industrial robotic manipulation scenarios, featuring challenges like fine-grained distractor objects and severe occlusions.


<details>
  <summary>Details</summary>
Motivation: Existing 6D pose estimation benchmarks inadequately address the challenges in industrial settings, as they focus on simpler environments like household or artificial setups.

Method: The authors created CHIP, a dataset of 77,811 RGBD images capturing seven distinct chairs in real-world industrial settings with annotations based on the robot's kinematics for benchmarking pose estimation methods.

Result: The benchmarking revealed significant performance gaps in 6D pose estimation methods when dealing with the dataset's challenging features, such as occlusions and fine-grained distinctions.

Conclusion: The CHIP dataset introduces novel, realistic challenges to advance research in 6D pose estimation, showing that existing methods need substantial improvement. The dataset will be made publicly available.

Abstract: Accurate 6D pose estimation of complex objects in 3D environments is
essential for effective robotic manipulation. Yet, existing benchmarks fall
short in evaluating 6D pose estimation methods under realistic industrial
conditions, as most datasets focus on household objects in domestic settings,
while the few available industrial datasets are limited to artificial setups
with objects placed on tables. To bridge this gap, we introduce CHIP, the first
dataset designed for 6D pose estimation of chairs manipulated by a robotic arm
in a real-world industrial environment. CHIP includes seven distinct chairs
captured using three different RGBD sensing technologies and presents unique
challenges, such as distractor objects with fine-grained differences and severe
occlusions caused by the robotic arm and human operators. CHIP comprises 77,811
RGBD images annotated with ground-truth 6D poses automatically derived from the
robot's kinematics, averaging 11,115 annotations per chair. We benchmark CHIP
using three zero-shot 6D pose estimation methods, assessing performance across
different sensor types, localization priors, and occlusion levels. Results show
substantial room for improvement, highlighting the unique challenges posed by
the dataset. CHIP will be publicly released.

</details>


### [142] [Non-Contact Health Monitoring During Daily Personal Care Routines](https://arxiv.org/abs/2506.09718)
*Xulin Ma,Jiankai Tang,Zhang Jiang,Songqin Cheng,Yuanchun Shi,Dong LI,Xin Liu,Daniel McDuff,Xiaojing Liu,Yuntao Wang*

Main category: cs.CV

TL;DR: This paper introduces LADH, a dataset for improving remote photoplethysmography (rPPG) in challenging conditions like high-altitude settings, using synchronized RGB and infrared video inputs.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of accurate long-term non-contact physiological monitoring in high-altitude environments with lighting changes, occlusions, and dynamic postures.

Method: Development of a novel dataset (LADH) featuring synchronized RGB and IR video inputs from 21 participants, paired with ground-truth physiological signals. Utilized multi-task learning to process and improve prediction accuracy.

Result: The method achieved a mean absolute error of 4.99 BPM in heart rate estimation and demonstrated improvements in monitoring multiple physiological markers using RGB and IR video fusion.

Conclusion: Using synchronized RGB and IR inputs combined with multi-task learning improves the accuracy and robustness of rPPG systems under challenging conditions. The dataset is made publicly available.

Abstract: Remote photoplethysmography (rPPG) enables non-contact, continuous monitoring
of physiological signals and offers a practical alternative to traditional
health sensing methods. Although rPPG is promising for daily health monitoring,
its application in long-term personal care scenarios, such as mirror-facing
routines in high-altitude environments, remains challenging due to ambient
lighting variations, frequent occlusions from hand movements, and dynamic
facial postures. To address these challenges, we present LADH (Long-term
Altitude Daily Health), the first long-term rPPG dataset containing 240
synchronized RGB and infrared (IR) facial videos from 21 participants across
five common personal care scenarios, along with ground-truth PPG, respiration,
and blood oxygen signals. Our experiments demonstrate that combining RGB and IR
video inputs improves the accuracy and robustness of non-contact physiological
monitoring, achieving a mean absolute error (MAE) of 4.99 BPM in heart rate
estimation. Furthermore, we find that multi-task learning enhances performance
across multiple physiological indicators simultaneously. Dataset and code are
open at https://github.com/McJackTang/FusionVitals.

</details>


### [143] [The Four Color Theorem for Cell Instance Segmentation](https://arxiv.org/abs/2506.09724)
*Ye Zhang,Yu Zhou,Yifeng Wang,Jun Xiao,Ziyue Wang,Yongbing Zhang,Jianxu Chen*

Main category: cs.CV

TL;DR: The paper introduces a novel method for cell instance segmentation inspired by the four-color theorem, transforming the problem into simpler semantic segmentation while achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Accurately segmenting tightly connected cells in biomedical images remains challenging, necessitating a balance between model performance and computational efficiency.

Method: A four-color encoding scheme represents adjacent cells with distinct labels, coupled with an asymptotic training strategy to stabilize the model.

Result: The proposed approach performs state-of-the-art across various segmentation tasks, demonstrating robustness and efficiency.

Conclusion: The method simplifies instance segmentation into a semantic segmentation problem, proving effective in addressing challenges related to tightly touching cells.

Abstract: Cell instance segmentation is critical to analyzing biomedical images, yet
accurately distinguishing tightly touching cells remains a persistent
challenge. Existing instance segmentation frameworks, including
detection-based, contour-based, and distance mapping-based approaches, have
made significant progress, but balancing model performance with computational
efficiency remains an open problem. In this paper, we propose a novel cell
instance segmentation method inspired by the four-color theorem. By
conceptualizing cells as countries and tissues as oceans, we introduce a
four-color encoding scheme that ensures adjacent instances receive distinct
labels. This reformulation transforms instance segmentation into a constrained
semantic segmentation problem with only four predicted classes, substantially
simplifying the instance differentiation process. To solve the training
instability caused by the non-uniqueness of four-color encoding, we design an
asymptotic training strategy and encoding transformation method. Extensive
experiments on various modes demonstrate our approach achieves state-of-the-art
performance. The code is available at https://github.com/zhangye-zoe/FCIS.

</details>


### [144] [MPFNet: A Multi-Prior Fusion Network with a Progressive Training Strategy for Micro-Expression Recognition](https://arxiv.org/abs/2506.09735)
*Chuang Ma,Shaokai Zhao,Dongdong Zhou,Yu Pei,Zhiguo Luo,Liang Xie,Ye Yan,Erwei Yin*

Main category: cs.CV

TL;DR: The paper introduces MPFNet, a novel MER system using multi-source prior knowledge and specialized encoders to improve accuracy and category balance, achieving state-of-the-art results on SMIC and SAMM datasets.


<details>
  <summary>Details</summary>
Motivation: Address challenges in micro-expression recognition (MER), such as its brief duration and low intensity, by incorporating multi-source prior knowledge to improve task performance.

Method: Develop Multi-Prior Fusion Network (MPFNet) with two encoders (GFE and AFE), based on I3D frameworks with Coordinate Attention. Propose progressive training and variants representing cognitive processing modes (parallel and hierarchical).

Result: MPFNet achieves accuracies of 0.811, 0.924, and 0.857 on SMIC, CASME II, and SAMM datasets, outperforming prior methods to set state-of-the-art benchmarks.

Conclusion: Integrating multi-source prior knowledge using MPFNet enhances MER performance comprehensively, showing promising results for advancing affective computing.

Abstract: Micro-expression recognition (MER), a critical subfield of affective
computing, presents greater challenges than macro-expression recognition due to
its brief duration and low intensity. While incorporating prior knowledge has
been shown to enhance MER performance, existing methods predominantly rely on
simplistic, singular sources of prior knowledge, failing to fully exploit
multi-source information. This paper introduces the Multi-Prior Fusion Network
(MPFNet), leveraging a progressive training strategy to optimize MER tasks. We
propose two complementary encoders: the Generic Feature Encoder (GFE) and the
Advanced Feature Encoder (AFE), both based on Inflated 3D ConvNets (I3D) with
Coordinate Attention (CA) mechanisms, to improve the model's ability to capture
spatiotemporal and channel-specific features. Inspired by developmental
psychology, we present two variants of MPFNet--MPFNet-P and
MPFNet-C--corresponding to two fundamental modes of infant cognitive
development: parallel and hierarchical processing. These variants enable the
evaluation of different strategies for integrating prior knowledge. Extensive
experiments demonstrate that MPFNet significantly improves MER accuracy while
maintaining balanced performance across categories, achieving accuracies of
0.811, 0.924, and 0.857 on the SMIC, CASME II, and SAMM datasets, respectively.
To the best of our knowledge, our approach achieves state-of-the-art
performance on the SMIC and SAMM datasets.

</details>


### [145] [Vision Matters: Simple Visual Perturbations Can Boost Multimodal Math Reasoning](https://arxiv.org/abs/2506.09736)
*Yuting Li,Lai Wei,Kaipeng Zheng,Jingyuan Huang,Linghe Kong,Lichao Sun,Weiran Huang*

Main category: cs.CV

TL;DR: This paper finds that language models using captions can rival multimodal models. A visual perturbation framework is proposed to enhance visual reasoning.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of current multimodal large language models that fail to integrate visual inputs effectively during reasoning.

Method: A visual perturbation framework using distractor concatenation, dominance-preserving mixup, and random rotation integrated into post-training pipelines.

Result: The approach improved mathematical reasoning performance significantly and performed competitively among open-source 7B RL-tuned models.

Conclusion: Visual perturbation is essential for multimodal reasoning, emphasizing the importance of perceptual robustness for better results.

Abstract: Despite the rapid progress of multimodal large language models (MLLMs), they
have largely overlooked the importance of visual processing. In a simple yet
revealing experiment, we interestingly find that language-only models, when
provided with image captions, can achieve comparable or even better performance
than MLLMs that consume raw visual inputs. This suggests that current MLLMs may
generate accurate visual descriptions but fail to effectively integrate them
during reasoning. Motivated by this, we propose a simple visual perturbation
framework that enhances perceptual robustness without requiring algorithmic
modifications or additional training data. Our approach introduces three
targeted perturbations: distractor concatenation, dominance-preserving mixup,
and random rotation, that can be easily integrated into existing post-training
pipelines including SFT, DPO, and GRPO. Through extensive experiments across
multiple datasets, we demonstrate consistent improvements in mathematical
reasoning performance, with gains comparable to those achieved through
algorithmic changes. Additionally, we achieve competitive performance among
open-source 7B RL-tuned models by training Qwen2.5-VL-7B with visual
perturbation. Through comprehensive ablation studies, we analyze the
effectiveness of different perturbation strategies, revealing that each
perturbation type contributes uniquely to different aspects of visual
reasoning. Our findings highlight the critical role of visual perturbation in
multimodal mathematical reasoning: better reasoning begins with better seeing.
Our code is available at https://github.com/YutingLi0606/Vision-Matters.

</details>


### [146] [ELBO-T2IAlign: A Generic ELBO-Based Method for Calibrating Pixel-level Text-Image Alignment in Diffusion Models](https://arxiv.org/abs/2506.09740)
*Qin Zhou,Zhiyang Zhang,Jinglong Wang,Xiaobin Li,Jing Zhang,Qian Yu,Lu Sheng,Dong Xu*

Main category: cs.CV

TL;DR: This paper investigates pixel-text misalignment in diffusion models and introduces a method (ELBO-T2IAlign) to improve alignment without additional training.


<details>
  <summary>Details</summary>
Motivation: Current methods assume perfect text-image alignment in diffusion models, which is often inaccurate, impacting downstream tasks like segmentation and image editing.

Method: Propose ELBO-T2IAlign, a training-free calibration method based on evidence lower bound (ELBO) to improve pixel-text alignment in various diffusion models.

Result: Experiments on benchmark datasets demonstrate the effectiveness of the proposed calibration method on tasks like image segmentation and generation.

Conclusion: ELBO-T2IAlign effectively addresses pixel-text misalignment in diffusion models, enhancing their utility in various applications without requiring additional training.

Abstract: Diffusion models excel at image generation. Recent studies have shown that
these models not only generate high-quality images but also encode text-image
alignment information through attention maps or loss functions. This
information is valuable for various downstream tasks, including segmentation,
text-guided image editing, and compositional image generation. However, current
methods heavily rely on the assumption of perfect text-image alignment in
diffusion models, which is not the case. In this paper, we propose using
zero-shot referring image segmentation as a proxy task to evaluate the
pixel-level image and class-level text alignment of popular diffusion models.
We conduct an in-depth analysis of pixel-text misalignment in diffusion models
from the perspective of training data bias. We find that misalignment occurs in
images with small sized, occluded, or rare object classes. Therefore, we
propose ELBO-T2IAlign, a simple yet effective method to calibrate pixel-text
alignment in diffusion models based on the evidence lower bound (ELBO) of
likelihood. Our method is training-free and generic, eliminating the need to
identify the specific cause of misalignment and works well across various
diffusion model architectures. Extensive experiments on commonly used benchmark
datasets on image segmentation and generation have verified the effectiveness
of our proposed calibration approach.

</details>


### [147] [Class Similarity-Based Multimodal Classification under Heterogeneous Category Sets](https://arxiv.org/abs/2506.09745)
*Yangrui Zhu,Junhua Bao,Yipan Wei,Yapeng Li,Bo Du*

Main category: cs.CV

TL;DR: The paper introduces a new setting called Multi-Modal Heterogeneous Category-Set Learning (MMHCL), proposing a Class Similarity-based Cross-modal Fusion model (CSCF) to better utilize multimodal data in real-world scenarios with inconsistent category sets.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge where traditional multimodal methods fail in real-world scenarios due to heterogeneous category distribution across modalities.

Method: The proposed method, CSCF, aligns features to a shared semantic space for unseen class handling, utilizes uncertainty estimation for decision fusion, and refines predictions using class similarity.

Result: Experimental results demonstrate that CSCF significantly outperforms state-of-the-art methods on multiple benchmark datasets in MMHCL tasks.

Conclusion: CSCF provides an effective framework for handling inconsistent category sets across multimodal datasets, improving cross-modal decision fusion and prediction accuracy.

Abstract: Existing multimodal methods typically assume that different modalities share
the same category set. However, in real-world applications, the category
distributions in multimodal data exhibit inconsistencies, which can hinder the
model's ability to effectively utilize cross-modal information for recognizing
all categories. In this work, we propose the practical setting termed
Multi-Modal Heterogeneous Category-set Learning (MMHCL), where models are
trained in heterogeneous category sets of multi-modal data and aim to recognize
complete classes set of all modalities during test. To effectively address this
task, we propose a Class Similarity-based Cross-modal Fusion model (CSCF).
Specifically, CSCF aligns modality-specific features to a shared semantic space
to enable knowledge transfer between seen and unseen classes. It then selects
the most discriminative modality for decision fusion through uncertainty
estimation. Finally, it integrates cross-modal information based on class
similarity, where the auxiliary modality refines the prediction of the dominant
one. Experimental results show that our method significantly outperforms
existing state-of-the-art (SOTA) approaches on multiple benchmark datasets,
effectively addressing the MMHCL task.

</details>


### [148] [Inverting Black-Box Face Recognition Systems via Zero-Order Optimization in Eigenface Space](https://arxiv.org/abs/2506.09777)
*Anton Razzhigaev,Matvey Mikhalchuk,Klim Kireev,Igor Udovichenko,Andrey Kuznetsov,Aleksandr Petiushko*

Main category: cs.CV

TL;DR: The study introduces DarkerBB, a method for reconstructing color facial images using only similarity scores from black-box models, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To address the privacy vulnerabilities posed by facial reconstruction when only similarity scores are available, especially in black-box recognition models.

Method: Proposes DarkerBB, which leverages zero-order optimization in a PCA-derived eigenface space to reconstruct faces using limited information.

Result: DarkerBB achieves state-of-the-art verification accuracy on LFW, AgeDB-30, and CFP-FP benchmarks under the similarity-only setting, with efficient query usage.

Conclusion: Even with highly restricted data access (only similarity scores), DarkerBB effectively reconstructs accurate color images, exposing privacy concerns in facial recognition systems.

Abstract: Reconstructing facial images from black-box recognition models poses a
significant privacy threat. While many methods require access to embeddings, we
address the more challenging scenario of model inversion using only similarity
scores. This paper introduces DarkerBB, a novel approach that reconstructs
color faces by performing zero-order optimization within a PCA-derived
eigenface space. Despite this highly limited information, experiments on LFW,
AgeDB-30, and CFP-FP benchmarks demonstrate that DarkerBB achieves
state-of-the-art verification accuracies in the similarity-only setting, with
competitive query efficiency.

</details>


### [149] [Q-SAM2: Accurate Quantization for Segment Anything Model 2](https://arxiv.org/abs/2506.09782)
*Nicola Farronato,Florian Scheidegger,Mattia Rigotti,Cristiano Malossi,Michele Magno,Haotong Qin*

Main category: cs.CV

TL;DR: This paper introduces Q-SAM2 to streamline the efficiency of the SAM2 model for image and video segmentation, through advanced low-bit quantization techniques.


<details>
  <summary>Details</summary>
Motivation: The high computational and memory requirements of SAM2 limit its applicability in resource-constrained environments.

Method: Two key techniques are proposed: a linear layer calibration method for weight distribution adjustment, and a Quantization-Aware Training pipeline to improve network adaptability to quantization.

Result: Q-SAM2 surpasses existing quantization methods in accuracy, especially for ultra-low (2-bit) quantization, and achieves significant accuracy improvements with post-training quantization techniques.

Conclusion: Q-SAM2 proves to be a highly efficient and accurate approach for resource-constrained applications of SAM2, making it a significant advancement in quantization methods.

Abstract: The Segment Anything Model 2 (SAM2) has gained significant attention as a
foundational approach for promptable image and video segmentation. However, its
expensive computational and memory consumption poses a severe challenge for its
application in resource-constrained scenarios. In this paper, we propose an
accurate low-bit quantization method for efficient SAM2, termed Q-SAM2. To
address the performance degradation caused by the singularities in weight and
activation distributions during quantization, Q-SAM2 introduces two novel
technical contributions. We first introduce a linear layer calibration method
for low-bit initialization of SAM2, which minimizes the Frobenius norm over a
small image batch to reposition weight distributions for improved quantization.
We then propose a Quantization-Aware Training (QAT) pipeline that applies
clipping to suppress outliers and allows the network to adapt to quantization
thresholds during training. Our comprehensive experiments demonstrate that
Q-SAM2 allows for highly accurate inference while substantially improving
efficiency. Both quantitative and visual results show that our Q-SAM2 surpasses
existing state-of-the-art general quantization schemes, especially for
ultra-low 2-bit quantization. While designed for quantization-aware training,
our proposed calibration technique also proves effective in post-training
quantization, achieving up to a 66% mIoU accuracy improvement over
non-calibrated models.

</details>


### [150] [Accurate and efficient zero-shot 6D pose estimation with frozen foundation models](https://arxiv.org/abs/2506.09784)
*Andrea Caraffa,Davide Boscaini,Fabio Poiesi*

Main category: cs.CV

TL;DR: FreeZeV2 is a training-free method for 6D pose estimation of unseen objects using pre-trained geometric and vision foundation models, improving accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of generalizing 6D pose estimation of unseen objects without relying on computationally expensive task-specific training.

Method: FreeZeV2 leverages sparse feature extraction, feature-aware scoring, and a modular design supporting segmentation model ensembles to enhance performance.

Result: FreeZeV2 achieves higher accuracy and efficiency, including an 8x speedup and 5% accuracy improvement over FreeZe, and further gains 8% accuracy with ensembles.

Conclusion: FreeZeV2 provides state-of-the-art results in the BOP Benchmark and redefines efficient 6D pose estimation, awarded Best Overall Method at BOP Challenge 2024.

Abstract: Estimating the 6D pose of objects from RGBD data is a fundamental problem in
computer vision, with applications in robotics and augmented reality. A key
challenge is achieving generalization to novel objects that were not seen
during training. Most existing approaches address this by scaling up training
on synthetic data tailored to the task, a process that demands substantial
computational resources. But is task-specific training really necessary for
accurate and efficient 6D pose estimation of novel objects? To answer No!, we
introduce FreeZeV2, the second generation of FreeZe: a training-free method
that achieves strong generalization to unseen objects by leveraging geometric
and vision foundation models pre-trained on unrelated data. FreeZeV2 improves
both accuracy and efficiency over FreeZe through three key contributions: (i) a
sparse feature extraction strategy that reduces inference-time computation
without sacrificing accuracy; (ii) a feature-aware scoring mechanism that
improves both pose selection during RANSAC-based 3D registration and the final
ranking of pose candidates; and (iii) a modular design that supports ensembles
of instance segmentation models, increasing robustness to segmentation masks
errors. We evaluate FreeZeV2 on the seven core datasets of the BOP Benchmark,
where it establishes a new state-of-the-art in 6D pose estimation of unseen
objects. When using the same segmentation masks, FreeZeV2 achieves a remarkable
8x speedup over FreeZe while also improving accuracy by 5%. When using
ensembles of segmentation models, FreeZeV2 gains an additional 8% in accuracy
while still running 2.5x faster than FreeZe. FreeZeV2 was awarded Best Overall
Method at the BOP Challenge 2024.

</details>


### [151] [DreamCS: Geometry-Aware Text-to-3D Generation with Unpaired 3D Reward Supervision](https://arxiv.org/abs/2506.09814)
*Xiandong Zou,Ruihao Xia,Hongsong Wang,Pan Zhou*

Main category: cs.CV

TL;DR: The research presents a novel framework, DreamCS, that improves text-to-3D generation by aligning 3D assets with human preferences through a newly introduced unpaired 3D preference dataset and a specialized reward model.


<details>
  <summary>Details</summary>
Motivation: Existing methods for text-to-3D generation fail to align well with human preferences and often produce geometric artifacts. Current solutions rely on 2D-biased reward models trained with hard-to-collect paired 2D images.

Method: The study introduces 3D-MeshPref, a large unpaired 3D preference dataset annotated by a language model and refined by human evaluators. It also proposes RewardCS, a new reward model trained with a novel Cauchy-Schwarz divergence objective to capture 3D human preferences. DreamCS integrates this model into text-to-3D pipelines.

Result: DreamCS achieves superior performance compared to earlier approaches by generating geometrically accurate and human-preferred 3D assets.

Conclusion: The proposed framework resolves limitations in previous methods by improving preference alignment in text-to-3D generation through an innovative unpaired dataset and a directly-trained reward model. It provides state-of-the-art results and its code/models will be publicly available.

Abstract: While text-to-3D generation has attracted growing interest, existing methods
often struggle to produce 3D assets that align well with human preferences.
Current preference alignment techniques for 3D content typically rely on
hardly-collected preference-paired multi-view 2D images to train 2D reward
models, when then guide 3D generation -- leading to geometric artifacts due to
their inherent 2D bias. To address these limitations, we construct 3D-MeshPref,
the first large-scale unpaired 3D preference dataset, featuring diverse 3D
meshes annotated by a large language model and refined by human evaluators. We
then develop RewardCS, the first reward model trained directly on unpaired
3D-MeshPref data using a novel Cauchy-Schwarz divergence objective, enabling
effective learning of human-aligned 3D geometric preferences without requiring
paired comparisons. Building on this, we propose DreamCS, a unified framework
that integrates RewardCS into text-to-3D pipelines -- enhancing both implicit
and explicit 3D generation with human preference feedback. Extensive
experiments show DreamCS outperforms prior methods, producing 3D assets that
are both geometrically faithful and human-preferred. Code and models will be
released publicly.

</details>


### [152] [MMME: A Spontaneous Multi-Modal Micro-Expression Dataset Enabling Visual-Physiological Fusion](https://arxiv.org/abs/2506.09834)
*Chuang Maa,Yu Peia,Jianhang Zhanga,Shaokai Zhaoa,Bowen Jib,Liang Xiea,Ye Yana,Erwei Yin*

Main category: cs.CV

TL;DR: This paper introduces the MMME dataset, which combines facial micro-expressions (MEs) with synchronized physiological signals (PS) to boost analysis through multimodal fusion.


<details>
  <summary>Details</summary>
Motivation: Current micro-expression (ME) research focuses heavily on visual modalities and neglects physiological signals, resulting in suboptimal recognition and spotting performance.

Method: The study develops the MMME dataset, enabling synchronized collection of facial ME, EEG, and six peripheral physiological signals (PPG, RSP, SKT, EDA, ECG). Extensive validation experiments were conducted.

Result: The dataset includes 634 MEs, 2,841 macro-expressions, and 2,890 trials with synchronized multimodal signals. Validation shows enhanced performance in ME recognition and spotting tasks when visual and physiological signals are integrated.

Conclusion: MMME represents a paradigm shift by enabling multimodal fusion for ME analysis, offering critical data for exploring neural mechanisms and visual-physiological synergy, and advancing the field beyond traditional single-modality approaches.

Abstract: Micro-expressions (MEs) are subtle, fleeting nonverbal cues that reveal an
individual's genuine emotional state. Their analysis has attracted considerable
interest due to its promising applications in fields such as healthcare,
criminal investigation, and human-computer interaction. However, existing ME
research is limited to single visual modality, overlooking the rich emotional
information conveyed by other physiological modalities, resulting in ME
recognition and spotting performance far below practical application needs.
Therefore, exploring the cross-modal association mechanism between ME visual
features and physiological signals (PS), and developing a multimodal fusion
framework, represents a pivotal step toward advancing ME analysis. This study
introduces a novel ME dataset, MMME, which, for the first time, enables
synchronized collection of facial action signals (MEs), central nervous system
signals (EEG), and peripheral PS (PPG, RSP, SKT, EDA, and ECG). By overcoming
the constraints of existing ME corpora, MMME comprises 634 MEs, 2,841
macro-expressions (MaEs), and 2,890 trials of synchronized multimodal PS,
establishing a robust foundation for investigating ME neural mechanisms and
conducting multimodal fusion-based analyses. Extensive experiments validate the
dataset's reliability and provide benchmarks for ME analysis, demonstrating
that integrating MEs with PS significantly enhances recognition and spotting
performance. To the best of our knowledge, MMME is the most comprehensive ME
dataset to date in terms of modality diversity. It provides critical data
support for exploring the neural mechanisms of MEs and uncovering the
visual-physiological synergistic effects, driving a paradigm shift in ME
research from single-modality visual analysis to multimodal fusion. The dataset
will be publicly available upon acceptance of this paper.

</details>


### [153] [DynaSplat: Dynamic-Static Gaussian Splatting with Hierarchical Motion Decomposition for Scene Reconstruction](https://arxiv.org/abs/2506.09836)
*Junli Deng,Ping Shi,Qipei Li,Jinyang Guo*

Main category: cs.CV

TL;DR: DynaSplat enhances Gaussian Splatting for dynamic scene reconstruction by integrating motion-focused spatial representation and hierarchical motion modeling.


<details>
  <summary>Details</summary>
Motivation: To address the limitations in existing methods dealing with complex real-world environments in dynamic scene reconstruction.

Method: DynaSplat separates dynamic-static elements using deformation offset statistics and 2D motion flow, applies hierarchical motion modeling for capturing global and local motions, and uses physically-based opacity estimation for visual coherence.

Result: Achieves superior accuracy and realism in dynamic scene reconstructions compared to state-of-the-art approaches.

Conclusion: DynaSplat offers an intuitive, efficient, and compact solution for reconstructing dynamic scenes with high visual fidelity and robustness to occlusions.

Abstract: Reconstructing intricate, ever-changing environments remains a central
ambition in computer vision, yet existing solutions often crumble before the
complexity of real-world dynamics. We present DynaSplat, an approach that
extends Gaussian Splatting to dynamic scenes by integrating dynamic-static
separation and hierarchical motion modeling. First, we classify scene elements
as static or dynamic through a novel fusion of deformation offset statistics
and 2D motion flow consistency, refining our spatial representation to focus
precisely where motion matters. We then introduce a hierarchical motion
modeling strategy that captures both coarse global transformations and
fine-grained local movements, enabling accurate handling of intricate,
non-rigid motions. Finally, we integrate physically-based opacity estimation to
ensure visually coherent reconstructions, even under challenging occlusions and
perspective shifts. Extensive experiments on challenging datasets reveal that
DynaSplat not only surpasses state-of-the-art alternatives in accuracy and
realism but also provides a more intuitive, compact, and efficient route to
dynamic scene reconstruction.

</details>


### [154] [Learning to Align: Addressing Character Frequency Distribution Shifts in Handwritten Text Recognition](https://arxiv.org/abs/2506.09846)
*Panagiotis Kaliosis,John Pavlopoulos*

Main category: cs.CV

TL;DR: The paper introduces a novel loss function utilizing Wasserstein distance to enhance handwritten text recognition's robustness and accuracy under temporal and contextual variations.


<details>
  <summary>Details</summary>
Motivation: Handwritten text recognition models often struggle with contextual and temporal variability in character sets and distributions. Current models trained on diverse corpora underperform on specific subsets.

Method: The method employs a loss function based on the Wasserstein distance to align character frequency distributions in predictions with target distributions from training data. It also uses this alignment in a scoring function for inference without retraining.

Result: Experimental results show that the proposed method improves generalization and performance across various datasets and architectures.

Conclusion: The approach effectively addresses dataset shifts in handwritten text recognition, boosting accuracy and demonstrating utility both during training and inference. The code is publicly available for further application.

Abstract: Handwritten text recognition aims to convert visual input into
machine-readable text, and it remains challenging due to the evolving and
context-dependent nature of handwriting. Character sets change over time, and
character frequency distributions shift across historical periods or regions,
often causing models trained on broad, heterogeneous corpora to underperform on
specific subsets. To tackle this, we propose a novel loss function that
incorporates the Wasserstein distance between the character frequency
distribution of the predicted text and a target distribution empirically
derived from training data. By penalizing divergence from expected
distributions, our approach enhances both accuracy and robustness under
temporal and contextual intra-dataset shifts. Furthermore, we demonstrate that
character distribution alignment can also improve existing models at inference
time without requiring retraining by integrating it as a scoring function in a
guided decoding scheme. Experimental results across multiple datasets and
architectures confirm the effectiveness of our method in boosting
generalization and performance. We open source our code at
https://github.com/pkaliosis/fada.

</details>


### [155] [IntPhys 2: Benchmarking Intuitive Physics Understanding In Complex Synthetic Environments](https://arxiv.org/abs/2506.09849)
*Florian Bordes,Quentin Garrido,Justine T Kao,Adina Williams,Michael Rabbat,Emmanuel Dupoux*

Main category: cs.CV

TL;DR: IntPhys 2 introduces a video benchmark to assess deep learning models' intuitive physics. Models generally struggle compared to human abilities.


<details>
  <summary>Details</summary>
Motivation: To evaluate and advance deep learning models' understanding of intuitive physics, inspired by principles observed in human early childhood development.

Method: The benchmark uses controlled virtual environments and the violation of expectation framework to test models against four intuitive physics principles: Permanence, Immutability, Spatio-Temporal Continuity, and Solidity.

Result: State-of-the-art models showcased basic visual understanding but struggled with intuitive physics, performing at chance levels (~50%), unlike humans who performed nearly perfectly.

Conclusion: A significant gap exists between deep learning models and human-like intuitive physics understanding. This calls for better architectures and training strategies to bridge the gap.

Abstract: We present IntPhys 2, a video benchmark designed to evaluate the intuitive
physics understanding of deep learning models. Building on the original IntPhys
benchmark, IntPhys 2 focuses on four core principles related to macroscopic
objects: Permanence, Immutability, Spatio-Temporal Continuity, and Solidity.
These conditions are inspired by research into intuitive physical understanding
emerging during early childhood. IntPhys 2 offers a comprehensive suite of
tests, based on the violation of expectation framework, that challenge models
to differentiate between possible and impossible events within controlled and
diverse virtual environments. Alongside the benchmark, we provide performance
evaluations of several state-of-the-art models. Our findings indicate that
while these models demonstrate basic visual understanding, they face
significant challenges in grasping intuitive physics across the four principles
in complex scenes, with most models performing at chance levels (50%), in stark
contrast to human performance, which achieves near-perfect accuracy. This
underscores the gap between current models and human-like intuitive physics
understanding, highlighting the need for advancements in model architectures
and training methodologies.

</details>


### [156] [Leveraging Depth and Language for Open-Vocabulary Domain-Generalized Semantic Segmentation](https://arxiv.org/abs/2506.09881)
*Siyu Chen,Ting Han,Chengzheng Fu,Changshe Zhang,Chaolei Wang,Jinhe Su,Guorong Cai,Meiliu Wu*

Main category: cs.CV

TL;DR: The paper introduces Vireo, a framework for Open-Vocabulary Domain-Generalized Semantic Segmentation (OV-DGSS), achieving state-of-the-art performance in handling unseen categories and domains for semantic segmentation.


<details>
  <summary>Details</summary>
Motivation: To develop a unified solution for semantic segmentation that can handle unseen categories and domains, making it robust in real-world scenarios such as autonomous driving in challenging environments.

Method: The paper proposes Vireo, leveraging frozen Visual Foundation Models (VFMs) and Depth VFMs to extract domain-invariant structural features. Three components are introduced: GeoText Prompts for aligning geometry with language, Coarse Mask Prior Embedding (CMPE) for better gradient flow, and the Domain-Open-Vocabulary Vector Embedding Head (DOV-VEH) for refined predictions.

Result: Vireo demonstrates groundbreaking performance in both domain generalization and open-vocabulary recognition, significantly outperforming existing methods.

Conclusion: The Vireo framework unifies Open-Vocabulary Semantic Segmentation (OVSS) and Domain Generalization (DGSS), providing a scalable, state-of-the-art solution for robust semantic segmentation in dynamic environments.

Abstract: Open-Vocabulary semantic segmentation (OVSS) and domain generalization in
semantic segmentation (DGSS) highlight a subtle complementarity that motivates
Open-Vocabulary Domain-Generalized Semantic Segmentation (OV-DGSS). OV-DGSS
aims to generate pixel-level masks for unseen categories while maintaining
robustness across unseen domains, a critical capability for real-world
scenarios such as autonomous driving in adverse conditions. We introduce Vireo,
a novel single-stage framework for OV-DGSS that unifies the strengths of OVSS
and DGSS for the first time. Vireo builds upon the frozen Visual Foundation
Models (VFMs) and incorporates scene geometry via Depth VFMs to extract
domain-invariant structural features. To bridge the gap between visual and
textual modalities under domain shift, we propose three key components: (1)
GeoText Prompts, which align geometric features with language cues and
progressively refine VFM encoder representations; (2) Coarse Mask Prior
Embedding (CMPE) for enhancing gradient flow for faster convergence and
stronger textual influence; and (3) the Domain-Open-Vocabulary Vector Embedding
Head (DOV-VEH), which fuses refined structural and semantic features for robust
prediction. Comprehensive evaluation on these components demonstrates the
effectiveness of our designs. Our proposed Vireo achieves the state-of-the-art
performance and surpasses existing methods by a large margin in both domain
generalization and open-vocabulary recognition, offering a unified and scalable
solution for robust visual understanding in diverse and dynamic environments.
Code is available at https://github.com/anonymouse-9c53tp182bvz/Vireo.

</details>


### [157] [3D-Aware Vision-Language Models Fine-Tuning with Geometric Distillation](https://arxiv.org/abs/2506.09883)
*Seonho Lee,Jiho Choi,Inha Kang,Jiwook Kim,Junsung Park,Hyunjung Shim*

Main category: cs.CV

TL;DR: This paper presents Geometric Distillation, a lightweight fine-tuning approach to enhance Vision-Language Models (VLMs) with 3D spatial understanding.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the limitations of Vision-Language Models (VLMs) in understanding 3D spatial structures, an essential capability for many real-world applications.

Method: The proposed Geometric Distillation framework fine-tunes VLMs using geometric cues distilled from 3D foundation models like MASt3R and VGGT. It leverages sparse correspondences, relative depth relations, and dense cost volumes, all without altering the architecture of the VLMs.

Result: With Geometric Distillation, the models demonstrate enhanced 3D spatial reasoning capabilities and outperform prior approaches on benchmarks for 3D vision-language reasoning while maintaining lower computational costs.

Conclusion: The study highlights a scalable and efficient strategy for bridging 2D-trained VLMs with 3D comprehension, thereby broadening their applicability in spatially grounded multimodal tasks.

Abstract: Vision-Language Models (VLMs) have shown remarkable performance on diverse
visual and linguistic tasks, yet they remain fundamentally limited in their
understanding of 3D spatial structures. We propose Geometric Distillation, a
lightweight, annotation-free fine-tuning framework that injects human-inspired
geometric cues into pretrained VLMs without modifying their architecture. By
distilling (1) sparse correspondences, (2) relative depth relations, and (3)
dense cost volumes from off-the-shelf 3D foundation models (e.g., MASt3R,
VGGT), our method shapes representations to be geometry-aware while remaining
compatible with natural image-text inputs. Through extensive evaluations on 3D
vision-language reasoning and 3D perception benchmarks, our method consistently
outperforms prior approaches, achieving improved 3D spatial reasoning with
significantly lower computational cost. Our work demonstrates a scalable and
efficient path to bridge 2D-trained VLMs with 3D understanding, opening up
wider use in spatially grounded multimodal tasks.

</details>


### [158] [The Less You Depend, The More You Learn: Synthesizing Novel Views from Sparse, Unposed Images without Any 3D Knowledge](https://arxiv.org/abs/2506.09885)
*Haoru Wang,Kai Ye,Yangyan Li,Wenzheng Chen,Baoquan Chen*

Main category: cs.CV

TL;DR: The paper addresses novel view synthesis from sparse and unposed 2D images by minimizing reliance on 3D knowledge, achieving comparable results to 3D-driven methods through a data-centric approach.


<details>
  <summary>Details</summary>
Motivation: To create photorealistic novel views without relying heavily on 3D knowledge or camera poses, as this reduces dependencies and leverages large-scale data better.

Method: The researchers propose an NVS framework that avoids 3D inductive bias and pose annotations, learning implicit 3D features directly from training on sparse 2D images.

Result: The method achieves photorealistic and 3D-consistent results, comparable to methods using posed inputs, demonstrating scalability and effectiveness.

Conclusion: Reducing reliance on 3D knowledge is increasingly impactful in large-scale data contexts, and the proposed method validates the effectiveness of a data-centric approach to NVS.

Abstract: We consider the problem of generalizable novel view synthesis (NVS), which
aims to generate photorealistic novel views from sparse or even unposed 2D
images without per-scene optimization. This task remains fundamentally
challenging, as it requires inferring 3D structure from incomplete and
ambiguous 2D observations. Early approaches typically rely on strong 3D
knowledge, including architectural 3D inductive biases (e.g., embedding
explicit 3D representations, such as NeRF or 3DGS, into network design) and
ground-truth camera poses for both input and target views. While recent efforts
have sought to reduce the 3D inductive bias or the dependence on known camera
poses of input views, critical questions regarding the role of 3D knowledge and
the necessity of circumventing its use remain under-explored. In this work, we
conduct a systematic analysis on the 3D knowledge and uncover a critical trend:
the performance of methods that requires less 3D knowledge accelerates more as
data scales, eventually achieving performance on par with their 3D
knowledge-driven counterparts, which highlights the increasing importance of
reducing dependence on 3D knowledge in the era of large-scale data. Motivated
by and following this trend, we propose a novel NVS framework that minimizes 3D
inductive bias and pose dependence for both input and target views. By
eliminating this 3D knowledge, our method fully leverages data scaling and
learns implicit 3D awareness directly from sparse 2D images, without any 3D
inductive bias or pose annotation during training. Extensive experiments
demonstrate that our model generates photorealistic and 3D-consistent novel
views, achieving even comparable performance with methods that rely on posed
inputs, thereby validating the feasibility and effectiveness of our
data-centric paradigm. Project page:
https://pku-vcl-geometry.github.io/Less3Depend/ .

</details>


### [159] [EquiCaps: Predictor-Free Pose-Aware Pre-Trained Capsule Networks](https://arxiv.org/abs/2506.09895)
*Athinoulla Konstantinou,Georgios Leontidis,Mamatha Thota,Aiden Durrant*

Main category: cs.CV

TL;DR: The paper introduces EquiCaps, a capsule network approach for self-supervised, transformation-aware learning, improving performance on 3D pose estimation tasks over state-of-the-art methods without requiring a specialized predictor.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of existing self-supervised methods that rely on predictor architectures for equivariance and explore capsule networks' inherent strengths in learning interpretable pose-aware representations.

Method: EquiCaps leverages the intrinsic pose-awareness of capsule networks, avoiding the need for a specialized equivariance-enforcing predictor. The approach is tested on a new benchmark dataset, 3DIEBench-T, involving multi-geometric transformations.

Result: EquiCaps achieves higher performance in pose estimation tasks, surpassing prior methods like SIE and CapsIE on the 3DIEBench rotation prediction benchmark, and maintains robustness under complex geometric transformations.

Conclusion: EquiCaps demonstrates that capsule-based architectures eliminate the need for additional predictors while excelling at pose-aware learning and generalization, indicating their potential for broader self-supervised learning applications.

Abstract: Learning self-supervised representations that are invariant and equivariant
to transformations is crucial for advancing beyond traditional visual
classification tasks. However, many methods rely on predictor architectures to
encode equivariance, despite evidence that architectural choices, such as
capsule networks, inherently excel at learning interpretable pose-aware
representations. To explore this, we introduce EquiCaps (Equivariant Capsule
Network), a capsule-based approach to pose-aware self-supervision that
eliminates the need for a specialised predictor for enforcing equivariance.
Instead, we leverage the intrinsic pose-awareness capabilities of capsules to
improve performance in pose estimation tasks. To further challenge our
assumptions, we increase task complexity via multi-geometric transformations to
enable a more thorough evaluation of invariance and equivariance by introducing
3DIEBench-T, an extension of a 3D object-rendering benchmark dataset. Empirical
results demonstrate that EquiCaps outperforms prior state-of-the-art
equivariant methods on rotation prediction, achieving a supervised-level $R^2$
of 0.78 on the 3DIEBench rotation prediction benchmark and improving upon SIE
and CapsIE by 0.05 and 0.04 $R^2$, respectively. Moreover, in contrast to
non-capsule-based equivariant approaches, EquiCaps maintains robust equivariant
performance under combined geometric transformations, underscoring its
generalisation capabilities and the promise of predictor-free capsule
architectures.

</details>


### [160] [CEM-FBGTinyDet: Context-Enhanced Foreground Balance with Gradient Tuning for tiny Objects](https://arxiv.org/abs/2506.09897)
*Tao Liu,Zhenchao Cui*

Main category: cs.CV

TL;DR: This paper addresses the problem of tiny object detection and proposes E-FPN-BS, which enhances low-level features by utilizing unused high-level semantics and includes novel modules for better feature fusion and gradient loss balance, achieving strong performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: The authors aim to solve the fundamental limitation in feature pyramid networks for tiny object detection, where high-level features frequently remain untrained due to zero positive anchors assigned, leading to semantic and gradient deficiencies.

Method: The approach involves introducing E-FPN-BS, which includes: (1) a Context Enhancement Module to align and compress high-level features into low-level ones, (2) a Foreground-Background Separation Module employing spatial gating masks for dynamic feature amplification, and (3) a Dynamic Gradient-Balanced Loss to modulate loss contributions proportionally for various object scales.

Result: Experiments on multiple benchmark datasets showcase exceptional performance and improved generalization ability for tiny object detection using the proposed method.

Conclusion: Leveraging high-level features to enrich low-level representations and addressing gradient imbalance can substantially enhance tiny object detection performance, as demonstrated by the E-FPN-BS architecture and its components.

Abstract: Tiny object detection (TOD) reveals a fundamental flaw in feature pyramid
networks: high-level features (P5-P6) frequently receive zero positive anchors
under standard label assignment protocols, leaving their semantic
representations untrained due to exclusion from loss computation. This creates
dual deficiencies: (1) Stranded high-level features become semantic dead-ends
without gradient updates, while (2) low-level features lack essential semantic
context for robust classification. We propose E-FPN-BS that systematically
converts wasted high-level semantics into low-level feature enhancements. To
address these issues, we propose E-FPN-BS, a novel architecture integrating
multi-scale feature enhancement and adaptive optimization. First, our Context
Enhancement Module(CEM) employs dual-branch processing to align and compress
high-level features for effective global-local fusion. Second, the
Foreground-Background Separation Module (FBSM) generates spatial gating masks
that dynamically amplify discriminative regions. To address gradient imbalance
across object scales, we further propose a Dynamic Gradient-Balanced Loss
(DCLoss) that automatically modulates loss contributions via scale-aware
gradient equilibrium. Extensive experiments across multiple benchmark datasets
demonstrate the outstanding performance and generalization ability of our
approach.

</details>


### [161] [Only-Style: Stylistic Consistency in Image Generation without Content Leakage](https://arxiv.org/abs/2506.09916)
*Tilemachos Aravanis,Panagiotis Filntisis,Petros Maragos,George Retsinas*

Main category: cs.CV

TL;DR: The paper proposes "Only-Style," a method to reduce content leakage while maintaining stylistic consistency in image generation, outperforming current methods.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenge of generating images that maintain a consistent visual style while avoiding semantic content leakage from the reference image.

Method: The approach involves the localization of content leakage during inference, enabling the adaptive tuning of style alignment parameters specifically on image patches with subjects from the reference image. It includes a standalone leakage localization component and introduces a new evaluation framework.

Result: "Only-Style" achieves robust stylistic consistency and significantly reduces undesired content leakage compared to state-of-the-art methods as shown through diverse evaluations.

Conclusion: The method offers an effective balance between style preservation and content integrity, making it a strong improvement over existing techniques in the domain.

Abstract: Generating images in a consistent reference visual style remains a
challenging computer vision task. State-of-the-art methods aiming for
style-consistent generation struggle to effectively separate semantic content
from stylistic elements, leading to content leakage from the image provided as
a reference to the targets. To address this challenge, we propose Only-Style: a
method designed to mitigate content leakage in a semantically coherent manner
while preserving stylistic consistency. Only-Style works by localizing content
leakage during inference, allowing the adaptive tuning of a parameter that
controls the style alignment process, specifically within the image patches
containing the subject in the reference image. This adaptive process best
balances stylistic consistency with leakage elimination. Moreover, the
localization of content leakage can function as a standalone component, given a
reference-target image pair, allowing the adaptive tuning of any
method-specific parameter that provides control over the impact of the
stylistic reference. In addition, we propose a novel evaluation framework to
quantify the success of style-consistent generations in avoiding undesired
content leakage. Our approach demonstrates a significant improvement over
state-of-the-art methods through extensive evaluation across diverse instances,
consistently achieving robust stylistic consistency without undesired content
leakage.

</details>


### [162] [MetricHMR: Metric Human Mesh Recovery from Monocular Images](https://arxiv.org/abs/2506.09919)
*He Zhang,Chentao Song,Hongwen Zhang,Tao Yu*

Main category: cs.CV

TL;DR: This paper introduces MetricHMR, a novel method for recovering human mesh with metric accuracy and precise global translation from monocular images.


<details>
  <summary>Details</summary>
Motivation: Existing HMR methods face significant issues with scale and depth ambiguities, leading to inconsistencies in geometric body shape and translation estimation.

Method: The approach utilizes a systematic analysis of camera models, applies a standard perspective projection model, validates ambiguity ranges, and introduces a ray map to encode bounding-box, camera, and geometric information for End2End metric HMR.

Result: MetricHMR outperforms state-of-the-art methods in estimating metric pose, shape, and global translation across diverse indoor and in-the-wild scenarios.

Conclusion: The method's use of a ray map and perspective projection model effectively mitigates scale and depth ambiguities, enabling accurate and geometrically plausible metric human mesh recovery.

Abstract: We introduce MetricHMR (Metric Human Mesh Recovery), an approach for metric
human mesh recovery with accurate global translation from monocular images. In
contrast to existing HMR methods that suffer from severe scale and depth
ambiguity, MetricHMR is able to produce geometrically reasonable body shape and
global translation in the reconstruction results. To this end, we first
systematically analyze previous HMR methods on camera models to emphasize the
critical role of the standard perspective projection model in enabling
metric-scale HMR. We then validate the acceptable ambiguity range of metric HMR
under the standard perspective projection model. Finally, we contribute a novel
approach that introduces a ray map based on the standard perspective projection
to jointly encode bounding-box information, camera parameters, and geometric
cues for End2End metric HMR without any additional metric-regularization
modules. Extensive experiments demonstrate that our method achieves
state-of-the-art performance, even compared with sequential HMR methods, in
metric pose, shape, and global translation estimation across both indoor and
in-the-wild scenarios.

</details>


### [163] [Structural-Spectral Graph Convolution with Evidential Edge Learning for Hyperspectral Image Clustering](https://arxiv.org/abs/2506.09920)
*Jianhan Qi,Yuheng Jia,Hui Liu,Junhui Hou*

Main category: cs.CV

TL;DR: The paper introduces a new hyperspectral image (HSI) clustering method leveraging spectral-spatial graph operators and adaptive edge learning, significantly improving clustering accuracy.


<details>
  <summary>Details</summary>
Motivation: HSI clustering is challenged by limited spectral feature exploitation and inaccuracies in superpixel topological graphs, leading to semantic confusion in clustering algorithms.

Method: The authors propose a structural-spectral graph convolutional operator (SSGCO) for spatial-spectral feature co-extraction and an evidence-guided adaptive edge learning (EGAEL) module. These techniques are integrated into a supervised contrastive learning framework.

Result: The method achieves a clustering accuracy improvement of 2.61%, 6.06%, 4.96%, and 3.15% across four different HSI datasets, outperforming existing approaches.

Conclusion: The proposed techniques effectively enhance robustness and accuracy in HSI clustering, marking a significant advancement in unsupervised image analysis.

Abstract: Hyperspectral image (HSI) clustering assigns similar pixels to the same class
without any annotations, which is an important yet challenging task. For
large-scale HSIs, most methods rely on superpixel segmentation and perform
superpixel-level clustering based on graph neural networks (GNNs). However,
existing GNNs cannot fully exploit the spectral information of the input HSI,
and the inaccurate superpixel topological graph may lead to the confusion of
different class semantics during information aggregation. To address these
challenges, we first propose a structural-spectral graph convolutional operator
(SSGCO) tailored for graph-structured HSI superpixels to improve their
representation quality through the co-extraction of spatial and spectral
features. Second, we propose an evidence-guided adaptive edge learning (EGAEL)
module that adaptively predicts and refines edge weights in the superpixel
topological graph. We integrate the proposed method into a contrastive learning
framework to achieve clustering, where representation learning and clustering
are simultaneously conducted. Experiments demonstrate that the proposed method
improves clustering accuracy by 2.61%, 6.06%, 4.96% and 3.15% over the best
compared methods on four HSI datasets. Our code is available at
https://github.com/jhqi/SSGCO-EGAEL.

</details>


### [164] [HadaNorm: Diffusion Transformer Quantization through Mean-Centered Transformations](https://arxiv.org/abs/2506.09932)
*Marco Federici,Riccardo Del Chiaro,Boris van Breugel,Paul Whatmough,Markus Nagel*

Main category: cs.CV

TL;DR: HadaNorm is introduced to mitigate outliers in post-training quantization for diffusion models, boosting efficiency.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of deploying diffusion models on resource-constrained devices.

Method: Employ HadaNorm to normalize activations before Hadamard transformations, enabling aggressive quantization.

Result: HadaNorm reduces quantization error and enhances efficiency-performance trade-offs compared to existing methods.

Conclusion: HadaNorm improves post-training quantization by effectively handling outliers, facilitating deployment of diffusion models on limited-resource platforms.

Abstract: Diffusion models represent the cutting edge in image generation, but their
high memory and computational demands hinder deployment on resource-constrained
devices. Post-Training Quantization (PTQ) offers a promising solution by
reducing the bitwidth of matrix operations. However, standard PTQ methods
struggle with outliers, and achieving higher compression often requires
transforming model weights and activations before quantization. In this work,
we propose HadaNorm, a novel linear transformation that extends existing
approaches and effectively mitigates outliers by normalizing activations
feature channels before applying Hadamard transformations, enabling more
aggressive activation quantization. We demonstrate that HadaNorm consistently
reduces quantization error across the various components of transformer blocks,
achieving superior efficiency-performance trade-offs when compared to
state-of-the-art methods.

</details>


### [165] [LEO-VL: Towards 3D Vision-Language Generalists via Data Scaling with Efficient Representation](https://arxiv.org/abs/2506.09935)
*Jiangyong Huang,Xiaojian Ma,Xiongkun Linghu,Yue Fan,Junchao He,Wenxin Tan,Qing Li,Song-Chun Zhu,Yixin Chen,Baoxiong Jia,Siyuan Huang*

Main category: cs.CV

TL;DR: The paper introduces LEO-VL, a scalable 3D vision-language model leveraging efficient condensed feature grids (CFG) and newly curated large-scale datasets, achieving state-of-the-art results in various 3D reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations in scalability and performance of existing 3D-VL models which lag behind 2D counterparts in terms of capability and robustness.

Method: Develop LEO-VL by using condensed feature grid (CFG) for efficient scene representation, curate 700k high-quality 3D-VL datasets from diverse domains and tasks, and introduce SceneDPO as a post-training objective for improved robustness.

Result: LEO-VL outperforms state-of-the-art benchmarks like SQA3D, MSQA, and Beacon3D and demonstrates improved robustness through ablation studies and the SceneDPO objective.

Conclusion: LEO-VL's innovations in representation efficiency and dataset diversity mark a significant step towards scalable, robust, and generalist 3D-VL models.

Abstract: Developing 3D-VL generalists capable of understanding 3D scenes and following
natural language instructions to perform a wide range of tasks has been a
long-standing goal in the 3D-VL community. Despite recent progress, 3D-VL
models still lag behind their 2D counterparts in capability and robustness,
falling short of the generalist standard. A key obstacle to developing 3D-VL
generalists lies in data scalability, hindered by the lack of an efficient
scene representation. We propose LEO-VL, a 3D-VL model built upon condensed
feature grid (CFG), an efficient scene representation that bridges 2D
perception and 3D spatial structure while significantly reducing token
overhead. This efficiency unlocks large-scale training towards 3D-VL
generalist, for which we curate over 700k high-quality 3D-VL data spanning four
domains of real-world indoor scenes and five tasks such as captioning and
dialogue. LEO-VL achieves state-of-the-art performance on a variety of 3D QA
benchmarks, including SQA3D, MSQA, and Beacon3D. Ablation studies confirm the
efficiency of our representation, the importance of task and scene diversity,
and the validity of our data curation principle. Furthermore, we introduce
SceneDPO, a novel post-training objective that enhances the robustness of 3D-VL
models. We hope our findings contribute to the advancement of scalable and
robust 3D-VL generalists.

</details>


### [166] [CausalVQA: A Physically Grounded Causal Reasoning Benchmark for Video Models](https://arxiv.org/abs/2506.09943)
*Aaron Foss,Chloe Evans,Sasha Mitts,Koustuv Sinha,Ammar Rizvi,Justine T. Kao*

Main category: cs.CV

TL;DR: CausalVQA is a new video question answering benchmark focusing on causal reasoning in real-world scenarios with five challenging question types.


<details>
  <summary>Details</summary>
Motivation: Existing VQA benchmarks lack focus on causal reasoning in real-world physical scenarios and often depend on perceptual or simulated environments.

Method: Developed CausalVQA with counterfactual, hypothetical, anticipation, planning, and descriptive questions, and quality controls to prevent exploitation of trivial shortcuts.

Result: Current multimodal models perform significantly below human levels, particularly on anticipation and hypothetical questions.

Conclusion: There is a need for better spatial-temporal, physical principle, and alternative comprehension reasoning in VQA systems for real-world applications.

Abstract: We introduce CausalVQA, a benchmark dataset for video question answering
(VQA) composed of question-answer pairs that probe models' understanding of
causality in the physical world. Existing VQA benchmarks either tend to focus
on surface perceptual understanding of real-world videos, or on narrow physical
reasoning questions created using simulation environments. CausalVQA fills an
important gap by presenting challenging questions that are grounded in
real-world scenarios, while focusing on models' ability to predict the likely
outcomes of different actions and events through five question types:
counterfactual, hypothetical, anticipation, planning and descriptive. We
designed quality control mechanisms that prevent models from exploiting trivial
shortcuts, requiring models to base their answers on deep visual understanding
instead of linguistic cues. We find that current frontier multimodal models
fall substantially below human performance on the benchmark, especially on
anticipation and hypothetical questions. This highlights a challenge for
current systems to leverage spatial-temporal reasoning, understanding of
physical principles, and comprehension of possible alternatives to make
accurate predictions in real-world settings.

</details>


### [167] [Outside Knowledge Conversational Video (OKCV) Dataset -- Dialoguing over Videos](https://arxiv.org/abs/2506.09953)
*Benjamin Reichman,Constantin Patsch,Jack Truxal,Atishay Jain,Larry Heck*

Main category: cs.CV

TL;DR: The paper extends outside knowledge visual question answering (OK-VQA) to a video-based, conversational dialogue setting, introducing a novel dataset that integrates visually grounded and externally informed dialogue tasks.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenges of combining visual information from videos with external knowledge and maintaining conversational context for multi-turn dialogues in a new task setting.

Method: The paper introduces a dataset of 2,017 videos with annotated dialogues spanning over 40,954 dialogue turns. Baselines for addressing this task are provided, facilitating exploration of models suited for visually grounded, knowledge-based conversations.

Result: Baseline models' performances on the dataset were evaluated, revealing challenges and opportunities for improvement in handling video-grounded, knowledge-intensive dialogues.

Conclusion: The introduced dataset represents a novel resource for the research community, highlighting the complexity of integrating visual and external knowledge in conversational AI tasks. Future work is necessary to address observed challenges.

Abstract: In outside knowledge visual question answering (OK-VQA), the model must
identify relevant visual information within an image and incorporate external
knowledge to accurately respond to a question. Extending this task to a
visually grounded dialogue setting based on videos, a conversational model must
both recognize pertinent visual details over time and answer questions where
the required information is not necessarily present in the visual information.
Moreover, the context of the overall conversation must be considered for the
subsequent dialogue. To explore this task, we introduce a dataset comprised of
$2,017$ videos with $5,986$ human-annotated dialogues consisting of $40,954$
interleaved dialogue turns. While the dialogue context is visually grounded in
specific video segments, the questions further require external knowledge that
is not visually present. Thus, the model not only has to identify relevant
video parts but also leverage external knowledge to converse within the
dialogue. We further provide several baselines evaluated on our dataset and
show future challenges associated with this task. The dataset is made publicly
available here: https://github.com/c-patsch/OKCV.

</details>


### [168] [UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal Gaussian Splatting](https://arxiv.org/abs/2506.09952)
*Ziyi Wang,Yanran Zhang,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: The paper introduces UniPre3D, a unified pre-training method designed for point clouds of any scale and architecture, addressing challenges in 3D vision representation learning.


<details>
  <summary>Details</summary>
Motivation: Existing pre-training methods fail to work effectively for both object- and scene-level point clouds, highlighting the need for a unified approach adaptable across scales.

Method: Using Gaussian primitive prediction and differentiable Gaussian splatting for precise pixel-level supervision, combined with pre-trained 2D texture features to balance task complexity and focus on geometric structures.

Result: UniPre3D demonstrated universal effectiveness across diverse tasks and point cloud models as validated through extensive experiments.

Conclusion: UniPre3D is a significant step toward unified 3D model pre-training, offering scalability and versatility for both object- and scene-level tasks with solid experimental validation.

Abstract: The scale diversity of point cloud data presents significant challenges in
developing unified representation learning techniques for 3D vision. Currently,
there are few unified 3D models, and no existing pre-training method is equally
effective for both object- and scene-level point clouds. In this paper, we
introduce UniPre3D, the first unified pre-training method that can be
seamlessly applied to point clouds of any scale and 3D models of any
architecture. Our approach predicts Gaussian primitives as the pre-training
task and employs differentiable Gaussian splatting to render images, enabling
precise pixel-level supervision and end-to-end optimization. To further
regulate the complexity of the pre-training task and direct the model's focus
toward geometric structures, we integrate 2D features from pre-trained image
models to incorporate well-established texture knowledge. We validate the
universal effectiveness of our proposed method through extensive experiments
across a variety of object- and scene-level tasks, using diverse point cloud
models as backbones. Code is available at https://github.com/wangzy22/UniPre3D.

</details>


### [169] [Vision Generalist Model: A Survey](https://arxiv.org/abs/2506.09954)
*Ziyi Wang,Yongming Rao,Shuofeng Sun,Xinrun Liu,Yi Wei,Xumin Yu,Zuyan Liu,Yanbo Wang,Hongmin Liu,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: This paper reviews the rising field of vision generalist models, exploring their design, applications, and performance metrics.


<details>
  <summary>Details</summary>
Motivation: Explore the transfer of generalist model successes from NLP to computer vision despite the diverse nature of visual data inputs and outputs.

Method: Comprehensive review of datasets, tasks, benchmarks, model designs, performance enhancement techniques, related fields, and applications.

Result: Provides an overview of vision generalist frameworks, their challenges, synergies with related domains, and future research directions.

Conclusion: Vision generalist models show promise; this paper highlights their potential, challenges, and opportunities for advancement in real-world applications.

Abstract: Recently, we have witnessed the great success of the generalist model in
natural language processing. The generalist model is a general framework
trained with massive data and is able to process various downstream tasks
simultaneously. Encouraged by their impressive performance, an increasing
number of researchers are venturing into the realm of applying these models to
computer vision tasks. However, the inputs and outputs of vision tasks are more
diverse, and it is difficult to summarize them as a unified representation. In
this paper, we provide a comprehensive overview of the vision generalist
models, delving into their characteristics and capabilities within the field.
First, we review the background, including the datasets, tasks, and benchmarks.
Then, we dig into the design of frameworks that have been proposed in existing
research, while also introducing the techniques employed to enhance their
performance. To better help the researchers comprehend the area, we take a
brief excursion into related domains, shedding light on their interconnections
and potential synergies. To conclude, we provide some real-world application
scenarios, undertake a thorough examination of the persistent challenges, and
offer insights into possible directions for future research endeavors.

</details>


### [170] [Kvasir-VQA-x1: A Multimodal Dataset for Medical Reasoning and Robust MedVQA in Gastrointestinal Endoscopy](https://arxiv.org/abs/2506.09958)
*Sushant Gautam,Michael A. Riegler,Pål Halvorsen*

Main category: cs.CV

TL;DR: The paper introduces Kvasir-VQA-x1, a large-scale dataset for GI endoscopy MedVQA, addressing limitations in existing datasets by including complex question-answer pairs and visual augmentations.


<details>
  <summary>Details</summary>
Motivation: To improve clinical decision support systems with more complex and diverse datasets for MedVQA, addressing gaps in clinical complexity and visual diversity.

Method: They utilized large language models to generate 159,549 clinically complex question-answer pairs and introduced visual augmentations to mimic imaging artifacts.

Result: Kvasir-VQA-x1 offers a challenging benchmark with stratified question complexities and visual perturbations, structured for VQA performance and robustness evaluation.

Conclusion: The dataset enhances the ability to develop reliable AI systems for clinical use, is FAIR-compliant, and provides a valuable, accessible resource for research.

Abstract: Medical Visual Question Answering (MedVQA) is a promising field for
developing clinical decision support systems, yet progress is often limited by
the available datasets, which can lack clinical complexity and visual
diversity. To address these gaps, we introduce Kvasir-VQA-x1, a new,
large-scale dataset for gastrointestinal (GI) endoscopy. Our work significantly
expands upon the original Kvasir-VQA by incorporating 159,549 new
question-answer pairs that are designed to test deeper clinical reasoning. We
developed a systematic method using large language models to generate these
questions, which are stratified by complexity to better assess a model's
inference capabilities. To ensure our dataset prepares models for real-world
clinical scenarios, we have also introduced a variety of visual augmentations
that mimic common imaging artifacts. The dataset is structured to support two
main evaluation tracks: one for standard VQA performance and another to test
model robustness against these visual perturbations. By providing a more
challenging and clinically relevant benchmark, Kvasir-VQA-x1 aims to accelerate
the development of more reliable and effective multimodal AI systems for use in
clinical settings. The dataset is fully accessible and adheres to FAIR data
principles, making it a valuable resource for the wider research community.
Code and data: https://github.com/Simula/Kvasir-VQA-x1 and
https://huggingface.co/datasets/SimulaMet/Kvasir-VQA-x1

</details>


### [171] [Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing](https://arxiv.org/abs/2506.09965)
*Junfei Wu,Jian Guan,Kaituo Feng,Qiang Liu,Shu Wu,Liang Wang,Wei Wu,Tieniu Tan*

Main category: cs.CV

TL;DR: The study introduces VILASR, a new model that enhances spatial reasoning in LVLMs using drawing-based visual manipulation, achieving superior performance compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Current LVLM approaches struggle with spatial reasoning tasks due to their text-centric design and lack of geometric understanding, requiring improved capabilities akin to human mental visualization.

Method: The proposed method involves a three-stage training framework: synthetic data training, reflective rejection sampling for self-reflection, and reinforcement learning, along with enabling drawing-based visual manipulations like bounding boxes and auxiliary lines.

Result: VILASR shows an average improvement of 18.4% across various spatial reasoning tasks, including maze navigation, static spatial reasoning, video-based reasoning, and multi-view-based reasoning benchmarks.

Conclusion: The research demonstrates that incorporating elementary drawing operations into LVLMs can significantly enhance their multimodal reasoning capabilities, overcoming limitations observed in text-centric approaches.

Abstract: As textual reasoning with large language models (LLMs) has advanced
significantly, there has been growing interest in enhancing the multimodal
reasoning capabilities of large vision-language models (LVLMs). However,
existing methods primarily approach multimodal reasoning in a straightforward,
text-centric manner, where both reasoning and answer derivation are conducted
purely through text, with the only difference being the presence of multimodal
input. As a result, these methods often encounter fundamental limitations in
spatial reasoning tasks that demand precise geometric understanding and
continuous spatial tracking-capabilities that humans achieve through mental
visualization and manipulation. To address the limitations, we propose drawing
to reason in space, a novel paradigm that enables LVLMs to reason through
elementary drawing operations in the visual space. By equipping models with
basic drawing operations, including annotating bounding boxes and drawing
auxiliary lines, we empower them to express and analyze spatial relationships
through direct visual manipulation, meanwhile avoiding the performance ceiling
imposed by specialized perception tools in previous tool-integrated reasoning
approaches. To cultivate this capability, we develop a three-stage training
framework: cold-start training with synthetic data to establish basic drawing
abilities, reflective rejection sampling to enhance self-reflection behaviors,
and reinforcement learning to directly optimize for target rewards. Extensive
experiments demonstrate that our model, named VILASR, consistently outperforms
existing methods across diverse spatial reasoning benchmarks, involving maze
navigation, static spatial reasoning, video-based reasoning, and
multi-view-based reasoning tasks, with an average improvement of 18.4%.

</details>


### [172] [Vectorized Region Based Brush Strokes for Artistic Rendering](https://arxiv.org/abs/2506.09969)
*Jeripothula Prudviraj,Vikram Jamwal*

Main category: cs.CV

TL;DR: The paper presents an image-to-painting method that enhances the artistic process by guiding brush strokes semantically, precisely segmenting, and arranging strokes for creating high-fidelity paintings.


<details>
  <summary>Details</summary>
Motivation: To reduce the emotional and educational gap between static artwork and its creation process by improving stroke composition and artistic principles in stroke-based painting systems.

Method: The method provides semantic guidance for strokes, calculates brush stroke parameters, and sequentially renders strokes using a region-based painting strategy.

Result: The proposed method demonstrates high fidelity and stroke quality across various input image types, including faces, paintings, and photographs.

Conclusion: The method successfully aligns artistic region-based strategies with technical precision, creating paintings that respect artistic intent and achieve superior quality.

Abstract: Creating a stroke-by-stroke evolution process of a visual artwork tries to
bridge the emotional and educational gap between the finished static artwork
and its creation process. Recent stroke-based painting systems focus on
capturing stroke details by predicting and iteratively refining stroke
parameters to maximize the similarity between the input image and the rendered
output. However, these methods often struggle to produce stroke compositions
that align with artistic principles and intent. To address this, we explore an
image-to-painting method that (i) facilitates semantic guidance for brush
strokes in targeted regions, (ii) computes the brush stroke parameters, and
(iii) establishes a sequence among segments and strokes to sequentially render
the final painting. Experimental results on various input image types, such as
face images, paintings, and photographic images, show that our method aligns
with a region-based painting strategy while rendering a painting with high
fidelity and superior stroke quality.

</details>


### [173] [Efficient Part-level 3D Object Generation via Dual Volume Packing](https://arxiv.org/abs/2506.09980)
*Jiaxiang Tang,Ruijie Lu,Zhaoshuo Li,Zekun Hao,Xuan Li,Fangyin Wei,Shuran Song,Gang Zeng,Ming-Yu Liu,Tsung-Yi Lin*

Main category: cs.CV

TL;DR: This paper introduces a framework for generating editable, part-level 3D objects from a single image, addressing limitations of earlier methods that produce fused object meshes.


<details>
  <summary>Details</summary>
Motivation: Most 3D object generation methods struggle with editing and manipulating individual parts because they generate single fused meshes. This paper aims to provide a solution for generating objects with arbitrary numbers of semantically meaningful parts.

Method: The authors propose an end-to-end framework using a dual volume packing strategy. This strategy organizes object parts into two complementary volumes for complete and interleaved part creation.

Result: The framework generates high-quality 3D objects with better quality, diversity, and generalization compared to previous image-based part-level generation methods.

Conclusion: The proposed method overcomes the limitations of fused mesh generation and opens possibilities for generating and editing semantically meaningful parts of 3D objects.

Abstract: Recent progress in 3D object generation has greatly improved both the quality
and efficiency. However, most existing methods generate a single mesh with all
parts fused together, which limits the ability to edit or manipulate individual
parts. A key challenge is that different objects may have a varying number of
parts. To address this, we propose a new end-to-end framework for part-level 3D
object generation. Given a single input image, our method generates
high-quality 3D objects with an arbitrary number of complete and semantically
meaningful parts. We introduce a dual volume packing strategy that organizes
all parts into two complementary volumes, allowing for the creation of complete
and interleaved parts that assemble into the final object. Experiments show
that our model achieves better quality, diversity, and generalization than
previous image-based part-level generation methods.

</details>


### [174] [Unify Graph Learning with Text: Unleashing LLM Potentials for Session Search](https://arxiv.org/abs/2505.14156)
*Songhao Wu,Quan Tu,Hong Liu,Jia Xu,Zhongyi Liu,Guannan Zhang,Ran Wang,Xiuying Chen,Rui Yan*

Main category: cs.CV

TL;DR: The paper introduces Symbolic Graph Ranker (SGR), which combines text and graph-based approaches for session search, leveraging Large Language Models with symbolic grammar rules and self-supervised learning tasks.


<details>
  <summary>Details</summary>
Motivation: Current session search strategies either focus on sequential modeling, neglecting graph structures, or fail to incorporate word-level semantics while using graph-based representations. This paper aims to bridge the gap between these approaches and integrate them with modern advancements in LLMs.

Method: The authors introduce symbolic grammar rules to convert session graphs into text, enabling LLMs to seamlessly process session history and tasks. They enhance LLMs' capability to understand graph structures with self-supervised learning tasks like link prediction, node content generation, and generative contrastive learning.

Result: Experiments on benchmark datasets AOL and Tiangong-ST show that Symbolic Graph Ranker outperforms competing approaches, effectively combining graphical and textual semantic understanding.

Conclusion: Symbolic Graph Ranker successfully unites traditional search techniques with modern LLMs, creating an innovative and effective paradigm for session search by leveraging both text-based and graph-based insights.

Abstract: Session search involves a series of interactive queries and actions to
fulfill user's complex information need. Current strategies typically
prioritize sequential modeling for deep semantic understanding, overlooking the
graph structure in interactions. While some approaches focus on capturing
structural information, they use a generalized representation for documents,
neglecting the word-level semantic modeling. In this paper, we propose Symbolic
Graph Ranker (SGR), which aims to take advantage of both text-based and
graph-based approaches by leveraging the power of recent Large Language Models
(LLMs). Concretely, we first introduce a set of symbolic grammar rules to
convert session graph into text. This allows integrating session history,
interaction process, and task instruction seamlessly as inputs for the LLM.
Moreover, given the natural discrepancy between LLMs pre-trained on textual
corpora, and the symbolic language we produce using our graph-to-text grammar,
our objective is to enhance LLMs' ability to capture graph structures within a
textual format. To achieve this, we introduce a set of self-supervised symbolic
learning tasks including link prediction, node content generation, and
generative contrastive learning, to enable LLMs to capture the topological
information from coarse-grained to fine-grained. Experiment results and
comprehensive analysis on two benchmark datasets, AOL and Tiangong-ST, confirm
the superiority of our approach. Our paradigm also offers a novel and effective
methodology that bridges the gap between traditional search strategies and
modern LLMs.

</details>


### [175] [AnimateAnyMesh: A Feed-Forward 4D Foundation Model for Text-Driven Universal Mesh Animation](https://arxiv.org/abs/2506.09982)
*Zijie Wu,Chaohui Yu,Fan Wang,Xiang Bai*

Main category: cs.CV

TL;DR: The paper introduces AnimateAnyMesh, a framework for generating text-driven animations for 3D meshes efficiently, using a novel DyMeshVAE architecture and a large dataset (DyMesh).


<details>
  <summary>Details</summary>
Motivation: To address the challenges in creating high-quality animated 3D models due to spatio-temporal modeling complexity and limited 4D data availability.

Method: The approach uses the DyMeshVAE architecture for disentangling spatial and temporal features, along with a Rectified Flow-based training strategy in a compressed latent space.

Result: The method produces semantically accurate and coherent mesh animations within seconds, outperforming existing techniques in both quality and efficiency.

Conclusion: AnimateAnyMesh significantly advances 4D content creation, improving accessibility and practicality for text-driven 3D animations, with plans to open-release all resources.

Abstract: Recent advances in 4D content generation have attracted increasing attention,
yet creating high-quality animated 3D models remains challenging due to the
complexity of modeling spatio-temporal distributions and the scarcity of 4D
training data. In this paper, we present AnimateAnyMesh, the first feed-forward
framework that enables efficient text-driven animation of arbitrary 3D meshes.
Our approach leverages a novel DyMeshVAE architecture that effectively
compresses and reconstructs dynamic mesh sequences by disentangling spatial and
temporal features while preserving local topological structures. To enable
high-quality text-conditional generation, we employ a Rectified Flow-based
training strategy in the compressed latent space. Additionally, we contribute
the DyMesh Dataset, containing over 4M diverse dynamic mesh sequences with text
annotations. Experimental results demonstrate that our method generates
semantically accurate and temporally coherent mesh animations in a few seconds,
significantly outperforming existing approaches in both quality and efficiency.
Our work marks a substantial step forward in making 4D content creation more
accessible and practical. All the data, code, and models will be open-released.

</details>


### [176] [InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio Conditions](https://arxiv.org/abs/2506.09984)
*Zhenzhi Wang,Jiaqi Yang,Jianwen Jiang,Chao Liang,Gaojie Lin,Zerong Zheng,Ceyuan Yang,Dahua Lin*

Main category: cs.CV

TL;DR: This paper introduces a framework for generating human-centric videos with multiple interacting subjects and objects, overcoming limitations of single-entity animations.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitation of existing animation methods that are restricted to single subjects with global modality control, hindering precise multi-concept management.

Method: A novel framework enforces region-specific bindings of modalities (text, image, audio) to each identity based on spatiotemporal footprints using mask predictors and iterative local audiovisual coupling.

Result: The framework enables high-quality generation of controllable videos involving multiple concepts and validates its efficiency through empirical studies and ablation experiments.

Conclusion: Explicit layout control for multi-modal conditions outperforms implicit approaches, allowing precise control over richer human and object interactions in animations.

Abstract: End-to-end human animation with rich multi-modal conditions, e.g., text,
image and audio has achieved remarkable advancements in recent years. However,
most existing methods could only animate a single subject and inject conditions
in a global manner, ignoring scenarios that multiple concepts could appears in
the same video with rich human-human interactions and human-object
interactions. Such global assumption prevents precise and per-identity control
of multiple concepts including humans and objects, therefore hinders
applications. In this work, we discard the single-entity assumption and
introduce a novel framework that enforces strong, region-specific binding of
conditions from modalities to each identity's spatiotemporal footprint. Given
reference images of multiple concepts, our method could automatically infer
layout information by leveraging a mask predictor to match appearance cues
between the denoised video and each reference appearance. Furthermore, we
inject local audio condition into its corresponding region to ensure
layout-aligned modality matching in a iterative manner. This design enables the
high-quality generation of controllable multi-concept human-centric videos.
Empirical results and ablation studies validate the effectiveness of our
explicit layout control for multi-modal conditions compared to implicit
counterparts and other existing methods.

</details>


### [177] [A Shortcut-aware Video-QA Benchmark for Physical Understanding via Minimal Video Pairs](https://arxiv.org/abs/2506.09987)
*Benno Krojer,Mojtaba Komeili,Candace Ross,Quentin Garrido,Koustuv Sinha,Nicolas Ballas,Mahmoud Assran*

Main category: cs.CV

TL;DR: This paper introduces the MVP benchmark to better assess video-language models by minimizing shortcuts in video QA tasks.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for video-language models are flawed due to shortcut solutions based on superficial visual or textual cues, which inflate performance scores.

Method: The authors propose the MVP benchmark composed of 55K multiple-choice video QA examples with a minimal-change pair structure for each sample to prevent reliance on shortcuts.

Result: Human performance on the benchmark is 92.9%, while the best current video-language model achieves only 40.2%, showing a significant performance gap.

Conclusion: MVP is a robust benchmark that effectively evaluates models' physical reasoning abilities, discouraging reliance on biases and shortcuts.

Abstract: Existing benchmarks for assessing the spatio-temporal understanding and
reasoning abilities of video language models are susceptible to score inflation
due to the presence of shortcut solutions based on superficial visual or
textual cues. This paper mitigates the challenges in accurately assessing model
performance by introducing the Minimal Video Pairs (MVP) benchmark, a simple
shortcut-aware video QA benchmark for assessing the physical understanding of
video language models. The benchmark is comprised of 55K high-quality
multiple-choice video QA examples focusing on physical world understanding.
Examples are curated from nine video data sources, spanning first-person
egocentric and exocentric videos, robotic interaction data, and cognitive
science intuitive physics benchmarks. To mitigate shortcut solutions that rely
on superficial visual or textual cues and biases, each sample in MVP has a
minimal-change pair -- a visually similar video accompanied by an identical
question but an opposing answer. To answer a question correctly, a model must
provide correct answers for both examples in the minimal-change pair; as such,
models that solely rely on visual or textual biases would achieve below random
performance. Human performance on MVP is 92.9\%, while the best open-source
state-of-the-art video-language model achieves 40.2\% compared to random
performance at 25\%.

</details>


### [178] [EditInspector: A Benchmark for Evaluation of Text-Guided Image Edits](https://arxiv.org/abs/2506.09988)
*Ron Yosef,Moran Yanuka,Yonatan Bitton,Dani Lischinski*

Main category: cs.CV

TL;DR: The paper introduces EditInspector, a benchmark for evaluating text-guided image edits, highlighting current model limitations and proposing improved methods.


<details>
  <summary>Details</summary>
Motivation: The need to evaluate and verify the quality of text-guided image edits as such edits become widespread with advancements in generative AI.

Method: Developing the EditInspector benchmark, collecting human-annotated data, evaluating models across various dimensions, and proposing two novel methods for improved performance.

Result: State-of-the-art models were shown to struggle with comprehensive evaluations, hallucinate descriptions, while the proposed methods improved artifact detection and change description.

Conclusion: Current AI models have limitations in evaluating text-guided edits; EditInspector and the proposed methods offer progress in addressing these challenges.

Abstract: Text-guided image editing, fueled by recent advancements in generative AI, is
becoming increasingly widespread. This trend highlights the need for a
comprehensive framework to verify text-guided edits and assess their quality.
To address this need, we introduce EditInspector, a novel benchmark for
evaluation of text-guided image edits, based on human annotations collected
using an extensive template for edit verification. We leverage EditInspector to
evaluate the performance of state-of-the-art (SoTA) vision and language models
in assessing edits across various dimensions, including accuracy, artifact
detection, visual quality, seamless integration with the image scene, adherence
to common sense, and the ability to describe edit-induced changes. Our findings
indicate that current models struggle to evaluate edits comprehensively and
frequently hallucinate when describing the changes. To address these
challenges, we propose two novel methods that outperform SoTA models in both
artifact detection and difference caption generation.

</details>


### [179] [Hearing Hands: Generating Sounds from Physical Interactions in 3D Scenes](https://arxiv.org/abs/2506.09989)
*Yiming Dou,Wonseok Oh,Yuqing Luo,Antonio Loquercio,Andrew Owens*

Main category: cs.CV

TL;DR: This paper explores interactive 3D scene reconstruction by predicting the sound of human hands interacting with objects based on their actions.


<details>
  <summary>Details</summary>
Motivation: The study aims to enhance 3D scene reconstructions by incorporating realistic sound predictions based on human-object interaction, adding another layer of immersive simulation.

Method: The authors recorded videos of humans interacting with objects, mapping 3D hand trajectories to corresponding sounds using a rectified flow model trained on these action-sound pairs.

Result: Generated sounds effectively represent material properties and motions, to the extent that humans often find them indistinguishable from real recorded sounds.

Conclusion: The paper demonstrates that predicting interaction sounds can make 3D scenes more interactive and realistic, providing valuable insights for applications in virtual environments.

Abstract: We study the problem of making 3D scene reconstructions interactive by asking
the following question: can we predict the sounds of human hands physically
interacting with a scene? First, we record a video of a human manipulating
objects within a 3D scene using their hands. We then use these action-sound
pairs to train a rectified flow model to map 3D hand trajectories to their
corresponding audio. At test time, a user can query the model for other
actions, parameterized as sequences of hand poses, to estimate their
corresponding sounds. In our experiments, we find that our generated sounds
accurately convey material properties and actions, and that they are often
indistinguishable to human observers from real sounds. Project page:
https://www.yimingdou.com/hearing_hands/

</details>


### [180] [Text-Aware Image Restoration with Diffusion Models](https://arxiv.org/abs/2506.09993)
*Jaewon Min,Jin Hyeon Kim,Paul Hyunbin Cho,Jaeeun Lee,Jihye Park,Minkyu Park,Sangpil Kim,Hyunhee Park,Seungryong Kim*

Main category: cs.CV

TL;DR: The paper introduces Text-Aware Image Restoration (TAIR), a new task focused on preserving textual fidelity in degraded images, alongside general visual content restoration.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based methods lack the ability to faithfully reconstruct textual regions in degraded images without inducing text-image hallucinations.

Method: A multi-task diffusion framework, TeReDiff, is proposed, which integrates diffusion models with a text-spotting module to mutually benefit from joint training, with extracted rich text representations serving as prompts during denoising.

Result: Experiments reveal that the proposed approach significantly outperforms existing methods, leading to notable improvements in text recognition accuracy.

Conclusion: Text-Aware Image Restoration (TAIR) and the benchmark SA-Text provide critical advancements in aligning visual restoration tasks with high text fidelity, addressing key limitations in existing methods.

Abstract: Image restoration aims to recover degraded images. However, existing
diffusion-based restoration methods, despite great success in natural image
restoration, often struggle to faithfully reconstruct textual regions in
degraded images. Those methods frequently generate plausible but incorrect
text-like patterns, a phenomenon we refer to as text-image hallucination. In
this paper, we introduce Text-Aware Image Restoration (TAIR), a novel
restoration task that requires the simultaneous recovery of visual contents and
textual fidelity. To tackle this task, we present SA-Text, a large-scale
benchmark of 100K high-quality scene images densely annotated with diverse and
complex text instances. Furthermore, we propose a multi-task diffusion
framework, called TeReDiff, that integrates internal features from diffusion
models into a text-spotting module, enabling both components to benefit from
joint training. This allows for the extraction of rich text representations,
which are utilized as prompts in subsequent denoising steps. Extensive
experiments demonstrate that our approach consistently outperforms
state-of-the-art restoration methods, achieving significant gains in text
recognition accuracy. See our project page: https://cvlab-kaist.github.io/TAIR/

</details>


### [181] [PlayerOne: Egocentric World Simulator](https://arxiv.org/abs/2506.09995)
*Yuanpeng Tu,Hao Luo,Xi Chen,Xiang Bai,Fan Wang,Hengshuang Zhao*

Main category: cs.CV

TL;DR: PlayerOne is a groundbreaking egocentric world simulator that creates immersive video experiences aligned to real-world human motion.


<details>
  <summary>Details</summary>
Motivation: To enable immersive and unrestricted exploration of realistic dynamic environments using egocentric perspectives.

Method: PlayerOne employs a coarse-to-fine training pipeline: pretraining on large-scale egocentric text-video pairs and finetuning on synchronous motion-video data. It includes part-disentangled motion injection for precise movement control and a joint reconstruction framework for consistent 4D modeling.

Result: PlayerOne demonstrates excellent generalization ability in controlling human movements and maintaining scene consistency across diverse scenarios.

Conclusion: This work introduces a novel approach to egocentric real-world simulation, opening pathways for advanced applications in world modeling and simulation.

Abstract: We introduce PlayerOne, the first egocentric realistic world simulator,
facilitating immersive and unrestricted exploration within vividly dynamic
environments. Given an egocentric scene image from the user, PlayerOne can
accurately construct the corresponding world and generate egocentric videos
that are strictly aligned with the real scene human motion of the user captured
by an exocentric camera. PlayerOne is trained in a coarse-to-fine pipeline that
first performs pretraining on large-scale egocentric text-video pairs for
coarse-level egocentric understanding, followed by finetuning on synchronous
motion-video data extracted from egocentric-exocentric video datasets with our
automatic construction pipeline. Besides, considering the varying importance of
different components, we design a part-disentangled motion injection scheme,
enabling precise control of part-level movements. In addition, we devise a
joint reconstruction framework that progressively models both the 4D scene and
video frames, ensuring scene consistency in the long-form video generation.
Experimental results demonstrate its great generalization ability in precise
control of varying human movements and worldconsistent modeling of diverse
scenarios. It marks the first endeavor into egocentric real-world simulation
and can pave the way for the community to delve into fresh frontiers of world
modeling and its diverse applications.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [182] [EdgeProfiler: A Fast Profiling Framework for Lightweight LLMs on Edge Using Analytical Model](https://arxiv.org/abs/2506.09061)
*Alyssa Pinnock,Shakya Jayakody,Kawsher A Roxy,Md Rubel Ahmed*

Main category: cs.DC

TL;DR: EdgeProfiler is a fast profiling framework for lightweight LLMs on edge systems, optimizing accuracy, energy, and computational efficiency through aggressive quantization.


<details>
  <summary>Details</summary>
Motivation: To address the resource challenges associated with deploying large language models on edge systems by developing effective profiling tools.

Method: The framework uses aggressive quantization (e.g., 4-bit), analytical modeling, and memory-specific constraints to profile parameters like latency, energy consumption, and FLOPs of compact LLMs.

Result: 4-bit quantization cuts memory usage by 60-70%, boosts inference speeds by 2-3x, and reduces energy consumption by 35-50% while maintaining accuracy within 2-5% of full precision.

Conclusion: EdgeProfiler makes lightweight LLMs feasible for edge hardware by effectively balancing efficiency and accuracy, enabling broader deployment in resource-constrained environments.

Abstract: This paper introduces EdgeProfiler, a fast profiling framework designed for
evaluating lightweight Large Language Models (LLMs) on edge systems. While LLMs
offer remarkable capabilities in natural language understanding and generation,
their high computational, memory, and power requirements often confine them to
cloud environments. EdgeProfiler addresses these challenges by providing a
systematic methodology for assessing LLM performance in resource-constrained
edge settings. The framework profiles compact LLMs, including TinyLLaMA,
Gemma3.1B, Llama3.2-1B, and DeepSeek-r1-1.5B, using aggressive quantization
techniques and strict memory constraints. Analytical modeling is used to
estimate latency, FLOPs, and energy consumption. The profiling reveals that
4-bit quantization reduces model memory usage by approximately 60-70%, while
maintaining accuracy within 2-5% of full-precision baselines. Inference speeds
are observed to improve by 2-3x compared to FP16 baselines across various edge
devices. Power modeling estimates a 35-50% reduction in energy consumption for
INT4 configurations, enabling practical deployment on hardware such as
Raspberry Pi 4/5 and Jetson Orin Nano Super. Our findings emphasize the
importance of efficient profiling tailored to lightweight LLMs in edge
environments, balancing accuracy, energy efficiency, and computational
feasibility.

</details>


### [183] [Multi-GPU Acceleration of PALABOS Fluid Solver using C++ Standard Parallelism](https://arxiv.org/abs/2506.09242)
*Jonas Latt,Christophe Coreixas*

Main category: cs.DC

TL;DR: The paper discusses the GPU adaptation of the Palabos software for lattice Boltzmann simulations, focusing on a hybrid CPU-GPU model for performance. The implementation uses modern C++ and achieves competitive performance through hardware-agnostic designs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance the Palabos lattice Boltzmann library by leveraging GPU acceleration for better performance while maintaining compatibility with its CPU-based legacy codebase.

Method: The authors adopt a hybrid execution model, combining object-oriented CPU design with data-oriented GPU programming. They use modern C++ features to create hardware-agnostic computational kernels, enabling GPU acceleration without relying on external libraries or non-standard extensions.

Result: The GPU-enabled Palabos demonstrates high performance comparable to native CUDA-based solvers on single GPUs. It also exhibits strong scalability across multiple GPUs through several three-dimensional fluid dynamics benchmarks.

Conclusion: The paper concludes that it is feasible to achieve efficient GPU acceleration while preserving the modularity and compatibility of the original Palabos library using modern C++ methodologies.

Abstract: This article presents the principles, software architecture, and performance
analysis of the GPU port of the lattice Boltzmann software library Palabos (J.
Latt et al., "Palabos: Parallel lattice Boltzmann solver", Comput. Math. Appl.
81, 334-350, (2021)). A hybrid CPU-GPU execution model is adopted, in which
numerical components are selectively assigned to either the CPU or the GPU,
depending on considerations of performance or convenience. This design enables
a progressive porting strategy, allowing most features of the original
CPU-based codebase to be gradually and seamlessly adapted to GPU execution. The
new architecture builds upon two complementary paradigms: a classical
object-oriented structure for CPU execution, and a data-oriented counterpart
for GPUs, which reproduces the modularity of the original code while
eliminating object-oriented overhead detrimental to GPU performance. Central to
this approach is the use of modern C++, including standard parallel algorithms
and template metaprogramming techniques, which permit the generation of
hardware-agnostic computational kernels. This facilitates the development of
user-defined, GPU-accelerated components such as collision operators or
boundary conditions, while preserving compatibility with the existing codebase
and avoiding the need for external libraries or non-standard language
extensions. The correctness and performance of the GPU-enabled Palabos are
demonstrated through a series of three-dimensional multiphysics benchmarks,
including the laminar-turbulent transition in a Taylor-Green vortex, lid-driven
cavity flow, and pore-scale flow in Berea sandstone. Despite the high-level
abstraction of the implementation, the single-GPU performance is similar to
CUDA-native solvers, and multi-GPU tests exhibit good weak and strong scaling
across all test cases.

</details>


### [184] [A Survey of End-to-End Modeling for Distributed DNN Training: Workloads, Simulators, and TCO](https://arxiv.org/abs/2506.09275)
*Jonas Svedas,Hannah Watson,Nathan Laubeuf,Diksha Moolchandani,Abubakr Nada,Arjun Singh,Dwaipayan Biswas,James Myers,Debjyoti Bhattacharjee*

Main category: cs.DC

TL;DR: This paper surveys distributed DNN training simulators, analyzing workload representation, simulation infrastructure, and cost models, while identifying trends, limitations, and challenges.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of scaling machine learning systems efficiently amidst growing model complexity and hardware limitations.

Method: A structured review of distributed DNN training simulators, including comparison tables and analyses of key dimensions such as workload representation, simulation infrastructure, and TCO models.

Result: Comprehensive synthesis and comparison of simulators' capabilities, with insights into emerging trends, limitations, and open research challenges.

Conclusion: The work aids in informed decision-making for designing efficient distributed training systems by summarizing tools and highlighting areas for future research.

Abstract: Distributed deep neural networks (DNNs) have become a cornerstone for scaling
machine learning to meet the demands of increasingly complex applications.
However, the rapid growth in model complexity far outpaces CMOS technology
scaling, making sustainable and efficient system design a critical challenge.
Addressing this requires coordinated co-design across software, hardware, and
technology layers. Due to the prohibitive cost and complexity of deploying
full-scale training systems, simulators play a pivotal role in enabling this
design exploration. This survey reviews the landscape of distributed DNN
training simulators, focusing on three major dimensions: workload
representation, simulation infrastructure, and models for total cost of
ownership (TCO) including carbon emissions. It covers how workloads are
abstracted and used in simulation, outlines common workload representation
methods, and includes comprehensive comparison tables covering both simulation
frameworks and TCO/emissions models, detailing their capabilities, assumptions,
and areas of focus. In addition to synthesizing existing tools, the survey
highlights emerging trends, common limitations, and open research challenges
across the stack. By providing a structured overview, this work supports
informed decision-making in the design and evaluation of distributed training
systems.

</details>


### [185] [TTrace: Lightweight Error Checking and Diagnosis for Distributed Training](https://arxiv.org/abs/2506.09280)
*Haitian Jiang,Shaowei Zhu,Zhen Zhang,Zhenyu Song,Xinwei Fu,Zhen Jia,Yida Wang,Jinyang Li*

Main category: cs.DC

TL;DR: Distributed training of neural networks is prone to silent bugs which adversely affect training outcomes. TTrace is designed to detect and localize those bugs by comparing tensors with a trusted reference, demonstrating success in various scenarios.


<details>
  <summary>Details</summary>
Motivation: Distributed neural network training, often used for large models like LLMs, is susceptible to silent bugs that compromise outcomes without explicit error signals. Debugging these bugs in such complex setups is highly challenging.

Method: TTrace collects intermediate tensors from distributed training and compares them to values from a trusted single-device reference using a novel floating-point comparison threshold to differentiate actual bugs from round-off errors.

Result: TTrace successfully detects 14 bugs (11 existing and 3 new) in the Megatron-LM framework, requiring minimal code changes and performing well in various training setups, including low-precision formats.

Conclusion: TTrace provides an efficient and reliable solution for detecting and localizing silent bugs during distributed neural network training, enhancing debugging practices especially for complex scenarios involving low precision.

Abstract: Distributed training is essential for scaling the training of large neural
network models, such as large language models (LLMs), across thousands of GPUs.
However, the complexity of distributed training programs makes them
particularly prone to silent bugs, which do not produce explicit error signal
but lead to incorrect training outcome. Effectively detecting and localizing
such silent bugs in distributed training is challenging. Common debugging
practice using metrics like training loss or gradient norm curves can be
inefficient and ineffective. Additionally, obtaining intermediate tensor values
and determining whether they are correct during silent bug localization is
difficult, particularly in the context of low-precision training.
  To address those challenges, we design and implement TTrace, the first system
capable of detecting and localizing silent bugs in distributed training. TTrace
collects intermediate tensors from distributing training in a fine-grained
manner and compares them against those from a trusted single-device reference
implementation. To properly compare the floating-point values in the tensors,
we propose novel mathematical analysis that provides a guideline for setting
thresholds, enabling TTrace to distinguish bug-induced errors from
floating-point round-off errors. Experimental results demonstrate that TTrace
effectively detects 11 existing bugs and 3 new bugs in the widely used
Megatron-LM framework, while requiring fewer than 10 lines of code change.
TTrace is effective in various training recipes, including low-precision
recipes involving BF16 and FP8.

</details>


### [186] [ScalableHD: Scalable and High-Throughput Hyperdimensional Computing Inference on Multi-Core CPUs](https://arxiv.org/abs/2506.09282)
*Dhruv Parikh,Viktor Prasanna*

Main category: cs.DC

TL;DR: Hyperdimensional Computing (HDC) performs highly parallel, noise-resistant information manipulation with high-dimensional vectors, but suffers from accuracy limitations in traditional methods. ScalableHD addresses these issues by enhancing inference for general-purpose multi-core CPUs with innovative pipelining and memory optimization techniques.


<details>
  <summary>Details</summary>
Motivation: There is a lack of efficient methodologies for HDC inference on general-purpose multi-core CPUs, as prior research focuses on specialized hardware like FPGAs and GPUs, which restricts the widespread adoption of HDC.

Method: ScalableHD employs a two-stage pipelined execution model with parallelized stages, producer-consumer mechanisms for intermediate results, and optimization techniques like memory tiling and NUMA-aware binding. Two separate execution variants are designed for small and large batch sizes to balance workload characteristics and boost performance.

Result: ScalableHD demonstrates a throughput speedup of up to 10x compared to TorchHD across various tasks without compromising accuracy. It also shows robust scalability, achieving near-proportional throughput increases when more cores are added.

Conclusion: ScalableHD provides a scalable, high-throughput solution for HDC inference on multi-core CPUs with advanced execution and optimization strategies, setting a new benchmark for performance and scalability in HDC applications.

Abstract: Hyperdimensional Computing (HDC) is a brain-inspired computing paradigm that
represents and manipulates information using high-dimensional vectors, called
hypervectors (HV). Traditional HDC methods, while robust to noise and
inherently parallel, rely on single-pass, non-parametric training and often
suffer from low accuracy. To address this, recent approaches adopt iterative
training of base and class HVs, typically accelerated on GPUs. Inference,
however, remains lightweight and well-suited for real-time execution. Yet,
efficient HDC inference has been studied almost exclusively on specialized
hardware such as FPGAs and GPUs, with limited attention to general-purpose
multi-core CPUs. To address this gap, we propose ScalableHD for scalable and
high-throughput HDC inference on multi-core CPUs. ScalableHD employs a
two-stage pipelined execution model, where each stage is parallelized across
cores and processes chunks of base and class HVs. Intermediate results are
streamed between stages using a producer-consumer mechanism, enabling
on-the-fly consumption and improving cache locality. To maximize performance,
ScalableHD integrates memory tiling and NUMA-aware worker-to-core binding.
Further, it features two execution variants tailored for small and large batch
sizes, each designed to exploit compute parallelism based on workload
characteristics while mitigating the memory-bound compute pattern that limits
HDC inference performance on modern multi-core CPUs. ScalableHD achieves up to
10x speedup in throughput (samples per second) over state-of-the-art baselines
such as TorchHD, across a diverse set of tasks ranging from human activity
recognition to image classification, while preserving task accuracy.
Furthermore, ScalableHD exhibits robust scalability: increasing the number of
cores yields near-proportional throughput improvements.

</details>


### [187] [SLED: A Speculative LLM Decoding Framework for Efficient Edge Serving](https://arxiv.org/abs/2506.09397)
*Xiangchen Li,Dimitrios Spatharakis,Saeid Ghafouri,Jiakun Fan,Dimitrios Nikolopoulos*

Main category: cs.DC

TL;DR: This paper introduces SLED, a new strategy for edge computing using speculative decoding to optimize LLM inference with better efficiency without compromising accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in efficiently deploying large language models (LLMs) at the edge due to hardware constraints like limited memory and power.

Method: The method, SLED, uses speculative decoding to distribute computation between lightweight draft models on edge devices and a shared server using a precise target model.

Result: Initial experiments show reduced latency, improved energy efficiency, and increased inference sessions without model accuracy loss.

Conclusion: SLED effectively balances efficiency and accuracy for edge inferencing of LLMs, enabling better use of heterogeneous device environments.

Abstract: Regardless the advancements in device capabilities, efficient inferencing
advanced large language models (LLMs) at the edge remains challenging due to
limited device memory and power constraints. Existing strategies, such as
aggressive quantization, pruning, or remote inference, trade accuracy for
efficiency or lead to substantial cost burdens. This position paper introduces
a new approach that leverages speculative decoding, previously viewed primarily
as a decoding acceleration technique for autoregressive generation of LLMs, as
a promising approach specifically adapted for edge computing by orchestrating
computation across heterogeneous devices. We propose SLED, a method that allows
lightweight edge devices to draft multiple candidate tokens locally using
diverse draft models, while a single, shared edge server efficiently batches
and verifies the tokens utilizing a more precise target model. This approach
supports device heterogeneity and reduces server-side memory footprint by
avoiding the need to deploy multiple target models. Our initial experiments
with Jetson Orin Nano, Raspberry Pi 5, and an RTX 6000 edge server indicate
substantial benefits: significantly reduced latency, improved energy
efficiency, and increased concurrent inference sessions, all without
sacrificing model accuracy.

</details>


### [188] [Efficient Task Graph Scheduling for Parallel QR Factorization in SLSQP](https://arxiv.org/abs/2506.09463)
*Soumyajit Chatterjee,Rahul Utkoor,Uppu Eshwar,Sathya Peri,V. Krishna Nandivada*

Main category: cs.DC

TL;DR: The study introduces a novel two-queue scheduling method for QR factorization in SLSQP, achieving a 10x performance boost over the sequential approach.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the limitation in QR factorization where current methods fail to utilize intermediate results required for back-substitution in SLSQP effectively.

Method: The method involves a two-queue scheduling system implemented in C++, which enhances compiler optimizations and permits storing intermediate outcomes crucial for SLSQP's iterative logic.

Result: Empirical evaluation reveals a 10x performance increase compared to the traditional sequential QR approach in SLSQP.

Conclusion: The proposed two-queue scheduling technique successfully optimizes QR factorization for SLSQP, making it significantly more efficient while retaining critical intermediate data.

Abstract: Efficient task scheduling is paramount in parallel programming on multi-core
architectures, where tasks are fundamental computational units. QR
factorization is a critical sub-routine in Sequential Least Squares Quadratic
Programming (SLSQP) for solving non-linear programming (NLP) problems. QR
factorization decomposes a matrix into an orthogonal matrix Q and an upper
triangular matrix R, which are essential for solving systems of linear
equations arising from optimization problems. SLSQP uses an in-place version of
QR factorization, which requires storing intermediate results for the next
steps of the algorithm. Although DAG-based approaches for QR factorization are
prevalent in the literature, they often lack control over the intermediate
kernel results, providing only the final output matrices Q and R. This
limitation is particularly challenging in SLSQP, where intermediate results of
QR factorization are crucial for back-substitution logic at each iteration. Our
work introduces novel scheduling techniques using a two-queue approach to
execute the QR factorization kernel effectively. This approach, implemented in
high-level C++ programming language, facilitates compiler optimizations and
allows storing intermediate results required by back-substitution logic.
Empirical evaluations demonstrate substantial performance gains, including a
10x improvement over the sequential QR version of the SLSQP algorithm.

</details>


### [189] [On the Performance of Cloud-based ARM SVE for Zero-Knowledge Proving Systems](https://arxiv.org/abs/2506.09505)
*Dumitrel Loghin,Shuang Liang,Shengwei Liu,Xiong Liu,Pingcheng Ruan,Zhigang Ye*

Main category: cs.DC

TL;DR: Current ARM CPUs are less efficient than x86-64 CPUs in ZKP operations, but improvements in vector size could make ARM more competitive.


<details>
  <summary>Details</summary>
Motivation: Explore whether ARM servers can outperform x86-64 servers in zero-knowledge proof tasks that are integral to Web3 and blockchain scaling.

Method: Analyzed the performance of ARM servers equipped with SVE against x86-64 servers with AVX/AVX512 for Merkle tree generation.

Result: ARM-based servers are slower by a factor of 1.4X-1.6X compared to x86-64 servers due to smaller vector size and lower clock frequency.

Conclusion: ARM servers have potential to outperform x86-64 servers in ZKP tasks if their vector size increases to 512 bits, leveraging their cost-effectiveness.

Abstract: Zero-knowledge proofs (ZKP) are becoming a gold standard in scaling
blockchains and bringing Web3 to life. At the same time, ZKP for transactions
running on the Ethereum Virtual Machine require powerful servers with hundreds
of CPU cores. The current zkProver implementation from Polygon is optimized for
x86-64 CPUs by vectorizing key operations, such as Merkle tree building with
Poseidon hashes over the Goldilocks field, with Advanced Vector Extensions (AVX
and AVX512). With these optimizations, a ZKP for a batch of transactions is
generated in less than two minutes. With the advent of cloud servers with ARM
which are at least 10% cheaper than x86-64 servers and the implementation of
ARM Scalable Vector Extension (SVE), we wonder if ARM servers can take over
their x86-64 counterparts. Unfortunately, our analysis shows that current ARM
CPUs are not a match for their x86-64 competitors. Graviton4 from Amazon Web
Services (AWS) and Axion from Google Cloud Platform (GCP) are 1.6X and 1.4X
slower compared to the latest AMD EPYC and Intel Xeon servers from AWS with AVX
and AVX512, respectively, when building a Merkle tree with over four million
leaves. This low performance is due to (1) smaller vector size in these ARM
CPUs (128 bits versus 512 bits in AVX512) and (2) lower clock frequency. On the
other hand, ARM SVE/SVE2 Instruction Set Architecture (ISA) is at least as
powerful as AVX/AVX512 but more flexible. Moreover, we estimate that increasing
the vector size to 512 bits will enable higher performance in ARM CPUs compared
to their x86-64 counterparts while maintaining their price advantage.

</details>


### [190] [Understanding the Performance and Power of LLM Inferencing on Edge Accelerators](https://arxiv.org/abs/2506.09554)
*Mayank Arya,Yogesh Simmhan*

Main category: cs.DC

TL;DR: The paper evaluates the feasibility and performance of running large language model (LLM) inference on Nvidia Jetson Orin AGX edge accelerators, analyzing trade-offs in efficiency, speed, and resources.


<details>
  <summary>Details</summary>
Motivation: To explore the feasibility of hosting and running inference tasks for large language models locally on power-efficient edge devices like the Nvidia Jetson Orin AGX, especially for privacy-sensitive and mission-critical applications.

Method: The study evaluates LLM inference of four state-of-the-art models (2.7B-32.8B parameters) on Nvidia Jetson Orin AGX by varying batch sizes, sequence lengths, quantization levels, and operating in different power modes. It examines performance metrics like latency, throughput, perplexity, power, and energy consumption.

Result: The evaluation reveals insights, such as the decrease in token throughput with increased sequence length and slower performance for smaller LLMs with quantization. It also highlights trade-offs in efficiency, inference speed, and resource utilization.

Conclusion: The results provide useful guidelines for optimizing the practical deployment of large language models on edge accelerators, balancing between performance and resource efficiency.

Abstract: Large Language Models (LLMs) have demonstrated exceptional benefits to a wide
range of domains, for tasks as diverse as code generation and robot navigation.
While LLMs are usually served from cloud data centers, mission-critical and
privacy-sensitive applications may require local hosting of open LLM models.
Given the large GPU memory footprint needed for LLMs, edge accelerators such as
Nvidia Jetson Orin AGX with 64GB of shared GPU-CPU RAM are a compelling choice.
However, the feasibility and performance of LLM inference on edge accelerators
is under-explored. This study presents a detailed evaluation of LLM inference
on the NVIDIA Jetson Orin AGX, on four SOTA models ranging from 2.7B to 32.8B
parameters, such as Meta Llama3.1, Microsoft-Phi2, Deepseek-R1-Qwen.We
investigate the impact of varying batch sizes, sequence lengths, and
quantization levels on latency, throughput, and perplexity, and also explore
various custom power modes on the Orin AGX to perform power and energy
consumption analysis. Our findings offer interesting insights on the trade-offs
between efficiency, inference speed and resource use, e.g., increasing the
sequence length causes a decrease in token throughput and quantization causes
smaller LLMs to be slower. These results can help optimize LLM serving on edge
accelerators for practical applications.

</details>


### [191] [Frosty for partial synchrony](https://arxiv.org/abs/2506.09823)
*Stephen Buttolph,Andrew Lewis-Pye,Kevin Sekniqi*

Main category: cs.DC

TL;DR: The paper improves 'Frosty', a liveness module of the Snowman consensus protocol, to work under partial synchrony.


<details>
  <summary>Details</summary>
Motivation: The existing 'Frosty' module for Snowman ensures liveness but only under strong synchrony assumptions. This limitation needs to be addressed to make it compatible with partially synchronous systems.

Method: The authors propose modifications to the Frosty module to adapt it to the partially synchronous setting of Snowman.

Result: The paper successfully adapts Frosty to operate with Snowman under partial synchrony.

Conclusion: This adaptation increases the robustness of Snowman by enabling Frosty to function in partially synchronous environments, overcoming its prior limitations.

Abstract: Snowman is the consensus protocol used by blockchains on Avalanche. Recent
work has shown both how to augment Snowman with a `liveness' module called
`Frosty' that protects against liveness attacks, and also how to modify Snowman
so as to be consistent in partial synchrony. Since Frosty assumes (a strong
form of) synchrony, the aim of this note is to show how to modify Frosty to
deal with the partially synchronous version of Snowman.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [192] [Synergizing Reinforcement Learning and Genetic Algorithms for Neural Combinatorial Optimization](https://arxiv.org/abs/2506.09404)
*Shengda Gu,Kai Li,Junliang Xing,Yifan Zhang,Jian Cheng*

Main category: cs.LG

TL;DR: This study proposes a method called Evolutionary Augmentation Mechanism (EAM) that combines deep reinforcement learning (DRL) with genetic algorithms (GAs) to solve challenging combinatorial optimization problems efficiently.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of DRL methods (limited exploration and local optima susceptibility) and overcome the inefficiency of evolutionary algorithms like GAs, while solving combinatorial optimization problems effectively.

Method: The authors introduced EAM, a framework where policies generated by DRL are refined using genetic operations (e.g., crossover and mutation). Evolved solutions are reintroduced to the DRL training process, benefiting exploration and convergence. They also provided theoretical guarantees via KL divergence analysis.

Result: EAM significantly improves solution quality and training efficiency when applied to benchmark problems like TSP, CVRP, PCTSP, and OP, outperforming strong baselines.

Conclusion: The study proves EAM's effectiveness as a plug-and-play approach that enhances DRL solvers with evolutionary strategies, making it a robust method for challenging optimization tasks.

Abstract: Combinatorial optimization problems are notoriously challenging due to their
discrete structure and exponentially large solution space. Recent advances in
deep reinforcement learning (DRL) have enabled the learning heuristics directly
from data. However, DRL methods often suffer from limited exploration and
susceptibility to local optima. On the other hand, evolutionary algorithms such
as Genetic Algorithms (GAs) exhibit strong global exploration capabilities but
are typically sample inefficient and computationally intensive. In this work,
we propose the Evolutionary Augmentation Mechanism (EAM), a general and
plug-and-play framework that synergizes the learning efficiency of DRL with the
global search power of GAs. EAM operates by generating solutions from a learned
policy and refining them through domain-specific genetic operations such as
crossover and mutation. These evolved solutions are then selectively reinjected
into the policy training loop, thereby enhancing exploration and accelerating
convergence. We further provide a theoretical analysis that establishes an
upper bound on the KL divergence between the evolved solution distribution and
the policy distribution, ensuring stable and effective policy updates. EAM is
model-agnostic and can be seamlessly integrated with state-of-the-art DRL
solvers such as the Attention Model, POMO, and SymNCO. Extensive results on
benchmark problems (e.g., TSP, CVRP, PCTSP, and OP) demonstrate that EAM
significantly improves both solution quality and training efficiency over
competitive baselines.

</details>


### [193] [Llama-Affinity: A Predictive Antibody Antigen Binding Model Integrating Antibody Sequences with Llama3 Backbone Architecture](https://arxiv.org/abs/2506.09052)
*Delower Hossain,Ehsan Saghapour,Kevin Song,Jake Y. Chen*

Main category: cs.LG

TL;DR: This paper introduces LlamaAffinity, an advanced antibody-antigen binding affinity prediction model using Llama 3 architecture. It outperforms existing methods in accuracy, efficiency, and other performance metrics.


<details>
  <summary>Details</summary>
Motivation: The paper aims to enhance antibody-antigen binding affinity prediction, a critical aspect in therapeutic antibody development, by addressing the inefficiencies and limitations of conventional experimental methods with AI-based approaches.

Method: The researchers developed LlamaAffinity, leveraging the Llama 3 large language model and antibody sequence data from the Observed Antibody Space (OAS) database, to predict binding affinities more effectively.

Result: LlamaAffinity demonstrated superior performance over state-of-the-art models (AntiFormer, AntiBERTa, AntiBERTy), achieving high accuracy (0.9640), F1-score (0.9643), precision (0.9702), recall (0.9586), and AUC-ROC (0.9936). Additionally, it showcased five-fold greater computational efficiency with reduced training time.

Conclusion: The integration of advanced AI models like Llama 3 in antibody affinity prediction provides a more accurate, efficient, and promising tool for therapeutic antibody development.

Abstract: Antibody-facilitated immune responses are central to the body's defense
against pathogens, viruses, and other foreign invaders. The ability of
antibodies to specifically bind and neutralize antigens is vital for
maintaining immunity. Over the past few decades, bioengineering advancements
have significantly accelerated therapeutic antibody development. These
antibody-derived drugs have shown remarkable efficacy, particularly in treating
cancer, SARS-CoV-2, autoimmune disorders, and infectious diseases.
Traditionally, experimental methods for affinity measurement have been
time-consuming and expensive. With the advent of artificial intelligence, in
silico medicine has been revolutionized; recent developments in machine
learning, particularly the use of large language models (LLMs) for representing
antibodies, have opened up new avenues for AI-based design and improved
affinity prediction. Herein, we present an advanced antibody-antigen binding
affinity prediction model (LlamaAffinity), leveraging an open-source Llama 3
backbone and antibody sequence data sourced from the Observed Antibody Space
(OAS) database. The proposed approach shows significant improvement over
existing state-of-the-art (SOTA) methods (AntiFormer, AntiBERTa, AntiBERTy)
across multiple evaluation metrics. Specifically, the model achieved an
accuracy of 0.9640, an F1-score of 0.9643, a precision of 0.9702, a recall of
0.9586, and an AUC-ROC of 0.9936. Moreover, this strategy unveiled higher
computational efficiency, with a five-fold average cumulative training time of
only 0.46 hours, significantly lower than in previous studies.

</details>


### [194] [Spiking Neural Models for Decision-Making Tasks with Learning](https://arxiv.org/abs/2506.09087)
*Sophie Jaffard,Giulia Mezzadri,Patricia Reynaud-Bouret,Etienne Tanré*

Main category: cs.LG

TL;DR: The paper introduces a biologically plausible Spiking Neural Network (SNN) for decision-making, modeling neuronal activities with a multivariate Hawkes process. It bridges the gap between cognitive and biological mechanisms.


<details>
  <summary>Details</summary>
Motivation: To address the lack of learning mechanisms and biological plausibility in traditional decision-making models like Drift Diffusion Models (DDMs) and Poisson counter models.

Method: The authors propose an SNN model based on a multivariate Hawkes process, establish theoretical connections between DDM and Poisson counter models, and validate predictions using an online categorization task.

Result: The study demonstrates that DDM can be approximated by spiking Poisson neurons and a specific DDM with correlated noise can be derived from a Hawkes network. The model achieves effective categorization and reaction time predictions.

Conclusion: This work advances the integration of biological neural mechanisms into cognitive models, improving our understanding of neural activity in decision-making behavior.

Abstract: In cognition, response times and choices in decision-making tasks are
commonly modeled using Drift Diffusion Models (DDMs), which describe the
accumulation of evidence for a decision as a stochastic process, specifically a
Brownian motion, with the drift rate reflecting the strength of the evidence.
In the same vein, the Poisson counter model describes the accumulation of
evidence as discrete events whose counts over time are modeled as Poisson
processes, and has a spiking neurons interpretation as these processes are used
to model neuronal activities. However, these models lack a learning mechanism
and are limited to tasks where participants have prior knowledge of the
categories. To bridge the gap between cognitive and biological models, we
propose a biologically plausible Spiking Neural Network (SNN) model for
decision-making that incorporates a learning mechanism and whose neurons
activities are modeled by a multivariate Hawkes process. First, we show a
coupling result between the DDM and the Poisson counter model, establishing
that these two models provide similar categorizations and reaction times and
that the DDM can be approximated by spiking Poisson neurons. To go further, we
show that a particular DDM with correlated noise can be derived from a Hawkes
network of spiking neurons governed by a local learning rule. In addition, we
designed an online categorization task to evaluate the model predictions. This
work provides a significant step toward integrating biologically relevant
neural mechanisms into cognitive models, fostering a deeper understanding of
the relationship between neural activity and behavior.

</details>


### [195] [FinHEAR: Human Expertise and Adaptive Risk-Aware Temporal Reasoning for Financial Decision-Making](https://arxiv.org/abs/2506.09080)
*Jiaxiang Chen,Mingxi Zou,Zhuo Wang,Qifan Wang,Dongning Sun,Chi Zhang,Zenglin Xu*

Main category: cs.LG

TL;DR: FinHEAR, a multi-agent framework grounded in behavioral economics, utilizes LLMs to address the complexities of financial decision-making, outperforming baselines in trend prediction and trading tasks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of LLMs in handling the nuanced aspects of financial decision-making, including behavioral patterns such as expert reliance and feedback-driven adjustments.

Method: FinHEAR uses an LLM-based multi-agent framework for analyzing historical data, current events, and retrieving expert-informed precedents, incorporating features like confidence-adjusted sizing and outcome-based refinement.

Result: Empirical results demonstrate FinHEAR's superior performance over strong baselines in accuracy and risk-adjusted financial returns.

Conclusion: The framework offers a robust and interpretable approach to improving financial decision-making using specialized LLM-based agents.

Abstract: Financial decision-making presents unique challenges for language models,
demanding temporal reasoning, adaptive risk assessment, and responsiveness to
dynamic events. While large language models (LLMs) show strong general
reasoning capabilities, they often fail to capture behavioral patterns central
to human financial decisions-such as expert reliance under information
asymmetry, loss-averse sensitivity, and feedback-driven temporal adjustment. We
propose FinHEAR, a multi-agent framework for Human Expertise and Adaptive
Risk-aware reasoning. FinHEAR orchestrates specialized LLM-based agents to
analyze historical trends, interpret current events, and retrieve
expert-informed precedents within an event-centric pipeline. Grounded in
behavioral economics, it incorporates expert-guided retrieval,
confidence-adjusted position sizing, and outcome-based refinement to enhance
interpretability and robustness. Empirical results on curated financial
datasets show that FinHEAR consistently outperforms strong baselines across
trend prediction and trading tasks, achieving higher accuracy and better
risk-adjusted returns.

</details>


### [196] [Enhanced Whole Page Optimization via Mixed-Grained Reward Mechanism-Adapted Language Models](https://arxiv.org/abs/2506.09084)
*Xinyuan Wang,Liang Wu,Yanjie Fu*

Main category: cs.LG

TL;DR: The paper introduces PageLLM, a method for optimizing search and recommendation systems using user feedback-based fine-tuning of large language models. It focuses on improving both page-level and item-level presentations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in fine-tuning pre-trained large language models for Whole Page Optimization (WPO), aiming to enhance user experience without relying on prohibitively expensive human-annotated data.

Method: The authors propose PageLLM, a reward-based fine-tuning approach using mixed-grained rewards from user feedback, addressing both page-level quality and item-level precision.

Result: PageLLM outperformed baseline methods and achieved a 0.44% GMV increase in an online A/B test involving more than 10 million users.

Conclusion: PageLLM provides a practical, scalable solution for optimizing large-scale recommendation and search systems through effective utilization of noisy user feedback.

Abstract: Optimizing the presentation of search and recommendation results is crucial
to enhancing user experience and engagement. Whole Page Optimization (WPO)
plays a pivotal role in this process, as it directly influences how information
is surfaced to users. While Pre-trained Large Language Models (LLMs) have
demonstrated remarkable capabilities in generating coherent and contextually
relevant content, fine-tuning these models for complex tasks like WPO presents
challenges. Specifically, the need for extensive human-annotated data to
mitigate issues such as hallucinations and model instability can be
prohibitively expensive, especially in large-scale systems that interact with
millions of items daily. In this work, we address the challenge of fine-tuning
LLMs for WPO by using user feedback as the supervision. Unlike manually labeled
datasets, user feedback is inherently noisy and less precise. To overcome this,
we propose a reward-based fine-tuning approach, PageLLM, which employs a
mixed-grained reward mechanism that combines page-level and item-level rewards.
The page-level reward evaluates the overall quality and coherence, while the
item-level reward focuses on the accuracy and relevance of key recommendations.
This dual-reward structure ensures that both the holistic presentation and the
critical individual components are optimized. We validate PageLLM on both
public and industrial datasets. PageLLM outperforms baselines and achieves a
0.44\% GMV increase in an online A/B test with over 10 million users,
demonstrating its real-world impact.

</details>


### [197] [LLM-ML Teaming: Integrated Symbolic Decoding and Gradient Search for Valid and Stable Generative Feature Transformation](https://arxiv.org/abs/2506.09085)
*Xinyuan Wang,Haoyue Bai,Nanxu Gong,Wangyang Ying,Sixun Dong,Xiquan Cui,Yanjie Fu*

Main category: cs.LG

TL;DR: The paper introduces a novel teaming framework combining LLMs and ML approaches for robust feature transformation, achieving better performance and fewer errors.


<details>
  <summary>Details</summary>
Motivation: Existing methods for feature transformation face challenges: traditional ML has low validity, and LLMs show instability. The paper aims to resolve these issues by combining their strengths.

Method: The proposed method involves a four-step teaming framework: generating high-quality examples with LLM, exploring latent space embeddings, distilling knowledge into a student LLM, and combining LLM-ML probabilities for stable outputs.

Result: The framework achieved a 5% improvement in downstream tasks and nearly halved error cases, proving its efficiency and robustness on diverse datasets.

Conclusion: Combining the strengths of symbolic generation from LLMs and gradient optimization from ML ensures better feature transformation. The approach also highlights LLMs' understanding of data.

Abstract: Feature transformation enhances data representation by deriving new features
from the original data. Generative AI offers potential for this task, but faces
challenges in stable generation (consistent outputs) and valid generation
(error-free sequences). Existing methods--traditional MLs' low validity and
LLMs' instability--fail to resolve both. We find that LLMs ensure valid syntax,
while ML's gradient-steered search stabilizes performance. To bridge this gap,
we propose a teaming framework combining LLMs' symbolic generation with ML's
gradient optimization. This framework includes four steps: (1) golden examples
generation, aiming to prepare high-quality samples with the ground knowledge of
the teacher LLM; (2) feature transformation sequence embedding and search,
intending to uncover potentially superior embeddings within the latent space;
(3) student LLM feature transformation, aiming to distill knowledge from the
teacher LLM; (4) LLM-ML decoder teaming, dedicating to combine ML and the
student LLM probabilities for valid and stable generation. The experiments on
various datasets show that the teaming policy can achieve 5\% improvement in
downstream performance while reducing nearly half of the error cases. The
results also demonstrate the efficiency and robustness of the teaming policy.
Additionally, we also have exciting findings on LLMs' capacity to understand
the original data.

</details>


### [198] [Integrating Asynchronous AdaBoost into Federated Learning: Five Real World Applications](https://arxiv.org/abs/2506.09090)
*Arthur Oghlukyan,Nuria Gomez Blas*

Main category: cs.LG

TL;DR: Enhanced asynchronous AdaBoost for Federated Learning (FL) demonstrates reduced communication and training overhead, improved scalability, convergence, and accuracy across five domains.


<details>
  <summary>Details</summary>
Motivation: To address communication inefficiencies and synchronization challenges in Federated Learning, particularly for diverse applications like edge devices and healthcare diagnostics.

Method: AdaBoost framework enhancements include adaptive communication scheduling and delayed weight compensation strategies, tested across different domains with comparative metrics.

Result: Empirical findings show a 20-35% reduction in training time, 30-40% reduction in communication overhead, with faster convergence achieved in fewer boosting rounds.

Conclusion: The enhanced AdaBoost algorithm exhibits improved efficiency, scalability, and robustness, making it highly applicable across various FL use cases.

Abstract: This paper presents a comprehensive analysis of an enhanced asynchronous
AdaBoost framework for federated learning (FL), focusing on its application
across five distinct domains: computer vision on edge devices, blockchain-based
model transparency, on-device mobile personalization, IoT anomaly detection,
and federated healthcare diagnostics. The proposed algorithm incorporates
adaptive communication scheduling and delayed weight compensation to reduce
synchronization frequency and communication overhead while preserving or
improving model accuracy. We examine how these innovations improve
communication efficiency, scalability, convergence, and robustness in each
domain. Comparative metrics including training time, communication overhead,
convergence iterations, and classification accuracy are evaluated using data
and estimates derived from Oghlukyan's enhanced AdaBoost framework. Empirical
results show, for example, training time reductions on the order of 20-35% and
communication overhead reductions of 30-40% compared to baseline AdaBoost, with
convergence achieved in significantly fewer boosting rounds. Tables and charts
summarize these improvements by domain. Mathematical formulations of the
adaptive scheduling rule and error-driven synchronization thresholds are
provided. Overall, the enhanced AdaBoost exhibits markedly improved efficiency
and robustness across diverse FL scenarios, suggesting broad applicability of
the approach.

</details>


### [199] [Variational Inference Optimized Using the Curved Geometry of Coupled Free Energy](https://arxiv.org/abs/2506.09091)
*Kenric Nelson,Igor Oliveira,Amenah Al-Najafi,Fode Zhang,Hon Keung Tony Ng*

Main category: cs.LG

TL;DR: This paper proposes a framework called Coupled Variational Autoencoder (CVAE) that improves variational inference using a strategy based on coupled free energy, targeting heavy-tailed distributions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the accuracy and robustness of variational models through better handling of curved geometries and heavy-tailed distributions, which are not well-modeled by traditional variational techniques.

Method: They introduce a coupled free energy framework to extend variational inference to the curved geometry of coupled exponential families and design a CVAE model that leverages this for both distributions and cost functions.

Result: The CVAE achieves a 3% improvement in the Fréchet Inception Distance on CelebA images compared to a standard VAE after just 5 epochs of training.

Conclusion: The approach demonstrates that modeling heavy-tailed distributions with coupled free energies can improve generative model performance, particularly in terms of robustness and handling outliers.

Abstract: We introduce an optimization framework for variational inference based on the
coupled free energy, extending variational inference techniques to account for
the curved geometry of the coupled exponential family. This family includes
important heavy-tailed distributions such as the generalized Pareto and the
Student's t. By leveraging the coupled free energy, which is equal to the
coupled evidence lower bound (ELBO) of the inverted probabilities, we improve
the accuracy and robustness of the learned model. The coupled generalization of
Fisher Information metric and the affine connection. The method is applied to
the design of a coupled variational autoencoder (CVAE). By using the coupling
for both the distributions and cost functions, the reconstruction metric is
derived to still be the mean-square average loss with modified constants. The
novelty comes from sampling the heavy-tailed latent distribution with its
associated coupled probability, which has faster decaying tails. The result is
the ability to train a model with high penalties in the tails, while assuring
that the training samples have a reduced number of outliers. The Wasserstein-2
or Fr\'echet Inception Distance of the reconstructed CelebA images shows the
CVAE has a 3\% improvement over the VAE after 5 epochs of training.

</details>


### [200] [CUDA-LLM: LLMs Can Write Efficient CUDA Kernels](https://arxiv.org/abs/2506.09092)
*Wentao Chen,Jiace Zhu,Qi Fan,Yehan Ma,An Zou*

Main category: cs.LG

TL;DR: This paper presents the FSR framework that enhances LLMs to automatically generate and optimize CUDA programs, achieving faster GPU performance.


<details>
  <summary>Details</summary>
Motivation: To address the difficulty of automating the generation of high-performance, architecture-aware CUDA code for GPUs using LLMs.

Method: The authors introduced a framework called Feature Search and Reinforcement (FSR) that iteratively validates and optimizes CUDA program correctness and performance through GPU-specific execution latency feedback.

Result: FSR-enabled LLMs generate CUDA kernels that demonstrate up to 179× speed improvements over generic human-written code while maintaining correctness.

Conclusion: Combining LLMs with FSR effectively automates the creation of hardware-specific, performance-driven GPU programs, showing great potential for AI and computational workloads.

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in
general-purpose code generation. However, generating the code which is deeply
hardware-specific, architecture-aware, and performance-critical, especially for
massively parallel GPUs, remains a complex challenge. In this work, we explore
the use of LLMs for the automated generation and optimization of CUDA programs,
with the goal of producing high-performance GPU kernels that fully exploit the
underlying hardware. To address this challenge, we propose a novel framework
called \textbf{Feature Search and Reinforcement (FSR)}. FSR jointly optimizes
compilation and functional correctness, as well as the runtime performance,
which are validated through extensive and diverse test cases, and measured by
actual kernel execution latency on the target GPU, respectively. This approach
enables LLMs not only to generate syntactically and semantically correct CUDA
code but also to iteratively refine it for efficiency, tailored to the
characteristics of the GPU architecture. We evaluate FSR on representative CUDA
kernels, covering AI workloads and computational intensive algorithms. Our
results show that LLMs augmented with FSR consistently guarantee correctness
rates. Meanwhile, the automatically generated kernels can outperform general
human-written code by a factor of up to 179$\times$ in execution speeds. These
findings highlight the potential of combining LLMs with performance
reinforcement to automate GPU programming for hardware-specific,
architecture-sensitive, and performance-critical applications.

</details>


### [201] [Merging Smarter, Generalizing Better: Enhancing Model Merging on OOD Data](https://arxiv.org/abs/2506.09093)
*Bingjie Zhang,Hongkang Li,Changlong Shi,Guowei Rong,He Zhao,Dongsheng Wang,Dandan Guo,Meng Wang*

Main category: cs.LG

TL;DR: This paper introduces LwPTV, a method for improving out-of-domain (OOD) performance in multi-task learning by employing layer-wise pruning of task vectors.


<details>
  <summary>Details</summary>
Motivation: Current model merging approaches in multi-task learning focus on improving in-domain (ID) dataset performance but often neglect effectiveness on OOD datasets, which limits their applicability.

Method: The authors proposed LwPTV, which uses a saliency score to measure parameter redundancy in task vectors and create mask vectors. This enables layer-wise pruning of task vectors while retaining pre-trained model parameters for merging.

Result: Extensive experiments demonstrate that LwPTV significantly enhances OOD performance while maintaining in-domain (ID) task performance.

Conclusion: LwPTV addresses the limitation of existing model merging methods by leveraging its flexibility to seamlessly integrate with them and improve performance on OOD tasks without compromising ID tasks.

Abstract: Multi-task learning (MTL) concurrently trains a model on diverse task
datasets to exploit common features, thereby improving overall performance
across the tasks. Recent studies have dedicated efforts to merging multiple
independent model parameters into a unified model for MTL, thus circumventing
the need for training data and expanding the scope of applicable scenarios of
MTL. However, current approaches to model merging predominantly concentrate on
enhancing performance within in-domain (ID) datasets, often overlooking their
efficacy on out-of-domain (OOD) datasets. In this work, we proposed LwPTV
(Layer-wise Pruning Task Vector) by building a saliency score, measuring the
redundancy of parameters in task vectors. Designed in this way ours can achieve
mask vector for each task and thus perform layer-wise pruning on the task
vectors, only keeping the pre-trained model parameters at the corresponding
layer in merged model. Owing to its flexibility, our method can be seamlessly
integrated with most of existing model merging methods to improve their
performance on OOD tasks. Extensive experiments demonstrate that the
application of our method results in substantial enhancements in OOD
performance while preserving the ability on ID tasks.

</details>


### [202] [Feature Shift Localization Network](https://arxiv.org/abs/2506.09101)
*Míriam Barrabés,Daniel Mas Montserrat,Kapal Dev,Alexander G. Ioannidis*

Main category: cs.LG

TL;DR: The study introduces 'FSL-Net,' a neural network designed to quickly and accurately identify feature shifts in high-dimensional datasets without re-training.


<details>
  <summary>Details</summary>
Motivation: Feature shifts in datasets from various domains (e.g., healthcare, financial, multi-sensor) can introduce errors that degrade downstream analytics. Existing methods for detecting shifts are either inaccurate or not scalable, necessitating better solutions.

Method: The authors developed the Feature Shift Localization Network (FSL-Net), a neural network that analyzes statistical properties of datasets to pinpoint feature shifts. Trained on various datasets, it generalizes to unseen cases without re-training.

Result: FSL-Net is effective for fast and accurate feature shift localization, even in unseen datasets, and performs well on large, high-dimensional data.

Conclusion: The proposed FSL-Net enhances the ability to identify dataset feature shifts, improving data quality and paving the way for more reliable downstream analyses. Tools and pre-trained models are made publicly available.

Abstract: Feature shifts between data sources are present in many applications
involving healthcare, biomedical, socioeconomic, financial, survey, and
multi-sensor data, among others, where unharmonized heterogeneous data sources,
noisy data measurements, or inconsistent processing and standardization
pipelines can lead to erroneous features. Localizing shifted features is
important to address the underlying cause of the shift and correct or filter
the data to avoid degrading downstream analysis. While many techniques can
detect distribution shifts, localizing the features originating them is still
challenging, with current solutions being either inaccurate or not scalable to
large and high-dimensional datasets. In this work, we introduce the Feature
Shift Localization Network (FSL-Net), a neural network that can localize
feature shifts in large and high-dimensional datasets in a fast and accurate
manner. The network, trained with a large number of datasets, learns to extract
the statistical properties of the datasets and can localize feature shifts from
previously unseen datasets and shifts without the need for re-training. The
code and ready-to-use trained model are available at
https://github.com/AI-sandbox/FSL-Net.

</details>


### [203] [Intra-Trajectory Consistency for Reward Modeling](https://arxiv.org/abs/2506.09096)
*Chaoyang Zhou,Shunyu Liu,Zengmao Wang,Di Wang,Rong-Cheng Tu,Bo Du,Dacheng Tao*

Main category: cs.LG

TL;DR: This paper introduces intra-trajectory consistency regularization to improve reward learning in large language models by leveraging generation probabilities for fine-grained signal propagation.


<details>
  <summary>Details</summary>
Motivation: Existing reward models rely on coarse-grained response-level scores, which struggle to generalize well on unseen responses due to a lack of fine-grained supervision.

Method: The paper proposes leveraging generation probabilities to propagate supervisory signals across response trajectories and introduces a Bayesian framework-based regularization to enforce consistent rewards for processes with higher next-token generation probabilities.

Result: The regularization improves the reward model performance on RewardBench, develops policies aligned with proximal optimality, and achieves better inference-time verification using the best-of-N strategy.

Conclusion: Using fine-grained signals from generation probabilities enhances reward model effectiveness, improving both policy alignment and inference-time verification outcomes in large language models.

Abstract: Reward models are critical for improving large language models (LLMs),
particularly in reinforcement learning from human feedback (RLHF) or
inference-time verification. Current reward modeling typically relies on scores
of overall responses to learn the outcome rewards for the responses. However,
since the response-level scores are coarse-grained supervision signals, the
reward model struggles to identify the specific components within a response
trajectory that truly correlate with the scores, leading to poor generalization
on unseen responses. In this paper, we propose to leverage generation
probabilities to establish reward consistency between processes in the response
trajectory, which allows the response-level supervisory signal to propagate
across processes, thereby providing additional fine-grained signals for reward
learning. Building on analysis under the Bayesian framework, we develop an
intra-trajectory consistency regularization to enforce that adjacent processes
with higher next-token generation probability maintain more consistent rewards.
We apply the proposed regularization to the advanced outcome reward model,
improving its performance on RewardBench. Besides, we show that the reward
model trained with the proposed regularization induces better DPO-aligned
policies and achieves better best-of-N (BON) inference-time verification
results. Our code is provided in https://github.com/chaoyang101/ICRM.

</details>


### [204] [FLoRIST: Singular Value Thresholding for Efficient and Accurate Federated Fine-Tuning of Large Language Models](https://arxiv.org/abs/2506.09199)
*Hariharan Ramesh,Jyotikrishna Dass*

Main category: cs.LG

TL;DR: FLoRIST introduces an efficient federated learning framework utilizing Low-Rank Adaptation (LoRA) for fine-tuning Large Language Models (LLMs).


<details>
  <summary>Details</summary>
Motivation: To address challenges in federated LoRA methods related to communication efficiency, model accuracy, and computational cost among heterogeneous clients.

Method: FLoRIST employs singular value decomposition on stacked local adapters to achieve mathematically accurate aggregation in a compact intermediate space, reducing communication and computational overhead.

Result: Extensive experiments show that FLoRIST significantly improves communication efficiency and provides competitive model performance across various datasets and LLMs.

Conclusion: FLoRIST achieves an effective balance between communication efficiency and model accuracy for federated setups involving both homogeneous and heterogeneous clients.

Abstract: Integrating Low-Rank Adaptation (LoRA) into federated learning offers a
promising solution for parameter-efficient fine-tuning of Large Language Models
(LLMs) without sharing local data. However, several methods designed for
federated LoRA present significant challenges in balancing communication
efficiency, model accuracy, and computational cost, particularly among
heterogeneous clients. These methods either rely on simplistic averaging of
local adapters, which introduces aggregation noise, require transmitting large
stacked local adapters, leading to poor communication efficiency, or
necessitate reconstructing memory-dense global weight-update matrix and
performing computationally expensive decomposition to design client-specific
low-rank adapters. In this work, we propose FLoRIST, a federated fine-tuning
framework that achieves mathematically accurate aggregation without incurring
high communication or computational overhead. Instead of constructing the full
global weight-update matrix at the server, FLoRIST employs an efficient
decomposition pipeline by performing singular value decomposition on stacked
local adapters separately. This approach operates within a compact intermediate
space to represent the accumulated information from local LoRAs. We introduce
tunable singular value thresholding for server-side optimal rank selection to
construct a pair of global low-rank adapters shared by all clients. Extensive
empirical evaluations across multiple datasets and LLMs demonstrate that
FLoRIST consistently strikes the best balance between superior communication
efficiency and competitive performance in both homogeneous and heterogeneous
setups.

</details>


### [205] [Scalable Spatiotemporal Inference with Biased Scan Attention Transformer Neural Processes](https://arxiv.org/abs/2506.09163)
*Daniel Jenson,Jhonathan Navott,Piotr Grynfelder,Mengyan Zhang,Makkunda Sharma,Elizaveta Semenova,Seth Flaxman*

Main category: cs.LG

TL;DR: The paper introduces the BSA-TNP, a Neural Process model that achieves high accuracy and scalability without compromising efficiency, suitable for applications like geology, epidemiology, and robotics.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need to alleviate the tradeoff between scalability and accuracy in Neural Processes for demanding applications in diverse fields.

Method: The paper proposes BSA-TNP, incorporating Kernel Regression Blocks, group-invariant biases, and memory-efficient Biased Scan Attention.

Result: BSA-TNP achieves state-of-the-art accuracy, supports modeling spatial-temporal processes, handles high-dimensional data, and performs efficient inference even with large datasets.

Conclusion: The architecture offers a scalable yet accurate approach, redefining Neural Processes by enabling efficient modeling of complex processes across various domains.

Abstract: Neural Processes (NPs) are a rapidly evolving class of models designed to
directly model the posterior predictive distribution of stochastic processes.
While early architectures were developed primarily as a scalable alternative to
Gaussian Processes (GPs), modern NPs tackle far more complex and data hungry
applications spanning geology, epidemiology, climate, and robotics. These
applications have placed increasing pressure on the scalability of these
models, with many architectures compromising accuracy for scalability. In this
paper, we demonstrate that this tradeoff is often unnecessary, particularly
when modeling fully or partially translation invariant processes. We propose a
versatile new architecture, the Biased Scan Attention Transformer Neural
Process (BSA-TNP), which introduces Kernel Regression Blocks (KRBlocks),
group-invariant attention biases, and memory-efficient Biased Scan Attention
(BSA). BSA-TNP is able to: (1) match or exceed the accuracy of the best models
while often training in a fraction of the time, (2) exhibit translation
invariance, enabling learning at multiple resolutions simultaneously, (3)
transparently model processes that evolve in both space and time, (4) support
high dimensional fixed effects, and (5) scale gracefully -- running inference
with over 1M test points with 100K context points in under a minute on a single
24GB GPU.

</details>


### [206] [Too Big to Think: Capacity, Memorization, and Generalization in Pre-Trained Transformers](https://arxiv.org/abs/2506.09099)
*Joshua Barron,Devin White*

Main category: cs.LG

TL;DR: The study explores the trade-off between generalization and memorization in Transformer-based language models of varying sizes, finding that larger models prioritize memorization while smaller ones generalize better, but neither achieves both effectively when trained jointly.


<details>
  <summary>Details</summary>
Motivation: The research aims to understand how model capacity affects the trade-off between memorization and generalization in large language models, which remains a key question in the field.

Method: The authors pre-trained Transformer models of varying capacities on synthetic character-level tasks designed to test either arithmetic generalization or factual memorization, both in isolation and conjointly.

Result: Smaller models excel at extrapolating to unseen cases but struggle with memorization, whereas larger models effectively memorize but fail at generalization. Joint training on both tasks leads to failure in generalization across all models.

Conclusion: Model capacity significantly influences learning behavior, suggesting an intrinsic trade-off between memorization and generalization, which has implications for small language model design and deployment.

Abstract: The relationship between memorization and generalization in large language
models (LLMs) remains an open area of research, with growing evidence that the
two are deeply intertwined. In this work, we investigate this relationship by
pre-training a series of capacity-limited Transformer models from scratch on
two synthetic character-level tasks designed to separately probe generalization
(via arithmetic extrapolation) and memorization (via factual recall). We
observe a consistent trade-off: small models extrapolate to unseen arithmetic
cases but fail to memorize facts, while larger models memorize but fail to
extrapolate. An intermediate-capacity model exhibits a similar shift toward
memorization. When trained on both tasks jointly, no model (regardless of size)
succeeds at extrapolation. These findings suggest that pre-training may
intrinsically favor one learning mode over the other. By isolating these
dynamics in a controlled setting, our study offers insight into how model
capacity shapes learning behavior and offers broader implications for the
design and deployment of small language models.

</details>


### [207] [An Interpretable N-gram Perplexity Threat Model for Large Language Model Jailbreaks](https://arxiv.org/abs/2410.16222)
*Valentyn Boreiko,Alexander Panfilov,Vaclav Voracek,Matthias Hein,Jonas Geiping*

Main category: cs.LG

TL;DR: This study proposes a unified threat model for assessing jailbreaking attacks on safety-tuned LLMs using an N-gram language model, offering a standardized comparison method.


<details>
  <summary>Details</summary>
Motivation: The motivation is to establish a principled method to evaluate and compare jailbreaking attacks on LLMs, as existing methods vary in effectiveness, fluency, and computational demands.

Method: The authors developed a unified threat model using an N-gram language model trained on 1T tokens. This approach is nonparametric, LLM-agnostic, and interpretable. Existing attacks were adapted and benchmarked under this model for the first time.

Result: The study found that attack success rates are lower than previously reported, with discrete optimization-based attacks outperforming recent LLM-based ones. Effective attacks commonly exploit rare or infrequent bigrams.

Conclusion: The proposed threat model provides a standardized and interpretable framework to evaluate and compare LLM jailbreak attacks, aiding in understanding mechanisms behind effective attacks.

Abstract: A plethora of jailbreaking attacks have been proposed to obtain harmful
responses from safety-tuned LLMs. These methods largely succeed in coercing the
target output in their original settings, but their attacks vary substantially
in fluency and computational effort. In this work, we propose a unified threat
model for the principled comparison of these methods. Our threat model checks
if a given jailbreak is likely to occur in the distribution of text. For this,
we build an N-gram language model on 1T tokens, which, unlike model-based
perplexity, allows for an LLM-agnostic, nonparametric, and inherently
interpretable evaluation. We adapt popular attacks to this threat model, and,
for the first time, benchmark these attacks on equal footing with it. After an
extensive comparison, we find attack success rates against safety-tuned modern
models to be lower than previously presented and that attacks based on discrete
optimization significantly outperform recent LLM-based attacks. Being
inherently interpretable, our threat model allows for a comprehensive analysis
and comparison of jailbreak attacks. We find that effective attacks exploit and
abuse infrequent bigrams, either selecting the ones absent from real-world text
or rare ones, e.g., specific to Reddit or code datasets.

</details>


### [208] [Generalization Error Analysis for Attack-Free and Byzantine-Resilient Decentralized Learning with Data Heterogeneity](https://arxiv.org/abs/2506.09438)
*Haoxiang Ye,Tao Sun,Qing Ling*

Main category: cs.LG

TL;DR: This paper investigates the generalization errors in decentralized learning, emphasizing heterogeneous data and resilience to Byzantine attacks.


<details>
  <summary>Details</summary>
Motivation: To address the under-explored area of generalization errors in decentralized learning, which is critical for real-world applicability and scalability of trained models.

Method: The paper conducts fine-grained theoretical analysis on attack-free and Byzantine-resilient decentralized learning, focusing on heterogeneous data, while relaxing assumptions around stochastic gradients.

Result: Findings reveal the effects of data heterogeneity, model initialization, and stochastic noise on generalization errors, highlighting the dependence of errors from Byzantine attacks on data heterogeneity rather than sample size.

Conclusion: The study provides valuable insights into factors influencing generalization errors in decentralized learning, establishing the impact of malicious agents and validating findings with numerical experiments on both convex and non-convex tasks.

Abstract: Decentralized learning, which facilitates joint model training across
geographically scattered agents, has gained significant attention in the field
of signal and information processing in recent years. While the optimization
errors of decentralized learning algorithms have been extensively studied,
their generalization errors remain relatively under-explored. As the
generalization errors reflect the scalability of trained models on unseen data
and are crucial in determining the performance of trained models in real-world
applications, understanding the generalization errors of decentralized learning
is of paramount importance. In this paper, we present fine-grained
generalization error analysis for both attack-free and Byzantine-resilient
decentralized learning with heterogeneous data as well as under mild
assumptions, in contrast to prior studies that consider homogeneous data and/or
rely on a stringent bounded stochastic gradient assumption. Our results shed
light on the impact of data heterogeneity, model initialization and stochastic
gradient noise -- factors that have not been closely investigated before -- on
the generalization error of decentralized learning. We also reveal that
Byzantine attacks performed by malicious agents largely affect the
generalization error, and their negative impact is inherently linked to the
data heterogeneity while remaining independent on the sample size. Numerical
experiments on both convex and non-convex tasks are conducted to validate our
theoretical findings.

</details>


### [209] [CFMI: Flow Matching for Missing Data Imputation](https://arxiv.org/abs/2506.09258)
*Vaidotas Simkus,Michael U. Gutmann*

Main category: cs.LG

TL;DR: The paper introduces CFMI, a flow-based method for imputing missing data, outperforming traditional and modern methods across various metrics.


<details>
  <summary>Details</summary>
Motivation: Develop an efficient, scalable method for data imputation to overcome the limitations of traditional approaches.

Method: The CFMI combines continuous normalizing flows, flow-matching, and shared conditional modeling to address challenges in data imputation.

Result: CFMI outperforms nine other imputation methods across 24 datasets and achieves superior computational efficiency in time-series imputation.

Conclusion: CFMI is scalable, efficient, and performs well for various data types and dimensions, making it a versatile solution for imputation tasks.

Abstract: We introduce conditional flow matching for imputation (CFMI), a new
general-purpose method to impute missing data. The method combines continuous
normalising flows, flow-matching, and shared conditional modelling to deal with
intractabilities of traditional multiple imputation. Our comparison with nine
classical and state-of-the-art imputation methods on 24 small to
moderate-dimensional tabular data sets shows that CFMI matches or outperforms
both traditional and modern techniques across a wide range of metrics. Applying
the method to zero-shot imputation of time-series data, we find that it matches
the accuracy of a related diffusion-based method while outperforming it in
terms of computational efficiency. Overall, CFMI performs at least as well as
traditional methods on lower-dimensional data while remaining scalable to
high-dimensional settings, matching or exceeding the performance of other deep
learning-based approaches, making it a go-to imputation method for a wide range
of data types and dimensionalities.

</details>


### [210] [Unifying Block-wise PTQ and Distillation-based QAT for Progressive Quantization toward 2-bit Instruction-Tuned LLMs](https://arxiv.org/abs/2506.09104)
*Jung Hyun Lee,Seungjae Shin,Vinnam Kim,Jaeseong You,An Chen*

Main category: cs.LG

TL;DR: This paper introduces Unified Progressive Quantization (UPQ), a framework to achieve 2-bit quantization for instruction-tuned LLMs, resulting in efficient models with state-of-the-art performance on key benchmarks.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of LLMs presents deployment challenges on resource-constrained devices. Although 2-bit quantization has been shown to be efficient for pre-trained LLMs, its application to instruction-tuned models was unexplored.

Method: UPQ includes a two-step progressive quantization process: (1) FP16 models are converted to INT4 using block-wise post-training quantization to reduce error, and (2) Distill-QAT is applied to further quantize to INT2, aligning the output with the original model using generalized Jensen-Shannon divergence minimization.

Result: UPQ successfully quantizes instruction-tuned LLMs to INT2, achieving competitive performance without proprietary post-training data and showing state-of-the-art results on MMLU and IFEval benchmarks.

Conclusion: UPQ offers a novel and effective 2-bit quantization solution for instruction-tuned LLMs, addressing deployment limitations while retaining strong performance without requiring proprietary datasets.

Abstract: As the rapid scaling of large language models (LLMs) poses significant
challenges for deployment on resource-constrained devices, there is growing
interest in extremely low-bit quantization, such as 2-bit. Although prior works
have shown that 2-bit large models are pareto-optimal over their 4-bit smaller
counterparts in both accuracy and latency, these advancements have been limited
to pre-trained LLMs and have not yet been extended to instruction-tuned models.
To bridge this gap, we propose Unified Progressive Quantization (UPQ)$-$a novel
progressive quantization framework (FP16$\rightarrow$INT4$\rightarrow$INT2)
that unifies block-wise post-training quantization (PTQ) with
distillation-based quantization-aware training (Distill-QAT) for INT2
instruction-tuned LLM quantization. UPQ first quantizes FP16 instruction-tuned
models to INT4 using block-wise PTQ to significantly reduce the quantization
error introduced by subsequent INT2 quantization. Next, UPQ applies Distill-QAT
to enable INT2 instruction-tuned LLMs to generate responses consistent with
their original FP16 counterparts by minimizing the generalized Jensen-Shannon
divergence (JSD) between the two. To the best of our knowledge, we are the
first to demonstrate that UPQ can quantize open-source instruction-tuned LLMs
to INT2 without relying on proprietary post-training data, while achieving
state-of-the-art performances on MMLU and IFEval$-$two of the most
representative benchmarks for evaluating instruction-tuned LLMs.

</details>


### [211] [SyncFed: Time-Aware Federated Learning through Explicit Timestamping and Synchronization](https://arxiv.org/abs/2506.09660)
*Baran Can Gül,Stefanos Tziampazis,Nasser Jazdi,Michael Weyrich*

Main category: cs.LG

TL;DR: The paper introduces SyncFed, a Federated Learning framework that enhances model reliability by employing timestamp-based synchronization to effectively address staleness of client updates.


<details>
  <summary>Details</summary>
Motivation: To address challenges in Federated Learning caused by network delays, clock unsynchronicity, and client update variability, which undermine consistency and convergence.

Method: The proposed SyncFed framework uses timestamps and Network Time Protocol (NTP) to synchronize temporal references across clients, enabling numerical quantification of staleness and temporally informed update aggregation.

Result: SyncFed demonstrated improved accuracy and information freshness in its global model during tests on a geographically distributed setup, compared to round-based methods.

Conclusion: Explicit synchronization and timestamping significantly enhance model consistency and accuracy in Federated Learning, particularly in challenging distributed environments.

Abstract: As Federated Learning (FL) expands to larger and more distributed
environments, consistency in training is challenged by network-induced delays,
clock unsynchronicity, and variability in client updates. This combination of
factors may contribute to misaligned contributions that undermine model
reliability and convergence. Existing methods like staleness-aware aggregation
and model versioning address lagging updates heuristically, yet lack mechanisms
to quantify staleness, especially in latency-sensitive and cross-regional
deployments. In light of these considerations, we introduce \emph{SyncFed}, a
time-aware FL framework that employs explicit synchronization and timestamping
to establish a common temporal reference across the system. Staleness is
quantified numerically based on exchanged timestamps under the Network Time
Protocol (NTP), enabling the server to reason about the relative freshness of
client updates and apply temporally informed weighting during aggregation. Our
empirical evaluation on a geographically distributed testbed shows that, under
\emph{SyncFed}, the global model evolves within a stable temporal context,
resulting in improved accuracy and information freshness compared to
round-based baselines devoid of temporal semantics.

</details>


### [212] [G-Sim: Generative Simulations with Large Language Models and Gradient-Free Calibration](https://arxiv.org/abs/2506.09272)
*Samuel Holt,Max Ruiz Luyten,Antonin Berthon,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: G-Sim is a hybrid framework that combines Large Language Model (LLM)-driven structural design with empirical calibration techniques to create robust simulators for complex domains.


<details>
  <summary>Details</summary>
Motivation: Current simulation methods struggle with generalization or accuracy, particularly in critical fields like healthcare and logistics. G-Sim is introduced to overcome these issues by integrating structural design and empirical evidence.

Method: G-Sim uses an iterative loop where LLM proposes and refines simulator components and causal relationships, complemented by flexible calibration techniques that estimate parameters using likelihood-free and gradient-free methods.

Result: The framework creates causally-informed, empirically grounded simulators capable of handling non-differentiable and stochastic systems, improving reliability and data efficiency.

Conclusion: G-Sim enables robust decision-making and system-level interventions using reliable simulators that blend domain-specific knowledge with empirical accuracy.

Abstract: Constructing robust simulators is essential for asking "what if?" questions
and guiding policy in critical domains like healthcare and logistics. However,
existing methods often struggle, either failing to generalize beyond historical
data or, when using Large Language Models (LLMs), suffering from inaccuracies
and poor empirical alignment. We introduce G-Sim, a hybrid framework that
automates simulator construction by synergizing LLM-driven structural design
with rigorous empirical calibration. G-Sim employs an LLM in an iterative loop
to propose and refine a simulator's core components and causal relationships,
guided by domain knowledge. This structure is then grounded in reality by
estimating its parameters using flexible calibration techniques. Specifically,
G-Sim can leverage methods that are both likelihood-free and gradient-free with
respect to the simulator, such as gradient-free optimization for direct
parameter estimation or simulation-based inference for obtaining a posterior
distribution over parameters. This allows it to handle non-differentiable and
stochastic simulators. By integrating domain priors with empirical evidence,
G-Sim produces reliable, causally-informed simulators, mitigating
data-inefficiency and enabling robust system-level interventions for complex
decision-making.

</details>


### [213] [MetaTT: A Global Tensor-Train Adapter for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2506.09105)
*Javier Lopez-Piqueres,Pranav Deshpande,Archan Ray,Mattia J. Villani,Marco Pistoia,Niraj Kumar*

Main category: cs.LG

TL;DR: MetaTT is a Tensor Train (TT) adapter framework for fine-tuning transformers with reduced parameters by using a shared TT structure.


<details>
  <summary>Details</summary>
Motivation: To create a more parameter-efficient fine-tuning method for transformers by leveraging global low-rank approximations using Tensor Train (TT) decompositions.

Method: The method introduces MetaTT, a framework that uses a shared Tensor Train (TT) structure to factorize all sub-modules of a transformer model, indexed by structural axes like layer and matrix type. It reduces parameter addition compared to previous methods like LoRA.

Result: MetaTT achieves significant parameter reduction compared to LoRA and other tensor-based methods, while maintaining or improving accuracy on standard language modeling benchmarks.

Conclusion: MetaTT provides an efficient and accurate fine-tuning framework for transformers, offering parameter compression and scalability across multiple tasks without redesigning its core structure.

Abstract: We present MetaTT, a unified Tensor Train (TT) adapter framework for global
low-rank fine-tuning of pre-trained transformers. Unlike LoRA, which fine-tunes
each weight matrix independently, MetaTT uses a single shared TT to factorize
all transformer sub-modules -- query, key, value, projection, and feed-forward
layers -- by indexing the structural axes like layer and matrix type, and
optionally heads and tasks. For a given rank, while LoRA adds parameters
proportional to the product across modes, MetaTT only adds parameters
proportional to the sum across modes leading to a significantly compressed
final adapter. Our benchmarks compare MetaTT with LoRA along with recent
state-of-the-art matrix and tensor decomposition based fine-tuning schemes. We
observe that when tested on standard language modeling benchmarks, MetaTT leads
to the most reduction in the parameters while maintaining similar accuracy to
LoRA and even outperforming other tensor-based methods. Unlike CP or other
rank-factorizations, the TT ansatz benefits from mature optimization routines
-- e.g., DMRG-style rank adaptive minimization in addition to Adam, which we
find simplifies training. Because new modes can be appended cheaply, MetaTT
naturally extends to shared adapters across many tasks without redesigning the
core tensor.

</details>


### [214] [Private Aggregation for Byzantine-Resilient Heterogeneous Federated Learning](https://arxiv.org/abs/2506.09870)
*Maximilian Egger,Rawad Bitar*

Main category: cs.LG

TL;DR: The paper addresses federated learning challenges with Byzantine clients and data privacy, proposing a multi-stage method combining secret sharing, secure aggregation, and private information retrieval.


<details>
  <summary>Details</summary>
Motivation: To ensure Byzantine resilience and data privacy in federated learning, especially in heterogeneous data settings where prior solutions fail.

Method: A co-design of verifiable secret sharing, secure aggregation, and symmetric private information retrieval to maintain privacy and Byzantine resilience, paired with zero-order estimation for scalability.

Result: The proposed method outperforms previous techniques across various attacks and reduces communication costs in federated learning tasks.

Conclusion: The multi-stage method successfully combines privacy preservation, Byzantine resilience, and scalability for federated learning under data heterogeneity.

Abstract: Ensuring resilience to Byzantine clients while maintaining the privacy of the
clients' data is a fundamental challenge in federated learning (FL). When the
clients' data is homogeneous, suitable countermeasures were studied from an
information-theoretic perspective utilizing secure aggregation techniques while
ensuring robust aggregation of the clients' gradients. However, the
countermeasures used fail when the clients' data is heterogeneous. Suitable
pre-processing techniques, such as nearest neighbor mixing, were recently shown
to enhance the performance of those countermeasures in the heterogeneous
setting. Nevertheless, those pre-processing techniques cannot be applied with
the introduced privacy-preserving mechanisms.
  We propose a multi-stage method encompassing a careful co-design of
verifiable secret sharing, secure aggregation, and a tailored symmetric private
information retrieval scheme to achieve information-theoretic privacy
guarantees and Byzantine resilience under data heterogeneity. We evaluate the
effectiveness of our scheme on a variety of attacks and show how it outperforms
the previously known techniques. Since the communication overhead of secure
aggregation is non-negligible, we investigate the interplay with zero-order
estimation methods that reduce the communication cost in state-of-the-art FL
tasks and thereby make private aggregation scalable.

</details>


### [215] [Adversarial Surrogate Risk Bounds for Binary Classification](https://arxiv.org/abs/2506.09348)
*Natalie S. Frank*

Main category: cs.LG

TL;DR: The paper discusses robust classification in the presence of adversarial attacks, focusing on convergence rates of adversarial surrogate risk minimization and providing distribution-dependent bounds.


<details>
  <summary>Details</summary>
Motivation: To address the lack of understanding about the convergence rate of adversarial classification risk to its optimal value when using adversarial training methods.

Method: The authors derive surrogate risk bounds to quantify the convergence rate, along with distribution-dependent bounds for standard learning settings.

Result: The paper offers theoretical bounds on adversarial and standard surrogate risks and establishes distribution-dependent convergence rates.

Conclusion: These findings contribute to improved understanding and robustness in machine learning against adversarial attacks, and provide insights potentially useful outside adversarial settings.

Abstract: A central concern in classification is the vulnerability of machine learning
models to adversarial attacks. Adversarial training is one of the most popular
techniques for training robust classifiers, which involves minimizing an
adversarial surrogate risk. Recent work characterized when a minimizing
sequence of an adversarial surrogate risk is also a minimizing sequence of the
adversarial classification risk for binary classification -- a property known
as adversarial consistency. However, these results do not address the rate at
which the adversarial classification risk converges to its optimal value for
such a sequence of functions that minimize the adversarial surrogate. This
paper provides surrogate risk bounds that quantify that convergence rate.
Additionally, we derive distribution-dependent surrogate risk bounds in the
standard (non-adversarial) learning setting, that may be of independent
interest.

</details>


### [216] [SensorLM: Learning the Language of Wearable Sensors](https://arxiv.org/abs/2506.09108)
*Yuwei Zhang,Kumar Ayush,Siyuan Qiao,A. Ali Heydari,Girish Narayanswamy,Maxwell A. Xu,Ahmed A. Metwally,Shawn Xu,Jake Garrison,Xuhai Xu,Tim Althoff,Yun Liu,Pushmeet Kohli,Jiening Zhan,Mark Malhotra,Shwetak Patel,Cecilia Mascolo,Xin Liu,Daniel McDuff,Yuzhe Yang*

Main category: cs.LG

TL;DR: SensorLM is a sensor-language foundational model aimed at understanding wearable sensor data through natural language. It leverages a hierarchical caption generation method to curate the largest sensor-language dataset and supports multimodal architectures for various tasks.


<details>
  <summary>Details</summary>
Motivation: The authors sought to bridge the gap between wearable sensor data interpretation and natural language due to the lack of annotated sensor-text data in real-world contexts.

Method: The paper introduces a hierarchical caption generation pipeline and extends multimodal pretraining architectures into a generic architecture. This enables curation of a large sensor-language dataset and effective model training.

Result: SensorLM outperforms state-of-the-art methods in zero-shot recognition, few-shot learning, cross-modal retrieval, and demonstrates capabilities like scaling behaviors and zero-shot generalization.

Conclusion: SensorLM represents a significant advancement in merging sensor data and language-based understanding, providing new benchmarks for human activity analysis and healthcare applications.

Abstract: We present SensorLM, a family of sensor-language foundation models that
enable wearable sensor data understanding with natural language. Despite its
pervasive nature, aligning and interpreting sensor data with language remains
challenging due to the lack of paired, richly annotated sensor-text
descriptions in uncurated, real-world wearable data. We introduce a
hierarchical caption generation pipeline designed to capture statistical,
structural, and semantic information from sensor data. This approach enabled
the curation of the largest sensor-language dataset to date, comprising over
59.7 million hours of data from more than 103,000 people. Furthermore, SensorLM
extends prominent multimodal pretraining architectures (e.g., CLIP, CoCa) and
recovers them as specific variants within a generic architecture. Extensive
experiments on real-world tasks in human activity analysis and healthcare
verify the superior performance of SensorLM over state-of-the-art in zero-shot
recognition, few-shot learning, and cross-modal retrieval. SensorLM also
demonstrates intriguing capabilities including scaling behaviors, label
efficiency, sensor captioning, and zero-shot generalization to unseen tasks.

</details>


### [217] [CodeBrain: Bridging Decoupled Tokenizer and Multi-Scale Architecture for EEG Foundation Model](https://arxiv.org/abs/2506.09110)
*Jingying Ma,Feng Wu,Qika Lin,Yucheng Xing,Chenyu Liu,Ziyu Jia,Mengling Feng*

Main category: cs.LG

TL;DR: The paper introduces CodeBrain, an Electroencephalography (EEG) foundation model that effectively models brain organization and improves generalizability across tasks.


<details>
  <summary>Details</summary>
Motivation: Existing EEG models face challenges such as limited heterogeneous representation capacity and inefficiency in capturing multi-scale brain dependencies. This motivated the design of a more generalizable and biologically informed model.

Method: The CodeBrain framework includes a TFDual-Tokenizer to tokenize temporal and frequency components for heterogeneous representation. Additionally, it utilizes EEGSSM, a structured global convolution and sliding window attention mechanism to model sparse and local dependencies.

Result: Experimental validation on 10 EEG datasets demonstrated the superior generalizability of CodeBrain through linear probing techniques.

Conclusion: CodeBrain improves EEG modeling by incorporating brain organization-inspired structures and interpretable features, establishing a foundation for neuroscience applications.

Abstract: Electroencephalography (EEG) provides real-time insights into brain activity
and is widely used in neuroscience. However, variations in channel
configurations, sequence lengths, and task objectives limit the transferability
of traditional task-specific models. Although recent EEG foundation models
(EFMs) aim to learn generalizable representations, they struggle with limited
heterogeneous representation capacity and inefficiency in capturing multi-scale
brain dependencies. To address these challenges, we propose CodeBrain, an
efficient EFM structurally aligned with brain organization, trained in two
stages. (1) We introduce a TFDual-Tokenizer that independently tokenizes
heterogeneous temporal and frequency components, enabling a quadratic expansion
of the discrete representation space. This also offers a degree of
interpretability through cross-domain token analysis. (2) We propose the
EEGSSM, which combines a structured global convolution architecture and a
sliding window attention mechanism to jointly model sparse long-range and local
dependencies. Unlike fully connected Transformer models, EEGSSM better reflects
the brain's small-world topology and efficiently captures EEG's inherent
multi-scale structure. EEGSSM is trained with a masked self-supervised learning
objective to predict token indices obtained in TFDual-Tokenizer. Comprehensive
experiments on 10 public EEG datasets demonstrate the generalizability of
CodeBrain with linear probing. By offering biologically informed and
interpretable EEG modeling, CodeBrain lays the foundation for future
neuroscience research. Both code and pretraining weights will be released in
the future version.

</details>


### [218] [Safe Screening Rules for Group SLOPE](https://arxiv.org/abs/2506.09451)
*Runxue Bao,Quanchao Lu,Yanfu Zhang*

Main category: cs.LG

TL;DR: The paper introduces a safe screening rule for Group SLOPE to identify inactive groups, reducing computational costs in sparse high-dimensional learning.


<details>
  <summary>Details</summary>
Motivation: Variable selection in high-dimensional sparse learning is challenging, especially with group structures. Existing methods for Group SLOPE are inefficient due to block non-separable group effects.

Method: A safe screening rule is proposed to detect inactive groups with zero coefficients in the Group SLOPE model, integrated with current solvers for batch and stochastic algorithms.

Result: The screening rule effectively identifies inactive feature groups, reducing computational costs and memory usage while maintaining accuracy.

Conclusion: The screening rule enhances the efficiency and practicality of Group SLOPE, improving high-dimensional sparse learning applications.

Abstract: Variable selection is a challenging problem in high-dimensional sparse
learning, especially when group structures exist. Group SLOPE performs well for
the adaptive selection of groups of predictors. However, the block
non-separable group effects in Group SLOPE make existing methods either invalid
or inefficient. Consequently, Group SLOPE tends to incur significant
computational costs and memory usage in practical high-dimensional scenarios.
To overcome this issue, we introduce a safe screening rule tailored for the
Group SLOPE model, which efficiently identifies inactive groups with zero
coefficients by addressing the block non-separable group effects. By excluding
these inactive groups during training, we achieve considerable gains in
computational efficiency and memory usage. Importantly, the proposed screening
rule can be seamlessly integrated into existing solvers for both batch and
stochastic algorithms. Theoretically, we establish that our screening rule can
be safely employed with existing optimization algorithms, ensuring the same
results as the original approaches. Experimental results confirm that our
method effectively detects inactive feature groups and significantly boosts
computational efficiency without compromising accuracy.

</details>


### [219] [TRACE: Grounding Time Series in Context for Multimodal Embedding and Retrieval](https://arxiv.org/abs/2506.09114)
*Jialin Chen,Ziyu Zhao,Gaukhar Nurbek,Aosong Feng,Ali Maatouk,Leandros Tassiulas,Yifeng Gao,Rex Ying*

Main category: cs.LG

TL;DR: TRACE is a generic multimodal retriever designed to enhance the interpretation and retrieval of time-series data by grounding embeddings in aligned textual contexts across multiple domains.


<details>
  <summary>Details</summary>
Motivation: The growing ubiquity of dynamic time-series data tied with domain-specific contexts demands effective methods for interpretation and flexible cross-modal retrieval to improve predictive modeling and task performance.

Method: TRACE employs fine-grained channel-level alignment, hard negative mining, and multimodal context grounding to enable cross-modal semantic retrieval across text and time-series data. It works flexibly as a standalone encoder and retriever.

Result: TRACE achieved state-of-the-art performance on downstream forecasting and classification tasks by providing semantically enriched and aligned representations.

Conclusion: TRACE is a robust encoder and retriever enhancing cross-modal alignment and context-aware downstream model performance for various time-series applications.

Abstract: The ubiquity of dynamic data in domains such as weather, healthcare, and
energy underscores a growing need for effective interpretation and retrieval of
time-series data. These data are inherently tied to domain-specific contexts,
such as clinical notes or weather narratives, making cross-modal retrieval
essential not only for downstream tasks but also for developing robust
time-series foundation models by retrieval-augmented generation (RAG). Despite
the increasing demand, time-series retrieval remains largely underexplored.
Existing methods often lack semantic grounding, struggle to align heterogeneous
modalities, and have limited capacity for handling multi-channel signals. To
address this gap, we propose TRACE, a generic multimodal retriever that grounds
time-series embeddings in aligned textual context. TRACE enables fine-grained
channel-level alignment and employs hard negative mining to facilitate
semantically meaningful retrieval. It supports flexible cross-modal retrieval
modes, including Text-to-Timeseries and Timeseries-to-Text, effectively linking
linguistic descriptions with complex temporal patterns. By retrieving
semantically relevant pairs, TRACE enriches downstream models with informative
context, leading to improved predictive accuracy and interpretability. Beyond a
static retrieval engine, TRACE also serves as a powerful standalone encoder,
with lightweight task-specific tuning that refines context-aware
representations while maintaining strong cross-modal alignment. These
representations achieve state-of-the-art performance on downstream forecasting
and classification tasks. Extensive experiments across multiple domains
highlight its dual utility, as both an effective encoder for downstream
applications and a general-purpose retriever to enhance time-series models.

</details>


### [220] [Efficient Preference-Based Reinforcement Learning: Randomized Exploration Meets Experimental Design](https://arxiv.org/abs/2506.09508)
*Andreas Schlaginhaufen,Reda Ouhamma,Maryam Kamgarpour*

Main category: cs.LG

TL;DR: This paper introduces a method to enhance reinforcement learning from human preferences using a randomized exploration-based meta-algorithm and optimal experimental design.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of designing efficient algorithms for reinforcement learning from human feedback, ensuring theoretical guarantees while minimizing computational complexity.

Method: The paper proposes a randomized exploration meta-algorithm to avoid complex computations and incorporates batch-based preference queries for improved query efficiency through optimal experimental design.

Result: The proposed method achieves theoretical guarantees, allows parallelized preference queries, and performs competitively with reward-based reinforcement learning while requiring fewer preference inputs.

Conclusion: Randomized exploration and optimal experimental design present practical and effective solutions to reinforcement learning from human preferences, balancing theoretical rigor and empirical performance.

Abstract: We study reinforcement learning from human feedback in general Markov
decision processes, where agents learn from trajectory-level preference
comparisons. A central challenge in this setting is to design algorithms that
select informative preference queries to identify the underlying reward while
ensuring theoretical guarantees. We propose a meta-algorithm based on
randomized exploration, which avoids the computational challenges associated
with optimistic approaches and remains tractable. We establish both regret and
last-iterate guarantees under mild reinforcement learning oracle assumptions.
To improve query complexity, we introduce and analyze an improved algorithm
that collects batches of trajectory pairs and applies optimal experimental
design to select informative comparison queries. The batch structure also
enables parallelization of preference queries, which is relevant in practical
deployment as feedback can be gathered concurrently. Empirical evaluation
confirms that the proposed method is competitive with reward-based
reinforcement learning while requiring a small number of preference queries.

</details>


### [221] [Improving LLM Agent Planning with In-Context Learning via Atomic Fact Augmentation and Lookahead Search](https://arxiv.org/abs/2506.09171)
*Samuel Holt,Max Ruiz Luyten,Thomas Pouplin,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: This paper proposes a framework for Large Language Model (LLM) agents that improves their planning and decision-making using atomic fact augmentation and recursive lookahead search, without requiring fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Current LLMs often struggle in interactive, multi-step reasoning tasks as they require significant guidance or fine-tuning to adapt to new information and utilize past experiences effectively.

Method: The framework uses in-context learning, atomic fact extraction, and a depth-limited, recursive lookahead search. It dynamically augments prompts for various LLM components and simulates potential trajectories for better planning and decision-making.

Result: The proposed framework improves LLM performance and adaptability in tasks like TextFrozenLake and ALFWorld, showcasing optimized behavior as the agent accumulates experience.

Conclusion: This method demonstrates how in-context learning and fact augmentation can enhance LLM's online decision-making capabilities, reducing the need for fine-tuning while enabling adaptability and improved task performance.

Abstract: Large Language Models (LLMs) are increasingly capable but often require
significant guidance or extensive interaction history to perform effectively in
complex, interactive environments. Existing methods may struggle with adapting
to new information or efficiently utilizing past experiences for multi-step
reasoning without fine-tuning. We introduce a novel LLM agent framework that
enhances planning capabilities through in-context learning, facilitated by
atomic fact augmentation and a recursive lookahead search. Our agent learns to
extract task-critical ``atomic facts'' from its interaction trajectories. These
facts dynamically augment the prompts provided to LLM-based components
responsible for action proposal, latent world model simulation, and state-value
estimation. Planning is performed via a depth-limited lookahead search, where
the LLM simulates potential trajectories and evaluates their outcomes, guided
by the accumulated facts and interaction history. This approach allows the
agent to improve its understanding and decision-making online, leveraging its
experience to refine its behavior without weight updates. We provide a
theoretical motivation linking performance to the quality of fact-based
abstraction and LLM simulation accuracy. Empirically, our agent demonstrates
improved performance and adaptability on challenging interactive tasks,
achieving more optimal behavior as it accumulates experience, showcased in
tasks such as TextFrozenLake and ALFWorld.

</details>


### [222] [On the Similarities of Embeddings in Contrastive Learning](https://arxiv.org/abs/2506.09781)
*Chungpa Lee,Sehee Lim,Kibok Lee,Jy-yong Sohn*

Main category: cs.LG

TL;DR: The paper unifies and enhances understanding of contrastive learning (CL) by analyzing cosine similarity and introduces an auxiliary loss to improve efficiency in small-batch training.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive framework for understanding and addressing limitations of existing contrastive learning objectives, particularly in mini-batch training.

Method: Unified the analysis of CL based on cosine similarity and introduced an auxiliary loss term to minimize variance in negative pair similarities in mini-batches.

Result: The proposed auxiliary loss improves contrastive learning methods' performance, especially for small-batch training.

Conclusion: This work presents a systematic framework for CL and suggests actionable solutions to optimize learning in variable batch settings.

Abstract: Contrastive learning (CL) operates on a simple yet effective principle:
embeddings of positive pairs are pulled together, while those of negative pairs
are pushed apart. Although various forms of contrastive loss have been proposed
and analyzed from different perspectives, prior works lack a comprehensive
framework that systematically explains a broad class of these objectives. In
this paper, we present a unified framework for understanding CL, which is based
on analyzing the cosine similarity between embeddings of positive and negative
pairs. In full-batch settings, we show that perfect alignment of positive pairs
is unattainable when similarities of negative pairs fall below a certain
threshold, and that this misalignment can be alleviated by incorporating
within-view negative pairs. In mini-batch settings, we demonstrate that smaller
batch sizes incur stronger separation among negative pairs within batches,
which leads to higher variance in similarities of negative pairs. To address
this limitation of mini-batch CL, we introduce an auxiliary loss term that
reduces the variance of similarities of negative pairs in CL. Empirical results
demonstrate that incorporating the proposed loss consistently improves the
performance of CL methods in small-batch training.

</details>


### [223] [MultiNet: An Open-Source Software Toolkit \& Benchmark Suite for the Evaluation and Adaptation of Multimodal Action Models](https://arxiv.org/abs/2506.09172)
*Pranav Guruprasad,Yangyue Wang,Harshvardhan Sikka*

Main category: cs.LG

TL;DR: MultiNet introduces a comprehensive open-source benchmark and software ecosystem to evaluate and adapt vision, language, and action models across various domains.


<details>
  <summary>Details</summary>
Motivation: To create standardized evaluation protocols and tools for assessing and improving multimodal systems that integrate vision, language, and action capabilities.

Method: Development of the MultiNet benchmark, open-source software for data/models/evaluation, and a composite dataset covering diverse AI tasks.

Result: MultiNet has supported downstream research on VLA model generalization limitations and provides extensive resources for the AI research community.

Conclusion: MultiNet aids in advancing multimodal AI research by offering tools to evaluate and understand the capabilities and limitations of VLMs and VLAs.

Abstract: Recent innovations in multimodal action models represent a promising
direction for developing general-purpose agentic systems, combining visual
understanding, language comprehension, and action generation. We introduce
MultiNet - a novel, fully open-source benchmark and surrounding software
ecosystem designed to rigorously evaluate and adapt models across vision,
language, and action domains. We establish standardized evaluation protocols
for assessing vision-language models (VLMs) and vision-language-action models
(VLAs), and provide open source software to download relevant data, models, and
evaluations. Additionally, we provide a composite dataset with over 1.3
trillion tokens of image captioning, visual question answering, commonsense
reasoning, robotic control, digital game-play, simulated
locomotion/manipulation, and many more tasks. The MultiNet benchmark,
framework, toolkit, and evaluation harness have been used in downstream
research on the limitations of VLA generalization.

</details>


### [224] [The Curious Language Model: Strategic Test-Time Information Acquisition](https://arxiv.org/abs/2506.09173)
*Michael Cooper,Rohan Wadhawan,John Michael Giorgi,Chenhao Tan,Davis Liang*

Main category: cs.LG

TL;DR: The paper proposes CuriosiTree, a heuristic-based strategy for cost-effective information acquisition using large language models.


<details>
  <summary>Details</summary>
Motivation: Decision-makers often lack sufficient information and need to acquire it cost-effectively for accurate decisions.

Method: CuriosiTree employs a greedy tree search to select actions based on anticipated information gain versus associated costs in a zero-shot setting.

Result: CuriosiTree outperforms baseline strategies in a clinical diagnosis simulation by enabling accurate, cost-effective information acquisition.

Conclusion: The method provides an effective approach for integrating diverse information sources in decision-making scenarios.

Abstract: Decision-makers often possess insufficient information to render a confident
decision. In these cases, the decision-maker can often undertake actions to
acquire the necessary information about the problem at hand, e.g., by
consulting knowledgeable authorities or by conducting experiments. Importantly,
different levers of information acquisition come with different costs, posing
the challenge of selecting the actions that are both informative and
cost-effective. In this work, we propose CuriosiTree, a heuristic-based,
test-time policy for zero-shot information acquisition in large language models
(LLMs). CuriosiTree employs a greedy tree search to estimate the expected
information gain of each action and strategically chooses actions based on a
balance of anticipated information gain and associated cost. Empirical
validation in a clinical diagnosis simulation shows that CuriosiTree enables
cost-effective integration of heterogenous sources of information, and
outperforms baseline action selection strategies in selecting action sequences
that enable accurate diagnosis.

</details>


### [225] [Learning single-index models via harmonic decomposition](https://arxiv.org/abs/2506.09887)
*Nirmit Joshi,Hugo Koubbi,Theodor Misiakiewicz,Nathan Srebro*

Main category: cs.LG

TL;DR: The authors explore learning single-index models by introducing a new approach based on spherical harmonics, addressing symmetric input distributions and providing novel insights for estimators.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of Hermite polynomials in analyzing single-index models, the authors aim to leverage the framework of spherical harmonics to capture intrinsic rotational symmetries in these models.

Method: The study utilizes spherical harmonics for understanding single-index models, introduces tensor unfolding and online SGD-based estimators for optimal sample efficiency or runtime, and provides theoretical insights under various input distributions.

Result: The analysis demonstrates that spherical harmonics provide a better framework than Hermite polynomials, introduces effective estimators, and recovers existing results under Gaussian inputs while revealing new phenomena.

Conclusion: Spherical harmonics enhance the understanding of single-index models under spherical symmetry, and while optimal estimators may not achieve both minimal sample complexity and runtime, the study advances theoretical and practical insights.

Abstract: We study the problem of learning single-index models, where the label $y \in
\mathbb{R}$ depends on the input $\boldsymbol{x} \in \mathbb{R}^d$ only through
an unknown one-dimensional projection $\langle
\boldsymbol{w}_*,\boldsymbol{x}\rangle$. Prior work has shown that under
Gaussian inputs, the statistical and computational complexity of recovering
$\boldsymbol{w}_*$ is governed by the Hermite expansion of the link function.
In this paper, we propose a new perspective: we argue that "spherical
harmonics" -- rather than "Hermite polynomials" -- provide the natural basis
for this problem, as they capture its intrinsic "rotational symmetry". Building
on this insight, we characterize the complexity of learning single-index models
under arbitrary spherically symmetric input distributions. We introduce two
families of estimators -- based on tensor unfolding and online SGD -- that
respectively achieve either optimal sample complexity or optimal runtime, and
argue that estimators achieving both may not exist in general. When specialized
to Gaussian inputs, our theory not only recovers and clarifies existing results
but also reveals new phenomena that had previously been overlooked.

</details>


### [226] [Multivariate Long-term Time Series Forecasting with Fourier Neural Filter](https://arxiv.org/abs/2506.09174)
*Chenheng Xu,Dan Wu,Yixin Zhu,Ying Nian Wu*

Main category: cs.LG

TL;DR: The paper tackles challenges in multivariate long-term time series forecasting by introducing FNF and DBD, new neural architecture components specific to time series modeling.


<details>
  <summary>Details</summary>
Motivation: Current methods for time series forecasting often use domain-agnostic backbones repurposed from NLP or computer vision, which struggle with periodic properties and spatio-temporal dependencies of time series.

Method: The proposed backbone (FNF) integrates local and global frequency-domain processing while the architecture (DBD) optimizes gradient flow and capacity via information bottleneck theory.

Result: Evaluation on 11 public datasets across diverse domains showed state-of-the-art performance without auxiliary techniques, highlighting the effectiveness of the approach.

Conclusion: Properly designed architectures like FNF and DBD eliminate the need for auxiliary methods and significantly improve time series modeling for both scientific and industrial applications.

Abstract: Multivariate long-term time series forecasting has been suffering from the
challenge of capturing both temporal dependencies within variables and spatial
correlations across variables simultaneously. Current approaches predominantly
repurpose backbones from natural language processing or computer vision (e.g.,
Transformers), which fail to adequately address the unique properties of time
series (e.g., periodicity). The research community lacks a dedicated backbone
with temporal-specific inductive biases, instead relying on domain-agnostic
backbones supplemented with auxiliary techniques (e.g., signal decomposition).
We introduce FNF as the backbone and DBD as the architecture to provide
excellent learning capabilities and optimal learning pathways for
spatio-temporal modeling, respectively. Our theoretical analysis proves that
FNF unifies local time-domain and global frequency-domain information
processing within a single backbone that extends naturally to spatial modeling,
while information bottleneck theory demonstrates that DBD provides superior
gradient flow and representation capacity compared to existing unified or
sequential architectures. Our empirical evaluation across 11 public benchmark
datasets spanning five domains (energy, meteorology, transportation,
environment, and nature) confirms state-of-the-art performance with consistent
hyperparameter settings. Notably, our approach achieves these results without
any auxiliary techniques, suggesting that properly designed neural
architectures can capture the inherent properties of time series, potentially
transforming time series modeling in scientific and industrial applications.

</details>


### [227] [Bayesian Probabilistic Matrix Factorization](https://arxiv.org/abs/2506.09928)
*Ruixuan Xu,Xiangxiang Weng*

Main category: cs.LG

TL;DR: The study compares two Bayesian methods, MCMC and VI, for Probabilistic Matrix Factorization, finding VI faster but MCMC more accurate.


<details>
  <summary>Details</summary>
Motivation: Traditional matrix factorization doesn't account for uncertainties, necessitating probabilistic approaches like PMF.

Method: The researchers applied MCMC and VI to approximate the posterior distributions for PMF and tested their efficiency on the MovieLens dataset.

Result: Experiments showed VI achieves faster convergence, while MCMC produces more accurate posterior distributions.

Conclusion: Both methods have trade-offs, with VI suited for speed and MCMC better for precision in posterior estimation.

Abstract: Matrix factorization is a widely used technique in recommendation systems.
Probabilistic Matrix Factorization (PMF) [1] extends traditional matrix
factorization by incorporating probability distributions over latent factors,
allowing for uncertainty quantification. However, computing the posterior
distribution is intractable due to the high-dimensional integral. To address
this, we employ two Bayesian inference methods: Markov Chain Monte Carlo (MCMC)
[2] and Variational Inference (VI) [3] to approximate the posterior. We
evaluate their performance on MovieLens dataset and compare their convergence
speed, predictive accuracy, and computational efficiency. Experimental results
demonstrate that VI offers faster convergence, while MCMC provides more
accurate posterior estimates.

</details>


### [228] [Multi-Task Reward Learning from Human Ratings](https://arxiv.org/abs/2506.09183)
*Mingkang Wu,Devin White,Evelyn Rose,Vernon Lawhern,Nicholas R Waytowich,Yongcan Cao*

Main category: cs.LG

TL;DR: This paper introduces a novel reinforcement learning approach that integrates multiple tasks, using human ratings to more accurately replicate human decision-making.


<details>
  <summary>Details</summary>
Motivation: The existing RLHF methods oversimplify the human reasoning process and fail to capture its complexity, thereby limiting their ability to align model behavior effectively with user goals.

Method: The authors propose a framework that jointly considers both classification and regression tasks, incorporating learnable weights to balance their contributions and adaptively model human decision-making.

Result: Experiments using synthetic human ratings demonstrate that the proposed method consistently outperforms existing RL methods, achieving superior alignment with human-like decision strategies.

Conclusion: The approach provides a more nuanced way to align RL models with human goals, underscoring its potential for improving success over traditional RL systems and current RLHF methods.

Abstract: Reinforcement learning from human feeback (RLHF) has become a key factor in
aligning model behavior with users' goals. However, while humans integrate
multiple strategies when making decisions, current RLHF approaches often
simplify this process by modeling human reasoning through isolated tasks such
as classification or regression. In this paper, we propose a novel
reinforcement learning (RL) method that mimics human decision-making by jointly
considering multiple tasks. Specifically, we leverage human ratings in
reward-free environments to infer a reward function, introducing learnable
weights that balance the contributions of both classification and regression
models. This design captures the inherent uncertainty in human decision-making
and allows the model to adaptively emphasize different strategies. We conduct
several experiments using synthetic human ratings to validate the effectiveness
of the proposed approach. Results show that our method consistently outperforms
existing rating-based RL methods, and in some cases, even surpasses traditional
RL approaches.

</details>


### [229] [The Sample Complexity of Online Strategic Decision Making with Information Asymmetry and Knowledge Transportability](https://arxiv.org/abs/2506.09940)
*Jiachen Hu,Rui Ai,Han Zhong,Xiaoyu Chen,Liwei Wang,Zhaoran Wang,Zhuoran Yang*

Main category: cs.LG

TL;DR: The paper introduces a sample-efficient algorithm for reinforcement learning in multi-agent systems under information asymmetry and knowledge transfer constraints.


<details>
  <summary>Details</summary>
Motivation: To address complications from confounding variables and knowledge transfer issues in multi-agent systems where agents operate with private information.

Method: Developed a reinforcement learning approach using a sample-efficient algorithm that considers non-i.i.d. actions to learn system dynamics under information asymmetry.

Result: The proposed algorithm achieves an ε-optimal policy with a sample complexity of O(1/ε^2).

Conclusion: The study demonstrates the feasibility of learning efficient policies in environments with both information asymmetry and knowledge transfer demands.

Abstract: Information asymmetry is a pervasive feature of multi-agent systems,
especially evident in economics and social sciences. In these settings, agents
tailor their actions based on private information to maximize their rewards.
These strategic behaviors often introduce complexities due to confounding
variables. Simultaneously, knowledge transportability poses another significant
challenge, arising from the difficulties of conducting experiments in target
environments. It requires transferring knowledge from environments where
empirical data is more readily available. Against these backdrops, this paper
explores a fundamental question in online learning: Can we employ non-i.i.d.
actions to learn about confounders even when requiring knowledge transfer? We
present a sample-efficient algorithm designed to accurately identify system
dynamics under information asymmetry and to navigate the challenges of
knowledge transfer effectively in reinforcement learning, framed within an
online strategic interaction model. Our method provably achieves learning of an
$\epsilon$-optimal policy with a tight sample complexity of $O(1/\epsilon^2)$.

</details>


### [230] [LaDCast: A Latent Diffusion Model for Medium-Range Ensemble Weather Forecasting](https://arxiv.org/abs/2506.09193)
*Yilin Zhuang,Karthik Duraisamy*

Main category: cs.LG

TL;DR: The paper introduces LaDCast, a latent-diffusion model for probabilistic and efficient medium-range weather forecasting, outperforming traditional models while reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Accurate and computationally efficient weather forecasting is challenging, especially in incorporating uncertainty, as current methods are computationally expensive or lack precision.

Method: LaDCast compresses high-dimensional weather data into a latent space using an autoencoder and employs a transformer-based diffusion model for updates. It integrates specialized components like GeoRoPE, sinusoidal temporal embeddings, and dual-stream attention for geometry, seasonality, and efficient conditioning.

Result: LaDCast achieves skill levels comparable to state-of-the-art weather models like IFS-ENS without needing explicit perturbations. It performs well in predicting extreme events and reduces computational and storage demands significantly.

Conclusion: LaDCast offers a scalable, efficient, and accurate approach to weather forecasting, particularly beneficial for real-time and high-resolution applications, with its open-source tools fostering further advancements.

Abstract: Accurate probabilistic weather forecasting demands both high accuracy and
efficient uncertainty quantification, challenges that overburden both ensemble
numerical weather prediction (NWP) and recent machine-learning methods. We
introduce LaDCast, the first global latent-diffusion framework for medium-range
ensemble forecasting, which generates hourly ensemble forecasts entirely in a
learned latent space. An autoencoder compresses high-dimensional ERA5
reanalysis fields into a compact representation, and a transformer-based
diffusion model produces sequential latent updates with arbitrary hour
initialization. The model incorporates Geometric Rotary Position Embedding
(GeoRoPE) to account for the Earth's spherical geometry, a dual-stream
attention mechanism for efficient conditioning, and sinusoidal temporal
embeddings to capture seasonal patterns. LaDCast achieves deterministic and
probabilistic skill close to that of the European Centre for Medium-Range
Forecast IFS-ENS, without any explicit perturbations. Notably, LaDCast
demonstrates superior performance in tracking rare extreme events such as
cyclones, capturing their trajectories more accurately than established models.
By operating in latent space, LaDCast reduces storage and compute by orders of
magnitude, demonstrating a practical path toward forecasting at kilometer-scale
resolution in real time. We open-source our code and models and provide the
training and evaluation pipelines at: https://github.com/tonyzyl/ladcast.

</details>


### [231] [FedRAG: A Framework for Fine-Tuning Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2506.09200)
*Val Andrei Fajardo,David B. Emerson,Amandeep Singh,Veronica Chatrath,Marcelo Lotif,Ravi Theja,Alex Cheung,Izuki Matsubi*

Main category: cs.LG

TL;DR: FedRAG is a framework for fine-tuning retrieval-augmented generation (RAG) systems, supporting centralized and federated architectures.


<details>
  <summary>Details</summary>
Motivation: To address the lack of tools for seamlessly fine-tuning RAG systems across centralized and federated settings, while improving system performance.

Method: The authors developed FedRAG, which integrates modern RAG ecosystems and supports fine-tuning methods, enabling easy transitions between centralized and federated architectures.

Result: FedRAG was designed as a comprehensive platform that fills a critical gap in RAG system training capabilities, enhancing usability and performance.

Conclusion: FedRAG contributes significantly to improving RAG tools, providing an intuitive and integrated solution for both centralized and federated fine-tuning of models.

Abstract: Retrieval-augmented generation (RAG) systems have been shown to be effective
in addressing many of the drawbacks of relying solely on the parametric memory
of large language models. Recent work has demonstrated that RAG systems can be
improved via fine-tuning of their retriever and generator models. In this work,
we introduce FedRAG, a framework for fine-tuning RAG systems across centralized
and federated architectures. FedRAG supports state-of-the-art fine-tuning
methods, offering a simple and intuitive interface and a seamless conversion
from centralized to federated training tasks. FedRAG is also deeply integrated
with the modern RAG ecosystem, filling a critical gap in available tools.

</details>


### [232] [Policy-Based Trajectory Clustering in Offline Reinforcement Learning](https://arxiv.org/abs/2506.09202)
*Hao Hu,Xinqi Wang,Simon Shaolei Du*

Main category: cs.LG

TL;DR: The paper introduces a method for clustering offline RL trajectories by associating each cluster with a generating policy, using novel techniques PG-Kmeans and CAAE.


<details>
  <summary>Details</summary>
Motivation: To address the need for effective clustering of trajectories in offline RL datasets to identify and group the policies that generated them.

Method: Proposed two approaches: PG-Kmeans, which iteratively trains behavior cloning policies and assigns trajectories based on generation probabilities, and CAAE, which guides latent trajectory representations toward specific codebook entries for clustering.

Result: The paper provides theoretical guarantees for PG-Kmeans convergence and validates both methods (PG-Kmeans and CAAE) on the D4RL dataset and GridWorld environments. Successful trajectory clustering was achieved.

Conclusion: PG-Kmeans and CAAE provide effective frameworks for clustering trajectories in offline RL datasets, with potential applications across various offline RL scenarios.

Abstract: We introduce a novel task of clustering trajectories from offline
reinforcement learning (RL) datasets, where each cluster center represents the
policy that generated its trajectories. By leveraging the connection between
the KL-divergence of offline trajectory distributions and a mixture of
policy-induced distributions, we formulate a natural clustering objective. To
solve this, we propose Policy-Guided K-means (PG-Kmeans) and Centroid-Attracted
Autoencoder (CAAE). PG-Kmeans iteratively trains behavior cloning (BC) policies
and assigns trajectories based on policy generation probabilities, while CAAE
resembles the VQ-VAE framework by guiding the latent representations of
trajectories toward the vicinity of specific codebook entries to achieve
clustering. Theoretically, we prove the finite-step convergence of PG-Kmeans
and identify a key challenge in offline trajectory clustering: the inherent
ambiguity of optimal solutions due to policy-induced conflicts, which can
result in multiple equally valid but structurally distinct clusterings.
Experimentally, we validate our methods on the widely used D4RL dataset and
custom GridWorld environments. Our results show that both PG-Kmeans and CAAE
effectively partition trajectories into meaningful clusters. They offer a
promising framework for policy-based trajectory clustering, with broad
applications in offline RL and beyond.

</details>


### [233] [mLaSDI: Multi-stage latent space dynamics identification](https://arxiv.org/abs/2506.09207)
*William Anderson,Kevin Chung,Youngsoo Choi*

Main category: cs.LG

TL;DR: This paper introduces mLaSDI, an improved model for solving PDEs using multi-stage autoencoders to enhance prediction accuracy and reduce training time.


<details>
  <summary>Details</summary>
Motivation: Solving PDEs efficiently is essential for various scientific applications, but existing models like LaSDI struggle with accuracy and computational efficiency in challenging regimes.

Method: The proposed mLaSDI trains sequential, small-scale autoencoders to refine latent dynamics gradually, correcting errors at each stage.

Result: mLaSDI achieves better prediction and reconstruction accuracy while also reducing the training time compared to the original LaSDI framework.

Conclusion: Using staged autoencoders in mLaSDI enhances overall performance, addressing limitations in LaSDI, making it more suitable for complex scenarios.

Abstract: Determining accurate numerical solutions of partial differential equations
(PDEs) is an important task in many scientific disciplines. However, solvers
can be computationally expensive, leading to the development of reduced-order
models (ROMs). Recently, Latent Space Dynamics Identification (LaSDI) was
proposed as a data-driven, non-intrusive ROM framework. LaSDI compresses the
training data using an autoencoder and learns a system of user-chosen ordinary
differential equations (ODEs), which govern the latent space dynamics. This
allows for rapid predictions by interpolating and evolving the low-dimensional
ODEs in the latent space. While LaSDI has produced effective ROMs for numerous
problems, the autoencoder can have difficulty accurately reconstructing
training data while also satisfying the imposed dynamics in the latent space,
particularly in complex or high-frequency regimes. To address this, we propose
multi-stage Latent Space Dynamics Identification (mLaSDI). With mLaSDI, several
autoencoders are trained sequentially in stages, where each autoencoder learns
to correct the error of the previous stages. We find that applying mLaSDI with
small autoencoders results in lower prediction and reconstruction errors, while
also reducing training time compared to LaSDI.

</details>


### [234] [Robust Noise Attenuation via Adaptive Pooling of Transformer Outputs](https://arxiv.org/abs/2506.09215)
*Greyson Brothers*

Main category: cs.LG

TL;DR: This study critiques traditional transformer pooling methods and introduces an attention-based adaptive pooling approach to improve robustness in signal-to-noise ratio-sensitive scenarios.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by challenges in reinforcement learning and vision tasks where input signals are mixed with noise, making reliable data aggregation crucial.

Method: Pooling is reframed as vector quantization to address signal loss, and theoretical analysis supports an adaptive attention-based pooling strategy.

Result: Validation includes synthetic datasets and various benchmarks, demonstrating improved performance and resilience to noise using adaptive pooling.

Conclusion: Adaptive pooling enhances transformer output aggregation, offering superior robustness under fluctuating signal-to-noise conditions across diverse applications.

Abstract: We investigate the design of pooling methods used to summarize the outputs of
transformer embedding models, primarily motivated by reinforcement learning and
vision applications. This work considers problems where a subset of the input
vectors contains requisite information for a downstream task (signal) while the
rest are distractors (noise). By framing pooling as vector quantization with
the goal of minimizing signal loss, we demonstrate that the standard methods
used to aggregate transformer outputs, AvgPool, MaxPool, and ClsToken, are
vulnerable to performance collapse as the signal-to-noise ratio (SNR) of inputs
fluctuates. We then show that an attention-based adaptive pooling method can
approximate the signal-optimal vector quantizer within derived error bounds for
any SNR. Our theoretical results are first validated by supervised experiments
on a synthetic dataset designed to isolate the SNR problem, then generalized to
standard relational reasoning, multi-agent reinforcement learning, and vision
benchmarks with noisy observations, where transformers with adaptive pooling
display superior robustness across tasks.

</details>


### [235] [SoK: Machine Unlearning for Large Language Models](https://arxiv.org/abs/2506.09227)
*Jie Ren,Yue Xing,Yingqian Cui,Charu C. Aggarwal,Hui Liu*

Main category: cs.LG

TL;DR: This paper explores techniques and intentions behind unlearning in large language models (LLMs), proposes a taxonomy for understanding them, critiques current evaluation methods, and addresses practical challenges.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the gap in understanding unlearning techniques for LLMs by focusing on their underlying intentions (true removal vs. behavior suppression), instead of merely analyzing technical methods.

Method: The authors propose a taxonomy based on intention (true removal vs. suppression), revisit the functionality of existing unlearning methods, critique and suggest improvements for evaluation metrics, and discuss practical challenges.

Result: The study offers insights into whether existing unlearning methods achieve true knowledge removal or suppression, exposes flaws in current evaluation metrics, and identifies obstacles for scaling and sequential unlearning.

Conclusion: The proposed framework and intention-oriented taxonomy provide a foundation for improving unlearning in generative AI, offering directions for future research and policymaking on data privacy and removal.

Abstract: Large language model (LLM) unlearning has become a critical topic in machine
learning, aiming to eliminate the influence of specific training data or
knowledge without retraining the model from scratch. A variety of techniques
have been proposed, including Gradient Ascent, model editing, and re-steering
hidden representations. While existing surveys often organize these methods by
their technical characteristics, such classifications tend to overlook a more
fundamental dimension: the underlying intention of unlearning--whether it seeks
to truly remove internal knowledge or merely suppress its behavioral effects.
In this SoK paper, we propose a new taxonomy based on this intention-oriented
perspective. Building on this taxonomy, we make three key contributions. First,
we revisit recent findings suggesting that many removal methods may
functionally behave like suppression, and explore whether true removal is
necessary or achievable. Second, we survey existing evaluation strategies,
identify limitations in current metrics and benchmarks, and suggest directions
for developing more reliable and intention-aligned evaluations. Third, we
highlight practical challenges--such as scalability and support for sequential
unlearning--that currently hinder the broader deployment of unlearning methods.
In summary, this work offers a comprehensive framework for understanding and
advancing unlearning in generative AI, aiming to support future research and
guide policy decisions around data removal and privacy.

</details>


### [236] [Agent-based Condition Monitoring Assistance with Multimodal Industrial Database Retrieval Augmented Generation](https://arxiv.org/abs/2506.09247)
*Karl Löwenmark,Daniel Strömbergsson,Chang Liu,Marcus Liwicki,Fredrik Sandin*

Main category: cs.LG

TL;DR: This paper introduces MindRAG, a modular framework integrating large language models with condition monitoring workflows to improve fault severity estimation, decision support, and explainability, while reducing false alarms in the process industry.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the limitations of existing computerized maintenance systems in effectively estimating fault severity and aiding decision-making, which still require human intervention due to high uncertainty and false alarm rates.

Method: The proposed method involves the development of a modular framework called MindRAG, which combines multimodal retrieval-augmented generation (RAG) with custom vector store structures designed for structured industry CM data, leveraging annotations and work orders as labels for supervised learning.

Result: Preliminary results demonstrate that MindRAG provides meaningful decision support by reducing false alarms, improving fault severity estimation, and enhancing interpretability in condition monitoring systems.

Conclusion: The integration of LLM-based reasoning agents using the MindRAG framework shows promise in improving the reliability and efficiency of condition monitoring systems, addressing human-expert dependency and advancing industrial workflows.

Abstract: Condition monitoring (CM) plays a crucial role in ensuring reliability and
efficiency in the process industry. Although computerised maintenance systems
effectively detect and classify faults, tasks like fault severity estimation,
and maintenance decisions still largely depend on human expert analysis. The
analysis and decision making automatically performed by current systems
typically exhibit considerable uncertainty and high false alarm rates, leading
to increased workload and reduced efficiency.
  This work integrates large language model (LLM)-based reasoning agents with
CM workflows to address analyst and industry needs, namely reducing false
alarms, enhancing fault severity estimation, improving decision support, and
offering explainable interfaces. We propose MindRAG, a modular framework
combining multimodal retrieval-augmented generation (RAG) with novel vector
store structures designed specifically for CM data. The framework leverages
existing annotations and maintenance work orders as surrogates for labels in a
supervised learning protocol, addressing the common challenge of training
predictive models on unlabelled and noisy real-world datasets.
  The primary contributions include: (1) an approach for structuring industry
CM data into a semi-structured multimodal vector store compatible with
LLM-driven workflows; (2) developing multimodal RAG techniques tailored for CM
data; (3) developing practical reasoning agents capable of addressing
real-world CM queries; and (4) presenting an experimental framework for
integrating and evaluating such agents in realistic industrial scenarios.
Preliminary results, evaluated with the help of an experienced analyst,
indicate that MindRAG provide meaningful decision support for more efficient
management of alarms, thereby improving the interpretability of CM systems.

</details>


### [237] [Uncertainty Prioritized Experience Replay](https://arxiv.org/abs/2506.09270)
*Rodrigo Carrasco-Davis,Sebastian Lee,Claudia Clopath,Will Dabney*

Main category: cs.LG

TL;DR: This paper proposes using epistemic uncertainty for better transition prioritization in experience replay for reinforcement learning, addressing issues with traditional temporal difference error prioritization.


<details>
  <summary>Details</summary>
Motivation: Traditional temporal difference error prioritization in experience replay tends to favor noisy transitions, leading to inefficiencies in learning. This resembles the noisy TV problem in exploration, where agents may overemphasize irrelevant noise.

Method: The authors propose using epistemic uncertainty—measuring reducible uncertainty through learning—to guide prioritization of transitions in the replay buffer, instead of relying solely on temporal difference error.

Result: The proposed scheme demonstrates improved performance in toy models (multi-arm bandit and noisy gridworld) and on the Atari suite, outperforming benchmark methods like quantile regression deep Q-learning.

Conclusion: Epistemic uncertainty prioritization effectively mitigates noise-related disruptions and enhances performance, highlighting its potential for broader application in reinforcement learning.

Abstract: Prioritized experience replay, which improves sample efficiency by selecting
relevant transitions to update parameter estimates, is a crucial component of
contemporary value-based deep reinforcement learning models. Typically,
transitions are prioritized based on their temporal difference error. However,
this approach is prone to favoring noisy transitions, even when the value
estimation closely approximates the target mean. This phenomenon resembles the
noisy TV problem postulated in the exploration literature, in which
exploration-guided agents get stuck by mistaking noise for novelty. To mitigate
the disruptive effects of noise in value estimation, we propose using epistemic
uncertainty estimation to guide the prioritization of transitions from the
replay buffer. Epistemic uncertainty quantifies the uncertainty that can be
reduced by learning, hence reducing transitions sampled from the buffer
generated by unpredictable random processes. We first illustrate the benefits
of epistemic uncertainty prioritized replay in two tabular toy models: a simple
multi-arm bandit task, and a noisy gridworld. Subsequently, we evaluate our
prioritization scheme on the Atari suite, outperforming quantile regression
deep Q-learning benchmarks; thus forging a path for the use of uncertainty
prioritized replay in reinforcement learning agents.

</details>


### [238] [Learning The Minimum Action Distance](https://arxiv.org/abs/2506.09276)
*Lorenzo Steccanella,Joshua B. Evans,Özgür Şimşek,Anders Jonsson*

Main category: cs.LG

TL;DR: The paper introduces a self-supervised learning framework for MDPs that learns state representations using minimum action distance (MAD), enabling tasks like goal-conditioned reinforcement learning and reward shaping.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of learning effective state representations in MDPs without relying on reward signals or agent actions.

Method: Develop a self-supervised approach that embeds states based on MAD values, allowing geometric interpretation of distances. Test this approach on environments with varied dynamics and state spaces.

Result: Empirical analysis shows efficient learning of MAD representations and superior performance compared to existing state representation methods.

Conclusion: The proposed framework effectively models environment structure using MAD, enhancing state representation quality and applicability to diverse MDP tasks.

Abstract: This paper presents a state representation framework for Markov decision
processes (MDPs) that can be learned solely from state trajectories, requiring
neither reward signals nor the actions executed by the agent. We propose
learning the minimum action distance (MAD), defined as the minimum number of
actions required to transition between states, as a fundamental metric that
captures the underlying structure of an environment. MAD naturally enables
critical downstream tasks such as goal-conditioned reinforcement learning and
reward shaping by providing a dense, geometrically meaningful measure of
progress. Our self-supervised learning approach constructs an embedding space
where the distances between embedded state pairs correspond to their MAD,
accommodating both symmetric and asymmetric approximations. We evaluate the
framework on a comprehensive suite of environments with known MAD values,
encompassing both deterministic and stochastic dynamics, as well as discrete
and continuous state spaces, and environments with noisy observations.
Empirical results demonstrate that the proposed approach not only efficiently
learns accurate MAD representations across these diverse settings but also
significantly outperforms existing state representation methods in terms of
representation quality.

</details>


### [239] [A Topic Modeling Analysis of Stigma Dimensions, Social, and Related Behavioral Circumstances in Clinical Notes Among Patients with HIV](https://arxiv.org/abs/2506.09279)
*Ziyi Chen,Yiyang Liu,Mattia Prosperi,Krishna Vaddiparti,Robert L Cook,Jiang Bian,Yi Guo,Yonghui Wu*

Main category: cs.LG

TL;DR: Researchers utilized natural language processing on clinical notes of 9,140 people living with HIV (PLWHs) to identify dimensions of HIV-related stigma, social, and behavioral contexts.


<details>
  <summary>Details</summary>
Motivation: To address limitations in traditional survey-based assessments and better understand HIV-related stigma and its impacts, using scalable methods applied to real-world health records.

Method: The study leveraged Latent Dirichlet Allocation (LDA) topic modeling on clinical notes, guided by stigma-related keywords, to identify pertinent themes. Subgroup analyses were also conducted based on demographics like age and sex.

Result: The analysis uncovered themes such as mental health concerns, social support, limited healthcare access, and treatment refusal. Differences in topics were observed among age-specific subgroups.

Conclusion: This approach provides an efficient alternative to questionnaires, enabling deeper insights into stigma dimensions and potentially improving patient outcomes.

Abstract: Objective: To characterize stigma dimensions, social, and related behavioral
circumstances in people living with HIV (PLWHs) seeking care, using natural
language processing methods applied to a large collection of electronic health
record (EHR) clinical notes from a large integrated health system in the
southeast United States. Methods: We identified 9,140 cohort of PLWHs from the
UF Health IDR and performed topic modeling analysis using Latent Dirichlet
Allocation (LDA) to uncover stigma dimensions, social, and related behavioral
circumstances. Domain experts created a seed list of HIV-related stigma
keywords, then applied a snowball strategy to iteratively review notes for
additional terms until saturation was reached. To identify more target topics,
we tested three keyword-based filtering strategies. Domain experts manually
reviewed the detected topics using the prevalent terms and key discussion
topics. Word frequency analysis was used to highlight the prevalent terms
associated with each topic. In addition, we conducted topic variation analysis
among subgroups to examine differences across age and sex-specific
demographics. Results and Conclusion: Topic modeling on sentences containing at
least one keyword uncovered a wide range of topic themes associated with
HIV-related stigma, social, and related behaviors circumstances, including
"Mental Health Concern and Stigma", "Social Support and Engagement", "Limited
Healthcare Access and Severe Illness", "Treatment Refusal and Isolation" and so
on. Topic variation analysis across age subgroups revealed differences.
Extracting and understanding the HIV-related stigma dimensions, social, and
related behavioral circumstances from EHR clinical notes enables scalable,
time-efficient assessment, overcoming the limitations of traditional
questionnaires and improving patient outcomes.

</details>


### [240] [Causal Graph Recovery in Neuroimaging through Answer Set Programming](https://arxiv.org/abs/2506.09286)
*Mohammadsajad Abavisani,Kseniya Solovyeva,David Danks,Vince Calhoun,Sergey Plis*

Main category: cs.LG

TL;DR: The paper addresses challenges in inferring causal graphs from sub-sampled time series data, using Answer Set Programming (ASP) for optimal graph reconstruction.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome challenges in identifying causal structures when the system's causal timescale does not match data measurement frequency, leading to incomplete or inaccurate causal graphs.

Method: The authors leverage Answer Set Programming (ASP) to identify the most probable causal graph and an equivalence class of possible graphs, while pruning the solution space using graph theory.

Result: The approach outperforms traditional methods, showing a 12% improvement in F1 score, state-of-the-art precision and recall, and robustness to varying levels of sub-sampling.

Conclusion: The proposed method provides a superior meta-approach for causal graph reconstruction from sub-sampled time series, improving accuracy and robustness compared to existing techniques.

Abstract: Learning graphical causal structures from time series data presents
significant challenges, especially when the measurement frequency does not
match the causal timescale of the system. This often leads to a set of equally
possible underlying causal graphs due to information loss from sub-sampling
(i.e., not observing all possible states of the system throughout time). Our
research addresses this challenge by incorporating the effects of sub-sampling
in the derivation of causal graphs, resulting in more accurate and intuitive
outcomes. We use a constraint optimization approach, specifically answer set
programming (ASP), to find the optimal set of answers. ASP not only identifies
the most probable underlying graph, but also provides an equivalence class of
possible graphs for expert selection. In addition, using ASP allows us to
leverage graph theory to further prune the set of possible solutions, yielding
a smaller, more accurate answer set significantly faster than traditional
approaches. We validate our approach on both simulated data and empirical
structural brain connectivity, and demonstrate its superiority over established
methods in these experiments. We further show how our method can be used as a
meta-approach on top of established methods to obtain, on average, 12%
improvement in F1 score. In addition, we achieved state of the art results in
terms of precision and recall of reconstructing causal graph from sub-sampled
time series data. Finally, our method shows robustness to varying degrees of
sub-sampling on realistic simulations, whereas other methods perform worse for
higher rates of sub-sampling.

</details>


### [241] [On-the-Fly Adaptive Distillation of Transformer to Dual-State Linear Attention](https://arxiv.org/abs/2506.09316)
*Yeonju Ro,Zhenyu Zhang,Souvik Kundu,Zhangyang Wang,Aditya Akella*

Main category: cs.LG

TL;DR: This paper tackles the challenge of efficiency and accuracy in processing long inputs using large language models (LLMs). It introduces DSLA, a dual-state linear attention mechanism, and Serve, an adaptive inference framework, achieving faster inference and maintaining strong performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the inefficiency and memory limitations of LLMs when dealing with lengthy inputs, while preserving accuracy and global context.

Method: The authors proposed DSLA, a dual-state linear attention mechanism for better context preservation, and Serve, an adaptive distillation framework that replaces Transformer layers with DSLA layers during inference based on sensitivity metrics.

Result: The study demonstrates that Serve achieves inference speeds that are 2.3x faster than Llama2-7B and 3.0x faster than Zamba-7B, with performance comparable across various tasks.

Conclusion: The dual-state mechanism addresses shortcomings of linear attentions in capturing dependencies, and Serve provides an efficient, adaptive approach for inference in large language models without compromising accuracy.

Abstract: Large language models (LLMs) excel at capturing global token dependencies via
self-attention but face prohibitive compute and memory costs on lengthy inputs.
While sub-quadratic methods (e.g., linear attention) can reduce these costs,
they often degrade accuracy due to overemphasizing recent tokens. In this work,
we first propose \textit{dual-state linear attention} (\textbf{\dsla}), a novel
design that maintains two specialized hidden states-one for preserving
historical context and one for tracking recency-thereby mitigating the
short-range bias typical of linear-attention architectures. To further balance
efficiency and accuracy under dynamic workload conditions, we introduce
\textbf{\serve}, an online \textit{adaptive distillation} framework that
progressively replaces Transformer layers with DSLA layers at inference time,
guided by a sensitivity-based layer ordering. \serve\ uses a chained
fine-tuning strategy to ensure that each newly converted DSLA layer remains
consistent with previously replaced layers, preserving the overall quality.
Extensive evaluations on commonsense reasoning, long-context QA, and text
summarization demonstrate that \serve\ yields \textbf{2.3x} faster inference
than Llama2-7B and \textbf{3.0x} faster than the hybrid Zamba-7B, while
retaining comparable performance across downstream tasks. Our ablation studies
show that DSLA's dual states capture both global and local dependencies,
addressing the historical-token underrepresentation seen in prior linear
attentions. Codes are available at https://github.com/utnslab/DSLA-Serve.

</details>


### [242] [Natural Language Guided Ligand-Binding Protein Design](https://arxiv.org/abs/2506.09332)
*Zhenqiao Song,Ramith Hettiarachchi,Chuan Li,Jianwen Xie,Lei Li*

Main category: cs.LG

TL;DR: This paper introduces InstructPro, an AI model that designs ligand-binding proteins based on natural language instructions and ligand formulas, outperforming previous models.


<details>
  <summary>Details</summary>
Motivation: Protein design targeting specific ligand binding is critical for biological and chemical applications, but data scarcity has limited advancements. This paper leverages abundant curated textual data instead of scarce protein-ligand complex data.

Method: The authors propose InstructPro, a generative model trained on a novel dataset (InstructProBench) of 9.5M function descriptions, ligand formulas, and protein sequences. Two model variants—InstructPro-1B and InstructPro-3B—were developed using natural language instructions and SMILES representation of ligands.

Result: InstructPro models outperform existing options like ProGen2, ESM3, and Pinal. The 1B variant achieved an 81.52% docking success rate with a low RMSD of 4.026Å, while the 3B variant reduced RMSD further to 2.527Å.

Conclusion: InstructPro successfully ties natural language processing with protein design to achieve high-accuracy ligand-binding proteins, establishing a promising method for innovative protein engineering.

Abstract: Can AI protein models follow human language instructions and design proteins
with desired functions (e.g. binding to a ligand)? Designing proteins that bind
to a given ligand is crucial in a wide range of applications in biology and
chemistry. Most prior AI models are trained on protein-ligand complex data,
which is scarce due to the high cost and time requirements of laboratory
experiments. In contrast, there is a substantial body of human-curated text
descriptions about protein-ligand interactions and ligand formula. In this
paper, we propose InstructPro, a family of protein generative models that
follow natural language instructions to design ligand-binding proteins. Given a
textual description of the desired function and a ligand formula in SMILES,
InstructPro generates protein sequences that are functionally consistent with
the specified instructions. We develop the model architecture, training
strategy, and a large-scale dataset, InstructProBench, to support both training
and evaluation. InstructProBench consists of 9,592,829 triples of (function
description, ligand formula, protein sequence). We train two model variants:
InstructPro-1B (with 1 billion parameters) and InstructPro-3B~(with 3 billion
parameters). Both variants consistently outperform strong baselines, including
ProGen2, ESM3, and Pinal. Notably, InstructPro-1B achieves the highest docking
success rate (81.52% at moderate confidence) and the lowest average root mean
square deviation (RMSD) compared to ground truth structures (4.026{\AA}).
InstructPro-3B further descreases the average RMSD to 2.527{\AA}, demonstrating
InstructPro's ability to generate ligand-binding proteins that align with the
functional specifications.

</details>


### [243] [ErrorEraser: Unlearning Data Bias for Improved Continual Learning](https://arxiv.org/abs/2506.09347)
*Xuemei Cao,Hanlin Gu,Xin Yang,Bingjun Wei,Haoyang Liang,Xiangkun Wang,Tianrui Li*

Main category: cs.LG

TL;DR: Continual Learning (CL) faces challenges with data biases, causing spurious correlations that hinder its performance. The proposed solution, ErrorEraser, identifies and removes erroneous memories, boosting accuracy and retaining knowledge effectively.


<details>
  <summary>Details</summary>
Motivation: Current CL methods suffer from learning spurious correlations due to biases in real-world data, leading to reduced ability to retain and transfer knowledge.

Method: ErrorEraser, a plugin with Error Identification and Error Erasure modules, detects biased samples in feature space and modifies the decision space to erase only erroneous knowledge. It also uses incremental feature distribution learning to improve efficiency.

Result: Experimental results demonstrate that ErrorEraser significantly reduces the impact of data biases, improving accuracy and lowering forgetting rates in various CL methods.

Conclusion: ErrorEraser effectively addresses biases in CL, offering a robust way to enhance knowledge retention and transfer while reducing computational overhead.

Abstract: Continual Learning (CL) primarily aims to retain knowledge to prevent
catastrophic forgetting and transfer knowledge to facilitate learning new
tasks. Unlike traditional methods, we propose a novel perspective: CL not only
needs to prevent forgetting, but also requires intentional forgetting.This
arises from existing CL methods ignoring biases in real-world data, leading the
model to learn spurious correlations that transfer and amplify across tasks.
From feature extraction and prediction results, we find that data biases
simultaneously reduce CL's ability to retain and transfer knowledge. To address
this, we propose ErrorEraser, a universal plugin that removes erroneous
memories caused by biases in CL, enhancing performance in both new and old
tasks. ErrorEraser consists of two modules: Error Identification and Error
Erasure. The former learns the probability density distribution of task data in
the feature space without prior knowledge, enabling accurate identification of
potentially biased samples. The latter ensures only erroneous knowledge is
erased by shifting the decision space of representative outlier samples.
Additionally, an incremental feature distribution learning strategy is designed
to reduce the resource overhead during error identification in downstream
tasks. Extensive experimental results show that ErrorEraser significantly
mitigates the negative impact of data biases, achieving higher accuracy and
lower forgetting rates across three types of CL methods. The code is available
at https://github.com/diadai/ErrorEraser.

</details>


### [244] [Anomaly Detection and Generation with Diffusion Models: A Survey](https://arxiv.org/abs/2506.09368)
*Yang Liu,Jing Liu,Chengfang Li,Rui Xi,Wenchao Li,Liang Cao,Jin Wang,Laurence T. Yang,Junsong Yuan,Wei Zhou*

Main category: cs.LG

TL;DR: This survey explores the use of diffusion models for anomaly detection and generation (ADGDM), highlighting their synergistic relationship and potential across various data types.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of anomaly data scarcity and build on the versatility of diffusion models for detecting and generating anomalies across multiple domains.

Method: The paper reviews theoretical foundations, practical implementations, data modalities, and categorizes ADGDM methods based on anomaly scoring mechanisms, conditioning strategies, and architectures.

Result: Illustrates how diffusion models enable a feedback loop between anomaly detection and generation to enhance both processes synergistically.

Conclusion: Provides a comprehensive taxonomy, discusses challenges like scalability, and suggests future improvements such as efficient architectures and integration with foundation models to advance ADGDM research.

Abstract: Anomaly detection (AD) plays a pivotal role across diverse domains, including
cybersecurity, finance, healthcare, and industrial manufacturing, by
identifying unexpected patterns that deviate from established norms in
real-world data. Recent advancements in deep learning, specifically diffusion
models (DMs), have sparked significant interest due to their ability to learn
complex data distributions and generate high-fidelity samples, offering a
robust framework for unsupervised AD. In this survey, we comprehensively review
anomaly detection and generation with diffusion models (ADGDM), presenting a
tutorial-style analysis of the theoretical foundations and practical
implementations and spanning images, videos, time series, tabular, and
multimodal data. Crucially, unlike existing surveys that often treat anomaly
detection and generation as separate problems, we highlight their inherent
synergistic relationship. We reveal how DMs enable a reinforcing cycle where
generation techniques directly address the fundamental challenge of anomaly
data scarcity, while detection methods provide critical feedback to improve
generation fidelity and relevance, advancing both capabilities beyond their
individual potential. A detailed taxonomy categorizes ADGDM methods based on
anomaly scoring mechanisms, conditioning strategies, and architectural designs,
analyzing their strengths and limitations. We final discuss key challenges
including scalability and computational efficiency, and outline promising
future directions such as efficient architectures, conditioning strategies, and
integration with foundation models (e.g., visual-language models and large
language models). By synthesizing recent advances and outlining open research
questions, this survey aims to guide researchers and practitioners in
leveraging DMs for innovative AD solutions across diverse applications.

</details>


### [245] [LPO: Towards Accurate GUI Agent Interaction via Location Preference Optimization](https://arxiv.org/abs/2506.09373)
*Jiaqi Tang,Yu Xia,Yi-Feng Wu,Yuwei Hu,Yuhui Chen,Qing-Guo Chen,Xiaogang Xu,Xiangyu Wu,Hao Lu,Yanqing Ma,Shiyin Lu,Qifeng Chen*

Main category: cs.LG

TL;DR: This paper introduces Location Preference Optimization (LPO), a method to improve spatial localization in GUI interactions through locational data and dynamic reward functions, outperforming current methods like supervised fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing methods like supervised fine-tuning and reinforcement learning struggle to accurately assess positional data and optimize GUI interactions, creating a need for more effective approaches.

Method: The proposed method, LPO, uses locational data and information entropy to predict optimal interaction zones. It introduces a reward function based on physical distance to dynamically adjust location importance and is supported by Group Relative Preference Optimization (GRPO).

Result: LPO achieves state-of-the-art performance in GUI interaction tasks, validated by experiments on offline benchmarks and real-world online evaluations.

Conclusion: LPO significantly enhances the precision of GUI-based interactions, offering a novel optimization strategy that is practical for both simulated and real-world environments.

Abstract: The advent of autonomous agents is transforming interactions with Graphical
User Interfaces (GUIs) by employing natural language as a powerful
intermediary. Despite the predominance of Supervised Fine-Tuning (SFT) methods
in current GUI agents for achieving spatial localization, these methods face
substantial challenges due to their limited capacity to accurately perceive
positional data. Existing strategies, such as reinforcement learning, often
fail to assess positional accuracy effectively, thereby restricting their
utility. In response, we introduce Location Preference Optimization (LPO), a
novel approach that leverages locational data to optimize interaction
preferences. LPO uses information entropy to predict interaction positions by
focusing on zones rich in information. Besides, it further introduces a dynamic
location reward function based on physical distance, reflecting the varying
importance of interaction positions. Supported by Group Relative Preference
Optimization (GRPO), LPO facilitates an extensive exploration of GUI
environments and significantly enhances interaction precision. Comprehensive
experiments demonstrate LPO's superior performance, achieving SOTA results
across both offline benchmarks and real-world online evaluations. Our code will
be made publicly available soon, at https://github.com/AIDC-AI/LPO.

</details>


### [246] [Revisiting Diffusion Models: From Generative Pre-training to One-Step Generation](https://arxiv.org/abs/2506.09376)
*Bowen Zheng,Tianming Yang*

Main category: cs.LG

TL;DR: This paper highlights the limitations of diffusion distillation in reducing sampling costs and explores the advantages of replacing distillation losses with a GAN objective to create efficient one-step generation models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the shortcomings of diffusion distillation, particularly its extensive training requirements and the performance degradation of student models.

Method: The authors identified limitations related to mismatched step sizes and parameter numbers between teacher and student models, and demonstrated that a GAN objective can eliminate the need for a distillation loss. They fine-tune diffusion models with frozen parameters to create efficient one-step generation models.

Result: The proposed method achieved strong performance with minimal fine-tuning data (0.2M images) and near-SOTA results with only 5M images. A frequency-domain analysis was also presented to explain the one-step generative capability.

Conclusion: Diffusion training should be viewed as a generative pre-training process, demonstrating that lightweight GAN fine-tuning can unlock its potential to create one-step generation models efficiently.

Abstract: Diffusion distillation is a widely used technique to reduce the sampling cost
of diffusion models, yet it often requires extensive training, and the student
performance tends to be degraded. Recent studies show that incorporating a GAN
objective may alleviate these issues, yet the underlying mechanism remains
unclear. In this work, we first identify a key limitation of distillation:
mismatched step sizes and parameter numbers between the teacher and the student
model lead them to converge to different local minima, rendering direct
imitation suboptimal. We further demonstrate that a standalone GAN objective,
without relying a distillation loss, overcomes this limitation and is
sufficient to convert diffusion models into efficient one-step generators.
Based on this finding, we propose that diffusion training may be viewed as a
form of generative pre-training, equipping models with capabilities that can be
unlocked through lightweight GAN fine-tuning. Supporting this view, we create a
one-step generation model by fine-tuning a pre-trained model with 85% of
parameters frozen, achieving strong performance with only 0.2M images and
near-SOTA results with 5M images. We further present a frequency-domain
analysis that may explain the one-step generative capability gained in
diffusion training. Overall, our work provides a new perspective for diffusion
training, highlighting its role as a powerful generative pre-training process,
which can be the basis for building efficient one-step generation models.

</details>


### [247] [Efficient Prediction of SO(3)-Equivariant Hamiltonian Matrices via SO(2) Local Frames](https://arxiv.org/abs/2506.09398)
*Haiyang Yu,Yuchao Lin,Xuan Zhang,Xiaofeng Qian,Shuiwang Ji*

Main category: cs.LG

TL;DR: The paper proposes QHNetV2, a novel neural network for predicting Hamiltonian matrices with global SO(3) equivariance, avoiding costly tensor products and leveraging efficient SO(2)-equivariant operations.


<details>
  <summary>Details</summary>
Motivation: Accelerating electronic structure calculations for applications in physics, chemistry, and materials science, addressing the inherent symmetry in Hamiltonian matrices to improve computational efficiency.

Method: The authors introduce QHNetV2, employing SO(2)-equivariant operations and local frames to eliminate SO(3) tensor products, and utilize SO(2) tensor products for merging node features efficiently.

Result: The model demonstrates superior performance and generalization across diverse molecular datasets (QH9 and MD17) compared to benchmarks.

Conclusion: SO(2)-equivariant operations within SO(2) local frames present an efficient, scalable direction for symmetry-aware learning in electronic structure calculations, with potential broad applications.

Abstract: We consider the task of predicting Hamiltonian matrices to accelerate
electronic structure calculations, which plays an important role in physics,
chemistry, and materials science. Motivated by the inherent relationship
between the off-diagonal blocks of the Hamiltonian matrix and the SO(2) local
frame, we propose a novel and efficient network, called QHNetV2, that achieves
global SO(3) equivariance without the costly SO(3) Clebsch-Gordan tensor
products. This is achieved by introducing a set of new efficient and powerful
SO(2)-equivariant operations and performing all off-diagonal feature updates
and message passing within SO(2) local frames, thereby eliminating the need of
SO(3) tensor products. Moreover, a continuous SO(2) tensor product is performed
within the SO(2) local frame at each node to fuse node features, mimicking the
symmetric contraction operation. Extensive experiments on the large QH9 and
MD17 datasets demonstrate that our model achieves superior performance across a
wide range of molecular structures and trajectories, highlighting its strong
generalization capability. The proposed SO(2) operations on SO(2) local frames
offer a promising direction for scalable and symmetry-aware learning of
electronic structures. Our code will be released as part of the AIRS library
https://github.com/divelab/AIRS.

</details>


### [248] [Mitigating Spurious Correlations in LLMs via Causality-Aware Post-Training](https://arxiv.org/abs/2506.09433)
*Shurui Gui,Shuiwang Ji*

Main category: cs.LG

TL;DR: The paper proposes causality-aware post-training (CAPT) to improve large language models' generalization by addressing spurious correlations acquired during pre-training.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) often fail in out-of-distribution tasks due to spurious correlations developed during pre-training. The paper aims to address this limitation.

Method: The CAPT method is introduced, which uses two unbiased steps, event estimation and event intervention, to reduce pre-training biases without introducing new biases.

Result: Using just 100 in-distribution fine-tuning samples, 3B-scale LLMs fine-tuned with CAPT outperformed standard fine-tuning (SFT) and larger LLMs in both in-distribution and out-of-distribution tasks.

Conclusion: CAPT is effective and sample-efficient in enhancing the generalization abilities of LLMs, demonstrating improvement over traditional fine-tuning and larger models.

Abstract: While large language models (LLMs) have demonstrated remarkable capabilities
in language modeling, recent studies reveal that they often fail on
out-of-distribution (OOD) samples due to spurious correlations acquired during
pre-training. Here, we aim to mitigate such spurious correlations through
causality-aware post-training (CAPT). By decomposing a biased prediction into
two unbiased steps, known as \textit{event estimation} and \textit{event
intervention}, we reduce LLMs' pre-training biases without incurring additional
fine-tuning biases, thus enhancing the model's generalization ability.
Experiments on the formal causal inference benchmark CLadder and the logical
reasoning dataset PrOntoQA show that 3B-scale language models fine-tuned with
CAPT can outperform both traditional SFT and larger LLMs on in-distribution
(ID) and OOD tasks using only 100 ID fine-tuning samples, demonstrating the
effectiveness and sample efficiency of CAPT.

</details>


### [249] [Learning Obfuscations Of LLM Embedding Sequences: Stained Glass Transform](https://arxiv.org/abs/2506.09452)
*Jay Roberts,Kyle Mylonakis,Sidhartha Roy,Kaan Kale*

Main category: cs.LG

TL;DR: The paper introduces Stained Glass Transform, a method to secure data privacy when deploying Large Language Models (LLMs) in shared or multi-tenant infrastructures, while maintaining model utility.


<details>
  <summary>Details</summary>
Motivation: To address the security concerns of enterprise data owners who are hesitant to allow plaintext data exposure on shared AI compute infrastructures.

Method: A sequence-dependent transformation of word embeddings, connected to Gaussian Mixture Models, quantifies privacy using mutual information metrics.

Result: The transformed embeddings were shown to preserve privacy and utility through theoretical analyses and practical benchmarks.

Conclusion: Stained Glass Transform provides a balance between data privacy and model performance in managed or on-premises AI deployments.

Abstract: The high cost of ownership of AI compute infrastructure and challenges of
robust serving of large language models (LLMs) has led to a surge in managed
Model-as-a-service deployments. Even when enterprises choose on-premises
deployments, the compute infrastructure is typically shared across many teams
in order to maximize the return on investment. In both scenarios the deployed
models operate only on plaintext data, and so enterprise data owners must allow
their data to appear in plaintext on a shared or multi-tenant compute
infrastructure. This results in data owners with private or sensitive data
being hesitant or restricted in what data they use with these types of
deployments. In this work we introduce the Stained Glass Transform, a learned,
stochastic, and sequence dependent transformation of the word embeddings of an
LLM which information theoretically provides privacy to the input of the LLM
while preserving the utility of model. We theoretically connect a particular
class of Stained Glass Transforms to the theory of mutual information of
Gaussian Mixture Models. We then calculate a-postiori privacy estimates, based
on mutual information, and verify the privacy and utility of instances of
transformed embeddings through token level metrics of privacy and standard LLM
performance benchmarks.

</details>


### [250] [NDCG-Consistent Softmax Approximation with Accelerated Convergence](https://arxiv.org/abs/2506.09454)
*Yuanhao Pu,Defu Lian,Xiaolong Chen,Xu Huang,Jin Chen,Enhong Chen*

Main category: cs.LG

TL;DR: The paper introduces novel loss formulations (RG$^2$ and RG$^\times$ Loss) to improve ranking tasks' computational efficiency while maintaining comparable or superior performance to Softmax Loss.


<details>
  <summary>Details</summary>
Motivation: Softmax Loss is widely used for ranking but struggles with high computational overhead and scalability issues in large-scale settings.

Method: The authors propose RG$^2$ and RG$^\times$ Loss, derived from Taylor expansions of Softmax Loss, and integrate them with the efficient Alternating Least Squares optimization method.

Result: The proposed approach achieved similar or better ranking performance compared to Softmax Loss while substantially speeding up convergence.

Conclusion: The contributions offer enhanced computational efficiency and theoretical insights for similarity learning frameworks applicable across diverse ranking tasks.

Abstract: Ranking tasks constitute fundamental components of extreme similarity
learning frameworks, where extremely large corpora of objects are modeled
through relative similarity relationships adhering to predefined ordinal
structures. Among various ranking surrogates, Softmax (SM) Loss has been widely
adopted due to its natural capability to handle listwise ranking via global
negative comparisons, along with its flexibility across diverse application
scenarios. However, despite its effectiveness, SM Loss often suffers from
significant computational overhead and scalability limitations when applied to
large-scale object spaces. To address this challenge, we propose novel loss
formulations that align directly with ranking metrics: the
Ranking-Generalizable \textbf{squared} (RG$^2$) Loss and the
Ranking-Generalizable interactive (RG$^\times$) Loss, both derived through
Taylor expansions of the SM Loss. Notably, RG$^2$ reveals the intrinsic
mechanisms underlying weighted squared losses (WSL) in ranking methods and
uncovers fundamental connections between sampling-based and non-sampling-based
loss paradigms. Furthermore, we integrate the proposed RG losses with the
highly efficient Alternating Least Squares (ALS) optimization method, providing
both generalization guarantees and convergence rate analyses. Empirical
evaluations on real-world datasets demonstrate that our approach achieves
comparable or superior ranking performance relative to SM Loss, while
significantly accelerating convergence. This framework offers the similarity
learning community both theoretical insights and practically efficient tools,
with methodologies applicable to a broad range of tasks where balancing ranking
quality and computational efficiency is essential.

</details>


### [251] [On a few pitfalls in KL divergence gradient estimation for RL](https://arxiv.org/abs/2506.09477)
*Yunhao Tang,Rémi Munos*

Main category: cs.LG

TL;DR: The paper identifies critical pitfalls in KL divergence gradient estimation during reinforcement learning training for large language models (LLMs), proposing corrections.


<details>
  <summary>Details</summary>
Motivation: To address the recurrent errors in existing implementations of KL divergence gradient estimation for RL training in LLMs which affect gradient correctness and RL optimization.

Method: The authors highlight problems in current practices and propose fixes, including avoiding differentiation through KL as a loss function and correcting for sequential estimation issues. They validate findings through both tabular and LLM experiments.

Result: Through experiments, the study demonstrates the impact of flawed implementations and shows how the proposed corrections lead to more accurate KL gradient derivations and better outcomes in RL training.

Conclusion: Incorrect KL divergence gradient estimations can significantly impair RL training. Accurate implementation is key, and the authors provide guidance for achieving this.

Abstract: We point out a few pitfalls in implementing gradient estimation for KL
divergence in RL training for LLM, as seen in a number of open source projects
and papers. The first major pitfall is to differentiate through the KL estimate
as loss functions to minimize KL divergence. We show that such implementations
are generally incorrect and do not produce the desired KL gradient. Secondly,
we show that some implementations do not account for the sequential nature of
the estimation problem and produce a partial gradient at best. We demonstrate
the impact of such issues with illustrative tabular and LLM experiments, and
show the correct way to implement the KL gradient.

</details>


### [252] [EnerBridge-DPO: Energy-Guided Protein Inverse Folding with Markov Bridges and Direct Preference Optimization](https://arxiv.org/abs/2506.09496)
*Dingyi Rong,Haotian Lu,Wenzhuo Zheng,Fan Zhang,Shuangjia Zheng,Ning Liu*

Main category: cs.LG

TL;DR: The paper introduces EnerBridge-DPO, a framework for designing low-energy, stable protein sequences, overcoming energy-neglect limitations in current methods.


<details>
  <summary>Details</summary>
Motivation: Current deep learning approaches to protein inverse folding focus heavily on sequence recovery rates while neglecting the energy stability of the generated sequences.

Method: The proposed method integrates Markov Bridges with Direct Preference Optimization (DPO), introduces an energy constraint loss, and utilizes energy-based preferences to improve sequence energy and stability.

Result: EnerBridge-DPO generates protein sequences with lower energy while maintaining competitive sequence recovery rates compared to state-of-the-art models. It also successfully predicts ΔΔG values for different sequences.

Conclusion: EnerBridge-DPO addresses the energy-stability gap in protein design by effectively capturing sequence energy representations and generating structurally stable, low-energy protein sequences.

Abstract: Designing protein sequences with optimal energetic stability is a key
challenge in protein inverse folding, as current deep learning methods are
primarily trained by maximizing sequence recovery rates, often neglecting the
energy of the generated sequences. This work aims to overcome this limitation
by developing a model that directly generates low-energy, stable protein
sequences. We propose EnerBridge-DPO, a novel inverse folding framework focused
on generating low-energy, high-stability protein sequences. Our core innovation
lies in: First, integrating Markov Bridges with Direct Preference Optimization
(DPO), where energy-based preferences are used to fine-tune the Markov Bridge
model. The Markov Bridge initiates optimization from an information-rich prior
sequence, providing DPO with a pool of structurally plausible sequence
candidates. Second, an explicit energy constraint loss is introduced, which
enhances the energy-driven nature of DPO based on prior sequences, enabling the
model to effectively learn energy representations from a wealth of prior
knowledge and directly predict sequence energy values, thereby capturing
quantitative features of the energy landscape. Our evaluations demonstrate that
EnerBridge-DPO can design protein complex sequences with lower energy while
maintaining sequence recovery rates comparable to state-of-the-art models, and
accurately predicts $\Delta \Delta G$ values between various sequences.

</details>


### [253] [A Unified Theory of Compositionality, Modularity, and Interpretability in Markov Decision Processes](https://arxiv.org/abs/2506.09499)
*Thomas J. Ringstrom,Paul R. Schrater*

Main category: cs.LG

TL;DR: The paper introduces Option Kernel Bellman Equations (OKBEs) that optimize state-time option kernels (STOKs) for modular and interpretable goal-oriented planning in high-dimensional Markov Decision Processes.


<details>
  <summary>Details</summary>
Motivation: To resolve limitations in reward-maximization approaches that conflict with compositionality, modularity, and interpretability in long-horizon planning problems.

Method: The work develops and uses OKBEs to directly construct state-time option kernels (STOKs), which serve as predictive maps enabling goal-conditioned policies under the Options Framework. STOKs are efficiently factorizable, aim for verifiability, and support goal-focused forward planning.

Result: Highly flexible agents are generated that can synthesize meta-policies quickly, reuse planning representations, and achieve scalable planning in high-dimensional dynamic environments.

Conclusion: OKBEs offer a framework that replaces traditional reward-maximization with verifiable, modular, and interpretable methods, enhancing meta-policy planning and intrinsic motivation in reinforcement learning.

Abstract: We introduce Option Kernel Bellman Equations (OKBEs) for a new reward-free
Markov Decision Process. Rather than a value function, OKBEs directly construct
and optimize a predictive map called a state-time option kernel (STOK) to
maximize the probability of completing a goal while avoiding constraint
violations. STOKs are compositional, modular, and interpretable
initiation-to-termination transition kernels for policies in the Options
Framework of Reinforcement Learning. This means: 1) STOKs can be composed using
Chapman-Kolmogorov equations to make spatiotemporal predictions for multiple
policies over long horizons, 2) high-dimensional STOKs can be represented and
computed efficiently in a factorized and reconfigurable form, and 3) STOKs
record the probabilities of semantically interpretable goal-success and
constraint-violation events, needed for formal verification. Given a
high-dimensional state-transition model for an intractable planning problem, we
can decompose it with local STOKs and goal-conditioned policies that are
aggregated into a factorized goal kernel, making it possible to forward-plan at
the level of goals in high-dimensions to solve the problem. These properties
lead to highly flexible agents that can rapidly synthesize meta-policies, reuse
planning representations across many tasks, and justify goals using
empowerment, an intrinsic motivation function. We argue that
reward-maximization is in conflict with the properties of compositionality,
modularity, and interpretability. Alternatively, OKBEs facilitate these
properties to support verifiable long-horizon planning and intrinsic motivation
that scales to dynamic high-dimensional world-models.

</details>


### [254] [Neural Functions for Learning Periodic Signal](https://arxiv.org/abs/2506.09526)
*Woojin Cho,Minju Jo,Kookjin Lee,Noseong Park*

Main category: cs.LG

TL;DR: The paper proposes a new network architecture to improve extrapolation of periodic signals using neural representations.


<details>
  <summary>Details</summary>
Motivation: Coordinate-based MLPs struggle with overfitting and poor extrapolation for periodic signals.

Method: Introduces a novel network architecture that extracts and leverages periodic patterns to represent signals.

Result: Experiments show improved learning of periodic solutions for differential equations, and better interpolation/forecasting on real-world time series data.

Conclusion: The method enhances generalization and extrapolation for signals with periodic properties, addressing key limitations of existing models.

Abstract: As function approximators, deep neural networks have served as an effective
tool to represent various signal types. Recent approaches utilize multi-layer
perceptrons (MLPs) to learn a nonlinear mapping from a coordinate to its
corresponding signal, facilitating the learning of continuous neural
representations from discrete data points. Despite notable successes in
learning diverse signal types, coordinate-based MLPs often face issues of
overfitting and limited generalizability beyond the training region, resulting
in subpar extrapolation performance. This study addresses scenarios where the
underlying true signals exhibit periodic properties, either spatially or
temporally. We propose a novel network architecture, which extracts periodic
patterns from measurements and leverages this information to represent the
signal, thereby enhancing generalization and improving extrapolation
performance. We demonstrate the efficacy of the proposed method through
comprehensive experiments, including the learning of the periodic solutions for
differential equations, and time series imputation (interpolation) and
forecasting (extrapolation) on real-world datasets.

</details>


### [255] [Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward Models](https://arxiv.org/abs/2506.09532)
*Shuai Wang,Zhenhua Liu,Jiaheng Wei,Xuanwu Yin,Dong Li,Emad Barsoum*

Main category: cs.LG

TL;DR: Athena-PRM introduces a multimodal process reward model (PRM) to enhance the evaluation of reasoning steps in complex problems, demonstrating impressive results across benchmarks with minimal annotations.


<details>
  <summary>Details</summary>
Motivation: The motivation behind Athena-PRM is to develop an efficient and effective PRM capable of assessing reasoning step correctness, reducing the dependency on costly annotation methods and addressing limitations of noisy labels from conventional tools.

Method: Athena-PRM utilizes predictive consistency between weak and strong completers to label data, alongside two strategies: ORM initialization and negative data up-sampling, and validates performance in verification, direct evaluation, and reward optimization scenarios.

Result: Athena-PRM achieves notable improvements in various benchmarks, surpassing previous state-of-the-art models and significantly enhancing performance using test time scaling and reward-ranked fine-tuning.

Conclusion: Athena-PRM establishes itself as a SoTA approach for multimodal reasoning, demonstrating outstanding capability in accurate reasoning evaluation, efficiency in annotation usage, and strong performance across diverse tasks.

Abstract: We present Athena-PRM, a multimodal process reward model (PRM) designed to
evaluate the reward score for each step in solving complex reasoning problems.
Developing high-performance PRMs typically demands significant time and
financial investment, primarily due to the necessity for step-level annotations
of reasoning steps. Conventional automated labeling methods, such as Monte
Carlo estimation, often produce noisy labels and incur substantial
computational costs. To efficiently generate high-quality process-labeled data,
we propose leveraging prediction consistency between weak and strong completers
as a criterion for identifying reliable process labels. Remarkably, Athena-PRM
demonstrates outstanding effectiveness across various scenarios and benchmarks
with just 5,000 samples. Furthermore, we also develop two effective strategies
to improve the performance of PRMs: ORM initialization and up-sampling for
negative data. We validate our approach in three specific scenarios:
verification for test time scaling, direct evaluation of reasoning step
correctness, and reward ranked fine-tuning. Our Athena-PRM consistently
achieves superior performance across multiple benchmarks and scenarios.
Notably, when using Qwen2.5-VL-7B as the policy model, Athena-PRM enhances
performance by 10.2 points on WeMath and 7.1 points on MathVista for test time
scaling. Furthermore, Athena-PRM sets the state-of-the-art (SoTA) results in
VisualProcessBench and outperforms the previous SoTA by 3.9 F1-score,
showcasing its robust capability to accurately assess the correctness of the
reasoning step. Additionally, utilizing Athena-PRM as the reward model, we
develop Athena-7B with reward ranked fine-tuning and outperforms baseline with
a significant margin on five benchmarks.

</details>


### [256] [STOAT: Spatial-Temporal Probabilistic Causal Inference Network](https://arxiv.org/abs/2506.09544)
*Yang Yang,Du Yin,Hao Xue,Flora Salim*

Main category: cs.LG

TL;DR: STOAT is a framework for probabilistic forecasting in spatial-temporal causal time series, outperforming existing methods, particularly in capturing spatial dependencies.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing methods that model spatial and temporal dynamics independently and neglect causality-driven probabilistic forecasting.

Method: STOAT integrates a causal inference approach with a spatial relation matrix to analyze interregional dependencies and employs deep probabilistic models to estimate distribution parameters for calibrated uncertainty modeling.

Result: STOAT achieves superior performance, validated through experiments on COVID-19 data across six countries, surpassing existing models in metrics for regions with strong spatial dependencies.

Conclusion: STOAT effectively bridges causal inference and geospatial probabilistic forecasting, offering a robust solution for complex spatial-temporal tasks like epidemic management.

Abstract: Spatial-temporal causal time series (STC-TS) involve region-specific temporal
observations driven by causally relevant covariates and interconnected across
geographic or network-based spaces. Existing methods often model spatial and
temporal dynamics independently and overlook causality-driven probabilistic
forecasting, limiting their predictive power. To address this, we propose STOAT
(Spatial-Temporal Probabilistic Causal Inference Network), a novel framework
for probabilistic forecasting in STC-TS. The proposed method extends a causal
inference approach by incorporating a spatial relation matrix that encodes
interregional dependencies (e.g. proximity or connectivity), enabling spatially
informed causal effect estimation. The resulting latent series are processed by
deep probabilistic models to estimate the parameters of the distributions,
enabling calibrated uncertainty modeling. We further explore multiple output
distributions (e.g., Gaussian, Student's-$t$, Laplace) to capture
region-specific variability. Experiments on COVID-19 data across six countries
demonstrate that STOAT outperforms state-of-the-art probabilistic forecasting
models (DeepAR, DeepVAR, Deep State Space Model, etc.) in key metrics,
particularly in regions with strong spatial dependencies. By bridging causal
inference and geospatial probabilistic forecasting, STOAT offers a
generalizable framework for complex spatial-temporal tasks, such as epidemic
management.

</details>


### [257] [MOORL: A Framework for Integrating Offline-Online Reinforcement Learning](https://arxiv.org/abs/2506.09574)
*Gaurav Chaudhary,Wassim Uddin Mondal,Laxmidhar Behera*

Main category: cs.LG

TL;DR: The paper introduces MOORL, a framework combining offline and online RL, improving efficiency and scalability without added complexity.


<details>
  <summary>Details</summary>
Motivation: Challenges in Deep Reinforcement Learning include limited sample efficiency, exploration, and out-of-distribution actions affecting policy performance.

Method: MOORL uses a meta-policy to integrate offline and online RL interactions, leveraging offline data for initialization and online exploration.

Result: Theoretical analysis and experiments on 28 tasks validate MOORL's effectiveness, showing superior performance compared to baselines.

Conclusion: MOORL proves it can enhance exploration, learn stable Q-functions, and achieve high performance with minimal computational overhead, making it suitable for real-world applications.

Abstract: Sample efficiency and exploration remain critical challenges in Deep
Reinforcement Learning (DRL), particularly in complex domains. Offline RL,
which enables agents to learn optimal policies from static, pre-collected
datasets, has emerged as a promising alternative. However, offline RL is
constrained by issues such as out-of-distribution (OOD) actions that limit
policy performance and generalization. To overcome these limitations, we
propose Meta Offline-Online Reinforcement Learning (MOORL), a hybrid framework
that unifies offline and online RL for efficient and scalable learning. While
previous hybrid methods rely on extensive design components and added
computational complexity to utilize offline data effectively, MOORL introduces
a meta-policy that seamlessly adapts across offline and online trajectories.
This enables the agent to leverage offline data for robust initialization while
utilizing online interactions to drive efficient exploration. Our theoretical
analysis demonstrates that the hybrid approach enhances exploration by
effectively combining the complementary strengths of offline and online data.
Furthermore, we demonstrate that MOORL learns a stable Q-function without added
complexity. Extensive experiments on 28 tasks from the D4RL and V-D4RL
benchmarks validate its effectiveness, showing consistent improvements over
state-of-the-art offline and hybrid RL baselines. With minimal computational
overhead, MOORL achieves strong performance, underscoring its potential for
practical applications in real-world scenarios.

</details>


### [258] [Beyond Overconfidence: Foundation Models Redefine Calibration in Deep Neural Networks](https://arxiv.org/abs/2506.09593)
*Achim Hekler,Lukas Kuhn,Florian Buettner*

Main category: cs.LG

TL;DR: This paper studies the calibration behavior of foundation models, finding that they exhibit underconfidence in predictions, respond well to post-hoc calibration techniques in-distribution, but are less reliable under distribution shifts.


<details>
  <summary>Details</summary>
Motivation: Uncertainty calibration is critical for deploying deep learning models in high-stakes scenarios, but the calibration properties of foundation models like ConvNeXt and EVA are not well understood.

Method: The paper performs a detailed empirical analysis of calibration behavior in foundation models under both in-distribution and distribution shift scenarios, and evaluates the effectiveness of post-hoc calibration methods.

Result: Foundation models are found to be underconfident in in-distribution predictions and achieve better calibration under distribution shifts. Post-hoc calibration effectively mitigates in-distribution bias but is less reliable under severe distribution shifts.

Conclusion: The findings reveal complex effects of architectural and training advancements on model calibration, challenging the assumption of continuous improvement and highlighting the nuanced behavior of foundation models.

Abstract: Reliable uncertainty calibration is essential for safely deploying deep
neural networks in high-stakes applications. Deep neural networks are known to
exhibit systematic overconfidence, especially under distribution shifts.
Although foundation models such as ConvNeXt, EVA and BEiT have demonstrated
significant improvements in predictive performance, their calibration
properties remain underexplored. This paper presents a comprehensive
investigation into the calibration behavior of foundation models, revealing
insights that challenge established paradigms. Our empirical analysis shows
that these models tend to be underconfident in in-distribution predictions,
resulting in higher calibration errors, while demonstrating improved
calibration under distribution shifts. Furthermore, we demonstrate that
foundation models are highly responsive to post-hoc calibration techniques in
the in-distribution setting, enabling practitioners to effectively mitigate
underconfidence bias. However, these methods become progressively less reliable
under severe distribution shifts and can occasionally produce counterproductive
results. Our findings highlight the complex, non-monotonic effects of
architectural and training innovations on calibration, challenging established
narratives of continuous improvement.

</details>


### [259] [Accelerating Large-Scale Regularized High-Order Tensor Recovery](https://arxiv.org/abs/2506.09594)
*Wenjin Qin,Hailin Wang,Jingyao Hou,Jianjun Wang*

Main category: cs.LG

TL;DR: The paper introduces efficient and accurate randomized techniques for large-scale tensor recovery, addressing computational challenges through novel modeling and optimization methods.


<details>
  <summary>Details</summary>
Motivation: Existing tensor recovery methods face challenges in managing tensor scale variations and the computational costs associated with large-scale high-order tensor data.

Method: The study applies Krylov subspace iteration, block Lanczos bidiagonalization, and random projection strategies to develop new algorithms and integrates these with nonconvex modeling frameworks for tensor recovery across different scenarios.

Result: Extensive experiments show the proposed methods outperform state-of-the-art approaches in efficiency and effectiveness for large-scale tensor applications.

Conclusion: The paper successfully alleviates computational constraints in tensor recovery while enhancing accuracy, offering practical solutions for handling large-scale tensor data.

Abstract: Currently, existing tensor recovery methods fail to recognize the impact of
tensor scale variations on their structural characteristics. Furthermore,
existing studies face prohibitive computational costs when dealing with
large-scale high-order tensor data. To alleviate these issue, assisted by the
Krylov subspace iteration, block Lanczos bidiagonalization process, and random
projection strategies, this article first devises two fast and accurate
randomized algorithms for low-rank tensor approximation (LRTA) problem.
Theoretical bounds on the accuracy of the approximation error estimate are
established. Next, we develop a novel generalized nonconvex modeling framework
tailored to large-scale tensor recovery, in which a new regularization paradigm
is exploited to achieve insightful prior representation for large-scale
tensors. On the basis of the above, we further investigate new unified
nonconvex models and efficient optimization algorithms, respectively, for
several typical high-order tensor recovery tasks in unquantized and quantized
situations. To render the proposed algorithms practical and efficient for
large-scale tensor data, the proposed randomized LRTA schemes are integrated
into their central and time-intensive computations. Finally, we conduct
extensive experiments on various large-scale tensors, whose results demonstrate
the practicability, effectiveness and superiority of the proposed method in
comparison with some state-of-the-art approaches.

</details>


### [260] [SparseSSM: Efficient Selective Structured State Space Models Can Be Pruned in One-Shot](https://arxiv.org/abs/2506.09613)
*Kaiwen Tuo,Huan Wang*

Main category: cs.LG

TL;DR: SparseSSM provides a training-free pruning method for state-space models, cutting 50% of weights without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: State-space models like Mamba have high performance but remain computationally expensive due to billions of parameters. Current pruning methods are not well-suited to these models.

Method: Introduced SparseSSM, a layer-wise algorithm derived from the optimal brain surgeon framework. It uses second-order saliency scores and sensitivity analysis for pruning, supporting multiple sparsity types.

Result: Achieved 50% pruning of SSM weights without fine-tuning while maintaining zero-shot accuracy, setting a new state-of-the-art for pruning Mamba-based models.

Conclusion: SparseSSM is an efficient and effective pruning methodology, opening up practical deployment opportunities for computationally heavy state-space models like Mamba.

Abstract: State-space language models such as Mamba match Transformer quality while
permitting linear complexity inference, yet still comprise billions of
parameters that hinder deployment. Existing one-shot pruning methods are
tailored to attention blocks and fail to account for the time-shared and
discretized state-transition matrix at the heart of the selective state-space
module (SSM). In this paper, we introduce SparseSSM, the first training-free
pruning framework that extends the classic optimal brain surgeon (OBS)
framework to state space architectures. Our layer-wise algorithm (i) derives an
approximate second-order saliency score that aggregates Hessian-trace
information across time steps, (ii) incorporates a component sensitivity
analysis to guide feed-forward network (FFN) pruning, which also sheds light on
where redundancy resides in mamba architecture, (iii) can be easily extended to
semi-structured and structured sparsity. Empirically, we prune 50% of SSM
weights without fine-tuning and observe no zero-shot accuracy loss, achieving
the current state-of-the-art pruning algorithm for Mamba-based LLMs.

</details>


### [261] [GLGENN: A Novel Parameter-Light Equivariant Neural Networks Architecture Based on Clifford Geometric Algebras](https://arxiv.org/abs/2506.09625)
*Ekaterina Filimoshina,Dmitry Shirokov*

Main category: cs.LG

TL;DR: The paper introduces GLGENN, a novel architecture for equivariant neural networks based on geometric algebras, which excels in handling pseudo-orthogonal transformations with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Develop a neural network architecture that leverages geometric algebras to enhance equivariance, reduce overfitting, and improve efficiency compared to existing models.

Method: GLGENN uses weight-sharing parametrization based on geometric algebras' structures to encode equivariance to pseudo-orthogonal transformations efficiently.

Result: Experiments demonstrate that GLGENN matches or outperforms competitors in tasks like equivariant function estimation and convex hull determination while utilizing fewer parameters.

Conclusion: GLGENN offers a parameter-efficient yet highly effective approach for equivariant tasks, establishing its potential for broader applications in geometric deep learning.

Abstract: We propose, implement, and compare with competitors a new architecture of
equivariant neural networks based on geometric (Clifford) algebras: Generalized
Lipschitz Group Equivariant Neural Networks (GLGENN). These networks are
equivariant to all pseudo-orthogonal transformations, including rotations and
reflections, of a vector space with any non-degenerate or degenerate symmetric
bilinear form. We propose a weight-sharing parametrization technique that takes
into account the fundamental structures and operations of geometric algebras.
Due to this technique, GLGENN architecture is parameter-light and has less
tendency to overfitting than baseline equivariant models. GLGENN outperforms or
matches competitors on several benchmarking equivariant tasks, including
estimation of an equivariant function and a convex hull experiment, while using
significantly fewer optimizable parameters.

</details>


### [262] [In-Context Bias Propagation in LLM-Based Tabular Data Generation](https://arxiv.org/abs/2506.09630)
*Pol G. Recasens,Alberto Gutierrez,Jordi Torres,Josep. Ll Berral,Anisa Halimi,Kieran Fraser*

Main category: cs.LG

TL;DR: The paper examines how biases in in-context examples affect synthetic data generation using Large Language Models (LLMs), leading to fairness issues in sensitive domains.


<details>
  <summary>Details</summary>
Motivation: To address the dependency of LLM-based data generation pipelines on unbiased in-context examples and examine how statistical biases influence synthetic datasets in real-world scenarios.

Method: The study systematically analyzes the propagation of biased in-context examples to synthetic data distributions, including an adversarial scenario where malicious biases are introduced.

Result: Biased in-context examples result in significant distortions in synthetic data, and adversarial manipulation further compromises fairness for targeted groups.

Conclusion: LLM-based data generation workflows are vulnerable to bias in sensitive domains, underscoring the need for strategies to ensure fairness and mitigate malicious exploitation.

Abstract: Large Language Models (LLMs) are increasingly used for synthetic tabular data
generation through in-context learning (ICL), offering a practical solution for
data augmentation in data scarce scenarios. While prior work has shown the
potential of LLMs to improve downstream task performance through augmenting
underrepresented groups, these benefits often assume access to a subset of
unbiased in-context examples, representative of the real dataset. In real-world
settings, however, data is frequently noisy and demographically skewed. In this
paper, we systematically study how statistical biases within in-context
examples propagate to the distribution of synthetic tabular data, showing that
even mild in-context biases lead to global statistical distortions. We further
introduce an adversarial scenario where a malicious contributor can inject bias
into the synthetic dataset via a subset of in-context examples, ultimately
compromising the fairness of downstream classifiers for a targeted and
protected subgroup. Our findings demonstrate a new vulnerability associated
with LLM-based data generation pipelines that rely on in-context prompts with
in sensitive domains.

</details>


### [263] [FedVLMBench: Benchmarking Federated Fine-Tuning of Vision-Language Models](https://arxiv.org/abs/2506.09638)
*Weiying Zheng,Ziyue Lin,Pengxin Guo,Yuyin Zhou,Feifei Wang,Liangqiong Qu*

Main category: cs.LG

TL;DR: FedVLMBench introduces a pioneering benchmark for federated vision-language model fine-tuning, addressing privacy challenges and offering insights into architectures, strategies, and task optimization.


<details>
  <summary>Details</summary>
Motivation: To provide a systematic evaluation framework for federated fine-tuning of VLMs, addressing privacy concerns in domains like healthcare.

Method: FedVLMBench explores two VLM architectures, four fine-tuning strategies, five FL algorithms, and utilizes six multimodal datasets across multiple tasks.

Result: Key findings include the optimal configuration for encoder-based VLMs in FL and identification of sensitivity in FL methods to data heterogeneity in vision-centric tasks.

Conclusion: FedVLMBench serves as a benchmark and toolkit to advance privacy-preserving, federated training of multimodal models.

Abstract: Vision-Language Models (VLMs) have demonstrated remarkable capabilities in
cross-modal understanding and generation by integrating visual and textual
information. While instruction tuning and parameter-efficient fine-tuning
methods have substantially improved the generalization of VLMs, most existing
approaches rely on centralized training, posing challenges for deployment in
domains with strict privacy requirements like healthcare. Recent efforts have
introduced Federated Learning (FL) into VLM fine-tuning to address these
privacy concerns, yet comprehensive benchmarks for evaluating federated
fine-tuning strategies, model architectures, and task generalization remain
lacking. In this work, we present \textbf{FedVLMBench}, the first systematic
benchmark for federated fine-tuning of VLMs. FedVLMBench integrates two
mainstream VLM architectures (encoder-based and encoder-free), four fine-tuning
strategies, five FL algorithms, six multimodal datasets spanning four
cross-domain single-task scenarios and two cross-domain multitask settings,
covering four distinct downstream task categories. Through extensive
experiments, we uncover key insights into the interplay between VLM
architectures, fine-tuning strategies, data heterogeneity, and multi-task
federated optimization. Notably, we find that a 2-layer multilayer perceptron
(MLP) connector with concurrent connector and LLM tuning emerges as the optimal
configuration for encoder-based VLMs in FL. Furthermore, current FL methods
exhibit significantly higher sensitivity to data heterogeneity in
vision-centric tasks than text-centric ones, across both encoder-free and
encoder-based VLM architectures. Our benchmark provides essential tools,
datasets, and empirical guidance for the research community, offering a
standardized platform to advance privacy-preserving, federated training of
multimodal foundation models.

</details>


### [264] [Wavelet Scattering Transform and Fourier Representation for Offline Detection of Malicious Clients in Federated Learning](https://arxiv.org/abs/2506.09674)
*Alessandro Licciardi,Davide Leo,Davide Carbone*

Main category: cs.LG

TL;DR: WAFFLE, a method for detecting anomalous clients in Federated Learning (FL), uses Wavelet Scattering Transform or Fourier Transform to improve detection and model performance.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of maintaining model performance in FL when corrupted or anomalous clients compromise data quality.

Method: WAFFLE uses low-dimensional representations created with WST or Fourier Transform to detect malicious clients before training. A lightweight detector, trained on a public dataset, labels clients with minimal overhead.

Result: Empirical results on benchmark datasets demonstrate enhanced anomaly detection accuracy and improved downstream classification compared to existing methods.

Conclusion: WAFFLE is effective as a pre-training method for detecting anomalous clients in FL, offering benefits like stability and minimal resource requirements.

Abstract: Federated Learning (FL) enables the training of machine learning models
across decentralized clients while preserving data privacy. However, the
presence of anomalous or corrupted clients - such as those with faulty sensors
or non representative data distributions - can significantly degrade model
performance. Detecting such clients without accessing raw data remains a key
challenge. We propose WAFFLE (Wavelet and Fourier representations for Federated
Learning) a detection algorithm that labels malicious clients {\it before
training}, using locally computed compressed representations derived from
either the Wavelet Scattering Transform (WST) or the Fourier Transform. Both
approaches provide low-dimensional, task-agnostic embeddings suitable for
unsupervised client separation. A lightweight detector, trained on a
distillated public dataset, performs the labeling with minimal communication
and computational overhead. While both transforms enable effective detection,
WST offers theoretical advantages, such as non-invertibility and stability to
local deformations, that make it particularly well-suited to federated
scenarios. Experiments on benchmark datasets show that our method improves
detection accuracy and downstream classification performance compared to
existing FL anomaly detection algorithms, validating its effectiveness as a
pre-training alternative to online detection strategies.

</details>


### [265] [Wasserstein Hypergraph Neural Network](https://arxiv.org/abs/2506.09682)
*Iulia Duta,Pietro Liò*

Main category: cs.LG

TL;DR: The paper introduces the Wasserstein Hypergraph Neural Network, employing Sliced Wasserstein Pooling to aggregate information in hypergraphs, enhancing node classification tasks and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Relational modeling through hypergraphs has seen rapid advancements, but current approaches often simplify aggregations, limiting their ability to capture complex statistics or geometric properties.

Method: The proposed method aggregates information in hypergraphs by treating node and hyperedge neighborhoods as distributions and uses Sliced Wasserstein Pooling to better preserve geometric properties like shape and spread.

Result: The Wasserstein Hypergraph Neural Network shows superior performance in node classification tasks across various real-world datasets compared to existing methods.

Conclusion: Using Sliced Wasserstein Pooling enables the embedding to capture deeper geometric insights in hypergraphs, setting a new benchmark for tasks like node classification.

Abstract: The ability to model relational information using machine learning has driven
advancements across various domains, from medicine to social science. While
graph representation learning has become mainstream over the past decade,
representing higher-order relationships through hypergraphs is rapidly gaining
momentum. In the last few years, numerous hypergraph neural networks have
emerged, most of them falling under a two-stage, set-based framework. The
messages are sent from nodes to edges and then from edges to nodes. However,
most of the advancement still takes inspiration from the graph counterpart,
often simplifying the aggregations to basic pooling operations. In this paper
we are introducing Wasserstein Hypergraph Neural Network, a model that treats
the nodes and hyperedge neighbourhood as distributions and aggregate the
information using Sliced Wasserstein Pooling. Unlike conventional aggregators
such as mean or sum, which only capture first-order statistics, our approach
has the ability to preserve geometric properties like the shape and spread of
distributions. This enables the learned embeddings to reflect how easily one
hyperedge distribution can be transformed into another, following principles of
optimal transport. Experimental results demonstrate that applying Wasserstein
pooling in a hypergraph setting significantly benefits node classification
tasks, achieving top performance on several real-world datasets.

</details>


### [266] [TRIDENT: Temporally Restricted Inference via DFA-Enhanced Neural Traversal](https://arxiv.org/abs/2506.09701)
*Vincenzo Collura,Karim Tit,Laura Bussi,Eleonora Giunchiglia,Maxime Cordy*

Main category: cs.LG

TL;DR: TRIDENT is an inference-time algorithm that ensures temporal constraints compliance for outputs generated by neural models without retraining.


<details>
  <summary>Details</summary>
Motivation: Current neural architectures, including LLMs, struggle to strictly enforce temporal constraints in their outputs, particularly those defined by Linear Temporal Logic over finite traces (LTLf).

Method: TRIDENT translates LTLf formulas into a Deterministic Finite Automaton (DFA) and uses this to guide a modified beam search that masks constraint-violating transitions and re-ranks paths dynamically.

Result: TRIDENT guarantees satisfaction of temporal constraints and improves output quality. Empirical validation demonstrates perfect compliance and higher efficiency compared to state-of-the-art methods.

Conclusion: TRIDENT is an effective, model-agnostic algorithm for enforcing temporal constraints during decoding, ensuring compliance while boosting output quality metrics.

Abstract: Large Language Models (LLMs) and other neural architectures have achieved
impressive results across a variety of generative and classification tasks.
However, they remain fundamentally ill-equipped to ensure that their outputs
satisfy temporal constraints, such as those expressible in Linear Temporal
Logic over finite traces (LTLf). In this paper, we introduce TRIDENT: a general
and model-agnostic inference-time algorithm that guarantees compliance with
such constraints without requiring any retraining. TRIDENT compiles LTLf
formulas into a Deterministic Finite Automaton (DFA), which is used to guide a
constrained variant of beam search. At each decoding step, transitions that
would lead to constraint violations are masked, while remaining paths are
dynamically re-ranked based on both the model's probabilities and the DFA's
acceptance structure. We formally prove that the resulting sequences are
guaranteed to satisfy the given LTLf constraints, and we empirically
demonstrate that TRIDENT also improves output quality. We validate our approach
on two distinct tasks: temporally constrained image-stream classification and
controlled text generation. In both settings, TRIDENT achieves perfect
constraint satisfaction, while comparison with the state of the art shows
improved efficiency and high standard quality metrics.

</details>


### [267] [Auto-Compressing Networks](https://arxiv.org/abs/2506.09714)
*Vaggelis Dorovatas,Georgios Paraskevopoulos,Alexandros Potamianos*

Main category: cs.LG

TL;DR: This paper introduces Auto-Compressing Networks (ACNs), a neural network architecture with long feedforward connections that organically compress information during training, improving efficiency, noise robustness, and generalization.


<details>
  <summary>Details</summary>
Motivation: To address computational redundancy in deep neural networks without proportional improvements in representation quality, especially when increasing depth.

Method: The authors design ACNs by replacing short residual connections with additive long feedforward connections to allow dynamic information compression during training. They analyze this mechanism both theoretically and experimentally.

Result: ACNs reduce catastrophic forgetting by 18%, achieve 30-80% architectural compression, exhibit robustness in low-data settings, and perform better in transfer learning. Coupled with pruning, ACNs outmatch conventional architectures in sparsity-performance trade-offs.

Conclusion: ACNs provide an efficient alternative for deep networks by adapting computational demands to task complexity, enhancing representation quality, and maintaining accuracy with fewer parameters.

Abstract: Deep neural networks with short residual connections have demonstrated
remarkable success across domains, but increasing depth often introduces
computational redundancy without corresponding improvements in representation
quality. In this work, we introduce Auto-Compressing Networks (ACNs), an
architectural variant where additive long feedforward connections from each
layer to the output replace traditional short residual connections. ACNs
showcase a unique property we coin as "auto-compression", the ability of a
network to organically compress information during training with gradient
descent, through architectural design alone. Through auto-compression,
information is dynamically "pushed" into early layers during training,
enhancing their representational quality and revealing potential redundancy in
deeper ones. We theoretically show that this property emerges from layer-wise
training patterns present in ACNs, where layers are dynamically utilized during
training based on task requirements. We also find that ACNs exhibit enhanced
noise robustness compared to residual networks, superior performance in
low-data settings, improved transfer learning capabilities, and mitigate
catastrophic forgetting suggesting that they learn representations that
generalize better despite using fewer parameters. Our results demonstrate up to
18% reduction in catastrophic forgetting and 30-80% architectural compression
while maintaining accuracy across vision transformers, MLP-mixers, and BERT
architectures. Furthermore, we demonstrate that coupling ACNs with traditional
pruning techniques, enables significantly better sparsity-performance
trade-offs compared to conventional architectures. These findings establish
ACNs as a practical approach to developing efficient neural architectures that
automatically adapt their computational footprint to task complexity, while
learning robust representations.

</details>


### [268] [AtmosMJ: Revisiting Gating Mechanism for AI Weather Forecasting Beyond the Year Scale](https://arxiv.org/abs/2506.09733)
*Minjong Cheon*

Main category: cs.LG

TL;DR: The paper introduces AtmosMJ, a deep convolutional network for weather forecasting that operates directly on standard latitude-longitude grids, achieving 500-day stability using a novel Gated Residual Fusion mechanism.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of existing long-range weather models that rely on complex spatial remapping techniques to achieve stability.

Method: Developed a neural network called AtmosMJ, utilizing the Gated Residual Fusion mechanism to mitigate error propagation during long-term autoregressive forecasting on standard data grids.

Result: AtmosMJ demonstrated stability for 500-day forecasts with competitive 10-day accuracy against state-of-the-art models, requiring significantly less computational training time.

Conclusion: Efficient architecture design can replace complex data representations for achieving stability and accuracy in long-range weather modeling.

Abstract: The advent of Large Weather Models (LWMs) has marked a turning point in
data-driven forecasting, with many models now outperforming traditional
numerical systems in the medium range. However, achieving stable, long-range
autoregressive forecasts beyond a few weeks remains a significant challenge.
Prevailing state-of-the-art models that achieve year-long stability, such as
SFNO and DLWP-HPX, have relied on transforming input data onto non-standard
spatial domains like spherical harmonics or HEALPix meshes. This has led to the
prevailing assumption that such representations are necessary to enforce
physical consistency and long-term stability. This paper challenges that
assumption by investigating whether comparable long-range performance can be
achieved on the standard latitude-longitude grid. We introduce AtmosMJ, a deep
convolutional network that operates directly on ERA5 data without any spherical
remapping. The model's stability is enabled by a novel Gated Residual Fusion
(GRF) mechanism, which adaptively moderates feature updates to prevent error
accumulation over long recursive simulations. Our results demonstrate that
AtmosMJ produces stable and physically plausible forecasts for about 500 days.
In quantitative evaluations, it achieves competitive 10-day forecast accuracy
against models like Pangu-Weather and GraphCast, all while requiring a
remarkably low training budget of 5.7 days on a V100 GPU. Our findings suggest
that efficient architectural design, rather than non-standard data
representation, can be the key to unlocking stable and computationally
efficient long-range weather prediction.

</details>


### [269] [Towards Multi-modal Graph Large Language Model](https://arxiv.org/abs/2506.09738)
*Xin Wang,Zeyang Zhang,Linxin Xiao,Haibo Chen,Chendi Ge,Wenwu Zhu*

Main category: cs.LG

TL;DR: The paper explores Multi-modal Graph Large Language Models (MG-LLM), aiming to unify and generalize multi-modal graph data and tasks.


<details>
  <summary>Details</summary>
Motivation: Existing multi-modal graph learning methods are limited in generalization, being designed for specific data and tasks. The paper aims to overcome this limitation.

Method: The authors propose a unified framework that incorporates multi-granularity and multi-scale characteristics, identifying five desired characteristics for MG-LLM.

Result: The study elucidates key challenges, reviews related work, and identifies future directions while summarizing relevant multi-modal graph datasets.

Conclusion: The paper provides a foundational perspective to advance MG-LLM research for generalizing across diverse multi-modal graph data and tasks.

Abstract: Multi-modal graphs, which integrate diverse multi-modal features and
relations, are ubiquitous in real-world applications. However, existing
multi-modal graph learning methods are typically trained from scratch for
specific graph data and tasks, failing to generalize across various multi-modal
graph data and tasks. To bridge this gap, we explore the potential of
Multi-modal Graph Large Language Models (MG-LLM) to unify and generalize across
diverse multi-modal graph data and tasks. We propose a unified framework of
multi-modal graph data, task, and model, discovering the inherent
multi-granularity and multi-scale characteristics in multi-modal graphs.
Specifically, we present five key desired characteristics for MG-LLM: 1)
unified space for multi-modal structures and attributes, 2) capability of
handling diverse multi-modal graph tasks, 3) multi-modal graph in-context
learning, 4) multi-modal graph interaction with natural language, and 5)
multi-modal graph reasoning. We then elaborate on the key challenges, review
related works, and highlight promising future research directions towards
realizing these ambitious characteristics. Finally, we summarize existing
multi-modal graph datasets pertinent for model training. We believe this paper
can contribute to the ongoing advancement of the research towards MG-LLM for
generalization across multi-modal graph data and tasks.

</details>


### [270] [Feature Engineering for Agents: An Adaptive Cognitive Architecture for Interpretable ML Monitoring](https://arxiv.org/abs/2506.09742)
*Gusseppe Bravo-Rocca,Peini Liu,Jordi Guitart,Rodrigo M Carrillo-Larco,Ajay Dholakia,David Ellison*

Main category: cs.LG

TL;DR: The paper proposes a cognitive architecture for monitoring ML models using feature-engineering principles and LLMs to produce interpretable and actionable insights.


<details>
  <summary>Details</summary>
Motivation: Current ML model monitoring approaches produce verbose and hard-to-interpret outputs, which hinder decision-making.

Method: The authors created a Decision Procedure module employing steps like Refactor, Break Down, and Compile to improve data representation, extract detailed insights, and integrate these into interpretative results with reduced reliance on LLMs' inconsistencies.

Result: The approach, tested on multiple LLMs, demonstrated significant accuracy improvements over several baselines across domains.

Conclusion: By combining deterministic feature-engineering and selective LLM usage, the proposed system delivers robust and highly interpretable decision support for monitoring ML models.

Abstract: Monitoring Machine Learning (ML) models in production environments is
crucial, yet traditional approaches often yield verbose, low-interpretability
outputs that hinder effective decision-making. We propose a cognitive
architecture for ML monitoring that applies feature engineering principles to
agents based on Large Language Models (LLMs), significantly enhancing the
interpretability of monitoring outputs. Central to our approach is a Decision
Procedure module that simulates feature engineering through three key steps:
Refactor, Break Down, and Compile. The Refactor step improves data
representation to better capture feature semantics, allowing the LLM to focus
on salient aspects of the monitoring data while reducing noise and irrelevant
information. Break Down decomposes complex information for detailed analysis,
and Compile integrates sub-insights into clear, interpretable outputs. This
process leads to a more deterministic planning approach, reducing dependence on
LLM-generated planning, which can sometimes be inconsistent and overly general.
The combination of feature engineering-driven planning and selective LLM
utilization results in a robust decision support system, capable of providing
highly interpretable and actionable insights. Experiments using multiple LLMs
demonstrate the efficacy of our approach, achieving significantly higher
accuracy compared to various baselines across several domains.

</details>


### [271] [Load-Aware Training Scheduling for Model Circulation-based Decentralized Federated Learning](https://arxiv.org/abs/2506.09769)
*Haruki Kainuma,Takayuki Nishio*

Main category: cs.LG

TL;DR: The paper presents Load-aware Tram-FL, a method to minimize training time in decentralized federated learning by considering both computation and communication loads.


<details>
  <summary>Details</summary>
Motivation: Developing a decentralized federated learning approach that efficiently reduces training latency under variable computational and communication loads and non-IID data distributions.

Method: Proposes a scheduling mechanism decomposed into node-wise subproblems to address a global optimization problem while incorporating a variance constraint for balanced data utilization.

Result: Simulation experiments on MNIST and CIFAR-10 show reduced training time and faster convergence compared to existing methods.

Conclusion: Load-aware Tram-FL effectively improves training efficiency in decentralized federated learning scenarios by optimizing workload distribution and minimizing latency.

Abstract: This paper proposes Load-aware Tram-FL, an extension of Tram-FL that
introduces a training scheduling mechanism to minimize total training time in
decentralized federated learning by accounting for both computational and
communication loads. The scheduling problem is formulated as a global
optimization task, which-though intractable in its original form-is made
solvable by decomposing it into node-wise subproblems. To promote balanced data
utilization under non-IID distributions, a variance constraint is introduced,
while the overall training latency, including both computation and
communication costs, is minimized through the objective function. Simulation
results on MNIST and CIFAR-10 demonstrate that Load-aware Tram-FL significantly
reduces training time and accelerates convergence compared to baseline methods.

</details>


### [272] [A theoretical framework for self-supervised contrastive learning for continuous dependent data](https://arxiv.org/abs/2506.09785)
*Alexander Marusov,Alexander Yuhay,Alexey Zaytsev*

Main category: cs.LG

TL;DR: The paper introduces a contrastive self-supervised learning (SSL) framework for continuous dependent data, outperforming existing methods like TS2Vec on several benchmarks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of existing SSL techniques in handling dependent data, such as temporal and spatio-temporal correlations, which are ignored by traditional contrastive methods assuming semantic independence.

Method: The proposed framework introduces dependency-aware loss functions by defining hard and soft ground truth similarity measures. An analytical similarity matrix is derived to explicitly account for sample dependencies, enabling tailored SSL for dependent data.

Result: The proposed approach, Dependent TS2Vec, achieves superior results on temporal and spatio-temporal benchmarks, including a 4.17% and 2.08% accuracy improvement on UEA and UCR benchmarks, respectively, and a 7% ROC-AUC improvement on drought classification tasks.

Conclusion: The paper demonstrates the effectiveness of dependency-aware SSL methods in capturing spatio-temporal patterns, successfully outperforming existing approaches while introducing theoretical advancements for dependent data.

Abstract: Self-supervised learning (SSL) has emerged as a powerful approach to learning
representations, particularly in the field of computer vision. However, its
application to dependent data, such as temporal and spatio-temporal domains,
remains underexplored. Besides, traditional contrastive SSL methods often
assume \emph{semantic independence between samples}, which does not hold for
dependent data exhibiting complex correlations. We propose a novel theoretical
framework for contrastive SSL tailored to \emph{continuous dependent data},
which allows the nearest samples to be semantically close to each other. In
particular, we propose two possible \textit{ground truth similarity measures}
between objects -- \emph{hard} and \emph{soft} closeness. Under it, we derive
an analytical form for the \textit{estimated similarity matrix} that
accommodates both types of closeness between samples, thereby introducing
dependency-aware loss functions. We validate our approach, \emph{Dependent
TS2Vec}, on temporal and spatio-temporal downstream problems. Given the
dependency patterns presented in the data, our approach surpasses modern ones
for dependent data, highlighting the effectiveness of our theoretically
grounded loss functions for SSL in capturing spatio-temporal dependencies.
Specifically, we outperform TS2Vec on the standard UEA and UCR benchmarks, with
accuracy improvements of $4.17$\% and $2.08$\%, respectively. Furthermore, on
the drought classification task, which involves complex spatio-temporal
patterns, our method achieves a $7$\% higher ROC-AUC score.

</details>


### [273] [Devil's Hand: Data Poisoning Attacks to Locally Private Graph Learning Protocols](https://arxiv.org/abs/2506.09803)
*Longzhu He,Chaozhuo Li,Peng Tang,Litian Zhang,Sen Su*

Main category: cs.LG

TL;DR: The study highlights critical privacy and security gaps in locally private graph learning protocols, particularly their vulnerability to data poisoning attacks, and emphasizes the need for improved defense strategies.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address privacy concerns in graph neural networks due to the sensitive personal information present in real-world graphs. Previous research overlooked the threat posed by data poisoning attacks in locally private graph learning protocols.

Method: This study introduces a novel data poisoning attack. The attacker injects fake users, establishes links with genuine users, and sends manipulated data to the graph learning protocol, compromising its utility. Both theoretical and empirical demonstrations showcase the effectiveness of this attack.

Result: The data poisoning attack was proven to significantly degrade the effectiveness (e.g., node classification accuracy) of locally private graph learning protocols. Explored defense strategies exhibited limited effectiveness.

Conclusion: Locally private graph learning protocols are vulnerable to data poisoning attacks, and current defense methods are insufficient. There's an urgent need for stronger, more robust defenses to ensure their utility and security.

Abstract: Graph neural networks (GNNs) have achieved significant success in graph
representation learning and have been applied to various domains. However, many
real-world graphs contain sensitive personal information, such as user profiles
in social networks, raising serious privacy concerns when graph learning is
performed using GNNs. To address this issue, locally private graph learning
protocols have gained considerable attention. These protocols leverage the
privacy advantages of local differential privacy (LDP) and the effectiveness of
GNN's message-passing in calibrating noisy data, offering strict privacy
guarantees for users' local data while maintaining high utility (e.g., node
classification accuracy) for graph learning. Despite these advantages, such
protocols may be vulnerable to data poisoning attacks, a threat that has not
been considered in previous research. Identifying and addressing these threats
is crucial for ensuring the robustness and security of privacy-preserving graph
learning frameworks. This work introduces the first data poisoning attack
targeting locally private graph learning protocols. The attacker injects fake
users into the protocol, manipulates these fake users to establish links with
genuine users, and sends carefully crafted data to the server, ultimately
compromising the utility of private graph learning. The effectiveness of the
attack is demonstrated both theoretically and empirically. In addition, several
defense strategies have also been explored, but their limited effectiveness
highlights the need for more robust defenses.

</details>


### [274] [Generalizing Supervised Contrastive learning: A Projection Perspective](https://arxiv.org/abs/2506.09810)
*Minoh Jeong,Alfred Hero*

Main category: cs.LG

TL;DR: This paper explores supervised contrastive learning (SupCon) and introduces ProjNCE to unify supervised and self-supervised objectives, providing flexibility in projection design and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Supervised contrastive approaches have not been studied extensively regarding their relationship with mutual information, presenting a gap in understanding.

Method: The authors propose ProjNCE, which adjusts projection functions and negative pair terms, making it a valid mutual information bound. They design and test varied projection strategies.

Result: ProjNCE consistently surpasses SupCon and standard cross-entropy in experiments across diverse datasets and setups.

Conclusion: The development of ProjNCE improves SupCon significantly, offering enhanced mutual information interpretation and projection design flexibility for a broad range of applications.

Abstract: Self-supervised contrastive learning (SSCL) has emerged as a powerful
paradigm for representation learning and has been studied from multiple
perspectives, including mutual information and geometric viewpoints. However,
supervised contrastive (SupCon) approaches have received comparatively little
attention in this context: for instance, while InfoNCE used in SSCL is known to
form a lower bound on mutual information (MI), the relationship between SupCon
and MI remains unexplored. To address this gap, we introduce ProjNCE, a
generalization of the InfoNCE loss that unifies supervised and self-supervised
contrastive objectives by incorporating projection functions and an adjustment
term for negative pairs. We prove that ProjNCE constitutes a valid MI bound and
affords greater flexibility in selecting projection strategies for class
embeddings. Building on this flexibility, we further explore the centroid-based
class embeddings in SupCon by exploring a variety of projection methods.
Extensive experiments on multiple datasets and settings demonstrate that
ProjNCE consistently outperforms both SupCon and standard cross-entropy
training. Our work thus refines SupCon along two complementary
perspective--mutual information interpretation and projection design--and
offers broadly applicable improvements whenever SupCon serves as the
foundational contrastive objective.

</details>


### [275] [Metritocracy: Representative Metrics for Lite Benchmarks](https://arxiv.org/abs/2506.09813)
*Ariel Procaccia,Benjamin Schiffer,Serena Wang,Shirley Zhang*

Main category: cs.LG

TL;DR: The paper addresses metric subset selection for evaluations using social choice theory, introducing notions of representational fairness.


<details>
  <summary>Details</summary>
Motivation: Subset selection of evaluation metrics is often arbitrary and lacks a formal definition of representation, leading to inefficiencies and potential biases.

Method: Two formal properties of metric representation, positional representation and positional proportionality, are introduced and analyzed using theoretical bounds.

Result: The authors provide theoretical results on upper and lower bounds, as well as extended properties for additional input. Real-world applications in LLM and hospital evaluations are explored.

Conclusion: The study offers novel formalizations of representation in metric selection, bridging theory and practice to improve evaluation frameworks.

Abstract: A common problem in LLM evaluation is how to choose a subset of metrics from
a full suite of possible metrics. Subset selection is usually done for
efficiency or interpretability reasons, and the goal is often to select a
``representative'' subset of metrics. However, ``representative'' is rarely
clearly defined. In this work, we use ideas from social choice theory to
formalize two notions of representation for the selection of a subset of
evaluation metrics. We first introduce positional representation, which
guarantees every alternative is sufficiently represented at every position
cutoff. We then introduce positional proportionality, which guarantees no
alternative is proportionally over- or under-represented by more than a small
error at any position. We prove upper and lower bounds on the smallest number
of metrics needed to guarantee either of these properties in the worst case. We
also study a generalized form of each property that allows for additional input
on groups of metrics that must be represented. Finally, we tie theory to
practice through real-world case studies on both LLM evaluation and hospital
quality evaluation.

</details>


### [276] [Identifiability Challenges in Sparse Linear Ordinary Differential Equations](https://arxiv.org/abs/2506.09816)
*Cecilia Casolo,Sören Becker,Niki Kilbertus*

Main category: cs.LG

TL;DR: Sparse linear ODE systems can be unidentifiable, unlike dense systems that are mostly identifiable from data.


<details>
  <summary>Details</summary>
Motivation: The importance of modeling dynamical systems for scientific inquiry and the need for understanding identifiability in sparse regimes, which are often overlooked.

Method: Characterization of sparse linear ODE identifiability, derivation of lower bounds for unidentifiability probability, and empirical assessment using state-of-the-art methods.

Result: Sparse systems are theoretically and practically unidentifiable with positive probability in relevant regimes, and this limitation isn't resolved by optimization or inductive biases.

Conclusion: A rethinking of expectations from data-driven dynamical system modeling is necessary, especially for sparse setups.

Abstract: Dynamical systems modeling is a core pillar of scientific inquiry across
natural and life sciences. Increasingly, dynamical system models are learned
from data, rendering identifiability a paramount concept. For systems that are
not identifiable from data, no guarantees can be given about their behavior
under new conditions and inputs, or about possible control mechanisms to steer
the system. It is known in the community that "linear ordinary differential
equations (ODE) are almost surely identifiable from a single trajectory."
However, this only holds for dense matrices. The sparse regime remains
underexplored, despite its practical relevance with sparsity arising naturally
in many biological, social, and physical systems. In this work, we address this
gap by characterizing the identifiability of sparse linear ODEs. Contrary to
the dense case, we show that sparse systems are unidentifiable with a positive
probability in practically relevant sparsity regimes and provide lower bounds
for this probability. We further study empirically how this theoretical
unidentifiability manifests in state-of-the-art methods to estimate linear ODEs
from data. Our results corroborate that sparse systems are also practically
unidentifiable. Theoretical limitations are not resolved through inductive
biases or optimization dynamics. Our findings call for rethinking what can be
expected from data-driven dynamical system modeling and allows for quantitative
assessments of how much to trust a learned linear ODE.

</details>


### [277] [Weighted Loss Methods for Robust Federated Learning under Data Heterogeneity](https://arxiv.org/abs/2506.09824)
*Johan Erbani,Sonia Ben Mokhtar,Pierre-Edouard Portier,Elod Egyed-Zsigmond,Diana Nurbakova*

Main category: cs.LG

TL;DR: The paper addresses the issue of Byzantine attacks in federated learning by introducing a new method, Worker Label Alignment Loss (WoLA), to better distinguish honest gradients from malicious ones in heterogeneous settings, improving model robustness.


<details>
  <summary>Details</summary>
Motivation: Federated learning faces security threats from Byzantine participants who can contribute malicious gradients, affecting model convergence, especially in heterogeneous data scenarios.

Method: The study introduces Worker Label Alignment Loss (WoLA), a weighted loss mechanism designed to align honest worker gradients, enabling better segregation of honest and Byzantine participants.

Result: WoLA significantly outperforms existing methods in heterogeneous federated learning environments, as demonstrated through theoretical insights and empirical evidence.

Conclusion: WoLA enhances the robustness of federated learning against Byzantine attacks, particularly in settings with significant data heterogeneity, making it a promising technique for secure and effective model training.

Abstract: Federated learning (FL) is a machine learning paradigm that enables multiple
data holders to collaboratively train a machine learning model without sharing
their training data with external parties. In this paradigm, workers locally
update a model and share with a central server their updated gradients (or
model parameters). While FL seems appealing from a privacy perspective, it
opens a number of threats from a security perspective as (Byzantine)
participants can contribute poisonous gradients (or model parameters) harming
model convergence. Byzantine-resilient FL addresses this issue by ensuring that
the training proceeds as if Byzantine participants were absent. Towards this
purpose, common strategies ignore outlier gradients during model aggregation,
assuming that Byzantine gradients deviate more from honest gradients than
honest gradients do from each other. However, in heterogeneous settings, honest
gradients may differ significantly, making it difficult to distinguish honest
outliers from Byzantine ones. In this paper, we introduce the Worker Label
Alignement Loss (WoLA), a weighted loss that aligns honest worker gradients
despite data heterogeneity, which facilitates the identification of Byzantines'
gradients. This approach significantly outperforms state-of-the-art methods in
heterogeneous settings. In this paper, we provide both theoretical insights and
empirical evidence of its effectiveness.

</details>


### [278] [Guided Graph Compression for Quantum Graph Neural Networks](https://arxiv.org/abs/2506.09862)
*Mikel Casals,Vasilis Belis,Elias F. Combarro,Eduard Alarcón,Sofia Vallecorsa,Michele Grossi*

Main category: cs.LG

TL;DR: The paper introduces the Guided Graph Compression (GGC) framework for reducing graph size and feature dimensionality to enable Quantum Graph Neural Networks (QGNNs) and classical classifiers to handle large graphs efficiently.


<details>
  <summary>Details</summary>
Motivation: Existing Quantum Graph Neural Networks (QGNNs) are hampered by quantum hardware limitations and inefficient handling of large graphs, prompting the need for advanced graph compression techniques.

Method: The GGC framework employs a graph autoencoder to compress graphs by reducing nodes and feature dimensions. It optimizes compression to improve downstream classification tasks for both classical and quantum classifiers.

Result: The GGC approach outperformed standalone autoencoders and baseline classical GNN classifiers in classifying particle jets in high energy physics, demonstrating its superior performance.

Conclusion: GGC enables efficient handling of large, realistic graph datasets for both classical and quantum classifiers, advancing the state of QGNN research and practical applications.

Abstract: Graph Neural Networks (GNNs) are effective for processing graph-structured
data but face challenges with large graphs due to high memory requirements and
inefficient sparse matrix operations on GPUs. Quantum Computing (QC) offers a
promising avenue to address these issues and inspires new algorithmic
approaches. In particular, Quantum Graph Neural Networks (QGNNs) have been
explored in recent literature. However, current quantum hardware limits the
dimension of the data that can be effectively encoded. Existing approaches
either simplify datasets manually or use artificial graph datasets. This work
introduces the Guided Graph Compression (GGC) framework, which uses a graph
autoencoder to reduce both the number of nodes and the dimensionality of node
features. The compression is guided to enhance the performance of a downstream
classification task, which can be applied either with a quantum or a classical
classifier. The framework is evaluated on the Jet Tagging task, a
classification problem of fundamental importance in high energy physics that
involves distinguishing particle jets initiated by quarks from those by gluons.
The GGC is compared against using the autoencoder as a standalone preprocessing
step and against a baseline classical GNN classifier. Our numerical results
demonstrate that GGC outperforms both alternatives, while also facilitating the
testing of novel QGNN ansatzes on realistic datasets.

</details>


### [279] [Machine Learning-Based Classification of Oils Using Dielectric Properties and Microwave Resonant Sensing](https://arxiv.org/abs/2506.09867)
*Amit Baran Dey,Wasim Arif,Rakhesh Singh Kshetrimayum*

Main category: cs.LG

TL;DR: A machine learning-powered system accurately classifies oil types based on microwave resonant sensor data, achieving a 99.41% accuracy.


<details>
  <summary>Details</summary>
Motivation: The study aims to develop a non-destructive, real-time method for identifying oil types in industrial applications.

Method: A microwave resonant sensor collects dielectric property changes of oils. The data is processed to extract features used in training machine learning classifiers, including random forests.

Result: The system successfully classifies oil samples with 99.41% accuracy using the random forest classifier.

Conclusion: This approach shows strong potential for efficient and reliable oil characterization in industrial settings, making it practical and highly applicable in real-world scenarios.

Abstract: This paper proposes a machine learning-based methodology for the
classification of various oil samples based on their dielectric properties,
utilizing a microwave resonant sensor. The dielectric behaviour of oils,
governed by their molecular composition, induces distinct shifts in the
sensor's resonant frequency and amplitude response. These variations are
systematically captured and processed to extract salient features, which serve
as inputs for multiple machine learning classifiers. The microwave resonant
sensor operates in a non-destructive, low-power manner, making it particularly
well-suited for real-time industrial applications. A comprehensive dataset is
developed by varying the permittivity of oil samples and acquiring the
corresponding sensor responses. Several classifiers are trained and evaluated
using the extracted resonant features to assess their capability in
distinguishing between oil types. Experimental results demonstrate that the
proposed approach achieves a high classification accuracy of 99.41% with the
random forest classifier, highlighting its strong potential for automated oil
identification. The system's compact form factor, efficiency, and high
performance underscore its viability for fast and reliable oil characterization
in industrial environments.

</details>


### [280] [Causal Climate Emulation with Bayesian Filtering](https://arxiv.org/abs/2506.09891)
*Sebastian Hickman,Ilija Trajkovic,Julia Kaltenborn,Francis Pelletier,Alex Archibald,Yaniv Gurwicz,Peer Nowack,David Rolnick,Julien Boussard*

Main category: cs.LG

TL;DR: The paper develops an interpretable machine learning emulator for climate models to efficiently replicate accurate climate dynamics using causal representation learning.


<details>
  <summary>Details</summary>
Motivation: Current climate models are computationally expensive and lack the capability of integrating physics-informed causal relationships, which limits their utility in predicting and analyzing climate change.

Method: The paper introduces a machine learning emulator using causal representation learning, including a physics-informed approach and a Bayesian filter, for long-term and stable autoregressive emulation.

Result: The emulator accurately learns climate dynamics and demonstrates the significance of its components on both synthetic and real-world climate models.

Conclusion: The proposed emulator overcomes limitations of traditional models by offering interpretable, efficient, and physics-informed replication of climate dynamics, enabling better analyses and predictions of climate change.

Abstract: Traditional models of climate change use complex systems of coupled equations
to simulate physical processes across the Earth system. These simulations are
highly computationally expensive, limiting our predictions of climate change
and analyses of its causes and effects. Machine learning has the potential to
quickly emulate data from climate models, but current approaches are not able
to incorporate physics-informed causal relationships. Here, we develop an
interpretable climate model emulator based on causal representation learning.
We derive a physics-informed approach including a Bayesian filter for stable
long-term autoregressive emulation. We demonstrate that our emulator learns
accurate climate dynamics, and we show the importance of each one of its
components on a realistic synthetic dataset and data from two widely deployed
climate models.

</details>


### [281] [A look at adversarial attacks on radio waveforms from discrete latent space](https://arxiv.org/abs/2506.09896)
*Attanasia Garuso,Silvija Kokalj-Filipovic,Yagna Kaasaragadda*

Main category: cs.LG

TL;DR: The paper explores how a VQ-VAE model can suppress the effectiveness of adversarial attacks on high-SNR radio-frequency waveforms, preserving classification accuracy and detecting attacks through latent space properties.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the vulnerability of RF data classifiers to adversarial attacks, which can significantly degrade their performance, especially in high-SNR settings.

Method: The authors designed a VQ-VAE to map RF waveforms into discrete latent space and created adversarial attacks targeting amplitude modulations while comparing attacks with and without phase preservation. They then analyzed the classifier accuracy and latent-space properties of the adversarial reconstructions.

Result: The VQ-VAE significantly reduced the impact of adversarial attacks, as evidenced by the restored classification performance on reconstructed data and the clear distinctions in latent space distributions under attack and non-attack conditions.

Conclusion: The study demonstrates that VQ-VAE suppresses adversarial attack effectiveness, highlighting its potential for robust RF data processing and attack detection.

Abstract: Having designed a VQVAE that maps digital radio waveforms into discrete
latent space, and yields a perfectly classifiable reconstruction of the
original data, we here analyze the attack suppressing properties of VQVAE when
an adversarial attack is performed on high-SNR radio-frequency (RF)
data-points. To target amplitude modulations from a subset of digitally
modulated waveform classes, we first create adversarial attacks that preserve
the phase between the in-phase and quadrature component whose values are
adversarially changed. We compare them with adversarial attacks of the same
intensity where phase is not preserved. We test the classification accuracy of
such adversarial examples on a classifier trained to deliver 100% accuracy on
the original data. To assess the ability of VQVAE to suppress the strength of
the attack, we evaluate the classifier accuracy on the reconstructions by VQVAE
of the adversarial datapoints and show that VQVAE substantially decreases the
effectiveness of the attack. We also compare the I/Q plane diagram of the
attacked data, their reconstructions and the original data. Finally, using
multiple methods and metrics, we compare the probability distribution of the
VQVAE latent space with and without attack. Varying the attack strength, we
observe interesting properties of the discrete space, which may help detect the
attacks.

</details>


### [282] ["What are my options?": Explaining RL Agents with Diverse Near-Optimal Alternatives (Extended)](https://arxiv.org/abs/2506.09901)
*Noel Brindise,Vijeth Hebbar,Riya Shah,Cedric Langbort*

Main category: cs.LG

TL;DR: The paper introduces Diverse Near-Optimal Alternatives (DNA), an approach in reinforcement learning that provides diverse trajectory options for explainability and adaptive planning.


<details>
  <summary>Details</summary>
Motivation: To enhance explainability in reinforcement learning by offering users meaningful trajectory options for understanding an agent's decision-making process.

Method: DNA uses reward shaping within local, modified Q-learning problems to generate distinct policies with epsilon-optimality, enabling the creation of diverse yet valid trajectory options for agents.

Result: The approach generates qualitatively different policies while ensuring they are viable options for agent planning and decision-making in simulation.

Conclusion: DNA contributes to reinforcement learning by improving explainability, enriching adaptive planning, and fostering exploration in new trajectory options with guaranteed near-optimality.

Abstract: In this work, we provide an extended discussion of a new approach to
explainable Reinforcement Learning called Diverse Near-Optimal Alternatives
(DNA), first proposed at L4DC 2025. DNA seeks a set of reasonable "options" for
trajectory-planning agents, optimizing policies to produce qualitatively
diverse trajectories in Euclidean space. In the spirit of explainability, these
distinct policies are used to "explain" an agent's options in terms of
available trajectory shapes from which a human user may choose. In particular,
DNA applies to value function-based policies on Markov decision processes where
agents are limited to continuous trajectories. Here, we describe DNA, which
uses reward shaping in local, modified Q-learning problems to solve for
distinct policies with guaranteed epsilon-optimality. We show that it
successfully returns qualitatively different policies that constitute
meaningfully different "options" in simulation, including a brief comparison to
related approaches in the stochastic optimization field of Quality Diversity.
Beyond the explanatory motivation, this work opens new possibilities for
exploration and adaptive planning in RL.

</details>


### [283] [Flipping Against All Odds: Reducing LLM Coin Flip Bias via Verbalized Rejection Sampling](https://arxiv.org/abs/2506.09998)
*Tim Z. Xiao,Johannes Zenn,Zhen Liu,Weiyang Liu,Robert Bamler,Bernhard Schölkopf*

Main category: cs.LG

TL;DR: The study proposes Verbalized Rejection Sampling (VRS) to improve large language models' ability to generate unbiased distributions, focusing on Bernoulli distributions.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) excel at describing distributions but struggle to generate unbiased samples, which limits their application in tasks requiring stochasticity.

Method: The authors adapt classical rejection sampling principles into a natural language framework, prompting LLMs to reason and accept or reject proposed samples.

Result: VRS reduces sampling bias across LLMs with theoretical and empirical evidence, demonstrating improvements compared to direct sampling approaches.

Conclusion: Verbalized classical probabilistic tools like VRS enhance sampling reliability in LLMs' workflows without needing internal model modifications or extensive prompt engineering.

Abstract: Large language models (LLMs) can often accurately describe probability
distributions using natural language, yet they still struggle to generate
faithful samples from them. This mismatch limits their use in tasks requiring
reliable stochasticity, such as Monte Carlo methods, agent-based simulations,
and randomized decision-making. We investigate this gap between knowledge and
sampling in the context of Bernoulli distributions. We introduce Verbalized
Rejection Sampling (VRS), a natural-language adaptation of classical rejection
sampling that prompts the LLM to reason about and accept or reject proposed
samples. Despite relying on the same Bernoulli mechanism internally, VRS
substantially reduces sampling bias across models. We provide theoretical
analysis showing that, under mild assumptions, VRS improves over direct
sampling, with gains attributable to both the algorithm and prompt design. More
broadly, our results show how classical probabilistic tools can be verbalized
and embedded into LLM workflows to improve reliability, without requiring
access to model internals or heavy prompt engineering.

</details>


### [284] [Apollo: A Posteriori Label-Only Membership Inference Attack Towards Machine Unlearning](https://arxiv.org/abs/2506.09923)
*Liou Tang,James Joshi,Ashish Kundu*

Main category: cs.LG

TL;DR: Apollo introduces a new privacy attack that identifies 'unlearned' data based on limited access to an ML model, needing only label-output data, achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the gaps in security surrounding Machine Unlearning, where traditional attacks assume unrealistic levels of model access, weakening their real-world applicability.

Method: The paper develops Apollo, a privacy inference attack focusing solely on label-output data from ML models, overcoming limitations of existing approaches demanding both original and unlearned model access.

Result: Apollo demonstrates promise in accurately inferring the membership status of unlearned samples, offering a precise attack with stricter access constraints.

Conclusion: Apollo highlights security vulnerabilities in current MU practices, emphasizing the need for stronger defenses even under restricted access by attackers.

Abstract: Machine Unlearning (MU) aims to update Machine Learning (ML) models following
requests to remove training samples and their influences on a trained model
efficiently without retraining the original ML model from scratch. While MU
itself has been employed to provide privacy protection and regulatory
compliance, it can also increase the attack surface of the model. Existing
privacy inference attacks towards MU that aim to infer properties of the
unlearned set rely on the weaker threat model that assumes the attacker has
access to both the unlearned model and the original model, limiting their
feasibility toward real-life scenarios. We propose a novel privacy attack, A
Posteriori Label-Only Membership Inference Attack towards MU, Apollo, that
infers whether a data sample has been unlearned, following a strict threat
model where an adversary has access to the label-output of the unlearned model
only. We demonstrate that our proposed attack, while requiring less access to
the target model compared to previous attacks, can achieve relatively high
precision on the membership status of the unlearned samples.

</details>


### [285] [Canonical Latent Representations in Conditional Diffusion Models](https://arxiv.org/abs/2506.09955)
*Yitao Xu,Tong Zhang,Ehsan Pajouheshgar,Sabine Süsstrunk*

Main category: cs.LG

TL;DR: This paper introduces Canonical Latent Representations (CLAReps) within Conditional Diffusion Models (CDMs) to enhance downstream discriminative learning. CLAReps retain essential class features while reducing irrelevant details, enabling robust feature distillation with just 10% of training data.


<details>
  <summary>Details</summary>
Motivation: CDMs are highly effective in generative tasks but struggle with disentangling class-relevant features from irrelevant data. This paper seeks to address this gap by proposing a method for robust and interpretable feature extraction.

Method: The authors define CLAReps to isolate class-specific features and develop CaDistill, a feature-distillation method where a student model learns from compact CLAReps generated by a CDM teacher.

Result: The student model trained using CLAReps exhibits improved adversarial robustness and generalization by focusing on class signals instead of background cues.

Conclusion: CDMs, aside from being generative models, can serve as interpretable teachers that facilitate robust representation learning, emphasizing their utility beyond image synthesis.

Abstract: Conditional diffusion models (CDMs) have shown impressive performance across
a range of generative tasks. Their ability to model the full data distribution
has opened new avenues for analysis-by-synthesis in downstream discriminative
learning. However, this same modeling capacity causes CDMs to entangle the
class-defining features with irrelevant context, posing challenges to
extracting robust and interpretable representations. To this end, we identify
Canonical LAtent Representations (CLAReps), latent codes whose internal CDM
features preserve essential categorical information while discarding
non-discriminative signals. When decoded, CLAReps produce representative
samples for each class, offering an interpretable and compact summary of the
core class semantics with minimal irrelevant details. Exploiting CLAReps, we
develop a novel diffusion-based feature-distillation paradigm, CaDistill. While
the student has full access to the training set, the CDM as teacher transfers
core class knowledge only via CLAReps, which amounts to merely 10 % of the
training data in size. After training, the student achieves strong adversarial
robustness and generalization ability, focusing more on the class signals
instead of spurious background cues. Our findings suggest that CDMs can serve
not just as image generators but also as compact, interpretable teachers that
can drive robust representation learning.

</details>


### [286] [Multiverse: Your Language Models Secretly Decide How to Parallelize and Merge Generation](https://arxiv.org/abs/2506.09991)
*Xinyu Yang,Yuwei An,Hongyi Liu,Tianqi Chen,Beidi Chen*

Main category: cs.LG

TL;DR: The paper introduces Multiverse, a generative model enabling parallel generation through a three-stage MapReduce paradigm. It demonstrates superior efficiency and scaling compared to autoregressive models.


<details>
  <summary>Details</summary>
Motivation: Autoregressive Language Models often lack efficient parallelism despite implicit capabilities. The study aims to overcome these limitations with a new model design.

Method: Multiverse applies a MapReduce paradigm: Map for task decomposition, Process for parallel subtask execution, and Reduce for result synthesis. It employs LLM-assisted data curation, structured training data, Multiverse Attention for reasoning separation, and a dedicated parallel inference engine.

Result: After fine-tuning for 3 hours on 1K examples, Multiverse-32B attains performance comparable to leading AR-LLMs of similar scale. It exhibits superior scaling with practical efficiency gains like 2x speedup across batch sizes.

Conclusion: Multiverse demonstrates efficient parallelization capability that enhances performance, scalability, and speed, making it a viable open-sourced alternative to autoregressive models.

Abstract: Autoregressive Large Language Models (AR-LLMs) frequently exhibit implicit
parallelism in sequential generation. Inspired by this, we introduce
Multiverse, a new generative model that enables natively parallel generation.
Multiverse internalizes a MapReduce paradigm, generating automatically through
three stages: (i) a Map stage for adaptive task decomposition, (ii) a Process
stage for parallel subtask execution, and (iii) a Reduce stage for lossless
result synthesis. Next, we build a real-world Multiverse reasoning model with
co-design of data, algorithm, and system, enabling rapid and seamless transfer
from frontier AR-LLMs. Starting from sequential reasoning chains, we create
Multiverse 1K by converting them into structured training data using an
automated LLM-assisted pipeline, avoiding costly human annotations.
Algorithmically, we design Multiverse Attention to separate parallel reasoning
steps while keeping compatibility with causal attention for efficient training.
Systematically, we implement Multiverse Engine to enable parallel inference. It
features a dedicated scheduler that dynamically switches between sequential and
parallel generation, triggered directly by the model. After a 3-hour
fine-tuning with 1K examples, our Multiverse-32B stands as the only
open-sourced non-AR model achieving performance on par with leading AR-LLMs of
the same scale, evidenced by AIME24 & 25 scores of 54% and 46%, respectively.
Moreover, our budget control experiments show that Multiverse-32B exhibits
superior scaling, outperforming AR-LLMs by 1.87% on average using the same
context length. Such scaling further leads to practical efficiency gain,
achieving up to 2x speedup across varying batch sizes. We have open-sourced the
entire Multiverse ecosystem, including data, model weights, engine, supporting
tools, as well as complete data curation prompts and detailed training and
evaluation recipes.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [287] [A Topological Improvement of the Overall Performance of Sparse Evolutionary Training: Motif-Based Structural Optimization of Sparse MLPs Project](https://arxiv.org/abs/2506.09204)
*Xiaotian Chen,Hongyun Liu,Seyed Sahand Mohammadi Ziabari*

Main category: cs.NE

TL;DR: This paper explores how structural optimization using motif-based methods can improve Sparse Evolutionary Training (SET) applied to Multi-layer Perceptrons (MLPs), aiming for efficiency gains without significant accuracy losses.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the growing need for reducing computational costs and memory overheads in complex Deep Neural Networks. It focuses on exploiting sparsity in MLPs as a promising approach to maintain performance while optimizing resource usage.

Method: The study investigates motif-based structural optimization of Sparse Evolutionary Training (SET) applied to MLPs, aiming for efficiency improvements in sparse models.

Result: The research suggests efficiency gains over 40% with a performance decline of under 4% when using motif-based optimization in SET-MLPs.

Conclusion: Structural optimization methods, like motif-based optimization in SET-MLPs, show promising results in enhancing computational efficiency while maintaining acceptable accuracy levels.

Abstract: Deep Neural Networks (DNNs) have been proven to be exceptionally effective
and have been applied across diverse domains within deep learning. However, as
DNN models increase in complexity, the demand for reduced computational costs
and memory overheads has become increasingly urgent. Sparsity has emerged as a
leading approach in this area. The robustness of sparse Multi-layer Perceptrons
(MLPs) for supervised feature selection, along with the application of Sparse
Evolutionary Training (SET), illustrates the feasibility of reducing
computational costs without compromising accuracy. Moreover, it is believed
that the SET algorithm can still be improved through a structural optimization
method called motif-based optimization, with potential efficiency gains
exceeding 40% and a performance decline of under 4%. This research investigates
whether the structural optimization of Sparse Evolutionary Training applied to
Multi-layer Perceptrons (SET-MLP) can enhance performance and to what extent
this improvement can be achieved.

</details>


### [288] [Energy Aware Development of Neuromorphic Implantables: From Metrics to Action](https://arxiv.org/abs/2506.09599)
*Enrique Barba Roque,Luis Cruz*

Main category: cs.NE

TL;DR: This paper reviews energy efficiency metrics for Spiking Neural Networks (SNNs), highlighting limitations and proposing future directions for improvement.


<details>
  <summary>Details</summary>
Motivation: To evaluate and improve energy efficiency in SNN models used in edge and implantable devices.

Method: Classified 13 metrics based on Accessibility, Fidelity, Actionability, and Trend-Based analysis, emphasizing gaps in energy assessment.

Result: Identified a lack of actionable and accessible metrics for guiding energy-efficient SNN development, especially for neuromorphic hardware.

Conclusion: Future research should focus on developing metrics that are accessible, actionable, and trend-based to enhance SNN energy efficiency in practical applications.

Abstract: Spiking Neural Networks (SNNs) and neuromorphic computing present a promising
alternative to traditional Artificial Neural Networks (ANNs) by significantly
improving energy efficiency, particularly in edge and implantable devices.
However, assessing the energy performance of SNN models remains a challenge due
to the lack of standardized and actionable metrics and the difficulty of
measuring energy consumption in experimental neuromorphic hardware. In this
paper, we conduct a preliminary exploratory study of energy efficiency metrics
proposed in the SNN benchmarking literature. We classify 13 commonly used
metrics based on four key properties: Accessibility, Fidelity, Actionability,
and Trend-Based analysis. Our findings indicate that while many existing
metrics provide useful comparisons between architectures, they often lack
practical insights for SNN developers. Notably, we identify a gap between
accessible and high-fidelity metrics, limiting early-stage energy assessment.
Additionally, we emphasize the lack of metrics that provide practitioners with
actionable insights, making it difficult to guide energy-efficient SNN
development. To address these challenges, we outline research directions for
bridging accessibility and fidelity and finding new Actionable metrics for
implantable neuromorphic devices, introducing more Trend-Based metrics, metrics
that reflect changes in power requirements, battery-aware metrics, and
improving energy-performance tradeoff assessments. The results from this paper
pave the way for future research on enhancing energy metrics and their
Actionability for SNNs.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [289] [WD-DETR: Wavelet Denoising-Enhanced Real-Time Object Detection Transformer for Robot Perception with Event Cameras](https://arxiv.org/abs/2506.09098)
*Yangjie Cui,Boyang Gao,Yiwei Zhang,Xin Dong,Jinwu Xiang,Daochun Li,Zhan Tu*

Main category: cs.RO

TL;DR: This paper introduces WD-DETR, a detection network for event cameras that uses wavelet denoising for improved performance and faster inference, addressing the issue of noise in dense event representations.


<details>
  <summary>Details</summary>
Motivation: To overcome the noise challenges in dense event representations from event cameras, which can degrade detection quality and lead to missed detections.

Method: The authors propose a framework called WD-DETR combining dense event representation, wavelet transform for denoising, feature extraction integrated with the backbone, and a transformer-based network for object prediction. Additionally, a Dynamic Reorganization Convolution Block (DRCB) is introduced to optimize fusion and reduce inference time.

Result: The WD-DETR network is tested on three event-based object detection datasets (DSEC, Gen1, and 1Mpx) and achieves superior performance compared to state-of-the-art methods. It also runs at 35 FPS on NVIDIA Jetson Orin NX using TensorRT FP16.

Conclusion: WD-DETR effectively addresses noise and performance issues in event-based detection, offering real-time processing capabilities that make it suitable for onboard robotic systems.

Abstract: Previous studies on event camera sensing have demonstrated certain detection
performance using dense event representations. However, the accumulated noise
in such dense representations has received insufficient attention, which
degrades the representation quality and increases the likelihood of missed
detections. To address this challenge, we propose the Wavelet
Denoising-enhanced DEtection TRansformer, i.e., WD-DETR network, for event
cameras. In particular, a dense event representation is presented first, which
enables real-time reconstruction of events as tensors. Then, a wavelet
transform method is designed to filter noise in the event representations. Such
a method is integrated into the backbone for feature extraction. The extracted
features are subsequently fed into a transformer-based network for object
prediction. To further reduce inference time, we incorporate the Dynamic
Reorganization Convolution Block (DRCB) as a fusion module within the hybrid
encoder. The proposed method has been evaluated on three event-based object
detection datasets, i.e., DSEC, Gen1, and 1Mpx. The results demonstrate that
WD-DETR outperforms tested state-of-the-art methods. Additionally, we implement
our approach on a common onboard computer for robots, the NVIDIA Jetson Orin
NX, achieving a high frame rate of approximately 35 FPS using TensorRT FP16,
which is exceptionally well-suited for real-time perception of onboard robotic
systems.

</details>


### [290] [Hearing the Slide: Acoustic-Guided Constraint Learning for Fast Non-Prehensile Transport](https://arxiv.org/abs/2506.09169)
*Yuemin Mao,Bardienus P. Duisterhof,Moonyoung Lee,Jeffrey Ichnowski*

Main category: cs.RO

TL;DR: The paper introduces a method using acoustic sensing to learn a dynamic friction model for improving non-prehensile object transport tasks, reducing displacement during robot motion.


<details>
  <summary>Details</summary>
Motivation: Transporting objects efficiently and safely in robotic automation is often hindered by inaccuracies in friction modeling, particularly during fast motions causing vibrations.

Method: The researchers use acoustic sensing to dynamically condition a learned friction coefficient, which is incorporated into an optimization-based motion planner for adjusting constraints at each control step.

Result: Experiments with a UR5e robot show the learned model reduces object displacement by up to 86.0%, demonstrating its superiority over the conventional Coulomb friction model.

Conclusion: Acoustic sensing provides a valuable tool for overcoming limitations in standard friction modeling, offering significant improvements in the accuracy and efficiency of robotic object transport.

Abstract: Object transport tasks are fundamental in robotic automation, emphasizing the
importance of efficient and secure methods for moving objects. Non-prehensile
transport can significantly improve transport efficiency, as it enables
handling multiple objects simultaneously and accommodating objects unsuitable
for parallel-jaw or suction grasps. Existing approaches incorporate constraints
based on the Coulomb friction model, which is imprecise during fast motions
where inherent mechanical vibrations occur. Imprecise constraints can cause
transported objects to slide or even fall off the tray. To address this
limitation, we propose a novel method to learn a friction model using acoustic
sensing that maps a tray's motion profile to a dynamically conditioned friction
coefficient. This learned model enables an optimization-based motion planner to
adjust the friction constraint at each control step according to the planned
motion at that step. In experiments, we generate time-optimized trajectories
for a UR5e robot to transport various objects with constraints using both the
standard Coulomb friction model and the learned friction model. Results suggest
that the learned friction model reduces object displacement by up to 86.0%
compared to the baseline, highlighting the effectiveness of acoustic sensing in
learning real-world friction constraints.

</details>


### [291] [Towards Full-Scenario Safety Evaluation of Automated Vehicles: A Volume-Based Method](https://arxiv.org/abs/2506.09182)
*Hang Zhou,Chengyuan Ma,Shiyu Shen,Xiaopeng Li*

Main category: cs.RO

TL;DR: This paper presents a novel framework for evaluating the safety of automated vehicles (AVs) in complex scenarios, overcoming limitations in current methods focused on basic maneuvers and crash rate metrics.


<details>
  <summary>Details</summary>
Motivation: Current safety evaluation methods for AVs are insufficient for assessing high-level automation in complex driving scenarios, relying heavily on crash rate metrics and facing challenges like incomplete data and computational inefficiencies.

Method: The proposed framework introduces a unified model to standardize diverse driving scenarios, reducing dimensionality, and a volume-based evaluation method to quantify risky scenarios instead of probability-based metrics. It includes convexity analysis for car-following scenarios to facilitate exact volume computation.

Result: Experimental validation using both conceptual AV behavior models and real-world production AV models from the Ultra-AV dataset confirms the effectiveness of the volume-based safety evaluation method.

Conclusion: The paper successfully establishes a scalable and rigorous safety evaluation framework for AVs in complex driving scenarios, offering a significant advance over traditional methods. The authors will release code and data upon acceptance.

Abstract: With the rapid development of automated vehicles (AVs) in recent years,
commercially available AVs are increasingly demonstrating high-level automation
capabilities. However, most existing AV safety evaluation methods are primarily
designed for simple maneuvers such as car-following and lane-changing. While
suitable for basic tests, these methods are insufficient for assessing
high-level automation functions deployed in more complex environments. First,
these methods typically use crash rate as the evaluation metric, whose accuracy
heavily depends on the quality and completeness of naturalistic driving
environment data used to estimate scenario probabilities. Such data is often
difficult and expensive to collect. Second, when applied to diverse scenarios,
these methods suffer from the curse of dimensionality, making large-scale
evaluation computationally intractable. To address these challenges, this paper
proposes a novel framework for full-scenario AV safety evaluation. A unified
model is first introduced to standardize the representation of diverse driving
scenarios. This modeling approach constrains the dimension of most scenarios to
a regular highway setting with three lanes and six surrounding background
vehicles, significantly reducing dimensionality. To further avoid the
limitations of probability-based method, we propose a volume-based evaluation
method that quantifies the proportion of risky scenarios within the entire
scenario space. For car-following scenarios, we prove that the set of safe
scenarios is convex under specific settings, enabling exact volume computation.
Experimental results validate the effectiveness of the proposed volume-based
method using both AV behavior models from existing literature and six
production AV models calibrated from field-test trajectory data in the Ultra-AV
dataset. Code and data will be made publicly available upon acceptance of this
paper.

</details>


### [292] [Perception Characteristics Distance: Measuring Stability and Robustness of Perception System in Dynamic Conditions under a Certain Decision Rule](https://arxiv.org/abs/2506.09217)
*Boyu Jiang,Liang Shi,Zhengzhi Lin,Loren Stowe,Feng Guo*

Main category: cs.RO

TL;DR: This paper introduces the Perception Characteristics Distance (PCD) as an innovative metric to measure reliable object detection distance considering uncertainty. It also presents a novel SensorRainFall dataset for benchmarking autonomous driving perception systems under varying weather conditions.


<details>
  <summary>Details</summary>
Motivation: Traditional evaluation metrics for perception systems in autonomous driving fail to account for variability caused by external factors like weather and object distance. There is a need to develop dynamic measures that consider stochastic outputs of AI models.

Method: The authors propose the PCD metric, which quantifies detection reliability inclusive of uncertainty, and introduce the SensorRainFall dataset, featuring precise controlled scenario data. They compute mPCD by averaging PCD values across detection quality thresholds for comprehensive system evaluation.

Result: Statistical analysis identifies change points in detection confidence variance. Using the introduced metrics reveals reliability differences among models under different weather conditions. PCD captures performance insights neglected by traditional static metrics.

Conclusion: PCD improves the evaluation of perception systems by focusing on uncertainty and dynamic detection capacity. The SensorRainFall dataset supports these evaluations, advancing the robustness and safety of autonomous driving systems.

Abstract: The performance of perception systems in autonomous driving systems (ADS) is
strongly influenced by object distance, scene dynamics, and environmental
conditions such as weather. AI-based perception outputs are inherently
stochastic, with variability driven by these external factors, while
traditional evaluation metrics remain static and event-independent, failing to
capture fluctuations in confidence over time. In this work, we introduce the
Perception Characteristics Distance (PCD) -- a novel evaluation metric that
quantifies the farthest distance at which an object can be reliably detected,
incorporating uncertainty in model outputs. To support this, we present the
SensorRainFall dataset, collected on the Virginia Smart Road using a
sensor-equipped vehicle (cameras, radar, LiDAR) under controlled daylight-clear
and daylight-rain scenarios, with precise ground-truth distances to the target
objects. Statistical analysis reveals the presence of change points in the
variance of detection confidence score with distance. By averaging the PCD
values across a range of detection quality thresholds and probabilistic
thresholds, we compute the mean PCD (mPCD), which captures the overall
perception characteristics of a system with respect to detection distance.
Applying state-of-the-art perception models shows that mPCD captures meaningful
reliability differences under varying weather conditions -- differences that
static metrics overlook. PCD provides a principled, distribution-aware measure
of perception performance, supporting safer and more robust ADS operation,
while the SensorRainFall dataset offers a valuable benchmark for evaluation.
The SensorRainFall dataset is publicly available at
https://www.kaggle.com/datasets/datadrivenwheels/sensorrainfall, and the
evaluation code is open-sourced at
https://github.com/datadrivenwheels/PCD_Python.

</details>


### [293] [UAD: Unsupervised Affordance Distillation for Generalization in Robotic Manipulation](https://arxiv.org/abs/2506.09284)
*Yihe Tang,Wenlong Huang,Yingke Wang,Chengshu Li,Roy Yuan,Ruohan Zhang,Jiajun Wu,Li Fei-Fei*

Main category: cs.RO

TL;DR: This paper introduces UAD, an unsupervised method to teach robots object affordances for manipulation tasks without manual annotation, utilizing foundation models.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of visual affordance prediction methods that rely on manual annotation and predefined task sets, enabling robots to adapt to unstructured environments.

Method: UAD distills affordance knowledge by leveraging vision and vision-language foundation models to automatically annotate datasets. It trains a task-conditioned lightweight decoder on frozen features derived from rendered objects in simulation.

Result: UAD demonstrated strong generalization across robotic scenes and human activities, even when trained on simulated data. The affordance space created enabled imitation learning policies to generalize well after minimal training demonstrations.

Conclusion: UAD proves effective for unsupervised learning of object affordances, showing potential for broader applications in robotic manipulation and adaptability in dynamic environments.

Abstract: Understanding fine-grained object affordances is imperative for robots to
manipulate objects in unstructured environments given open-ended task
instructions. However, existing methods of visual affordance predictions often
rely on manually annotated data or conditions only on a predefined set of
tasks. We introduce UAD (Unsupervised Affordance Distillation), a method for
distilling affordance knowledge from foundation models into a task-conditioned
affordance model without any manual annotations. By leveraging the
complementary strengths of large vision models and vision-language models, UAD
automatically annotates a large-scale dataset with detailed $<$instruction,
visual affordance$>$ pairs. Training only a lightweight task-conditioned
decoder atop frozen features, UAD exhibits notable generalization to
in-the-wild robotic scenes and to various human activities, despite only being
trained on rendered objects in simulation. Using affordance provided by UAD as
the observation space, we show an imitation learning policy that demonstrates
promising generalization to unseen object instances, object categories, and
even variations in task instructions after training on as few as 10
demonstrations. Project website: https://unsup-affordance.github.io/

</details>


### [294] [SkillBlender: Towards Versatile Humanoid Whole-Body Loco-Manipulation via Skill Blending](https://arxiv.org/abs/2506.09366)
*Yuxuan Kuang,Haoran Geng,Amine Elhafsi,Tan-Dzung Do,Pieter Abbeel,Jitendra Malik,Marco Pavone,Yue Wang*

Main category: cs.RO

TL;DR: SkillBlender introduces a hierarchical reinforcement learning framework to enhance humanoid loco-manipulation with minimal task-specific tuning, surpassing standard methods.


<details>
  <summary>Details</summary>
Motivation: Current methods for humanoid robots require extensive task-specific tuning to adapt to diverse daily scenarios, which hampers scalability.

Method: The framework pretrains primitive, task-agnostic skills and dynamically blends them to perform complex tasks, utilizing minimal reward engineering.

Result: SkillBlender outperformed baselines in simulated experiments and avoided reward hacking, demonstrating regularized behaviors and improving accuracy across tasks.

Conclusion: The framework and benchmark system advance scalable and versatile humanoid robot control, promising innovations in daily task environments.

Abstract: Humanoid robots hold significant potential in accomplishing daily tasks
across diverse environments thanks to their flexibility and human-like
morphology. Recent works have made significant progress in humanoid whole-body
control and loco-manipulation leveraging optimal control or reinforcement
learning. However, these methods require tedious task-specific tuning for each
task to achieve satisfactory behaviors, limiting their versatility and
scalability to diverse tasks in daily scenarios. To that end, we introduce
SkillBlender, a novel hierarchical reinforcement learning framework for
versatile humanoid loco-manipulation. SkillBlender first pretrains
goal-conditioned task-agnostic primitive skills, and then dynamically blends
these skills to accomplish complex loco-manipulation tasks with minimal
task-specific reward engineering. We also introduce SkillBench, a parallel,
cross-embodiment, and diverse simulated benchmark containing three embodiments,
four primitive skills, and eight challenging loco-manipulation tasks,
accompanied by a set of scientific evaluation metrics balancing accuracy and
feasibility. Extensive simulated experiments show that our method significantly
outperforms all baselines, while naturally regularizing behaviors to avoid
reward hacking, resulting in more accurate and feasible movements for diverse
loco-manipulation tasks in our daily scenarios. Our code and benchmark will be
open-sourced to the community to facilitate future research. Project page:
https://usc-gvl.github.io/SkillBlender-web/.

</details>


### [295] [Bipedal Balance Control with Whole-body Musculoskeletal Standing and Falling Simulations](https://arxiv.org/abs/2506.09383)
*Chengtian Ma,Yunyue Wei,Chenhui Zuo,Chen Zhang,Yanan Sui*

Main category: cs.RO

TL;DR: The paper introduces a hierarchical control system for simulating musculoskeletal dynamics and enhancing balance control in humans, highlighting muscle injury effects as well as the role of hip exoskeletons.


<details>
  <summary>Details</summary>
Motivation: To address limited quantitative understanding of static balance and falling in humans while contributing to advances in humanoid robotics and interventions for balance impairments.

Method: Hierarchical control pipeline simulating whole-body musculoskeletal systems, studying spatiotemporal dynamics, muscle injuries, and exoskeleton effects on perturbation handling.

Result: Identified dynamics during stable standing, muscle injury's impact on balance, clinically relevant fall patterns, and improved balance via hip exoskeleton assistance.

Conclusion: The simulation insights provide a foundation for interventions targeting balance impairments and advancing humanoid robotic balance mechanisms.

Abstract: Balance control is important for human and bipedal robotic systems. While
dynamic balance during locomotion has received considerable attention,
quantitative understanding of static balance and falling remains limited. This
work presents a hierarchical control pipeline for simulating human balance via
a comprehensive whole-body musculoskeletal system. We identified spatiotemporal
dynamics of balancing during stable standing, revealed the impact of muscle
injury on balancing behavior, and generated fall contact patterns that aligned
with clinical data. Furthermore, our simulated hip exoskeleton assistance
demonstrated improvement in balance maintenance and reduced muscle effort under
perturbation. This work offers unique muscle-level insights into human balance
dynamics that are challenging to capture experimentally. It could provide a
foundation for developing targeted interventions for individuals with balance
impairments and support the advancement of humanoid robotic systems.

</details>


### [296] [Analyzing Key Objectives in Human-to-Robot Retargeting for Dexterous Manipulation](https://arxiv.org/abs/2506.09384)
*Chendong Xin,Mingrui Yu,Yongpeng Jiang,Zhefeng Zhang,Xiang Li*

Main category: cs.RO

TL;DR: The paper addresses the challenge of translating human hand movements to robot hands for tasks like manipulation and learning. It proposes a comprehensive retargeting method and conducts comparative experiments to evaluate its effectiveness.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the accuracy and effectiveness of motion transfer from human hands to robot hands, given their mechanical differences, especially for tasks like teleoperation and imitation learning.

Method: The paper integrates factors from recent approaches into a comprehensive kinematic posture retargeting objective and evaluates these factors through ablation studies and real-world experiments.

Result: The study provides significant insights through experimental findings, highlighting the importance of certain retargeting objectives in achieving effective manipulation.

Conclusion: The conclusions suggest methods for designing more accurate retargeting algorithms, enhancing dexterous manipulation in robots through experimental understanding of key objectives.

Abstract: Kinematic retargeting from human hands to robot hands is essential for
transferring dexterity from humans to robots in manipulation teleoperation and
imitation learning. However, due to mechanical differences between human and
robot hands, completely reproducing human motions on robot hands is impossible.
Existing works on retargeting incorporate various optimization objectives,
focusing on different aspects of hand configuration. However, the lack of
experimental comparative studies leaves the significance and effectiveness of
these objectives unclear. This work aims to analyze these retargeting
objectives for dexterous manipulation through extensive real-world comparative
experiments. Specifically, we propose a comprehensive retargeting objective
formulation that integrates intuitively crucial factors appearing in recent
approaches. The significance of each factor is evaluated through experimental
ablation studies on the full objective in kinematic posture retargeting and
real-world teleoperated manipulation tasks. Experimental results and
conclusions provide valuable insights for designing more accurate and effective
retargeting algorithms for real-world dexterous manipulation.

</details>


### [297] [Scoop-and-Toss: Dynamic Object Collection for Quadrupedal Systems](https://arxiv.org/abs/2506.09406)
*Minji Kang,Chanwoo Baek,Yoonsang Lee*

Main category: cs.RO

TL;DR: This paper introduces a framework enabling quadruped robots to dynamically collect objects using their legs, augmented with a simple scoop attachment.


<details>
  <summary>Details</summary>
Motivation: Quadruped robots have advanced in locomotion, but their leg-based manipulation capabilities remain underexplored, particularly for dynamic tasks like object collection.

Method: The authors implement a hierarchical policy structure with two expert policies (scooping/tossing and object approach) and a meta-policy for coordination. These policies are trained separately, enabling robots to scoop objects with a leg-mounted scoop and deposit them in a tray.

Result: The framework successfully allows quadruped robots to dynamically collect and deposit objects, demonstrating agility and effective leg utilization for non-static tasks.

Conclusion: Legs of quadruped robots can be expanded beyond locomotion to dynamic object manipulation with effective hierarchical policy training, opening new avenues for versatile applications.

Abstract: Quadruped robots have made significant advances in locomotion, extending
their capabilities from controlled environments to real-world applications.
Beyond movement, recent work has explored loco-manipulation using the legs to
perform tasks such as pressing buttons or opening doors. While these efforts
demonstrate the feasibility of leg-based manipulation, most have focused on
relatively static tasks. In this work, we propose a framework that enables
quadruped robots to collect objects without additional actuators by leveraging
the agility of their legs. By attaching a simple scoop-like add-on to one leg,
the robot can scoop objects and toss them into a collection tray mounted on its
back. Our method employs a hierarchical policy structure comprising two expert
policies-one for scooping and tossing, and one for approaching object
positions-and a meta-policy that dynamically switches between them. The expert
policies are trained separately, followed by meta-policy training for
coordinated multi-object collection. This approach demonstrates how quadruped
legs can be effectively utilized for dynamic object manipulation, expanding
their role beyond locomotion.

</details>


### [298] [Time-Unified Diffusion Policy with Action Discrimination for Robotic Manipulation](https://arxiv.org/abs/2506.09422)
*Ye Niu,Sanping Zhou,Yizhe Li,Ye Den,Le Wang*

Main category: cs.RO

TL;DR: The paper introduces Time-Unified Diffusion Policy (TUDP) to improve efficiency and accuracy in robotic manipulation by unifying the denoising process of diffusion models.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion policies for robotic manipulation are robust but suffer from inefficiency and complexity in action denoising, hindering real-time responses and model training.

Method: TUDP builds a time-unified velocity field and incorporates an action discrimination branch for action-wise training, streamlining policy learning and enhancing denoising accuracy.

Result: TUDP achieves state-of-the-art performance with success rates of 82.6% in multi-view and 83.8% in single-view setups, showing significant improvements even under fewer denoising iterations.

Conclusion: TUDP enhances real-time robotic manipulation by unifying denoising timesteps and introducing action discrimination, making it suitable for diverse real-world tasks.

Abstract: In many complex scenarios, robotic manipulation relies on generative models
to estimate the distribution of multiple successful actions. As the diffusion
model has better training robustness than other generative models, it performs
well in imitation learning through successful robot demonstrations. However,
the diffusion-based policy methods typically require significant time to
iteratively denoise robot actions, which hinders real-time responses in robotic
manipulation. Moreover, existing diffusion policies model a time-varying action
denoising process, whose temporal complexity increases the difficulty of model
training and leads to suboptimal action accuracy. To generate robot actions
efficiently and accurately, we present the Time-Unified Diffusion Policy
(TUDP), which utilizes action recognition capabilities to build a time-unified
denoising process. On the one hand, we build a time-unified velocity field in
action space with additional action discrimination information. By unifying all
timesteps of action denoising, our velocity field reduces the difficulty of
policy learning and speeds up action generation. On the other hand, we propose
an action-wise training method, which introduces an action discrimination
branch to supply additional action discrimination information. Through
action-wise training, the TUDP implicitly learns the ability to discern
successful actions to better denoising accuracy. Our method achieves
state-of-the-art performance on RLBench with the highest success rate of 82.6%
on a multi-view setup and 83.8% on a single-view setup. In particular, when
using fewer denoising iterations, TUDP achieves a more significant improvement
in success rate. Additionally, TUDP can produce accurate actions for a wide
range of real-world tasks.

</details>


### [299] [Design of an innovative robotic surgical instrument for circular stapling](https://arxiv.org/abs/2506.09444)
*Paul Tucan,Nadim Al Hajjar,Calin Vaida,Alexandru Pusca,Tiberiu Antal,Corina Radu,Daniel Jucan,Adrian Pisla,Damien Chablat,Doina Pisla*

Main category: cs.RO

TL;DR: The paper introduces a robotic circular stapler to improve esophagectomy outcomes, addressing the limitations of traditional manual techniques.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address esophageal cancer surgery challenges, including complications and limitations of traditional circular staplers, to improve patient outcomes.

Method: A robotic circular stapler, integrated with a cognitive robotic assistant, is developed using three actuators for precise tissue manipulation and bending. Kinematic analysis ensures tip synchronization.

Result: The prototype provides enhanced dexterity, improved tissue alignment, and possibly reduced risks during esophagectomy.

Conclusion: The novel robotic stapler could transform surgical interventions by offering higher precision and better patient recovery outcomes.

Abstract: Esophageal cancer remains a highly aggressive malignancy with low survival
rates, requiring advanced surgical interventions like esophagectomy.
Traditional manual techniques, including circular staplers, face challenges
such as limited precision, prolonged recovery times, and complications like
leaks and tissue misalignment. This paper presents a novel robotic circular
stapler designed to enhance the dexterity in confined spaces, improve tissue
alignment, and reduce post-operative risks. Integrated with a cognitive robot
that serves as a surgeon's assistant, the surgical stapler uses three actuators
to perform anvil motion, cutter/stapler motion and allows a 75-degree bending
of the cartridge (distal tip). Kinematic analysis is used to compute the
stapler tip's position, ensuring synchronization with a robotic system.

</details>


### [300] [Adv-BMT: Bidirectional Motion Transformer for Safety-Critical Traffic Scenario Generation](https://arxiv.org/abs/2506.09485)
*Yuxin Liu,Zhenghao Peng,Xuanhao Cui,Bolei Zhou*

Main category: cs.RO

TL;DR: This paper presents the Adv-BMT framework for scenario-based testing of autonomous driving systems, augmenting real-world data with adversarial interactions to improve collision scenario generation.


<details>
  <summary>Details</summary>
Motivation: Current datasets lack sufficient long-tailed, safety-critical scenarios for testing autonomous driving systems, necessitating augmentation techniques.

Method: The proposed Adv-BMT framework uses a bidirectional motion transformer (BMT) for inverse traffic motion predictions, reconstructing traffic in reverse chronological order through a two-stage pipeline: adversarial initialization and inverse predictions.

Result: Adv-BMT can generate diverse and realistic collision interactions without requiring pretraining on collision data, achieving a 20% reduction in episode collision rates compared to previous methods.

Conclusion: Adv-BMT enhances autonomous driving system testing by addressing data scarcity with more realistic scenario augmentation.

Abstract: Scenario-based testing is essential for validating the performance of
autonomous driving (AD) systems. However, such testing is limited by the
scarcity of long-tailed, safety-critical scenarios in existing datasets
collected in the real world. To tackle the data issue, we propose the Adv-BMT
framework, which augments real-world scenarios with diverse and realistic
adversarial interactions. The core component of Adv-BMT is a bidirectional
motion transformer (BMT) model to perform inverse traffic motion predictions,
which takes agent information in the last time step of the scenario as input,
and reconstruct the traffic in the inverse of chronological order until the
initial time step. The Adv-BMT framework is a two-staged pipeline: it first
conducts adversarial initializations and then inverse motion predictions.
Different from previous work, we do not need any collision data for
pretraining, and are able to generate realistic and diverse collision
interactions. Our experimental results validate the quality of generated
collision scenarios by Adv-BMT: training in our augmented dataset would reduce
episode collision rates by 20\% compared to previous work.

</details>


### [301] [DCIRNet: Depth Completion with Iterative Refinement for Dexterous Grasping of Transparent and Reflective Objects](https://arxiv.org/abs/2506.09491)
*Guanghu Xie,Zhiduo Jiang,Yonglong Zhang,Yang Liu,Zongwu Xie,Baoshi Cao,Hong Liu*

Main category: cs.RO

TL;DR: The paper introduces DCIRNet, a multimodal depth completion network aimed at addressing depth estimation challenges in transparent and reflective objects by leveraging RGB images and depth maps.


<details>
  <summary>Details</summary>
Motivation: To improve depth estimation quality for transparent and reflective objects, which are problematic for depth sensors and hinder downstream vision tasks such as object recognition and robotic manipulation.

Method: DCIRNet combines an innovative multimodal feature fusion module and a multi-stage supervision strategy to refine depth completion and improve object boundary quality.

Result: DCIRNet achieves a 44% improvement in grasp success rates for transparent and reflective objects in robotic applications and demonstrates superior performance across public datasets.

Conclusion: DCIRNet successfully enhances depth estimation and confirms strong generalization capabilities, enabling better handling of transparent and reflective objects in everyday environments.

Abstract: Transparent and reflective objects in everyday environments pose significant
challenges for depth sensors due to their unique visual properties, such as
specular reflections and light transmission. These characteristics often lead
to incomplete or inaccurate depth estimation, which severely impacts downstream
geometry-based vision tasks, including object recognition, scene
reconstruction, and robotic manipulation. To address the issue of missing depth
information in transparent and reflective objects, we propose DCIRNet, a novel
multimodal depth completion network that effectively integrates RGB images and
depth maps to enhance depth estimation quality. Our approach incorporates an
innovative multimodal feature fusion module designed to extract complementary
information between RGB images and incomplete depth maps. Furthermore, we
introduce a multi-stage supervision and depth refinement strategy that
progressively improves depth completion and effectively mitigates the issue of
blurred object boundaries. We integrate our depth completion model into
dexterous grasping frameworks and achieve a $44\%$ improvement in the grasp
success rate for transparent and reflective objects. We conduct extensive
experiments on public datasets, where DCIRNet demonstrates superior
performance. The experimental results validate the effectiveness of our
approach and confirm its strong generalization capability across various
transparent and reflective objects.

</details>


### [302] [Advances on Affordable Hardware Platforms for Human Demonstration Acquisition in Agricultural Applications](https://arxiv.org/abs/2506.09494)
*Alberto San-Miguel-Tello,Gennaro Scarati,Alejandro Hernández,Mario Cavero-Vidal,Aakash Maroti,Néstor García*

Main category: cs.RO

TL;DR: This paper improves a hand-held gripper for robot learning in agricultural tasks by using efficient sampling and trajectory generation techniques.


<details>
  <summary>Details</summary>
Motivation: To address challenges in acquiring effective demonstration samples for robots in complex agricultural environments with minimal user burden.

Method: The study reduces user cognitive load by segmenting continuous demonstrations into samples based on task events and enhances trajectory reliability by combining inertial measurements with visual markers using EKF.

Result: The proposed advancements outperform the default pipeline in a fruit harvesting task.

Conclusion: The enhanced hand-held gripper system effectively improves robot learning in complex agricultural scenarios with better sample acquisition and trajectory accuracy.

Abstract: This paper presents advances on the Universal Manipulation Interface (UMI), a
low-cost hand-held gripper for robot Learning from Demonstration (LfD), for
complex in-the-wild scenarios found in agricultural settings. The focus is on
improving the acquisition of suitable samples with minimal additional setup.
Firstly, idle times and user's cognitive load are reduced through the
extraction of individual samples from a continuous demonstration considering
task events. Secondly, reliability on the generation of task sample's
trajectories is increased through the combination on-board inertial
measurements and external visual marker localization usage using Extended
Kalman Filtering (EKF). Results are presented for a fruit harvesting task,
outperforming the default pipeline.

</details>


### [303] [Tightly-Coupled LiDAR-IMU-Leg Odometry with Online Learned Leg Kinematics Incorporating Foot Tactile Information](https://arxiv.org/abs/2506.09548)
*Taku Okawara,Kenji Koide,Aoki Takanose,Shuji Oishi,Masashi Yokozuka,Kentaro Uno,Kazuya Yoshida*

Main category: cs.RO

TL;DR: The paper introduces a tightly coupled LiDAR-IMU-leg odometry system with a learning-based neural leg kinematics model to tackle challenges like featureless environments and deformable terrains, achieving state-of-the-art performance in challenging real-world experiments.


<details>
  <summary>Details</summary>
Motivation: To address the need for robust and adaptable odometry in robots operating in challenging conditions such as featureless environments and deformable terrains.

Method: The authors developed a neural leg kinematics model that integrates tactile information to capture foot-ground nonlinear dynamics and allows online training for adaptability. They employed a unified factor graph approach to integrate this model with odometry while maintaining consistency.

Result: The proposed method, tested on a quadruped robot in challenging terrains like sandy beaches and mixed terrain campuses, demonstrated superior odometry performance compared to state-of-the-art methods.

Conclusion: The combination of the neural leg kinematics model and tightly coupled odometry offers a robust solution to odometry challenges, enabling superior adaptability and precision in dynamic and feature-scarce environments.

Abstract: In this letter, we present tightly coupled LiDAR-IMU-leg odometry, which is
robust to challenging conditions such as featureless environments and
deformable terrains. We developed an online learning-based leg kinematics model
named the neural leg kinematics model, which incorporates tactile information
(foot reaction force) to implicitly express the nonlinear dynamics between
robot feet and the ground. Online training of this model enhances its
adaptability to weight load changes of a robot (e.g., assuming delivery or
transportation tasks) and terrain conditions. According to the \textit{neural
adaptive leg odometry factor} and online uncertainty estimation of the leg
kinematics model-based motion predictions, we jointly solve online training of
this kinematics model and odometry estimation on a unified factor graph to
retain the consistency of both. The proposed method was verified through real
experiments using a quadruped robot in two challenging situations: 1) a sandy
beach, representing an extremely featureless area with a deformable terrain,
and 2) a campus, including multiple featureless areas and terrain types of
asphalt, gravel (deformable terrain), and grass. Experimental results showed
that our odometry estimation incorporating the \textit{neural leg kinematics
model} outperforms state-of-the-art works. Our project page is available for
further details: https://takuokawara.github.io/RAL2025_project_page/

</details>


### [304] [Enhancing Human-Robot Collaboration: A Sim2Real Domain Adaptation Algorithm for Point Cloud Segmentation in Industrial Environments](https://arxiv.org/abs/2506.09552)
*Fatemeh Mohammadi Amin,Darwin G. Caldwell,Hans Wernher van de Venn*

Main category: cs.RO

TL;DR: This paper introduces a novel approach to Sim2Real domain adaptation for semantic segmentation of 3D point cloud data in human-robot collaboration (HRC) settings with a dual-stream network architecture achieving 97.76% accuracy.


<details>
  <summary>Details</summary>
Motivation: Human-robot collaboration demands robust 3D environment interpretation for safety and efficiency, necessitating effective semantic segmentation, yet real-world annotated industrial data remains scarce.

Method: A dual-stream network integrating Dynamic Graph Convolutional Neural Networks (DGCNN) and Convolutional Neural Networks (CNN) with residual layers is proposed for Sim2Real domain adaptation.

Result: Achieved segmentation accuracy of 97.76% in real-world and simulated industrial environments, surpassing the state-of-the-art performances.

Conclusion: The proposed architecture proves effective for semantic segmentation in HRC, demonstrating robustness and practical utility for industrial applications.

Abstract: The robust interpretation of 3D environments is crucial for human-robot
collaboration (HRC) applications, where safety and operational efficiency are
paramount. Semantic segmentation plays a key role in this context by enabling a
precise and detailed understanding of the environment. Considering the intense
data hunger for real-world industrial annotated data essential for effective
semantic segmentation, this paper introduces a pioneering approach in the
Sim2Real domain adaptation for semantic segmentation of 3D point cloud data,
specifically tailored for HRC. Our focus is on developing a network that
robustly transitions from simulated environments to real-world applications,
thereby enhancing its practical utility and impact on a safe HRC.
  In this work, we propose a dual-stream network architecture (FUSION)
combining Dynamic Graph Convolutional Neural Networks (DGCNN) and Convolutional
Neural Networks (CNN) augmented with residual layers as a Sim2Real domain
adaptation algorithm for an industrial environment. The proposed model was
evaluated on real-world HRC setups and simulation industrial point clouds, it
showed increased state-of-the-art performance, achieving a segmentation
accuracy of 97.76%, and superior robustness compared to existing methods.

</details>


### [305] [Integrating Quantized LLMs into Robotics Systems as Edge AI to Leverage their Natural Language Processing Capabilities](https://arxiv.org/abs/2506.09581)
*Miguel Á. González-Santamarta,Francisco J. Rodríguez-Lera,David Sobrín-Hidalgo,Ángel Manuel Guerrero-Higueras,Vicente MatellÁn-Olivera*

Main category: cs.RO

TL;DR: The paper introduces llama_ros, a tool integrating quantized Large Language Models (LLMs) into robotics systems using ROS 2, enhancing decision-making and interactions.


<details>
  <summary>Details</summary>
Motivation: To improve robotics through natural language understanding and enhance human-robot interaction, navigation, planning, and decision-making.

Method: The study utilizes llama.cpp to enable efficient execution of quantized LLMs as edge AI within resource-constrained robotic systems, addressing computational efficiency and memory challenges.

Result: The tool demonstrates its utility in using LLMs for planning and explainability in robotics, contributing to improved autonomy and interaction quality.

Conclusion: Llama_ros represents a significant step forward in autonomously integrating natural language functionality into robotic systems, making them more capable even under resource limitations.

Abstract: Large Language Models (LLMs) have experienced great advancements in the last
year resulting in an increase of these models in several fields to face natural
language tasks. The integration of these models in robotics can also help to
improve several aspects such as human-robot interaction, navigation, planning
and decision-making. Therefore, this paper introduces llama\_ros, a tool
designed to integrate quantized Large Language Models (LLMs) into robotic
systems using ROS 2. Leveraging llama.cpp, a highly optimized runtime engine,
llama\_ros enables the efficient execution of quantized LLMs as edge artificial
intelligence (AI) in robotics systems with resource-constrained environments,
addressing the challenges of computational efficiency and memory limitations.
By deploying quantized LLMs, llama\_ros empowers robots to leverage the natural
language understanding and generation for enhanced decision-making and
interaction which can be paired with prompt engineering, knowledge graphs,
ontologies or other tools to improve the capabilities of autonomous robots.
Additionally, this paper provides insights into some use cases of using
llama\_ros for planning and explainability in robotics.

</details>


### [306] [VAULT: A Mobile Mapping System for ROS 2-based Autonomous Robots](https://arxiv.org/abs/2506.09583)
*Miguel Á. González-Santamarta,Francisco J. Rodríguez-Lera,Vicente Matellán-Olivera*

Main category: cs.RO

TL;DR: The paper introduces VAULT, a ROS 2-based mapping system combining multiple sensors to enable precise 3D localization and mapping for autonomous robots in challenging outdoor settings.


<details>
  <summary>Details</summary>
Motivation: Outdoor environments like agriculture and forestry pose unique navigation challenges that demand effective real-time localization and mapping solutions.

Method: VAULT integrates GNSS, VIO, IMU, and EKF to achieve accurate 3D odometry, supported by Visual SLAM for creating a detailed 3D point cloud map.

Result: The system successfully enables robust indoor and outdoor localization and mapping, providing confidence in navigating complex terrains.

Conclusion: VAULT proves to be a reliable and advanced localization and mapping solution tailored for outdoor environments, enhancing the capabilities of autonomous mobile robots.

Abstract: Localization plays a crucial role in the navigation capabilities of
autonomous robots, and while indoor environments can rely on wheel odometry and
2D LiDAR-based mapping, outdoor settings such as agriculture and forestry,
present unique challenges that necessitate real-time localization and
consistent mapping. Addressing this need, this paper introduces the VAULT
prototype, a ROS 2-based mobile mapping system (MMS) that combines various
sensors to enable robust outdoor and indoor localization. The proposed solution
harnesses the power of Global Navigation Satellite System (GNSS) data,
visual-inertial odometry (VIO), inertial measurement unit (IMU) data, and the
Extended Kalman Filter (EKF) to generate reliable 3D odometry. To further
enhance the localization accuracy, Visual SLAM (VSLAM) is employed, resulting
in the creation of a comprehensive 3D point cloud map. By leveraging these
sensor technologies and advanced algorithms, the prototype offers a
comprehensive solution for outdoor localization in autonomous mobile robots,
enabling them to navigate and map their surroundings with confidence and
precision.

</details>


### [307] [Attention-Based Map Encoding for Learning Generalized Legged Locomotion](https://arxiv.org/abs/2506.09588)
*Junzhe He,Chong Zhang,Fabian Jenelten,Ruben Grandia,Moritz BÄcher,Marco Hutter*

Main category: cs.RO

TL;DR: This paper introduces an attention-based map encoding for legged robot locomotion, enhancing robustness and precision on diverse terrains.


<details>
  <summary>Details</summary>
Motivation: Improving dynamic locomotion of legged robots in diverse terrains while addressing limitations of model-based and learning-based controllers.

Method: Proposes learning an attention-based map encoding conditioned on robot proprioception using reinforcement learning.

Result: Trained controllers for quadrupedal and humanoid robots show robustness and agility on challenging terrains in both simulation and real-world tests.

Conclusion: The approach enables precise, robust legged locomotion across diverse terrains and interprets neural network topographical perception.

Abstract: Dynamic locomotion of legged robots is a critical yet challenging topic in
expanding the operational range of mobile robots. It requires precise planning
when possible footholds are sparse, robustness against uncertainties and
disturbances, and generalizability across diverse terrains. While traditional
model-based controllers excel at planning on complex terrains, they struggle
with real-world uncertainties. Learning-based controllers offer robustness to
such uncertainties but often lack precision on terrains with sparse steppable
areas. Hybrid methods achieve enhanced robustness on sparse terrains by
combining both methods but are computationally demanding and constrained by the
inherent limitations of model-based planners. To achieve generalized legged
locomotion on diverse terrains while preserving the robustness of
learning-based controllers, this paper proposes to learn an attention-based map
encoding conditioned on robot proprioception, which is trained as part of the
end-to-end controller using reinforcement learning. We show that the network
learns to focus on steppable areas for future footholds when the robot
dynamically navigates diverse and challenging terrains. We synthesize behaviors
that exhibit robustness against uncertainties while enabling precise and agile
traversal of sparse terrains. Additionally, our method offers a way to
interpret the topographical perception of a neural network. We have trained two
controllers for a 12-DoF quadrupedal robot and a 23-DoF humanoid robot
respectively and tested the resulting controllers in the real world under
various challenging indoor and outdoor scenarios, including ones unseen during
training.

</details>


### [308] [Analytic Task Scheduler: Recursive Least Squares Based Method for Continual Learning in Embodied Foundation Models](https://arxiv.org/abs/2506.09623)
*Lipei Xie,Yingxin Li,Huiping Zhuang*

Main category: cs.RO

TL;DR: The paper introduces the Analytic Task Scheduler (ATS) to address catastrophic forgetting in embodied foundation models for robots, enabling continual learning and task adaptability.


<details>
  <summary>Details</summary>
Motivation: Catastrophic forgetting hinders embodied foundation models from retaining previously learned tasks while acquiring new skills, a critical issue in real-world dynamic environments.

Method: The proposed framework includes a task-specific model library and an analytic scheduler using recursive least squares to map language instructions to appropriate task models, enabling incremental learning without revisiting historical data.

Result: ATS demonstrated strong resistance to forgetting and adaptability to task variations on a robotic platform, validating its effectiveness in continual learning.

Conclusion: ATS provides a scalable and deployable solution for addressing catastrophic forgetting in embodied foundation models, advancing their application in robotics and dynamic AI systems.

Abstract: Embodied foundation models are crucial for Artificial Intelligence (AI)
interacting with the physical world by integrating multi-modal inputs, such as
proprioception, vision and language, to understand human intentions and
generate actions to control robots. While these models demonstrate strong
generalization and few-shot learning capabilities, they face significant
challenges in continually acquiring new skills without forgetting previously
learned skills, a problem known as catastrophic forgetting. To address this
issue, we propose the Analytic Task Scheduler (ATS), a novel framework for
continual learning in embodied foundation models. ATS consists of a
task-specific model library, where each model is fine-tuned independently on a
single task, and an analytic scheduler trained using recursive least squares
(RLS) to learn the mapping between language instructions and task-specific
models. This architecture enables accurate task recognition and dynamic model
selection while fundamentally avoiding parameter interference across tasks. The
scheduler updates its parameters incrementally using only statistics
(autocorrelation and cross-correlation matrices), enabling forgetting-resistant
learning without the need to revisit historical data. We validate ATS on a
real-world robot platform (RM65B), demonstrating superior resistance to
forgetting and strong adaptability to task variations. The results highlight
ATS as an effective, scalable, and deployable solution for continual learning
in embodied foundation models operating in complex, dynamic environments. Our
code will be available at
https://github.com/MIAA-Embodied-AI/AnalyticTaskScheduler

</details>


### [309] [R-CARLA: High-Fidelity Sensor Simulations with Interchangeable Dynamics for Autonomous Racing](https://arxiv.org/abs/2506.09629)
*Maurice Brunner,Edoardo Ghignone,Nicolas Baumann,Michele Magno*

Main category: cs.RO

TL;DR: This paper presents R-CARLA, an enhanced version of the CARLA simulator for more realistic autonomous racing simulations, reducing the simulation-to-reality gap significantly.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need for a simulation environment that balances accurate vehicle dynamics and high-fidelity sensor behavior to improve full-stack autonomous driving algorithms, especially in the domain of racing.

Method: The authors developed R-CARLA by improving the CARLA simulator, integrating accurate vehicle dynamics, high-fidelity sensor simulations, NPC simulation, and a digital-twin creation pipeline.

Result: The enhanced R-CARLA showed a 42% reduction in the Sim-to-Real gap for vehicle dynamics and 82% for sensor simulation, enabling more realistic and effective testing of autonomous systems.

Conclusion: R-CARLA provides a comprehensive solution for testing autonomous driving technologies, offering significant improvements in realism and performance, which are critical for pushing the boundaries of autonomous racing research.

Abstract: Autonomous racing has emerged as a crucial testbed for autonomous driving
algorithms, necessitating a simulation environment for both vehicle dynamics
and sensor behavior. Striking the right balance between vehicle dynamics and
sensor accuracy is crucial for pushing vehicles to their performance limits.
However, autonomous racing developers often face a trade-off between accurate
vehicle dynamics and high-fidelity sensor simulations. This paper introduces
R-CARLA, an enhancement of the CARLA simulator that supports holistic
full-stack testing, from perception to control, using a single system. By
seamlessly integrating accurate vehicle dynamics with sensor simulations,
opponents simulation as NPCs, and a pipeline for creating digital twins from
real-world robotic data, R-CARLA empowers researchers to push the boundaries of
autonomous racing development. Furthermore, it is developed using CARLA's rich
suite of sensor simulations. Our results indicate that incorporating the
proposed digital-twin framework into R-CARLA enables more realistic full-stack
testing, demonstrating a significant reduction in the Sim-to-Real gap of car
dynamics simulation by 42% and by 82% in the case of sensor simulation across
various testing scenarios.

</details>


### [310] [Human-robot collaborative transport personalization via Dynamic Movement Primitives and velocity scaling](https://arxiv.org/abs/2506.09697)
*Paolo Franceschi,Andrea Bussolan,Vincenzo Pomponi,Oliver Avram,Stefano Baraldo,Anna Valente*

Main category: cs.RO

TL;DR: This paper introduces an approach that utilizes Dynamic Movement Primitives (DMPs) with velocity scaling based on human feedback for personalized human-robot collaboration, tested in industrial settings.


<details>
  <summary>Details</summary>
Motivation: The growing interest in industries for human-robot collaboration drives the need for intelligent motion-planning strategies that consider both task constraints and human-specific factors.

Method: The authors developed a novel DMP-based trajectory generation method enhanced with real-time velocity scaling informed by human feedback.

Result: Experiments demonstrated the adaptability and preference for DMP-based trajectories, supported by objective evaluations such as physiological measurements and subjective user feedback.

Conclusion: The study highlights the advantages of DMPs in human-robot interaction, improving user experience by enabling personalized trajectories.

Abstract: Nowadays, industries are showing a growing interest in human-robot
collaboration, particularly for shared tasks. This requires intelligent
strategies to plan a robot's motions, considering both task constraints and
human-specific factors such as height and movement preferences. This work
introduces a novel approach to generate personalized trajectories using Dynamic
Movement Primitives (DMPs), enhanced with real-time velocity scaling based on
human feedback. The method was rigorously tested in industrial-grade
experiments, focusing on the collaborative transport of an engine cowl lip
section. Comparative analysis between DMP-generated trajectories and a
state-of-the-art motion planner (BiTRRT) highlights their adaptability combined
with velocity scaling. Subjective user feedback further demonstrates a clear
preference for DMP- based interactions. Objective evaluations, including
physiological measurements from brain and skin activity, reinforce these
findings, showcasing the advantages of DMPs in enhancing human-robot
interaction and improving user experience.

</details>


### [311] [Learning to Optimize Package Picking for Large-Scale, Real-World Robot Induction](https://arxiv.org/abs/2506.09765)
*Shuai Li,Azarakhsh Keipour,Sicong Zhao,Srinath Rajagopalan,Charles Swan,Kostas E. Bekris*

Main category: cs.RO

TL;DR: The paper proposes a machine learning framework that significantly reduces pick failure rates in robotic warehouses by optimizing suction cup selection and transform adjustments.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of heuristic-based pick sampling methods in warehouse automation and improve performance at scale using data-driven techniques.

Method: Developed an ML-based framework for predicting transform adjustments and optimizing suction cup selection in multi-suction end effectors.

Result: Achieved a 20% reduction in pick failure rates over 2 million picks compared to a heuristic-based baseline.

Conclusion: The proposed ML framework is effective in enhancing robotic warehouse operations, making it a practical tool for large-scale automation.

Abstract: Warehouse automation plays a pivotal role in enhancing operational
efficiency, minimizing costs, and improving resilience to workforce
variability. While prior research has demonstrated the potential of machine
learning (ML) models to increase picking success rates in large-scale robotic
fleets by prioritizing high-probability picks and packages, these efforts
primarily focused on predicting success probabilities for picks sampled using
heuristic methods. Limited attention has been given, however, to leveraging
data-driven approaches to directly optimize sampled picks for better
performance at scale. In this study, we propose an ML-based framework that
predicts transform adjustments as well as improving the selection of suction
cups for multi-suction end effectors for sampled picks to enhance their success
probabilities. The framework was integrated and evaluated in test workcells
that resemble the operations of Amazon Robotics' Robot Induction (Robin) fleet,
which is used for package manipulation. Evaluated on over 2 million picks, the
proposed method achieves a 20\% reduction in pick failure rates compared to a
heuristic-based pick sampling baseline, demonstrating its effectiveness in
large-scale warehouse automation scenarios.

</details>


### [312] [Reinforced Refinement with Self-Aware Expansion for End-to-End Autonomous Driving](https://arxiv.org/abs/2506.09800)
*Haochen Liu,Tianyu Li,Haohan Yang,Li Chen,Caojun Wang,Ke Guo,Haochen Tian,Hongchen Li,Hongyang Li,Chen Lv*

Main category: cs.RO

TL;DR: This paper presents R2SE, a new learning pipeline for refining autonomous driving models by combining components of IL and RL to handle generalization and hard-case optimization.


<details>
  <summary>Details</summary>
Motivation: Existing imitation learning models struggle with generalization and lack corrective feedback in deployed systems, while reinforcement learning faces overfitting and inefficiency, limiting their use in autonomous driving.

Method: The paper introduces R2SE, a learning pipeline with three core components: Generalist Pretraining for hard-case allocation, Residual Reinforced Specialist Fine-tuning for hard-case refinement, and Self-aware Adapter Expansion for continuous policy improvements.

Result: R2SE shows enhanced generalization, safety, and robustness in both simulations and real-world datasets compared to current state-of-the-art systems.

Conclusion: R2SE demonstrates a scalable and effective refinement process for end-to-end autonomous driving systems, addressing limitations of IL and RL approaches.

Abstract: End-to-end autonomous driving has emerged as a promising paradigm for
directly mapping sensor inputs to planning maneuvers using learning-based
modular integrations. However, existing imitation learning (IL)-based models
suffer from generalization to hard cases, and a lack of corrective feedback
loop under post-deployment. While reinforcement learning (RL) offers a
potential solution to tackle hard cases with optimality, it is often hindered
by overfitting to specific driving cases, resulting in catastrophic forgetting
of generalizable knowledge and sample inefficiency. To overcome these
challenges, we propose Reinforced Refinement with Self-aware Expansion (R2SE),
a novel learning pipeline that constantly refines hard domain while keeping
generalizable driving policy for model-agnostic end-to-end driving systems.
Through reinforcement fine-tuning and policy expansion that facilitates
continuous improvement, R2SE features three key components: 1) Generalist
Pretraining with hard-case allocation trains a generalist imitation learning
(IL) driving system while dynamically identifying failure-prone cases for
targeted refinement; 2) Residual Reinforced Specialist Fine-tuning optimizes
residual corrections using reinforcement learning (RL) to improve performance
in hard case domain while preserving global driving knowledge; 3) Self-aware
Adapter Expansion dynamically integrates specialist policies back into the
generalist model, enhancing continuous performance improvement. Experimental
results in closed-loop simulation and real-world datasets demonstrate
improvements in generalization, safety, and long-horizon policy robustness over
state-of-the-art E2E systems, highlighting the effectiveness of reinforce
refinement for scalable autonomous driving.

</details>


### [313] [Hierarchical Learning-Enhanced MPC for Safe Crowd Navigation with Heterogeneous Constraints](https://arxiv.org/abs/2506.09859)
*Huajian Liu,Yixuan Feng,Wei Dong,Kunpeng Fan,Chao Wang,Yongzhuo Gao*

Main category: cs.RO

TL;DR: The paper introduces a new hierarchical framework for robot navigation that combines graph neural networks and reinforcement learning to improve planning in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of robot navigation in dynamic environments with heterogeneous constraints, particularly avoiding reliance on high-fidelity simulation for scalability.

Method: Employing graph neural networks trained with reinforcement learning for local goal recommendations and combining them with spatio-temporal path-searching modules for trajectory optimization.

Result: Their method achieves state-of-the-art (SOTA) local planning performance in complex dynamic environments and surpasses existing methods in computational efficiency and scalability.

Conclusion: The proposed framework successfully integrates learning and optimization, eliminates dependency on high-fidelity simulations, and achieves superior navigation performance, with code availability promised upon acceptance.

Abstract: In this paper, we propose a novel hierarchical framework for robot navigation
in dynamic environments with heterogeneous constraints. Our approach leverages
a graph neural network trained via reinforcement learning (RL) to efficiently
estimate the robot's cost-to-go, formulated as local goal recommendations. A
spatio-temporal path-searching module, which accounts for kinematic
constraints, is then employed to generate a reference trajectory to facilitate
solving the non-convex optimization problem used for explicit constraint
enforcement. More importantly, we introduce an incremental action-masking
mechanism and a privileged learning strategy, enabling end-to-end training of
the proposed planner. Both simulation and real-world experiments demonstrate
that the proposed method effectively addresses local planning in complex
dynamic environments, achieving state-of-the-art (SOTA) performance. Compared
with existing learning-optimization hybrid methods, our approach eliminates the
dependency on high-fidelity simulation environments, offering significant
advantages in computational efficiency and training scalability. The code will
be released as open-source upon acceptance of the paper.

</details>


### [314] [Aucamp: An Underwater Camera-Based Multi-Robot Platform with Low-Cost, Distributed, and Robust Localization](https://arxiv.org/abs/2506.09876)
*Jisheng Xu,Ding Lin,Pangkit Fong,Chongrong Fang,Xiaoming Duan,Jianping He*

Main category: cs.RO

TL;DR: The paper presents Aucamp, an underwater multi-robot platform utilizing monocular-camera-based sensing and distributed protocols for improved positioning and control.


<details>
  <summary>Details</summary>
Motivation: To address the need for cost-effective and robust underwater localization systems that enable efficient marine exploration.

Method: Developed a platform with monocular imaging models for distance measurement and integrated distributed update protocols, coupled with a robust orientation control framework for dynamic stability.

Result: Validated the system through extensive experiments, demonstrating improved accuracy, stability, and wide-range perception capabilities.

Conclusion: The Aucamp platform enhances underwater localization and could significantly support marine exploration via underwater sensor networks.

Abstract: This paper introduces an underwater multi-robot platform, named Aucamp,
characterized by cost-effective monocular-camera-based sensing, distributed
protocol and robust orientation control for localization. We utilize the
clarity feature to measure the distance, present the monocular imaging model,
and estimate the position of the target object. We achieve global positioning
in our platform by designing a distributed update protocol. The distributed
algorithm enables the perception process to simultaneously cover a broader
range, and greatly improves the accuracy and robustness of the positioning.
Moreover, the explicit dynamics model of the robot in our platform is obtained,
based on which, we propose a robust orientation control framework. The control
system ensures that the platform maintains a balanced posture for each robot,
thereby ensuring the stability of the localization system. The platform can
swiftly recover from an forced unstable state to a stable horizontal posture.
Additionally, we conduct extensive experiments and application scenarios to
evaluate the performance of our platform. The proposed new platform may provide
support for extensive marine exploration by underwater sensor networks.

</details>


### [315] [From Theory to Practice: Advancing Multi-Robot Path Planning Algorithms and Applications](https://arxiv.org/abs/2506.09914)
*Teng Guo*

Main category: cs.RO

TL;DR: The paper focuses on the multi-robot path planning (MRPP) problem, introducing scalable methods for efficient routing and collision avoidance.


<details>
  <summary>Details</summary>
Motivation: Address the complexity of multi-robot path planning and its industrial significance by improving solution efficiency and quality.

Method: Proposed the Rubik Table method for dense MRPP on 2D grids, optimal layouts for structured environments, a puzzle-based parking system, and extended MRPP to Reeds-Shepp robots with motion primitives and smoothing techniques.

Result: Achieved $(1 + \delta)$-optimal makespan for dense MRPP with high robot density, validated approaches through simulations and real-world tests in urban and robotic transport contexts.

Conclusion: The research sets a new theoretical benchmark while solving real-world MRPP efficiently, ensuring collision-free paths even under complex conditions like nonholonomic constraints.

Abstract: The labeled MRPP (Multi-Robot Path Planning) problem involves routing robots
from start to goal configurations efficiently while avoiding collisions.
Despite progress in solution quality and runtime, its complexity and industrial
relevance continue to drive research.
  This dissertation introduces scalable MRPP methods with provable guarantees
and practical heuristics. First, we study dense MRPP on 2D grids, relevant to
warehouse and parcel systems. We propose the Rubik Table method, achieving $(1
+ \delta)$-optimal makespan (with $\delta \in (0, 0.5]$) for up to $\frac{m_1
m_2}{2}$ robots, solving large instances efficiently and setting a new
theoretical benchmark.
  Next, we address real-world MRPP. We design optimal layouts for structured
environments (e.g., warehouses, parking systems) and propose a puzzle-based
system for dense, deadlock-free autonomous vehicle parking. We also extend MRPP
to Reeds-Shepp robots, introducing motion primitives and smoothing techniques
to ensure feasible, efficient paths under nonholonomic constraints. Simulations
and real-world tests validate the approach in urban driving and robotic
transport scenarios.

</details>


### [316] [From Intention to Execution: Probing the Generalization Boundaries of Vision-Language-Action Models](https://arxiv.org/abs/2506.09930)
*Irving Fang,Juexiao Zhang,Shengbang Tong,Chen Feng*

Main category: cs.RO

TL;DR: This paper introduces a simulation-based benchmark for evaluating Vision-Language-Action models in robotics, focusing on their generalization capabilities and the challenges they face in motor execution.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of comprehensive, reproducible benchmarks for evaluating the generalization capabilities of Vision-Language-Action models in robotics.

Method: A unified probing suite of 50 simulation-based tasks across multiple categories was developed. These tasks systematically evaluate state-of-the-art VLA architectures in terms of perception and action execution.

Result: Findings reveal that Vision-Language-Action models exhibit strong perceptual understanding and planning (good intentions) but often fail in precise motor execution, especially with out-of-distribution observations. Finetuning on action data can degrade general reasoning abilities.

Conclusion: The introduced benchmark provides a new standardized way to evaluate VLAs and highlights the perception-to-action gap as a key area for future research. The task suite and code are released for wider research use.

Abstract: One promise that Vision-Language-Action (VLA) models hold over traditional
imitation learning for robotics is to leverage the broad generalization
capabilities of large Vision-Language Models (VLMs) to produce versatile,
"generalist" robot policies. However, current evaluations of VLAs remain
insufficient. Traditional imitation learning benchmarks are unsuitable due to
the lack of language instructions. Emerging benchmarks for VLAs that
incorporate language often come with limited evaluation tasks and do not intend
to investigate how much VLM pretraining truly contributes to the generalization
capabilities of the downstream robotic policy. Meanwhile, much research relies
on real-world robot setups designed in isolation by different institutions,
which creates a barrier for reproducibility and accessibility. To address this
gap, we introduce a unified probing suite of 50 simulation-based tasks across
10 subcategories spanning language instruction, vision, and objects. We
systematically evaluate several state-of-the-art VLA architectures on this
suite to understand their generalization capability. Our results show that
while VLM backbones endow VLAs with robust perceptual understanding and high
level planning, which we refer to as good intentions, this does not reliably
translate into precise motor execution: when faced with out-of-distribution
observations, policies often exhibit coherent intentions, but falter in action
execution. Moreover, finetuning on action data can erode the original VLM's
generalist reasoning abilities. We release our task suite and evaluation code
to serve as a standardized benchmark for future VLAs and to drive research on
closing the perception-to-action gap. More information, including the source
code, can be found at https://ai4ce.github.io/INT-ACT/

</details>


### [317] [Fluoroscopic Shape and Pose Tracking of Catheters with Custom Radiopaque Markers](https://arxiv.org/abs/2506.09934)
*Jared Lawson,Rohan Chitale,Nabil Simaan*

Main category: cs.RO

TL;DR: The paper presents a method for tracking the shape and pose of microcatheters in cerebral vasculature using radiopaque markers and biplane fluoroscopy.


<details>
  <summary>Details</summary>
Motivation: Current methods for tracking steerable catheters place a high cognitive load on interventionalists and rely on suboptimal technology, such as planar segmentation or bulky sensing tools, which are unsuitable for neurointervention.

Method: Radiopaque markers are strategically placed on microcatheters for shape and pose estimation under biplane fluoroscopy. A design measure was created to optimize the marker arrangement, reducing sensitivity to tracking uncertainties.

Result: The approach was validated with microcatheters below 2mm OD, achieving shape tracking errors under 1mm and roll errors below 40 degrees during navigation in phantom vasculature.

Conclusion: This method provides an efficient solution for autonomous navigation of steerable catheters in neurointervention, overcoming current limitations in tracking technology and perception burden.

Abstract: Safe navigation of steerable and robotic catheters in the cerebral
vasculature requires awareness of the catheters shape and pose. Currently, a
significant perception burden is placed on interventionalists to mentally
reconstruct and predict catheter motions from biplane fluoroscopy images.
Efforts to track these catheters are limited to planar segmentation or bulky
sensing instrumentation, which are incompatible with microcatheters used in
neurointervention. In this work, a catheter is equipped with custom radiopaque
markers arranged to enable simultaneous shape and pose estimation under biplane
fluoroscopy. A design measure is proposed to guide the arrangement of these
markers to minimize sensitivity to marker tracking uncertainty. This approach
was deployed for microcatheters smaller than 2mm OD navigating phantom
vasculature with shape tracking errors less than 1mm and catheter roll errors
below 40 degrees. This work can enable steerable catheters to autonomously
navigate under biplane imaging.

</details>


### [318] [SAFE: Multitask Failure Detection for Vision-Language-Action Models](https://arxiv.org/abs/2506.09937)
*Qiao Gu,Yuanliang Ju,Shengxiang Sun,Igor Gilitschenski,Haruki Nishimura,Masha Itkina,Florian Shkurti*

Main category: cs.RO

TL;DR: The paper introduces SAFE, a failure detection system for generalist robot policies like VLAs, achieving state-of-the-art performance in detecting task failures across unseen environments.


<details>
  <summary>Details</summary>
Motivation: Vision-language-action models (VLAs) struggle to generalize to novel tasks and environments, necessitating a reliable failure detector to ensure safe and effective robotic interactions.

Method: SAFE leverages high-level features from the internal feature space of VLAs to predict task failure likelihood. It is designed to generalize across tasks and environments and uses conformal prediction for optimized accuracy and timing.

Result: SAFE was tested extensively with VLAs in simulated and real-world environments, outperforming diverse baselines in failure detection accuracy and speed.

Conclusion: The proposed SAFE system significantly enhances failure detection capabilities for generalist robot policies, enabling safer and more reliable operation in novel tasks and environments.

Abstract: While vision-language-action models (VLAs) have shown promising robotic
behaviors across a diverse set of manipulation tasks, they achieve limited
success rates when deployed on novel tasks out-of-the-box. To allow these
policies to safely interact with their environments, we need a failure detector
that gives a timely alert such that the robot can stop, backtrack, or ask for
help. However, existing failure detectors are trained and tested only on one or
a few specific tasks, while VLAs require the detector to generalize and detect
failures also in unseen tasks and novel environments. In this paper, we
introduce the multitask failure detection problem and propose SAFE, a failure
detector for generalist robot policies such as VLAs. We analyze the VLA feature
space and find that VLAs have sufficient high-level knowledge about task
success and failure, which is generic across different tasks. Based on this
insight, we design SAFE to learn from VLA internal features and predict a
single scalar indicating the likelihood of task failure. SAFE is trained on
both successful and failed rollouts, and is evaluated on unseen tasks. SAFE is
compatible with different policy architectures. We test it on OpenVLA, $\pi_0$,
and $\pi_0$-FAST in both simulated and real-world environments extensively. We
compare SAFE with diverse baselines and show that SAFE achieves
state-of-the-art failure detection performance and the best trade-off between
accuracy and detection time using conformal prediction. More qualitative
results can be found at https://vla-safe.github.io/.

</details>


### [319] [Locomotion on Constrained Footholds via Layered Architectures and Model Predictive Control](https://arxiv.org/abs/2506.09979)
*Zachary Olkin,Aaron D. Ames*

Main category: cs.RO

TL;DR: The paper proposes a layered control architecture for real-time legged locomotion by splitting discrete variable selection and continuous control, demonstrating improvements in efficiency and reliability in robot navigation.


<details>
  <summary>Details</summary>
Motivation: Real-time control of legged robots is difficult due to their hybrid, nonlinear, and high-dimensional dynamics, making numerical optimal control challenging.

Method: The authors introduced a layered architecture combining sampling-based selection for discrete variables and classical smooth Model Predictive Controller (MPC) for continuous control.

Result: The approach successfully enabled quadrupedal and humanoid robots to traverse challenging terrains in simulation and experiments, outperforming heuristic and sampling-based methods in speed and reliability.

Conclusion: The proposed layered methodology enhances optimality and computation speed for legged robot control, addressing challenges in nonlinear and hybrid dynamics effectively.

Abstract: Computing stabilizing and optimal control actions for legged locomotion in
real time is difficult due to the nonlinear, hybrid, and high dimensional
nature of these robots. The hybrid nature of the system introduces a
combination of discrete and continuous variables which causes issues for
numerical optimal control. To address these challenges, we propose a layered
architecture that separates the choice of discrete variables and a smooth Model
Predictive Controller (MPC). The layered formulation allows for online
flexibility and optimality without sacrificing real-time performance through a
combination of gradient-free and gradient-based methods. The architecture
leverages a sampling-based method for determining discrete variables, and a
classical smooth MPC formulation using these fixed discrete variables. We
demonstrate the results on a quadrupedal robot stepping over gaps and onto
terrain with varying heights. In simulation, we demonstrate the controller on a
humanoid robot for gap traversal. The layered approach is shown to be more
optimal and reliable than common heuristic-based approaches and faster to
compute than pure sampling methods.

</details>


### [320] [Chain-of-Action: Trajectory Autoregressive Modeling for Robotic Manipulation](https://arxiv.org/abs/2506.09990)
*Wenbo Zhang,Tianrun Hu,Yanyuan Qiao,Hanbo Zhang,Yuchu Qin,Yang Li,Jiajun Liu,Tao Kong,Lingqiao Liu,Xiao Ma*

Main category: cs.RO

TL;DR: Chain-of-Action (CoA) introduces a novel visuo-motor policy based on trajectory autoregressive modeling, combining action-level backward reasoning through a Chain-of-Thought process to improve task performance.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitations of traditional forward-action prediction approaches in visuo-motor policies, aiming to achieve better task performance through a global-to-local reasoning structure that ties local actions closely to final goals.

Method: The method involves a backward reasoning framework where the first token represents a task-specific goal (keyframe action), and subsequent action tokens are autoregressively generated. The design includes continuous action token representation, dynamic stopping for varying trajectory lengths, reverse temporal ensemble, and multi-token prediction.

Result: CoA achieves state-of-the-art results across 60 RLBench tasks and 8 real-world manipulation tasks, demonstrating enhanced spatial generalization and task efficiency.

Conclusion: CoA provides a significant improvement in the design of visuo-motor policies by unifying global-to-local reasoning into a coherent autoregressive framework, enabling both strong generalization and simplicity.

Abstract: We present Chain-of-Action (CoA), a novel visuo-motor policy paradigm built
upon Trajectory Autoregressive Modeling. Unlike conventional approaches that
predict next step action(s) forward, CoA generates an entire trajectory by
explicit backward reasoning with task-specific goals through an action-level
Chain-of-Thought (CoT) process. This process is unified within a single
autoregressive structure: (1) the first token corresponds to a stable keyframe
action that encodes the task-specific goals; and (2) subsequent action tokens
are generated autoregressively, conditioned on the initial keyframe and
previously predicted actions. This backward action reasoning enforces a
global-to-local structure, allowing each local action to be tightly constrained
by the final goal. To further realize the action reasoning structure, CoA
incorporates four complementary designs: continuous action token
representation; dynamic stopping for variable-length trajectory generation;
reverse temporal ensemble; and multi-token prediction to balance action chunk
modeling with global structure. As a result, CoA gives strong spatial
generalization capabilities while preserving the flexibility and simplicity of
a visuo-motor policy. Empirically, we observe CoA achieves the state-of-the-art
performance across 60 RLBench tasks and 8 real-world manipulation tasks.

</details>


### [321] [eFlesh: Highly customizable Magnetic Touch Sensing using Cut-Cell Microstructures](https://arxiv.org/abs/2506.09994)
*Venkatesh Pattabiraman,Zizhou Huang,Daniele Panozzo,Denis Zorin,Lerrel Pinto,Raunaq Bhirangi*

Main category: cs.RO

TL;DR: The paper presents eFlesh, a customizable, low-cost, and easily fabricated magnetic tactile sensor for robots, enhancing force-sensing capabilities in unstructured environments.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of versatile and accessible tactile sensors, which have resulted in fragmented solutions for robotic manipulation and often sensorless approaches.

Method: The methodology involves creating eFlesh sensors using a 3D printer, off-the-shelf magnets, CAD models, and a magnetometer. The sensors are customizable via microstructured tiles, and an open-source design tool facilitates the CAD-to-fabrication process.

Result: The eFlesh sensor provides precise contact localization and force predictions, alongside a slip detection model with a high accuracy of 95%. It demonstrates significant improvements in manipulation tasks through visuotactile control policies.

Conclusion: The study concludes that eFlesh is a game-changing solution for tactile sensing, enabling better manipulation in robotics through its cost-effective, customizable, and open-source nature.

Abstract: If human experience is any guide, operating effectively in unstructured
environments -- like homes and offices -- requires robots to sense the forces
during physical interaction. Yet, the lack of a versatile, accessible, and
easily customizable tactile sensor has led to fragmented, sensor-specific
solutions in robotic manipulation -- and in many cases, to force-unaware,
sensorless approaches. With eFlesh, we bridge this gap by introducing a
magnetic tactile sensor that is low-cost, easy to fabricate, and highly
customizable. Building an eFlesh sensor requires only four components: a
hobbyist 3D printer, off-the-shelf magnets (<$5), a CAD model of the desired
shape, and a magnetometer circuit board. The sensor is constructed from tiled,
parameterized microstructures, which allow for tuning the sensor's geometry and
its mechanical response. We provide an open-source design tool that converts
convex OBJ/STL files into 3D-printable STLs for fabrication. This modular
design framework enables users to create application-specific sensors, and to
adjust sensitivity depending on the task. Our sensor characterization
experiments demonstrate the capabilities of eFlesh: contact localization RMSE
of 0.5 mm, and force prediction RMSE of 0.27 N for normal force and 0.12 N for
shear force. We also present a learned slip detection model that generalizes to
unseen objects with 95% accuracy, and visuotactile control policies that
improve manipulation performance by 40% over vision-only baselines -- achieving
91% average success rate for four precise tasks that require sub-mm accuracy
for successful completion. All design files, code and the CAD-to-eFlesh STL
conversion tool are open-sourced and available on https://e-flesh.com.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [322] [Formal Methods Meets Readability: Auto-Documenting JML Java Code](https://arxiv.org/abs/2506.09230)
*Juan Carlos Recio Abad,Ruben Saborido,Francisco Chicano*

Main category: cs.SE

TL;DR: The paper finds that Java Modeling Language (JML) annotations improve LLM-generated Javadocs, especially in enhancing class-level completeness and invariant specification.


<details>
  <summary>Details</summary>
Motivation: Exploring whether incorporating formal specifications via JML can improve the quality of LLM-generated documentation compared to code-only approaches.

Method: A systematic comparison of Javadocs generated from JML-annotated and non-annotated Java classes was performed, evaluating quality using automated metrics and expert reviews.

Result: JML significantly enhances class-level documentation completeness and helps capture complex invariants, but has a moderate impact at the method level and limited effect on descriptive quality.

Conclusion: JML provides robust benefits for classes with richer invariants and aids in ensuring comprehensive documentation coverage, offering valuable insights for integrating formal methods in software documentation workflows.

Abstract: This paper investigates whether formal specifications using Java Modeling
Language (JML) can enhance the quality of Large Language Model (LLM)-generated
Javadocs. While LLMs excel at producing documentation from code alone, we
hypothesize that incorporating formally verified invariants yields more
complete and accurate results. We present a systematic comparison of
documentation generated from JML-annotated and non-annotated Java classes,
evaluating quality through both automated metrics and expert analysis. Our
findings demonstrate that JML significantly improves class-level documentation
completeness, with more moderate gains at the method level. Formal
specifications prove particularly effective in capturing complex class
invariants and design contracts that are frequently overlooked in code-only
documentation. A threshold effect emerges, where the benefits of JML become
more pronounced for classes with richer sets of invariants. While JML enhances
specification coverage, its impact on core descriptive quality is limited,
suggesting that formal specifications primarily ensure comprehensive coverage
rather than fundamentally altering implementation descriptions. These results
offer actionable insights for software teams adopting formal methods in
documentation workflows, highlighting scenarios where JML provides clear
advantages. The study contributes to AI-assisted software documentation
research by demonstrating how formal methods and LLMs can synergistically
improve documentation quality.

</details>


### [323] [UTBoost: Rigorous Evaluation of Coding Agents on SWE-Bench](https://arxiv.org/abs/2506.09289)
*Boxi Yu,Yuxuan Zhu,Pinjia He,Daniel Kang*

Main category: cs.SE

TL;DR: UTGenerator is an LLM-driven tool that enhances Python test cases for real-world projects and improves the reliability of the SWE-Bench evaluation benchmark.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation benchmarks like SWE-Bench often include insufficient manually written test cases, enabling flawed code patches to pass tests without truly resolving their issues.

Method: The authors developed UTGenerator, an LLM-based test case generator that analyzes codebases and dependencies. They further introduced UTBoost, a test case augmentation framework, to identify inadequate test cases and refine the SWE-Bench benchmark.

Result: The approach identified 36 instances of insufficient test cases and corrected 345 patches mislabeled as passed. These corrections impacted 40.9% of SWE-Bench Lite and 24.4% of SWE-Bench Verified leaderboard entries, leading to 18 and 11 ranking changes, respectively.

Conclusion: UTGenerator and UTBoost significantly enhance the robustness of SWE-Bench as a code-generation benchmark by addressing deficiencies in its test-case coverage.

Abstract: The advent of Large Language Models (LLMs) has spurred the development of
coding agents for real-world code generation. As a widely used benchmark for
evaluating the code generation capabilities of these agents, SWE-Bench uses
real-world problems based on GitHub issues and their corresponding pull
requests. However, the manually written test cases included in these pull
requests are often insufficient, allowing generated patches to pass the tests
without resolving the underlying issue. To address this challenge, we introduce
UTGenerator, an LLM-driven test case generator that automatically analyzes
codebases and dependencies to generate test cases for real-world Python
projects. Building on UTGenerator, we propose UTBoost, a comprehensive
framework for test case augmentation. In our evaluation, we identified 36 task
instances with insufficient test cases and uncovered 345 erroneous patches
incorrectly labeled as passed in the original SWE Bench. These corrections,
impacting 40.9% of SWE-Bench Lite and 24.4% of SWE-Bench Verified leaderboard
entries, yield 18 and 11 ranking changes, respectively.

</details>


### [324] [Assessing the Impact of Refactoring Energy-Inefficient Code Patterns on Software Sustainability: An Industry Case Study](https://arxiv.org/abs/2506.09370)
*Rohit Mehra,Priyavanshi Pathania,Vibhu Saujanya Sharma,Vikrant Kaulgud,Sanjay Podder,Adam P. Burden*

Main category: cs.SE

TL;DR: This paper studies how automated tools can identify and refactor energy-inefficient code patterns, demonstrating a 29% reduction in energy consumption for a large application.


<details>
  <summary>Details</summary>
Motivation: The widespread use of software systems is contributing to increasing carbon emissions, highlighting the need for sustainable software optimization.

Method: The authors conducted a case study using automated tools to identify energy-inefficient code patterns and assessed their impact post-refactoring.

Result: Refactoring identified code patterns led to a significant reduction in per-user per-month energy consumption by 29%.

Conclusion: Adopting automated tools to assess and refactor code can effectively optimize software sustainability and reduce energy consumption.

Abstract: Advances in technologies like artificial intelligence and metaverse have led
to a proliferation of software systems in business and everyday life. With this
widespread penetration, the carbon emissions of software are rapidly growing as
well, thereby negatively impacting the long-term sustainability of our
environment. Hence, optimizing software from a sustainability standpoint
becomes more crucial than ever. We believe that the adoption of automated tools
that can identify energy-inefficient patterns in the code and guide appropriate
refactoring can significantly assist in this optimization. In this extended
abstract, we present an industry case study that evaluates the sustainability
impact of refactoring energy-inefficient code patterns identified by automated
software sustainability assessment tools for a large application. Preliminary
results highlight a positive impact on the application's sustainability
post-refactoring, leading to a 29% decrease in per-user per-month energy
consumption.

</details>


### [325] [Reasoning as a Resource: Optimizing Fast and Slow Thinking in Code Generation Models](https://arxiv.org/abs/2506.09396)
*Zongjie Li,Shuai Wang*

Main category: cs.SE

TL;DR: This position paper advocates for managing reasoning depth in code generation models as a controllable resource to optimize accuracy, latency, and cost.


<details>
  <summary>Details</summary>
Motivation: Current methods in code generation models lack explicit control over reasoning depth, which could lead to suboptimal trade-offs among model performance aspects.

Method: The authors propose adaptive control of reasoning depth, combining rapid 'fast thinking' with elaborate 'slow thinking' to optimize the model lifecycle.

Result: The paper outlines how managing reasoning depth can improve data creation, benchmarking, and deployment of code generation models.

Conclusion: Controlling the depth of reasoning can lead to enhanced performance, cost-efficiency, and informed deployment strategies for code generation agents.

Abstract: This position paper proposes a fundamental shift in designing code generation
models: treating reasoning depth as a controllable resource. Rather than being
an incidental byproduct of prompting, we argue that the trade-off between
rapid, direct answers ("fast thinking") and elaborate, chain-of-thought
deliberation ("slow thinking") must be explicitly managed. We contend that
optimizing reasoning budgets across the entire model lifecycle - from synthetic
data creation and benchmarking to real-world deploymen - can unlock superior
trade-offs among accuracy, latency, and cost. This paper outlines how adaptive
control over reasoning can enrich supervision signals, motivate new
multi-dimensional benchmarks, and inform cost-aware, security-conscious
deployment policies. By viewing fast and slow thinking as complementary modes
to be scheduled, we envision coding agents that think deep when necessary and
act fast when possible.

</details>


### [326] [Automated Synthesis of Formally Verified Multi-Abstraction Function Summaries](https://arxiv.org/abs/2506.09550)
*Fanpeng Yang,Xu Ma,Shuling Wang,Xiong Xu,Qinxiang Cao,Naijun Zhan,Xiaofeng Li,Bin Gu*

Main category: cs.SE

TL;DR: The paper proposes a method for automatically generating function summaries for C programs, integrating symbolic execution, large language models (LLMs), and formal verification.


<details>
  <summary>Details</summary>
Motivation: In safety-critical domains, understanding and verifying software is crucial, yet many legacy programs lack formal specifications, making it challenging to generate function summaries.

Method: The approach combines VST-A's symbolic execution, LLMs for loop invariant inference, and Frama-C for verification, with an iterative refinement system to generate Relatively Strongest Postconditions (RSPs) and synthesize reliable summaries.

Result: Extensive experiments demonstrate the effectiveness of the proposed framework over existing methods, addressing C program challenges like loops and pointer aliasing.

Conclusion: The method successfully generates multi-abstraction-level function summaries that are precise for formal verification and intuitive for human understanding.

Abstract: Function summaries, which characterize the behavior of code segments
(typically functions) through preconditions and postconditions, are essential
for understanding, reusing, and verifying software, particularly in
safety-critical domains like aerospace embedded systems. However, these
mission-critical legacy code serving as a valuable reused asset often lacks
formal specifications. It is challenging to automatically generate function
summaries for C programs, due to the existence of complex features such as
loops, nested function calls, pointer aliasing, and so on. Moreover, function
summaries should support multiple abstraction levels to meet diverse
requirements, e.g. precise summaries capturing full functionality for formal
verification and intuitive summaries for human understanding.
  To address these challenges, we first propose a novel framework that combines
symbolic execution, large language models (LLMs), and formal verification to
generate Relatively Strongest Postconditions (RSPs) and build function
summaries that fully capture program behavior. Our approach leverages VST-A's
symbolic execution to precisely track program execution paths and state
transitions, employs LLMs to infer loop invariants based on predefined
templates, and uses Frama-C to guarantee soundness of generated summaries in an
iterative refinement loop. Furthermore, from generated RSPs, we automatically
synthesize strongest non-redundant postconditions expressed within given domain
specific language. We compare our approach with existing work through extensive
experiments.

</details>


### [327] [ASTAGEN: Empirical Evaluation of Automated SATD Taxonomy Generation with LLMs](https://arxiv.org/abs/2506.09601)
*Sota Nakashima,Yuta Ishimoto,Masanari Kondo,Tao Xiao,Yasutaka Kamei*

Main category: cs.SE

TL;DR: The paper introduces ASTAGEN, a method that uses large language models (LLMs) to automate the generation of self-admitted technical debt (SATD) taxonomies, making the process faster and more consistent.


<details>
  <summary>Details</summary>
Motivation: Identifying and categorizing self-admitted technical debt (SATD) is crucial because it hinders software maintenance and quality, but the traditional methods rely heavily on time-intensive and inconsistent manual processes.

Method: ASTAGEN uses LLMs to generate concise explanations for SATD comments and iteratively updates and constructs taxonomies by analyzing these comments along with their surrounding code.

Result: ASTAGEN accurately recovers domain-specific categories from prior work, is more consistent than naive LLM applications, and efficiently generates taxonomies within two hours and at a cost below one USD for large datasets.

Conclusion: ASTAGEN demonstrates the feasibility of leveraging LLMs for semi-automated SATD taxonomy generation, offering a faster and more cost-effective alternative to manual efforts while paving the way for future advancements in automatic taxonomy generation across other domains.

Abstract: Technical debt refers to suboptimal code that degrades software quality. When
developers intentionally introduce such debt, it is called self-admitted
technical debt (SATD). Since SATD hinders maintenance, identifying its
categories is key to uncovering quality issues. Traditionally, constructing
such taxonomies requires manually inspecting SATD comments and surrounding
code, which is time-consuming, labor-intensive, and often inconsistent due to
annotator subjectivity. This study presents ASTAGEN, an initial step toward
automating SATD taxonomy generation using large language models (LLMs). Given a
comment and its surrounding code, ASTAGEN first generates a concise explanation
for each SATD comment, then incrementally generates and updates categories to
construct a taxonomy. We evaluate ASTAGEN on SATD datasets from three domains:
quantum software, smart contracts, and machine learning. It successfully
recovers domain-specific categories reported in prior work, such as Layer
Configuration in machine learning. Compared to a naive use of an LLM, ASTAGEN
produces more consistent category assignments due to its explanation-driven,
iterative design. It also completes taxonomy generation in under two hours and
for less than one USD, even on the largest dataset. These results suggest that
while full automation remains challenging, ASTAGEN is able to support
semi-automated taxonomy construction. Furthermore, our work opens up avenues
for future work, such as automatic taxonomy generation in other areas.

</details>


### [328] [Translating a VDM Model of a Medical Device into Kapture](https://arxiv.org/abs/2506.09636)
*Joe Hare,Leo Freitas,Ken Pierce*

Main category: cs.SE

TL;DR: The paper evaluates Kapture, a formal modeling tool, in translating a VDM model of a medical epilepsy implant, achieving over 90% coverage despite challenges.


<details>
  <summary>Details</summary>
Motivation: The growing complexity of safety-critical medical devices demands clear and verifiable software requirements, motivating exploration of tools like Kapture for effective modeling.

Method: The study involved translating a formal VDM model of a medical epilepsy implant into Kapture, assessing usability, challenges, and outcomes encountered during the learning curve.

Result: The resulting Kapture model covered over 90% of the original VDM model and produced matching result traces, demonstrating its feasibility for complex systems modelling.

Conclusion: Kapture is effective for modeling complex systems even by inexperienced users, though challenges in translating VDM to Kapture and initial learning were noted.

Abstract: As the complexity of safety-critical medical devices increases, so does the
need for clear, verifiable, software requirements. This paper explores the use
of Kapture, a formal modelling tool developed by D-RisQ, to translate an
existing formal VDM model of a medical implant for treating focal epilepsy
called CANDO. The work was undertaken without prior experience in formal
methods. The paper assess Kapture's usability, the challenges of formal
modelling, and the effectiveness of the translated model. The result is a model
in Kapture which covers over 90% of the original VDM model, and produces
matching traces of results. While several issues were encountered during design
and implementation, mainly due to the initial learning curve, this paper
demonstrates that complex systems can be effectively modelled in Kapture by
inexperienced users and highlights some difficulties in translating VDM
specifications to Kapture.

</details>


### [329] [Calculating Software's Energy Use and Carbon Emissions: A Survey of the State of Art, Challenges, and the Way Ahead](https://arxiv.org/abs/2506.09683)
*Priyavanshi Pathania,Nikhil Bamby,Rohit Mehra,Samarth Sikand,Vibhu Saujanya Sharma,Vikrant Kaulgud,Sanjay Podder,Adam P. Burden*

Main category: cs.SE

TL;DR: The paper reviews methods and tools for measuring software and AI energy and carbon emissions, introduces a taxonomy of approaches, and highlights current challenges.


<details>
  <summary>Details</summary>
Motivation: To address the growing energy and carbon emissions footprint of software and AI by understanding and optimizing their environmental impact.

Method: A state-of-the-art review, taxonomy categorization of methods (Monitoring, Estimation, Black-Box), and tool comparison across dimensions like emission measurement and hardware components.

Result: Insights into the practical use and component-wise consolidation of approaches, along with identified challenges in the field.

Conclusion: The authors stress the need for collaborative efforts in tackling the critical challenges of sustainable energy and carbon emission management in software and AI systems.

Abstract: The proliferation of software and AI comes with a hidden risk: its growing
energy and carbon footprint. As concerns regarding environmental sustainability
come to the forefront, understanding and optimizing how software impacts the
environment becomes paramount. In this paper, we present a state-of-the-art
review of methods and tools that enable the measurement of software and
AI-related energy and/or carbon emissions. We introduce a taxonomy to
categorize the existing work as Monitoring, Estimation, or Black-Box
approaches. We delve deeper into the tools and compare them across different
dimensions and granularity - for example, whether their measurement encompasses
energy and carbon emissions and the components considered (like CPU, GPU, RAM,
etc.). We present our observations on the practical use (component wise
consolidation of approaches) as well as the challenges that we have identified
across the current state-of-the-art. As we start an initiative to address these
challenges, we emphasize active collaboration across the community in this
important field.

</details>


### [330] [Mapping NVD Records to Their VFCs: How Hard is it?](https://arxiv.org/abs/2506.09702)
*Huu Hung Nguyen,Duc Manh Tran,Yiran Cheng,Thanh Le-Cong,Hong Jin Kang,Ratnadira Widyasari,Shar Lwin Khin,Ouh Eng Lieh,Ting Zhang,David Lo*

Main category: cs.SE

TL;DR: The study investigates the feasibility of mapping National Vulnerability Database (NVD) entries to vulnerability-fixing commits (VFCs), achieving success in pipelines leveraging Git references but highlighting challenges elsewhere.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of mapping NVD records to their corresponding vulnerability-fixing commits for improving vulnerability analysis.

Method: An empirical approach was followed using Git references, external security databases, and GitHub repositories, combined with an automated pipeline for mapping NVD records.

Result: The study achieved 11.3% coverage, extracting VFCs for thousands of NVD records at high precision, mainly using Git references, while highlighting significant gaps in mapping.

Conclusion: Mapping VFCs is feasible but highly reliant on Git references; a majority of records remain unmapped, presenting potential for future improvements in vulnerability datasets and tools.

Abstract: Mapping National Vulnerability Database (NVD) records to vulnerability-fixing
commits (VFCs) is crucial for vulnerability analysis but challenging due to
sparse explicit links in NVD references.This study explores this mapping's
feasibility through an empirical approach. Manual analysis of NVD references
showed Git references enable over 86% success, while non-Git references achieve
under 14%. Using these findings, we built an automated pipeline extracting
31,942 VFCs from 20,360 NVD records (8.7% of 235,341) with 87% precision,
mainly from Git references. To fill gaps, we mined six external security
databases, yielding 29,254 VFCs for 18,985 records (8.1%) at 88.4% precision,
and GitHub repositories, adding 3,686 VFCs for 2,795 records (1.2%) at 73%
precision. Combining these, we mapped 26,710 unique records (11.3% coverage)
from 7,634 projects, with overlap between NVD and external databases, plus
unique GitHub contributions. Despite success with Git references, 88.7% of
records remain unmapped, highlighting the difficulty without Git links. This
study offers insights for enhancing vulnerability datasets and guiding future
automated security research.

</details>


### [331] [A First Look at Bugs in LLM Inference Engines](https://arxiv.org/abs/2506.09713)
*Mugeng Liu,Siqi Zhong,Weichen Bi,Yixuan Zhang,Zhiyang Chen,Zhenpeng Chen,Xuanzhe Liu,Yun Ma*

Main category: cs.SE

TL;DR: This paper studies bugs in Large Language Model (LLM) inference engines, analyzing 929 real-world bugs to identify patterns and propose solutions.


<details>
  <summary>Details</summary>
Motivation: LLM inference engines are crucial for modern AI infrastructure but are prone to bugs due to high resource demands and cross-platform complexities. A systematic study of these bugs has been missing.

Method: The authors analyzed 929 bugs mined from the official repositories of 5 popular LLM inference engines using a rigorous open coding process.

Result: The study identified six major bug symptoms and a taxonomy of 28 root causes, highlighting key challenges in bug detection and resolution.

Conclusion: The insights offer actionable implications for researchers, inference engine vendors, and app developers to improve bug detection and handling.

Abstract: Large language model-specific inference engines (in short as \emph{LLM
inference engines}) have become a fundamental component of modern AI
infrastructure, enabling the deployment of LLM-powered applications (LLM apps)
across cloud and local devices. Despite their critical role, LLM inference
engines are prone to bugs due to the immense resource demands of LLMs and the
complexities of cross-platform compatibility. However, a systematic
understanding of these bugs remains lacking. To bridge this gap, we present the
first empirical study on bugs in LLM inference engines. We mine official
repositories of 5 widely adopted LLM inference engines, constructing a
comprehensive dataset of 929 real-world bugs. Through a rigorous open coding
process, we analyze these bugs to uncover their symptoms, root causes, and
commonality. Our findings reveal six major bug symptoms and a taxonomy of 28
root causes, shedding light on the key challenges in bug detection and location
within LLM inference engines. Based on these insights, we propose a series of
actionable implications for researchers, inference engine vendors, and LLM app
developers.

</details>


### [332] [Towards Bridging Formal Methods and Human Interpretability](https://arxiv.org/abs/2506.09759)
*Abhijit Paul,Proma Chowdhury,Kazi Sakib*

Main category: cs.SE

TL;DR: The paper investigates how humans comprehend Labeled Transition Systems (LTS) and identifies meaningful metrics to represent this understanding.


<details>
  <summary>Details</summary>
Motivation: The research is motivated by the lack of prior studies on human understanding of LTS designs, which are crucial for debugging, identifying inconsistencies, and validating systems.

Method: The authors identified 7 key metrics from software engineering and graph theory, created a dataset of 148 LTS designs, performed 324 paired comparisons, and ranked the designs using the Bradley-Terry model followed by Kendall's Tau correlation analysis.

Result: The study concluded that Albin complexity, state space size, cyclomatic complexity, and redundancy metrics most accurately correlate with human comprehension. Applying the Albin complexity metric in a design repair tool reduced comprehension time by 39%.

Conclusion: Metrics that emphasize human factors can meaningfully enhance the interpretability of LTS designs, improving debugging and validation processes.

Abstract: Labeled Transition Systems (LTS) are integral to model checking and design
repair tools. System engineers frequently examine LTS designs during model
checking or design repair to debug, identify inconsistencies, and validate
system behavior. Despite LTS's significance, no prior research has examined
human comprehension of these designs. To address this, we draw on traditional
software engineering and graph theory, identifying 7 key metrics: cyclomatic
complexity, state space size, average branching factor, maximum depth, Albin
complexity, modularity, and redundancy. We created a dataset of 148 LTS
designs, sampling 48 for 324 paired comparisons, and ranked them using the
Bradley-Terry model. Through Kendall's Tau correlation analysis, we found that
Albin complexity ($\tau = 0.444$), state space size ($\tau = 0.420$),
cyclomatic complexity ($\tau = 0.366$), and redundancy ($\tau = 0.315$) most
accurately reflect human comprehension of LTS designs. To showcase the metrics'
utility, we applied the Albin complexity metric within the Fortis design repair
tool, ranking system redesigns. This ranking reduced annotators' comprehension
time by 39\%, suggesting that metrics emphasizing human factors can enhance
formal design interpretability.

</details>


### [333] [variability.dev: Towards an Online Toolbox for Feature Modeling](https://arxiv.org/abs/2506.09845)
*Tobias Heß,Lukas Ostheimer,Tobias Betz,Simon Karrer,Tim Jannik Schmidt,Pierre Coquet,Sean Semmler,Thomas Thüm*

Main category: cs.SE

TL;DR: The paper introduces an online toolbox called variability.dev for feature modeling, addressing issues with existing tools like lack of functionality and offline requirements.


<details>
  <summary>Details</summary>
Motivation: The motivation is to offer a modern, online-compatible solution for feature modeling since existing tools are limited or outdated.

Method: Built an online platform leveraging FeatureIDE for collaboration, editing, and configuration of feature models.

Result: The toolbox successfully provides an online editor and configurator for feature modeling that supports collaboration.

Conclusion: Variability.dev is positioned as a promising alternative to existing solutions, enhancing accessibility and functionality in feature modeling.

Abstract: The emergence of feature models as the default to model the variability in
configurable systems fosters a rich diversity in applications, application
domains, and perspectives. Independent of their domain, modelers require to
open, view, edit, transform, save, and configure models as well as to
collaborate with others. However, at the time of writing, the top five results
when googling ``Online Editor Feature Model'' point to editors that either have
minimal functionality, are unmaintained or defunct, or require an offline
installation, such as FeatureIDE. In this work we present a preview of our
in-development online toolbox for feature modeling, variability.dev. In
particular, we showcase our collaborative feature-model editor and our online
configurator both of which are built on top of the FeatureIDE library.

</details>


### [334] [Stakeholder Participation for Responsible AI Development: Disconnects Between Guidance and Current Practice](https://arxiv.org/abs/2506.09873)
*Emma Kallina,Thomas Bohné,Jat Singh*

Main category: cs.SE

TL;DR: The paper investigates stakeholder involvement (SHI) in AI development to evaluate its effectiveness in achieving responsible AI (rAI), contrasting its commercial use and proposing improvements.


<details>
  <summary>Details</summary>
Motivation: To understand the role and alignment of stakeholder involvement (SHI) in promoting rAI, especially given its different applications in commercial software development.

Method: The authors analyzed 56 rAI guidance documents, conducted an online survey with 130 AI practitioners, and held 10 semi-structured interviews.

Result: The study found that SHI in commercial settings is driven by commercial priorities like customer value and compliance, with limited contribution to rAI goals.

Conclusion: Existing SHI practices do not sufficiently support rAI development, and the authors recommend targeted interventions and further research to better align SHI with rAI objectives.

Abstract: Responsible AI (rAI) guidance increasingly promotes stakeholder involvement
(SHI) during AI development. At the same time, SHI is already common in
commercial software development, but with potentially different foci. This
study clarifies the extent to which established SHI practices are able to
contribute to rAI efforts as well as potential disconnects -- essential
insights to inform and tailor future interventions that further shift industry
practice towards rAI efforts. First, we analysed 56 rAI guidance documents to
identify why SHI is recommended (i.e. its expected benefits for rAI) and
uncovered goals such as redistributing power, improving socio-technical
understandings, anticipating risks, and enhancing public oversight. To
understand why and how SHI is currently practised in commercial settings, we
then conducted an online survey (n=130) and semi-structured interviews (n=10)
with AI practitioners. Our findings reveal that SHI in practice is primarily
driven by commercial priorities (e.g. customer value, compliance) and several
factors currently discourage more rAI-aligned SHI practices. This suggests that
established SHI practices are largely not contributing to rAI efforts. To
address this disconnect, we propose interventions and research opportunities to
advance rAI development in practice.

</details>


### [335] [Assessing a Safety Case: Bottom-up Guidance for Claims and Evidence Evaluation](https://arxiv.org/abs/2506.09929)
*Scott Schnelle,Francesca Favaro,Laura Fraade-Blanar,David Wichner,Holland Broce,Justin Miranda*

Main category: cs.SE

TL;DR: The paper develops a method to evaluate the credibility of safety cases for Automated Driving Systems (ADS) by assessing the support for each claim and the quality of evidence.


<details>
  <summary>Details</summary>
Motivation: Advancing Automated Driving Systems requires robust safety assurance frameworks to establish public trust. This paper addresses the need for credibility evaluation of safety cases as a key element.

Method: The authors propose an assessment framework that evaluates claim support (procedural and implementation) and evidence status. It includes scoring strategies, evaluation guidelines, and considerations for governance and continual improvement.

Result: The presented framework provides a systematic way to assess the credibility of safety cases, including scoring tables and evaluation guidelines for both claims and evidence.

Conclusion: This work advances the discourse on ADS safety assurance by offering a starting point for credible safety case evaluations, promoting responsible ADS integration into society.

Abstract: As Automated Driving Systems (ADS) technology advances, ensuring safety and
public trust requires robust assurance frameworks, with safety cases emerging
as a critical tool toward such a goal. This paper explores an approach to
assess how a safety case is supported by its claims and evidence, toward
establishing credibility for the overall case. Starting from a description of
the building blocks of a safety case (claims, evidence, and optional
format-dependent entries), this paper delves into the assessment of support of
each claim through the provided evidence. Two domains of assessment are
outlined for each claim: procedural support (formalizing process specification)
and implementation support (demonstrating process application). Additionally,
an assessment of evidence status is also undertaken, independently from the
claims support. Scoring strategies and evaluation guidelines are provided,
including detailed scoring tables for claim support and evidence status
assessment. The paper further discusses governance, continual improvement, and
timing considerations for safety case assessments. Reporting of results and
findings is contextualized within its primary use for internal decision-making
on continual improvement efforts. The presented approach builds on state of the
art auditing practices, but specifically tackles the question of judging the
credibility of a safety case. While not conclusive on its own, it provides a
starting point toward a comprehensive "Case Credibility Assessment" (CCA),
starting from the evaluation of the support for each claim (individually and in
aggregate), as well as every piece of evidence provided. By delving into the
technical intricacies of ADS safety cases, this work contributes to the ongoing
discourse on safety assurance and aims to facilitate the responsible
integration of ADS technology into society.

</details>


### [336] [Microservices and Real-Time Processing in Retail IT: A Review of Open-Source Toolchains and Deployment Strategies](https://arxiv.org/abs/2506.09938)
*Aaditaa Vashisht,Rekha B S*

Main category: cs.SE

TL;DR: The paper reviews how event-driven, microservices architectures (Kafka, Spring Boot, MongoDB, Kubernetes) transform retail/financial systems for scalability and real-time analytics.


<details>
  <summary>Details</summary>
Motivation: Digital transformation is driving the need for real-time, scalable systems to optimize operations and customer experience in the retail and financial sectors.

Method: Utilized systematic literature review of academic papers, industry reports, and technical white papers to synthesize themes and strategies.

Result: Technologies like Kafka and Spring Boot enable real-time analytics and fraud detection, while MongoDB on Kubernetes ensures fault tolerance. Kubernetes automates scaling and deployment.

Conclusion: These insights aid practitioners in designing scalable systems and suggest research and educational opportunities in modern system architectures.

Abstract: With the rapid pace of digital transformation, the retail industry is
increasingly depending on real-time, scalable, and resilient systems to manage
financial transactions, analyze customer behavior, and streamline order
processing. This literature review explores how modern event-driven and
microservices-based architectures, particularly those leveraging Apache Kafka,
Spring Boot, MongoDB, and Kubernetes are transforming retail and financial
systems. By systematically reviewing academic publications, technical white
papers, and industry reports from recent years, this study synthesizes key
themes and implementation strategies. The analysis reveals that technologies
like Kafka and Spring Boot are instrumental in building low-latency,
event-driven applications that support real-time analytics and fraud detection,
while MongoDB, when deployed on Kubernetes, ensures fault tolerance and high
availability in inventory and transaction systems. Kubernetes itself plays a
crucial role in automating deployment and scaling of microservices. These
findings provide valuable insights for industry practitioners aiming to design
scalable infrastructures, identify research opportunities in hybrid deployment
models, and offer educators a foundation to integrate modern system
architectures into professional and technical communication training.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [337] [How attention simplifies mental representations for planning](https://arxiv.org/abs/2506.09520)
*Jason da Silva Castanheira,Nicholas Shea,Stephen M. Fleming*

Main category: q-bio.NC

TL;DR: The paper explores how spatial attention influences planning and decision-making in virtual maze navigation, finding that attention affects task representation and performance differently across individuals.


<details>
  <summary>Details</summary>
Motivation: To understand how humans efficiently and flexibly balance cognitive resource constraints, perception, and planning in adapting to novel environments and solving tasks.

Method: Using virtual maze navigation as a test environment, the study analyzes how spatial attention determines the parts of a maze that come into subjective awareness, thus simplifying mental representations used for planning.

Result: The study finds that spatial proximity and natural attentional contours (such as lateralisation) significantly influence task representation, making planning easier. Furthermore, individual variability in attentional effects explains differences in behavior and task performance.

Conclusion: The research integrates visuospatial attention mechanisms into computational models of decision-making, bridging perception and planning insights to better understand individual differences in environmental task representations.

Abstract: Human planning is efficient -- it frugally deploys limited cognitive
resources to accomplish difficult tasks -- and flexible -- adapting to novel
problems and environments. Computational approaches suggest that people
construct simplified mental representations of their environment, balancing the
complexity of a task representation with its utility. These models imply a
nested optimisation in which planning shapes perception, and perception shapes
planning -- but the perceptual and attentional mechanisms governing how this
interaction unfolds remain unknown. Here, we harness virtual maze navigation to
characterise how spatial attention controls which aspects of a task
representation enter subjective awareness and are available for planning. We
find that spatial proximity governs which aspects of a maze are available for
planning, and that when task-relevant information follows natural (lateralised)
contours of attention, people can more easily construct simplified and useful
maze representations. This influence of attention varies considerably across
individuals, explaining differences in people's task representations and
behaviour. Inspired by the 'spotlight of attention' analogy, we incorporate the
effects of visuospatial attention into existing computational accounts of
value-guided construal. Together, our work bridges computational perspectives
on perception and decision-making to better understand how individuals
represent their environments in aid of planning.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [338] [Know What You Don't Know: Uncertainty Calibration of Process Reward Models](https://arxiv.org/abs/2506.09338)
*Young-Jin Park,Kristjan Greenewald,Kaveh Alim,Hao Wang,Navid Azizan*

Main category: stat.ML

TL;DR: The paper proposes a new calibrated reward model and instance-adaptive scaling framework to improve inference-time cost optimization for large language models while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in addressing the poor calibration of process reward models (PRMs), which often overestimate success probabilities, leading to inefficiencies in scaling algorithms for large language models.

Method: The method involves calibrating PRMs using quantile regression to make success probability estimates more accurate. Additionally, these calibrated estimates are integrated into an instance-adaptive scaling (IAS) framework, which dynamically adjusts inference budgets based on estimated success likelihood.

Result: The results show that the calibration achieves small errors and outperforms baseline methods. This calibration is shown to be essential for the IAS framework, which reduces computation costs while retaining output accuracy.

Conclusion: The proposed combination of PRM calibration and IAS proves effective, enabling cost-efficient inference by dynamically adapting compute resources for confident and challenging tasks alike.

Abstract: Process reward models (PRMs) play a central role in guiding inference-time
scaling algorithms for large language models (LLMs). However, we observe that
even state-of-the-art PRMs can be poorly calibrated and often overestimate
success probabilities. To address this, we present a calibration approach,
performed via quantile regression, that adjusts PRM outputs to better align
with true success probabilities. Leveraging these calibrated success estimates
and their associated confidence bounds, we introduce an \emph{instance-adaptive
scaling} (IAS) framework that dynamically adjusts the inference budget based on
the estimated likelihood that a partial reasoning trajectory will yield a
correct final answer. Unlike conventional methods that allocate a fixed number
of reasoning trajectories per query, this approach successfully adapts to each
instance and reasoning step when using our calibrated PRMs. Experiments on
mathematical reasoning benchmarks show that (i) our PRM calibration method
successfully achieves small calibration error, outperforming the baseline
methods, (ii) calibration is crucial for enabling effective adaptive scaling,
and (iii) the proposed IAS strategy reduces inference costs while maintaining
final answer accuracy, utilizing less compute on more confident problems as
desired.

</details>


### [339] [Attention-Bayesian Hybrid Approach to Modular Multiple Particle Tracking](https://arxiv.org/abs/2506.09441)
*Piyush Mishra,Philippe Roudot*

Main category: stat.ML

TL;DR: This paper proposes a hybrid framework for particle tracking, integrating transformers and Bayesian filtering to improve accuracy and robustness in cluttered scenarios.


<details>
  <summary>Details</summary>
Motivation: To address challenges in multiple particle tracking in noisy, cluttered environments, where traditional methods struggle due to the rapid growth in trajectory hypotheses.

Method: A hybrid approach combines transformer self-attention for narrowing trajectory hypotheses and Bayesian filtering for reliability and interpretability, solving a label prediction problem to improve particle tracking.

Result: The framework improves tracking accuracy and robustness, especially in high-clutter scenarios prone to spurious detections.

Conclusion: The proposed hybrid framework effectively merges the strengths of transformers and Bayesian filtering, offering a robust solution for challenging multiple-particle tracking situations.

Abstract: Tracking multiple particles in noisy and cluttered scenes remains challenging
due to a combinatorial explosion of trajectory hypotheses, which scales
super-exponentially with the number of particles and frames. The transformer
architecture has shown a significant improvement in robustness against this
high combinatorial load. However, its performance still falls short of the
conventional Bayesian filtering approaches in scenarios presenting a reduced
set of trajectory hypothesis. This suggests that while transformers excel at
narrowing down possible associations, they may not be able to reach the
optimality of the Bayesian approach in locally sparse scenario. Hence, we
introduce a hybrid tracking framework that combines the ability of
self-attention to learn the underlying representation of particle behavior with
the reliability and interpretability of Bayesian filtering. We perform
trajectory-to-detection association by solving a label prediction problem,
using a transformer encoder to infer soft associations between detections
across frames. This prunes the hypothesis set, enabling efficient
multiple-particle tracking in Bayesian filtering framework. Our approach
demonstrates improved tracking accuracy and robustness against spurious
detections, offering a solution for high clutter multiple particle tracking
scenarios.

</details>


### [340] [LLM-Powered CPI Prediction Inference with Online Text Time Series](https://arxiv.org/abs/2506.09516)
*Yingying Fan,Jinchi Lv,Ao Sun,Yurou Wang*

Main category: stat.ML

TL;DR: This paper introduces LLM-CPI, a novel approach using large language models (LLMs) and high-frequency online text data to predict the Consumer Price Index (CPI), improving forecast accuracy compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Most existing CPI forecasting methods rely on low-frequency, survey-based data, which may not fully capture real-time trends. With the emergence of LLMs and their ability to process high-frequency text data, there is a growing opportunity to improve CPI predictions.

Method: The paper proposes a framework combining online text embeddings extracted using LDA and BERT with traditional CPI data. It employs a joint time series approach, where monthly CPI data is modeled using ARX and daily CPI prediction uses a VARX structure trained on LLM-generated CPI surrogates.

Result: The proposed LLM-CPI approach demonstrates superior performance in both simulated and real-world datasets, showing greater precision and practical utility compared to traditional survey-based prediction methods.

Conclusion: Incorporating high-frequency online text data and LLMs into CPI forecasting enhances prediction accuracy and showcases the utility of modern AI techniques for economic insights.

Abstract: Forecasting the Consumer Price Index (CPI) is an important yet challenging
task in economics, where most existing approaches rely on low-frequency,
survey-based data. With the recent advances of large language models (LLMs),
there is growing potential to leverage high-frequency online text data for
improved CPI prediction, an area still largely unexplored. This paper proposes
LLM-CPI, an LLM-based approach for CPI prediction inference incorporating
online text time series. We collect a large set of high-frequency online texts
from a popularly used Chinese social network site and employ LLMs such as
ChatGPT and the trained BERT models to construct continuous inflation labels
for posts that are related to inflation. Online text embeddings are extracted
via LDA and BERT. We develop a joint time series framework that combines
monthly CPI data with LLM-generated daily CPI surrogates. The monthly model
employs an ARX structure combining observed CPI data with text embeddings and
macroeconomic variables, while the daily model uses a VARX structure built on
LLM-generated CPI surrogates and text embeddings. We establish the asymptotic
properties of the method and provide two forms of constructed prediction
intervals. The finite-sample performance and practical advantages of LLM-CPI
are demonstrated through both simulation and real data examples.

</details>


### [341] [Evasion Attacks Against Bayesian Predictive Models](https://arxiv.org/abs/2506.09640)
*Pablo G. Arce,Roi Naveiro,David Ríos Insua*

Main category: stat.ML

TL;DR: The paper explores Bayesian predictive models' vulnerabilities to evasion attacks, proposing gradient-based attack methodologies.


<details>
  <summary>Details</summary>
Motivation: Research on adversarial machine learning has largely disregarded the susceptibility of Bayesian predictive models to attacks, which this paper aims to address.

Method: The authors introduced a general framework for optimal evasion attacks, focusing on perturbing point predictions and posterior predictive distributions, using gradient-based methods.

Result: They developed and tested the gradient-based adversarial attacks for Bayesian models, studying their properties in different computational setups.

Conclusion: The study broadens the understanding of adversarial vulnerabilities in Bayesian systems and provides methodological contributions for attack designs.

Abstract: There is an increasing interest in analyzing the behavior of machine learning
systems against adversarial attacks. However, most of the research in
adversarial machine learning has focused on studying weaknesses against evasion
or poisoning attacks to predictive models in classical setups, with the
susceptibility of Bayesian predictive models to attacks remaining
underexplored. This paper introduces a general methodology for designing
optimal evasion attacks against such models. We investigate two adversarial
objectives: perturbing specific point predictions and altering the entire
posterior predictive distribution. For both scenarios, we propose novel
gradient-based attacks and study their implementation and properties in various
computational setups.

</details>


### [342] [Scaling Laws for Uncertainty in Deep Learning](https://arxiv.org/abs/2506.09648)
*Mattia Rosso,Simone Rossi,Giulio Franzese,Markus Heinonen,Maurizio Filippone*

Main category: stat.ML

TL;DR: This paper identifies and empirically validates scaling laws for predictive uncertainties in deep learning, demonstrating that uncertainties follow predictable trends based on dataset and model sizes.


<details>
  <summary>Details</summary>
Motivation: The study was motivated by the emergence of scaling laws in deep learning, particularly in over-parameterized regimes, and aimed to test whether similar laws apply to predictive uncertainties.

Method: The researchers conducted empirical evaluations on vision and language tasks using approximate Bayesian inference and ensemble methods to measure predictive uncertainty across different data and model sizes.

Result: Scaling laws for predictive uncertainties were observed in both in-distribution and out-of-distribution scenarios, with findings showing epistemic uncertainty does not become negligible even with large datasets.

Conclusion: This work reinforces the practical utility of Bayesian approaches in deep learning, countering skepticism that large datasets eliminate the need for Bayes by showing the persistence of epistemic uncertainty.

Abstract: Deep learning has recently revealed the existence of scaling laws,
demonstrating that model performance follows predictable trends based on
dataset and model sizes. Inspired by these findings and fascinating phenomena
emerging in the over-parameterized regime, we examine a parallel direction: do
similar scaling laws govern predictive uncertainties in deep learning? In
identifiable parametric models, such scaling laws can be derived in a
straightforward manner by treating model parameters in a Bayesian way. In this
case, for example, we obtain $O(1/N)$ contraction rates for epistemic
uncertainty with respect to the number of data $N$. However, in
over-parameterized models, these guarantees do not hold, leading to largely
unexplored behaviors. In this work, we empirically show the existence of
scaling laws associated with various measures of predictive uncertainty with
respect to dataset and model sizes. Through experiments on vision and language
tasks, we observe such scaling laws for in- and out-of-distribution predictive
uncertainty estimated through popular approximate Bayesian inference and
ensemble methods. Besides the elegance of scaling laws and the practical
utility of extrapolating uncertainties to larger data or models, this work
provides strong evidence to dispel recurring skepticism against Bayesian
approaches: "In many applications of deep learning we have so much data
available: what do we need Bayes for?". Our findings show that "so much data"
is typically not enough to make epistemic uncertainty negligible.

</details>


### [343] [Assessing the Quality of Denoising Diffusion Models in Wasserstein Distance: Noisy Score and Optimal Bounds](https://arxiv.org/abs/2506.09681)
*Vahan Arsenyan,Elen Vardanyan,Arnak Dalalyan*

Main category: stat.ML

TL;DR: The paper explores the robustness of denoising diffusion probabilistic models (DDPMs) to noise in score evaluations and provides improved finite-sample guarantees with faster convergence rates.


<details>
  <summary>Details</summary>
Motivation: To understand and quantify the robustness of DDPMs to noisy score estimates and improve upon previous results regarding convergence rates.

Method: The authors empirically validate DDPMs' robustness to constant-variance noise and establish finite-sample guarantees in Wasserstein-2 distance with faster convergence rates.

Result: The analysis confirms the robustness of DDPMs and demonstrates that the achieved convergence rates are faster and optimal, matching those in the Gaussian case.

Conclusion: The findings enhance the theoretical understanding of DDPMs, providing key insights into their robustness and optimal performance under noisy score evaluations.

Abstract: Generative modeling aims to produce new random examples from an unknown
target distribution, given access to a finite collection of examples. Among the
leading approaches, denoising diffusion probabilistic models (DDPMs) construct
such examples by mapping a Brownian motion via a diffusion process driven by an
estimated score function. In this work, we first provide empirical evidence
that DDPMs are robust to constant-variance noise in the score evaluations. We
then establish finite-sample guarantees in Wasserstein-2 distance that exhibit
two key features: (i) they characterize and quantify the robustness of DDPMs to
noisy score estimates, and (ii) they achieve faster convergence rates than
previously known results. Furthermore, we observe that the obtained rates match
those known in the Gaussian case, implying their optimality.

</details>


### [344] [A Deep Generative Model for the Simulation of Discrete Karst Networks](https://arxiv.org/abs/2506.09832)
*Dany Lauzon,Julien Straubhaar,Philippe Renard*

Main category: stat.ML

TL;DR: The paper develops a novel method to simulate discrete karst networks using graph generative models based on deep learning approaches, combining GraphRNN for topology learning and G-DDPM for node features, achieving realistic and stochastic results.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenge of simulating discrete karst networks, which have complex physicochemical interactions tied to geological and hydrogeological conditions, by providing a novel computational approach that can handle this complexity effectively.

Method: The method involves two steps: 1) Using GraphRNN to learn and generate the topological structure of karst networks by sequentially building nodes and edges based on prior structures, and 2) Employing G-DDPM to generate the spatial properties and features of nodes, ensuring that the statistical properties are consistent with observed data.

Result: The approach was tested on real-world karst networks, where the generated subgraphs were compared with actual data using geometry and topology metrics, successfully demonstrating that the simulated networks are realistic and reflect the essential characteristics of the originals.

Conclusion: The work provides a powerful tool for the stochastic simulation of karst networks, aiding the study of associated physical processes (like flow and transport), and offers promising prospects for modeling diverse geological formations.

Abstract: The simulation of discrete karst networks presents a significant challenge
due to the complexity of the physicochemical processes occurring within various
geological and hydrogeological contexts over extended periods. This complex
interplay leads to a wide variety of karst network patterns, each intricately
linked to specific hydrogeological conditions. We explore a novel approach that
represents karst networks as graphs and applies graph generative models (deep
learning techniques) to capture the intricate nature of karst environments. In
this representation, nodes retain spatial information and properties, while
edges signify connections between nodes. Our generative process consists of two
main steps. First, we utilize graph recurrent neural networks (GraphRNN) to
learn the topological distribution of karst networks. GraphRNN decomposes the
graph simulation into a sequential generation of nodes and edges, informed by
previously generated structures. Second, we employ denoising diffusion
probabilistic models on graphs (G-DDPM) to learn node features (spatial
coordinates and other properties). G-DDPMs enable the generation of nodes
features on the graphs produced by the GraphRNN that adhere to the learned
statistical properties by sampling from the derived probability distribution,
ensuring that the generated graphs are realistic and capture the essential
features of the original data. We test our approach using real-world karst
networks and compare generated subgraphs with actual subgraphs from the
database, by using geometry and topology metrics. Our methodology allows
stochastic simulation of discrete karst networks across various types of
formations, a useful tool for studying the behavior of physical processes such
as flow and transport.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [345] [Genetic Transformer-Assisted Quantum Neural Networks for Optimal Circuit Design](https://arxiv.org/abs/2506.09205)
*Haiyan Wang*

Main category: quant-ph

TL;DR: The paper introduces a hybrid framework called Genetic Transformer Assisted Quantum Neural Networks (GTQNNs), combining a transformer with a quantum circuit and optimizing it using a genetic algorithm to improve performance on various benchmarks while using fewer gates.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of optimizing quantum neural networks to achieve competitive accuracy while minimizing gate usage, crucial for NISQ devices.

Method: Combines a transformer encoder to compact classical data, variational quantum circuits, and NSGA-II genetic algorithm for optimizing accuracy and gate count.

Result: GTQNNs outperform existing quantum models on benchmarks like Iris, Breast Cancer, MNIST, and Heart Disease, using fewer gates.

Conclusion: GTQNNs are efficient and well-suited for current NISQ devices, demonstrating strong performance with limited quantum resources.

Abstract: We introduce Genetic Transformer Assisted Quantum Neural Networks (GTQNNs), a
hybrid learning framework that combines a transformer encoder with a shallow
variational quantum circuit and automatically fine tunes the circuit via the
NSGA-II multi objective genetic algorithm. The transformer reduces
high-dimensional classical data to a compact, qubit sized representation, while
NSGA-II searches for Pareto optimal circuits that (i) maximize classification
accuracy and (ii) minimize primitive gate count an essential constraint for
noisy intermediate-scale quantum (NISQ) hardware. Experiments on four
benchmarks (Iris, Breast Cancer, MNIST, and Heart Disease) show that GTQNNs
match or exceed state of the art quantum models while requiring much fewer
gates for most cases. A hybrid Fisher information analysis further reveals that
the trained networks operate far from barren plateaus; the leading curvature
directions increasingly align with the quantum subspace as the qubit budget
grows, confirming that the transformer front end has effectively condensed the
data. Together, these results demonstrate that GTQNNs deliver competitive
performance with a quantum resource budget well suited to present-day NISQ
devices.

</details>


### [346] [Low-Level and NUMA-Aware Optimization for High-Performance Quantum Simulation](https://arxiv.org/abs/2506.09198)
*Ali Rezaei,Luc Jaulmes,Maria Bahna,Oliver Thomson Brown,Antonio Barbalace*

Main category: quant-ph

TL;DR: The paper presents scalable performance improvements in classical simulation of quantum circuits by implementing advanced hardware-specific optimizations in an open-source QuEST simulator extension.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance classical quantum circuit simulations for both algorithm development and hardware validation, addressing the lack of open-source and independently evaluative low-level optimized implementations.

Method: The study applies advanced low-level and NUMA-aware optimizations, such as memory allocation tuning, thread pinning, AVX-512 vectorization, and loop unrolling, to improve simulations on a single-node quantum simulator.

Result: The optimized simulator achieved significant speedups: 5.5-6.5x for single-qubit gates, 4.5x for two-qubit gates, 4x for Random Quantum Circuits (RQC), and 1.8x for Quantum Fourier Transform (QFT).

Conclusion: The approach proves that carefully engineered tuning methods can practically extend the capabilities of quantum simulators, bridging the gap toward noiseless quantum computing using current classical hardware.

Abstract: Scalable classical simulation of quantum circuits is crucial for advancing
both quantum algorithm development and hardware validation. In this work, we
focus on performance enhancements through meticulous low-level tuning on a
single-node system, thereby not only advancing the performance of classical
quantum simulations but also laying the groundwork for scalable, heterogeneous
implementations that may eventually bridge the gap toward noiseless quantum
computing. Although similar efforts in low-level tuning have been reported in
the literature, such implementations have not been released as open-source
software, thereby impeding independent evaluation and further development. We
introduce an open-source, high-performance extension to the QuEST simulator
that brings state-of-the-art low-level and NUMA optimizations to modern
computers. Our approach emphasizes locality-aware computation and incorporates
hardware-specific optimizations such as NUMA-aware memory allocation, thread
pinning, AVX-512 vectorization, aggressive loop unrolling, and explicit memory
prefetching. Experiments demonstrate significant speedups - 5.5-6.5x for
single-qubit gate operations, 4.5x for two-qubit gates, 4x for Random Quantum
Circuits (RQC), and 1.8x for Quantum Fourier Transform (QFT), demonstrating
that rigorous performance tuning can substantially extend the practical
simulation capacity of classical quantum simulators on current hardware.

</details>


### [347] [Devanagari Digit Recognition using Quantum Machine Learning](https://arxiv.org/abs/2506.09069)
*Sahaj Raj Malla*

Main category: quant-ph

TL;DR: This paper presents a hybrid quantum-classical model for Devanagari handwritten digit recognition, achieving superior accuracy, fewer parameters, and demonstrating the potential of quantum machine learning in low-resource language contexts.


<details>
  <summary>Details</summary>
Motivation: To address the challenges posed by Devanagari script's complexity and limited annotated datasets in handwritten digit recognition.

Method: The model combines a convolutional neural network (CNN) for spatial feature extraction with a 10-qubit variational quantum circuit (VQC) for classification.

Result: Achieved state-of-the-art test accuracy of 99.80%, test loss of 0.2893, and an average per-class F1-score of 0.9980 on the Devanagari Handwritten Character Dataset (DHCD), surpassing classical models.

Conclusion: This research establishes a new standard in regional script recognition using quantum machine learning, demonstrating its viability in low-resource, multilingual settings.

Abstract: Handwritten digit recognition in regional scripts, such as Devanagari, is
crucial for multilingual document digitization, educational tools, and the
preservation of cultural heritage. The script's complex structure and limited
annotated datasets pose significant challenges to conventional models. This
paper introduces the first hybrid quantum-classical architecture for Devanagari
handwritten digit recognition, combining a convolutional neural network (CNN)
for spatial feature extraction with a 10-qubit variational quantum circuit
(VQC) for quantum-enhanced classification. Trained and evaluated on the
Devanagari Handwritten Character Dataset (DHCD), the proposed model achieves a
state-of-the-art test accuracy for quantum implementation of 99.80% and a test
loss of 0.2893, with an average per-class F1-score of 0.9980. Compared to
equivalent classical CNNs, our model demonstrates superior accuracy with
significantly fewer parameters and enhanced robustness. By leveraging quantum
principles such as superposition and entanglement, this work establishes a
novel benchmark for regional script recognition, highlighting the promise of
quantum machine learning (QML) in real-world, low-resource language settings.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [348] [Knockoffs Inference under Privacy Constraints](https://arxiv.org/abs/2506.09690)
*Zhanrui Cai,Yingying Fan,Lan Gao*

Main category: stat.ME

TL;DR: The paper introduces a framework for integrating the Model-X knockoff variable selection method with differential privacy to ensure data privacy while maintaining false discovery rate (FDR) control.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of ensuring data privacy in the Model-X knockoff variable selection process, particularly given the complexity of generating knockoff variables under a model-free assumption.

Method: The authors propose a differential privacy knockoff (DP-knockoff) framework that incorporates privacy-preserving mechanisms into the original Model-X knockoff procedure, while analyzing the trade-offs between privacy noise and statistical power.

Result: The method guarantees robust privacy protection and exact FDR control, with conditions established for minimal impact on statistical power due to added noise.

Conclusion: The DP-knockoff method is effective for variable selection with FDR control while safeguarding data privacy, demonstrated across various settings including low and high dimensions.

Abstract: Model-X knockoff framework offers a model-free variable selection method that
ensures finite sample false discovery rate (FDR) control. However, the
complexity of generating knockoff variables, coupled with the model-free
assumption, presents significant challenges for protecting data privacy in this
context. In this paper, we propose a comprehensive framework for knockoff
inference within the differential privacy paradigm. Our proposed method
guarantees robust privacy protection while preserving the exact FDR control
entailed by the original model-X knockoff procedure. We further conduct power
analysis and establish sufficient conditions under which the noise added for
privacy preservation does not asymptotically compromise power. Through various
applications, we demonstrate that the differential privacy knockoff
(DP-knockoff) method can be effectively utilized to safeguard privacy during
variable selection with FDR control in both low and high dimensional settings.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [349] [Surrogate models to optimize plasma assisted atomic layer deposition in high aspect ratio features](https://arxiv.org/abs/2506.09313)
*Angel Yanguas-Gil,Jeffrey W. Elam*

Main category: cond-mat.mtrl-sci

TL;DR: This study utilizes artificial neural networks and machine learning to optimize plasma enhanced atomic layer deposition (PEALD) in high aspect ratio features, achieving accuracy and efficiency in predicting process parameters.


<details>
  <summary>Details</summary>
Motivation: Surface recombination in plasma-based processes like PEALD often results in prolonged exposure times for achieving conformality in nanostructures such as high aspect ratio vias. This paper intends to address this challenge.

Method: Using a synthetic dataset from PEALD simulations, artificial neural networks were trained to predict saturation times and assess plasma-surface interaction dominance by analyzing cross section thickness data from undersaturated experiments.

Result: The models successfully predicted saturation times within 10% error and determined the dominance of surface recombination with 99% accuracy, based on minimal experimental data.

Conclusion: Machine learning offers a robust method to optimize PEALD processes in microelectronics, with potential applications in atomic layer etching and complex nanostructures.

Abstract: In this work we explore surrogate models to optimize plasma enhanced atomic
layer deposition (PEALD) in high aspect ratio features. In plasma-based
processes such as PEALD and atomic layer etching, surface recombination can
dominate the reactivity of plasma species with the surface, which can lead to
unfeasibly long exposure times to achieve full conformality inside
nanostructures like high aspect ratio vias. Using a synthetic dataset based on
simulations of PEALD, we train artificial neural networks to predict saturation
times based on cross section thickness data obtained for partially coated
conditions. The results obtained show that just two experiments in
undersaturated conditions contain enough information to predict saturation
times within 10% of the ground truth. A surrogate model trained to determine
whether surface recombination dominates the plasma-surface interactions in a
PEALD process achieves 99% accuracy. This demonstrates that machine learning
can provide a new pathway to accelerate the optimization of PEALD processes in
areas such as microelectronics. Our approach can be easily extended to atomic
layer etching and more complex structures.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [350] [Advancing Exchange Rate Forecasting: Leveraging Machine Learning and AI for Enhanced Accuracy in Global Financial Markets](https://arxiv.org/abs/2506.09851)
*Md. Yeasin Rahat,Rajan Das Gupta,Nur Raisa Rahman,Sudipto Roy Pritom,Samiur Rahman Shakir,Md Imrul Hasan Showmick,Md. Jakir Hossen*

Main category: q-fin.ST

TL;DR: This paper develops machine learning models, particularly LSTM, for accurate USD/BDT forex rate predictions, achieving high accuracy and outperforming traditional methods like ARIMA.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy of foreign exchange rate predictions for USD/BDT, which are crucial for global trade, investments, and economic stability.

Method: Used historical USD/BDT data (2018–2023) and applied deep learning models such as LSTM for forecasting and Gradient Boosting Classifier for directional prediction, with additional analysis of historical trends and daily volatility.

Result: LSTM achieved exceptional forecasting accuracy (99.449%) with lower RMSE (0.9858) compared to ARIMA (1.342). GBC produced a 40.82% profitable trade rate in backtesting but resulted in a net loss of $20,653.25 over 49 trades.

Conclusion: Deep learning models, like LSTM, show strong potential in forex forecasting, providing useful tools for traders and policymakers, though further enhancements like sentiment analysis may be required in future work.

Abstract: The prediction of foreign exchange rates, such as the US Dollar (USD) to
Bangladeshi Taka (BDT), plays a pivotal role in global financial markets,
influencing trade, investments, and economic stability. This study leverages
historical USD/BDT exchange rate data from 2018 to 2023, sourced from Yahoo
Finance, to develop advanced machine learning models for accurate forecasting.
A Long Short-Term Memory (LSTM) neural network is employed, achieving an
exceptional accuracy of 99.449%, a Root Mean Square Error (RMSE) of 0.9858, and
a test loss of 0.8523, significantly outperforming traditional methods like
ARIMA (RMSE 1.342). Additionally, a Gradient Boosting Classifier (GBC) is
applied for directional prediction, with backtesting on a $10,000 initial
capital revealing a 40.82% profitable trade rate, though resulting in a net
loss of $20,653.25 over 49 trades. The study analyzes historical trends,
showing a decline in BDT/USD rates from 0.012 to 0.009, and incorporates
normalized daily returns to capture volatility. These findings highlight the
potential of deep learning in forex forecasting, offering traders and
policymakers robust tools to mitigate risks. Future work could integrate
sentiment analysis and real-time economic indicators to further enhance model
adaptability in volatile markets.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [351] [A theoretical basis for model collapse in recursive training](https://arxiv.org/abs/2506.09401)
*Vivek Shripad Borkar*

Main category: math.PR

TL;DR: This paper explores recursive training in generative models and identifies two asymptotic behaviors based on the presence of external sample sources.


<details>
  <summary>Details</summary>
Motivation: To investigate the impact of recursive training in generative models and the role of external samples in preventing distribution collapse.

Method: Theoretical analysis of the generative model's asymptotic behaviors with and without external sample contribution.

Result: Two distinct asymptotic behaviors are revealed: collapse occurs without external samples, while another stable behavior may emerge with even minor external sampling.

Conclusion: External sample sources, even minimal, can influence the long-term stability of generative model's simulated distributions during recursive training.

Abstract: It is known that recursive training from generative models can lead to the so
called `collapse' of the simulated probability distribution. This note shows
that one in fact gets two different asymptotic behaviours depending on whether
an external source, howsoever minor, is also contributing samples.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [352] [A Multi-Armed Bandit Framework for Online Optimisation in Green Integrated Terrestrial and Non-Terrestrial Networks](https://arxiv.org/abs/2506.09268)
*Henri Alam,Antonio de Domenico,Tareq Si Salem,Florian Kaltenberger*

Main category: cs.NI

TL;DR: The paper introduces a framework leveraging NTNs to optimize network performance, including energy efficiency and capacity, through real-time adjustments in TN-NTN architectures.


<details>
  <summary>Details</summary>
Motivation: By addressing growing concerns over terrestrial network densification, the paper aims to explore NTNs' underutilized potential in easing load and promoting energy-efficient operations.

Method: The proposed online optimization framework utilizes multi-armed bandit techniques and the BCOMD algorithm to dynamically manage bandwidth allocation, UE association, and MBS shutdown for integrated TN-NTN systems.

Result: Simulations show up to 19% throughput gains, 5% energy savings during low traffic, and significant improvements in addressing UE dissatisfaction during peak periods compared to 3GPP guidelines.

Conclusion: Integrating NTNs with adaptive optimization can enhance network sustainability, balancing efficiency and user satisfaction, showing promise beyond conventional systems.

Abstract: Integrated terrestrial and non-terrestrial network (TN-NTN) architectures
offer a promising solution for expanding coverage and improving capacity for
the network. While non-terrestrial networks (NTNs) are primarily exploited for
these specific reasons, their role in alleviating terrestrial network (TN) load
and enabling energy-efficient operation has received comparatively less
attention. In light of growing concerns associated with the densification of
terrestrial deployments, this work aims to explore the potential of NTNs in
supporting a more sustainable network. In this paper, we propose a novel online
optimisation framework for integrated TN-NTN architectures, built on a
multi-armed bandit (MAB) formulation and leveraging the Bandit-feedback
Constrained Online Mirror Descent (BCOMD) algorithm. Our approach adaptively
optimises key system parameters--including bandwidth allocation, user equipment
(UE) association, and macro base station (MBS) shutdown--to balance network
capacity and energy efficiency in real time. Extensive system-level simulations
over a 24-hour period show that our framework significantly reduces the
proportion of unsatisfied UEs during peak hours and achieves up to 19%
throughput gains and 5% energy savings in low-traffic periods, outperforming
standard network settings following 3GPP recommendations.

</details>


### [353] [Real-Time Network Traffic Forecasting with Missing Data: A Generative Model Approach](https://arxiv.org/abs/2506.09647)
*Lei Deng,Wenhan Xu,Jingwei Li,Danny H. K. Tsang*

Main category: cs.NI

TL;DR: This paper introduces a generative model-based solution for real-time network traffic forecasting, handling missing data efficiently and yielding accurate and fast predictions.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of real-time network traffic forecasting in scenarios where data is often incomplete, which is problematic for existing methods.

Method: The authors model the forecasting as a tensor completion problem and leverage a pre-trained generative model. Optimization is performed on the latent representation rather than the tensor itself, simplifying computations and enabling real-time predictions.

Result: This method achieves accurate forecasts in under 100 ms with minimal error (MAE < 0.002), demonstrated on the Abilene dataset.

Conclusion: The proposed approach handles missing data effectively, ensures both accuracy and speed in network traffic forecasting, and introduces theoretical guarantees for recovery.

Abstract: Real-time network traffic forecasting is crucial for network management and
early resource allocation. Existing network traffic forecasting approaches
operate under the assumption that the network traffic data is fully observed.
However, in practical scenarios, the collected data are often incomplete due to
various human and natural factors. In this paper, we propose a generative model
approach for real-time network traffic forecasting with missing data. Firstly,
we model the network traffic forecasting task as a tensor completion problem.
Secondly, we incorporate a pre-trained generative model to achieve the low-rank
structure commonly associated with tensor completion. The generative model
effectively captures the intrinsic low-rank structure of network traffic data
during pre-training and enables the mapping from a compact latent
representation to the tensor space. Thirdly, rather than directly optimizing
the high-dimensional tensor, we optimize its latent representation, which
simplifies the optimization process and enables real-time forecasting. We also
establish a theoretical recovery guarantee that quantifies the error bound of
the proposed approach. Experiments on real-world datasets demonstrate that our
approach achieves accurate network traffic forecasting within 100 ms, with a
mean absolute error (MAE) below 0.002, as validated on the Abilene dataset.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [354] [Estimating Visceral Adiposity from Wrist-Worn Accelerometry](https://arxiv.org/abs/2506.09167)
*James R. Williamson,Andrew Alini,Brian A. Telfer,Adam W. Potter,Karl E. Friedl*

Main category: eess.SP

TL;DR: This study investigates how physical activity (PA) correlates with visceral adipose tissue (VAT) by analyzing accelerometry and demographic data, achieving high accuracy in VAT estimation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore the strong link between VAT, metabolic health risks (e.g., diabetes, insulin resistance), and physical activity in an accessible way, as direct VAT measurements require advanced imaging.

Method: Two methods were employed: (1) engineered features (e.g., gait, sleep movement) processed via ridge regression and (2) deep neural networks with transformers analyzing accelerometry data to estimate VAT. Both approaches used subject demographic data for better accuracy.

Result: Combining both estimation methods achieved a high correlation (r=0.86) in VAT predictions, demonstrating their reliability in deriving VAT estimates from PA data.

Conclusion: The study highlights the potential to use accelerometry and machine learning to estimate VAT, linking physical activity with metabolic health risks.

Abstract: Visceral adipose tissue (VAT) is a key marker of both metabolic health and
habitual physical activity (PA). Excess VAT is highly correlated with type 2
diabetes and insulin resistance. The mechanistic basis for this pathophysiology
relates to overloading the liver with fatty acids. VAT is also a highly labile
fat depot, with increased turnover stimulated by catecholamines during
exercise. VAT can be measured with sophisticated imaging technologies, but can
also be inferred directly from PA. We tested this relationship using National
Health and Nutrition Examination Survey (NHANES) data from 2011-2014, for
individuals aged 20-60 years with 7 days of accelerometry data (n=2,456 men;
2,427 women) [1]. Two approaches were used for estimating VAT from activity.
The first used engineered features based on movements during gait and sleep,
and then ridge regression to map summary statistics of these features into a
VAT estimate. The second approach used deep neural networks trained on 24 hours
of continuous accelerometry. A foundation model first mapped each 10s frame
into a high-dimensional feature vector. A transformer model then mapped each
day's feature vector time series into a VAT estimate, which were averaged over
multiple days. For both approaches, the most accurate estimates were obtained
with the addition of covariate information about subject demographics and body
measurements. The best performance was obtained by combining the two
approaches, resulting in VAT estimates with correlations of r=0.86. These
findings demonstrate a strong relationship between PA and VAT and, by
extension, between PA and metabolic health risks.

</details>


### [355] [Integration of Contrastive Predictive Coding and Spiking Neural Networks](https://arxiv.org/abs/2506.09194)
*Emirhan Bilgiç,Neslihan Serap Şengör,Namık Berk Yalabık,Yavuz Selim İşler,Aykut Görkem Gelen,Rahmi Elibol*

Main category: eess.SP

TL;DR: This study integrates Contrastive Predictive Coding (CPC) with Spiking Neural Networks (SNN) to create a biologically plausible predictive coding model, achieving high classification rates on the MNIST dataset.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between computational and biological models by integrating CPC with SNN, thereby enhancing biological plausibility in predictive coding.

Method: The researchers integrated CPC to learn predictive data structures with SNN for simulating biological neural processes and tested the model on the MNIST dataset.

Result: The proposed model achieved high accuracy in distinguishing positive sequential samples from negative non-sequential samples.

Conclusion: CPC can be effectively merged with SNN for tasks like classification and encoding, advancing the development of biologically plausible computational systems.

Abstract: This study examines the integration of Contrastive Predictive Coding (CPC)
with Spiking Neural Networks (SNN). While CPC learns the predictive structure
of data to generate meaningful representations, SNN mimics the computational
processes of biological neural systems over time. In this study, the goal is to
develop a predictive coding model with greater biological plausibility by
processing inputs and outputs in a spike-based system. The proposed model was
tested on the MNIST dataset and achieved a high classification rate in
distinguishing positive sequential samples from non-sequential negative
samples. The study demonstrates that CPC can be effectively combined with SNN,
showing that an SNN trained for classification tasks can also function as an
encoding mechanism. Project codes and detailed results can be accessed on our
GitHub page: https://github.com/vnd-ogrenme/ongorusel-kodlama/tree/main/CPC_SNN

</details>


### [356] [Graph Attention-based Decentralized Actor-Critic for Dual-Objective Control of Multi-UAV Swarms](https://arxiv.org/abs/2506.09195)
*Haoran Peng,Ying-Jun Angela Zhang*

Main category: eess.SP

TL;DR: The paper introduces a Graph Attention-based Decentralized Actor-Critic (GADC) method to optimize service coverage and battery lifetime for multi-UAV systems, showing superior performance compared to state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the dual challenge of maximizing service coverage while extending battery life in multi-UAV systems, which are critical for efficient aerial operations.

Method: The authors propose GADC, which incorporates a graph attention network for local observations and an actor-double-critic network to manage dual optimization policies, along with a KL divergence factor to balance trade-offs.

Result: Comprehensive benchmarking and testing, including experiments in both ideal and realistic environments, show GADC's improved scalability and efficiency compared to existing methods.

Conclusion: The study concludes that the GADC approach effectively optimizes the dual objectives of service coverage and battery lifetime in multi-UAV systems and outperforms contemporary approaches in diverse conditions.

Abstract: This research focuses on optimizing multi-UAV systems with dual objectives:
maximizing service coverage as the primary goal while extending battery
lifetime as the secondary objective. We propose a Graph Attention-based
Decentralized Actor-Critic (GADC) to optimize the dual objectives. The proposed
approach leverages a graph attention network to process UAVs' limited local
observation and reduce the dimension of the environment states. Subsequently,
an actor-double-critic network is developed to manage dual policies for joint
objective optimization. The proposed GADC uses a Kullback-Leibler (KL)
divergence factor to balance the tradeoff between coverage performance and
battery lifetime in the multi-UAV system. We assess the scalability and
efficiency of GADC through comprehensive benchmarking against state-of-the-art
methods, considering both theory and experimental aspects. Extensive testing in
both ideal settings and NVIDIA Sionna's realistic ray tracing environment
demonstrates GADC's superior performance.

</details>


### [357] [AI-Driven SEEG Channel Ranking for Epileptogenic Zone Localization](https://arxiv.org/abs/2506.09255)
*Saeed Hashemi,Genchang Peng,Mehrdad Nourani,Omar Nofal,Jay Harvey*

Main category: eess.SP

TL;DR: The paper presents a machine learning method leveraging XGBoost and SHAP to rank impactful SEEG channels for epilepsy surgery, validated on data from five patients.


<details>
  <summary>Details</summary>
Motivation: Analyzing SEEG signals manually is inefficient and time-consuming, necessitating automated approaches for ranking impactful channels in epilepsy evaluations.

Method: The approach uses XGBoost for classification of SEEG channels during seizures and SHAP scores to rank channels. Additionally, a channel extension strategy broadens the search space beyond clinician-selected zones.

Result: The method was applied to SEEG data from five patients and demonstrated promising results in terms of accuracy, consistency, and explainability.

Conclusion: Machine learning combined with clinician input can streamline SEEG channel evaluation for epilepsy surgery, offering a reliable and explainable alternative to manual visual inspection.

Abstract: Stereo-electroencephalography (SEEG) is an invasive technique to implant
depth electrodes and collect data for pre-surgery evaluation. Visual inspection
of signals recorded from hundreds of channels is time consuming and
inefficient. We propose a machine learning approach to rank the impactful
channels by incorporating clinician's selection and computational finding. A
classification model using XGBoost is trained to learn the discriminative
features of each channel during ictal periods. Then, the SHapley Additive
exPlanations (SHAP) scoring is utilized to rank SEEG channels based on their
contribution to seizures. A channel extension strategy is also incorporated to
expand the search space and identify suspicious epileptogenic zones beyond
those selected by clinicians. For validation, SEEG data for five patients were
analyzed showing promising results in terms of accuracy, consistency, and
explainability.

</details>


### [358] [Cross-Channel Unlabeled Sensing over a Union of Signal Subspaces](https://arxiv.org/abs/2506.09773)
*Taulant Koka,Manolis C. Tsakiris,Benjamín Béjar Haro,Michael Muma*

Main category: eess.SP

TL;DR: The paper tackles cross-channel unlabeled sensing by extending it to signals in a union of subspaces, showing improved sample requirements and real-world utility.


<details>
  <summary>Details</summary>
Motivation: To address the recovery of shuffled multi-channel signals in complex structures, such as applications in brain calcium imaging or multi-target tracking.

Method: Extended cross-channel unlabeled sensing framework, derived tighter sample requirement bounds, and applied it to real-world imaging scenarios.

Result: The study demonstrates accurate signal reconstruction in challenging real-world applications like whole-brain calcium imaging.

Conclusion: The framework is effective for handling mismatched sample-channel associations and offers improvements for complex signal reconstruction tasks.

Abstract: Cross-channel unlabeled sensing addresses the problem of recovering a
multi-channel signal from measurements that were shuffled across channels. This
work expands the cross-channel unlabeled sensing framework to signals that lie
in a union of subspaces. The extension allows for handling more complex signal
structures and broadens the framework to tasks like compressed sensing. These
mismatches between samples and channels often arise in applications such as
whole-brain calcium imaging of freely moving organisms or multi-target
tracking. We improve over previous models by deriving tighter bounds on the
required number of samples for unique reconstruction, while supporting more
general signal types. The approach is validated through an application in
whole-brain calcium imaging, where organism movements disrupt sample-to-neuron
mappings. This demonstrates the utility of our framework in real-world settings
with imprecise sample-channel associations, achieving accurate signal
reconstruction.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [359] [Exploring Image Transforms derived from Eye Gaze Variables for Progressive Autism Diagnosis](https://arxiv.org/abs/2506.09065)
*Abigail Copiaco,Christian Ritz,Yassine Himeur,Valsamma Eapen,Ammar Albanna,Wathiq Mansoor*

Main category: eess.IV

TL;DR: This paper explores an AI-driven system that uses transfer learning and eye gaze variables to streamline ASD diagnosis, making it more accessible, timely, and privacy-conscious.


<details>
  <summary>Details</summary>
Motivation: The need to address challenges in communication, behavior, and focus experienced by individuals with Autism Spectrum Disorder (ASD), coupled with the limitations of existing time-intensive diagnostic methods.

Method: Integration of transfer learning with image transforms derived from eye gaze variables, enabling in-home periodical diagnosis and preserving privacy through transformed image data.

Result: The system facilitates more accessible and efficient diagnosis and management of ASD, while improving communication between caregivers and therapists.

Conclusion: The proposed AI-powered system enhances ASD diagnosis by making it timely and convenient, while safeguarding privacy, reducing caregiver stress, and supporting better patient outcomes.

Abstract: The prevalence of Autism Spectrum Disorder (ASD) has surged rapidly over the
past decade, posing significant challenges in communication, behavior, and
focus for affected individuals. Current diagnostic techniques, though
effective, are time-intensive, leading to high social and economic costs. This
work introduces an AI-powered assistive technology designed to streamline ASD
diagnosis and management, enhancing convenience for individuals with ASD and
efficiency for caregivers and therapists. The system integrates transfer
learning with image transforms derived from eye gaze variables to diagnose ASD.
This facilitates and opens opportunities for in-home periodical diagnosis,
reducing stress for individuals and caregivers, while also preserving user
privacy through the use of image transforms. The accessibility of the proposed
method also offers opportunities for improved communication between guardians
and therapists, ensuring regular updates on progress and evolving support
needs. Overall, the approach proposed in this work ensures timely, accessible
diagnosis while protecting the subjects' privacy, improving outcomes for
individuals with ASD.

</details>


### [360] [Foundation Models in Medical Imaging -- A Review and Outlook](https://arxiv.org/abs/2506.09095)
*Vivien van Veldhuizen,Vanessa Botha,Chunyao Lu,Melis Erdal Cesur,Kevin Groot Lipman,Edwin D. de Jong,Hugo Horlings,Clárisa Sanchez,Cees Snoek,Ritse Mann,Eric Marcus,Jonas Teuwen*

Main category: eess.IV

TL;DR: This paper reviews the impact of foundation models (FMs) in medical image analysis and discusses their development, application, and challenges.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore how foundation models (FMs) transform medical image analysis by enabling learning from large unlabeled datasets and limiting the need for manual annotation.

Method: This review examines over 150 studies to analyze FM development in medical domains, focusing on architectures, self-supervised learning techniques, and domain-specific applications.

Result: The review highlights the design choices across different medical imaging domains such as pathology, radiology, and ophthalmology, while identifying shared methodologies and innovations.

Conclusion: The research synthesizes current achievements, identifies challenges, and outlines open questions to inspire further progress in FM applications for medical imaging.

Abstract: Foundation models (FMs) are changing the way medical images are analyzed by
learning from large collections of unlabeled data. Instead of relying on
manually annotated examples, FMs are pre-trained to learn general-purpose
visual features that can later be adapted to specific clinical tasks with
little additional supervision. In this review, we examine how FMs are being
developed and applied in pathology, radiology, and ophthalmology, drawing on
evidence from over 150 studies. We explain the core components of FM pipelines,
including model architectures, self-supervised learning methods, and strategies
for downstream adaptation. We also review how FMs are being used in each
imaging domain and compare design choices across applications. Finally, we
discuss key challenges and open questions to guide future research.

</details>


### [361] [Low-Rank Augmented Implicit Neural Representation for Unsupervised High-Dimensional Quantitative MRI Reconstruction](https://arxiv.org/abs/2506.09100)
*Haonan Zhang,Guoyan Lao,Yuyao Zhang,Hongjiang Wei*

Main category: eess.IV

TL;DR: This paper introduces LoREIN, an unsupervised dual-prior framework for reconstructing 3D MP-qMRI from undersampled data, combining low-rank and continuity priors to improve reconstruction accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in reconstructing qMRI from undersampled and high-dimensional data using traditional single-prior models that often yield suboptimal results.

Method: The proposed LoREIN framework integrates low-rank representation (LRR) for low-rank prior and implicit neural representation (INR) for continuity prior, enabling optimized spatial bases and zero-shot learning for accurate reconstructions.

Result: LoREIN achieves enhanced fidelity and accuracy in reconstructing weighted images and quantitative parameter maps, demonstrating broad potential in spatiotemporal and high-dimensional image reconstruction tasks.

Conclusion: LoREIN overcomes limitations of single-prior models and introduces a robust unsupervised framework for qMRI reconstruction, advancing medical imaging technology.

Abstract: Quantitative magnetic resonance imaging (qMRI) provides tissue-specific
parameters vital for clinical diagnosis. Although simultaneous multi-parametric
qMRI (MP-qMRI) technologies enhance imaging efficiency, robustly reconstructing
qMRI from highly undersampled, high-dimensional measurements remains a
significant challenge. This difficulty arises primarily because current
reconstruction methods that rely solely on a single prior or physics-informed
model to solve the highly ill-posed inverse problem, which often leads to
suboptimal results. To overcome this limitation, we propose LoREIN, a novel
unsupervised and dual-prior-integrated framework for accelerated 3D MP-qMRI
reconstruction. Technically, LoREIN incorporates both low-rank prior and
continuity prior via low-rank representation (LRR) and implicit neural
representation (INR), respectively, to enhance reconstruction fidelity. The
powerful continuous representation of INR enables the estimation of optimal
spatial bases within the low-rank subspace, facilitating high-fidelity
reconstruction of weighted images. Simultaneously, the predicted multi-contrast
weighted images provide essential structural and quantitative guidance, further
enhancing the reconstruction accuracy of quantitative parameter maps.
Furthermore, our work introduces a zero-shot learning paradigm with broad
potential in complex spatiotemporal and high-dimensional image reconstruction
tasks, further advancing the field of medical imaging.

</details>


### [362] [An Explainable Deep Learning Framework for Brain Stroke and Tumor Progression via MRI Interpretation](https://arxiv.org/abs/2506.09161)
*Rajan Das Gupta,Md Imrul Hasan Showmick,Mushfiqur Rahman Abir,Shanjida Akter,Md. Yeasin Rahat,Md. Jakir Hossen*

Main category: eess.IV

TL;DR: A deep learning-based system using MobileNet V2 and ResNet-50 classifies brain MRIs, achieving up to 88% validation accuracy.


<details>
  <summary>Details</summary>
Motivation: Timely detection of brain abnormalities is crucial for effective treatment and improved patient outcomes.

Method: Leveraged convolutional neural networks MobileNet V2 and ResNet-50 through transfer learning, applied data augmentation and dropout layers, and curated a diverse MRI dataset.

Result: Achieved 93% training accuracy and 88% validation accuracy, with ResNet-50 slightly outperforming MobileNet V2.

Conclusion: The presented solution is a practical AI-driven approach for early brain abnormality detection, offering clinical potential and opportunities for enhancement with larger, multimodal datasets.

Abstract: Early and accurate detection of brain abnormalities, such as tumors and
strokes, is essential for timely intervention and improved patient outcomes. In
this study, we present a deep learning-based system capable of identifying both
brain tumors and strokes from MRI images, along with their respective stages.
We have executed two groundbreaking strategies involving convolutional neural
networks, MobileNet V2 and ResNet-50-optimized through transfer learning to
classify MRI scans into five diagnostic categories. Our dataset, aggregated and
augmented from various publicly available MRI sources, was carefully curated to
ensure class balance and image diversity. To enhance model generalization and
prevent overfitting, we applied dropout layers and extensive data augmentation.
The models achieved strong performance, with training accuracy reaching 93\%
and validation accuracy up to 88\%. While ResNet-50 demonstrated slightly
better results, Mobile Net V2 remains a promising option for real-time
diagnosis in low resource settings due to its lightweight architecture. This
research offers a practical AI-driven solution for early brain abnormality
detection, with potential for clinical deployment and future enhancement
through larger datasets and multi modal inputs.

</details>


### [363] [The RSNA Lumbar Degenerative Imaging Spine Classification (LumbarDISC) Dataset](https://arxiv.org/abs/2506.09162)
*Tyler J. Richards,Adam E. Flanders,Errol Colak,Luciano M. Prevedello,Robyn L. Ball,Felipe Kitamura,John Mongan,Maryam Vazirabad,Hui-Ming Lin,Anne Kendell,Thanat Kanthawang,Salita Angkurawaranon,Emre Altinmakas,Hakan Dogan,Paulo Eduardo de Aguiar Kuriki,Arjuna Somasundaram,Christopher Ruston,Deniz Bulja,Naida Spahovic,Jennifer Sommer,Sirui Jiang,Eduardo Moreno Judice de Mattos Farina,Eduardo Caminha Nunes,Michael Brassil,Megan McNamara,Johanna Ortiz,Jacob Peoples,Vinson L. Uytana,Anthony Kam,Venkata N. S. Dola,Daniel Murphy,David Vu,Dataset Contributor Group,Dataset Annotator Group,Competition Data Notebook Group,Jason F. Talbott*

Main category: eess.IV

TL;DR: The paper introduces the RSNA LumbarDISC dataset, the largest publicly available dataset of adult lumbar spine MRI examinations annotated for degenerative changes.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive dataset that facilitates research and development in machine learning for lumbar spine imaging, aiming to improve patient care and clinical efficiency.

Method: The dataset consists of 2,697 patients' MRI lumbar spine examinations (8,593 image series), annotated by expert neuroradiologists and musculoskeletal radiologists for grading degenerative changes at intervertebral disc levels.

Result: The RSNA LumbarDISC dataset was created for the RSNA 2024 Lumbar Spine Degenerative Classification competition and is available for free non-commercial use on Kaggle and RSNA MIRA.

Conclusion: This dataset serves as a valuable resource for the machine learning and medical imaging community to advance research and improve diagnostics for lumbar spine issues.

Abstract: The Radiological Society of North America (RSNA) Lumbar Degenerative Imaging
Spine Classification (LumbarDISC) dataset is the largest publicly available
dataset of adult MRI lumbar spine examinations annotated for degenerative
changes. The dataset includes 2,697 patients with a total of 8,593 image series
from 8 institutions across 6 countries and 5 continents. The dataset is
available for free for non-commercial use via Kaggle and RSNA Medical Imaging
Resource of AI (MIRA). The dataset was created for the RSNA 2024 Lumbar Spine
Degenerative Classification competition where competitors developed deep
learning models to grade degenerative changes in the lumbar spine. The degree
of spinal canal, subarticular recess, and neural foraminal stenosis was graded
at each intervertebral disc level in the lumbar spine. The images were
annotated by expert volunteer neuroradiologists and musculoskeletal
radiologists from the RSNA, American Society of Neuroradiology, and the
American Society of Spine Radiology. This dataset aims to facilitate research
and development in machine learning and lumbar spine imaging to lead to
improved patient care and clinical efficiency.

</details>


### [364] [A Cytology Dataset for Early Detection of Oral Squamous Cell Carcinoma](https://arxiv.org/abs/2506.09661)
*Garima Jain,Sanghamitra Pati,Mona Duggal,Amit Sethi,Abhijeet Patil,Gururaj Malekar,Nilesh Kowe,Jitender Kumar,Jatin Kashyap,Divyajeet Rout,Deepali,Hitesh,Nishi Halduniya,Sharat Kumar,Heena Tabassum,Rupinder Singh Dhaliwal,Sucheta Devi Khuraijam,Sushma Khuraijam,Sharmila Laishram,Simmi Kharb,Sunita Singh,K. Swaminadtan,Ranjana Solanki,Deepika Hemranjani,Shashank Nath Singh,Uma Handa,Manveen Kaur,Surinder Singhal,Shivani Kalhan,Rakesh Kumar Gupta,Ravi. S,D. Pavithra,Sunil Kumar Mahto,Arvind Kumar,Deepali Tirkey,Saurav Banerjee,L. Sreelakshmi*

Main category: eess.IV

TL;DR: This paper introduces a large, multicenter oral cytology dataset to facilitate AI-based early detection of oral squamous cell carcinoma (OSCC), aiming to improve outcomes in resource-constrained settings.


<details>
  <summary>Details</summary>
Motivation: To address the global burden of OSCC by overcoming limitations of traditional histopathology methods, which are invasive, expensive, and rely on expert pathologists, particularly in low-resource settings.

Method: The study developed and validated a first-of-its-kind large, multicenter oral cytology dataset. The dataset includes annotated slides stained with Papanicolaou and May-Grunwald-Giemsa protocols, labeled for cellular anomaly classification and detection by expert pathologists.

Result: The creation of this comprehensive dataset fills a critical gap in publicly available resources for oral cytology, enabling the development of robust AI models for automated diagnosis and reducing diagnostic variability.

Conclusion: By enabling AI-driven diagnostic methods, this dataset has the potential to improve early OSCC detection, reduce diagnostic errors, and enhance patient outcomes, particularly in low-resource global settings.

Abstract: Oral squamous cell carcinoma OSCC is a major global health burden,
particularly in several regions across Asia, Africa, and South America, where
it accounts for a significant proportion of cancer cases. Early detection
dramatically improves outcomes, with stage I cancers achieving up to 90 percent
survival. However, traditional diagnosis based on histopathology has limited
accessibility in low-resource settings because it is invasive,
resource-intensive, and reliant on expert pathologists. On the other hand, oral
cytology of brush biopsy offers a minimally invasive and lower cost
alternative, provided that the remaining challenges, inter observer variability
and unavailability of expert pathologists can be addressed using artificial
intelligence. Development and validation of robust AI solutions requires access
to large, labeled, and multi-source datasets to train high capacity models that
generalize across domain shifts. We introduce the first large and multicenter
oral cytology dataset, comprising annotated slides stained with
Papanicolaou(PAP) and May-Grunwald-Giemsa(MGG) protocols, collected from ten
tertiary medical centers in India. The dataset is labeled and annotated by
expert pathologists for cellular anomaly classification and detection, is
designed to advance AI driven diagnostic methods. By filling the gap in
publicly available oral cytology datasets, this resource aims to enhance
automated detection, reduce diagnostic errors, and improve early OSCC diagnosis
in resource-constrained settings, ultimately contributing to reduced mortality
and better patient outcomes worldwide.

</details>


### [365] [Sampling Theory for Super-Resolution with Implicit Neural Representations](https://arxiv.org/abs/2506.09949)
*Mahrokh Najaf,Gregory Ongie*

Main category: eess.IV

TL;DR: This paper examines the sample complexity requirements for recovering continuous-domain images using Implicit Neural Representations (INRs) with low-pass Fourier samples and a single hidden-layer INR model.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand sampling requirements and theoretical guarantees for recovering images with INRs from low-pass Fourier samples, filling knowledge gaps in INR-based inverse problems.

Method: The paper employs a theoretical analysis linking solutions of non-convex INR optimization problems to minimizers of convex penalties in infinite-dimensional measure spaces. It also introduces weight decay regularization and conducts empirical validation via super-resolution experiments.

Result: The study proves sufficient Fourier sampling conditions for exact recovery of images using single hidden-layer INRs and validates the theory with experiments showing high recovery probability.

Conclusion: The findings provide insights into sampling conditions for INR-based image recovery and demonstrate their practical viability for tasks like super-resolution.

Abstract: Implicit neural representations (INRs) have emerged as a powerful tool for
solving inverse problems in computer vision and computational imaging. INRs
represent images as continuous domain functions realized by a neural network
taking spatial coordinates as inputs. However, unlike traditional pixel
representations, little is known about the sample complexity of estimating
images using INRs in the context of linear inverse problems. Towards this end,
we study the sampling requirements for recovery of a continuous domain image
from its low-pass Fourier samples by fitting a single hidden-layer INR with
ReLU activation and a Fourier features layer using a generalized form of weight
decay regularization. Our key insight is to relate minimizers of this
non-convex parameter space optimization problem to minimizers of a convex
penalty defined over an infinite-dimensional space of measures. We identify a
sufficient number of Fourier samples for which an image realized by an INR is
exactly recoverable by solving the INR training problem. To validate our
theory, we empirically assess the probability of achieving exact recovery of
images realized by low-width single hidden-layer INRs, and illustrate the
performance of INRs on super-resolution recovery of continuous domain phantom
images.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [366] [Mainframe-style channel controllers for modern disaggregated memory systems](https://arxiv.org/abs/2506.09758)
*Zikai Liu,Jasmin Schult,Pengcheng Xu,Timothy Roscoe*

Main category: cs.OS

TL;DR: This paper presents an abstraction for Near-Data Processing (NDP) called memory channel controllers (MCCs), enabling better OS integration and richer programming models in disaggregated memory systems.


<details>
  <summary>Details</summary>
Motivation: Near-Data Processing has potential but remains underutilized despite hardware support due to the lack of a clear, OS-level abstraction facilitating adoption, especially in disaggregated memory systems.

Method: The authors propose memory channel controllers (MCCs), inspired by mainframe channel controllers, to serve as a portable and virtualizable OS-centric abstraction for NDP. These MCCs leverage cache coherence in modern interconnects for enhanced programming capabilities.

Result: The proposed MCC abstraction offers better compatibility with operating systems and facilitates more fine-grained programming interactions compared to existing NDP systems.

Conclusion: The introduction of MCCs addresses barriers to NDP adoption by providing a clean, OS-compatible abstraction and enriching the programming experience, potentially driving broader deployment in modern memory architectures.

Abstract: Despite the promise of alleviating the main memory bottleneck, and the
existence of commercial hardware implementations, techniques for Near-Data
Processing have seen relatively little real-world deployment. The idea has
received renewed interest with the appearance of disaggregated or "far" memory,
for example in the use of CXL memory pools.
  However, we argue that the lack of a clear OS-centric abstraction of
Near-Data Processing is a major barrier to adoption of the technology. Inspired
by the channel controllers which interface the CPU to disk drives in mainframe
systems, we propose memory channel controllers as a convenient, portable, and
virtualizable abstraction of Near-Data Processing for modern disaggregated
memory systems.
  In addition to providing a clean abstraction that enables OS integration
while requiring no changes to CPU architecture, memory channel controllers
incorporate another key innovation: they exploit the cache coherence provided
by emerging interconnects to provide a much richer programming model, with more
fine-grained interaction, than has been possible with existing designs.

</details>


### [367] [On the Impossibility of a Perfect Hypervisor](https://arxiv.org/abs/2506.09825)
*Mordechai Guri*

Main category: cs.OS

TL;DR: The paper establishes that a 'perfect hypervisor', capable of fully replicating bare-metal behavior without overhead or detection, is fundamentally impossible due to finite computational resources.


<details>
  <summary>Details</summary>
Motivation: To understand the theoretical limitations of virtualization systems and establish the principles governing their behavior and performance constraints.

Method: The authors propose a theoretical framework and develop two theorems: an Indetectability Theorem, stating that if such a hypervisor were possible, it would be indistinguishable from native execution, and an Impossibility Theorem, proving such a hypervisor cannot exist given finite computational resources.

Result: The work proves that no hypervisor, emulator, sandbox, or similar framework can theoretically meet the criteria of perfect replication without resource or timing overhead.

Conclusion: This impossibility result lays a formal foundation for future exploration into the principles, limitations, and trade-offs involved in virtualization technologies.

Abstract: We establish a fundamental impossibility result for a `perfect hypervisor',
one that (1) preserves every observable behavior of any program exactly as on
bare metal and (2) adds zero timing or resource overhead.
  Within this model we prove two theorems. (1) Indetectability Theorem. If such
a hypervisor existed, no guest-level program, measurement, or timing test could
distinguish it from native execution; all traces, outputs, and timings would be
identical.
  (2) Impossibility Theorem. Despite that theoretical indetectability, a
perfect hypervisor cannot exist on any machine with finite computational
resources.
  These results are architecture-agnostic and extend beyond hypervisors to any
virtualization layer emulators, sandboxes, containers, or
runtime-instrumentation frameworks. Together they provide a formal foundation
for future work on the principles and limits of virtualization.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [368] [Adaptive event-triggered robust tracking control of soft robots](https://arxiv.org/abs/2506.09523)
*Renjie Ma,Ziyao Qu,Zhijian Hu,Dong Zhao,Marios M. Polycarpou*

Main category: eess.SY

TL;DR: This paper presents a control strategy for soft robots under uncertainties using backstepping methodology and event-triggered approaches.


<details>
  <summary>Details</summary>
Motivation: Soft robots are prone to uncertainties like unmodeled dynamics and external disturbances, requiring robust control methods to ensure precise operation.

Method: A switching function is introduced, with compensated tracking error dynamics and backstepping-based virtual controllers integrated into an event-triggered control strategy.

Result: The proposed control algorithm is validated through a case study, demonstrating its effectiveness under various scenarios.

Conclusion: The control strategy ensures finite-time stability and robustness against uncertainties, advancing soft robot applications in complex environments.

Abstract: Soft robots manufactured with flexible materials can be highly compliant and
adaptive to their surroundings, which facilitates their application in areas
such as dexterous manipulation and environmental exploration. This paper aims
at investigating the tracking control problem for soft robots under uncertainty
such as unmodeled dynamics and external disturbance. First, we establish a
novel switching function and design the compensated tracking error dynamics by
virtue of the command filter. Then, based on the backstepping methodology, the
virtual controllers and the adaptive logic estimating the supremum of
uncertainty impacts are developed for synthesizing an event-triggered control
strategy. In addition, the uniformed finite-time stability certification is
derived for different scenarios of the switching function. Finally, we perform
a case study of a soft robot to illustrate the effectiveness of the proposed
control algorithm.

</details>


### [369] [A Survey on the Role of Artificial Intelligence and Machine Learning in 6G-V2X Applications](https://arxiv.org/abs/2506.09512)
*Donglin Wang,Anjie Qiu,Qiuheng Zhou,Hans D. Schotten*

Main category: eess.SY

TL;DR: This paper reviews AI and ML advancements in 6G-V2X communication, addressing techniques, applications, challenges, and future directions.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in addressing the lack of a systematic review of recent AI and ML research applied to 6G-V2X communication systems.

Method: The paper conducts a comprehensive survey focusing on recent AI/ML models like DL, RL, GL, and FL applied to 6G-V2X, analyzing their applications and identifying challenges.

Result: It highlights the notable progress in AI, especially GL, in 6G-V2X applications such as resource allocation and traffic management, while outlining challenges such as computational complexity and data privacy.

Conclusion: This study provides a systematic summary of state-of-the-art AI advancements for 6G-V2X, offering insights for progress in intelligent and connected transportation systems.

Abstract: The rapid advancement of Vehicle-to-Everything (V2X) communication is
transforming Intelligent Transportation Systems (ITS), with 6G networks
expected to provide ultra-reliable, low-latency, and high-capacity connectivity
for Connected and Autonomous Vehicles (CAVs). Artificial Intelligence (AI) and
Machine Learning (ML) have emerged as key enablers in optimizing V2X
communication by enhancing network management, predictive analytics, security,
and cooperative driving due to their outstanding performance across various
domains, such as natural language processing and computer vision. This survey
comprehensively reviews recent advances in AI and ML models applied to 6G-V2X
communication. It focuses on state-of-the-art techniques, including Deep
Learning (DL), Reinforcement Learning (RL), Generative Learning (GL), and
Federated Learning (FL), with particular emphasis on developments from the past
two years. Notably, AI, especially GL, has shown remarkable progress and
emerging potential in enhancing the performance, adaptability, and intelligence
of 6G-V2X systems. Despite these advances, a systematic summary of recent
research efforts in this area remains lacking, which this survey aims to
address. We analyze their roles in 6G-V2X applications, such as intelligent
resource allocation, beamforming, intelligent traffic management, and security
management. Furthermore, we explore the technical challenges, including
computational complexity, data privacy, and real-time decision-making
constraints, while identifying future research directions for AI-driven 6G-V2X
development. This study aims to provide valuable insights for researchers,
engineers, and policymakers working towards realizing intelligent, AI-powered
V2X ecosystems in 6G communication.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [370] [Designing conflict-based communicative tasks in Teaching Chinese as a Foreign Language with ChatGPT](https://arxiv.org/abs/2506.09089)
*Xia Li*

Main category: cs.HC

TL;DR: The paper explores using ChatGPT to design a communicative oral expression course in teaching Chinese as a foreign language, emphasizing teacher-ChatGPT interaction dynamics.


<details>
  <summary>Details</summary>
Motivation: The study aims to enhance oral interaction skills among learners through innovative task design, leveraging AI tools like ChatGPT in educational settings.

Method: The teacher collaborated with ChatGPT to create communicative oral tasks by building scenarios based on conflicts, fostering student engagement and interaction.

Result: The paper identifies key interaction features between the teacher and ChatGPT, highlighting the effectiveness of AI-assisted design in enriching educational programs.

Conclusion: ChatGPT's integration into course design positively impacts educational outcomes by streamlining task development and promoting interactive learning.

Abstract: In developing the teaching program for a course in Oral Expression in
Teaching Chinese as a Foreign Language at the university level, the teacher
designs communicative tasks based on conflicts to encourage learners to engage
in interactive dynamics and develop their oral interaction skills. During the
design of these tasks, the teacher uses ChatGPT to assist in finalizing the
program. This article aims to present the key characteristics of the
interactions between the teacher and ChatGPT during this program development
process, as well as to examine the use of ChatGPT and its impacts in this
specific context.

</details>


### [371] ["Is This Really a Human Peer Supporter?": Misalignments Between Peer Supporters and Experts in LLM-Supported Interactions](https://arxiv.org/abs/2506.09354)
*Kellie Yu Hui Sim,Roy Ka-Wei Lee,Kenny Tsu Wei Choo*

Main category: cs.HC

TL;DR: The paper explores the use of AI systems, specifically Large Language Models (LLMs), to improve peer support for mental health, highlighting potential benefits and key issues.


<details>
  <summary>Details</summary>
Motivation: Mental health is a growing concern worldwide, and while peer support complements professional care, inconsistencies in training and efficacy remain challenges. This paper seeks to investigate AI-driven solutions to address these gaps.

Method: The authors implemented an AI system featuring an LLM-simulated distressed client, context-sensitive suggestions, and real-time emotion visualizations. Two mixed-method studies involving peer supporters and mental health professionals were conducted to evaluate its effectiveness.

Result: The study found that while peer supporters interacted meaningfully with the AI system, experts identified critical gaps such as missed distress signals and premature advice-giving. Both groups acknowledged the system's training potential but highlighted limitations in current peer support practices.

Conclusion: The research emphasizes the necessity for standardized, psychologically-informed training programs for peer support. It also suggests that LLMs, if carefully designed and overseen by experts, can help improve and scale peer-delivered mental health care responsibly.

Abstract: Mental health is a growing global concern, prompting interest in AI-driven
solutions to expand access to psychosocial support. Peer support, grounded in
lived experience, offers a valuable complement to professional care. However,
variability in training, effectiveness, and definitions raises concerns about
quality, consistency, and safety. Large Language Models (LLMs) present new
opportunities to enhance peer support interactions, particularly in real-time,
text-based interactions. We present and evaluate an AI-supported system with an
LLM-simulated distressed client, context-sensitive LLM-generated suggestions,
and real-time emotion visualisations. 2 mixed-methods studies with 12 peer
supporters and 5 mental health professionals (i.e., experts) examined the
system's effectiveness and implications for practice. Both groups recognised
its potential to enhance training and improve interaction quality. However, we
found a key tension emerged: while peer supporters engaged meaningfully,
experts consistently flagged critical issues in peer supporter responses, such
as missed distress cues and premature advice-giving. This misalignment
highlights potential limitations in current peer support training, especially
in emotionally charged contexts where safety and fidelity to best practices are
essential. Our findings underscore the need for standardised, psychologically
grounded training, especially as peer support scales globally. They also
demonstrate how LLM-supported systems can scaffold this development--if
designed with care and guided by expert oversight. This work contributes to
emerging conversations on responsible AI integration in mental health and the
evolving role of LLMs in augmenting peer-delivered care.

</details>


### [372] ["I Said Things I Needed to Hear Myself": Peer Support as an Emotional, Organisational, and Sociotechnical Practice in Singapore](https://arxiv.org/abs/2506.09362)
*Kellie Yu Hui Sim,Kenny Tsu Wei Choo*

Main category: cs.HC

TL;DR: This paper explores how peer supporters in Singapore manage online, offline, and hybrid environments for mental health support, emphasizing their motivations, emotional labor, and cultural influences. It also suggests culturally responsive design directions for digital tools and AI.


<details>
  <summary>Details</summary>
Motivation: The paper aims to investigate the design and impact of technologies mediating peer mental health support, especially in under-studied Asian contexts like Singapore.

Method: The researchers conducted a thematic analysis of interviews with 20 peer supporters in Singapore who operate in various environments (online, offline, and hybrid).

Result: The study highlights the motivations, emotional labor, and sociocultural factors shaping peer supporters' practices, and derives insights for the responsible use of AI in augmenting peer support.

Conclusion: The findings contribute to the field of human-centered computing by offering a nuanced understanding of peer support practices and proposing design implications for culturally sensitive and trustworthy AI tools in mental health care.

Abstract: Peer support plays a vital role in expanding access to mental health care by
providing empathetic, community-based support outside formal clinical systems.
As digital platforms increasingly mediate such support, the design and impact
of these technologies remain under-examined, particularly in Asian contexts.
This paper presents findings from an interview study with 20 peer supporters in
Singapore, who operate across diverse online, offline, and hybrid environments.
Through a thematic analysis, we unpack how participants start, conduct, and
sustain peer support, highlighting their motivations, emotional labour, and the
sociocultural dimensions shaping their practices. Building on this grounded
understanding, we surface design directions for culturally responsive digital
tools that scaffold rather than supplant relational care. Drawing insights from
qualitative accounts, we offer a situated perspective on how AI might
responsibly augment peer support. This research contributes to human-centred
computing by articulating the lived realities of peer supporters and proposing
design implications for trustworthy and context-sensitive AI in mental health.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [373] [ThinkQE: Query Expansion via an Evolving Thinking Process](https://arxiv.org/abs/2506.09260)
*Yibin Lei,Tao Shen,Andrew Yates*

Main category: cs.IR

TL;DR: ThinkQE is a framework enhancing query expansion for web search, focusing on deeper semantic exploration and iterative refinement using retrieval feedback.


<details>
  <summary>Details</summary>
Motivation: To address limitations in LLM-based query expansions that tend to be narrowly focused and neglect exploration and diversity.

Method: ThinkQE introduces a thinking-based expansion process for semantic exploration and employs corpus-interaction strategies to iteratively refine search expansions based on feedback.

Result: ThinkQE outperformed prior methods, including dense retrievers and rerankers, across benchmarks like DL19, DL20, and BRIGHT.

Conclusion: ThinkQE improves query expansion by promoting exploration and result diversity, offering effective enhancements without training-intensive demands.

Abstract: Effective query expansion for web search benefits from promoting both
exploration and result diversity to capture multiple interpretations and facets
of a query. While recent LLM-based methods have improved retrieval performance
and demonstrate strong domain generalization without additional training, they
often generate narrowly focused expansions that overlook these desiderata. We
propose ThinkQE, a test-time query expansion framework addressing this
limitation through two key components: a thinking-based expansion process that
encourages deeper and comprehensive semantic exploration, and a
corpus-interaction strategy that iteratively refines expansions using retrieval
feedback from the corpus. Experiments on diverse web search benchmarks (DL19,
DL20, and BRIGHT) show ThinkQE consistently outperforms prior approaches,
including training-intensive dense retrievers and rerankers.

</details>


### [374] [Revisiting Graph Projections for Effective Complementary Product Recommendation](https://arxiv.org/abs/2506.09209)
*Leandro Anghinoni,Pablo Zivic,Jorge Adrian Sanchez*

Main category: cs.IR

TL;DR: This paper introduces a method that uses directed weighted graph projections from a user-item bipartite graph to recommend complementary products, showing substantial accuracy gains over existing methods.


<details>
  <summary>Details</summary>
Motivation: To improve customer experience and increase retail sales through effective complementary product recommendations, addressing challenges posed by noisy and sparse user-item interaction data.

Method: A novel approach using directed weighted graphs projected from user-item bipartite graphs to infer complementarity relationships, with comparisons against existing sequential and graph-based recommendation methods.

Result: The proposed method achieves significant improvements, with an average accuracy gain of +43% over sequential recommenders and +38% over graph-based recommenders across various benchmarks.

Conclusion: Despite its simplicity, the approach proves effective in predicting complementary products, showcasing its potential for enhancing product recommendation systems.

Abstract: Complementary product recommendation is a powerful strategy to improve
customer experience and retail sales. However, recommending the right product
is not a simple task because of the noisy and sparse nature of user-item
interactions. In this work, we propose a simple yet effective method to predict
a list of complementary products given a query item, based on the structure of
a directed weighted graph projected from the user-item bipartite graph. We
revisit bipartite graph projections for recommender systems and propose a novel
approach for inferring complementarity relationships from historical user-item
interactions. We compare our model with recent methods from the literature and
show, despite the simplicity of our approach, an average improvement of +43%
and +38% over sequential and graph-based recommenders, respectively, over
different benchmarks.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [375] [Alice and the Caterpillar: A more descriptive null model for assessing data mining results](https://arxiv.org/abs/2506.09764)
*Giulia Preti,Gianmarco De Francisci Morales,Matteo Riondato*

Main category: cs.SI

TL;DR: The paper introduces novel statistical null models for analyzing binary transactional and sequence datasets, ensuring advanced property preservation. It also presents efficient Markov chain Monte Carlo algorithms for dataset sampling.


<details>
  <summary>Details</summary>
Motivation: Existing null models inadequately preserve dataset properties, limiting their utility in assessing results from binary transactional and sequence datasets. This drives the need for enhanced models.

Method: The authors propose null models that preserve the Bipartite Joint Degree Matrix, including caterpillars, and utilize a suite of Markov chain Monte Carlo algorithms (Alice) for efficient sampling based on carefully designed states and operations.

Result: Experimental evaluation demonstrates that Alice is computationally efficient, mixes quickly, scales well, and identifies different significant results compared to previous models.

Conclusion: The study achieves advanced null model design that better evaluates transactional and sequence dataset results, supported by Alice's effective sampling capabilities.

Abstract: We introduce novel null models for assessing the results obtained from
observed binary transactional and sequence datasets, using statistical
hypothesis testing. Our null models maintain more properties of the observed
dataset than existing ones. Specifically, they preserve the Bipartite Joint
Degree Matrix of the bipartite (multi-)graph corresponding to the dataset,
which ensures that the number of caterpillars, i.e., paths of length three, is
preserved, in addition to other properties considered by other models. We
describe Alice, a suite of Markov chain Monte Carlo algorithms for sampling
datasets from our null models, based on a carefully defined set of states and
efficient operations to move between them. The results of our experimental
evaluation show that Alice mixes fast and scales well, and that our null model
finds different significant results than ones previously considered in the
literature.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [376] [Empirical and computer-aided robustness analysis of long-step and accelerated methods in smooth convex optimization](https://arxiv.org/abs/2506.09730)
*Pierre Vernimmen,François Glineur*

Main category: math.OC

TL;DR: This paper analyzes the robustness of first-order optimization methods against gradient inaccuracies, proposing improved approaches for stability.


<details>
  <summary>Details</summary>
Motivation: To address the growing issue of gradient inaccuracies in large-scale GPU-based optimization problems.

Method: Analyzed three optimization methods — gradient descent, long-step methods, and accelerated methods — introducing a "shortening factor" to enhance robustness.

Result: Accelerated methods exhibited unexpected robustness; "shortening factor" improved long-step methods; shortened methods performed well under gradient inexactness.

Conclusion: The study highlights the potential of modified optimization methods in handling relative gradient inaccuracies effectively.

Abstract: This work assesses both empirically and theoretically, using the performance
estimation methodology, how robust different first-order optimization methods
are when subject to relative inexactness in their gradient computations.
Relative inexactness occurs, for example, when compressing the gradient using
fewer bits of information, which happens when dealing with large-scale problems
on GPUs. Three major families of methods are analyzed: constant step gradient
descent, long-step methods, and accelerated methods. The latter two are first
shown to be theoretically not robust to inexactness. Then, a semi-heuristic
shortening factor is introduced to improve their theoretical guarantees. All
methods are subsequently tested on a concrete inexact problem, with two
different types of relative inexactness, and it is observed that both
accelerated methods are much more robust than expected, and that the shortening
factor significantly helps the long-step methods. In the end, all shortened
methods appear to be promising, even in this inexact setting.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [377] [Efficient Modular Multiplier over GF (2^m) for ECPM](https://arxiv.org/abs/2506.09464)
*Ruby Kumari,Gaurav Purohit,Abhijit Karmakar*

Main category: cs.CR

TL;DR: This paper introduces a hardware-efficient hybrid multiplication approach combining conventional and Karatsuba methods for ECC systems, targeting NIST binary field parameters. The design improves speed, computational efficiency, and resource utilization.


<details>
  <summary>Details</summary>
Motivation: The paper addresses computational inefficiencies in ECC systems by optimizing modular multiplication, a bottleneck in elliptic curve point multiplication, using innovative hybrid techniques.

Method: The hybrid technique combines conventional multiplication for small operands and Karatsuba multiplication for larger operands. This reduces computational complexity and optimizes resource usage, evaluated on field sizes m=163, 233, 283, and 571.

Result: The hybrid implementation achieves significant improvements in LUT utilization (up to 70.70% reduction), delay performance (37.60% improvement for m=163), and area-delay product metrics compared to conventional multiplication methods.

Conclusion: The hybrid multiplication design successfully enhances computational efficiency, speed, and resource utilization in ECC cryptographic systems, particularly for NIST-standardized parameters.

Abstract: Elliptic curve cryptography (ECC) has emerged as the dominant public-key
protocol, with NIST standardizing parameters for binary field GF(2^m) ECC
systems. This work presents a hardware implementation of a Hybrid
Multiplication technique for modular multiplication over binary field GF(2m),
targeting NIST B-163, 233, 283, and 571 parameters. The design optimizes the
combination of conventional multiplication (CM) and Karatsuba multiplication
(KM) to enhance elliptic curve point multiplication (ECPM). The key innovation
uses CM for smaller operands (up to 41 bits for m=163) and KM for larger ones,
reducing computational complexity and enhancing efficiency. The design is
evaluated in three areas: Resource Utilization For m=163, the hybrid design
uses 6,812 LUTs, a 39.82% reduction compared to conventional methods. For
m=233, LUT usage reduces by 45.53% and 70.70% compared to overlap-free and
bit-parallel implementations. Delay Performance For m=163, achieves 13.31ns
delay, improving by 37.60% over bit-parallel implementations. For m=233,
maintains 13.39ns delay. Area-Delay Product For m=163, achieves ADP of 90,860,
outperforming bit-parallel (75,337) and digit-serial (43,179) implementations.
For m=233, demonstrates 16.86% improvement over overlap-free and 96.10% over
bit-parallel designs. Results show the hybrid technique significantly improves
speed, hardware efficiency, and resource utilization for ECC cryptographic
systems.

</details>


### [378] [Adversarial Text Generation with Dynamic Contextual Perturbation](https://arxiv.org/abs/2506.09148)
*Hetvi Waghela,Jaydip Sen,Sneha Rakshit,Subhasis Dasgupta*

Main category: cs.CR

TL;DR: This paper introduces Dynamic Contextual Perturbation (DCP), a novel adversarial attack method on NLP models, which generates context-aware text perturbations across multiple levels (sentences, paragraphs, documents) to ensure fluency and semantic consistency while leading to model misclassifications.


<details>
  <summary>Details</summary>
Motivation: Current adversarial attacks on NLP models often fail to account for broader context, resulting in detectable and semantically incoherent perturbations.

Method: DCP dynamically generates text perturbations using pre-trained language models and refines them iteratively via an adversarial objective function balancing misclassification induction and text naturalness.

Result: DCP generates sophisticated adversarial examples that challenge NLP systems more effectively by mimicking natural language patterns, as demonstrated on various models and datasets.

Conclusion: DCP underscores the importance of context in adversarial attacks and provides insights for improving the robustness of NLP systems against advanced adversarial strategies.

Abstract: Adversarial attacks on Natural Language Processing (NLP) models expose
vulnerabilities by introducing subtle perturbations to input text, often
leading to misclassification while maintaining human readability. Existing
methods typically focus on word-level or local text segment alterations,
overlooking the broader context, which results in detectable or semantically
inconsistent perturbations. We propose a novel adversarial text attack scheme
named Dynamic Contextual Perturbation (DCP). DCP dynamically generates
context-aware perturbations across sentences, paragraphs, and documents,
ensuring semantic fidelity and fluency. Leveraging the capabilities of
pre-trained language models, DCP iteratively refines perturbations through an
adversarial objective function that balances the dual objectives of inducing
model misclassification and preserving the naturalness of the text. This
comprehensive approach allows DCP to produce more sophisticated and effective
adversarial examples that better mimic natural language patterns. Our
experimental results, conducted on various NLP models and datasets, demonstrate
the efficacy of DCP in challenging the robustness of state-of-the-art NLP
systems. By integrating dynamic contextual analysis, DCP significantly enhances
the subtlety and impact of adversarial attacks. This study highlights the
critical role of context in adversarial attacks and lays the groundwork for
creating more robust NLP systems capable of withstanding sophisticated
adversarial strategies.

</details>


### [379] [Empirical Quantification of Spurious Correlations in Malware Detection](https://arxiv.org/abs/2506.09662)
*Bianca Perasso,Ludovico Lozza,Andrea Ponte,Luca Demetrio,Luca Oneto,Fabio Roli*

Main category: cs.CR

TL;DR: This paper explores how spurious correlations, specifically reliance on empty spaces in compiled code, affect the performance of end-to-end deep learning models for malware detection.


<details>
  <summary>Details</summary>
Motivation: To address the issue of deep learning models for malware detection exploiting spurious correlations, which undermine the reliability of their predictions.

Method: The authors performed an analysis using a small-scale balanced dataset to evaluate how much models depend on irrelevant features, such as empty spaces left by the compiler.

Result: The study ranked two end-to-end models to assess their suitability for production and revealed their susceptibility to misleading features like compiler-inserted empty spaces.

Conclusion: Deep learning for malware detection is highly influenced by spurious correlations, and model rankings can guide better deployments in practical applications.

Abstract: End-to-end deep learning exhibits unmatched performance for detecting
malware, but such an achievement is reached by exploiting spurious correlations
-- features with high relevance at inference time, but known to be useless
through domain knowledge. While previous work highlighted that deep networks
mainly focus on metadata, none investigated the phenomenon further, without
quantifying their impact on the decision. In this work, we deepen our
understanding of how spurious correlation affects deep learning for malware
detection by highlighting how much models rely on empty spaces left by the
compiler, which diminishes the relevance of the compiled code. Through our
seminal analysis on a small-scale balanced dataset, we introduce a ranking of
two end-to-end models to better understand which is more suitable to be put in
production.

</details>


### [380] [DAVSP: Safety Alignment for Large Vision-Language Models via Deep Aligned Visual Safety Prompt](https://arxiv.org/abs/2506.09353)
*Yitong Zhang,Jia Li,Liyi Cai,Ge Li*

Main category: cs.CR

TL;DR: This paper introduces DAVSP, a method improving the robustness of Large Vision-Language Models (LVLMs) against malicious visual inputs.


<details>
  <summary>Details</summary>
Motivation: Existing LVLM alignment methods struggle to resist malicious queries while maintaining utility for benign uses, creating a security-vulnerability trade-off.

Method: The authors propose DAVSP, combining a Visual Safety Prompt—a trainable padding around images—and Deep Alignment, a training method leveraging activation space supervision.

Result: DAVSP significantly improves resistance to malicious inputs while maintaining performance on benign inputs, as confirmed by tests on five benchmarks and two LVLMs.

Conclusion: DAVSP improves the safety of LVLMs, offering robust defense against malicious queries with minimal impact on benign input utility. Its complementary technologies contribute to its success.

Abstract: Large Vision-Language Models (LVLMs) have achieved impressive progress across
various applications but remain vulnerable to malicious queries that exploit
the visual modality. Existing alignment approaches typically fail to resist
malicious queries while preserving utility on benign ones effectively. To
address these challenges, we propose Deep Aligned Visual Safety Prompt (DAVSP),
which is built upon two key innovations. First, we introduce the Visual Safety
Prompt, which appends a trainable padding region around the input image. It
preserves visual features and expands the optimization space. Second, we
propose Deep Alignment, a novel approach to train the visual safety prompt
through supervision in the model's activation space. It enhances the inherent
ability of LVLMs to perceive malicious queries, achieving deeper alignment than
prior works. Extensive experiments across five benchmarks on two representative
LVLMs demonstrate that DAVSP effectively resists malicious queries while
preserving benign input utility. Furthermore, DAVSP exhibits great cross-model
generation ability. Ablation studies further reveal that both the Visual Safety
Prompt and Deep Alignment are essential components, jointly contributing to its
overall effectiveness. The code is publicly available at
https://github.com/zhangyitonggg/DAVSP.

</details>


### [381] [What is the Cost of Differential Privacy for Deep Learning-Based Trajectory Generation?](https://arxiv.org/abs/2506.09312)
*Erik Buchholz,Natasha Fernandes,David D. Nguyen,Alsharif Abuadbba,Surya Nepal,Salil S. Kanhere*

Main category: cs.CR

TL;DR: The paper evaluates the impact of Differential Privacy (DP) mechanisms on the utility of deep-learning models generating synthetic location trajectories.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of achieving a favorable utility-privacy trade-off in generating synthetic location trajectories using deep learning.

Method: The study evaluates DP-SGD's impact on generative models, introduces a novel DP mechanism for conditional generation, and compares model types (Diffusion, VAE, GAN) regarding utility-privacy trade-offs.

Result: DP-SGD affects model performance, the proposed DP mechanism stabilizes training, and GANs outperform others under DP constraints. Large datasets are crucial for effective DP guarantees.

Conclusion: DP trajectory generation remains hard; achieving formal guarantees is only practical with large datasets or constrained scenarios.

Abstract: While location trajectories offer valuable insights, they also reveal
sensitive personal information. Differential Privacy (DP) offers formal
protection, but achieving a favourable utility-privacy trade-off remains
challenging. Recent works explore deep learning-based generative models to
produce synthetic trajectories. However, current models lack formal privacy
guarantees and rely on conditional information derived from real data during
generation. This work investigates the utility cost of enforcing DP in such
models, addressing three research questions across two datasets and eleven
utility metrics. (1) We evaluate how DP-SGD, the standard DP training method
for deep learning, affects the utility of state-of-the-art generative models.
(2) Since DP-SGD is limited to unconditional models, we propose a novel DP
mechanism for conditional generation that provides formal guarantees and assess
its impact on utility. (3) We analyse how model types - Diffusion, VAE, and GAN
- affect the utility-privacy trade-off. Our results show that DP-SGD
significantly impacts performance, although some utility remains if the
datasets is sufficiently large. The proposed DP mechanism improves training
stability, particularly when combined with DP-SGD, for unstable models such as
GANs and on smaller datasets. Diffusion models yield the best utility without
guarantees, but with DP-SGD, GANs perform best, indicating that the best
non-private model is not necessarily optimal when targeting formal guarantees.
In conclusion, DP trajectory generation remains a challenging task, and formal
guarantees are currently only feasible with large datasets and in constrained
use cases.

</details>


### [382] [TooBadRL: Trigger Optimization to Boost Effectiveness of Backdoor Attacks on Deep Reinforcement Learning](https://arxiv.org/abs/2506.09562)
*Songze Li,Mingxuan Zhang,Oubo Ma,Kang Wei,Shouling Ji*

Main category: cs.CR

TL;DR: TooBadRL introduces an optimized approach for backdoor attacks in deep reinforcement learning, focusing on systematic trigger optimization across temporal, spatial, and magnitude dimensions.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of existing backdoor attacks on deep reinforcement learning, which rely on simplistic and heuristic configurations and fail to exploit optimized trigger design.

Method: TooBadRL leverages a performance-aware adaptive mechanism for injection timing, uses Shapley value analysis for identifying critical state variables for dimension selection, and applies a gradient-based procedure to optimize trigger injection magnitude under constraints.

Result: Evaluations across DRL algorithms and benchmark tasks demonstrated significantly improved attack success rates with minimal performance impact on normal task execution.

Conclusion: Principled trigger optimization is crucial for enhancing the effectiveness of backdoor attacks in DRL, as evidenced by the success of TooBadRL in optimizing both attack performance and stealth.

Abstract: Deep reinforcement learning (DRL) has achieved remarkable success in a wide
range of sequential decision-making domains, including robotics, healthcare,
smart grids, and finance. Recent research demonstrates that attackers can
efficiently exploit system vulnerabilities during the training phase to execute
backdoor attacks, producing malicious actions when specific trigger patterns
are present in the state observations. However, most existing backdoor attacks
rely primarily on simplistic and heuristic trigger configurations, overlooking
the potential efficacy of trigger optimization. To address this gap, we
introduce TooBadRL (Trigger Optimization to Boost Effectiveness of Backdoor
Attacks on DRL), the first framework to systematically optimize DRL backdoor
triggers along three critical axes, i.e., temporal, spatial, and magnitude.
Specifically, we first introduce a performance-aware adaptive freezing
mechanism for injection timing. Then, we formulate dimension selection as a
cooperative game, utilizing Shapley value analysis to identify the most
influential state variable for the injection dimension. Furthermore, we propose
a gradient-based adversarial procedure to optimize the injection magnitude
under environment constraints. Evaluations on three mainstream DRL algorithms
and nine benchmark tasks show that TooBadRL significantly improves attack
success rates, while ensuring minimal degradation of normal task performance.
These results highlight the previously underappreciated importance of
principled trigger optimization in DRL backdoor attacks. The source code of
TooBadRL can be found at https://github.com/S3IC-Lab/TooBadRL.

</details>


### [383] [LLMail-Inject: A Dataset from a Realistic Adaptive Prompt Injection Challenge](https://arxiv.org/abs/2506.09956)
*Sahar Abdelnabi,Aideen Fay,Ahmed Salem,Egor Zverev,Kai-Chieh Liao,Chi-Huang Liu,Chun-Chih Kuo,Jannis Weigend,Danyael Manlangit,Alex Apostolov,Haris Umair,João Donato,Masayuki Kawakita,Athar Mahboob,Tran Huu Bach,Tsun-Han Chiang,Myeongjin Cho,Hajin Choi,Byeonghyeon Kim,Hyeonjin Lee,Benjamin Pannell,Conor McCauley,Mark Russinovich,Andrew Paverd,Giovanni Cherubin*

Main category: cs.CR

TL;DR: Indirect Prompt Injection attacks can't differentiate between instructions and data entered into Large Language Models (LLMs). The paper examines a public challenge where attackers tried to exploit vulnerabilities in LLMs through email-based scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the risks of indirect prompt injection attacks and improve security since many real-world applications using LLMs remain vulnerable.

Method: Researchers conducted a simulated public challenge called LLMail-Inject with varied defense mechanisms, LLM architectures, and retrieval setups where participants attempted adaptive attacks via email.

Result: Generated data consists of 208,095 attack submissions from 839 participants, highlighting the vulnerabilities of current LLM systems and providing useful insights.

Conclusion: By releasing data and analysis from LLMail-Inject, the paper aims to enable future work focused on resolving the instruction-data separation problem in LLMs for practical solutions and improved security.

Abstract: Indirect Prompt Injection attacks exploit the inherent limitation of Large
Language Models (LLMs) to distinguish between instructions and data in their
inputs. Despite numerous defense proposals, the systematic evaluation against
adaptive adversaries remains limited, even when successful attacks can have
wide security and privacy implications, and many real-world LLM-based
applications remain vulnerable. We present the results of LLMail-Inject, a
public challenge simulating a realistic scenario in which participants
adaptively attempted to inject malicious instructions into emails in order to
trigger unauthorized tool calls in an LLM-based email assistant. The challenge
spanned multiple defense strategies, LLM architectures, and retrieval
configurations, resulting in a dataset of 208,095 unique attack submissions
from 839 participants. We release the challenge code, the full dataset of
submissions, and our analysis demonstrating how this data can provide new
insights into the instruction-data separation problem. We hope this will serve
as a foundation for future research towards practical structural solutions to
prompt injection.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [384] [A Probabilistic Framework for Imputing Genetic Distances in Spatiotemporal Pathogen Models](https://arxiv.org/abs/2506.09076)
*Haley Stone,Jing Du,Hao Xue,Matthew Scotch,David Heslop,Andreas Züfle,Chandini Raina MacIntyre,Flora Salim*

Main category: q-bio.GN

TL;DR: The paper presents a probabilistic framework to deduce genetic distances between unsequenced and sequenced cases, improving genomic dataset completeness for spatial models, applied to avian influenza. 


<details>
  <summary>Details</summary>
Motivation: To address the challenge of incomplete genome sequencing coverage in using pathogen data for spatial models and improve its integration into modeling workflows.

Method: Developed a probabilistic framework leveraging time-aware evolutionary distance modeling based on collection dates and genetic divergences; does not require sequence alignment or known transmission chains.

Result: Applied this method to avian influenza A/H5 in wild U.S. birds; demonstrated scalable and uncertainty-aware augmentation of genomic datasets.

Conclusion: The framework enables biologically plausible imputations for incomplete data and enhances evolutionary information integration into spatial-temporal modeling, increasing utility of genomic datasets.

Abstract: Pathogen genome data offers valuable structure for spatial models, but its
utility is limited by incomplete sequencing coverage. We propose a
probabilistic framework for inferring genetic distances between unsequenced
cases and known sequences within defined transmission chains, using time-aware
evolutionary distance modeling. The method estimates pairwise divergence from
collection dates and observed genetic distances, enabling biologically
plausible imputation grounded in observed divergence patterns, without
requiring sequence alignment or known transmission chains. Applied to highly
pathogenic avian influenza A/H5 cases in wild birds in the United States, this
approach supports scalable, uncertainty-aware augmentation of genomic datasets
and enhances the integration of evolutionary information into spatiotemporal
modeling workflows.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [385] [STREAMINGGS: Voxel-Based Streaming 3D Gaussian Splatting with Memory Optimization and Architectural Support](https://arxiv.org/abs/2506.09070)
*Chenqi Zhang,Yu Feng,Jieru Zhao,Guangda Liu,Wenchao Ding,Chentao Wu,Minyi Guo*

Main category: cs.GR

TL;DR: The paper introduces STREAMINGGS, a memory-efficient architecture for 3D Gaussian Splatting that significantly improves rendering speed and energy efficiency on mobile devices.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of achieving real-time 3D Gaussian Splatting (3DGS) on resource-limited mobile devices, as current methods fail to meet the 90 FPS requirement due to high DRAM traffic.

Method: The authors propose STREAMINGGS, a co-design algorithm and architecture that transitions from tile-centric to memory-centric rendering, enabling fine-grained pipelining and reduced DRAM traffic.

Result: STREAMINGGS delivers up to a 45.7× speedup and 62.9× energy savings compared to mobile Ampere GPUs.

Conclusion: STREAMINGGS demonstrates that memory-centric rendering and efficient architecture design can overcome the limitations of 3DGS on mobile devices, making real-time rendering feasible.

Abstract: 3D Gaussian Splatting (3DGS) has gained popularity for its efficiency and
sparse Gaussian-based representation. However, 3DGS struggles to meet the
real-time requirement of 90 frames per second (FPS) on resource-constrained
mobile devices, achieving only 2 to 9 FPS.Existing accelerators focus on
compute efficiency but overlook memory efficiency, leading to redundant DRAM
traffic. We introduce STREAMINGGS, a fully streaming 3DGS
algorithm-architecture co-design that achieves fine-grained pipelining and
reduces DRAM traffic by transforming from a tile-centric rendering to a
memory-centric rendering. Results show that our design achieves up to 45.7
$\times$ speedup and 62.9 $\times$ energy savings over mobile Ampere GPUs.

</details>


### [386] [SILK: Smooth InterpoLation frameworK for motion in-betweening A Simplified Computational Approach](https://arxiv.org/abs/2506.09075)
*Elly Akhoundi,Hung Yu Ling,Anup Anand Deshmukh,Judith Butepage*

Main category: cs.GR

TL;DR: The paper proposes a simplified Transformer-based framework for motion in-betweening that outperforms complex models using strategic data choices.


<details>
  <summary>Details</summary>
Motivation: To simplify motion in-betweening while improving quality, as existing methods are overly complex.

Method: Used a single Transformer encoder and emphasized data modeling factors, such as pose representation and velocity inputs.

Result: Findings indicate comparable or better performance through data choices rather than model complexity.

Conclusion: Data-centric decisions in motion in-betweening can simplify processes without degrading animation quality.

Abstract: Motion in-betweening is a crucial tool for animators, enabling intricate
control over pose-level details in each keyframe. Recent machine learning
solutions for motion in-betweening rely on complex models, incorporating
skeleton-aware architectures or requiring multiple modules and training steps.
In this work, we introduce a simple yet effective Transformer-based framework,
employing a single Transformer encoder to synthesize realistic motions for
motion in-betweening tasks. We find that data modeling choices play a
significant role in improving in-betweening performance. Among others, we show
that increasing data volume can yield equivalent or improved motion
transitions, that the choice of pose representation is vital for achieving
high-quality results, and that incorporating velocity input features enhances
animation performance. These findings challenge the assumption that model
complexity is the primary determinant of animation quality and provide insights
into a more data-centric approach to motion interpolation. Additional videos
and supplementary material are available at https://silk-paper.github.io.

</details>


### [387] [VideoMat: Extracting PBR Materials from Video Diffusion Models](https://arxiv.org/abs/2506.09665)
*Jacob Munkberg,Zian Wang,Ruofan Liang,Tianchang Shen,Jon Hasselgren*

Main category: cs.GR

TL;DR: This paper presents a method to generate high-quality materials for 3D models using video diffusion models, intrinsic decomposition, and differentiable rendering.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of generating high-quality, physically-based materials for 3D models from limited inputs like text prompts or single images.

Method: The approach involves conditioning a finetuned video diffusion model to respect 3D model geometry and lighting, extracting intrinsics (base color, roughness, metallic) from the generated video, and applying a differentiable path tracer to create PBR materials.

Result: Achieves consistent and coherent material properties across multiple views of a 3D model, resulting in robust, high-quality PBR materials.

Conclusion: The proposed method enables the creation of high-quality, physically-based materials compatible with common content creation tools, using minimal input like a text prompt or single image.

Abstract: We leverage finetuned video diffusion models, intrinsic decomposition of
videos, and physically-based differentiable rendering to generate high quality
materials for 3D models given a text prompt or a single image. We condition a
video diffusion model to respect the input geometry and lighting condition.
This model produces multiple views of a given 3D model with coherent material
properties. Secondly, we use a recent model to extract intrinsics (base color,
roughness, metallic) from the generated video. Finally, we use the intrinsics
alongside the generated video in a differentiable path tracer to robustly
extract PBR materials directly compatible with common content creation tools.

</details>


### [388] [DGS-LRM: Real-Time Deformable 3D Gaussian Reconstruction From Monocular Videos](https://arxiv.org/abs/2506.09997)
*Chieh Hubert Lin,Zhaoyang Lv,Songyin Wu,Zhen Xu,Thu Nguyen-Phuoc,Hung-Yu Tseng,Julian Straub,Numair Khan,Lei Xiao,Ming-Hsuan Yang,Yuheng Ren,Richard Newcombe,Zhao Dong,Zhengqin Li*

Main category: cs.GR

TL;DR: The paper introduces DGS-LRM, a feed-forward model that reconstructs deformable 3D Gaussian splats from posed videos of dynamic scenes.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the limitations of existing feed-forward scene reconstruction methods, which often fail to handle dynamic scenes and motion.

Method: A synthetic dataset with multi-view videos and dense supervision, deformable 3D Gaussian representations, and a large transformer network for real-time operations are combined.

Result: DGS-LRM achieves high dynamic scene reconstruction quality, outperforms existing methods in real-world examples, and is capable of accurate long-range 3D tracking.

Conclusion: The model demonstrates significant advancements in predictive dynamic reconstruction and 3D video tracking, comparable to optimization-based methods and established tracking techniques.

Abstract: We introduce the Deformable Gaussian Splats Large Reconstruction Model
(DGS-LRM), the first feed-forward method predicting deformable 3D Gaussian
splats from a monocular posed video of any dynamic scene. Feed-forward scene
reconstruction has gained significant attention for its ability to rapidly
create digital replicas of real-world environments. However, most existing
models are limited to static scenes and fail to reconstruct the motion of
moving objects. Developing a feed-forward model for dynamic scene
reconstruction poses significant challenges, including the scarcity of training
data and the need for appropriate 3D representations and training paradigms. To
address these challenges, we introduce several key technical contributions: an
enhanced large-scale synthetic dataset with ground-truth multi-view videos and
dense 3D scene flow supervision; a per-pixel deformable 3D Gaussian
representation that is easy to learn, supports high-quality dynamic view
synthesis, and enables long-range 3D tracking; and a large transformer network
that achieves real-time, generalizable dynamic scene reconstruction. Extensive
qualitative and quantitative experiments demonstrate that DGS-LRM achieves
dynamic scene reconstruction quality comparable to optimization-based methods,
while significantly outperforming the state-of-the-art predictive dynamic
reconstruction method on real-world examples. Its predicted physically grounded
3D deformation is accurate and can readily adapt for long-range 3D tracking
tasks, achieving performance on par with state-of-the-art monocular video 3D
tracking methods.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [389] [Abstraction-Based Proof Production in Formal Verification of Neural Networks](https://arxiv.org/abs/2506.09455)
*Yizhak Yisrael Elboher,Omri Isac,Guy Katz,Tobias Ladner,Haoze Wu*

Main category: cs.LO

TL;DR: The paper introduces a novel framework for proof-producing, abstraction-based verification of deep neural networks (DNNs) to combine scalability with provable guarantees.


<details>
  <summary>Details</summary>
Motivation: The scalability of DNN verification tools relies on abstraction, but current systems lack proof production for abstract reasoning, reducing reliability.

Method: The framework divides verification into two aspects: 1) Verifying correctness on the abstract network (using existing tools), and 2) Proving the abstraction’s soundness for the original DNN with a new formal proof method.

Result: This approach bridges the gap between scalability and rigorous guarantees, supporting common abstraction techniques in a formal proof framework.

Conclusion: By enabling both scalable and trustworthy DNN verification, the framework advances the field and integrates abstraction techniques with formal proof production.

Abstract: Modern verification tools for deep neural networks (DNNs) increasingly rely
on abstraction to scale to realistic architectures. In parallel, proof
production is becoming a critical requirement for increasing the reliability of
DNN verification results. However, current proofproducing verifiers do not
support abstraction-based reasoning, creating a gap between scalability and
provable guarantees. We address this gap by introducing a novel framework for
proof-producing abstraction-based DNN verification. Our approach modularly
separates the verification task into two components: (i) proving the
correctness of an abstract network, and (ii) proving the soundness of the
abstraction with respect to the original DNN. The former can be handled by
existing proof-producing verifiers, whereas we propose the first method for
generating formal proofs for the latter. This preliminary work aims to enable
scalable and trustworthy verification by supporting common abstraction
techniques within a formal proof framework.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [390] [Large Language Models for Design Structure Matrix Optimization](https://arxiv.org/abs/2506.09749)
*Shuo Jiang,Min Xie,Jianxi Luo*

Main category: cs.CE

TL;DR: Design Structure Matrix (DSM) optimization, a challenging combinatorial problem, can be enhanced using Large Language Models (LLMs), combining semantic reasoning and domain knowledge for better results.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional optimization methods in engineering systems where dependency networks are complex and require contextual understanding.

Method: A novel framework integrates LLMs with network topology and contextual domain knowledge to iteratively optimize DSM sequencing.

Result: Experiments demonstrated faster convergence and improved solution quality compared to traditional stochastic and deterministic methods.

Conclusion: Leveraging LLMs in optimization holds promise for advancing engineering design efficiency by merging semantic reasoning with mathematical approaches.

Abstract: In complex engineering systems, the interdependencies among components or
development activities are often modeled and analyzed using Design Structure
Matrix (DSM). Reorganizing elements within a DSM to minimize feedback loops and
enhance modularity or process efficiency constitutes a challenging
combinatorial optimization (CO) problem in engineering design and operations.
As problem sizes increase and dependency networks become more intricate,
traditional optimization methods that solely use mathematical heuristics often
fail to capture the contextual nuances and struggle to deliver effective
solutions. In this study, we explore the potential of Large Language Models
(LLMs) for helping solve such CO problems by leveraging their capabilities for
advanced reasoning and contextual understanding. We propose a novel LLM-based
framework that integrates network topology with contextual domain knowledge for
iterative optimization of DSM element sequencing - a common CO problem.
Experiments on various DSM cases show that our method consistently achieves
faster convergence and superior solution quality compared to both stochastic
and deterministic baselines. Notably, we find that incorporating contextual
domain knowledge significantly enhances optimization performance regardless of
the chosen LLM backbone. These findings highlight the potential of LLMs to
solve complex engineering CO problems by combining semantic and mathematical
reasoning. This approach paves the way towards a new paradigm in LLM-based
engineering design optimization.

</details>


### [391] [Intelligent Design 4.0: Paradigm Evolution Toward the Agentic AI Era](https://arxiv.org/abs/2506.09755)
*Shuo Jiang,Min Xie,Frank Youhua Chen,Jian Ma,Jianxi Luo*

Main category: cs.CE

TL;DR: The paper introduces ‘Intelligent Design 4.0,’ driven by AI-powered multi-agent systems, to advance engineering processes via automation and enhanced design capabilities.


<details>
  <summary>Details</summary>
Motivation: The study aims to leverage advancements in large-scale AI models and multi-agent systems to further transform and automate engineering design processes, building on the successes of previous innovation stages in Intelligent Design.

Method: The authors propose a conceptual framework for Intelligent Design 4.0 and review the evolution of Intelligent Design across four stages, ending with a focus on multi-agent AI-driven collaboration.

Result: Intelligent Design 4.0 is positioned as a paradigm for enabling end-to-end automation and more adaptable, autonomous design processes for complex engineering challenges.

Conclusion: The proposed ID 4.0 framework could significantly enhance engineering design through autonomous, multi-agent collaboration, providing a roadmap for tackling complex design challenges via advanced AI technologies.

Abstract: Research and practice in Intelligent Design (ID) have significantly enhanced
engineering innovation, efficiency, quality, and productivity over recent
decades, fundamentally reshaping how engineering designers think, behave, and
interact with design processes. The recent emergence of Foundation Models
(FMs), particularly Large Language Models (LLMs), has demonstrated general
knowledge-based reasoning capabilities, and open new paths and avenues for
further transformation in engineering design. In this context, this paper
introduces Intelligent Design 4.0 (ID 4.0) as an emerging paradigm empowered by
agentic AI systems. We review the historical evolution of ID across four
distinct stages: rule-based expert systems, task-specific machine learning
models, large-scale foundation AI models, and the recent emerging paradigm of
multi-agent collaboration. We propose a conceptual framework for ID 4.0 and
discuss its potential to support end-to-end automation of engineering design
processes through coordinated, autonomous multi-agent-based systems.
Furthermore, we discuss future perspectives to enhance and fully realize ID
4.0's potential, including more complex design scenarios, more practical design
implementations, novel agent coordination mechanisms, and autonomous design
goal-setting with better human value alignment. In sum, these insights lay a
foundation for advancing Intelligent Design toward greater adaptivity,
autonomy, and effectiveness in addressing increasingly complex design
challenges.

</details>


### [392] [Superstudent intelligence in thermodynamics](https://arxiv.org/abs/2506.09822)
*Rebecca Loubet,Pascal Zittlau,Marco Hoffmann,Luisa Vollmer,Sophie Fellenz,Heike Leitte,Fabian Jirasek,Johannes Lenhard,Hans Hasse*

Main category: cs.CE

TL;DR: OpenAI's model o3 excelled in a challenging university thermodynamics exam, outperforming all students.


<details>
  <summary>Details</summary>
Motivation: To explore if advanced AI models like o3 can surpass human capabilities in solving complex intellectual tasks like thermodynamics exams.

Method: Conducted a rigorous comparison between students and OpenAI's o3 model in a university-level thermodynamics exam, evaluating o3 in zero-shot mode.

Result: The AI achieved perfect scores, surpassing thousands of human participants in an exam with historically low success rates.

Conclusion: AI's capability to excel in such tasks raises important questions for engineering education and professional practices.

Abstract: In this short note, we report and analyze a striking event: OpenAI's large
language model o3 has outwitted all students in a university exam on
thermodynamics. The thermodynamics exam is a difficult hurdle for most
students, where they must show that they have mastered the fundamentals of this
important topic. Consequently, the failure rates are very high, A-grades are
rare - and they are considered proof of the students' exceptional intellectual
abilities. This is because pattern learning does not help in the exam. The
problems can only be solved by knowledgeably and creatively combining
principles of thermodynamics. We have given our latest thermodynamics exam not
only to the students but also to OpenAI's most powerful reasoning model, o3,
and have assessed the answers of o3 exactly the same way as those of the
students. In zero-shot mode, the model o3 solved all problems correctly, better
than all students who took the exam; its overall score was in the range of the
best scores we have seen in more than 10,000 similar exams since 1985. This is
a turning point: machines now excel in complex tasks, usually taken as proof of
human intellectual capabilities. We discuss the consequences this has for the
work of engineers and the education of future engineers.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [393] [Detecting malignant dynamics on very few blood sample using signature coefficients](https://arxiv.org/abs/2506.09097)
*Rémi Vaucher,Stéphane Chrétien*

Main category: q-bio.QM

TL;DR: The paper explores the use of Signature theory combined with ctDNA dynamics modeling for early and efficient cancer detection, addressing challenges like data scarcity.


<details>
  <summary>Details</summary>
Motivation: To improve early and accurate detection of aggressive tumors using ctDNA levels, leveraging its non-invasive nature and data patterns for multi-cancer diagnosis.

Method: The authors utilized a novel approach by combining continuous time Markov modeling of ctDNA dynamics with Signature theory for feature extraction, targeting irregularly sampled data from limited blood samples.

Result: The proposed method effectively handled data scarcity and demonstrated efficiency in cancer detection through extensive numerical experiments.

Conclusion: Signature theory, integrated with ctDNA dynamics modeling, offers a promising pathway for non-invasive and early cancer detection amidst data challenges.

Abstract: Recent discoveries have suggested that the promising avenue of using
circulating tumor DNA (ctDNA) levels in blood samples provides reasonable
accuracy for cancer monitoring, with extremely low burden on the patient's
side. It is known that the presence of ctDNA can result from various mechanisms
leading to DNA release from cells, such as apoptosis, necrosis or active
secretion. One key idea in recent cancer monitoring studies is that monitoring
the dynamics of ctDNA levels might be sufficient for early multi-cancer
detection. This interesting idea has been turned into commercial products, e.g.
in the company named GRAIL.
  In the present work, we propose to explore the use of Signature theory for
detecting aggressive cancer tumors based on the analysis of blood samples. Our
approach combines tools from continuous time Markov modelling for the dynamics
of ctDNA levels in the blood, with Signature theory for building efficient
testing procedures. Signature theory is a topic of growing interest in the
Machine Learning community (see Chevyrev2016 and Fermanian2021), which is now
recognised as a powerful feature extraction tool for irregularly sampled
signals. The method proposed in the present paper is shown to correctly address
the challenging problem of overcoming the inherent data scarsity due to the
extremely small number of blood samples per patient. The relevance of our
approach is illustrated with extensive numerical experiments that confirm the
efficiency of the proposed pipeline.

</details>


### [394] [Simulation-trained conditional normalizing flows for likelihood approximation: a case study in stress regulation kinetics in yeast](https://arxiv.org/abs/2506.09374)
*Pedro Pessoa,Juan Andres Martinez,Vincent Vandenbroucke,Frank Delvigne,Steve Pressé*

Main category: q-bio.QM

TL;DR: This paper addresses protein production estimation in dividing cells, where non-Markovian dynamics complicate likelihood construction, using conditional normalizing flows. They apply this to analyze yeast gene expression.


<details>
  <summary>Details</summary>
Motivation: The motivation is to solve the challenge of constructing tractable likelihood functions for models with history-dependent dynamics, such as in protein production in dividing cells.

Method: The authors utilize conditional normalizing flows (neural network models) to approximate otherwise intractable likelihoods based on simulated data, with a specific focus on protein-measurement scenarios.

Result: By applying this model to analyze yeast 'glc3' gene expression, the study showed that traditional analysis overestimates activation levels. The corrected approach indicated mostly inactive and transient expression under nutrient-limiting conditions.

Conclusion: Explicitly accounting for cell division dynamics through conditional normalizing flows offers deeper insights, revealing that 'glc3' gene expression is rare and short-lived under stress.

Abstract: Physics-inspired inference often hinges on the ability to construct a
likelihood, or the probability of observing a sequence of data given a model.
These likelihoods can be directly maximized for parameter estimation,
incorporated into Bayesian frameworks, or even used as loss functions in neural
networks. Yet, many models, despite being conceptually simple, lack tractable
likelihoods. A notable example arises in estimating protein production from
snapshot measurements of actively dividing cells. Here, the challenge stems
from cell divisions occurring at non-Exponentially distributed intervals with
each division stochastically partitioning protein content between daughter
cells, making protein counts in any given cell a function of its full division
history. Such history dependence precludes a straightforward likelihood based
on a (standard Markovian) master equation. Instead, we employ conditional
normalizing flows (a class of neural network models designed to learn
probability distributions) to approximate otherwise intractable likelihoods
from simulated data. As a case study, we examine activation of the \emph{glc3}
gene in yeast involved in glycogen synthesis and expressed under
nutrient-limiting conditions. We monitor this activity using snapshot
fluorescence measurements via flow cytometry, where GFP expression reflects
\emph{glc3} promoter activity. A na\"ive analysis of flow cytometry data
ignoring cell division suggests many cells are active with low expression.
However, fluorescent proteins persist and can be inherited, so cells may appear
active from retaining ancestral fluorescence. Explicitly accounting for the
(non-Markovian) effects of cell division reveals \emph{glc3} is mostly inactive
under stress, showing that while cells occasionally activate it, expression is
brief and transient.

</details>


### [395] [Reconstructing Heterogeneous Biomolecules via Hierarchical Gaussian Mixtures and Part Discovery](https://arxiv.org/abs/2506.09063)
*Shayan Shekarforoush,David B. Lindell,Marcus A. Brubaker,David J. Fleet*

Main category: q-bio.QM

TL;DR: This paper introduces CryoSPIRE, a 3D molecular reconstruction method for cryo-EM that handles particle conformational and compositional variability, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To address limitations in modeling 3D molecular structures from noisy cryo-EM data, especially when particles display non-rigid flexibility and compositional variation.

Method: A hierarchical Gaussian mixture model approach inspired by Gaussian Splatting, combined with part-based segmentation for inductive bias. Named CryoSPIRE, this framework analyzes cryo-EM data.

Result: CryoSPIRE captures biologically meaningful structures on complex datasets and outperforms existing methods on the CryoBench benchmark.

Conclusion: CryoSPIRE is a significant advancement in cryo-EM heterogeneity modeling, offering a robust framework for understanding flexible and variable molecular structures.

Abstract: Cryo-EM is a transformational paradigm in molecular biology where
computational methods are used to infer 3D molecular structure at atomic
resolution from extremely noisy 2D electron microscope images. At the forefront
of research is how to model the structure when the imaged particles exhibit
non-rigid conformational flexibility and compositional variation where parts
are sometimes missing. We introduce a novel 3D reconstruction framework with a
hierarchical Gaussian mixture model, inspired in part by Gaussian Splatting for
4D scene reconstruction. In particular, the structure of the model is grounded
in an initial process that infers a part-based segmentation of the particle,
providing essential inductive bias in order to handle both conformational and
compositional variability. The framework, called CryoSPIRE, is shown to reveal
biologically meaningful structures on complex experimental datasets, and
establishes a new state-of-the-art on CryoBench, a benchmark for cryo-EM
heterogeneity methods.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [396] [Almost-Optimal Local-Search Methods for Sparse Tensor PCA](https://arxiv.org/abs/2506.09959)
*Max Lovig,Conor Sheehan,Konstantinos Tsirkas,Ilias Zadik*

Main category: math.ST

TL;DR: The paper develops local-search methods that rival the best polynomial-time algorithms for sparse tensor PCA, innovating random-threshold techniques to improve performance.


<details>
  <summary>Details</summary>
Motivation: Local-search methods are commonly used in statistical applications, but their theoretical understanding lags behind methods like low-degree polynomials and spectral approaches.

Method: The proposed framework includes greedy and randomized greedy algorithms applied to the posterior model and introduces a random-threshold variant to enhance trajectory analysis.

Result: The introduced random-threshold methods close the 'local-computational' gap for sparse tensor PCA, achieving comparable performance to leading polynomial-time techniques.

Conclusion: The innovations in local-search methods, especially random-threshold variants, tighten mathematical analysis and could independently benefit broader research areas.

Abstract: Local-search methods are widely employed in statistical applications, yet
interestingly, their theoretical foundations remain rather underexplored,
compared to other classes of estimators such as low-degree polynomials and
spectral methods. Of note, among the few existing results recent studies have
revealed a significant "local-computational" gap in the context of a
well-studied sparse tensor principal component analysis (PCA), where a broad
class of local Markov chain methods exhibits a notable underperformance
relative to other polynomial-time algorithms. In this work, we propose a series
of local-search methods that provably "close" this gap to the best known
polynomial-time procedures in multiple regimes of the model, including and
going beyond the previously studied regimes in which the broad family of local
Markov chain methods underperforms. Our framework includes: (1) standard greedy
and randomized greedy algorithms applied to the (regularized) posterior of the
model; and (2) novel random-threshold variants, in which the randomized greedy
algorithm accepts a proposed transition if and only if the corresponding change
in the Hamiltonian exceeds a random Gaussian threshold-rather that if and only
if it is positive, as is customary. The introduction of the random thresholds
enables a tight mathematical analysis of the randomized greedy algorithm's
trajectory by crucially breaking the dependencies between the iterations, and
could be of independent interest to the community.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [397] [You Are What You Say: Exploiting Linguistic Content for VoicePrivacy Attacks](https://arxiv.org/abs/2506.09521)
*Ünal Ege Gaznepoglu,Anna Leschanowsky,Ahmad Aloradi,Prachi Singh,Daniel Tenbrinck,Emanuël A. P. Habets,Nils Peters*

Main category: eess.AS

TL;DR: The paper evaluates the impact of linguistic content similarity on speaker anonymization systems by adapting BERT as an ASV system, revealing biases in existing datasets.


<details>
  <summary>Details</summary>
Motivation: To assess whether linguistic content similarity in datasets influences the effectiveness and fairness of speaker anonymization systems and their privacy evaluations.

Method: The authors adapted BERT, a language model, to function as an automatic speaker verification (ASV) system and tested it on the VoicePrivacy Attacker Challenge datasets.

Result: The adapted system achieved a mean equal error rate (EER) of 35%, with some speakers showing EERs as low as 2%, linking system decisions to semantically similar keywords derived from dataset curation.

Conclusion: The findings highlight biases in the VoicePrivacy datasets, suggesting a need to revise these datasets and reconsider the use of global EER for privacy evaluations.

Abstract: Speaker anonymization systems hide the identity of speakers while preserving
other information such as linguistic content and emotions. To evaluate their
privacy benefits, attacks in the form of automatic speaker verification (ASV)
systems are employed. In this study, we assess the impact of intra-speaker
linguistic content similarity in the attacker training and evaluation datasets,
by adapting BERT, a language model, as an ASV system. On the VoicePrivacy
Attacker Challenge datasets, our method achieves a mean equal error rate (EER)
of 35%, with certain speakers attaining EERs as low as 2%, based solely on the
textual content of their utterances. Our explainability study reveals that the
system decisions are linked to semantically similar keywords within utterances,
stemming from how LibriSpeech is curated. Our study suggests reworking the
VoicePrivacy datasets to ensure a fair and unbiased evaluation and challenge
the reliance on global EER for privacy evaluations.

</details>


### [398] [Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal Localization of Prolonged Exposure Therapy Elements](https://arxiv.org/abs/2506.09707)
*Suhas BN,Andrew M. Sherrill,Jyoti Alaparthi,Dominik Mattioli,Rosa I. Arriaga,Chris W. Wiese,Saeed Abdullah*

Main category: eess.AS

TL;DR: This paper introduces a scalable method to automatically determine the temporal localization of key elements in Prolonged Exposure (PE) therapy sessions for PTSD, using a fine-tuned Qwen2-Audio model.


<details>
  <summary>Details</summary>
Motivation: Current methods for assessing therapist adherence in PE therapy require manual review of session recordings, which is time-consuming and resource-intensive.

Method: The authors fine-tuned Qwen2-Audio, a pre-trained audio-language model, using a Low-Rank Adaptation (LoRA) method to automatically identify the start and stop times of core PE therapy phases from 30-second audio-transcript windows. Fidelity labels were generated and validated by LLM-based prompting and trained raters.

Result: The fine-tuned model achieved a mean absolute error (MAE) of 5.3 seconds in temporal localization tasks on a dataset of 313 real PE therapy sessions. Window size and LoRA rank were shown to affect performance.

Conclusion: The study provides an effective and scalable model for tracking key PE therapy fidelity elements, with implications in improving clinician training, supervision, and quality assurance.

Abstract: Prolonged Exposure (PE) therapy is an effective treatment for post-traumatic
stress disorder (PTSD), but evaluating therapist fidelity remains
labor-intensive due to the need for manual review of session recordings. We
present a method for the automatic temporal localization of key PE fidelity
elements -- identifying their start and stop times -- directly from session
audio and transcripts. Our approach fine-tunes a large pre-trained
audio-language model, Qwen2-Audio, using Low-Rank Adaptation (LoRA) to process
focused 30-second windows of audio-transcript input. Fidelity labels for three
core protocol phases -- therapist orientation (P1), imaginal exposure (P2), and
post-imaginal processing (P3) -- are generated via LLM-based prompting and
verified by trained raters. The model is trained to predict normalized boundary
offsets using soft supervision guided by task-specific prompts. On a dataset of
313 real PE sessions, our best configuration (LoRA rank 8, 30s windows)
achieves a mean absolute error (MAE) of 5.3 seconds across tasks. We further
analyze the effects of window size and LoRA rank, highlighting the importance
of context granularity and model adaptation. This work introduces a scalable
framework for fidelity tracking in PE therapy, with potential to support
clinician training, supervision, and quality assurance.

</details>


### [399] [Regularizing Learnable Feature Extraction for Automatic Speech Recognition](https://arxiv.org/abs/2506.09804)
*Peter Vieting,Maximilian Kannen,Benedikt Hilmes,Ralf Schlüter,Hermann Ney*

Main category: eess.AS

TL;DR: This research addresses the performance gap in ASR systems with neural front-ends by introducing new regularization techniques, including audio perturbation and modified SpecAugment.


<details>
  <summary>Details</summary>
Motivation: The work aims to overcome the susceptibility of neural front-ends in ASR systems to overfitting, which impacts their effectiveness compared to traditional feature extraction methods.

Method: The study investigates audio perturbation techniques and proposes a modification to SpecAugment by applying masking in the STFT-domain to improve the robustness of learnable feature front-ends.

Result: The combination of audio perturbation and STFT-domain SpecAugment successfully reduces overfitting and eliminates the performance disadvantage of neural front-ends relative to traditional ones.

Conclusion: By applying the proposed regularization methods, ASR systems with neural front-ends achieve comparable performance to those with traditional fixed feature extraction methods, showcasing their viability.

Abstract: Neural front-ends are an appealing alternative to traditional, fixed feature
extraction pipelines for automatic speech recognition (ASR) systems since they
can be directly trained to fit the acoustic model. However, their performance
often falls short compared to classical methods, which we show is largely due
to their increased susceptibility to overfitting. This work therefore
investigates regularization methods for training ASR models with learnable
feature extraction front-ends. First, we examine audio perturbation methods and
show that larger relative improvements can be obtained for learnable features.
Additionally, we identify two limitations in the standard use of SpecAugment
for these front-ends and propose masking in the short time Fourier transform
(STFT)-domain as a simple but effective modification to address these
challenges. Finally, integrating both regularization approaches effectively
closes the performance gap between traditional and learnable features.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [400] [Revolutionizing Clinical Trials: A Manifesto for AI-Driven Transformation](https://arxiv.org/abs/2506.09102)
*Mihaela van der Schaar,Richard Peck,Eoin McKinney,Jim Weatherall,Stuart Bailey,Justine Rochon,Chris Anagnostopoulos,Pierre Marquet,Anthony Wood,Nicky Best,Harry Amad,Julianna Piskorz,Krzysztof Kacprzyk,Rafik Salama,Christina Gunther,Francesca Frau,Antoine Pugeat,Ramon Hernandez*

Main category: cs.CY

TL;DR: This manifesto outlines the transformative potential of AI technologies like causal inference and digital twins in enhancing clinical trials.


<details>
  <summary>Details</summary>
Motivation: To revolutionize clinical trials by making them faster, safer, and more tailored to individual patients using AI technologies.

Method: Proposes actionable integration of causal inference and digital twins into clinical trials within current regulatory frameworks.

Result: Provides a roadmap showcasing how AI can enhance personalization, speed, and safety in clinical research.

Conclusion: AI technologies can redefine the gold standard for clinical trials, significantly improving their effectiveness and efficiency.

Abstract: This manifesto represents a collaborative vision forged by leaders in
pharmaceuticals, consulting firms, clinical research, and AI. It outlines a
roadmap for two AI technologies - causal inference and digital twins - to
transform clinical trials, delivering faster, safer, and more personalized
outcomes for patients. By focusing on actionable integration within existing
regulatory frameworks, we propose a way forward to revolutionize clinical
research and redefine the gold standard for clinical trials using AI.

</details>


### [401] [FAIRTOPIA: Envisioning Multi-Agent Guardianship for Disrupting Unfair AI Pipelines](https://arxiv.org/abs/2506.09107)
*Athena Vakali,Ilias Dimitriadis*

Main category: cs.CY

TL;DR: The paper proposes a framework called FAIRTOPIA to ensure fairness across AI pipelines using multi-role agents.


<details>
  <summary>Details</summary>
Motivation: Address the harmful incidents and fairness criticisms in AI decision-making by ensuring fairness throughout the pipeline.

Method: Introduce FAIRTOPIA, an agent-driven, fairness-by-design framework with a three-layered architecture for robust fairness monitoring.

Result: FAIRTOPIA integrates human-centric fairness principles into AI workflows, inspiring new research and methods for systematic fairness.

Conclusion: Adaptive and realistic AI fairness frameworks, like FAIRTOPIA, could mitigate biases and embed fairness throughout AI lifecycle.

Abstract: AI models have become active decision makers, often acting without human
supervision. The rapid advancement of AI technology has already caused harmful
incidents that have hurt individuals and societies and AI unfairness in heavily
criticized. It is urgent to disrupt AI pipelines which largely neglect human
principles and focus on computational biases exploration at the data (pre),
model(in), and deployment (post) processing stages. We claim that by exploiting
the advances of agents technology, we will introduce cautious, prompt, and
ongoing fairness watch schemes, under realistic, systematic, and human-centric
fairness expectations. We envision agents as fairness guardians, since agents
learn from their environment, adapt to new information, and solve complex
problems by interacting with external tools and other systems. To set the
proper fairness guardrails in the overall AI pipeline, we introduce a
fairness-by-design approach which embeds multi-role agents in an end-to-end
(human to AI) synergetic scheme. Our position is that we may design adaptive
and realistic AI fairness frameworks, and we introduce a generalized algorithm
which can be customized to the requirements and goals of each AI decision
making scenario. Our proposed, so called FAIRTOPIA framework, is structured
over a three-layered architecture, which encapsulates the AI pipeline inside an
agentic guardian and a knowledge-based, self-refining layered scheme. Based on
our proposition, we enact fairness watch in all of the AI pipeline stages,
under robust multi-agent workflows, which will inspire new fairness research
hypothesis, heuristics, and methods grounded in human-centric, systematic,
interdisciplinary, socio-technical principles.

</details>


### [402] [Understanding Human-AI Trust in Education](https://arxiv.org/abs/2506.09160)
*Griffin Pitts,Sanaz Motamedi*

Main category: cs.CY

TL;DR: The study investigates how trust toward AI chatbots, perceived either as human-like or system-like, impacts students' engagement and learning outcomes, and proposes the concept of 'human-AI trust.'


<details>
  <summary>Details</summary>
Motivation: The paper addresses the ambiguity in how students trust AI chatbots—whether akin to interpersonal trust (as with humans) or technology trust (as with non-social tools)—and the lack of theoretical models for anthropomorphic systems.

Method: The researchers used partial least squares structural equation modeling to analyze how human-like and system-like trust affect students' perceptions of enjoyment, trusting intention, behavioral intention to use, and perceived usefulness of an AI chatbot.

Result: Human-like trust predominantly influenced trusting intention, while system-like trust significantly predicted behavioral intention and perceived usefulness. Both types of trust had similar effects on perceived enjoyment.

Conclusion: The study suggests students develop a unique 'human-AI trust' that differs from traditional trust models and emphasizes the need for new theoretical frameworks to optimize trust for AI's educational adoption and impact.

Abstract: As AI chatbots become increasingly integrated in education, students are
turning to these systems for guidance, feedback, and information. However, the
anthropomorphic characteristics of these chatbots create ambiguity regarding
whether students develop trust toward them as they would a human peer or
instructor, based in interpersonal trust, or as they would any other piece of
technology, based in technology trust. This ambiguity presents theoretical
challenges, as interpersonal trust models may inappropriately ascribe human
intentionality and morality to AI, while technology trust models were developed
for non-social technologies, leaving their applicability to anthropomorphic
systems unclear. To address this gap, we investigate how human-like and
system-like trusting beliefs comparatively influence students' perceived
enjoyment, trusting intention, behavioral intention to use, and perceived
usefulness of an AI chatbot - factors associated with students' engagement and
learning outcomes. Through partial least squares structural equation modeling,
we found that human-like and system-like trust significantly influenced student
perceptions, with varied effects. Human-like trust more strongly predicted
trusting intention, while system-like trust better predicted behavioral
intention and perceived usefulness. Both had similar effects on perceived
enjoyment. Given the partial explanatory power of each type of trust, we
propose that students develop a distinct form of trust with AI chatbots
(human-AI trust) that differs from human-human and human-technology models of
trust. Our findings highlight the need for new theoretical frameworks specific
to human-AI trust and offer practical insights for fostering appropriately
calibrated trust, which is critical for the effective adoption and pedagogical
impact of AI in education.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [403] [Terabyte-Scale Analytics in the Blink of an Eye](https://arxiv.org/abs/2506.09226)
*Bowen Wu,Wei Cui,Carlo Curino,Matteo Interlandi,Rathijit Sen*

Main category: cs.DB

TL;DR: This paper explores scaling SQL queries on GPU clusters, leveraging ML/HPC techniques to improve performance with gains of up to 60x.


<details>
  <summary>Details</summary>
Motivation: The growing use of GPU clusters in data centers presents an opportunity for significant performance improvements in distributed data analytics over CPU-only clusters.

Method: The authors devised a performance-maximizing prototype that uses ML/HPC group communication techniques to study the scaling of analytical SQL on GPU clusters.

Result: The prototype demonstrated a performance opportunity of at least 60x, successfully running all 22 TPC-H queries at a 1TB scale before a brief moment.

Conclusion: Distributed GPU clusters offer transformative gains in analytical SQL query processes, indicating a paradigm shift in data analytics infrastructure.

Abstract: For the past two decades, the DB community has devoted substantial research
to take advantage of cheap clusters of machines for distributed data analytics
-- we believe that we are at the beginning of a paradigm shift. The scaling
laws and popularity of AI models lead to the deployment of incredibly powerful
GPU clusters in commercial data centers. Compared to CPU-only solutions, these
clusters deliver impressive improvements in per-node compute, memory bandwidth,
and inter-node interconnect performance. In this paper, we study the problem of
scaling analytical SQL queries on distributed clusters of GPUs, with the stated
goal of establishing an upper bound on the likely performance gains. To do so,
we build a prototype designed to maximize performance by leveraging ML/HPC best
practices, such as group communication primitives for cross-device data
movements. This allows us to conduct thorough performance experimentation to
point our community towards a massive performance opportunity of at least
60$\times$. To make these gains more relatable, before you can blink twice, our
system can run all 22 queries of TPC-H at a 1TB scale factor!

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [404] [SimClass: A Classroom Speech Dataset Generated via Game Engine Simulation For Automatic Speech Recognition Research](https://arxiv.org/abs/2506.09206)
*Ahmed Adel Attia,Jing Liu,Carl Espy-Wilson*

Main category: cs.SD

TL;DR: The paper addresses the lack of large-scale classroom speech data by introducing SimClass, a dataset containing synthesized classroom noise and speech data via a scalable methodology using game engines.


<details>
  <summary>Details</summary>
Motivation: The development of AI-driven speech models for education has been hindered by scarce classroom speech datasets and the unavailability of dedicated classroom noise data.

Method: The researchers leveraged game engines to synthesize classroom noise data and paired a public children's speech corpus with YouTube lecture videos to simulate classroom speech interactions.

Result: SimClass was created, containing synthesized classroom noise and a simulated speech dataset that closely approximates real classroom conditions.

Conclusion: SimClass is a significant resource for training robust speech recognition and enhancement models, overcoming prior data limitations in the educational domain.

Abstract: The scarcity of large-scale classroom speech data has hindered the
development of AI-driven speech models for education. Public classroom datasets
remain limited, and the lack of a dedicated classroom noise corpus prevents the
use of standard data augmentation techniques.
  In this paper, we introduce a scalable methodology for synthesizing classroom
noise using game engines, a framework that extends to other domains. Using this
methodology, we present SimClass, a dataset that includes both a synthesized
classroom noise corpus and a simulated classroom speech dataset. The speech
data is generated by pairing a public children's speech corpus with YouTube
lecture videos to approximate real classroom interactions in clean conditions.
Our experiments on clean and noisy speech demonstrate that SimClass closely
approximates real classroom speech, making it a valuable resource for
developing robust speech recognition and enhancement models.

</details>


### [405] [OWSM-Biasing: Contextualizing Open Whisper-Style Speech Models for Automatic Speech Recognition with Dynamic Vocabulary](https://arxiv.org/abs/2506.09448)
*Yui Sudo,Yusuke Fujita,Atsushi Kojima,Tomoya Mizumoto,Lianbo Liu*

Main category: cs.SD

TL;DR: This paper proposes combining contextual biasing (CB) methods with pre-trained speech foundation models (SFMs) to boost recognition of rare/unseen words while preserving SFMs' advantages, achieving better word error rate (WER) and efficiency.


<details>
  <summary>Details</summary>
Motivation: SFMs like Open Whisper-Style Speech Models excel at general speech recognition but struggle with rare or unseen words; integrating contextual biasing can address this limitation.

Method: The approach integrates a contextual biasing method into OWSM v3.1 by freezing pre-trained parameters of the SFMs, ensuring that their existing knowledge is retained while enhancing CB performance.

Result: The proposed method improved biasing word error rate (B-WER) by 11.6 points, overall WER by 0.9 points, and reduced real-time factor by 7.5% on the LibriSpeech 100 test-clean set compared to a non-biasing baseline.

Conclusion: This integrated method successfully enhances rare word recognition while maintaining the strengths of SFMs, offering both accuracy and efficiency improvements.

Abstract: Speech foundation models (SFMs), such as Open Whisper-Style Speech Models
(OWSM), are trained on massive datasets to achieve accurate automatic speech
recognition. However, even SFMs struggle to accurately recognize rare and
unseen words. While contextual biasing (CB) is a promising approach to improve
recognition of such words, most CB methods are trained from scratch, resulting
in lower performance than SFMs due to the lack of pre-trained knowledge. This
paper integrates an existing CB method with OWSM v3.1 while freezing its
pre-trained parameters. By leveraging the knowledge embedded in SFMs, the
proposed method enables effective CB while preserving the advantages of SFMs,
even with a small dataset. Experimental results show that the proposed method
improves the biasing word error rate (B-WER) by 11.6 points, resulting in a 0.9
point improvement in the overall WER while reducing the real-time factor by
7.5% compared to the non-biasing baseline on the LibriSpeech 100 test-clean
set.

</details>


### [406] [BemaGANv2: A Tutorial and Comparative Survey of GAN-based Vocoders for Long-Term Audio Generation](https://arxiv.org/abs/2506.09487)
*Taesoo Park,Mungwi Jeong,Mingyu Park,Narae Kim,Junyoung Kim,Mujung Kim,Jisang Yoo,Hoyun Lee,Sanghoon Kim,Soonchul Kwon*

Main category: cs.SD

TL;DR: BemaGANv2 is a GAN-based audio vocoder focusing on high-fidelity audio with innovations in both generator and discriminator architectures. It offers improved periodic structure modeling and analysis.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve audio generation fidelity and long-term dependency modeling in GAN-based vocoders to better capture audio periodic structures.

Method: Key innovations include replacing ResBlocks with AMP modules in its generator, utilizing Snake activation for periodic structure modeling, and leveraging MED and MRD discriminators for better periodicity detection and long-term dependencies.

Result: Paper systematically evaluates configurations like MSD+MED, MSD+MRD, and MPD+MED+MRD using objective (FAD, SSIM, PLCC, MCD) and subjective (MOS, SMOS) metrics. Results show architectural improvements over prior methods.

Conclusion: BemaGANv2 is effective in enhancing long-term audio fidelity through architectural innovations. The tutorial and public code ensure reproducibility for further research and development.

Abstract: This paper presents a tutorial-style survey and implementation guide of
BemaGANv2, an advanced GAN-based vocoder designed for high-fidelity and
long-term audio generation. Built upon the original BemaGAN architecture,
BemaGANv2 incorporates major architectural innovations by replacing traditional
ResBlocks in the generator with the Anti-aliased Multi-Periodicity composition
(AMP) module, which internally applies the Snake activation function to better
model periodic structures. In the discriminator framework, we integrate the
Multi-Envelope Discriminator (MED), a novel architecture we originally
proposed, to extract rich temporal envelope features crucial for periodicity
detection. Coupled with the Multi-Resolution Discriminator (MRD), this
combination enables more accurate modeling of long-range dependencies in audio.
We systematically evaluate various discriminator configurations, including MSD
+ MED, MSD + MRD, and MPD + MED + MRD, using objective metrics (FAD, SSIM,
PLCC, MCD) and subjective evaluations (MOS, SMOS). This paper also provides a
comprehensive tutorial on the model architecture, training methodology, and
implementation to promote reproducibility. The code and pre-trained models are
available at: https://github.com/dinhoitt/BemaGANv2.

</details>


### [407] [Training-Free Voice Conversion with Factorized Optimal Transport](https://arxiv.org/abs/2506.09709)
*Alexander Lobashev,Assel Yermekova,Maria Larchenko*

Main category: cs.SD

TL;DR: The paper presents MKL-VC, a training-free method for high-quality, cross-lingual voice conversion using only 5 seconds of reference audio, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: To improve the quality, robustness, and efficiency of cross-lingual voice conversion with limited reference audio.

Method: The method replaces kNN regression with a factorized optimal transport map from Monge-Kantorovich Linear solutions in WavLM embedding subspaces, addressing non-uniform variance and enhancing feature transformation.

Result: Experiments on LibriSpeech and FLEURS datasets show significant improvements in content preservation and robustness over kNN-VC, with performance comparable to FACodec.

Conclusion: MKL-VC is a robust alternative to existing pipelines, particularly effective for cross-lingual voice conversion with minimal reference audio.

Abstract: This paper introduces Factorized MKL-VC, a training-free modification for
kNN-VC pipeline. In contrast with original pipeline, our algorithm performs
high quality any-to-any cross-lingual voice conversion with only 5 second of
reference audio. MKL-VC replaces kNN regression with a factorized optimal
transport map in WavLM embedding subspaces, derived from Monge-Kantorovich
Linear solution. Factorization addresses non-uniform variance across
dimensions, ensuring effective feature transformation. Experiments on
LibriSpeech and FLEURS datasets show MKL-VC significantly improves content
preservation and robustness with short reference audio, outperforming kNN-VC.
MKL-VC achieves performance comparable to FACodec, especially in cross-lingual
voice conversion domain.

</details>


### [408] [Incorporating Linguistic Constraints from External Knowledge Source for Audio-Visual Target Speech Extraction](https://arxiv.org/abs/2506.09792)
*Wenxuan Wu,Shuai Wang,Xixin Wu,Helen Meng,Haizhou Li*

Main category: cs.SD

TL;DR: The paper enhances audio-visual target speaker extraction (AV-TSE) by incorporating linguistic constraints from pre-trained speech-language and language models, improving performance without extra inference costs.


<details>
  <summary>Details</summary>
Motivation: To enhance AV-TSE models by leveraging humans' use of linguistic knowledge for improved speech perception.

Method: Integrating linguistic constraints from pre-trained speech-language models and pre-trained language models as supervision signals for AV-TSE without increasing inference computational costs.

Result: The proposed method consistently improves speech quality, intelligibility, and robustness, particularly in multi-language and visual cue-impaired settings.

Conclusion: Incorporating linguistic constraints into AV-TSE models enhances performance across varying conditions while maintaining computational efficiency during inference.

Abstract: Audio-visual target speaker extraction (AV-TSE) models primarily rely on
target visual cues to isolate the target speaker's voice from others. We know
that humans leverage linguistic knowledge, such as syntax and semantics, to
support speech perception. Inspired by this, we explore the potential of
pre-trained speech-language models (PSLMs) and pre-trained language models
(PLMs) as auxiliary knowledge sources for AV-TSE. In this study, we propose
incorporating the linguistic constraints from PSLMs or PLMs for the AV-TSE
model as additional supervision signals. Without introducing any extra
computational cost during inference, the proposed approach consistently
improves speech quality and intelligibility. Furthermore, we evaluate our
method in multi-language settings and visual cue-impaired scenarios and show
robust performance gains.

</details>


### [409] [UmbraTTS: Adapting Text-to-Speech to Environmental Contexts with Flow Matching](https://arxiv.org/abs/2506.09874)
*Neta Glazer,Aviv Navon,Yael Segal,Aviv Shamsian,Hilit Segev,Asaf Buchnick,Menachem Pirchi,Gil Hetz,Joseph Keshet*

Main category: cs.SD

TL;DR: UmbraTTS is a novel model for generating speech and environmental audio together, achieving context-aware, high-quality audio scenes without requiring paired training data.


<details>
  <summary>Details</summary>
Motivation: To address the difficulty of integrating speech synthesis with complex background audio environments.

Method: Developed UmbraTTS, a flow-matching TTS model with a self-supervised framework for extracting necessary data from unannotated recordings.

Result: UmbraTTS demonstrated significant performance improvements over existing baselines and achieved natural and environmentally aware audio synthesis.

Conclusion: The model enables fine-grained control over audio scenes and represents a step forward in TTS technology for producing coherent audio environments.

Abstract: Recent advances in Text-to-Speech (TTS) have enabled highly natural speech
synthesis, yet integrating speech with complex background environments remains
challenging. We introduce UmbraTTS, a flow-matching based TTS model that
jointly generates both speech and environmental audio, conditioned on text and
acoustic context. Our model allows fine-grained control over background volume
and produces diverse, coherent, and context-aware audio scenes. A key challenge
is the lack of data with speech and background audio aligned in natural
context. To overcome the lack of paired training data, we propose a
self-supervised framework that extracts speech, background audio, and
transcripts from unannotated recordings. Extensive evaluations demonstrate that
UmbraTTS significantly outperformed existing baselines, producing natural,
high-quality, environmentally aware audios.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [410] [Integrated Analysis for Electronic Health Records with Structured and Sporadic Missingness](https://arxiv.org/abs/2506.09208)
*Jianbin Tan,Yan Zhang,Chuan Hong,T. Tony Cai,Tianxi Cai,Anru R. Zhang*

Main category: stat.AP

TL;DR: The paper introduces Macomss, an imputation method for handling structured and sporadic missing data in Electronic Health Records (EHRs). It outperforms existing methods and supports robust integrated analysis.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address gaps caused by structured and sporadic missingness in EHRs during integrated analysis, which limits data utility and research in population health.

Method: The authors propose Macomss, a method with theoretical guarantees, validated through simulations and real-world EHR data from Duke University Health System (DUHS).

Result: Macomss consistently demonstrates lower imputation errors and superior or comparable performance in downstream predictions compared to current methods.

Conclusion: Macomss provides a reliable and robust imputation framework, facilitating accurate analysis of integrated EHR datasets and advancing population health research.

Abstract: Objectives: We propose a novel imputation method tailored for Electronic
Health Records (EHRs) with structured and sporadic missingness. Such
missingness frequently arises in the integration of heterogeneous EHR datasets
for downstream clinical applications. By addressing these gaps, our method
provides a practical solution for integrated analysis, enhancing data utility
and advancing the understanding of population health.
  Materials and Methods: We begin by demonstrating structured and sporadic
missing mechanisms in the integrated analysis of EHR data. Following this, we
introduce a novel imputation framework, Macomss, specifically designed to
handle structurally and heterogeneously occurring missing data. We establish
theoretical guarantees for Macomss, ensuring its robustness in preserving the
integrity and reliability of integrated analyses. To assess its empirical
performance, we conduct extensive simulation studies that replicate the complex
missingness patterns observed in real-world EHR systems, complemented by
validation using EHR datasets from the Duke University Health System (DUHS).
  Results: Simulation studies show that our approach consistently outperforms
existing imputation methods. Using datasets from three hospitals within DUHS,
Macomss achieves the lowest imputation errors for missing data in most cases
and provides superior or comparable downstream prediction performance compared
to benchmark methods.
  Conclusions: We provide a theoretically guaranteed and practically meaningful
method for imputing structured and sporadic missing data, enabling accurate and
reliable integrated analysis across multiple EHR datasets. The proposed
approach holds significant potential for advancing research in population
health.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [411] [Intelligent System of Emergent Knowledge: A Coordination Fabric for Billions of Minds](https://arxiv.org/abs/2506.09335)
*Moshi Wei,Sparks Li*

Main category: cs.MA

TL;DR: ISEK introduces a decentralized platform integrating human and AI agents for collaborative cognitive tasks, utilizing Web3 technologies, blockchain, and a unique token mechanism.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of centralized platforms by creating a self-organizing, decentralized system where humans and AI can collaborate effectively.

Method: ISEK employs Web3 infrastructure, decentralized AI-human collaboration, distributed consensus, a six-phase task allocation process, $ISEK token-driven economy, NFT-based identity, and fault-tolerant mechanisms.

Result: ISEK successfully demonstrates an innovative decentralized framework for large-scale collaboration between human and AI agents, emphasizing adaptability and decentralized governance.

Conclusion: ISEK advances the state of decentralized systems, paving the way for scalable, organic AI-human ecosystems that surpass centralized limitations.

Abstract: The Intelligent System of Emergent Knowledge (ISEK) establishes a
decentralized network where human and artificial intelligence agents
collaborate as peers, forming a self-organizing cognitive ecosystem. Built on
Web3 infrastructure, ISEK combines three fundamental principles: (1) a
decentralized multi-agent architecture resistant to censorship, (2) symbiotic
AI-human collaboration with equal participation rights, and (3) resilient
self-adaptation through distributed consensus mechanisms.
  The system implements an innovative coordination protocol featuring a
six-phase workflow (Publish, Discover, Recruit, Execute, Settle, Feedback) for
dynamic task allocation, supported by robust fault tolerance and a
multidimensional reputation system. Economic incentives are governed by the
native $ISEK token, facilitating micropayments, governance participation, and
reputation tracking, while agent sovereignty is maintained through NFT-based
identity management.
  This synthesis of blockchain technology, artificial intelligence, and
incentive engineering creates an infrastructure that actively facilitates
emergent intelligence. ISEK represents a paradigm shift from conventional
platforms, enabling the organic development of large-scale, decentralized
cognitive systems where autonomous agents collectively evolve beyond
centralized constraints.

</details>


### [412] [When Is Diversity Rewarded in Cooperative Multi-Agent Learning?](https://arxiv.org/abs/2506.09434)
*Michael Amir,Matteo Bettini,Amanda Prorok*

Main category: cs.MA

TL;DR: This paper investigates when heterogeneous teams outperform homogeneous ones in multi-agent task allocation, using theoretical insights and proposing a new method for exploring reward design.


<details>
  <summary>Details</summary>
Motivation: To understand under what conditions behavioral diversity and specialization in teams (heterogeneity) provide advantages over uniformity, particularly in robotics, nature, and society, with a focus on reward design in multi-agent systems.

Method: The authors analyze reward design through theoretical curvature properties of aggregation operators and propose Heterogeneous Environment Design (HED), a MARL-based algorithm to identify advantageous settings for heterogeneity.

Result: Theoretical analysis shows curvature determines heterogeneity benefits, validated by experiments in matrix games and a Multi-Goal-Capture setting where HED effectively identifies reward conditions that favor diversity.

Conclusion: This study provides theoretical tools and computational methods to discern when and why heterogeneity is beneficial in multi-agent systems, offering insights for reward design in such settings.

Abstract: The success of teams in robotics, nature, and society often depends on the
division of labor among diverse specialists; however, a principled explanation
for when such diversity surpasses a homogeneous team is still missing. Focusing
on multi-agent task allocation problems, our goal is to study this question
from the perspective of reward design: what kinds of objectives are best suited
for heterogeneous teams? We first consider an instantaneous, non-spatial
setting where the global reward is built by two generalized aggregation
operators: an inner operator that maps the $N$ agents' effort allocations on
individual tasks to a task score, and an outer operator that merges the $M$
task scores into the global team reward. We prove that the curvature of these
operators determines whether heterogeneity can increase reward, and that for
broad reward families this collapses to a simple convexity test. Next, we ask
what incentivizes heterogeneity to emerge when embodied, time-extended agents
must learn an effort allocation policy. To study heterogeneity in such
settings, we use multi-agent reinforcement learning (MARL) as our computational
paradigm, and introduce Heterogeneous Environment Design (HED), a
gradient-based algorithm that optimizes the parameter space of underspecified
MARL environments to find scenarios where heterogeneity is advantageous.
Experiments in matrix games and an embodied Multi-Goal-Capture environment show
that, despite the difference in settings, HED rediscovers the reward regimes
predicted by our theory to maximize the advantage of heterogeneity, both
validating HED and connecting our theoretical insights to reward design in
MARL. Together, these results help us understand when behavioral diversity
delivers a measurable benefit.

</details>


### [413] [Effective Red-Teaming of Policy-Adherent Agents](https://arxiv.org/abs/2506.09600)
*Itay Nakash,George Kour,Koren Lazar,Matan Vetzler,Guy Uziel,Ateret Anaby-Tavor*

Main category: cs.MA

TL;DR: This paper introduces tools and strategies to defend task-oriented LLM agents from adversarial manipulative behaviors.


<details>
  <summary>Details</summary>
Motivation: Task-oriented LLM-based agents must adhere strictly to rules and policies in sensitive domains while maintaining helpful interactions, but they are vulnerable to manipulation by adversarial users.

Method: The authors propose CRAFT, a multi-agent red-teaming system to test agent resilience using policy-aware adversarial strategies. Additionally, they present tau-break, a benchmark to assess robustness against manipulative behaviors.

Result: The CRAFT system demonstrated superior capabilities in undermining policy adherence compared to traditional methods. The tau-break benchmark provided a rigorous evaluation of susceptibility to adversarial attacks.

Conclusion: Existing defenses provide limited protection against adversarial manipulation, signaling the need for stronger, research-focused safeguards for policy-adherent agents.

Abstract: Task-oriented LLM-based agents are increasingly used in domains with strict
policies, such as refund eligibility or cancellation rules. The challenge lies
in ensuring that the agent consistently adheres to these rules and policies,
appropriately refusing any request that would violate them, while still
maintaining a helpful and natural interaction. This calls for the development
of tailored design and evaluation methodologies to ensure agent resilience
against malicious user behavior. We propose a novel threat model that focuses
on adversarial users aiming to exploit policy-adherent agents for personal
benefit. To address this, we present CRAFT, a multi-agent red-teaming system
that leverages policy-aware persuasive strategies to undermine a
policy-adherent agent in a customer-service scenario, outperforming
conventional jailbreak methods such as DAN prompts, emotional manipulation, and
coercive. Building upon the existing tau-bench benchmark, we introduce
tau-break, a complementary benchmark designed to rigorously assess the agent's
robustness against manipulative user behavior. Finally, we evaluate several
straightforward yet effective defense strategies. While these measures provide
some protection, they fall short, highlighting the need for stronger,
research-driven safeguards to protect policy-adherent agents from adversarial
attacks

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [414] [Automatic Treatment Planning using Reinforcement Learning for High-dose-rate Prostate Brachytherapy](https://arxiv.org/abs/2506.09805)
*Tonghe Wang,Yining Feng,Xiaofeng Yang*

Main category: physics.med-ph

TL;DR: This paper explores using reinforcement learning (RL) to optimize needle placement and dwell times for HDR prostate brachytherapy, enabling faster, consistent, and effective treatment plans.


<details>
  <summary>Details</summary>
Motivation: Current HDR prostate brachytherapy relies heavily on physician experience, leading to potential inconsistencies and longer procedure times. The authors sought to investigate whether RL could standardize the process while maintaining or improving plan quality.

Method: The authors developed an RL agent to optimize needle positioning and dwell times based on patient anatomy, using a reward function to guide decision-making. The system iteratively adjusted needle settings until an optimal plan was achieved. Data from 11 patients were used for training and testing.

Result: The RL-generated plans matched or outperformed clinical plans in prostate coverage and rectal sparing while reducing prostate hotspots and urethral doses. Additionally, RL plans used fewer needles compared to clinical plans.

Conclusion: This study demonstrates that reinforcement learning can autonomously create HDR prostate brachytherapy plans with equal or better quality compared to conventional methods. The method has potential to standardize planning practices, minimize variability, and improve patient care overall.

Abstract: Purpose: In high-dose-rate (HDR) prostate brachytherapy procedures, the
pattern of needle placement solely relies on physician experience. We
investigated the feasibility of using reinforcement learning (RL) to provide
needle positions and dwell times based on patient anatomy during pre-planning
stage. This approach would reduce procedure time and ensure consistent plan
quality. Materials and Methods: We train a RL agent to adjust the position of
one selected needle and all the dwell times on it to maximize a pre-defined
reward function after observing the environment. After adjusting, the RL agent
then moves on to the next needle, until all needles are adjusted. Multiple
rounds are played by the agent until the maximum number of rounds is reached.
Plan data from 11 prostate HDR boost patients (1 for training, and 10 for
testing) treated in our clinic were included in this study. The dosimetric
metrics and the number of used needles of RL plan were compared to those of the
clinical results (ground truth). Results: On average, RL plans and clinical
plans have very similar prostate coverage (Prostate V100) and Rectum D2cc (no
statistical significance), while RL plans have less prostate hotspot (Prostate
V150) and Urethra D20% plans with statistical significance. Moreover, RL plans
use 2 less needles than clinical plan on average. Conclusion: We present the
first study demonstrating the feasibility of using reinforcement learning to
autonomously generate clinically practical HDR prostate brachytherapy plans.
This RL-based method achieved equal or improved plan quality compared to
conventional clinical approaches while requiring fewer needles. With minimal
data requirements and strong generalizability, this approach has substantial
potential to standardize brachytherapy planning, reduce clinical variability,
and enhance patient outcomes.

</details>
