<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 28]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.CL](#cs.CL) [Total: 45]
- [cs.CV](#cs.CV) [Total: 98]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.LG](#cs.LG) [Total: 50]
- [cs.NE](#cs.NE) [Total: 2]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.RO](#cs.RO) [Total: 25]
- [cs.SE](#cs.SE) [Total: 12]
- [q-bio.NC](#q-bio.NC) [Total: 3]
- [stat.ML](#stat.ML) [Total: 6]
- [nlin.CG](#nlin.CG) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.SC](#cs.SC) [Total: 1]
- [cs.CR](#cs.CR) [Total: 18]
- [cs.DM](#cs.DM) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [quant-ph](#quant-ph) [Total: 3]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [stat.CO](#stat.CO) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.GT](#cs.GT) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [cs.LO](#cs.LO) [Total: 2]
- [eess.IV](#eess.IV) [Total: 5]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [astro-ph.CO](#astro-ph.CO) [Total: 1]
- [cs.CY](#cs.CY) [Total: 3]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.HC](#cs.HC) [Total: 8]
- [cs.MA](#cs.MA) [Total: 2]
- [math.OC](#math.OC) [Total: 2]
- [cs.SD](#cs.SD) [Total: 6]
- [cs.IR](#cs.IR) [Total: 25]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization](https://arxiv.org/abs/2508.05731)
*Yuhang Liu,Zeyu Liu,Shuanghe Zhu,Pengxiang Li,Congkai Xie,Jiasheng Wang,Xueyu Hu,Xiaotian Han,Jianbo Yuan,Xinyao Wang,Shengyu Zhang,Hongxia Yang,Fei Wu*

Main category: cs.AI

TL;DR: The paper proposes Adaptive Exploration Policy Optimization (AEPO) to overcome inefficiencies in semantic alignment for Multimodal Large Language Models (MLLMs) operating on GUIs, achieving state-of-the-art results on benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of robustly grounding natural language instructions in GUI-based MLLMs, where current models face issues with semantic alignment and inefficient exploration.

Method: Introduces AEPO, a policy optimization framework with a multi-answer generation strategy and an Adaptive Exploration Reward (AER) function based on efficiency to enhance exploration and improve learning of semantic associations.

Result: AEPO-trained models (InfiGUI-G1-3B and InfiGUI-G1-7B) significantly outperform the RLVR baseline, showing up to 9.0% improvement in GUI grounding benchmarks aimed at testing generalization and semantic understanding.

Conclusion: AEPO effectively addresses exploration inefficiencies, establishing new state-of-the-art methods for GUI grounding tasks and advancing the semantic understanding capabilities of MLLMs.

Abstract: The emergence of Multimodal Large Language Models (MLLMs) has propelled the
development of autonomous agents that operate on Graphical User Interfaces
(GUIs) using pure visual input. A fundamental challenge is robustly grounding
natural language instructions. This requires a precise spatial alignment, which
accurately locates the coordinates of each element, and, more critically, a
correct semantic alignment, which matches the instructions to the functionally
appropriate UI element. Although Reinforcement Learning with Verifiable Rewards
(RLVR) has proven to be effective at improving spatial alignment for these
MLLMs, we find that inefficient exploration bottlenecks semantic alignment,
which prevent models from learning difficult semantic associations. To address
this exploration problem, we present Adaptive Exploration Policy Optimization
(AEPO), a new policy optimization framework. AEPO employs a multi-answer
generation strategy to enforce broader exploration, which is then guided by a
theoretically grounded Adaptive Exploration Reward (AER) function derived from
first principles of efficiency eta=U/C. Our AEPO-trained models, InfiGUI-G1-3B
and InfiGUI-G1-7B, establish new state-of-the-art results across multiple
challenging GUI grounding benchmarks, achieving significant relative
improvements of up to 9.0% against the naive RLVR baseline on benchmarks
designed to test generalization and semantic understanding. Resources are
available at https://github.com/InfiXAI/InfiGUI-G1.

</details>


### [2] [A Framework for Inherently Safer AGI through Language-Mediated Active Inference](https://arxiv.org/abs/2508.05766)
*Bo Wen*

Main category: cs.AI

TL;DR: This paper introduces a framework for safer Artificial General Intelligence (AGI) using Active Inference principles combined with Large Language Models (LLMs).


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the limitations of traditional AI safety measures based on post-hoc interpretability and reward engineering by proposing a more integrated approach.

Method: The framework integrates safety through transparent belief representations, hierarchical value alignment, and multi-agent systems guided by Active Inference principles. It employs the natural language for oversight and includes safety mechanisms like belief-preference separation, resource-aware decision-making, and modular agent structures.

Result: The framework is proposed to ensure safety guarantees within AGI development and is validated through experiments using the Abstraction and Reasoning Corpus (ARC) benchmark.

Conclusion: The study suggests that AGI can be developed to be inherently safe by embedding safety principles into its core design rather than retrofitting them later.

Abstract: This paper proposes a novel framework for developing safe Artificial General
Intelligence (AGI) by combining Active Inference principles with Large Language
Models (LLMs). We argue that traditional approaches to AI safety, focused on
post-hoc interpretability and reward engineering, have fundamental limitations.
We present an architecture where safety guarantees are integrated into the
system's core design through transparent belief representations and
hierarchical value alignment. Our framework leverages natural language as a
medium for representing and manipulating beliefs, enabling direct human
oversight while maintaining computational tractability. The architecture
implements a multi-agent system where agents self-organize according to Active
Inference principles, with preferences and safety constraints flowing through
hierarchical Markov blankets. We outline specific mechanisms for ensuring
safety, including: (1) explicit separation of beliefs and preferences in
natural language, (2) bounded rationality through resource-aware free energy
minimization, and (3) compositional safety through modular agent structures.
The paper concludes with a research agenda centered on the Abstraction and
Reasoning Corpus (ARC) benchmark, proposing experiments to validate our
framework's safety properties. Our approach offers a path toward AGI
development that is inherently safer, rather than retrofitted with safety
measures.

</details>


### [3] [Whither symbols in the era of advanced neural networks?](https://arxiv.org/abs/2508.05776)
*Thomas L. Griffiths,Brenden M. Lake,R. Thomas McCoy,Ellie Pavlick,Taylor W. Webb*

Main category: cs.AI

TL;DR: The paper challenges the notion that human minds operate purely symbolically and argues that modern neural networks share similar cognitive capabilities.


<details>
  <summary>Details</summary>
Motivation: To re-evaluate the symbolic foundation of human cognition in light of the capabilities of modern neural networks.

Method: Comparatively analyzing the traits, functionality, and learning capabilities of neural networks versus symbolic systems.

Result: Modern neural networks show abilities akin to symbolic combinations, novelty, and quick learning traditionally attributed to human cognition.

Conclusion: Symbolic systems remain important for framing problems human minds solve, but a new research agenda is needed to understand symbolic aspects of cognition in light of neural networks.

Abstract: Some of the strongest evidence that human minds should be thought about in
terms of symbolic systems has been the way they combine ideas, produce novelty,
and learn quickly. We argue that modern neural networks -- and the artificial
intelligence systems built upon them -- exhibit similar abilities. This
undermines the argument that the cognitive processes and representations used
by human minds are symbolic, although the fact that these neural networks are
typically trained on data generated by symbolic systems illustrates that such
systems play an important role in characterizing the abstract problems that
human minds have to solve. This argument leads us to offer a new agenda for
research on the symbolic basis of human thought.

</details>


### [4] [Holistic Explainable AI (H-XAI): Extending Transparency Beyond Developers in AI-Driven Decision Making](https://arxiv.org/abs/2508.05792)
*Kausik Lakkaraju,Siva Likitha Valluru,Biplav Srivastava*

Main category: cs.AI

TL;DR: The paper proposes Holistic-XAI (H-XAI), a unified framework combining causal rating and traditional XAI methods to create multi-method, interactive explanations tailored to diverse stakeholder needs.


<details>
  <summary>Details</summary>
Motivation: Existing XAI methods focus predominantly on developers and lack inclusivity for diverse stakeholders, often limiting explanations to model justification rather than hypothesis testing and broader needs.

Method: Holistic-XAI integrates causal ratings with traditional XAI approaches, providing instance-level and global explanations tailored to stakeholder needs, and introduces hypothesis testing through comparison against random and biased baselines.

Result: H-XAI's generality was validated via two case studies in binary credit risk classification and financial time-series forecasting across six scenarios, demonstrating capability in answering stakeholder-specific questions at individual and model levels.

Conclusion: H-XAI addresses limitations in current XAI approaches by introducing a comprehensive framework that adapts explanations for stakeholders, offering both granular and global insights into AI behavior.

Abstract: Current eXplainable AI (XAI) methods largely serve developers, often focusing
on justifying model outputs rather than supporting diverse stakeholder needs. A
recent shift toward Evaluative AI reframes explanation as a tool for hypothesis
testing, but still focuses primarily on operational organizations. We introduce
Holistic-XAI (H-XAI), a unified framework that integrates causal rating methods
with traditional XAI methods to support explanation as an interactive,
multi-method process. H-XAI allows stakeholders to ask a series of questions,
test hypotheses, and compare model behavior against automatically constructed
random and biased baselines. It combines instance-level and global
explanations, adapting to each stakeholder's goals, whether understanding
individual decisions, assessing group-level bias, or evaluating robustness
under perturbations. We demonstrate the generality of our approach through two
case studies spanning six scenarios: binary credit risk classification and
financial time-series forecasting. H-XAI fills critical gaps left by existing
XAI methods by combining causal ratings and post-hoc explanations to answer
stakeholder-specific questions at both the individual decision level and the
overall model level.

</details>


### [5] [Safety of Embodied Navigation: A Survey](https://arxiv.org/abs/2508.05855)
*Zixia Wang,Jia Hu,Ronghui Mu*

Main category: cs.AI

TL;DR: The paper surveys the safety challenges in embodied navigation systems, analyzing attack and defense strategies and evaluation methods, while proposing future directions for safer systems.


<details>
  <summary>Details</summary>
Motivation: To address safety concerns in embodied AI navigation systems deployed in dynamic, real-world environments.

Method: A comprehensive analysis of attack strategies, defense mechanisms, evaluation methodologies, existing challenges, and unresolved issues.

Result: Identification of gaps, emerging technologies, and strategies for improving safety and reliability in embodied navigation systems.

Conclusion: Insights are provided to facilitate safer embodied navigation systems, with implications for societal safety and industrial efficiency.

Abstract: As large language models (LLMs) continue to advance and gain influence, the
development of embodied AI has accelerated, drawing significant attention,
particularly in navigation scenarios. Embodied navigation requires an agent to
perceive, interact with, and adapt to its environment while moving toward a
specified target in unfamiliar settings. However, the integration of embodied
navigation into critical applications raises substantial safety concerns. Given
their deployment in dynamic, real-world environments, ensuring the safety of
such systems is critical. This survey provides a comprehensive analysis of
safety in embodied navigation from multiple perspectives, encompassing attack
strategies, defense mechanisms, and evaluation methodologies. Beyond conducting
a comprehensive examination of existing safety challenges, mitigation
technologies, and various datasets and metrics that assess effectiveness and
robustness, we explore unresolved issues and future research directions in
embodied navigation safety. These include potential attack methods, mitigation
strategies, more reliable evaluation techniques, and the implementation of
verification frameworks. By addressing these critical gaps, this survey aims to
provide valuable insights that can guide future research toward the development
of safer and more reliable embodied navigation systems. Furthermore, the
findings of this study have broader implications for enhancing societal safety
and increasing industrial efficiency.

</details>


### [6] [Planning Agents on an Ego-Trip: Leveraging Hybrid Ego-Graph Ensembles for Improved Tool Retrieval in Enterprise Task Planning](https://arxiv.org/abs/2508.05888)
*Sahil Bansal,Sai Shruthi Sistla,Aarti Arikatala,Sebastian Schreiber*

Main category: cs.AI

TL;DR: The paper proposes a Knowledge Graph-based tool retrieval framework to improve AI tool selection accuracy, focusing on multi-step tasks.


<details>
  <summary>Details</summary>
Motivation: Current tool retrieval methods primarily rely on query-to-description similarity, which limits accuracy, especially for multi-step user tasks.

Method: The authors introduce a Knowledge Graph framework that models tools' semantic relationships and functional dependencies, utilizing 1-hop ego tool graphs for retrieval.

Result: Their method achieved 91.85% tool coverage in Complete Recall, outperforming traditional semantic-lexical approaches (89.26%).

Conclusion: The structural information embedded in Knowledge Graphs improves multi-step tool retrieval accuracy, addressing limitations of similarity-based methods.

Abstract: Effective tool retrieval is essential for AI agents to select from a vast
array of tools when identifying and planning actions in the context of complex
user queries. Despite its central role in planning, this aspect remains
underexplored in the literature. Traditional approaches rely primarily on
similarities between user queries and tool descriptions, which significantly
limits retrieval accuracy, specifically when handling multi-step user requests.
To address these limitations, we propose a Knowledge Graph (KG)-based tool
retrieval framework that captures the semantic relationships between tools and
their functional dependencies. Our retrieval algorithm leverages ensembles of
1-hop ego tool graphs to model direct and indirect connections between tools,
enabling more comprehensive and contextual tool selection for multi-step tasks.
We evaluate our approach on a synthetically generated internal dataset across
six defined user classes, extending previous work on coherent dialogue
synthesis and too retrieval benchmarks. Results demonstrate that our tool
graph-based method achieves 91.85% tool coverage on the micro-average Complete
Recall metric, compared to 89.26% for re-ranked semantic-lexical hybrid
retrieval, the strongest non-KG baseline in our experiments. These findings
support our hypothesis that the structural information in the KG provides
complementary signals to pure similarity matching, particularly for queries
requiring sequential tool composition.

</details>


### [7] [Mediator-Guided Multi-Agent Collaboration among Open-Source Models for Medical Decision-Making](https://arxiv.org/abs/2508.05996)
*Kaitao Chen,Mianxin Liu,Daoming Zong,Chaoyue Ding,Shaohao Rui,Yankai Jiang,Mu Zhou,Xiaosong Wang*

Main category: cs.AI

TL;DR: The paper proposes MedOrch, a mediator-guided collaboration framework for medical multimodal decision-making using VLM- and LLM-based agents. It enables better performance on medical benchmarks without additional model training.


<details>
  <summary>Details</summary>
Motivation: Medical decision-making is complex and involves cooperative workflows among clinicians. AI multi-agent systems could enhance this process, but existing approaches struggle with multimodal scenarios, especially due to limitations of vision-language models (VLMs) in instruction following and self-reflection.

Method: The paper introduces MedOrch, a framework using an LLM-based mediator agent to coordinate VLM-based expert agents. This approach focuses on collaboration without requiring model retraining, leveraging open-source general-purpose and domain-specific VLMs.

Result: MedOrch demonstrates superior performance on five medical vision question answering benchmarks, outperforming the abilities of individual agents and showing the strengths of heterogeneous model collaboration.

Conclusion: Mediator-guided collaboration among heterogeneous AI agents improves multimodal medical decision-making capabilities, revealing the potential of such systems in advancing medical intelligence.

Abstract: Complex medical decision-making involves cooperative workflows operated by
different clinicians. Designing AI multi-agent systems can expedite and augment
human-level clinical decision-making. Existing multi-agent researches primarily
focus on language-only tasks, yet their extension to multimodal scenarios
remains challenging. A blind combination of diverse vision-language models
(VLMs) can amplify an erroneous outcome interpretation. VLMs in general are
less capable in instruction following and importantly self-reflection, compared
to large language models (LLMs) of comparable sizes. This disparity largely
constrains VLMs' ability in cooperative workflows. In this study, we propose
MedOrch, a mediator-guided multi-agent collaboration framework for medical
multimodal decision-making. MedOrch employs an LLM-based mediator agent that
enables multiple VLM-based expert agents to exchange and reflect on their
outputs towards collaboration. We utilize multiple open-source general-purpose
and domain-specific VLMs instead of costly GPT-series models, revealing the
strength of heterogeneous models. We show that the collaboration within
distinct VLM-based agents can surpass the capabilities of any individual agent.
We validate our approach on five medical vision question answering benchmarks,
demonstrating superior collaboration performance without model training. Our
findings underscore the value of mediator-guided multi-agent collaboration in
advancing medical multimodal intelligence. Our code will be made publicly
available.

</details>


### [8] [Society of Mind Meets Real-Time Strategy: A Hierarchical Multi-Agent Framework for Strategic Reasoning](https://arxiv.org/abs/2508.06042)
*Daechul Ahn,San Kim,Jonghyun Choi*

Main category: cs.AI

TL;DR: The paper introduces a hierarchical multi-agent framework (HIMA) using specialized imitation learning agents orchestrated by a Strategic Planner (SP) to address challenges in dynamic, long-horizon tasks like playing StarCraft II.


<details>
  <summary>Details</summary>
Motivation: Traditional LLMs struggle with the complexity, partial observability, and dynamic planning required for tasks such as real-time strategy games.

Method: The proposed method employs a hierarchical system wherein specialized imitation agents are trained on expert demonstrations for specific strategies. A meta-controller (SP) then integrates these specialized outputs into adaptive, long-term plans.

Result: The hierarchical framework, named HIMA, outperforms existing methods in strategic clarity, adaptability, and computational efficiency in the newly introduced TEXTSCII-ALL testbed for StarCraft II.

Conclusion: Specialized imitation modules combined with a high-level orchestrator can significantly enhance the adaptability and performance of AI agents in dynamic, complex tasks.

Abstract: Large Language Models (LLMs) have recently demonstrated impressive action
sequence prediction capabilities but often struggle with dynamic, long-horizon
tasks such as real-time strategic games. In a game such as StarCraftII (SC2),
agents need to manage resource constraints and adapt to evolving battlefield
situations in a partially observable environment. This often overwhelms
exisiting LLM-based approaches. To address these challenges, we propose a
hierarchical multi-agent framework that employs specialized imitation learning
agents under a meta-controller called Strategic Planner (SP). By expert
demonstrations, each specialized agent learns a distinctive strategy, such as
aerial support or defensive maneuvers, and produces coherent, structured
multistep action sequences. The SP then orchestrates these proposals into a
single, environmentally adaptive plan that ensures local decisions aligning
with long-term strategies. We call this HIMA (Hierarchical Imitation
Multi-Agent). We also present TEXTSCII-ALL, a comprehensive SC2 testbed that
encompasses all race match combinations in SC2. Our empirical results show that
HIMA outperforms state of the arts in strategic clarity, adaptability, and
computational efficiency, underscoring the potential of combining specialized
imitation modules with meta-level orchestration to develop more robust,
general-purpose AI agents.

</details>


### [9] [LLMs for Resource Allocation: A Participatory Budgeting Approach to Inferring Preferences](https://arxiv.org/abs/2508.06060)
*Sankarshan Damle,Boi Faltings*

Main category: cs.AI

TL;DR: This paper explores the use of large language models (LLMs) for resource allocation tasks, introducing a dual-purpose framework based on Participatory Budgeting (PB) for both practical application and evaluation of reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the growing expectation for LLMs to handle complex decision-making tasks, such as structured resource allocation, and the challenges in evaluating their reasoning due to data issues and static benchmarks.

Method: The authors use Participatory Budgeting (PB) as a framework and propose three prompting strategies—greedy selection, direct optimization, and hill-climbing-inspired refinement. They also test LLMs' ability to infer structured preferences from natural language input.

Result: The study finds that prompt design plays a significant role in LLM performance. LLMs show promise in allocating resources under constraints, as well as inferring preferences from unstructured inputs like metadata or natural language.

Conclusion: LLMs have potential in mechanism design for resource allocation tasks, especially when dealing with unstructured or open-ended inputs. The findings emphasize the importance of prompt strategies in enhancing their capabilities.

Abstract: Large Language Models (LLMs) are increasingly expected to handle complex
decision-making tasks, yet their ability to perform structured resource
allocation remains underexplored. Evaluating their reasoning is also difficult
due to data contamination and the static nature of existing benchmarks. We
present a dual-purpose framework leveraging Participatory Budgeting (PB) both
as (i) a practical setting for LLM-based resource allocation and (ii) an
adaptive benchmark for evaluating their reasoning capabilities. We task LLMs
with selecting project subsets under feasibility (e.g., budget) constraints via
three prompting strategies: greedy selection, direct optimization, and a
hill-climbing-inspired refinement. We benchmark LLMs' allocations against a
utility-maximizing oracle. Interestingly, we also test whether LLMs can infer
structured preferences from natural-language voter input or metadata, without
explicit votes. By comparing allocations based on inferred preferences to those
from ground-truth votes, we evaluate LLMs' ability to extract preferences from
open-ended input. Our results underscore the role of prompt design and show
that LLMs hold promise for mechanism design with unstructured inputs.

</details>


### [10] [Don't Forget Imagination!](https://arxiv.org/abs/2508.06062)
*Evgenii E. Vityaev,Andrei Mantsivoda*

Main category: cs.AI

TL;DR: Cognitive imagination is crucial for AI, enabling reasoning through imaginary contexts. The paper advocates for semantic models to simulate cognitive imagination by ensuring consistency and causal relationships.


<details>
  <summary>Details</summary>
Motivation: Highlight the underestimated role of cognitive imagination in reasoning and decision-making, aiming to unlock its potential for AI innovation.

Method: Proposing semantic models that utilize probabilistic causal relationships to simulate cognitive imagination in a transparent, manipulable way.

Result: Semantic models provide a framework to simulate cognitive imagination, ensuring consistency and holistic reasoning, potentially advancing AI capabilities.

Conclusion: Recognizing cognitive imagination as central to reasoning can lead to breakthroughs in AI. Semantic models offer a promising approach to integrate this capability into AI systems.

Abstract: Cognitive imagination is a type of imagination that plays a key role in human
thinking. It is not a ``picture-in-the-head'' imagination. It is a faculty to
mentally visualize coherent and holistic systems of concepts and causal links
that serve as semantic contexts for reasoning, decision making and prediction.
Our position is that the role of cognitive imagination is still greatly
underestimated, and this creates numerous problems and diminishes the current
capabilities of AI. For instance, when reasoning, humans rely on imaginary
contexts to retrieve background info. They also constantly return to the
context for semantic verification that their reasoning is still reasonable.
Thus, reasoning without imagination is blind. This paper is a call for greater
attention to cognitive imagination as the next promising breakthrough in
artificial intelligence. As an instrument for simulating cognitive imagination,
we propose semantic models -- a new approach to mathematical models that can
learn, like neural networks, and are based on probabilistic causal
relationships. Semantic models can simulate cognitive imagination because they
ensure the consistency of imaginary contexts and implement a glass-box approach
that allows the context to be manipulated as a holistic and coherent system of
interrelated facts glued together with causal relations.

</details>


### [11] [A Generic Complete Anytime Beam Search for Optimal Decision Tree](https://arxiv.org/abs/2508.06064)
*Harold Silvère Kiossou,Siegfried Nijssen,Pierre Schaus*

Main category: cs.AI

TL;DR: The paper introduces CA-DL8.5, a new anytime beam search algorithm for generating optimal decision trees. It outperforms previous algorithms in anytime performance while ensuring optimality.


<details>
  <summary>Details</summary>
Motivation: Exact methods for generating optimal decision trees often exhibit poor anytime performance, struggling to find quality solutions quickly. Existing extended anytime methods are not systematically compared, creating a need for a unified framework.

Method: CA-DL8.5 is a beam search algorithm extending the DL8.5 framework, combining branch-and-bound pruning, caching, and a restart-based beam search that relaxes pruning criteria. It incorporates various heuristics (e.g., Purity, Gain, Discrepancy) to balance exploration.

Result: CA-DL8.5 using LDS (limited discrepancy strategy) achieved the best anytime performance on standard benchmarks, surpassing existing variants and Blossom in quality while maintaining optimality guarantees.

Conclusion: CA-DL8.5 provides a flexible and effective approach to exact and anytime decision tree learning, showcasing better anytime performance and adaptability through heuristic integration.

Abstract: Finding an optimal decision tree that minimizes classification error is known
to be NP-hard. While exact algorithms based on MILP, CP, SAT, or dynamic
programming guarantee optimality, they often suffer from poor anytime behavior
-- meaning they struggle to find high-quality decision trees quickly when the
search is stopped before completion -- due to unbalanced search space
exploration. To address this, several anytime extensions of exact methods have
been proposed, such as LDS-DL8.5, Top-k-DL8.5, and Blossom, but they have not
been systematically compared, making it difficult to assess their relative
effectiveness. In this paper, we propose CA-DL8.5, a generic, complete, and
anytime beam search algorithm that extends the DL8.5 framework and unifies some
existing anytime strategies. In particular, CA-DL8.5 generalizes previous
approaches LDS-DL8.5 and Top-k-DL8.5, by allowing the integration of various
heuristics and relaxation mechanisms through a modular design. The algorithm
reuses DL8.5's efficient branch-and-bound pruning and trie-based caching,
combined with a restart-based beam search that gradually relaxes pruning
criteria to improve solution quality over time. Our contributions are twofold:
(1) We introduce this new generic framework for exact and anytime decision tree
learning, enabling the incorporation of diverse heuristics and search
strategies; (2) We conduct a rigorous empirical comparison of several
instantiations of CA-DL8.5 -- based on Purity, Gain, Discrepancy, and Top-k
heuristics -- using an anytime evaluation metric called the primal gap
integral. Experimental results on standard classification benchmarks show that
CA-DL8.5 using LDS (limited discrepancy) consistently provides the best anytime
performance, outperforming both other CA-DL8.5 variants and the Blossom
algorithm while maintaining completeness and optimality guarantees.

</details>


### [12] [ME$^3$-BEV: Mamba-Enhanced Deep Reinforcement Learning for End-to-End Autonomous Driving with BEV-Perception](https://arxiv.org/abs/2508.06074)
*Siyi Lu,Run Liu,Dongsheng Yang,Lei He*

Main category: cs.AI

TL;DR: The paper proposes an enhanced deep reinforcement learning (DRL) framework, ME³-BEV, that integrates bird's-eye view perception and a novel Mamba-BEV network for improved autonomous driving performance in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: Enhancing real-time decision-making in autonomous driving, which struggles with challenges such as error propagation in modular systems and computational inefficiencies in end-to-end systems.

Method: The approach uses bird's-eye view (BEV) perception integrated with the Mamba-BEV model for spatio-temporal feature extraction, enabling unified encoding of vehicle surroundings, road details, and long-range dependencies. This is coupled with an end-to-end DRL framework.

Result: The proposed ME³-BEV framework demonstrated superior performance in urban driving scenarios within the CARLA simulator, excelling in collision rate reduction and trajectory accuracy compared to other state-of-the-art models.

Conclusion: ME³-BEV offers a scalable and interpretable solution for real-time autonomous driving, integrating BEV perception and DRL to overcome computational and error propagation challenges.

Abstract: Autonomous driving systems face significant challenges in perceiving complex
environments and making real-time decisions. Traditional modular approaches,
while offering interpretability, suffer from error propagation and coordination
issues, whereas end-to-end learning systems can simplify the design but face
computational bottlenecks. This paper presents a novel approach to autonomous
driving using deep reinforcement learning (DRL) that integrates bird's-eye view
(BEV) perception for enhanced real-time decision-making. We introduce the
\texttt{Mamba-BEV} model, an efficient spatio-temporal feature extraction
network that combines BEV-based perception with the Mamba framework for
temporal feature modeling. This integration allows the system to encode vehicle
surroundings and road features in a unified coordinate system and accurately
model long-range dependencies. Building on this, we propose the
\texttt{ME$^3$-BEV} framework, which utilizes the \texttt{Mamba-BEV} model as a
feature input for end-to-end DRL, achieving superior performance in dynamic
urban driving scenarios. We further enhance the interpretability of the model
by visualizing high-dimensional features through semantic segmentation,
providing insight into the learned representations. Extensive experiments on
the CARLA simulator demonstrate that \texttt{ME$^3$-BEV} outperforms existing
models across multiple metrics, including collision rate and trajectory
accuracy, offering a promising solution for real-time autonomous driving.

</details>


### [13] [Aggregate-Combine-Readout GNNs Are More Expressive Than Logic C2](https://arxiv.org/abs/2508.06091)
*Stan P Hauke,Przemysław Andrzej Wałęga*

Main category: cs.AI

TL;DR: This paper resolves a key open problem in graph neural networks (GNNs), proving that aggregate-combine-readout GNNs are more expressive than the logic C2.


<details>
  <summary>Details</summary>
Motivation: There has been significant interest in connecting the expressiveness of graph neural networks with logical frameworks, motivated by previous findings that related GNNs to specific logical logics such as C2.

Method: The authors analyzed the expressive capabilities of aggregate-combine-readout GNNs and compared them to the logical expressiveness of C2 over both undirected and directed graphs.

Result: The researchers demonstrated that the expressive power of aggregate-combine-readout GNNs surpasses that of C2, resolving a longstanding open problem in the field.

Conclusion: This study advances the theoretical understanding of GNN expressiveness and contributes insights into the expressive power of infinitary logics.

Abstract: In recent years, there has been growing interest in understanding the
expressive power of graph neural networks (GNNs) by relating them to logical
languages. This research has been been initialised by an influential result of
Barcel\'o et al. (2020), who showed that the graded modal logic (or a guarded
fragment of the logic C2), characterises the logical expressiveness of
aggregate-combine GNNs. As a ``challenging open problem'' they left the
question whether full C2 characterises the logical expressiveness of
aggregate-combine-readout GNNs. This question has remained unresolved despite
several attempts. In this paper, we solve the above open problem by proving
that the logical expressiveness of aggregate-combine-readout GNNs strictly
exceeds that of C2. This result holds over both undirected and directed graphs.
Beyond its implications for GNNs, our work also leads to purely logical
insights on the expressive power of infinitary logics.

</details>


### [14] [PanelTR: Zero-Shot Table Reasoning Framework Through Multi-Agent Scientific Discussion](https://arxiv.org/abs/2508.06110)
*Yiran Rex Ma*

Main category: cs.AI

TL;DR: PanelTR improves table reasoning using structured methods with isolated LLM agents, achieving strong performance without training data.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations of annotated data dependency and LLMs underperformance in table reasoning tasks.

Method: Utilized structured scientific workflow with LLM agent scientists performing investigations, reviews, and peer discussions.

Result: PanelTR surpasses vanilla LLMs and rivals supervised models on benchmarks without training data reliance.

Conclusion: Structured methodology within zero-shot setups enhances the versatility of LLMs in complex reasoning tasks like table interpretation.

Abstract: Table reasoning, including tabular QA and fact verification, often depends on
annotated data or complex data augmentation, limiting flexibility and
generalization. LLMs, despite their versatility, often underperform compared to
simple supervised models. To approach these issues, we introduce PanelTR, a
framework utilizing LLM agent scientists for robust table reasoning through a
structured scientific approach. PanelTR's workflow involves agent scientists
conducting individual investigations, engaging in self-review, and
participating in collaborative peer-review discussions. This process, driven by
five scientist personas, enables semantic-level transfer without relying on
data augmentation or parametric optimization. Experiments across four
benchmarks show that PanelTR outperforms vanilla LLMs and rivals fully
supervised models, all while remaining independent of training data. Our
findings indicate that structured scientific methodology can effectively handle
complex tasks beyond table reasoning with flexible semantic understanding in a
zero-shot context.

</details>


### [15] [SKATE, a Scalable Tournament Eval: Weaker LLMs differentiate between stronger ones using verifiable challenges](https://arxiv.org/abs/2508.06111)
*Dewi S. W. Gould,Bruno Mlodozeniec,Samuel F. Brown*

Main category: cs.AI

TL;DR: SKATE is an automated framework for evaluating large language models (LLMs) by treating evaluation as a competitive game of task generation and solving.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods for foundation models require extensive domain expertise, making them difficult to scale as models evolve rapidly.

Method: The SKATE framework involves LLMs acting as both task-creators and solvers, using verifiable tasks to objectively evaluate performance without human input.

Result: SKATE was tested using a code-output-prediction challenge and revealed that weaker models can score stronger ones, LLMs exhibit self-preferencing behavior, and fine-grained capability differences can be identified.

Conclusion: SKATE represents a step forward in scalable, generalizable LLM evaluation methods that adapt to model advancements.

Abstract: Evaluating the capabilities and risks of foundation models is paramount, yet
current methods demand extensive domain expertise, hindering their scalability
as these models rapidly evolve. We introduce SKATE: a novel evaluation
framework in which large language models (LLMs) compete by generating and
solving verifiable tasks for one another. Our core insight is to treat
evaluation as a game: models act as both task-setters and solvers, incentivized
to create questions which highlight their own strengths while exposing others'
weaknesses. SKATE offers several key advantages, balancing scalability,
open-endedness, and objectivity. It is fully automated, data-free, and
scalable, requiring no human input or domain expertise. By using verifiable
tasks rather than LLM judges, scoring is objective. Unlike domain-limited
programmatically-generated benchmarks (e.g. chess-playing or spatial
reasoning), having LLMs creatively pose challenges enables open-ended and
scalable evaluation. As a proof of concept, we introduce LLM-set
code-output-prediction (COP) challenges as a verifiable and extensible
framework in which to test our approach. Using a TrueSkill-based ranking
system, we evaluate six frontier LLMs and find that: (1) weaker models can
reliably differentiate and score stronger ones, (2) LLM-based systems are
capable of self-preferencing behavior, generating questions that align with
their own capabilities, and (3) SKATE automatically surfaces fine-grained
capability differences between models. Our findings are an important step
towards general, scalable evaluation frameworks which can keep pace with LLM
progress.

</details>


### [16] [Study of Robust Features in Formulating Guidance for Heuristic Algorithms for Solving the Vehicle Routing Problem](https://arxiv.org/abs/2508.06129)
*Bachtiar Herdianto,Romain Billot,Flavien Lucas,Marc Sevaux*

Main category: cs.AI

TL;DR: This paper explores leveraging machine learning, specifically explainable AI, to predict the quality of VRP solutions and analyze feature importance across different scenarios.


<details>
  <summary>Details</summary>
Motivation: Traditional VRP methods rely on human-crafted metaheuristics, which are limited in adaptability. Machine learning methods offer potential for improvement by analyzing structural solution features.

Method: The study conducts sensitivity analysis using multiple classifier models capable of predicting VRP solution quality. It employs feature importance analysis through explainable AI.

Result: The research identified consistent feature predictors, despite variability in importance, and proposed a unified framework for ranking their impact across diverse scenarios.

Conclusion: Feature importance analysis can inform metaheuristic algorithm design, enhancing their efficiency in solving VRPs.

Abstract: The Vehicle Routing Problem (VRP) is a complex optimization problem with
numerous real-world applications, mostly solved using metaheuristic algorithms
due to its $\mathcal{NP}$-Hard nature. Traditionally, these metaheuristics rely
on human-crafted designs developed through empirical studies. However, recent
research shows that machine learning methods can be used the structural
characteristics of solutions in combinatorial optimization, thereby aiding in
designing more efficient algorithms, particularly for solving VRP. Building on
this advancement, this study extends the previous research by conducting a
sensitivity analysis using multiple classifier models that are capable of
predicting the quality of VRP solutions. Hence, by leveraging explainable AI,
this research is able to extend the understanding of how these models make
decisions. Finally, our findings indicate that while feature importance varies,
certain features consistently emerge as strong predictors. Furthermore, we
propose a unified framework able of ranking feature impact across different
scenarios to illustrate this finding. These insights highlight the potential of
feature importance analysis as a foundation for developing a guidance mechanism
of metaheuristic algorithms for solving the VRP.

</details>


### [17] [Retrieval Augmented Large Language Model System for Comprehensive Drug Contraindications](https://arxiv.org/abs/2508.06145)
*Byeonghun Bang,Jongsuk Yoon,Dong-Jin Chang,Seho Park,Yong Oh Lee*

Main category: cs.AI

TL;DR: The paper enhanced large language models (LLMs) for healthcare applications, specifically to improve accuracy in identifying pharmaceutical contraindications using a Retrieval Augmented Generation (RAG) pipeline.


<details>
  <summary>Details</summary>
Motivation: The study aims to tackle the challenge of applying LLMs in healthcare, especially in delivering precise and reliable information about pharmaceutical contraindications.

Method: The approach utilizes OpenAI's GPT-4o-mini model for language generation and the text-embedding-3-small model for embeddings. It incorporates a hybrid retrieval system orchestrated by Langchain, leveraging Drug Utilization Review (DUR) data from public databases.

Result: The integration of the RAG pipeline significantly improved LLM accuracy, achieving scores of 0.94, 0.87, and 0.89 in contraindication categories (age groups, pregnancy, and concomitant drug use), compared to a baseline of 0.49 to 0.57.

Conclusion: Augmenting LLMs with a RAG pipeline shows promise in improving decision-making in healthcare by providing precise drug contraindication information.

Abstract: The versatility of large language models (LLMs) has been explored across
various sectors, but their application in healthcare poses challenges,
particularly in the domain of pharmaceutical contraindications where accurate
and reliable information is required. This study enhances the capability of
LLMs to address contraindications effectively by implementing a Retrieval
Augmented Generation (RAG) pipeline. Utilizing OpenAI's GPT-4o-mini as the base
model, and the text-embedding-3-small model for embeddings, our approach
integrates Langchain to orchestrate a hybrid retrieval system with re-ranking.
This system leverages Drug Utilization Review (DUR) data from public databases,
focusing on contraindications for specific age groups, pregnancy, and
concomitant drug use. The dataset includes 300 question-answer pairs across
three categories, with baseline model accuracy ranging from 0.49 to 0.57.
Post-integration of the RAG pipeline, we observed a significant improvement in
model accuracy, achieving rates of 0.94, 0.87, and 0.89 for contraindications
related to age groups, pregnancy, and concomitant drug use, respectively. The
results indicate that augmenting LLMs with a RAG framework can substantially
reduce uncertainty in prescription and drug intake decisions by providing more
precise and reliable drug contraindication information.

</details>


### [18] [Overconfidence in LLM-as-a-Judge: Diagnosis and Confidence-Driven Solution](https://arxiv.org/abs/2508.06225)
*Zailong Tian,Zhuoheng Han,Yanzhe Chen,Haozhe Xu,Xi Yang,richeng xuan,Hongfeng Wang,Lizi Liao*

Main category: cs.AI

TL;DR: This paper introduces the Overconfidence Phenomenon observed in current LLM-based judgment systems and proposes solutions for improving calibrated confidence. It introduces the TH-Score metric and an LLM-as-a-Fuser ensemble framework for risk-aware evaluations.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to improve the reliability and trustworthiness of LLMs when used as automated judges, focusing on the issue of overconfidence in their predictions.

Method: The authors identify the Overconfidence Phenomenon in LLMs and introduce TH-Score, a metric to quantify confidence-accuracy alignment. Additionally, they propose LLM-as-a-Fuser, an ensemble framework to enhance calibration and reliability.

Result: The proposed methods significantly improve confidence calibration and enable adaptive evaluation pipelines, achieving better reliability and accuracy compared to current approaches.

Conclusion: The paper advocates for a confidence-driven, risk-aware evaluation system for LLMs-as-a-Judges, emphasizing the significance of calibrated confidence in practical usage.

Abstract: Large Language Models (LLMs) are widely used as automated judges, where
practical value depends on both accuracy and trustworthy, risk-aware judgments.
Existing approaches predominantly focus on accuracy, overlooking the necessity
of well-calibrated confidence, which is vital for adaptive and reliable
evaluation pipelines. In this work, we advocate a shift from accuracy-centric
evaluation to confidence-driven, risk-aware LLM-as-a-Judge systems, emphasizing
the necessity of well-calibrated confidence for trustworthy and adaptive
evaluation. We systematically identify the **Overconfidence Phenomenon** in
current LLM-as-a-Judges, where predicted confidence significantly overstates
actual correctness, undermining reliability in practical deployment. To
quantify this phenomenon, we introduce **TH-Score**, a novel metric measuring
confidence-accuracy alignment. Furthermore, we propose **LLM-as-a-Fuser**, an
ensemble framework that transforms LLMs into reliable, risk-aware evaluators.
Extensive experiments demonstrate that our approach substantially improves
calibration and enables adaptive, confidence-driven evaluation pipelines,
achieving superior reliability and accuracy compared to existing baselines.

</details>


### [19] [GeoLaux: A Benchmark for Evaluating MLLMs' Geometry Performance on Long-Step Problems Requiring Auxiliary Lines](https://arxiv.org/abs/2508.06226)
*Yumeng Fu,Jiayin Zhu,Lingling Zhang,Bo Zhao,Shaoxuan Ma,Yushun Zhang,Yanrui Wu,Wenjun Wu*

Main category: cs.AI

TL;DR: This paper introduces GeoLaux, a benchmark with 2,186 geometry problems to evaluate MLLMs on complex geometric reasoning, particularly auxiliary line construction.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks do not sufficiently assess MLLMs' long-step reasoning skills or auxiliary line construction, crucial for geometry problem solving.

Method: GeoLaux provides problems requiring an average of 6.51 reasoning steps and incorporates auxiliary line construction. It also proposes a five-dimensional evaluation framework.

Result: Experiments reveal MLLMs struggle with extended reasoning, proving problems, and auxiliary line awareness, highlighting areas for improvement.

Conclusion: GeoLaux serves as both a benchmark for evaluating geometric reasoning capabilities and a roadmap for improving MLLMs in this domain. Dataset and code will be released for further use.

Abstract: Geometry problem solving (GPS) requires models to master diagram
comprehension, logical reasoning, knowledge application, numerical computation,
and auxiliary line construction. This presents a significant challenge for
Multimodal Large Language Models (MLLMs). However, existing benchmarks for
evaluating MLLM geometry skills overlook auxiliary line construction and lack
fine-grained process evaluation, making them insufficient for assessing MLLMs'
long-step reasoning abilities. To bridge these gaps, we present the GeoLaux
benchmark, comprising 2,186 geometry problems, incorporating both calculation
and proving questions. Notably, the problems require an average of 6.51
reasoning steps, with a maximum of 24 steps, and 41.8% of them need auxiliary
line construction. Building on the dataset, we design a novel five-dimensional
evaluation strategy assessing answer correctness, process correctness, process
quality, auxiliary line impact, and error causes. Extensive experiments on 13
leading MLLMs (including thinking models and non-thinking models) yield three
pivotal findings: First, models exhibit substantial performance degradation in
extended reasoning steps (nine models demonstrate over 50% performance drop).
Second, compared to calculation problems, MLLMs tend to take shortcuts when
solving proving problems. Third, models lack auxiliary line awareness, and
enhancing this capability proves particularly beneficial for overall geometry
reasoning improvement. These findings establish GeoLaux as both a benchmark for
evaluating MLLMs' long-step geometric reasoning with auxiliary lines and a
guide for capability advancement. Our dataset and code are included in
supplementary materials and will be released.

</details>


### [20] [Learning Logical Rules using Minimum Message Length](https://arxiv.org/abs/2508.06230)
*Ruben Sharma,Sebastijan Dumančić,Ross D. King,Andrew Cropper*

Main category: cs.AI

TL;DR: The paper presents a Bayesian inductive logic programming approach for learning minimum message length programs from noisy data, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenge of unifying probabilistic and logical learning in AI, especially when dealing with noisy data in various domains.

Method: The authors propose a Bayesian approach that balances hypothesis complexity and data fit using priors favoring generality and likelihood favoring accuracy. They test the method across multiple domains.

Result: The proposed approach outperforms previous methods, including those based on minimum description length, and proves to be data-efficient and robust against example imbalance, performing well even with exclusively positive examples.

Conclusion: Bayesian inductive logic programming effectively combines probabilistic and logical learning, delivering superior performance and data efficiency compared to existing methods.

Abstract: Unifying probabilistic and logical learning is a key challenge in AI. We
introduce a Bayesian inductive logic programming approach that learns minimum
message length programs from noisy data. Our approach balances hypothesis
complexity and data fit through priors, which explicitly favour more general
programs, and a likelihood that favours accurate programs. Our experiments on
several domains, including game playing and drug design, show that our method
significantly outperforms previous methods, notably those that learn minimum
description length programs. Our results also show that our approach is
data-efficient and insensitive to example balance, including the ability to
learn from exclusively positive examples.

</details>


### [21] [Symmetry breaking for inductive logic programming](https://arxiv.org/abs/2508.06263)
*Andrew Cropper,David M. Cerna,Matti Järvisalo*

Main category: cs.AI

TL;DR: The paper introduces a method to reduce solving times in inductive logic programming by breaking symmetries in the hypothesis space.


<details>
  <summary>Details</summary>
Motivation: The challenge of searching vast hypothesis spaces in inductive logic programming is exacerbated by the existence of many logically equivalent hypotheses.

Method: The paper introduces a symmetry-breaking method implemented using answer set programming.

Result: The experiments demonstrate that their approach can significantly reduce solving times, from over an hour to 17 seconds, across multiple domains.

Conclusion: Breaking symmetries in hypothesis spaces can drastically enhance the efficiency of inductive logic programming.

Abstract: The goal of inductive logic programming is to search for a hypothesis that
generalises training data and background knowledge. The challenge is searching
vast hypothesis spaces, which is exacerbated because many logically equivalent
hypotheses exist. To address this challenge, we introduce a method to break
symmetries in the hypothesis space. We implement our idea in answer set
programming. Our experiments on multiple domains, including visual reasoning
and game playing, show that our approach can reduce solving times from over an
hour to just 17 seconds.

</details>


### [22] [LLM Robustness Leaderboard v1 --Technical report](https://arxiv.org/abs/2508.06296)
*Pierre Peigné - Lefebvre,Quentin Feuillade-Montixi,Tom David,Nicolas Miailhe*

Main category: cs.AI

TL;DR: The paper presents the PRISM Eval Behavior Elicitation Tool (BET), which conducts automated red-teaming and achieves a high attack success against prominent LLMs, and proposes a nuanced robustness metric.


<details>
  <summary>Details</summary>
Motivation: To enhance the evaluation of language models' safety and vulnerabilities by developing an effective and scalable assessment tool.

Method: They used automated red-teaming through Dynamic Adversarial Optimization and introduced advanced metrics for vulnerability analysis across models.

Result: BET achieved a 100% attack success rate against 37 of 41 LLMs, showcasing universal vulnerabilities with significant variation in attack difficulty.

Conclusion: The study emphasizes the need for collaborative and distributed robustness evaluations to improve AI safety frameworks.

Abstract: This technical report accompanies the LLM robustness leaderboard published by
PRISM Eval for the Paris AI Action Summit. We introduce PRISM Eval Behavior
Elicitation Tool (BET), an AI system performing automated red-teaming through
Dynamic Adversarial Optimization that achieves 100% Attack Success Rate (ASR)
against 37 of 41 state-of-the-art LLMs. Beyond binary success metrics, we
propose a fine-grained robustness metric estimating the average number of
attempts required to elicit harmful behaviors, revealing that attack difficulty
varies by over 300-fold across models despite universal vulnerability. We
introduce primitive-level vulnerability analysis to identify which jailbreaking
techniques are most effective for specific hazard categories. Our collaborative
evaluation with trusted third parties from the AI Safety Network demonstrates
practical pathways for distributed robustness assessment across the community.

</details>


### [23] [A "good regulator theorem" for embodied agents](https://arxiv.org/abs/2508.06326)
*Nathaniel Virgo,Martin Biehl,Manuel Baltieri,Matteo Capucci*

Main category: cs.AI

TL;DR: The paper extends Conant and Ashby's theorem, arguing that systems regulating their environments can be interpreted as having 'beliefs' about their surroundings, which are updated based on sensory input. This introduces a more sophisticated model-based perspective but shifts the role of interpretation onto the observer.


<details>
  <summary>Details</summary>
Motivation: To explore whether Conant and Ashby's theorem on good regulators can generalize to scenarios where systems appear to operate without explicit models.

Method: The authors redefine the notion of a model to one involving 'belief updating' imposed by an observer, thereby broadening the theorem's applicability to different forms of regulation tasks.

Result: The extended theorem demonstrates that systems can always be interpreted as having belief-like models of their environment, irrespective of the type of regulation performed. This interpretation reconciles apparent counterexamples to the original theorem.

Conclusion: The work redefines system modeling by emphasizing the observer’s role, establishing a broader applicability for Conant and Ashby's theorem and resolving counterexamples by framing models as imposed interpretations.

Abstract: In a classic paper, Conant and Ashby claimed that "every good regulator of a
system must be a model of that system." Artificial Life has produced many
examples of systems that perform tasks with apparently no model in sight; these
suggest Conant and Ashby's theorem doesn't easily generalise beyond its
restricted setup. Nevertheless, here we show that a similar intuition can be
fleshed out in a different way: whenever an agent is able to perform a
regulation task, it is possible for an observer to interpret it as having
"beliefs" about its environment, which it "updates" in response to sensory
input. This notion of belief updating provides a notion of model that is more
sophisticated than Conant and Ashby's, as well as a theorem that is more
broadly applicable. However, it necessitates a change in perspective, in that
the observer plays an essential role in the theory: models are not a mere
property of the system but are imposed on it from outside. Our theorem holds
regardless of whether the system is regulating its environment in a classic
control theory setup, or whether it's regulating its own internal state; the
model is of its environment either way. The model might be trivial, however,
and this is how the apparent counterexamples are resolved.

</details>


### [24] [AntiCheatPT: A Transformer-Based Approach to Cheat Detection in Competitive Computer Games](https://arxiv.org/abs/2508.06348)
*Mille Mei Zhen Loo,Gert Luzkov,Paolo Burelli*

Main category: cs.AI

TL;DR: A transformer-based machine learning model, AntiCheatPT_256, is introduced for detecting cheating in Counter-Strike 2 using gameplay data, achieving 89.17% accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of detecting online video game cheating without invasive measures, enhancing anti-cheat system efficacy.

Method: Developed a transformer model trained on gameplay data including 90,707 augmented context windows from the newly created CS2CD dataset to tackle class imbalance.

Result: Achieved 89.17% accuracy and 93.36% AUC on an unaugmented test dataset, demonstrating the model's effectiveness.

Conclusion: AntiCheatPT_256 offers a reproducible, data-driven solution for cheat detection, setting a robust baseline for future advancements in the field.

Abstract: Cheating in online video games compromises the integrity of gaming
experiences. Anti-cheat systems, such as VAC (Valve Anti-Cheat), face
significant challenges in keeping pace with evolving cheating methods without
imposing invasive measures on users' systems. This paper presents
AntiCheatPT\_256, a transformer-based machine learning model designed to detect
cheating behaviour in Counter-Strike 2 using gameplay data. To support this, we
introduce and publicly release CS2CD: A labelled dataset of 795 matches. Using
this dataset, 90,707 context windows were created and subsequently augmented to
address class imbalance. The transformer model, trained on these windows,
achieved an accuracy of 89.17\% and an AUC of 93.36\% on an unaugmented test
set. This approach emphasizes reproducibility and real-world applicability,
offering a robust baseline for future research in data-driven cheat detection.

</details>


### [25] [From Explainable to Explanatory Artificial Intelligence: Toward a New Paradigm for Human-Centered Explanations through Generative AI](https://arxiv.org/abs/2508.06352)
*Christian Meske,Justin Brenne,Erdi Uenal,Sabahat Oelcer,Ayseguel Doganguen*

Main category: cs.AI

TL;DR: This paper introduces 'Explanatory AI,' emphasizing user-focused contextual reasoning over traditional algorithmic transparency and validates its efficacy with healthcare professionals.


<details>
  <summary>Details</summary>
Motivation: To address the shortcomings of traditional explainable AI (XAI) in supporting meaningful end-user understanding, particularly in sociotechnical decision-making contexts.

Method: The authors propose an eight-dimensional conceptual model for Explanatory AI, based on narrative communication, adaptive personalization, and progressive disclosure, and validate it through Rapid Contextual Design with healthcare professionals.

Result: Users consistently preferred context-sensitive and multimodal explanations provided by Explanatory AI over technical transparency.

Conclusion: There is a critical need for AI systems that prioritize human comprehension and contextual reasoning across various domains, suggesting a shift in research focus towards user-centered explanation methods.

Abstract: Current explainable AI (XAI) approaches prioritize algorithmic transparency
and present explanations in abstract, non-adaptive formats that often fail to
support meaningful end-user understanding. This paper introduces "Explanatory
AI" as a complementary paradigm that leverages generative AI capabilities to
serve as explanatory partners for human understanding rather than providers of
algorithmic transparency. While XAI reveals algorithmic decision processes for
model validation, Explanatory AI addresses contextual reasoning to support
human decision-making in sociotechnical contexts. We develop a definition and
systematic eight-dimensional conceptual model distinguishing Explanatory AI
through narrative communication, adaptive personalization, and progressive
disclosure principles. Empirical validation through Rapid Contextual Design
methodology with healthcare professionals demonstrates that users consistently
prefer context-sensitive, multimodal explanations over technical transparency.
Our findings reveal the practical urgency for AI systems designed for human
comprehension rather than algorithmic introspection, establishing a
comprehensive research agenda for advancing user-centered AI explanation
approaches across diverse domains and cultural contexts.

</details>


### [26] [Automated Creation of the Legal Knowledge Graph Addressing Legislation on Violence Against Women: Resource, Methodology and Lessons Learned](https://arxiv.org/abs/2508.06368)
*Claudia dAmato,Giuseppe Rubini,Francesco Didio,Donato Francioso,Fatima Zahra Amara,Nicola Fanizzi*

Main category: cs.AI

TL;DR: This paper presents the development of Legal Knowledge Graphs (KGs) for violence against women cases, using two complementary automated approaches: a domain-specific method and a Large Language Model-driven method.


<details>
  <summary>Details</summary>
Motivation: Legal decision-making requires structured access to detailed legislative information and case records, but there is a lack of domain-specific Legal Knowledge Graphs, especially for cases involving violence against women.

Method: The work introduces two approaches for constructing legal KGs: a systematic bottom-up method customized for the legal domain and a solution using Large Language Models. Both approaches involve structured data extraction, ontology creation, and semantic enrichment.

Result: Two Legal Knowledge Graphs tailored to violence against women cases were developed and validated through competency questions, demonstrating their ability to improve legal information accessibility and enable complex queries.

Conclusion: The developed KGs not only enhance access to legal information but also provide a foundation for machine learning solutions aimed at predictive justice, potentially aiding decision-making processes.

Abstract: Legal decision-making process requires the availability of comprehensive and
detailed legislative background knowledge and up-to-date information on legal
cases and related sentences/decisions. Legal Knowledge Graphs (KGs) would be a
valuable tool to facilitate access to legal information, to be queried and
exploited for the purpose, and to enable advanced reasoning and machine
learning applications. Indeed, legal KGs may act as knowledge intensive
component to be used by pre-dictive machine learning solutions supporting the
decision process of the legal expert. Nevertheless, a few KGs can be found in
the legal domain. To fill this gap, we developed a legal KG targeting legal
cases of violence against women, along with clear adopted methodologies.
Specifically, the paper introduces two complementary approaches for automated
legal KG construction; a systematic bottom-up approach, customized for the
legal domain, and a new solution leveraging Large Language Models. Starting
from legal sentences publicly available from the European Court of Justice, the
solutions integrate structured data extraction, ontology development, and
semantic enrichment to produce KGs tailored for legal cases involving violence
against women. After analyzing and comparing the results of the two approaches,
the developed KGs are validated via suitable competency questions. The obtained
KG may be impactful for multiple purposes: can improve the accessibility to
legal information both to humans and machine, can enable complex queries and
may constitute an important knowledge component to be possibly exploited by
machine learning tools tailored for predictive justice.

</details>


### [27] [The Fair Game: Auditing & Debiasing AI Algorithms Over Time](https://arxiv.org/abs/2508.06443)
*Debabrota Basu,Udvas Das*

Main category: cs.AI

TL;DR: The paper introduces 'Fair Game,' a dynamic mechanism using Reinforcement Learning to adaptively ensure fairness in ML predictions over time.


<details>
  <summary>Details</summary>
Motivation: Current Fair ML approaches face limitations as their bias definitions are often conflicting and static, hindering effective use in dynamic social environments.

Method: Introduces 'Fair Game,' combining an Auditor and Debiasing algorithm within a Reinforcement Learning loop to adapt ML system predictions based on societal feedback.

Result: The framework enables adaptive fairness that evolves with societal changes by modifying the Auditor's bias definitions.

Conclusion: 'Fair Game' facilitates the development of flexible ML systems capable of addressing fairness pre- and post-deployment, aligning with societal ethical and legal evolution.

Abstract: An emerging field of AI, namely Fair Machine Learning (ML), aims to quantify
different types of bias (also known as unfairness) exhibited in the predictions
of ML algorithms, and to design new algorithms to mitigate them. Often, the
definitions of bias used in the literature are observational, i.e. they use the
input and output of a pre-trained algorithm to quantify a bias under concern.
In reality,these definitions are often conflicting in nature and can only be
deployed if either the ground truth is known or only in retrospect after
deploying the algorithm. Thus,there is a gap between what we want Fair ML to
achieve and what it does in a dynamic social environment. Hence, we propose an
alternative dynamic mechanism,"Fair Game",to assure fairness in the predictions
of an ML algorithm and to adapt its predictions as the society interacts with
the algorithm over time. "Fair Game" puts together an Auditor and a Debiasing
algorithm in a loop around an ML algorithm. The "Fair Game" puts these two
components in a loop by leveraging Reinforcement Learning (RL). RL algorithms
interact with an environment to take decisions, which yields new observations
(also known as data/feedback) from the environment and in turn, adapts future
decisions. RL is already used in algorithms with pre-fixed long-term fairness
goals. "Fair Game" provides a unique framework where the fairness goals can be
adapted over time by only modifying the auditor and the different biases it
quantifies. Thus,"Fair Game" aims to simulate the evolution of ethical and
legal frameworks in the society by creating an auditor which sends feedback to
a debiasing algorithm deployed around an ML system. This allows us to develop a
flexible and adaptive-over-time framework to build Fair ML systems pre- and
post-deployment.

</details>


### [28] [What Voting Rules Actually Do: A Data-Driven Analysis of Multi-Winner Voting](https://arxiv.org/abs/2508.06454)
*Joshua Caiata,Ben Armstrong,Kate Larson*

Main category: cs.AI

TL;DR: The paper introduces a data-driven framework to examine how often different multi-winner voting rules violate axioms under various preference distributions, rather than relying solely on worst-case scenarios.


<details>
  <summary>Details</summary>
Motivation: Traditional analysis of multi-winner voting rules tends to focus on whether these rules satisfy axioms in a binary manner under worst-case scenarios. There is a need for a practical evaluation method that considers violations across diverse preference distributions.

Method: The researchers employed a data-driven framework to evaluate the frequency of axiom violations of multi-winner voting rules across diverse preference distributions. They also compared standard voting rules with neural networks acting as voting rules.

Result: Neural networks, when used as voting rules, exhibited better performance in reducing axiom violations compared to traditional voting mechanisms.

Conclusion: The study suggests that data-driven approaches can improve the design and evaluation of voting systems, emphasizing the potential for neural networks in developing advanced voting methodologies.

Abstract: Committee-selection problems arise in many contexts and applications, and
there has been increasing interest within the social choice research community
on identifying which properties are satisfied by different multi-winner voting
rules. In this work, we propose a data-driven framework to evaluate how
frequently voting rules violate axioms across diverse preference distributions
in practice, shifting away from the binary perspective of axiom satisfaction
given by worst-case analysis. Using this framework, we analyze the relationship
between multi-winner voting rules and their axiomatic performance under several
preference distributions. We then show that neural networks, acting as voting
rules, can outperform traditional rules in minimizing axiom violations. Our
results suggest that data-driven approaches to social choice can inform the
design of new voting systems and support the continuation of data-driven
research in social choice.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [29] [ConiQ: Enabling Concatenated Quantum Error Correction on Neutral Atom Arrays](https://arxiv.org/abs/2508.05779)
*Pengyu Liu,Mingkuan Xu,Hengyun Zhou,Hanrui Wang,Umut A. Acar,Yunong Shi*

Main category: cs.AR

TL;DR: The paper presents an efficient compilation method for many-hypercube codes using neutral atom arrays, significantly reducing overhead and enhancing practicality for fault-tolerant quantum computing.


<details>
  <summary>Details</summary>
Motivation: Although concatenated codes offer high space efficiency, they face challenges with efficient implementation of logical gates and hardware limitations on parallelism and long-range interactions.

Method: The authors propose two innovations: Automorphism-assisted Hierarchical Addressing (AHA) for efficient logical gates and Virtual Atom Intermediate Representation (VAIR) for optimization and legalization. These are integrated into ConiQ, a specialized quantum compiler for neutral atom arrays.

Result: ConiQ achieves up to 2000x spacetime overhead reduction, up to 10^6x faster compilation times, and an additional 20x overhead reduction from AHA gates compared to current methods.

Conclusion: The work demonstrates that concatenated codes, with the proposed methods, are highly effective for fault-tolerant quantum computing and pave the way for practical adoption in near-future quantum systems.

Abstract: Recent progress on concatenated codes, especially many-hypercube codes,
achieves unprecedented space efficiency. Yet two critical challenges persist in
practice. First, these codes lack efficient implementations of addressable
logical gates. Second, the required high degree of parallelism and long-range
interactions pose significant challenges for current hardware platforms. In
this paper, we propose an efficient compilation approach for concatenated
codes, specifically many-hypercube codes, targeted at neutral atom arrays,
which provide the necessary parallelism and long-range interactions. Our
approach builds on two key innovations. First, we introduce
Automorphism-assisted Hierarchical Addressing (AHA) logical CNOT gates that
significantly reduce spacetime overhead compared to conventional
distillation-based methods. Second, we develop Virtual Atom Intermediate
Representation (VAIR) that enables level-wise optimization and legalization. We
implement these innovations in ConiQ, a hardware-aware quantum compiler
designed to compile fault-tolerant quantum circuits for neutral atom arrays
using many-hypercube codes. Our evaluation demonstrates that ConiQ achieves up
to 2000x reduction in spacetime overhead and up to 10^6x reduction in
compilation time compared to state-of-the-art compilers, with our AHA gates
providing an additional overhead reduction of up to 20x. These results
establish concatenated codes as a promising approach for fault-tolerant quantum
computing in the near future.

</details>


### [30] [ArchXBench: A Complex Digital Systems Benchmark Suite for LLM Driven RTL Synthesis](https://arxiv.org/abs/2508.06047)
*Suresh Purini,Siddhant Garg,Mudit Gaur,Sankalp Bhat,Sohan Mupparapu,Arun Ravindran*

Main category: cs.AR

TL;DR: The paper introduces ArchXBench, a benchmark suite designed to evaluate the capabilities of LLMs in generating complex digital subsystems for RTL design. Current models perform well on simple circuits but fail on advanced ones.


<details>
  <summary>Details</summary>
Motivation: To address the limitations in applying LLMs for RTL design, particularly improving the automation of complex pipelined and hierarchical digital systems design.

Method: The authors created ArchXBench, a benchmark suite with six levels of complexity, including circuit designs from cryptography, image processing, machine learning, and signal processing. They tested various LLMs using zero-shot prompting and evaluated performance using a pass@5 criterion.

Result: The o4-mini-high model outperformed others, solving 16 out of 30 benchmarks from Levels 1-3. However, all models failed to solve Level 4 and above benchmarks.

Conclusion: Current LLM capabilities are insufficient for addressing complex RTL designs, revealing a need for further research and development in this area.

Abstract: Modern SoC datapaths include deeply pipelined, domain-specific accelerators,
but their RTL implementation and verification are still mostly done by hand.
While large language models (LLMs) exhibit advanced code-generation abilities
for programming languages like Python, their application to Verilog-like RTL
remains in its nascent stage. This is reflected in the simple arithmetic and
control circuits currently used to evaluate generative capabilities in existing
benchmarks. In this paper, we introduce ArchXBench, a six-level benchmark suite
that encompasses complex arithmetic circuits and other advanced digital
subsystems drawn from domains such as cryptography, image processing, machine
learning, and signal processing. Architecturally, some of these designs are
purely combinational, others are multi-cycle or pipelined, and many require
hierarchical composition of modules. For each benchmark, we provide a problem
description, design specification, and testbench, enabling rapid research in
the area of LLM-driven agentic approaches for complex digital systems design.
  Using zero-shot prompting with Claude Sonnet 4, GPT 4.1, o4-mini-high, and
DeepSeek R1 under a pass@5 criterion, we observed that o4-mini-high
successfully solves the largest number of benchmarks, 16 out of 30, spanning
Levels 1, 2, and 3. From Level 4 onward, however, all models consistently fail,
highlighting a clear gap in the capabilities of current state-of-the-art LLMs
and prompting/agentic approaches.

</details>


### [31] [Nail: Not Another Fault-Injection Framework for Chisel-generated RTL](https://arxiv.org/abs/2508.06344)
*Robin Sehm,Christian Ewert,Rainer Buchty,Mladen Berekovic,Saleh Mulhem*

Main category: cs.AR

TL;DR: This paper presents Nail, a Chisel-based fault injection framework that provides state-based fault modeling for precise and controllable fault emulation and simulation.


<details>
  <summary>Details</summary>
Motivation: Current Chisel-based fault injection frameworks are limited to coarse-grained, instruction-level fault modeling, reducing precision and usability in advanced applications.

Method: Authors developed Nail, utilizing Chisel to enable state-based fault modeling and runtime modification of system triggers via an automated software interface for enhanced flexibility and precision.

Result: Nail successfully models state-dependent faults, like a faulty register in a RISC-V processor, with validation in both simulation and FPGA emulation. It introduces less than 1% resource overhead.

Conclusion: Nail bridges the gap between emulation speed and simulation controllability, offering improved fault modeling techniques for fault injection campaigns without significantly impacting resource usage.

Abstract: Fault simulation and emulation are essential techniques for evaluating the
dependability of integrated circuits, enabling early-stage vulnerability
analysis and supporting the implementation of effective mitigation strategies.
High-level hardware description languages such as Chisel facilitate the rapid
development of complex fault scenarios with minimal modification to the design.
However, existing Chisel-based fault injection (FI) frameworks are limited by
coarse-grained, instruction-level controllability, restricting the precision of
fault modeling. This work introduces Nail, a Chisel-based open-source FI
framework that overcomes these limitations by introducing state-based faults.
This approach enables fault scenarios that depend on specific system states,
rather than solely on instruction-level triggers, thereby removing the need for
precise timing of fault activation. For greater controllability, Nail allows
users to arbitrarily modify internal trigger states via software at runtime. To
support this, Nail automatically generates a software interface, offering
straightforward access to the instrumented design. This enables fine-tuning of
fault parameters during active FI campaigns - a feature particularly beneficial
for FPGA emulation, where synthesis is time-consuming. Utilizing these
features, Nail narrows the gap between the high speed of emulation-based FI
frameworks, the usability of software-based approaches, and the controllability
achieved in simulation. We demonstrate Nail's state-based FI and software
framework by modeling a faulty general-purpose register in a RISC-V processor.
Although this might appear straightforward, it requires state-dependent FI and
was previously impossible without fundamental changes to the design. The
approach was validated in both simulation and FPGA emulation, where the
addition of Nail introduced less than 1% resource overhead.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [32] [PEACH: A sentence-aligned Parallel English-Arabic Corpus for Healthcare](https://arxiv.org/abs/2508.05722)
*Rania Al-Sabbagh*

Main category: cs.CL

TL;DR: The paper introduces PEACH, a publicly accessible, manually aligned English-Arabic healthcare text corpus.


<details>
  <summary>Details</summary>
Motivation: To create a high-quality resource aiding linguistics, translation studies, and natural language processing in the healthcare domain.

Method: Developed PEACH, a corpus with 51,671 parallel English-Arabic sentences manually aligned to ensure gold-standard quality.

Result: The corpus consists of ~590,517 English and ~567,707 Arabic word tokens, with an average sentence length of 9.52–11.83 words.

Conclusion: PEACH serves as a valuable tool for tasks like bilingual lexicon derivation, machine translation evaluation, and translation studies, and is made freely accessible.

Abstract: This paper introduces PEACH, a sentence-aligned parallel English-Arabic
corpus of healthcare texts encompassing patient information leaflets and
educational materials. The corpus contains 51,671 parallel sentences, totaling
approximately 590,517 English and 567,707 Arabic word tokens. Sentence lengths
vary between 9.52 and 11.83 words on average. As a manually aligned corpus,
PEACH is a gold-standard corpus, aiding researchers in contrastive linguistics,
translation studies, and natural language processing. It can be used to derive
bilingual lexicons, adapt large language models for domain-specific machine
translation, evaluate user perceptions of machine translation in healthcare,
assess patient information leaflets and educational materials' readability and
lay-friendliness, and as an educational resource in translation studies. PEACH
is publicly accessible.

</details>


### [33] [Guardians and Offenders: A Survey on Harmful Content Generation and Safety Mitigation](https://arxiv.org/abs/2508.05775)
*Chi Zhang,Changjia Zhu,Junjie Xiong,Xiaoran Xu,Lingyao Li,Yao Liu,Zhuo Lu*

Main category: cs.CL

TL;DR: This survey examines the dual nature of Large Language Models (LLMs) as both beneficial tools and potential risks, exploring toxicity, adversarial attacks, and mitigation strategies.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the sociotechnical challenges posed by LLMs' capability to produce both beneficial applications and harmful language.

Method: The approach involves a systematic review of studies related to toxicity, adversarial jailbreaking attacks, and content moderation, proposing a taxonomy of harms and defenses.

Result: The survey identifies evolving multimodal and LLM-assisted jailbreak strategies. It also evaluates mitigation techniques like RLHF, prompt engineering, and safety alignment.

Conclusion: The paper highlights the limitations in current evaluation methodologies and suggests future research directions to improve LLM safety and ethical alignment.

Abstract: Large Language Models (LLMs) have revolutionized content creation across
digital platforms, offering unprecedented capabilities in natural language
generation and understanding. These models enable beneficial applications such
as content generation, question and answering (Q&A), programming, and code
reasoning. Meanwhile, they also pose serious risks by inadvertently or
intentionally producing toxic, offensive, or biased content. This dual role of
LLMs, both as powerful tools for solving real-world problems and as potential
sources of harmful language, presents a pressing sociotechnical challenge. In
this survey, we systematically review recent studies spanning unintentional
toxicity, adversarial jailbreaking attacks, and content moderation techniques.
We propose a unified taxonomy of LLM-related harms and defenses, analyze
emerging multimodal and LLM-assisted jailbreak strategies, and assess
mitigation efforts, including reinforcement learning with human feedback
(RLHF), prompt engineering, and safety alignment. Our synthesis highlights the
evolving landscape of LLM safety, identifies limitations in current evaluation
methodologies, and outlines future research directions to guide the development
of robust and ethically aligned language technologies.

</details>


### [34] [FineDialFact: A benchmark for Fine-grained Dialogue Fact Verification](https://arxiv.org/abs/2508.05782)
*Xiangyan Chen,Yufeng Li,Yujian Gan,Arkaitz Zubiaga,Matthew Purver*

Main category: cs.CL

TL;DR: The paper introduces FineDialFact, a benchmark for fine-grained fact verification in dialogue systems, with Chain-of-Thought reasoning improving but not solving the problem.


<details>
  <summary>Details</summary>
Motivation: Detecting hallucinations in dialogue systems is critical as they often mix accurate, inaccurate, or unverifiable facts, making current single-label factual verification approaches insufficient.

Method: The authors create the FineDialFact benchmark and dataset for fine-grained fact verification in dialogue systems, using publicly available data and testing baseline methods, including Chain-of-Thought reasoning.

Result: Experiments show that Chain-of-Thought reasoning improves performance, but the best F1-score on HybriDialogue was 0.75, revealing the task remains challenging.

Conclusion: FineDialFact is a significant step towards improving fact verification in dialogue systems, but there is a lot of room for future advancements.

Abstract: Large Language Models (LLMs) are known to produce hallucinations - factually
incorrect or fabricated information - which poses significant challenges for
many Natural Language Processing (NLP) applications, such as dialogue systems.
As a result, detecting hallucinations has become a critical area of research.
Current approaches to hallucination detection in dialogue systems primarily
focus on verifying the factual consistency of generated responses. However,
these responses often contain a mix of accurate, inaccurate or unverifiable
facts, making one factual label overly simplistic and coarse-grained. In this
paper, we introduce a benchmark, FineDialFact, for fine-grained dialogue fact
verification, which involves verifying atomic facts extracted from dialogue
responses. To support this, we construct a dataset based on publicly available
dialogue datasets and evaluate it using various baseline methods. Experimental
results demonstrate that methods incorporating Chain-of-Thought (CoT) reasoning
can enhance performance in dialogue fact verification. Despite this, the best
F1-score achieved on the HybriDialogue, an open-domain dialogue dataset, is
only 0.75, indicating that the benchmark remains a challenging task for future
research. Our dataset and code will be public on GitHub.

</details>


### [35] [Human-like fleeting memory improves language learning but impairs reading time prediction in transformer language models](https://arxiv.org/abs/2508.05803)
*Abishek Thamma,Micha Heilbron*

Main category: cs.CL

TL;DR: Investigates the role of fleeting memory on language learning and its effects on performance and prediction in transformer models.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore why fleeting memory might paradoxically aid language learning, a notion at odds with the capabilities of transformer models which lack such limitations.

Method: Experiments were conducted by training transformer models with and without memory limitations using a realistic training set to analyze language modeling and syntactic evaluation performance.

Result: Fleeting memory improves language modeling and syntax evaluation but impairs prediction of human reading times, a counterintuitive result unexplained by prior theories.

Conclusion: Memory limitations benefit neural network-based language learning but do not enhance the models’ ability to predict human reading behaviors.

Abstract: Human memory is fleeting. As words are processed, the exact wordforms that
make up incoming sentences are rapidly lost. Cognitive scientists have long
believed that this limitation of memory may, paradoxically, help in learning
language - an idea supported by classic connectionist modelling work. The rise
of Transformers appears to challenge this idea, as these models can learn
language effectively, despite lacking memory limitations or other architectural
recency biases. Here, we investigate the hypothesized benefit of fleeting
memory for language learning in tightly controlled experiments on transformer
language models. Training transformers with and without fleeting memory on a
developmentally realistic training set, we find that fleeting memory
consistently improves language learning (as quantified by both overall language
modelling performance and targeted syntactic evaluation) but, unexpectedly,
impairs surprisal-based prediction of human reading times. Interestingly,
follow up analyses revealed that this discrepancy - better language modeling,
yet worse reading time prediction - could not be accounted for by prior
explanations of why better language models sometimes fit human reading time
worse. Together, these results support a benefit of memory limitations on
neural network language learning - but not on predicting behavior.

</details>


### [36] ["Mirror" Language AI Models of Depression are Criterion-Contaminated](https://arxiv.org/abs/2508.05830)
*Tong Li,Rasiq Hussain,Mehak Gupta,Joshua R. Oltmanns*

Main category: cs.CL

TL;DR: Studies find that AI language models achieve high accuracy in predicting depression scores based on direct input responses (Mirror models), but these models exhibit bias and limited generalizability due to criterion contamination. Non-Mirror models provide more reliable but smaller effect sizes.


<details>
  <summary>Details</summary>
Motivation: To assess the reliability and generalizability of language-based AI models for predicting depression scores, and highlight the limitations introduced by criterion contamination in Mirror models.

Method: The study compares "Mirror models"—developed from structured diagnostic language—to "Non-Mirror models," trained on non-diagnostic life history interviews. Predictions were tested using GPT-4, GPT-4o, and LLaMA3-70B across participant data.

Result: Mirror models achieved high effect sizes (R2 = .80) but were biased, while Non-Mirror models achieved lower effect sizes (R2 = .27). Despite this, both model types performed equally (r ~ .54) in correlating AI predictions with self-reported depression scores.

Conclusion: Mirror models show inflated effect sizes due to overfitting and bias, impairing generalizability. Non-Mirror models offer a path to more generalizable and interpretable AI-based semantic predictions for psychology applications.

Abstract: A growing number of studies show near-perfect LLM language-based prediction
of depression assessment scores (up to R2 of .70). However, many develop these
models directly from language responses to depression assessments. These
"Mirror models" suffer from "criterion contamination", which arises when a
predicted score depends in part on the predictors themselves. This causes
artificial effect size inflation which reduces model generalizability. The
present study compares the performance of Mirror models versus "Non-Mirror
models", which are developed from language that does not mirror the assessment
they are developed to predict. N = 110 research participants completed two
different interviews: structured diagnostic and life history interviews. GPT-4,
GPT-4o and LLaMA3-70B were then prompted to predict structured diagnostic
interview depression scores from the two transcripts separately. Mirror models
(using structured diagnostic data) showed very large effect sizes (e.g., R2 =
.80). As expected, NonMirror models (using life history data) demonstrated
smaller effect sizes, but were relatively large (e.g., R2 = .27). When Mirror
and Non-Mirror model-predicted structured interview depression scores were
correlated with self-reported depression symptoms, Mirror and NonMirror
performed the same (e.g., r = ~.54), indicating that Mirror models contain bias
perhaps due to criterion contamination. Topic modeling identified clusters
across Mirror and Non-Mirror models, as well as between true-positive and
false-positive predictions. In this head-to-head comparison study, Mirror
language AI models of depression showed artificially inflated effect sizes and
less generalizability. As language AI models for depression continue to evolve,
incorporating Non-Mirror models may identify interpretable, and generalizable
semantic features that have unique utility in real-world psychological
assessment.

</details>


### [37] [Discovering Properties of Inflectional Morphology in Neural Emergent Communication](https://arxiv.org/abs/2508.05843)
*Miles Gilberti,Shane Storks,Huteng Dai*

Main category: cs.CL

TL;DR: This paper enhances emergent communication research by simulating natural language morphological traits, like inflectional morphology, via small vocabularies and novel metrics.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between emergent communication in deep neural network-based agents and the complexities of human-like natural language.

Method: The study explores a variation of the attribute-value reconstruction game by introducing a small vocabulary and experiments reflecting real morphological properties such as concatenativity and fusionality.

Result: Simulated phonological constraints promoted concatenative morphology, and emergent languages showed natural tendencies towards fusing grammatical attributes.

Conclusion: The approach bridges emergent communication settings and natural language systems by drawing parallels in morphological characteristics.

Abstract: Emergent communication (EmCom) with deep neural network-based agents promises
to yield insights into the nature of human language, but remains focused
primarily on a few subfield-specific goals and metrics that prioritize
communication schemes which represent attributes with unique characters
one-to-one and compose them syntactically. We thus reinterpret a common EmCom
setting, the attribute-value reconstruction game, by imposing a
small-vocabulary constraint to simulate double articulation, and formulating a
novel setting analogous to naturalistic inflectional morphology (enabling
meaningful comparison to natural language communication schemes). We develop
new metrics and explore variations of this game motivated by real properties of
inflectional morphology: concatenativity and fusionality. Through our
experiments, we discover that simulated phonological constraints encourage
concatenative morphology, and emergent languages replicate the tendency of
natural languages to fuse grammatical attributes.

</details>


### [38] [Do Machines Think Emotionally? Cognitive Appraisal Analysis of Large Language Models](https://arxiv.org/abs/2508.05880)
*Sree Bhattacharyya,Lucas Craig,Tharun Dilliraj,Jia Li,James Z. Wang*

Main category: cs.CL

TL;DR: This paper introduces a benchmark, CoRE, to evaluate how Large Language Models (LLMs) reason about emotions based on cognitive appraisal theory rather than just performing superficial emotion tasks.


<details>
  <summary>Details</summary>
Motivation: Existing evaluations of LLMs on emotion tasks focus mainly on recognizing or generating emotions based on discrete labels, which fail to address deeper cognitive reasoning about emotions.

Method: The authors utilize cognitive appraisal theory to assess the emotional reasoning capabilities of LLMs, introducing the CoRE benchmark to measure their internal reasoning patterns.

Result: Findings indicate that LLMs exhibit varied cognitive reasoning patterns for emotion reasoning, suggesting reliance on specific cognitive dimensions and providing interpretability of emotional representations.

Conclusion: The study highlights the need for deeper, cognition-driven evaluations of LLMs and makes CoRE a foundational toolkit for assessing emotional reasoning in these models, with benchmark and code publicly available.

Abstract: Affective Computing has been established as a crucial field of inquiry to
advance the holistic development of Artificial Intelligence (AI) systems.
Foundation models -- especially Large Language Models (LLMs) -- have been
evaluated, trained, or instruction-tuned in several past works, to become
better predictors or generators of emotion. Most of these studies, however,
approach emotion-related tasks in a supervised manner, assessing or training
the capabilities of LLMs using discrete emotion labels associated with stimuli
(e.g., text, images, video, audio). Evaluation studies, in particular, have
often been limited to standard and superficial emotion-related tasks, such as
the recognition of evoked or expressed emotions. In this paper, we move beyond
surface-level emotion tasks to investigate how LLMs reason about emotions
through cognitive dimensions. Drawing from cognitive appraisal theory, we
examine whether LLMs produce coherent and plausible cognitive reasoning when
reasoning about emotionally charged stimuli. We introduce a large-scale
benchmark on Cognitive Reasoning for Emotions - CoRE - to evaluate internal
cognitive structures implicitly used by LLMs for emotional reasoning. Through a
plethora of evaluation experiments and analysis, we seek to answer: (a) Are
models more likely to implicitly rely on specific cognitive appraisal
dimensions?, (b) What cognitive dimensions are important for characterizing
specific emotions?, and, (c) Can the internal representations of different
emotion categories in LLMs be interpreted through cognitive appraisal
dimensions? Our results and analyses reveal diverse reasoning patterns across
different LLMs. Our benchmark and code will be made publicly available.

</details>


### [39] [Spectrum Projection Score: Aligning Retrieved Summaries with Reader Models in Retrieval-Augmented Generation](https://arxiv.org/abs/2508.05909)
*Zhanghao Hu,Qinglin Zhu,Siya Qi,Yulan He,Hanqi Yan,Lin Gui*

Main category: cs.CL

TL;DR: The paper introduces a lightweight, supervision-free metric called Spectrum Projection Score (SPS) to assess the semantic alignment of retrieved summaries in retrieval-augmented generation. It also proposes xCompress, a framework optimizing retrieval at inference time, demonstrating performance improvements across QA benchmarks.


<details>
  <summary>Details</summary>
Motivation: To enhance the evaluation of retrieval effectiveness in retrieval-augmented generation (RAG) and address the challenge of isolating retrieval contributions from generation, especially given the prompt sensitivity of large language models.

Method: The authors introduce SPS, which compares semantic alignment via the area of generated tokens and principal directions of the reader's hidden representation. Using SPS, they develop xCompress, which dynamically samples, ranks, and compresses retrieved summaries during inference.

Result: Experiments on five QA benchmarks with four open-source LLMs show that SPS and xCompress improve performance across tasks, providing useful insights into retrieval-generation interaction.

Conclusion: The study offers a novel metric (SPS) and inference framework (xCompress) that effectively improve retrieval-augmented generation while deepening the understanding of retrieval-generation dynamics.

Abstract: Large Language Models (LLMs) have shown improved generation performance
through retrieval-augmented generation (RAG) following the retriever-reader
paradigm, which supplements model inputs with externally retrieved knowledge.
However, prior work often evaluates RAG holistically, assessing the retriever
and reader jointly, making it difficult to isolate the true contribution of
retrieval, particularly given the prompt sensitivity of LLMs used as readers.
We introduce Spectrum Projection Score (SPS), a lightweight, supervision-free
metric that allows the reader to gauge the semantic alignment of a retrieved
summary with its hidden representation by comparing the area formed by
generated tokens from the summary, and the principal directions of subspace in
the reader and to measure the relevance. Building on SPS we present xCompress,
an inference time controller framework that dynamically samples, ranks, and
compresses retrieval summary candidates. Extensive experiments on five QA
benchmarks with four open source LLMs show that SPS not only enhances
performance across a range of tasks but also provides a principled perspective
on the interaction between retrieval and generation.

</details>


### [40] [Prosocial Behavior Detection in Player Game Chat: From Aligning Human-AI Definitions to Efficient Annotation at Scale](https://arxiv.org/abs/2508.05938)
*Rafal Kocielnik,Min Kim,Penphob,Boonyarungsrit,Fereshteh Soltani,Deshawn Sambrano,Animashree Anandkumar,R. Michael Alvarez*

Main category: cs.CL

TL;DR: The paper introduces a three-stage pipeline to classify prosocial text with high precision and minimal labeling and inference costs.


<details>
  <summary>Details</summary>
Motivation: There is a need for identifying and classifying prosocial text content, which supports or improves others' behavior, due to its significance in trust and safety systems—but there is a lack of established definitions and datasets for this task.

Method: The authors propose a practical pipeline involving: (1) determining effective LLM-based labeling using a small human-labeled dataset, (2) employing a human-AI refinement loop to improve annotation definitions and label quality, and (3) generating 10k high-quality labels with GPT-4 to train a two-stage model for efficient classification.

Result: The resulting system reduces inference costs by approximately 70%, ensures scalable implementation, and achieves high precision (~0.90).

Conclusion: The study demonstrates the effectiveness of combining human-AI interaction, thoughtful task design, and cost-efficient architecture to handle emerging classification tasks such as prosociality detection.

Abstract: Detecting prosociality in text--communication intended to affirm, support, or
improve others' behavior--is a novel and increasingly important challenge for
trust and safety systems. Unlike toxic content detection, prosociality lacks
well-established definitions and labeled data, requiring new approaches to both
annotation and deployment. We present a practical, three-stage pipeline that
enables scalable, high-precision prosocial content classification while
minimizing human labeling effort and inference costs. First, we identify the
best LLM-based labeling strategy using a small seed set of human-labeled
examples. We then introduce a human-AI refinement loop, where annotators review
high-disagreement cases between GPT-4 and humans to iteratively clarify and
expand the task definition-a critical step for emerging annotation tasks like
prosociality. This process results in improved label quality and definition
alignment. Finally, we synthesize 10k high-quality labels using GPT-4 and train
a two-stage inference system: a lightweight classifier handles high-confidence
predictions, while only $\sim$35\% of ambiguous instances are escalated to
GPT-4o. This architecture reduces inference costs by $\sim$70% while achieving
high precision ($\sim$0.90). Our pipeline demonstrates how targeted human-AI
interaction, careful task formulation, and deployment-aware architecture design
can unlock scalable solutions for novel responsible AI tasks.

</details>


### [41] [Adversarial Topic-aware Prompt-tuning for Cross-topic Automated Essay Scoring](https://arxiv.org/abs/2508.05987)
*Chunyun Zhang,Hongyan Zhao,Chaoran Cui,Qilong Song,Zhiqing Lu,Shuai Gong,Kailin Liu*

Main category: cs.CL

TL;DR: The study introduces ATOP, a method enhancing cross-topic essay scoring accuracy by integrating topic-shared and topic-specific features using language models.


<details>
  <summary>Details</summary>
Motivation: Existing methods for cross-topic AES often fail to properly assess topic adherence due to neglect of topic-specific features.

Method: ATOP uses topic-aware prompts to extract shared and specific features and applies adversarial training to improve robustness, alongside pseudo-label approaches for supervised learning.

Result: Experiments using the ASAP++ dataset show that ATOP outperforms current state-of-the-art methods for holistic and multi-trait scoring.

Conclusion: ATOP addresses key limitations in cross-topic AES, providing a novel framework that leverages pre-trained language models for improved scoring across diverse topics.

Abstract: Cross-topic automated essay scoring (AES) aims to develop a transferable
model capable of effectively evaluating essays on a target topic. A significant
challenge in this domain arises from the inherent discrepancies between topics.
While existing methods predominantly focus on extracting topic-shared features
through distribution alignment of source and target topics, they often neglect
topic-specific features, limiting their ability to assess critical traits such
as topic adherence. To address this limitation, we propose an Adversarial
TOpic-aware Prompt-tuning (ATOP), a novel method that jointly learns
topic-shared and topic-specific features to improve cross-topic AES. ATOP
achieves this by optimizing a learnable topic-aware prompt--comprising both
shared and specific components--to elicit relevant knowledge from pre-trained
language models (PLMs). To enhance the robustness of topic-shared prompt
learning and mitigate feature scale sensitivity introduced by topic alignment,
we incorporate adversarial training within a unified regression and
classification framework. In addition, we employ a neighbor-based classifier to
model the local structure of essay representations and generate pseudo-labels
for target-topic essays. These pseudo-labels are then used to guide the
supervised learning of topic-specific prompts tailored to the target topic.
Extensive experiments on the publicly available ASAP++ dataset demonstrate that
ATOP significantly outperforms existing state-of-the-art methods in both
holistic and multi-trait essay scoring. The implementation of our method is
publicly available at: https://anonymous.4open.science/r/ATOP-A271.

</details>


### [42] [Crisp Attention: Regularizing Transformers via Structured Sparsity](https://arxiv.org/abs/2508.06016)
*Sagar Gandhi,Vishal Gandhi*

Main category: cs.CL

TL;DR: The study demonstrates that introducing strategic sparsity to a Transformer model's attention improves both efficiency and accuracy, challenging traditional assumptions.


<details>
  <summary>Details</summary>
Motivation: Address the computational inefficiency of self-attention mechanisms in Transformer models while investigating whether sparsity negatively impacts accuracy.

Method: Integrate structured, post-hoc sparsity into the attention mechanism of a DistilBERT model during fine-tuning on the SST-2 sentiment analysis task.

Result: Achieved 91.59% validation accuracy with 80% attention sparsity, improving accuracy by 0.97% over a dense baseline.

Conclusion: Sparsity not only enhances computational efficiency but also acts as an implicit regularizer, improving the generalization and performance of Transformer models.

Abstract: The quadratic computational cost of the self-attention mechanism is a primary
challenge in scaling Transformer models. While attention sparsity is widely
studied as a technique to improve computational efficiency, it is almost
universally assumed to come at the cost of model accuracy. In this paper, we
report a surprising counter-example to this common wisdom. By introducing
structured, post-hoc sparsity to the attention mechanism of a DistilBERT model
during fine-tuning on the SST-2 sentiment analysis task, we find that model
accuracy improves significantly. Our model with 80\% attention sparsity
achieves a validation accuracy of 91.59\%, a 0.97\% absolute improvement over
the dense baseline. We hypothesize that this phenomenon is due to sparsity
acting as a powerful implicit regularizer, preventing the model from
overfitting by forcing it to make predictions with a more constrained and
robust set of features. Our work recasts attention sparsity not just as a tool
for computational efficiency, but as a potential method for improving the
generalization and performance of Transformer models.

</details>


### [43] [Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future](https://arxiv.org/abs/2508.06026)
*Yidong Wang,Xin Wang,Cunxiang Wang,Junfeng Fang,Qiufeng Wang,Jianing Chu,Xuran Meng,Shuxun Yang,Libo Qin,Yue Zhang,Wei Ye,Shikun Zhang*

Main category: cs.CL

TL;DR: The paper introduces Temporal Self-Rewarding Language Models to address limitations in existing self-rewarding setups for LLMs, showcasing improved capabilities through a framework coordinating past, present, and future generations.


<details>
  <summary>Details</summary>
Motivation: To overcome the representational narrowing in existing Self-Rewarding Language Models, which hampers effective preference learning.

Method: A dual-phase framework featuring Anchored Rejection for rejected responses and Future-Guided Chosen for chosen outputs, strategically utilizing past and future model generations.

Result: Achieved significant improvements in win rates and generalization across diverse tasks in experiments with multiple model families and sizes, notably outperforming Self-Rewarding baselines.

Conclusion: Temporal coordination in model training leads to better generative capabilities and adaptability, enabling robust out-of-distribution generalization without specialized training datasets.

Abstract: Self-Rewarding Language Models propose an architecture in which the Large
Language Models(LLMs) both generates responses and evaluates its own outputs
via LLM-as-a-Judge prompting, dynamically improving its generative capabilities
through iterative Direct Preference Optimization (DPO). However, our analysis
reveals a critical limitation in existing Self-Rewarding paradigms: the
synchronized improvement of chosen and rejected responses progressively narrows
the representational difference between contrasting samples, undermining
effective preference learning. We propose \textbf{Temporal Self-Rewarding
Language Models} that strategically coordinate past, present, and future model
generations to sustain learning signals. Our dual-phase framework introduces:
(1) \textit{Anchored Rejection} - fixing rejected responses using the past
initial model's outputs and (2) \textit{Future-Guided Chosen} - dynamically
curating chosen samples using next-generation model predictions. Extensive
experiments across three model families (Llama, Qwen, Mistral) and different
model sizes (Llama3B/8B/70B) demonstrate significant improvements when trained
with our method compared to Self-Rewarding using same computation resources.
For example, Llama3.1-8B reaches a 29.44 win rate on AlpacaEval 2.0 with our
method, outperforming the Self-Rewarding baseline (19.69) by 9.75. Notably, our
method also demonstrates superior out-of-distribution generalization across
mathematical reasoning (GSM8K), knowledge-based QA (ARC, TruthfulQA), and code
generation (HumanEval) tasks, even though we do not specifically collect such
training data.

</details>


### [44] [Efficient Knowledge Probing of Large Language Models by Adapting Pre-trained Embeddings](https://arxiv.org/abs/2508.06030)
*Kartik Sharma,Yiqiao Jin,Rakshit Trivedi,Srijan Kumar*

Main category: cs.CL

TL;DR: The paper proposes PEEK, a method using proxy embeddings to estimate the knowledge of large language models (LLMs) efficiently without computationally expensive forward passes.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of determining the factual knowledge acquired by LLMs in a computationally efficient manner.

Method: The authors use pre-trained embedding models to predict LLM outputs by adapting these embeddings through a linear decoder layer trained on a set of facts known by the LLMs.

Result: Embeddings were able to predict LLM knowledge on a held-out dataset with up to 90% accuracy, with sentence embeddings outperforming graph embeddings.

Conclusion: Knowledge-adapted embeddings can identify gaps in LLM knowledge at scale and offer insights into their inductive bias, making them a promising tool for efficiently probing LLM knowledge.

Abstract: Large language models (LLMs) acquire knowledge across diverse domains such as
science, history, and geography encountered during generative pre-training.
However, due to their stochasticity, it is difficult to predict what LLMs have
acquired. Prior work has developed different ways to probe this knowledge by
investigating the hidden representations, crafting specific task prompts,
curating representative samples, and estimating their uncertainty. However,
these methods require making forward passes through the underlying model to
probe the LLM's knowledge about a specific fact, making them computationally
expensive and time-consuming. To bridge this gap, we propose $\textbf{PEEK}$ or
$\textbf{P}$roxy $\textbf{E}$mbeddings to $\textbf{E}$stimate
$\textbf{K}$nowledge of LLMs, by leveraging the pre-trained embedding models
that effectively encode factual knowledge as text or graphs as proxies for
LLMs. First, we identify a training set of facts known by LLMs through various
probing strategies and then adapt embedding models to predict the LLM outputs
with a linear decoder layer. Comprehensive evaluation on $3$ Wikipedia-derived
datasets, $4$ LLMs, and $7$ embedding models shows that embeddings can predict
LLM knowledge on a held-out set with up to 90 % accuracy. Furthermore, we find
that sentence embedding models are more suitable than graph embeddings to
predict LLM knowledge, shedding light on the underlying representation of the
factual landscape. Thus, we believe that knowledge-adapted embeddings can be
used to identify knowledge gaps in LLMs at scale and can provide deeper
insights into LLMs' internal inductive bias. The code and data are made
available at https://github.com/claws-lab/peek.

</details>


### [45] [EvolvR: Self-Evolving Pairwise Reasoning for Story Evaluation to Enhance Generation](https://arxiv.org/abs/2508.06046)
*Xinda Wang,Zhengxu Hou,Yangshijie Zhang,Bingren Yan,Zhibo Yang,Xingsheng Zhang,Luxi Xing,Qiang Zhou,Chen Zhang*

Main category: cs.CL

TL;DR: The paper introduces EvolvR, a framework enhancing story evaluation through self-synthesized and refined reasoning data, achieving SOTA performance and improving story generation quality.


<details>
  <summary>Details</summary>
Motivation: Current methods for story evaluation face challenges: prompt engineering for closed-source models lacks adaptability, and fine-tuning for open-source models lacks reasoning rigor.

Method: EvolvR employs pairwise comparison to generate self-synthesized reasoning data with a multi-persona strategy, followed by a self-filtering process using multi-agent systems, training an evaluator as a reward model for story generation.

Result: EvolvR achieved state-of-the-art performance on three benchmarks (StoryER, HANNA, and OpenMEVA) and improved the quality of generated stories when used as a reward model.

Conclusion: The EvolvR framework improves story evaluation and generation through a superior self-synthesized data approach, addressing limitations in adaptability and reasoning capabilities of existing methods.

Abstract: Although the effectiveness of Large Language Models (LLMs) as judges
(LLM-as-a-judge) has been validated, their performance remains limited in
open-ended tasks, particularly in story evaluation. Accurate story evaluation
is crucial not only for assisting human quality judgment but also for providing
key signals to guide story generation. However, existing methods face a
dilemma: prompt engineering for closed-source models suffers from poor
adaptability, while fine-tuning approaches for open-source models lack the
rigorous reasoning capabilities essential for story evaluation. To address
this, we propose the Self-Evolving Pairwise Reasoning (EvolvR) framework.
Grounded in pairwise comparison, the framework first self-synthesizes
score-aligned Chain-of-Thought (CoT) data via a multi-persona strategy. To
ensure data quality, these raw CoTs undergo a self-filtering process, utilizing
multi-agents to guarantee their logical rigor and robustness. Finally, the
evaluator trained on the refined data is deployed as a reward model to guide
the story generation task. Experimental results demonstrate that our framework
achieves state-of-the-art (SOTA) performance on three evaluation benchmarks
including StoryER, HANNA and OpenMEVA. Furthermore, when served as a reward
model, it significantly enhances the quality of generated stories, thereby
fully validating the superiority of our self-evolving approach.

</details>


### [46] [ConlangCrafter: Constructing Languages with a Multi-Hop LLM Pipeline](https://arxiv.org/abs/2508.06094)
*Morris Alper,Moran Yanuka,Raja Giryes,Gašper Beguš*

Main category: cs.CL

TL;DR: The paper introduces ConlangCrafter, a pipeline utilizing large language models (LLMs) for automated constructed language (conlang) generation, showcasing its ability to produce consistent and diverse languages without human expertise.


<details>
  <summary>Details</summary>
Motivation: To explore how modern foundation models (i.e., LLMs) can assist in creative linguistic tasks, specifically in creating constructed languages, enhancing computational creativity in linguistics.

Method: The paper proposes a multi-hop pipeline called ConlangCrafter, breaking down language design into modular stages (e.g., phonology, morphology, syntax) and leveraging LLM capabilities for randomness and self-refinement.

Result: ConlangCrafter produces coherent, typologically diverse constructed languages, evaluated through specific metrics, proving effective even without human linguistic intervention.

Conclusion: LLMs exhibit potential as powerful computational aids for creative linguistic tasks, providing a scalable methodology for conlang creation while ensuring both diversity and consistency.

Abstract: Constructed languages (conlangs) such as Esperanto and Quenya have played
diverse roles in art, philosophy, and international communication. Meanwhile,
large-scale foundation models have revolutionized creative generation in text,
images, and beyond. In this work, we leverage modern LLMs as computational
creativity aids for end-to-end conlang creation. We introduce ConlangCrafter, a
multi-hop pipeline that decomposes language design into modular stages --
phonology, morphology, syntax, lexicon generation, and translation. At each
stage, our method leverages LLMs' meta-linguistic reasoning capabilities,
injecting randomness to encourage diversity and leveraging self-refinement
feedback to encourage consistency in the emerging language description. We
evaluate ConlangCrafter on metrics measuring coherence and typological
diversity, demonstrating its ability to produce coherent and varied conlangs
without human linguistic expertise.

</details>


### [47] [Few-Shot Prompting for Extractive Quranic QA with Instruction-Tuned LLMs](https://arxiv.org/abs/2508.06103)
*Mohamed Basem,Islam Oshallah,Ali Hamdi,Ammar Mohammed*

Main category: cs.CL

TL;DR: The paper proposes two approaches—few-shot prompting with instruction-tuned large language models and a post-processing system—for Extractive QA on the Quran. It includes an Arabic prompt framework and shows improved performance compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: To address challenges in Extractive QA on the Quran, including its complex language, unique terminology, and deep semantic meanings.

Method: The approach involves few-shot prompting with instruction-tuned large language models (like Gemini and DeepSeek), along with an Arabic prompt framework and a post-processing system for span extraction.

Result: Evaluations indicate that the instruction-tuned large language models surpass traditional fine-tuned models, achieving a pAP10 score of 0.637.

Conclusion: Prompt-based instruction tuning is effective for Extractive QA in low-resource, semantically rich tasks, such as on the Quran.

Abstract: This paper presents two effective approaches for Extractive Question
Answering (QA) on the Quran. It addresses challenges related to complex
language, unique terminology, and deep meaning in the text. The second uses
few-shot prompting with instruction-tuned large language models such as Gemini
and DeepSeek. A specialized Arabic prompt framework is developed for span
extraction. A strong post-processing system integrates subword alignment,
overlap suppression, and semantic filtering. This improves precision and
reduces hallucinations. Evaluations show that large language models with Arabic
instructions outperform traditional fine-tuned models. The best configuration
achieves a pAP10 score of 0.637. The results confirm that prompt-based
instruction tuning is effective for low-resource, semantically rich QA tasks.

</details>


### [48] [You Don't Need Pre-built Graphs for RAG: Retrieval Augmented Generation with Adaptive Reasoning Structures](https://arxiv.org/abs/2508.06105)
*Shengyuan Chen,Chuang Zhou,Zheng Yuan,Qinggang Zhang,Zeyang Cui,Hao Chen,Yilin Xiao,Jiannong Cao,Xiao Huang*

Main category: cs.CL

TL;DR: LogicRAG offers a dynamic, logic-aware framework for Retrieval-Augmented Generation without relying on pre-built graphs, addressing the issues of token cost and inefficiency in adapting to query complexity.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing Graph-based Retrieval-Augmented Generation (GraphRAG) methods that are resource-intensive and lack adaptability for diverse query types.

Method: LogicRAG decomposes input queries into subproblems, constructs directed acyclic graphs (DAGs), uses topological sorting for logical progression, and applies pruning mechanisms to optimize retrieval and context management.

Result: LogicRAG demonstrates superior performance and efficiency over state-of-the-art RAG methods through extensive experimental validation.

Conclusion: LogicRAG provides an effective and resource-efficient solution for dynamic and logic-aware retrieval-augmented generation, eliminating reliance on pre-built graphs and enhancing reasoning capabilities.

Abstract: Large language models (LLMs) often suffer from hallucination, generating
factually incorrect statements when handling questions beyond their knowledge
and perception. Retrieval-augmented generation (RAG) addresses this by
retrieving query-relevant contexts from knowledge bases to support LLM
reasoning. Recent advances leverage pre-constructed graphs to capture the
relational connections among distributed documents, showing remarkable
performance in complex tasks. However, existing Graph-based RAG (GraphRAG)
methods rely on a costly process to transform the corpus into a graph,
introducing overwhelming token cost and update latency. Moreover, real-world
queries vary in type and complexity, requiring different logic structures for
accurate reasoning. The pre-built graph may not align with these required
structures, resulting in ineffective knowledge retrieval. To this end, we
propose a \textbf{\underline{Logic}}-aware
\textbf{\underline{R}}etrieval-\textbf{\underline{A}}ugmented
\textbf{\underline{G}}eneration framework (\textbf{LogicRAG}) that dynamically
extracts reasoning structures at inference time to guide adaptive retrieval
without any pre-built graph. LogicRAG begins by decomposing the input query
into a set of subproblems and constructing a directed acyclic graph (DAG) to
model the logical dependencies among them. To support coherent multi-step
reasoning, LogicRAG then linearizes the graph using topological sort, so that
subproblems can be addressed in a logically consistent order. Besides, LogicRAG
applies graph pruning to reduce redundant retrieval and uses context pruning to
filter irrelevant context, significantly reducing the overall token cost.
Extensive experiments demonstrate that LogicRAG achieves both superior
performance and efficiency compared to state-of-the-art baselines.

</details>


### [49] [AURA: Affordance-Understanding and Risk-aware Alignment Technique for Large Language Models](https://arxiv.org/abs/2508.06124)
*Sayantan Adak,Pratyush Chatterjee,Somnath Banerjee,Rima Hazra,Somak Aditya,Animesh Mukherjee*

Main category: cs.CL

TL;DR: The paper introduces AURA, a new framework using Process Reward Models (PRMs) to enhance logical coherence and safety in AI reasoning, outperforming existing safety measures.


<details>
  <summary>Details</summary>
Motivation: Current large language models (LLMs) struggle with affordance-based safety risks, where outputs can unintentionally lead to harmful actions due to overlooked logical implications.

Method: The authors propose a multi-layered framework (AURA) combining introspective self-critique, PRMs for step-by-step evaluations, and adaptive decoding to guide safer reasoning.

Result: Empirical evidence shows that AURA significantly improves logical integrity and affordance-sensitive safety compared to traditional methods.

Conclusion: AURA marks a meaningful advancement in creating safer, context-aware AI, setting a higher standard for alignment-sensitive applications.

Abstract: Present day LLMs face the challenge of managing affordance-based safety
risks-situations where outputs inadvertently facilitate harmful actions due to
overlooked logical implications. Traditional safety solutions, such as scalar
outcome-based reward models, parameter tuning, or heuristic decoding
strategies, lack the granularity and proactive nature needed to reliably detect
and intervene during subtle yet crucial reasoning steps. Addressing this
fundamental gap, we introduce AURA, an innovative, multi-layered framework
centered around Process Reward Models (PRMs), providing comprehensive, step
level evaluations across logical coherence and safety-awareness. Our framework
seamlessly combines introspective self-critique, fine-grained PRM assessments,
and adaptive safety-aware decoding to dynamically and proactively guide models
toward safer reasoning trajectories. Empirical evidence clearly demonstrates
that this approach significantly surpasses existing methods, significantly
improving the logical integrity and affordance-sensitive safety of model
outputs. This research represents a pivotal step toward safer, more
responsible, and contextually aware AI, setting a new benchmark for
alignment-sensitive applications.

</details>


### [50] [Less is More: Selective Reflection for Compatible and Efficient Knowledge Distillation in Large Language Models](https://arxiv.org/abs/2508.06135)
*Lingyuan Liu,Mengxiang Zhang*

Main category: cs.CL

TL;DR: Selective Reflection Distillation (SRD) improves the efficiency and performance of compressing large language models by refining training data and implementing curriculum scheduling.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of existing Knowledge Distillation methods by prioritizing training data quality and compatibility with student models.

Method: Propose a data curation framework (SRD) that evaluates and selects training data based on comparison between ground truth and student outputs, alongside a curriculum scheduling strategy for incremental training.

Result: SRD consistently improves distilled model performance across various model architectures, reduces KD training runtime by up to 39%, and enhances sample efficiency without altering core KD algorithms.

Conclusion: SRD highlights the importance of data quality and compatibility in Knowledge Distillation, providing a scalable framework to refine training processes and improve outcomes.

Abstract: Knowledge Distillation (KD) is a fundamental technique for compressing large
language models (LLMs) into compact, efficient student models. However,
existing white-box KD methods mainly focus on balancing ground truth and
student-generated responses while overlooking two critical factors: training
data quality and student-model compatibility. To address these limitations, we
propose Selective Reflection Distillation (SRD), a novel data curation
framework that leverages reflections from student models to systematically
refine training data. SRD dynamically evaluates and selects prompt-response
pairs by comparing ground truth data with student model outputs, selectively
curating high-quality, student-compatible training instances through automated
ranking based on difficulty. Furthermore, after selecting the training data, a
curriculum scheduling strategy is employed to incrementally introduce these
curated subsets into the distillation process at fixed intervals. As a
plug-and-play enhancement, SRD consistently improves distillation outcomes
across diverse white-box KD approaches and model architectures, as well as
decreases computational cost significantly during KD training. Experiments on a
range of language model benchmarks demonstrate SRD's consistent improvements in
distilled model performance, as well as a reduction in training runtime by up
to 39%, under diverse KD methods and model families. Notably, SRD operates as a
plug-and-play module, enhancing sample efficiency without modifying underlying
KD algorithms. Our findings highlight that data quality and compatibility are
pivotal to effective and efficient distillation of LLMs, and SRD provides a
principled framework to achieve both. This work advances the understanding of
data-centric factors in KD and offers practical insights for enhancing the
capability and efficiency of compressed LLMs.

</details>


### [51] [Scaling Personality Control in LLMs with Big Five Scaler Prompts](https://arxiv.org/abs/2508.06149)
*Gunhee Cho,Yun-Gyung Cheong*

Main category: cs.CL

TL;DR: Big5-Scaler is a framework using prompts to control language models' Big Five personality traits without extra training. It is tested on personality expression, dialogue generation, and imitation tasks.


<details>
  <summary>Details</summary>
Motivation: To create a method for easily controlling personality traits in large language models (LLMs) for better dialogue generation and human trait imitation.

Method: The framework embeds numeric personality values into prompts for fine-grained control without additional training. Various tasks are conducted to evaluate the model's effectiveness.

Result: Big5-Scaler successfully induces distinct personality traits, with performance dependent on prompt design and trait intensity.

Conclusion: Concise prompts and lower-intensity traits improve efficiency, providing a streamlined method for building personality-aware dialogue systems.

Abstract: We present Big5-Scaler, a prompt-based framework for conditioning large
language models (LLMs) with controllable Big Five personality traits. By
embedding numeric trait values into natural language prompts, our method
enables fine-grained personality control without additional training. We
evaluate Big5-Scaler across trait expression, dialogue generation, and human
trait imitation tasks. Results show that it induces consistent and
distinguishable personality traits across models, with performance varying by
prompt type and scale. Our analysis highlights the effectiveness of concise
prompts and lower trait intensities, providing a efficient approach for
building personality-aware dialogue agents.

</details>


### [52] [Semantic and Structural Analysis of Implicit Biases in Large Language Models: An Interpretable Approach](https://arxiv.org/abs/2508.06155)
*Renhan Zhang,Lian Lian,Zhen Qi,Guiran Liu*

Main category: cs.CL

TL;DR: The paper proposes an interpretable method to detect hidden biases in large language models using nested semantic representation and contextual contrast mechanisms.


<details>
  <summary>Details</summary>
Motivation: Implicit social biases in large language models can lead to undesirable outcomes and need reliable methods for their detection.

Method: Combines nested semantic representation with contextual contrast mechanisms to extract bias features and analyze sensitivity using attention weight perturbation.

Result: Achieved strong bias detection across multiple dimensions including gender, profession, religion, and race, with high semantic alignment and output stability.

Conclusion: The method provides a transparent technical foundation for bias detection, offering interpretability and trustworthiness suitable for practical applications.

Abstract: This paper addresses the issue of implicit stereotypes that may arise during
the generation process of large language models. It proposes an interpretable
bias detection method aimed at identifying hidden social biases in model
outputs, especially those semantic tendencies that are not easily captured
through explicit linguistic features. The method combines nested semantic
representation with a contextual contrast mechanism. It extracts latent bias
features from the vector space structure of model outputs. Using attention
weight perturbation, it analyzes the model's sensitivity to specific social
attribute terms, thereby revealing the semantic pathways through which bias is
formed. To validate the effectiveness of the method, this study uses the
StereoSet dataset, which covers multiple stereotype dimensions including
gender, profession, religion, and race. The evaluation focuses on several key
metrics, such as bias detection accuracy, semantic consistency, and contextual
sensitivity. Experimental results show that the proposed method achieves strong
detection performance across various dimensions. It can accurately identify
bias differences between semantically similar texts while maintaining high
semantic alignment and output stability. The method also demonstrates high
interpretability in its structural design. It helps uncover the internal bias
association mechanisms within language models. This provides a more transparent
and reliable technical foundation for bias detection. The approach is suitable
for real-world applications where high trustworthiness of generated content is
required.

</details>


### [53] [One Size Does Not Fit All: A Distribution-Aware Sparsification for More Precise Model Merging](https://arxiv.org/abs/2508.06163)
*Yingfeng Luo,Dingyang Lin,Junxin Wang,Ziqiang Xu,Kaiyan Chang,Tong Zheng,Bei Li,Anxiang Ma,Tong Xiao,Zhengtao Yu,Jingbo Zhu*

Main category: cs.CL

TL;DR: The paper introduces TADrop, an adaptive sparsification method tailored to model parameters' distributions, significantly boosting performance in model merging tasks.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current sparsification strategies in model merging that fail to account for parameter heterogeneity, leading to suboptimal performance.

Method: TADrop uses tensor-wise adaptive sparsification by assigning tailored sparsity levels to each parameter tensor based on its distributional properties.

Result: TADrop consistently improves performance in model merging methods, achieving significant performance gains, including an average of 2.0% across multiple vision tasks.

Conclusion: TADrop offers an effective, plug-and-play solution to enhance model merging by mitigating parameter interference, establishing a new baseline for performance.

Abstract: Model merging has emerged as a compelling data-free paradigm for multi-task
learning, enabling the fusion of multiple fine-tuned models into a single,
powerful entity. A key technique in merging methods is sparsification, which
prunes redundant parameters from task vectors to mitigate interference.
However, prevailing approaches employ a ``one-size-fits-all'' strategy,
applying a uniform sparsity ratio that overlooks the inherent structural and
statistical heterogeneity of model parameters. This often leads to a suboptimal
trade-off, where critical parameters are inadvertently pruned while less useful
ones are retained. To address this limitation, we introduce \textbf{TADrop}
(\textbf{T}ensor-wise \textbf{A}daptive \textbf{Drop}), an adaptive
sparsification strategy that respects this heterogeneity. Instead of a global
ratio, TADrop assigns a tailored sparsity level to each parameter tensor based
on its distributional properties. The core intuition is that tensors with
denser, more redundant distributions can be pruned aggressively, while sparser,
more critical ones are preserved. As a simple and plug-and-play module, we
validate TADrop by integrating it with foundational, classic, and SOTA merging
methods. Extensive experiments across diverse tasks (vision, language, and
multimodal) and models (ViT, BEiT) demonstrate that TADrop consistently and
significantly boosts their performance. For instance, when enhancing a leading
merging method, it achieves an average performance gain of 2.0\% across 8
ViT-B/32 tasks. TADrop provides a more effective way to mitigate parameter
interference by tailoring sparsification to the model's structure, offering a
new baseline for high-performance model merging.

</details>


### [54] [UR$^2$: Unify RAG and Reasoning through Reinforcement Learning](https://arxiv.org/abs/2508.06165)
*Weitao Li,Boran Xiang,Xiaolong Wang,Zhinan Gou,Weizhi Ma,Yang Liu*

Main category: cs.CL

TL;DR: This paper introduces UR2, a framework unifying Retrieval-Augmented Generation and Reinforcement Learning for reasoning and knowledge retrieval, showing superior results across multiple tasks.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current models where retrieval and reasoning capabilities are often developed in isolation, reducing generalizability and applicability to broader tasks.

Method: The proposed UR2 framework uses a difficulty-aware curriculum for selective retrieval and a hybrid knowledge access strategy, dynamically integrating retrieval and reasoning through reinforcement learning.

Result: UR2 outperforms existing methods in open-domain QA, medical, mathematical reasoning, and competes with GPT-4 variants in benchmarks.

Conclusion: The proposed framework efficiently unifies retrieval and reasoning, advancing adaptability in LLMs across diverse tasks. Code and models are publicly available.

Abstract: Large Language Models (LLMs) have shown remarkable capabilities through two
complementary paradigms: Retrieval-Augmented Generation (RAG), which enhances
knowledge grounding, and Reinforcement Learning from Verifiable Rewards (RLVR),
which optimizes complex reasoning abilities. However, these two capabilities
are often developed in isolation, and existing efforts to unify them remain
narrow in scope-typically limited to open-domain QA with fixed retrieval
settings and task-specific assumptions. This lack of integration constrains
generalization and limits the applicability of RAG-RL methods to broader
domains. To bridge this gap, we propose UR2 (Unified RAG and Reasoning), a
general framework that unifies retrieval and reasoning through reinforcement
learning. UR2 introduces two key contributions: a difficulty-aware curriculum
training that selectively invokes retrieval only for challenging problems, and
a hybrid knowledge access strategy combining domain-specific offline corpora
with LLM-generated summaries. These components are designed to enable dynamic
coordination between retrieval and reasoning, improving adaptability across a
diverse range of tasks. Experiments across open-domain QA, MMLU-Pro, medical,
and mathematical reasoning tasks demonstrate that UR2 (built on Qwen2.5-3/7B
and LLaMA-3.1-8B) significantly outperforms existing RAG and RL methods,
achieving comparable performance to GPT-4o-mini and GPT-4.1-mini on several
benchmarks. We have released all code, models, and data at
https://github.com/Tsinghua-dhy/UR2.

</details>


### [55] [Pragmatics beyond humans: meaning, communication, and LLMs](https://arxiv.org/abs/2508.06167)
*Vít Gvoždiak*

Main category: cs.CL

TL;DR: The paper suggests reconceptualizing pragmatics as a dynamic interface central to language use, focusing on how large language models (LLMs) influence traditional pragmatic theories and practices.


<details>
  <summary>Details</summary>
Motivation: The motivation is to refine the understanding of pragmatics to account for the communicative implications of large language models and overcome limitations of human-centric traditional models.

Method: The paper challenges semiotic hierarchies, contrasts existing pragmatic theories with LLM-based systems, criticizes biases in evaluation, and introduces new concepts like context frustration.

Result: It highlights tensions in adapting traditional pragmatics to machine-centered communication, proposes probabilistic approaches, and outlines the co-construction of pragmatic contexts in LLM interactions.

Conclusion: Pragmatic theory may require expansion or adjustment to accommodate and better understand communication involving generative AI.

Abstract: The paper reconceptualizes pragmatics not as a subordinate, third dimension
of meaning, but as a dynamic interface through which language operates as a
socially embedded tool for action. With the emergence of large language models
(LLMs) in communicative contexts, this understanding needs to be further
refined and methodologically reconsidered. The first section challenges the
traditional semiotic trichotomy, arguing that connectionist LLM architectures
destabilize established hierarchies of meaning, and proposes the Human-Machine
Communication (HMC) framework as a more suitable alternative. The second
section examines the tension between human-centred pragmatic theories and the
machine-centred nature of LLMs. While traditional, Gricean-inspired pragmatics
continue to dominate, it relies on human-specific assumptions ill-suited to
predictive systems like LLMs. Probabilistic pragmatics, particularly the
Rational Speech Act framework, offers a more compatible teleology by focusing
on optimization rather than truth-evaluation. The third section addresses the
issue of substitutionalism in three forms - generalizing, linguistic, and
communicative - highlighting the anthropomorphic biases that distort LLM
evaluation and obscure the role of human communicative subjects. Finally, the
paper introduces the concept of context frustration to describe the paradox of
increased contextual input paired with a collapse in contextual understanding,
emphasizing how users are compelled to co-construct pragmatic conditions both
for the model and themselves. These arguments suggest that pragmatic theory may
need to be adjusted or expanded to better account for communication involving
generative AI.

</details>


### [56] [Comparing Knowledge Injection Methods for LLMs in a Low-Resource Regime](https://arxiv.org/abs/2508.06178)
*Hugo Abonizio,Thales Almeida,Roberto Lotufo,Rodrigo Nogueira*

Main category: cs.CL

TL;DR: This paper explores methods to inject small amounts of new knowledge into large language models (LLMs) while minimizing catastrophic forgetting and retaining existing capabilities.


<details>
  <summary>Details</summary>
Motivation: Updating large language models with small datasets (thousands or millions of tokens) is challenging due to factors like catastrophic forgetting, and the field lacks efficient techniques for knowledge injection with limited data.

Method: The authors used news data outside the model's training set to test knowledge acquisition, employed various synthetic data augmentation techniques, and evaluated the balance between learning new facts and retaining old ones through controlled experiments.

Result: Diverse prompting and synthetic data generation significantly improved knowledge acquisition, while RAG-based methods harmed performance on control datasets. Catastrophic forgetting remains a key challenge.

Conclusion: Efficient knowledge injection is achievable with small datasets by leveraging diverse textual variations and synthetic data. Self-generated synthetic training data shows potential as a pathway toward self-improving LLMs.

Abstract: Large language models (LLMs) often require vast amounts of text to
effectively acquire new knowledge. While continuing pre-training on large
corpora or employing retrieval-augmented generation (RAG) has proven
successful, updating an LLM with only a few thousand or million tokens remains
challenging. In this work, we investigate the task of injecting small,
unstructured information into LLMs and its relation to the catastrophic
forgetting phenomenon. We use a dataset of recent news -- ensuring no overlap
with the model's pre-training data -- to evaluate the knowledge acquisition by
probing the model with question-answer pairs related the learned information.
Starting from a continued pre-training baseline, we explored different
augmentation algorithms to generate synthetic data to improve the knowledge
acquisition capabilities. Our experiments show that simply continuing
pre-training on limited data yields modest improvements, whereas exposing the
model to diverse textual variations significantly improves the learning of new
facts -- particularly with methods that induce greater variability through
diverse prompting. Furthermore, we shed light on the forgetting phenomenon in
small-data regimes, illustrating the delicate balance between learning new
content and retaining existing capabilities. We also confirm the sensitivity of
RAG-based approaches for knowledge injection, which often lead to greater
degradation on control datasets compared to parametric methods. Finally, we
demonstrate that models can generate effective synthetic training data
themselves, suggesting a pathway toward self-improving model updates. All code
and generated data used in our experiments are publicly available, providing a
resource for studying efficient knowledge injection in LLMs with limited data
at https://github.com/hugoabonizio/knowledge-injection-methods.

</details>


### [57] [DKG-LLM : A Framework for Medical Diagnosis and Personalized Treatment Recommendations via Dynamic Knowledge Graph and Large Language Model Integration](https://arxiv.org/abs/2508.06186)
*Ali Sarabadani,Maryam Abdollahi Shamami,Hamidreza Sadeghsalehi,Borhan Asadi,Saba Hesaraki*

Main category: cs.CL

TL;DR: The paper introduces the DKG-LLM framework that combines dynamic knowledge graphs with large language models for medical diagnosis and personalized treatment recommendations, achieving high levels of diagnostic and recommendation accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to leverage the transformative potential of large language models by incorporating dynamic knowledge graphs to enhance medical diagnosis and personalized treatment recommendations.

Method: The study integrates a dynamic knowledge graph (DKG) with the Grok 3 large language model and employs the Adaptive Semantic Fusion Algorithm (ASFA) to process and dynamically update medical data, achieving scalable graph construction and accurate diagnostic modeling.

Result: Evaluation using real-world datasets like MIMIC-III and PubMed demonstrated that DKG-LLM achieves a diagnostic accuracy of 84.19%, a treatment recommendation accuracy of 89.63%, and a semantic coverage of 93.48%.

Conclusion: DKG-LLM is presented as a reliable tool for handling noisy medical data and complex diseases, providing transformative insights and personalization in medical domains.

Abstract: Large Language Models (LLMs) have grown exponentially since the release of
ChatGPT. These models have gained attention due to their robust performance on
various tasks, including language processing tasks. These models achieve
understanding and comprehension of tasks by training billions of parameters.
The development of these models is a transformative force in enhancing natural
language understanding and has taken a significant step towards artificial
general intelligence (AGI). In this study, we aim to present the DKG-LLM
framework. The DKG-LLM framework introduces a groundbreaking approach to
medical diagnosis and personalized treatment recommendations by integrating a
dynamic knowledge graph (DKG) with the Grok 3 large language model. Using the
Adaptive Semantic Fusion Algorithm (ASFA), heterogeneous medical data
(including clinical reports and PubMed articles) and patient records
dynamically generate a knowledge graph consisting of 15,964 nodes in 13
distinct types (e.g., diseases, symptoms, treatments, patient profiles) and
127,392 edges in 26 relationship types (e.g., causal, therapeutic,
association). ASFA utilizes advanced probabilistic models, Bayesian inference,
and graph optimization to extract semantic information, dynamically updating
the graph with approximately 150 new nodes and edges in each data category
while maintaining scalability with up to 987,654 edges. Real-world datasets,
including MIMIC-III and PubMed, were utilized to evaluate the proposed
architecture. The evaluation results show that DKG-LLM achieves a diagnostic
accuracy of 84.19%. The model also has a treatment recommendation accuracy of
89.63% and a semantic coverage of 93.48%. DKG-LLM is a reliable and
transformative tool that handles noisy data and complex multi-symptom diseases,
along with feedback-based learning from physician input.

</details>


### [58] [Beyond Uniform Criteria: Scenario-Adaptive Multi-Dimensional Jailbreak Evaluation](https://arxiv.org/abs/2508.06194)
*Lai Jiang,Yuekang Li,Xiaohan Zhang,Youtao Ding,Li Pan*

Main category: cs.CL

TL;DR: SceneJailEval introduces a scenario-adaptive multi-dimensional framework for precise jailbreak evaluation, significantly outperforming past methods with a remarkable F1 score of 0.917 and a specialized dataset.


<details>
  <summary>Details</summary>
Motivation: Current jailbreak evaluations lack precision due to binary-only methods and uniform criteria applied across diverse scenarios, necessitating a more adaptive approach.

Method: The paper presents a scenario-adaptive multi-dimensional evaluation framework and develops a comprehensive dataset covering 14 scenarios with varied jailbreak cases.

Result: SceneJailEval achieved state-of-the-art performance with an F1 score of 0.917 on a diverse dataset and 0.995 on JBB, improving by up to 6% over prior methods.

Conclusion: SceneJailEval's adaptable framework and dataset overcome critical limitations in existing methods, offering precise and extensible evaluation across heterogeneous scenarios.

Abstract: Precise jailbreak evaluation is vital for LLM red teaming and jailbreak
research. Current approaches employ binary classification ( e.g., string
matching, toxic text classifiers, LLM-driven methods), yielding only "yes/no"
labels without quantifying harm intensity. Existing multi-dimensional
frameworks ( e.g., Security Violation, Relative Truthfulness, Informativeness)
apply uniform evaluation criteria across scenarios, resulting in
scenario-specific mismatches--for instance, "Relative Truthfulness" is
irrelevant to "hate speech"--which compromise evaluation precision. To tackle
these limitations, we introduce SceneJailEval, with key contributions: (1) A
groundbreaking scenario-adaptive multi-dimensional framework for jailbreak
evaluation, overcoming the critical "one-size-fits-all" constraint of existing
multi-dimensional methods, and featuring strong extensibility to flexibly adapt
to customized or emerging scenarios. (2) A comprehensive 14-scenario dataset
with diverse jailbreak variants and regional cases, filling the long-standing
gap in high-quality, holistic benchmarks for scenario-adaptive evaluation. (3)
SceneJailEval achieves state-of-the-art results, with an F1 score of 0.917 on
our full-scenario dataset (+6% over prior SOTA) and 0.995 on JBB (+3% over
prior SOTA), surpassing accuracy limits of existing evaluation methods in
heterogeneous scenarios and confirming its advantage.

</details>


### [59] [EICAP: Deep Dive in Assessment and Enhancement of Large Language Models in Emotional Intelligence through Multi-Turn Conversations](https://arxiv.org/abs/2508.06196)
*Nizi Nazar,Ehsaneddin Asgari*

Main category: cs.CL

TL;DR: The paper introduces a taxonomy for Emotional Intelligence (EI) in large language models (LLMs) and evaluates six LLMs using a novel benchmark called EmoCap-Bench, finding limitations in current pretraining for emotional reasoning.


<details>
  <summary>Details</summary>
Motivation: To address the gap in equipping LLMs with Emotional Intelligence (EI) capabilities for human alignment.

Method: The authors propose a four-layer taxonomy for EI and design the multi-turn benchmark EICAP-Bench to test LLMs, followed by fine-tuning selected LLMs using LoRA adapters on UltraChat in multiple languages.

Result: They found significant improvement in the 'Appraisal' layer of EI through fine-tuning but observed general limitations in current pretraining and instruction-tuning methods.

Conclusion: Comprehensive EI alignment in LLMs requires more specialized data and modeling approaches as current paradigms are insufficient.

Abstract: Emotional Intelligence (EI) is a critical yet underexplored dimension in the
development of human-aligned LLMs. To address this gap, we introduce a unified,
psychologically grounded four-layer taxonomy of EI tailored for large language
models (LLMs), encompassing emotional tracking, cause inference, appraisal, and
emotionally appropriate response generation. Building on this framework, we
present EICAP-Bench, a novel MCQ style multi-turn benchmark designed to
evaluate EI capabilities in open-source LLMs across diverse linguistic and
cultural contexts. We evaluate six LLMs: LLaMA3 (8B), LLaMA3-Instruct, Gemma
(9B), Gemma-Instruct, Qwen2.5 (7B), and Qwen2.5-Instruct on EmoCap-Bench,
identifying Qwen2.5-Instruct as the strongest baseline. To assess the potential
for enhancing EI capabilities, we fine-tune both Qwen2.5-Base and
Qwen2.5-Instruct using LoRA adapters on UltraChat (UC), a large-scale,
instruction-tuned dialogue dataset, in both English and Arabic. Our statistical
analysis reveals that among the five EI layers, only the Appraisal layer shows
significant improvement through UC-based fine-tuning. These findings highlight
the limitations of existing pretraining and instruction-tuning paradigms in
equipping LLMs with deeper emotional reasoning and underscore the need for
targeted data and modeling strategies for comprehensive EI alignment.

</details>


### [60] [Classification is a RAG problem: A case study on hate speech detection](https://arxiv.org/abs/2508.06204)
*Richard Willats,Josh Pennington,Aravind Mohan,Bertie Vidgen*

Main category: cs.CL

TL;DR: This paper introduces a novel approach using Retrieval-Augmented Generation (RAG) for dynamic, explainable, and adaptable content moderation systems.


<details>
  <summary>Details</summary>
Motivation: Traditional classification systems require costly retraining to adapt to policy changes, presenting limitations in dynamic content moderation.

Method: The authors propose the Contextual Policy Engine (CPE), a RAG-based system, which retrieves contextual knowledge at inference to classify content dynamically according to updated policies.

Result: CPE achieves comparable accuracy to leading systems, provides inherent explainability, and dynamically adapts to policy updates without retraining.

Conclusion: RAG-based systems like CPE enhance flexibility, transparency, and adaptation for content moderation tasks, addressing critical limitations of conventional classification systems.

Abstract: Robust content moderation requires classification systems that can quickly
adapt to evolving policies without costly retraining. We present classification
using Retrieval-Augmented Generation (RAG), which shifts traditional
classification tasks from determining the correct category in accordance with
pre-trained parameters to evaluating content in relation to contextual
knowledge retrieved at inference. In hate speech detection, this transforms the
task from "is this hate speech?" to "does this violate the hate speech policy?"
  Our Contextual Policy Engine (CPE) - an agentic RAG system - demonstrates
this approach and offers three key advantages: (1) robust classification
accuracy comparable to leading commercial systems, (2) inherent explainability
via retrieved policy segments, and (3) dynamic policy updates without model
retraining. Through three experiments, we demonstrate strong baseline
performance and show that the system can apply fine-grained policy control by
correctly adjusting protection for specific identity groups without requiring
retraining or compromising overall performance. These findings establish that
RAG can transform classification into a more flexible, transparent, and
adaptable process for content moderation and wider classification problems.

</details>


### [61] [InfoCausalQA:Can Models Perform Non-explicit Causal Reasoning Based on Infographic?](https://arxiv.org/abs/2508.06220)
*Keummin Ka,Junhyeong Park,Jahyun Jeon,Youngjae Yu*

Main category: cs.CL

TL;DR: The paper introduces InfoCausalQA, a benchmark for evaluating causal reasoning in Vision-Language Models using infographics and textual context. Current models perform poorly on this task compared to humans.


<details>
  <summary>Details</summary>
Motivation: To address the lack of comprehensive evaluation benchmarks for causal reasoning in Vision-Language Models, particularly in multimodal scenarios involving infographics.

Method: The authors manually collected 494 infographic-text pairs from public sources and used GPT-4 to generate 1,482 multiple-choice QA pairs, which were human-revised to ensure genuine causal reasoning. Two tasks focusing on quantitative and semantic causal reasoning were designed.

Result: Experimental results show that existing Vision-Language Models perform poorly in computational and semantic causal reasoning tasks, falling significantly behind human performance.

Conclusion: There is a substantial gap in the ability of current multimodal AI systems to perform causal inference based on infographic data. Developing models with stronger causal reasoning capabilities is essential.

Abstract: Recent advances in Vision-Language Models (VLMs) have demonstrated impressive
capabilities in perception and reasoning. However, the ability to perform
causal inference -- a core aspect of human cognition -- remains underexplored,
particularly in multimodal settings. In this study, we introduce InfoCausalQA,
a novel benchmark designed to evaluate causal reasoning grounded in
infographics that combine structured visual data with textual context. The
benchmark comprises two tasks: Task 1 focuses on quantitative causal reasoning
based on inferred numerical trends, while Task 2 targets semantic causal
reasoning involving five types of causal relations: cause, effect,
intervention, counterfactual, and temporal. We manually collected 494
infographic-text pairs from four public sources and used GPT-4o to generate
1,482 high-quality multiple-choice QA pairs. These questions were then
carefully revised by humans to ensure they cannot be answered based on
surface-level cues alone but instead require genuine visual grounding. Our
experimental results reveal that current VLMs exhibit limited capability in
computational reasoning and even more pronounced limitations in semantic causal
reasoning. Their significantly lower performance compared to humans indicates a
substantial gap in leveraging infographic-based information for causal
inference. Through InfoCausalQA, we highlight the need for advancing the causal
reasoning abilities of multimodal AI systems.

</details>


### [62] [Large Language Model Data Generation for Enhanced Intent Recognition in German Speech](https://arxiv.org/abs/2508.06277)
*Theresa Pekarek Rosin,Burak Can Kaplan,Stefan Wermter*

Main category: cs.CL

TL;DR: This paper develops a method to improve intent recognition for elderly German speech using adapted ASR and synthetic LLM-generated data.


<details>
  <summary>Details</summary>
Motivation: Most existing intent recognition models focus on short commands and English language, leaving a gap for elderly German speakers.

Method: Combines a fine-tuned Whisper ASR model on elderly German speech with Transformer-based models trained on synthetic datasets from LLMs like LeoLM, Llama3, and ChatGPT, with evaluations through cross-dataset tests.

Result: Synthetic LLM-generated data enhances intent recognition performance and robustness, with the domain-specific LeoLM outperforming ChatGPT for German intent recognition.

Conclusion: Generative AI is effective in addressing data scarcity in low-resource domains, and the study provides detailed procedural transparency.

Abstract: Intent recognition (IR) for speech commands is essential for artificial
intelligence (AI) assistant systems; however, most existing approaches are
limited to short commands and are predominantly developed for English. This
paper addresses these limitations by focusing on IR from speech by elderly
German speakers. We propose a novel approach that combines an adapted Whisper
ASR model, fine-tuned on elderly German speech (SVC-de), with Transformer-based
language models trained on synthetic text datasets generated by three
well-known large language models (LLMs): LeoLM, Llama3, and ChatGPT. To
evaluate the robustness of our approach, we generate synthetic speech with a
text-to-speech model and conduct extensive cross-dataset testing. Our results
show that synthetic LLM-generated data significantly boosts classification
performance and robustness to different speaking styles and unseen vocabulary.
Notably, we find that LeoLM, a smaller, domain-specific 13B LLM, surpasses the
much larger ChatGPT (175B) in dataset quality for German intent recognition.
Our approach demonstrates that generative AI can effectively bridge data gaps
in low-resource domains. We provide detailed documentation of our data
generation and training process to ensure transparency and reproducibility.

</details>


### [63] [Matrix-Driven Instant Review: Confident Detection and Reconstruction of LLM Plagiarism on PC](https://arxiv.org/abs/2508.06309)
*Ruichong Zhang*

Main category: cs.CL

TL;DR: This paper introduces MDIR, a method for detecting plagiarism in large language model weights, capable of identifying copying even after transformations, with efficient processing.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the rise in intellectual property concerns over large language models, particularly the increasing instances of plagiarism such as weight copying, unauthorized adaptations, or retraining.

Method: The proposed method, MDIR, utilizes matrix analysis and Large Deviation Theory to detect weight relationships, computes $p$-values for statistical significance, and focuses strictly on weight similarity, avoiding full model inference.

Result: Experimental tests reveal that MDIR effectively detects plagiarism, even under extensive model transformations, and achieves this efficiently on a single PC within an hour.

Conclusion: MDIR offers a rigorous, reliable, and accessible tool for addressing intellectual property concerns in large language models by accurately identifying weight-based plagiarism.

Abstract: In recent years, concerns about intellectual property (IP) in large language
models (LLMs) have grown significantly. Plagiarizing other LLMs (through direct
weight copying, upcycling, pruning, or continual pretraining) and claiming
authorship without properly attributing to the original license, is a serious
misconduct that can lead to significant financial and reputational harm to the
original developers. However, existing methods for detecting LLM plagiarism
fall short in key areas. They fail to accurately reconstruct weight
correspondences, lack the ability to compute statistical significance measures
such as $p$-values, and may mistakenly flag models trained on similar data as
being related. To address these limitations, we propose Matrix-Driven Instant
Review (MDIR), a novel method that leverages matrix analysis and Large
Deviation Theory. MDIR achieves accurate reconstruction of weight
relationships, provides rigorous $p$-value estimation, and focuses exclusively
on weight similarity without requiring full model inference. Experimental
results demonstrate that MDIR reliably detects plagiarism even after extensive
transformations, such as random permutations and continual pretraining with
trillions of tokens. Moreover, all detections can be performed on a single PC
within an hour, making MDIR both efficient and accessible.

</details>


### [64] [Harnessing Adaptive Topology Representations for Zero-Shot Graph Question Answering](https://arxiv.org/abs/2508.06345)
*Yanbin Wei,Jiangyue Yan,Chun Kang,Yang Chen,Hua Liu,James T. Kwok,Yu Zhang*

Main category: cs.CL

TL;DR: This paper introduces DynamicTRF, a framework for improving zero-shot graph QA in Large Multimodal Models by adaptively selecting tailored graph representations based on their efficiency.


<details>
  <summary>Details</summary>
Motivation: To tackle the limitations of existing approaches in zero-shot graph QA that rely on single, generic graph representation styles, often leading to inaccuracies or verbose responses.

Method: DynamicTRF analyzes existing graph representation forms, designs multiple tailored representations ($F_{ZS}$), introduces a Graph Response Efficiency (GRE) metric, creates a preference dataset (TRFP), and trains a routing model to dynamically select the most suited representations for each question.

Result: Extensive experiments on 7 in-domain and 2 out-of-domain tasks demonstrate significant improvements in graph QA accuracy and conciseness using DynamicTRF.

Conclusion: The DynamicTRF framework effectively enhances zero-shot graph QA performance by adapting graph representations to question-specific preferences, ensuring better accuracy and response efficiency.

Abstract: Large Multimodal Models (LMMs) have shown generalized zero-shot capabilities
in diverse domain question-answering (QA) tasks, including graph QA that
involves complex graph topologies. However, most current approaches use only a
single type of graph representation, namely Topology Representation Form (TRF),
such as prompt-unified text descriptions or style-fixed visual styles. Those
"one-size-fits-all" approaches fail to consider the specific preferences of
different models or tasks, often leading to incorrect or overly long responses.
To address this, we first analyze the characteristics and weaknesses of
existing TRFs, and then design a set of TRFs, denoted by $F_{ZS}$, tailored to
zero-shot graph QA. We then introduce a new metric, Graph Response Efficiency
(GRE), which measures the balance between the performance and the brevity in
graph QA. Built on these, we develop the DynamicTRF framework, which aims to
improve both the accuracy and conciseness of graph QA. To be specific,
DynamicTRF first creates a TRF Preference (TRFP) dataset that ranks TRFs based
on their GRE scores, to probe the question-specific TRF preferences. Then it
trains a TRF router on the TRFP dataset, to adaptively assign the best TRF from
$F_{ZS}$ for each question during the inference. Extensive experiments across 7
in-domain algorithmic graph QA tasks and 2 out-of-domain downstream tasks show
that DynamicTRF significantly enhances the zero-shot graph QA of LMMs in terms
of accuracy

</details>


### [65] [Cyberbullying Detection via Aggression-Enhanced Prompting](https://arxiv.org/abs/2508.06360)
*Aisha Saeid,Anu Sabu,Girish A. Koushik,Ferrante Neri,Diptesh Kanojia*

Main category: cs.CL

TL;DR: The study explores enhancing large language models (LLMs) for cyberbullying detection by integrating aggression detection as an auxiliary task using various strategies, ultimately finding enriched prompt pipeline methods as effective solutions.


<details>
  <summary>Details</summary>
Motivation: Cyberbullying detection is a challenging problem due to its subtle and varied expressions. There is a necessity to improve the generalisation capabilities and performance of LLMs when applied to safety-related tasks such as cyberbullying detection.

Method: The study used five aggression datasets and one cyberbullying dataset with instruction-tuned LLMs to test strategies like zero-shot, few-shot, LoRA fine-tuning, multi-task learning, and enriched prompt pipeline approaches.

Result: The enriched prompt pipeline approach, which embeds aggression predictions into cyberbullying detection prompts, consistently performed better than standard LoRA fine-tuning in experiments, enhancing model context and detection accuracy.

Conclusion: Incorporating aggression detection as an auxiliary task through enriched prompts significantly improves cyberbullying detection, highlighting the value of contextual augmentation in safety-critical applications.

Abstract: Detecting cyberbullying on social media remains a critical challenge due to
its subtle and varied expressions. This study investigates whether integrating
aggression detection as an auxiliary task within a unified training framework
can enhance the generalisation and performance of large language models (LLMs)
in cyberbullying detection. Experiments are conducted on five aggression
datasets and one cyberbullying dataset using instruction-tuned LLMs. We
evaluated multiple strategies: zero-shot, few-shot, independent LoRA
fine-tuning, and multi-task learning (MTL). Given the inconsistent results of
MTL, we propose an enriched prompt pipeline approach in which aggression
predictions are embedded into cyberbullying detection prompts to provide
contextual augmentation. Preliminary results show that the enriched prompt
pipeline consistently outperforms standard LoRA fine-tuning, indicating that
aggression-informed context significantly boosts cyberbullying detection. This
study highlights the potential of auxiliary tasks, such as aggression
detection, to improve the generalisation of LLMs for safety-critical
applications on social networks.

</details>


### [66] [Evaluating Style-Personalized Text Generation: Challenges and Directions](https://arxiv.org/abs/2508.06374)
*Anubhav Jangra,Bahareh Sarrafzadeh,Adrian de Wynter,Silviu Cucerzan,Sujay Kumar Jauhar*

Main category: cs.CL

TL;DR: This paper critiques the effectiveness of traditional metrics like BLEU and ROUGE for evaluating style-personalized text generation and studies alternative evaluation paradigms.


<details>
  <summary>Details</summary>
Motivation: Most current evaluation methods in style-personalized text generation overly rely on traditional metrics, which may not be suited for nuanced evaluation, especially in low-resource settings.

Method: The research examines various evaluation tools such as style embeddings and LLM-as-judge and assesses them using a style discrimination benchmark across multiple writing tasks and settings.

Result: The study establishes through experiments that an ensemble of diverse evaluation metrics provides a more effective framework for assessing style-personalized text generation.

Conclusion: For comprehensive evaluation, the adoption of diverse, ensemble-based metrics is recommended over traditional metrics when evaluating personalized text generation.

Abstract: While prior research has built tools and benchmarks towards style
personalized text generation, there has been limited exploration of evaluation
in low-resource author style personalized text generation space. Through this
work, we question the effectiveness of the widely adopted evaluation metrics
like BLEU and ROUGE, and explore other evaluation paradigms such as style
embeddings and LLM-as-judge to holistically evaluate the style personalized
text generation task. We evaluate these metrics and their ensembles using our
style discrimination benchmark, that spans eight writing tasks, and evaluates
across three settings, domain discrimination, authorship attribution, and LLM
personalized vs non-personalized discrimination. We provide conclusive evidence
to adopt ensemble of diverse evaluation metrics to effectively evaluate style
personalized text generation.

</details>


### [67] [LLMs vs. Chinese Anime Enthusiasts: A Comparative Study on Emotionally Supportive Role-Playing](https://arxiv.org/abs/2508.06388)
*Lanlan Qiu,Xiao Pu,Yeqi Feng,Tianxing He*

Main category: cs.CL

TL;DR: This paper focuses on combining role-playing and emotional support in Large Language Models (LLMs), introducing ChatAnime, the first Emotionally Supportive Role-Playing (ESRP) dataset leveraging anime characters.


<details>
  <summary>Details</summary>
Motivation: The need to bridge the gap in enabling emotionally supportive conversations with role-playing virtual characters.

Method: The authors selected 20 anime characters and 60 emotion-driven scenarios, collaborated with 40 Chinese anime experts, and developed a dataset with responses from humans and 10 LLMs, evaluated using a user-oriented system with 9 metrics.

Result: LLMs outperformed human experts in role-playing and emotional support but fell short on response diversity.

Conclusion: ChatAnime offers valuable insights and resources to enhance LLMs for ESRP, pushing forward the integration of role-playing and emotional support capabilities.

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in
role-playing conversations and providing emotional support as separate research
directions. However, there remains a significant research gap in combining
these capabilities to enable emotionally supportive interactions with virtual
characters. To address this research gap, we focus on anime characters as a
case study because of their well-defined personalities and large fan bases.
This choice enables us to effectively evaluate how well LLMs can provide
emotional support while maintaining specific character traits. We introduce
ChatAnime, the first Emotionally Supportive Role-Playing (ESRP) dataset. We
first thoughtfully select 20 top-tier characters from popular anime communities
and design 60 emotion-centric real-world scenario questions. Then, we execute a
nationwide selection process to identify 40 Chinese anime enthusiasts with
profound knowledge of specific characters and extensive experience in
role-playing. Next, we systematically collect two rounds of dialogue data from
10 LLMs and these 40 Chinese anime enthusiasts. To evaluate the ESRP
performance of LLMs, we design a user experience-oriented evaluation system
featuring 9 fine-grained metrics across three dimensions: basic dialogue,
role-playing and emotional support, along with an overall metric for response
diversity. In total, the dataset comprises 2,400 human-written and 24,000
LLM-generated answers, supported by over 132,000 human annotations.
Experimental results show that top-performing LLMs surpass human fans in
role-playing and emotional support, while humans still lead in response
diversity. We hope this work can provide valuable resources and insights for
future research on optimizing LLMs in ESRP. Our datasets are available at
https://github.com/LanlanQiu/ChatAnime.

</details>


### [68] [Quantifying Conversation Drift in MCP via Latent Polytope](https://arxiv.org/abs/2508.06418)
*Haoran Shi,Hongwei Yao,Shuo Shao,Shaopeng Jiao,Ziqi Peng,Zhan Qin,Cong Wang*

Main category: cs.CL

TL;DR: The paper proposes SecMCP, a secure framework to address critical security risks in the Model Context Protocol (MCP), enhancing the detection and mitigation of threats like conversation hijacking and data exfiltration through latent polytope analysis.


<details>
  <summary>Details</summary>
Motivation: The paper identifies security vulnerabilities in the MCP framework, which integrates external tools into LLMs, making them susceptible to adversarial attacks like tool poisoning and indirect prompt injection.

Method: SecMCP uses a latent polytope-based approach to model LLM activation vectors in latent space, detecting conversational drift caused by adversarial content.

Result: SecMCP achieved robust detection performance with AUROC scores over 0.915 across various datasets and LLMs, demonstrating effective identification of security threats without compromising usability.

Conclusion: SecMCP enhances the security of MCP frameworks by enabling proactive detection of adversarial disruptions through latent space trajectory analysis, systematically addressing MCP-related threats.

Abstract: The Model Context Protocol (MCP) enhances large language models (LLMs) by
integrating external tools, enabling dynamic aggregation of real-time data to
improve task execution. However, its non-isolated execution context introduces
critical security and privacy risks. In particular, adversarially crafted
content can induce tool poisoning or indirect prompt injection, leading to
conversation hijacking, misinformation propagation, or data exfiltration.
Existing defenses, such as rule-based filters or LLM-driven detection, remain
inadequate due to their reliance on static signatures, computational
inefficiency, and inability to quantify conversational hijacking. To address
these limitations, we propose SecMCP, a secure framework that detects and
quantifies conversation drift, deviations in latent space trajectories induced
by adversarial external knowledge. By modeling LLM activation vectors within a
latent polytope space, SecMCP identifies anomalous shifts in conversational
dynamics, enabling proactive detection of hijacking, misleading, and data
exfiltration. We evaluate SecMCP on three state-of-the-art LLMs (Llama3,
Vicuna, Mistral) across benchmark datasets (MS MARCO, HotpotQA, FinQA),
demonstrating robust detection with AUROC scores exceeding 0.915 while
maintaining system usability. Our contributions include a systematic
categorization of MCP security threats, a novel latent polytope-based
methodology for quantifying conversation drift, and empirical validation of
SecMCP's efficacy.

</details>


### [69] [Memp: Exploring Agent Procedural Memory](https://arxiv.org/abs/2508.06433)
*Runnan Fang,Yuan Liang,Xiaobin Wang,Jialong Wu,Shuofei Qiao,Pengjun Xie,Fei Huang,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: This paper introduces "Memp," a system that equips large language model-based agents with lifelong procedural memory through strategies for building, retrieving, and updating such memory.


<details>
  <summary>Details</summary>
Motivation: LLMs-based agents lack robust procedural memory, which limits their adaptability and long-term effectiveness in tasks.

Method: The authors propose a procedural memory system called "Memp" that condenses agent trajectories into instructions and abstractions, updating continuously with new experiences.

Result: Empirical evaluations show improved success rates and task efficiency for agents equipped with refined procedural memory, and memory transfer boosts weaker models' performance.

Conclusion: Procedural memory enhances both immediate task execution and adaptability over time, showing promise in augmenting different types of models.

Abstract: Large Language Models (LLMs) based agents excel at diverse tasks, yet they
suffer from brittle procedural memory that is manually engineered or entangled
in static parameters. In this work, we investigate strategies to endow agents
with a learnable, updatable, and lifelong procedural memory. We propose Memp
that distills past agent trajectories into both fine-grained, step-by-step
instructions and higher-level, script-like abstractions, and explore the impact
of different strategies for Build, Retrieval, and Update of procedural memory.
Coupled with a dynamic regimen that continuously updates, corrects, and
deprecates its contents, this repository evolves in lockstep with new
experience. Empirical evaluation on TravelPlanner and ALFWorld shows that as
the memory repository is refined, agents achieve steadily higher success rates
and greater efficiency on analogous tasks. Moreover, procedural memory built
from a stronger model retains its value: migrating the procedural memory to a
weaker model yields substantial performance gains.

</details>


### [70] [Learning the Topic, Not the Language: How LLMs Classify Online Immigration Discourse Across Languages](https://arxiv.org/abs/2508.06435)
*Andrea Nasuto,Stefano Maria Iacus,Francisco Rowe,Devika Jain*

Main category: cs.CL

TL;DR: The paper explores how fine-tuned LLaMA models can classify immigration-related tweets across languages, finding that even minimal multilingual fine-tuning delivers reliable cross-lingual topic detection and reduced pre-training biases.


<details>
  <summary>Details</summary>
Motivation: The authors aim to enhance cross-lingual topic detection capabilities of large language models (LLMs) while addressing pre-training biases in multilingual contexts.

Method: Fine-tuned lightweight LLaMA models (3.2-3B) on monolingual, bilingual, and multilingual datasets to classify tweets in 13 different languages and observe cross-lingual transferability and bias correction.

Result: Fine-tuning in one or two languages reliably detects immigration topics in unseen languages, while stance detection requires multilingual fine-tuning. Minimal exposure to under-represented languages significantly corrects structural biases.

Conclusion: Cross-lingual generalization does not require extensive multilingual training; lightweight interventions like fine-tuning in specific languages can deliver scalable and affordable NLP solutions.

Abstract: Large language models (LLMs) are transforming social-science research by
enabling scalable, precise analysis. Their adaptability raises the question of
whether knowledge acquired through fine-tuning in a few languages can transfer
to unseen languages that only appeared during pre-training. To examine this, we
fine-tune lightweight LLaMA 3.2-3B models on monolingual, bilingual, or
multilingual data sets to classify immigration-related tweets from X/Twitter
across 13 languages, a domain characterised by polarised, culturally specific
discourse. We evaluate whether minimal language-specific fine-tuning enables
cross-lingual topic detection and whether adding targeted languages corrects
pre-training biases. Results show that LLMs fine-tuned in one or two languages
can reliably classify immigration-related content in unseen languages. However,
identifying whether a tweet expresses a pro- or anti-immigration stance
benefits from multilingual fine-tuning. Pre-training bias favours dominant
languages, but even minimal exposure to under-represented languages during
fine-tuning (as little as $9.62\times10^{-11}$ of the original pre-training
token volume) yields significant gains. These findings challenge the assumption
that cross-lingual mastery requires extensive multilingual training: limited
language coverage suffices for topic-level generalisation, and structural
biases can be corrected with lightweight interventions. By releasing
4-bit-quantised, LoRA fine-tuned models, we provide an open-source,
reproducible alternative to proprietary LLMs that delivers 35 times faster
inference at just 0.00000989% of the dollar cost of the OpenAI GPT-4o model,
enabling scalable, inclusive research.

</details>


### [71] [Echoes of Automation: The Increasing Use of LLMs in Newsmaking](https://arxiv.org/abs/2508.06445)
*Abolfazl Ansari,Delvin Ce Zhang,Nafis Irtiza Tripto,Dongwon Lee*

Main category: cs.CL

TL;DR: This study analyzes over 40,000 news articles and finds increased use of Generative AI in local and college media, often impacting writing style, word richness, and readability.


<details>
  <summary>Details</summary>
Motivation: Concerns over journalistic integrity and the authorship due to the rise of Generative AI tools in media production.

Method: Used three AI text detectors to analyze content from over 40,000 news articles across various media formats.

Result: Finds growing use of Generative AI, especially in local and college news, with AI often employed for introductions and linguistic improvements (e.g., richer vocabulary).

Conclusion: Generative AI contributes to enhanced readability and word richness but results in less formal and more uniform writing styles, especially noticeable in regional news media.

Abstract: The rapid rise of Generative AI (GenAI), particularly LLMs, poses concerns
for journalistic integrity and authorship. This study examines AI-generated
content across over 40,000 news articles from major, local, and college news
media, in various media formats. Using three advanced AI-text detectors (e.g.,
Binoculars, Fast-Detect GPT, and GPTZero), we find substantial increase of
GenAI use in recent years, especially in local and college news. Sentence-level
analysis reveals LLMs are often used in the introduction of news, while
conclusions usually written manually. Linguistic analysis shows GenAI boosts
word richness and readability but lowers formality, leading to more uniform
writing styles, particularly in local media.

</details>


### [72] [SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning](https://arxiv.org/abs/2508.06447)
*Lingkun Long,Rubing Yang,Yushi Huang,Desheng Hui,Ao Zhou,Jianlei Yang*

Main category: cs.CL

TL;DR: SlimInfer accelerates large language model inference by pruning redundant prompt tokens during computation, achieving significant speed and efficiency gains.


<details>
  <summary>Details</summary>
Motivation: Large Language Models struggle with computational demands, particularly in handling long-context inference. This paper addresses the inefficiencies arising from processing hidden states comprehensively at each layer.

Method: SlimInfer implements dynamic fine-grained pruning of less critical prompt tokens during the forward pass, leveraging an information diffusion process. It introduces an asynchronous KV cache manager to streamline data handling.

Result: Experiments demonstrate up to 2.53x speedup in time-to-first-token and 1.88x reduction in end-to-end latency for LLaMA3.1-8B-Instruct, with negligible performance compromise on evaluation benchmarks.

Conclusion: SlimInfer successfully optimizes inference for large language models, showcasing efficiency improvements without diminishing semantic performance, and highlights potential for broader application in large-scale model deployments.

Abstract: Long-context inference for Large Language Models (LLMs) is heavily limited by
high computational demands. While several existing methods optimize attention
computation, they still process the full set of hidden states at each layer,
limiting overall efficiency. In this work, we propose SlimInfer, an innovative
framework that aims to accelerate inference by directly pruning less critical
prompt tokens during the forward pass. Our key insight is an information
diffusion phenomenon: As information from critical tokens propagates through
layers, it becomes distributed across the entire sequence. This diffusion
process suggests that LLMs can maintain their semantic integrity when excessive
tokens, even including these critical ones, are pruned in hidden states.
Motivated by this, SlimInfer introduces a dynamic fine-grained pruning
mechanism that accurately removes redundant tokens of hidden state at
intermediate layers. This layer-wise pruning naturally enables an asynchronous
KV cache manager that prefetches required token blocks without complex
predictors, reducing both memory usage and I/O costs. Extensive experiments
show that SlimInfer can achieve up to $\mathbf{2.53\times}$ time-to-first-token
(TTFT) speedup and $\mathbf{1.88\times}$ end-to-end latency reduction for
LLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on
LongBench. Our code will be released upon acceptance.

</details>


### [73] [GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models](https://arxiv.org/abs/2508.06471)
*GLM-4. 5 Team,:,Aohan Zeng,Xin Lv,Qinkai Zheng,Zhenyu Hou,Bin Chen,Chengxing Xie,Cunxiang Wang,Da Yin,Hao Zeng,Jiajie Zhang,Kedong Wang,Lucen Zhong,Mingdao Liu,Rui Lu,Shulin Cao,Xiaohan Zhang,Xuancheng Huang,Yao Wei,Yean Cheng,Yifan An,Yilin Niu,Yuanhao Wen,Yushi Bai,Zhengxiao Du,Zihan Wang,Zilin Zhu,Bohan Zhang,Bosi Wen,Bowen Wu,Bowen Xu,Can Huang,Casey Zhao,Changpeng Cai,Chao Yu,Chen Li,Chendi Ge,Chenghua Huang,Chenhui Zhang,Chenxi Xu,Chenzheng Zhu,Chuang Li,Congfeng Yin,Daoyan Lin,Dayong Yang,Dazhi Jiang,Ding Ai,Erle Zhu,Fei Wang,Gengzheng Pan,Guo Wang,Hailong Sun,Haitao Li,Haiyang Li,Haiyi Hu,Hanyu Zhang,Hao Peng,Hao Tai,Haoke Zhang,Haoran Wang,Haoyu Yang,He Liu,He Zhao,Hongwei Liu,Hongxi Yan,Huan Liu,Huilong Chen,Ji Li,Jiajing Zhao,Jiamin Ren,Jian Jiao,Jiani Zhao,Jianyang Yan,Jiaqi Wang,Jiayi Gui,Jiayue Zhao,Jie Liu,Jijie Li,Jing Li,Jing Lu,Jingsen Wang,Jingwei Yuan,Jingxuan Li,Jingzhao Du,Jinhua Du,Jinxin Liu,Junkai Zhi,Junli Gao,Ke Wang,Lekang Yang,Liang Xu,Lin Fan,Lindong Wu,Lintao Ding,Lu Wang,Man Zhang,Minghao Li,Minghuan Xu,Mingming Zhao,Mingshu Zhai,Pengfan Du,Qian Dong,Shangde Lei,Shangqing Tu,Shangtong Yang,Shaoyou Lu,Shijie Li,Shuang Li,Shuang-Li,Shuxun Yang,Sibo Yi,Tianshu Yu,Wei Tian,Weihan Wang,Wenbo Yu,Weng Lam Tam,Wenjie Liang,Wentao Liu,Xiao Wang,Xiaohan Jia,Xiaotao Gu,Xiaoying Ling,Xin Wang,Xing Fan,Xingru Pan,Xinyuan Zhang,Xinze Zhang,Xiuqing Fu,Xunkai Zhang,Yabo Xu,Yandong Wu,Yida Lu,Yidong Wang,Yilin Zhou,Yiming Pan,Ying Zhang,Yingli Wang,Yingru Li,Yinpei Su,Yipeng Geng,Yitong Zhu,Yongkun Yang,Yuhang Li,Yuhao Wu,Yujiang Li,Yunan Liu,Yunqing Wang,Yuntao Li,Yuxuan Zhang,Zezhen Liu,Zhen Yang,Zhengda Zhou,Zhongpei Qiao,Zhuoer Feng,Zhuorui Liu,Zichen Zhang,Zihan Wang,Zijun Yao,Zikang Wang,Ziqiang Liu,Ziwei Chai,Zixuan Li,Zuodong Zhao,Wenguang Chen,Jidong Zhai,Bin Xu,Minlie Huang,Hongning Wang,Juanzi Li,Yuxiao Dong,Jie Tang*

Main category: cs.CL

TL;DR: The paper introduces GLM-4.5, a 355-billion parameter Mixture-of-Experts language model, emphasizing reasoning capabilities and efficient performance with fewer activated parameters. It achieves competitive benchmarks and is open-source.


<details>
  <summary>Details</summary>
Motivation: To create a large language model that excels in reasoning and agentic tasks, offering strong performance with fewer parameters compared to competitors and advancing research on reasoning and agentic AI systems.

Method: GLM-4.5 employs a Mixture-of-Experts architecture with 355B total parameters, activating 32B parameters per task. It undergoes multi-stage training on 23T tokens, expert model iteration, and reinforcement learning techniques for optimization.

Result: The model achieves strong benchmark results: 70.1% on TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. It ranks 3rd overall and 2nd on agentic benchmarks, outperforming some models with more parameters.

Conclusion: GLM-4.5 demonstrates that efficient large models can achieve strong reasoning and agentic task performance. By releasing the model and its compact version, GLM-4.5-Air, the authors aim to contribute to the research community and promote advancements in AI systems.

Abstract: We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language
model with 355B total parameters and 32B activated parameters, featuring a
hybrid reasoning method that supports both thinking and direct response modes.
Through multi-stage training on 23T tokens and comprehensive post-training with
expert model iteration and reinforcement learning, GLM-4.5 achieves strong
performance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on
TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer
parameters than several competitors, GLM-4.5 ranks 3rd overall among all
evaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B
parameters) and a compact version, GLM-4.5-Air (106B parameters), to advance
research in reasoning and agentic AI systems. Code, models, and more
information are available at https://github.com/zai-org/GLM-4.5.

</details>


### [74] [HapticLLaMA: A Multimodal Sensory Language Model for Haptic Captioning](https://arxiv.org/abs/2508.06475)
*Guimin Hu,Daniel Hershcovich,Hasti Seifi*

Main category: cs.CL

TL;DR: The paper introduces HapticLLaMA, which interprets haptic signals into descriptive captions, demonstrating its effectiveness via both automated metrics and human feedback.


<details>
  <summary>Details</summary>
Motivation: Explore haptic signals for natural language generation in applications like virtual reality and accessibility.

Method: Formalized haptic captioning task using LLaMA-based multimodal language model trained through supervised fine-tuning and RLHF.

Result: HapticLLaMA attained METEOR score of 59.98, BLEU-4 score of 32.06, and strong human feedback alignment, improving caption quality.

Conclusion: LLMs can process and adapt to haptic sensory data effectively, showcasing their applicability in novel multimodal domains.

Abstract: Haptic captioning is the task of generating natural language descriptions
from haptic signals, such as vibrations, for use in virtual reality,
accessibility, and rehabilitation applications. While previous multimodal
research has focused primarily on vision and audio, haptic signals for the
sense of touch remain underexplored. To address this gap, we formalize the
haptic captioning task and propose HapticLLaMA, a multimodal sensory language
model that interprets vibration signals into descriptions in a given sensory,
emotional, or associative category. We investigate two types of haptic
tokenizers, a frequency-based tokenizer and an EnCodec-based tokenizer, that
convert haptic signals into sequences of discrete units, enabling their
integration with the LLaMA model. HapticLLaMA is trained in two stages: (1)
supervised fine-tuning using the LLaMA architecture with LoRA-based adaptation,
and (2) fine-tuning via reinforcement learning from human feedback (RLHF). We
assess HapticLLaMA's captioning performance using both automated n-gram metrics
and human evaluation. HapticLLaMA demonstrates strong capability in
interpreting haptic vibration signals, achieving a METEOR score of 59.98 and a
BLEU-4 score of 32.06 respectively. Additionally, over 61% of the generated
captions received human ratings above 3.5 on a 7-point scale, with RLHF
yielding a 10% improvement in the overall rating distribution, indicating
stronger alignment with human haptic perception. These findings highlight the
potential of large language models to process and adapt to sensory data.

</details>


### [75] [Post-training for Efficient Communication via Convention Formation](https://arxiv.org/abs/2508.06482)
*Yilun Hua,Evan Wang,Yoav Artzi*

Main category: cs.CL

TL;DR: The paper addresses the limitation of LLMs in naturally adapting language for convention formation, proposing targeted fine-tuning and introducing two benchmarks to successfully improve this capability.


<details>
  <summary>Details</summary>
Motivation: Prior research indicates that unlike humans, LLMs do not naturally adapt their language or create ad-hoc conventions in multi-turn interactions. The paper aims to solve this limitation for enhancing language interaction capabilities.

Method: The authors use a post-training process involving targeted fine-tuning on demonstrations of convention formation, with two new evaluation methods: interaction benchmark and document-grounded reference completion task.

Result: Post-trained LLMs show significantly improved convention formation abilities, as evidenced by performance in cognitively motivated benchmarks and real-world-inspired tasks.

Conclusion: Targeted fine-tuning enhances LLMs' ability to form ad-hoc language conventions, effectively bridging a gap between human and LLM interaction efficiencies.

Abstract: Humans communicate with increasing efficiency in multi-turn interactions, by
adapting their language and forming ad-hoc conventions. In contrast, prior work
shows that LLMs do not naturally show this behavior. We develop a post-training
process to develop this ability through targeted fine-tuning on heuristically
identified demonstrations of convention formation. We evaluate with two new
benchmarks focused on this capability. First, we design a focused,
cognitively-motivated interaction benchmark that consistently elicits strong
convention formation trends in humans. Second, we create a new
document-grounded reference completion task that reflects in-the-wild
convention formation behavior. Our studies show significantly improved
convention formation abilities in post-trained LLMs across the two evaluation
methods.

</details>


### [76] [Indian Legal NLP Benchmarks : A Survey](https://arxiv.org/abs/2107.06056)
*Prathamesh Kalamkar,Janani Venugopalan Ph. D.,Vivek Raghavan Ph. D*

Main category: cs.CL

TL;DR: This paper discusses the importance of creating dedicated NLP benchmarks for Indian legal texts, reviews existing work, and proposes ideas for new benchmarks.


<details>
  <summary>Details</summary>
Motivation: Legal text differs significantly from standard English, necessitating specialized NLP benchmarks for Indian legal texts to advance AI applications in this domain.

Method: The paper reviews existing work in the field and suggests ideas for developing benchmarks tailored to Indian legal scenarios.

Result: The paper does not report tangible results but highlights the necessity of new benchmarks for Indian Legal NLP.

Conclusion: Creating such benchmarks will facilitate innovation in NLP applications for Indian legal systems and benefit both the AI community and the legal profession.

Abstract: Availability of challenging benchmarks is the key to advancement of AI in a
specific field.Since Legal Text is significantly different than normal English
text, there is a need to create separate Natural Language Processing benchmarks
for Indian Legal Text which are challenging and focus on tasks specific to
Legal Systems. This will spur innovation in applications of Natural language
Processing for Indian Legal Text and will benefit AI community and Legal
fraternity. We review the existing work in this area and propose ideas to
create new benchmarks for Indian Legal Natural Language Processing.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [77] [Boosting Adversarial Transferability via Residual Perturbation Attack](https://arxiv.org/abs/2508.05689)
*Jinjia Peng,Zeze Tao,Huibing Wang,Meng Wang,Yang Wang*

Main category: cs.CV

TL;DR: Deep neural networks are vulnerable to adversarial examples, and this paper addresses improvements in their transferability using a novel Residual Perturbation Attack (ResPA).


<details>
  <summary>Details</summary>
Motivation: The existing transfer-based attack methods focus insufficiently on perturbation directions, leading to limited efficacy in transferring adversarial examples.

Method: ResPA uses residual gradients as perturbation directions obtained via exponential moving averages on historical gradients, making the adversarial examples align with flatter loss regions.

Result: Experimental results show better adversarial transferability of ResPA compared to existing methods, and its effectiveness improves further when combined with input transformation techniques.

Conclusion: ResPA enhances transfer-based attacks by targeting perturbation directions and flat loss landscapes, contributing to improved black-box attack success rates.

Abstract: Deep neural networks are susceptible to adversarial examples while suffering
from incorrect predictions via imperceptible perturbations. Transfer-based
attacks create adversarial examples for surrogate models and transfer these
examples to target models under black-box scenarios. Recent studies reveal that
adversarial examples in flat loss landscapes exhibit superior transferability
to alleviate overfitting on surrogate models. However, the prior arts overlook
the influence of perturbation directions, resulting in limited transferability.
In this paper, we propose a novel attack method, named Residual Perturbation
Attack (ResPA), relying on the residual gradient as the perturbation direction
to guide the adversarial examples toward the flat regions of the loss function.
Specifically, ResPA conducts an exponential moving average on the input
gradients to obtain the first moment as the reference gradient, which
encompasses the direction of historical gradients. Instead of heavily relying
on the local flatness that stems from the current gradients as the perturbation
direction, ResPA further considers the residual between the current gradient
and the reference gradient to capture the changes in the global perturbation
direction. The experimental results demonstrate the better transferability of
ResPA than the existing typical transfer-based attack methods, while the
transferability can be further improved by combining ResPA with the current
input transformation methods. The code is available at
https://github.com/ZezeTao/ResPA.

</details>


### [78] [Generalized Few-Shot Out-of-Distribution Detection](https://arxiv.org/abs/2508.05732)
*Pinxuan Li,Bing Cao,Changqing Zhang,Qinghua Hu*

Main category: cs.CV

TL;DR: This paper introduces a Generalized Few-shot OOD Detection (GOOD) framework using auxiliary general knowledge models to overcome generalization limitations in few-shot OOD detection.


<details>
  <summary>Details</summary>
Motivation: Although few-shot OOD detection is important for practical use, existing methods struggle with generalization, overfitting to limited training data and inconsistent performance across scenarios.

Method: The authors propose a framework called GOOD that leverages a General Knowledge Model (GKM) to introduce general knowledge beyond the few-shot data, and develops a Knowledge Dynamic Embedding (KDE) mechanism to modulate this knowledge adaptively.

Result: The GOOD framework successfully improves generalization in few-shot OOD detection, reducing errors and achieving superior performance in practical OOD benchmarks.

Conclusion: The proposed approach effectively addresses generalization challenges in few-shot OOD detection by using a general knowledge model and adaptive embedding, setting a new direction for improved performance.

Abstract: Few-shot Out-of-Distribution (OOD) detection has emerged as a critical
research direction in machine learning for practical deployment. Most existing
Few-shot OOD detection methods suffer from insufficient generalization
capability for the open world. Due to the few-shot learning paradigm, the OOD
detection ability is often overfit to the limited training data itself, thus
degrading the performance on generalized data and performing inconsistently
across different scenarios. To address this challenge, we proposed a
Generalized Few-shot OOD Detection (GOOD) framework, which empowers the general
knowledge of the OOD detection model with an auxiliary General Knowledge Model
(GKM), instead of directly learning from few-shot data. We proceed to reveal
the few-shot OOD detection from a generalization perspective and theoretically
derive the Generality-Specificity balance (GS-balance) for OOD detection, which
provably reduces the upper bound of generalization error with a general
knowledge model. Accordingly, we propose a Knowledge Dynamic Embedding (KDE)
mechanism to adaptively modulate the guidance of general knowledge. KDE
dynamically aligns the output distributions of the OOD detection model to the
general knowledge model based on the Generalized Belief (G-Belief) of GKM,
thereby boosting the GS-balance. Experiments on real-world OOD benchmarks
demonstrate our superiority. Codes will be available.

</details>


### [79] [UnGuide: Learning to Forget with LoRA-Guided Diffusion Models](https://arxiv.org/abs/2508.05755)
*Agnieszka Polowczyk,Alicja Polowczyk,Dawid Malarz,Artur Kasymov,Marcin Mazur,Jacek Tabor,Przemysław Spurek*

Main category: cs.CV

TL;DR: The authors propose UnGuide, a method to improve machine unlearning in text-to-image diffusion models while maintaining overall performance.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address concerns about the misuse of large-scale text-to-image diffusion models for harmful or misleading content, emphasizing the need for precise unlearning without degrading model functionality.

Method: The authors develop UnGuide, a mechanism that integrates UnGuidance with Classifier-Free Guidance (CFG) to selectively modulate learning via a LoRA adapter. It dynamically adjusts guidance scales during early denoising steps to ensure controlled unlearning while preserving content fidelity.

Result: UnGuide outperforms existing LoRA solutions in tasks like object erasure and explicit content removal, providing higher fidelity and realism in unrelated image prompts.

Conclusion: UnGuide effectively balances the removal of undesired concepts while preserving the expressive power of diffusion models, presenting a significant improvement over previous approaches.

Abstract: Recent advances in large-scale text-to-image diffusion models have heightened
concerns about their potential misuse, especially in generating harmful or
misleading content. This underscores the urgent need for effective machine
unlearning, i.e., removing specific knowledge or concepts from pretrained
models without compromising overall performance. One possible approach is
Low-Rank Adaptation (LoRA), which offers an efficient means to fine-tune models
for targeted unlearning. However, LoRA often inadvertently alters unrelated
content, leading to diminished image fidelity and realism. To address this
limitation, we introduce UnGuide -- a novel approach which incorporates
UnGuidance, a dynamic inference mechanism that leverages Classifier-Free
Guidance (CFG) to exert precise control over the unlearning process. UnGuide
modulates the guidance scale based on the stability of a few first steps of
denoising processes, enabling selective unlearning by LoRA adapter. For prompts
containing the erased concept, the LoRA module predominates and is
counterbalanced by the base model; for unrelated prompts, the base model
governs generation, preserving content fidelity. Empirical results demonstrate
that UnGuide achieves controlled concept removal and retains the expressive
power of diffusion models, outperforming existing LoRA-based methods in both
object erasure and explicit content removal tasks.

</details>


### [80] [Improving Masked Style Transfer using Blended Partial Convolution](https://arxiv.org/abs/2508.05769)
*Seyed Hadi Seyed,Ayberk Cansever,David Hart*

Main category: cs.CV

TL;DR: This paper introduces a partial-convolution-based network for localized artistic style transfer, improving over traditional masking approaches.


<details>
  <summary>Details</summary>
Motivation: Existing artistic style transfer methods apply style to entire images, making them unsuitable for users who want stylization only for specific regions. Standard masking techniques are ineffective for accurately capturing style features within regions of interest.

Method: The authors developed a style transfer network that utilizes partial convolutions to apply styles precisely to selected regions. Internal blending techniques are integrated to address imperfections in region selection.

Result: Experimental results on the SA-1B dataset demonstrate visual and quantitative improvements in the stylization process.

Conclusion: The proposed method successfully enables precise and improved artistic style transfer to specific image regions. The work advances the field by overcoming limitations of conventional masking approaches, with code accessible for reproducibility.

Abstract: Artistic style transfer has long been possible with the advancements of
convolution- and transformer-based neural networks. Most algorithms apply the
artistic style transfer to the whole image, but individual users may only need
to apply a style transfer to a specific region in the image. The standard
practice is to simply mask the image after the stylization. This work shows
that this approach tends to improperly capture the style features in the region
of interest. We propose a partial-convolution-based style transfer network that
accurately applies the style features exclusively to the region of interest.
Additionally, we present network-internal blending techniques that account for
imperfections in the region selection. We show that this visually and
quantitatively improves stylization using examples from the SA-1B dataset. Code
is publicly available at https://github.com/davidmhart/StyleTransferMasked.

</details>


### [81] [MAISI-v2: Accelerated 3D High-Resolution Medical Image Synthesis with Rectified Flow and Region-specific Contrastive Loss](https://arxiv.org/abs/2508.05772)
*Can Zhao,Pengfei Guo,Dong Yang,Yucheng Tang,Yufan He,Benjamin Simon,Mason Belue,Stephanie Harmon,Baris Turkbey,Daguang Xu*

Main category: cs.CV

TL;DR: MAISI-v2 improves medical image synthesis using rectified flow for faster and higher-quality generation, while addressing prior limitations with condition fidelity and inference speed.


<details>
  <summary>Details</summary>
Motivation: Medical image synthesis using diffusion models faces challenges like limited generalizability, slow inference, and weak input alignment, particularly critical for clinical and research applications.

Method: MAISI-v2 integrates rectified flow to accelerate synthesis and introduces a region-specific contrastive loss for enhanced condition fidelity.

Result: MAISI-v2 achieves state-of-the-art image quality with 33× faster inference compared to latent diffusion models. Its synthetic images are validated for data augmentation in segmentation tasks.

Conclusion: MAISI-v2 addresses multiple limitations of prior methods in medical image synthesis, offering enhanced condition fidelity, faster generation, and reproducibility tools for broader adoption and development in the field.

Abstract: Medical image synthesis is an important topic for both clinical and research
applications. Recently, diffusion models have become a leading approach in this
area. Despite their strengths, many existing methods struggle with (1) limited
generalizability that only work for specific body regions or voxel spacings,
(2) slow inference, which is a common issue for diffusion models, and (3) weak
alignment with input conditions, which is a critical issue for medical imaging.
MAISI, a previously proposed framework, addresses generalizability issues but
still suffers from slow inference and limited condition consistency. In this
work, we present MAISI-v2, the first accelerated 3D medical image synthesis
framework that integrates rectified flow to enable fast and high quality
generation. To further enhance condition fidelity, we introduce a novel
region-specific contrastive loss to enhance the sensitivity to region of
interest. Our experiments show that MAISI-v2 can achieve SOTA image quality
with $33 \times$ acceleration for latent diffusion model. We also conducted a
downstream segmentation experiment to show that the synthetic images can be
used for data augmentation. We release our code, training details, model
weights, and a GUI demo to facilitate reproducibility and promote further
development within the community.

</details>


### [82] [Few-Shot Deployment of Pretrained MRI Transformers in Brain Imaging Tasks](https://arxiv.org/abs/2508.05783)
*Mengyu Li,Guoyao Shen,Chad W. Farris,Xin Zhang*

Main category: cs.CV

TL;DR: This study introduces a framework using pretrained MRI transformers for efficient few-shot brain imaging tasks, leveraging Masked Autoencoder (MAE) pretraining on a massive brain MRI dataset.


<details>
  <summary>Details</summary>
Motivation: To address the limited real-world applicability of transformer-based machine learning in medical imaging due to a lack of annotated data.

Method: The framework uses Masked Autoencoder (MAE) pretraining on a large dataset of over 31 million MRI slices, creating transferable representations. It utilizes a combination of techniques like frozen MAE encoders with linear heads for high-level tasks and a hybrid architecture (MAE-FUnet) for low-level tasks.

Result: The framework achieves state-of-the-art accuracy in classification tasks and outperforms baselines in segmentation tasks under data-scarce conditions.

Conclusion: The proposed method is suitable for low-resource clinical environments and broader neuroimaging applications, offering efficiency, stability, and scalability.

Abstract: Machine learning using transformers has shown great potential in medical
imaging, but its real-world applicability remains limited due to the scarcity
of annotated data. In this study, we propose a practical framework for the
few-shot deployment of pretrained MRI transformers in diverse brain imaging
tasks. By utilizing the Masked Autoencoder (MAE) pretraining strategy on a
large-scale, multi-cohort brain MRI dataset comprising over 31 million slices,
we obtain highly transferable latent representations that generalize well
across tasks and datasets. For high-level tasks such as classification, a
frozen MAE encoder combined with a lightweight linear head achieves
state-of-the-art accuracy in MRI sequence identification with minimal
supervision. For low-level tasks such as segmentation, we propose MAE-FUnet, a
hybrid architecture that fuses multiscale CNN features with pretrained MAE
embeddings. This model consistently outperforms other strong baselines in both
skull stripping and multi-class anatomical segmentation under data-limited
conditions. With extensive quantitative and qualitative evaluations, our
framework demonstrates efficiency, stability, and scalability, suggesting its
suitability for low-resource clinical environments and broader neuroimaging
applications.

</details>


### [83] [Optimization-Free Style Transfer for 3D Gaussian Splats](https://arxiv.org/abs/2508.05813)
*Raphael Du Sablon,David Hart*

Main category: cs.CV

TL;DR: This paper introduces a fast approach to 3D Gaussian splat style transfer using a graph-based surface method without needing reconstruction or optimization.


<details>
  <summary>Details</summary>
Motivation: Previous methods for stylizing 3D Gaussian splats required complex reconstruction, fine-tuning, or optimization steps. Simplifying this process while enabling style transfer quickly and on consumer-grade hardware was the goal.

Method: The method involves generating a graph structure across the implicit surface of the splat representation, applying a feed-forward surface-based stylization, and interpolating the stylization back to individual splats.

Result: The proposed approach achieves rapid style transfer under 2 minutes, even on consumer-grade hardware, producing quality results comparable to existing methods.

Conclusion: The approach successfully simplifies the process of 3D Gaussian splat stylization while maintaining high-quality results, making it accessible and efficient.

Abstract: The task of style transfer for 3D Gaussian splats has been explored in many
previous works, but these require reconstructing or fine-tuning the splat while
incorporating style information or optimizing a feature extraction network on
the splat representation. We propose a reconstruction- and optimization-free
approach to stylizing 3D Gaussian splats. This is done by generating a graph
structure across the implicit surface of the splat representation. A
feed-forward, surface-based stylization method is then used and interpolated
back to the individual splats in the scene. This allows for any style image and
3D Gaussian splat to be used without any additional training or optimization.
This also allows for fast stylization of splats, achieving speeds under 2
minutes even on consumer-grade hardware. We demonstrate the quality results
this approach achieves and compare to other 3D Gaussian splat style transfer
methods. Code is publicly available at
https://github.com/davidmhart/FastSplatStyler.

</details>


### [84] [MZEN: Multi-Zoom Enhanced NeRF for 3-D Reconstruction with Unknown Camera Poses](https://arxiv.org/abs/2508.05819)
*Jong-Ik Park,Carlee Joe-Wong,Gary K. Fedder*

Main category: cs.CV

TL;DR: NeRF struggles with fine details in industrial contexts like sub-micron defect detection due to fixed sensor resolution. MZEN addresses this by incorporating multi-zoom image sets and achieves superior accuracy.


<details>
  <summary>Details</summary>
Motivation: Industrial inspection requires accurate 3D reconstruction to detect fine structures, such as sub-micron defects, which traditional NeRF fails to capture due to fixed sensor resolution.

Method: MZEN introduces a learnable zoom scalar to scale focal lengths and employs a pose strategy that solves wide-field images first and then refines zoom-in images using a zoom-consistent procedure.

Result: MZEN outperforms baselines, boosting metrics like PSNR by 28%, SSIM by 10%, and reducing LPIPS by 222%, across diverse scenes including synthetic models and real-world SEM data.

Conclusion: MZEN makes NeRF applicable in factory settings by preserving global 3D accuracy while capturing micron-level details critical for industrial inspection.

Abstract: Neural Radiance Fields (NeRF) methods excel at 3D reconstruction from
multiple 2D images, even those taken with unknown camera poses. However, they
still miss the fine-detailed structures that matter in industrial inspection,
e.g., detecting sub-micron defects on a production line or analyzing chips with
Scanning Electron Microscopy (SEM). In these scenarios, the sensor resolution
is fixed and compute budgets are tight, so the only way to expose fine
structure is to add zoom-in images; yet, this breaks the multi-view consistency
that pose-free NeRF training relies on. We propose Multi-Zoom Enhanced NeRF
(MZEN), the first NeRF framework that natively handles multi-zoom image sets.
MZEN (i) augments the pin-hole camera model with an explicit, learnable zoom
scalar that scales the focal length, and (ii) introduces a novel pose strategy:
wide-field images are solved first to establish a global metric frame, and
zoom-in images are then pose-primed to the nearest wide-field counterpart via a
zoom-consistent crop-and-match procedure before joint refinement. Across eight
forward-facing scenes$\unicode{x2013}$synthetic TCAD models, real SEM of
micro-structures, and BLEFF objects$\unicode{x2013}$MZEN consistently
outperforms pose-free baselines and even high-resolution variants, boosting
PSNR by up to $28 \%$, SSIM by $10 \%$, and reducing LPIPS by up to $222 \%$.
MZEN, therefore, extends NeRF to real-world factory settings, preserving global
accuracy while capturing the micron-level details essential for industrial
inspection.

</details>


### [85] [TSMS-SAM2: Multi-scale Temporal Sampling Augmentation and Memory-Splitting Pruning for Promptable Video Object Segmentation and Tracking in Surgical Scenarios](https://arxiv.org/abs/2508.05829)
*Guoping Xu,Hua-Chieh Shao,You Zhang*

Main category: cs.CV

TL;DR: The paper presents TSMS-SAM2, a novel framework designed for video object segmentation and tracking (VOST) in surgical videos, improving motion robustness and memory efficiency.


<details>
  <summary>Details</summary>
Motivation: To address challenges in applying foundation models like SAM to surgical videos due to rapid object motion and memory redundancy during segmentation.

Method: The framework introduces multi-temporal-scale video sampling for motion variability and a memory mechanism to filter and organize past frame features for efficient segmentation.

Result: TSMS-SAM2 achieved the highest Dice scores of 95.24 and 86.73 on EndoVis2017 and EndoVis2018 datasets, outperforming previous methods.

Conclusion: TSMS-SAM2 is effective in improving segmentation robustness and efficiency in challenging surgical video scenarios, with code availability announced for reproducibility.

Abstract: Promptable video object segmentation and tracking (VOST) has seen significant
advances with the emergence of foundation models like Segment Anything Model 2
(SAM2); however, their application in surgical video analysis remains
challenging due to complex motion dynamics and the redundancy of memory that
impedes effective learning. In this work, we propose TSMS-SAM2, a novel
framework that enhances promptable VOST in surgical videos by addressing
challenges of rapid object motion and memory redundancy in SAM2. TSMS-SAM2
introduces two key strategies: multi-temporal-scale video sampling augmentation
to improve robustness against motion variability, and a memory splitting and
pruning mechanism that organizes and filters past frame features for more
efficient and accurate segmentation. Evaluated on EndoVis2017 and EndoVis2018
datasets, TSMS-SAM2 achieved the highest mean Dice scores of 95.24 and 86.73,
respectively, outperforming prior SAM-based and task-specific methods.
Extensive ablation studies confirm the effectiveness of multiscale temporal
augmentation and memory splitting, highlighting the framework's potential for
robust, efficient segmentation in complex surgical scenarios. Our source code
will be available at https://github.com/apple1986/TSMS-SAM2.

</details>


### [86] [Temporal Cluster Assignment for Efficient Real-Time Video Segmentation](https://arxiv.org/abs/2508.05851)
*Ka-Wai Yung,Felix J. S. Bragman,Jialang Xu,Imanol Luengo,Danail Stoyanov,Evangelos B. Mazomenos*

Main category: cs.CV

TL;DR: The paper introduces Temporal Cluster Assignment (TCA), a method to refine token clustering for video segmentation using temporal coherence, achieving better accuracy-speed trade-offs.


<details>
  <summary>Details</summary>
Motivation: Vision Transformers, particularly the Swin Transformer, excel in multi-scale video segmentation but face computational inefficiencies for real-time, resource-constrained scenarios.

Method: The study presents the TCA approach, which refines token clusters by leveraging temporal correlations across video frames, retaining fine-grained details without requiring model fine-tuning.

Result: TCA effectively reduces computation while enhancing performance in various datasets such as YouTube-VIS and surgical video datasets, improving clustering methods' accuracy-speed trade-off.

Conclusion: TCA is a generalizable and efficient solution for optimizing video segmentation models in both natural and specialized domains without extra training overhead.

Abstract: Vision Transformers have substantially advanced the capabilities of
segmentation models across both image and video domains. Among them, the Swin
Transformer stands out for its ability to capture hierarchical, multi-scale
representations, making it a popular backbone for segmentation in videos.
However, despite its window-attention scheme, it still incurs a high
computational cost, especially in larger variants commonly used for dense
prediction in videos. This remains a major bottleneck for real-time,
resource-constrained applications. Whilst token reduction methods have been
proposed to alleviate this, the window-based attention mechanism of Swin
requires a fixed number of tokens per window, limiting the applicability of
conventional pruning techniques. Meanwhile, training-free token clustering
approaches have shown promise in image segmentation while maintaining window
consistency. Nevertheless, they fail to exploit temporal redundancy, missing a
key opportunity to further optimize video segmentation performance. We
introduce Temporal Cluster Assignment (TCA), a lightweight and effective,
fine-tuning-free strategy that enhances token clustering by leveraging temporal
coherence across frames. Instead of indiscriminately dropping redundant tokens,
TCA refines token clusters using temporal correlations, thereby retaining
fine-grained details while significantly reducing computation. Extensive
evaluations on YouTube-VIS 2019, YouTube-VIS 2021, OVIS, and a private surgical
video dataset show that TCA consistently boosts the accuracy-speed trade-off of
existing clustering-based methods. Our results demonstrate that TCA generalizes
competently across both natural and domain-specific videos.

</details>


### [87] [VISTA: Vision-Language Imitation of Situational Thinking and Attention for Human-Like Driver Focus in Dynamic Environments](https://arxiv.org/abs/2508.05852)
*Kaiser Hamid,Khandakar Ashrafi Akbar,Nade Liang*

Main category: cs.CV

TL;DR: This study introduces a vision-language model to predict driver gaze shifts in natural language using few-shot and zero-shot learning on images.


<details>
  <summary>Details</summary>
Motivation: Understanding and predicting driver visual attention is crucial for autonomous driving and HCI, but most current approaches rely on static image analysis.

Method: The paper uses a vision-language framework anchored by a fine-tuned LLaVA model to translate driver gaze predictions into language. It employs caption refinement from the BDD-A dataset with human feedback and integrates visual and contextual cues.

Result: The fine-tuned model outperforms general-purpose models in attention shift detection and interpretability, demonstrating improved results in both few-shot and zero-shot scenarios.

Conclusion: This is a pioneering effort in generating natural language descriptions of driver gaze behavior, offering advancements toward more explainable AI in autonomous driving and related tasks like behavior forecasting.

Abstract: Driver visual attention prediction is a critical task in autonomous driving
and human-computer interaction (HCI) research. Most prior studies focus on
estimating attention allocation at a single moment in time, typically using
static RGB images such as driving scene pictures. In this work, we propose a
vision-language framework that models the changing landscape of drivers' gaze
through natural language, using few-shot and zero-shot learning on single RGB
images. We curate and refine high-quality captions from the BDD-A dataset using
human-in-the-loop feedback, then fine-tune LLaVA to align visual perception
with attention-centric scene understanding. Our approach integrates both
low-level cues and top-down context (e.g., route semantics, risk anticipation),
enabling language-based descriptions of gaze behavior. We evaluate performance
across training regimes (few shot, and one-shot) and introduce domain-specific
metrics for semantic alignment and response diversity. Results show that our
fine-tuned model outperforms general-purpose VLMs in attention shift detection
and interpretability. To our knowledge, this is among the first attempts to
generate driver visual attention allocation and shifting predictions in natural
language, offering a new direction for explainable AI in autonomous driving.
Our approach provides a foundation for downstream tasks such as behavior
forecasting, human-AI teaming, and multi-agent coordination.

</details>


### [88] [Multi-view Gaze Target Estimation](https://arxiv.org/abs/2508.05857)
*Qiaomu Miao,Vivek Raju Golani,Jingyi Xu,Progga Paromita Dutta,Minh Hoai,Dimitris Samaras*

Main category: cs.CV

TL;DR: This study proposes a multi-camera approach to gaze target estimation, addressing limitations like face occlusions and out-of-view targets.


<details>
  <summary>Details</summary>
Motivation: Current single-view gaze estimation methods struggle with challenges like occluded faces, ambiguity, and inability to handle out-of-view targets.

Method: The approach integrates head information aggregation, uncertainty-based gaze selection, and epipolar-based scene attention using pairs of camera views.

Result: The multi-camera method significantly outperforms single-camera systems, especially when a second camera captures a clear face view.

Conclusion: The method enhances gaze estimation capabilities and introduces a new multi-view dataset for evaluation, advancing the domain of gaze target estimation.

Abstract: This paper presents a method that utilizes multiple camera views for the gaze
target estimation (GTE) task. The approach integrates information from
different camera views to improve accuracy and expand applicability, addressing
limitations in existing single-view methods that face challenges such as face
occlusion, target ambiguity, and out-of-view targets. Our method processes a
pair of camera views as input, incorporating a Head Information Aggregation
(HIA) module for leveraging head information from both views for more accurate
gaze estimation, an Uncertainty-based Gaze Selection (UGS) for identifying the
most reliable gaze output, and an Epipolar-based Scene Attention (ESA) module
for cross-view background information sharing. This approach significantly
outperforms single-view baselines, especially when the second camera provides a
clear view of the person's face. Additionally, our method can estimate the gaze
target in the first view using the image of the person in the second view only,
a capability not possessed by single-view GTE methods. Furthermore, the paper
introduces a multi-view dataset for developing and evaluating multi-view GTE
methods. Data and code are available at
https://www3.cs.stonybrook.edu/~cvl/multiview_gte.html

</details>


### [89] [ETTA: Efficient Test-Time Adaptation for Vision-Language Models through Dynamic Embedding Updates](https://arxiv.org/abs/2508.05898)
*Hamidreza Dastmalchi,Aijun An,Ali cheraghian*

Main category: cs.CV

TL;DR: The paper introduces ETTA, a method that improves test-time adaptation by refining decision boundaries dynamically using all incoming test samples and reducing dependency on prompt tuning, achieving state-of-the-art results in accuracy and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Pretrained vision-language models like CLIP struggle to generalize under distribution shifts, leading to the need for effective test-time adaptation strategies.

Method: ETTA employs a Recursive Updating module to use all incoming test samples for dynamically refining decision boundaries, and an Adaptive Ensemble module to reduce prompt dependency by selecting optimal prompts and combining module scores adaptively.

Result: Experiments show ETTA significantly outperforms existing methods on benchmarks in terms of accuracy and computational complexity.

Conclusion: ETTA establishes a new benchmark for efficient and effective test-time adaptation, enhancing generalization without substantial memory or computational demands.

Abstract: Pretrained vision-language models (VLMs) like CLIP show strong zero-shot
performance but struggle with generalization under distribution shifts.
Test-Time Adaptation (TTA) addresses this by adapting VLMs to unlabeled test
data in new domains. While some TTA methods rely on prompt-tuning,
training-free cache-based approaches are preferred for efficiency. However,
current cache-based TTA models store only a limited set of high-confidence
samples, restricting the decision boundary to these samples and ignoring the
influence of other incoming test data. To address this, we propose Efficient
Test-Time Adaptation (ETTA), introducing a Recursive Updating module that
integrates all incoming test samples, progressively refining the decision
boundary. This strategy mimics an unbounded cache, dynamically updating
contextual embeddings for improved accuracy with minimal memory and
computational overhead. ETTA also includes an Adaptive Ensemble module to
reduce prompt dependency in image-to-text scores by dynamically selecting
optimal prompts for each class. Furthermore, ETTA adaptively combines scores
from both modules based on confidence levels, leveraging their complementary
strengths. Extensive experiments on two benchmarks confirm that ETTA surpasses
the state-of-the-art TTA models in computational complexity and accuracy,
setting a new standard for effective, efficient test-time adaptation. The code
has been released at https://github.com/hamidreza-dastmalchi/ETTA.

</details>


### [90] [HOLODECK 2.0: Vision-Language-Guided 3D World Generation with Editing](https://arxiv.org/abs/2508.05899)
*Zixuan Bian,Ruohan Ren,Yue Yang,Chris Callison-Burch*

Main category: cs.CV

TL;DR: HOLODECK 2.0 is an advanced framework for 3D scene generation from text, supporting diverse styles, high semantic accuracy, and interactive editing.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the high manual effort involved in current 3D scene creation and the limitations of automated methods in generating open-domain, flexible, editable 3D scenes directly from text.

Method: HOLODECK 2.0 employs vision-language models for object identification, high-quality asset generation through 3D generative models, and iterative spatial constraint application for scene layout coherence.

Result: Experiments show that HOLODECK 2.0 outperforms baselines in generating semantically accurate and visually diverse 3D scenes, validated by human evaluations and CLIP-based metrics.

Conclusion: HOLODECK 2.0 represents a significant step forward in automated 3D scene generation, with applications in game modeling and other fields requiring efficient 3D design workflows.

Abstract: 3D scene generation plays a crucial role in gaming, artistic creation,
virtual reality and many other domains. However, current 3D scene design still
relies heavily on extensive manual effort from creators, and existing automated
methods struggle to generate open-domain scenes or support flexible editing. As
a result, generating 3D worlds directly from text has garnered increasing
attention. In this paper, we introduce HOLODECK 2.0, an advanced
vision-language-guided framework for 3D world generation with support for
interactive scene editing based on human feedback. HOLODECK 2.0 can generate
diverse and stylistically rich 3D scenes (e.g., realistic, cartoon, anime, and
cyberpunk styles) that exhibit high semantic fidelity to fine-grained input
descriptions, suitable for both indoor and open-domain environments. HOLODECK
2.0 leverages vision-language models (VLMs) to identify and parse the objects
required in a scene and generates corresponding high-quality assets via
state-of-the-art 3D generative models. It then iteratively applies spatial
constraints derived from the VLMs to achieve semantically coherent and
physically plausible layouts. Human evaluations and CLIP-based assessments
demonstrate that HOLODECK 2.0 effectively generates high-quality scenes closely
aligned with detailed textual descriptions, consistently outperforming
baselines across indoor and open-domain scenarios. Additionally, we provide
editing capabilities that flexibly adapt to human feedback, supporting layout
refinement and style-consistent object edits. Finally, we present a practical
application of HOLODECK 2.0 in procedural game modeling, generating visually
rich and immersive environments, potentially boosting efficiency.

</details>


### [91] [Robust Image Stitching with Optimal Plane](https://arxiv.org/abs/2508.05903)
*Lang Nie,Yuan Mei,Kang Liao,Yunqiu Xu,Chunyu Lin,Bin Xiao*

Main category: cs.CV

TL;DR: This paper presents RopStitch, an unsupervised deep image stitching framework that ensures robustness and naturalness by using a dual-branch architecture and virtual optimal planes.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges of robustness and naturalness in image stitching, which are critical when dealing with diverse and unseen real-world scenes.

Method: RopStitch uses a dual-branch architecture to capture coarse and fine image features and merges them at a correlation level. It also employs virtual optimal planes and homography decomposition coefficients for content alignment and structural preservation.

Result: Extensive experiments show that RopStitch significantly outperforms existing methods in robustness and naturalness across various datasets.

Conclusion: RopStitch is an effective and generalizable image stitching framework that provides superior scene robustness and content naturalness in unsupervised settings.

Abstract: We present \textit{RopStitch}, an unsupervised deep image stitching framework
with both robustness and naturalness. To ensure the robustness of
\textit{RopStitch}, we propose to incorporate the universal prior of content
perception into the image stitching model by a dual-branch architecture. It
separately captures coarse and fine features and integrates them to achieve
highly generalizable performance across diverse unseen real-world scenes.
Concretely, the dual-branch model consists of a pretrained branch to capture
semantically invariant representations and a learnable branch to extract
fine-grained discriminative features, which are then merged into a whole by a
controllable factor at the correlation level. Besides, considering that content
alignment and structural preservation are often contradictory to each other, we
propose a concept of virtual optimal planes to relieve this conflict. To this
end, we model this problem as a process of estimating homography decomposition
coefficients, and design an iterative coefficient predictor and minimal
semantic distortion constraint to identify the optimal plane. This scheme is
finally incorporated into \textit{RopStitch} by warping both views onto the
optimal plane bidirectionally. Extensive experiments across various datasets
demonstrate that \textit{RopStitch} significantly outperforms existing methods,
particularly in scene robustness and content naturalness. The code is available
at {\color{red}https://github.com/MmelodYy/RopStitch}.

</details>


### [92] [Neural Field Representations of Mobile Computational Photography](https://arxiv.org/abs/2508.05907)
*Ilya Chugunov*

Main category: cs.CV

TL;DR: This paper explores using neural field models to enhance mobile photography by representing complex geometry and lighting effects, enabling advanced imaging applications like depth estimation and image stitching.


<details>
  <summary>Details</summary>
Motivation: The paper aims to leverage advancements in computational imaging and neural mapping techniques to address challenges in mobile photography and enhance its potential for sophisticated applications.

Method: The authors propose well-constructed, self-regularized neural field models that fit directly to raw data from smartphone sensors using stochastic gradient descent without relying on complex preprocessing or labeled data.

Result: The proposed neural field models demonstrated superior performance in applications like depth estimation, layer separation, and image stitching compared to state-of-the-art methods.

Conclusion: The study concludes that neural fields represent a promising and efficient approach to solving complex inverse problems in mobile photography, expanding its capabilities with less dependency on traditional machine learning techniques.

Abstract: Over the past two decades, mobile imaging has experienced a profound
transformation, with cell phones rapidly eclipsing all other forms of digital
photography in popularity. Today's cell phones are equipped with a diverse
range of imaging technologies - laser depth ranging, multi-focal camera arrays,
and split-pixel sensors - alongside non-visual sensors such as gyroscopes,
accelerometers, and magnetometers. This, combined with on-board integrated
chips for image and signal processing, makes the cell phone a versatile
pocket-sized computational imaging platform. Parallel to this, we have seen in
recent years how neural fields - small neural networks trained to map
continuous spatial input coordinates to output signals - enable the
reconstruction of complex scenes without explicit data representations such as
pixel arrays or point clouds. In this thesis, I demonstrate how carefully
designed neural field models can compactly represent complex geometry and
lighting effects. Enabling applications such as depth estimation, layer
separation, and image stitching directly from collected in-the-wild mobile
photography data. These methods outperform state-of-the-art approaches without
relying on complex pre-processing steps, labeled ground truth data, or machine
learning priors. Instead, they leverage well-constructed, self-regularized
models that tackle challenging inverse problems through stochastic gradient
descent, fitting directly to raw measurements from a smartphone.

</details>


### [93] [Enhancing Construction Site Analysis and Understanding with 3D Segmentation](https://arxiv.org/abs/2508.05922)
*Sri Ramana Saketh Vasanthawada,Pengkun Liu,Pingbo Tang*

Main category: cs.CV

TL;DR: The paper evaluates SAM and Mask3D's performance in construction progress monitoring, highlighting limits in their adaptability to outdoor scenarios.


<details>
  <summary>Details</summary>
Motivation: Improving efficiency and scalability in construction progress monitoring through computer-vision methods.

Method: Comparative evaluation of SAM and Mask3D in real-world construction environments and analyzing their segmentation capabilities.

Result: Weaknesses in current approaches for outdoor segmentation were noted, suggesting SAM and Mask3D need tailored workflows for better performance.

Conclusion: Advancements in automated techniques require improved methods for 3D segmentation in dynamic construction settings.

Abstract: Monitoring construction progress is crucial yet resource-intensive, prompting
the exploration of computer-vision-based methodologies for enhanced efficiency
and scalability. Traditional data acquisition methods, primarily focusing on
indoor environments, falter in construction site's complex, cluttered, and
dynamically changing conditions. This paper critically evaluates the
application of two advanced 3D segmentation methods, Segment Anything Model
(SAM) and Mask3D, in challenging outdoor and indoor conditions. Trained
initially on indoor datasets, both models' adaptability and performance are
assessed in real-world construction settings, highlighting the gap in current
segmentation approaches due to the absence of benchmarks for outdoor scenarios.
Through a comparative analysis, this study not only showcases the relative
effectiveness of SAM and Mask3D but also addresses the critical need for
tailored segmentation workflows capable of extracting actionable insights from
construction site data, thereby advancing the field towards more automated and
precise monitoring techniques.

</details>


### [94] [A 3DGS-Diffusion Self-Supervised Framework for Normal Estimation from a Single Image](https://arxiv.org/abs/2508.05950)
*Yanxing Liang,Yinghui Wang,Jinlong Yang,Wei Li*

Main category: cs.CV

TL;DR: This paper introduces SINGAD, a framework that improves single-image normal estimation by integrating physics-based modeling and a differentiable rendering strategy, addressing inconsistencies and reducing the need for dense annotations.


<details>
  <summary>Details</summary>
Motivation: To address challenges in single-image normal estimation, including multi-view inconsistencies and reliance on dense annotations, by exploiting light-surface interaction modeling.

Method: The SINGAD framework incorporates a 3D Gaussian splatting reparameterization model, geometric feature extraction guided by light transport principles, cross-domain fusion for embedding priors in a diffusion model, and a self-supervised differentiable reprojection loss strategy.

Result: SINGAD achieves consistent multi-view normal estimations and eliminates reliance on annotated datasets. It outperforms existing methods in experimental evaluations on the Google Scanned Objects dataset across various metrics.

Conclusion: SINGAD provides an effective, self-supervised solution for single-image normal estimation using physics-aware modeling and optimized diffusion frameworks.

Abstract: The lack of spatial dimensional information remains a challenge in normal
estimation from a single image. Recent diffusion-based methods have
demonstrated significant potential in 2D-to-3D implicit mapping, they rely on
data-driven statistical priors and miss the explicit modeling of light-surface
interaction, leading to multi-view normal direction conflicts. Moreover, the
discrete sampling mechanism of diffusion models causes gradient discontinuity
in differentiable rendering reconstruction modules, preventing 3D geometric
errors from being backpropagated to the normal generation network, thereby
forcing existing methods to depend on dense normal annotations. This paper
proposes SINGAD, a novel Self-supervised framework from a single Image for
Normal estimation via 3D GAussian splatting guided Diffusion. By integrating
physics-driven light-interaction modeling and a differentiable rendering-based
reprojection strategy, our framework directly converts 3D geometric errors into
normal optimization signals, solving the challenges of multi-view geometric
inconsistency and data dependency. Specifically, the framework constructs a
light-interaction-driven 3DGS reparameterization model to generate multi-scale
geometric features consistent with light transport principles, ensuring
multi-view normal consistency. A cross-domain feature fusion module is designed
within a conditional diffusion model, embedding geometric priors to constrain
normal generation while maintaining accurate geometric error propagation.
Furthermore, a differentiable 3D reprojection loss strategy is introduced for
self-supervised optimization that minimizes geometric error between the
reconstructed and input image, eliminating dependence on annotated normal
datasets. Quantitative evaluations on the Google Scanned Objects dataset
demonstrate that our method outperforms state-of-the-art approaches across
multiple metrics.

</details>


### [95] [Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents](https://arxiv.org/abs/2508.05954)
*Han Lin,Jaemin Cho,Amir Zadeh,Chuan Li,Mohit Bansal*

Main category: cs.CV

TL;DR: Bifrost-1 introduces a unified framework that integrates pretrained multimodal language models (MLLMs) and diffusion models using patch-level CLIP image embeddings, yielding high-quality image synthesis with efficient training.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of integrating high-fidelity visual generation in large language models without weakening their reasoning capabilities or incurring high computational costs during training.

Method: The method involves bridging pretrained MLLMs and diffusion models via patch-level CLIP image embeddings. These embeddings serve as latent variables aligned with the MLLM's visual encoder and are incorporated into the diffusion model using a lightweight adaptation of ControlNet. A visual generation branch is added to the MLLM for predicting these embeddings.

Result: The proposed Bifrost-1 framework achieves comparable or better results than existing methods in terms of image quality and multimodal reasoning, while significantly reducing training compute requirements.

Conclusion: Bifrost-1 demonstrates that seamlessly combining MLLMs and diffusion models with efficient embeddings enables high-quality controllable image generation and preserves multimodal reasoning capabilities, offering an effective and computationally efficient solution.

Abstract: There is growing interest in integrating high-fidelity visual synthesis
capabilities into large language models (LLMs) without compromising their
strong reasoning capabilities. Existing methods that directly train LLMs or
bridge LLMs and diffusion models usually suffer from costly training since the
backbone LLMs have not seen image representations during pretraining. We
present Bifrost-1, a unified framework that bridges pretrained multimodal LLMs
(MLLMs) and diffusion models using patch-level CLIP image embeddings as latent
variables, which are natively aligned with the MLLM's CLIP visual encoder.
These patch-level image embeddings are integrated into the diffusion model with
a lightweight adaptation of its ControlNet. To retain the original multimodal
reasoning capabilities of MLLMs, we equip the MLLM with a visual generation
branch initialized from the original MLLM parameters when predicting the
patch-level image embeddings. By seamlessly integrating pretrained MLLMs and
diffusion models with patch-level CLIP latents, our framework enables
high-fidelity controllable image generation with significant training
efficiency. Our experiments demonstrate that Bifrost-1 achieves comparable or
better performance than previous methods in terms of visual fidelity and
multimodal understanding, with substantially lower compute during training. We
also provide comprehensive ablation studies showing the effectiveness of our
design choices.

</details>


### [96] [PASG: A Closed-Loop Framework for Automated Geometric Primitive Extraction and Semantic Anchoring in Robotic Manipulation](https://arxiv.org/abs/2508.05976)
*Zhihao Zhu,Yifan Zheng,Siyu Pan,Yaohui Jin,Yao Mu*

Main category: cs.CV

TL;DR: The paper presents PASG, a framework for tackling the challenge of linking high-level task semantics with low-level geometric features in robotic manipulation using vision-language models and automatic primitive extraction.


<details>
  <summary>Details</summary>
Motivation: Robotic manipulation struggles with connecting high-level semantic concepts from tasks to detailed geometric features. Current vision-language models face limitations due to their lack of dynamic semantic grounding and reliance on manual annotations.

Method: The proposed PASG framework includes geometric primitive extraction, semantic anchoring using fine-tuned VLMs, and a spatial-semantic reasoning benchmark to enhance object semantic-affordance connections for robotics.

Result: PASG was validated through practical robotic manipulation tasks across varied scenarios, delivering performance similar to methods relying on manual annotations while improving semantic-affordance understanding.

Conclusion: PASG offers a unified approach to bridging geometric and semantic aspects in robotic manipulation, showing promise for improved automated handling of diverse tasks.

Abstract: The fragmentation between high-level task semantics and low-level geometric
features remains a persistent challenge in robotic manipulation. While
vision-language models (VLMs) have shown promise in generating affordance-aware
visual representations, the lack of semantic grounding in canonical spaces and
reliance on manual annotations severely limit their ability to capture dynamic
semantic-affordance relationships. To address these, we propose Primitive-Aware
Semantic Grounding (PASG), a closed-loop framework that introduces: (1)
Automatic primitive extraction through geometric feature aggregation, enabling
cross-category detection of keypoints and axes; (2) VLM-driven semantic
anchoring that dynamically couples geometric primitives with functional
affordances and task-relevant description; (3) A spatial-semantic reasoning
benchmark and a fine-tuned VLM (Qwen2.5VL-PA). We demonstrate PASG's
effectiveness in practical robotic manipulation tasks across diverse scenarios,
achieving performance comparable to manual annotations. PASG achieves a
finer-grained semantic-affordance understanding of objects, establishing a
unified paradigm for bridging geometric primitives with task semantics in
robotic manipulation.

</details>


### [97] [AnimateScene: Camera-controllable Animation in Any Scene](https://arxiv.org/abs/2508.05982)
*Qingyang Liu,Bingjie Gao,Weiheng Huang,Jun Zhang,Zhongqian Sun,Yang Wei,Zelin Peng,Qianli Ma,Shuai Yang,Zhaohe Liao,Haonan Zhao,Li Niu*

Main category: cs.CV

TL;DR: AnimateScene integrates 4D human animation with 3D scene reconstruction, addressing challenges like human placement, style inconsistency, and camera trajectory generation.


<details>
  <summary>Details</summary>
Motivation: To seamlessly integrate 4D human animation with 3D scene reconstruction for visually engaging videos while overcoming challenges like correct human placement, lighting inconsistencies, and camera motion.

Method: Proposes a unified framework with modules for automatic human placement, training-free style alignment, and joint post-reconstruction to handle camera trajectories.

Result: Generates visually dynamic scene videos with high detail and coherence across various camera and action setups.

Conclusion: AnimateScene improves 4D human animation within 3D reconstructed scenes, providing a cohesive and dynamic visual experience.

Abstract: 3D scene reconstruction and 4D human animation have seen rapid progress and
broad adoption in recent years. However, seamlessly integrating reconstructed
scenes with 4D human animation to produce visually engaging results remains
challenging. One key difficulty lies in placing the human at the correct
location and scale within the scene while avoiding unrealistic
interpenetration. Another challenge is that the human and the background may
exhibit different lighting and style, leading to unrealistic composites. In
addition, appealing character motion videos are often accompanied by camera
movements, which means that the viewpoints need to be reconstructed along a
specified trajectory. We present AnimateScene, which addresses the above issues
in a unified framework. First, we design an accurate placement module that
automatically determines a plausible 3D position for the human and prevents any
interpenetration within the scene during motion. Second, we propose a
training-free style alignment method that adapts the 4D human representation to
match the background's lighting and style, achieving coherent visual
integration. Finally, we design a joint post-reconstruction method for both the
4D human and the 3D scene that allows camera trajectories to be inserted,
enabling the final rendered video to feature visually appealing camera
movements. Extensive experiments show that AnimateScene generates dynamic scene
videos with high geometric detail and spatiotemporal coherence across various
camera and action combinations.

</details>


### [98] [ETA: Energy-based Test-time Adaptation for Depth Completion](https://arxiv.org/abs/2508.05989)
*Younjoon Chung,Hyoungseob Park,Patrick Rim,Xiaoran Zhang,Jihe He,Ziyao Zeng,Safa Cicek,Byung-Woo Hong,James S. Duncan,Alex Wong*

Main category: cs.CV

TL;DR: The method introduces Energy-based Test-time Adaptation (ETA) to address the issue of covariate shift in pretrained depth completion models by aligning test-time predictions with the source distribution using adversarial perturbations.


<details>
  <summary>Details</summary>
Motivation: Pretrained depth completion models struggle with covariate shifts when applied to data from novel environments, leading to inaccurate predictions.

Method: The method uses adversarial perturbations to explore data space and train an energy model that scores depth predictions as in- or out-of-distribution. At test time, model parameters are updated to minimize the energy and align predictions with the source distribution.

Result: ETA improves over the previous state-of-the-art method by an average of 6.94% in outdoor datasets and 10.23% in indoor datasets.

Conclusion: Energy-based Test-time Adaptation effectively mitigates covariate shift in depth completion models, enhancing their accuracy across diverse environments.

Abstract: We propose a method for test-time adaptation of pretrained depth completion
models. Depth completion models, trained on some ``source'' data, often predict
erroneous outputs when transferred to ``target'' data captured in novel
environmental conditions due to a covariate shift. The crux of our method lies
in quantifying the likelihood of depth predictions belonging to the source data
distribution. The challenge is in the lack of access to out-of-distribution
(target) data prior to deployment. Hence, rather than making assumptions
regarding the target distribution, we utilize adversarial perturbations as a
mechanism to explore the data space. This enables us to train an energy model
that scores local regions of depth predictions as in- or out-of-distribution.
We update the parameters of pretrained depth completion models at test time to
minimize energy, effectively aligning test-time predictions to those of the
source distribution. We call our method ``Energy-based Test-time Adaptation'',
or ETA for short. We evaluate our method across three indoor and three outdoor
datasets, where ETA improve over the previous state-of-the-art method by an
average of 6.94% for outdoors and 10.23% for indoors. Project Page:
https://fuzzythecat.github.io/eta.

</details>


### [99] [Fast Motion Estimation and Context-Aware Refinement for Efficient Bayer-Domain Video Vision](https://arxiv.org/abs/2508.05990)
*Haichao Wang,Xinyue Xi,Jiangtao Wen,Yuxing Han*

Main category: cs.CV

TL;DR: This paper introduces an efficient video computer vision system that reduces temporal redundancy and eliminates front-end computation overhead, achieving faster performance with minimal accuracy trade-off.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in achieving efficient video computer vision due to high temporal redundancy in videos and underutilized approaches to reduce computation overhead.

Method: The method includes removing the image signal processor, directly inputting Bayer-format data, using a block matching-based motion estimation algorithm with MV refinement, introducing a context-aware block refinement network, and employing a frame selection strategy.

Result: Experiments across multiple video vision tasks show significant speed-up while incurring only slight performance loss.

Conclusion: The proposed system effectively addresses inefficiencies in video computer vision by optimizing both front-end and motion estimation processes, balancing accuracy with computational efficiency.

Abstract: The efficiency of video computer vision system remains a challenging task due
to the high temporal redundancy inside a video. Existing works have been
proposed for efficient vision computer vision. However, they do not fully
reduce the temporal redundancy and neglect the front end computation overhead.
In this paper, we propose an efficient video computer vision system. First,
image signal processor is removed and Bayer-format data is directly fed into
video computer vision models, thus saving the front end computation. Second,
instead of optical flow models and video codecs, a fast block matching-based
motion estimation algorithm is proposed specifically for efficient video
computer vision, with a MV refinement module. To correct the error,
context-aware block refinement network is introduced to refine regions with
large error. To further balance the accuracy and efficiency, a frame selection
strategy is employed. Experiments on multiple video computer vision tasks
demonstrate that our method achieves significant acceleration with slight
performance loss.

</details>


### [100] [ECMF: Enhanced Cross-Modal Fusion for Multimodal Emotion Recognition in MER-SEMI Challenge](https://arxiv.org/abs/2508.05991)
*Juewen Hu,Yexin Li,Jiulin Li,Shuo Chen,Pring Wong*

Main category: cs.CV

TL;DR: The study proposes a novel multimodal framework for emotion recognition, leveraging advanced pre-trained models to improve feature extraction across visual, audio, and textual modalities.


<details>
  <summary>Details</summary>
Motivation: To improve human-computer interaction by addressing challenges in multimodal emotion recognition, especially issues related to data scarcity.

Method: The framework includes a dual-branch visual encoder for capturing diverse features, a context-enriched textual method using large language models, a fusion strategy with self-attention mechanisms and residual connections, and a multi-source labeling strategy to refine noisy training labels.

Result: The proposed framework achieved a significant performance improvement, reaching a weighted F-score of 87.49% on the MER2025-SEMI dataset, compared to the baseline score of 78.63%.

Conclusion: The study demonstrates the effectiveness of leveraging pre-trained models, advanced feature extraction methods, and innovative fusion strategies for improved emotion recognition in multimodal settings.

Abstract: Emotion recognition plays a vital role in enhancing human-computer
interaction. In this study, we tackle the MER-SEMI challenge of the MER2025
competition by proposing a novel multimodal emotion recognition framework. To
address the issue of data scarcity, we leverage large-scale pre-trained models
to extract informative features from visual, audio, and textual modalities.
Specifically, for the visual modality, we design a dual-branch visual encoder
that captures both global frame-level features and localized facial
representations. For the textual modality, we introduce a context-enriched
method that employs large language models to enrich emotional cues within the
input text. To effectively integrate these multimodal features, we propose a
fusion strategy comprising two key components, i.e., self-attention mechanisms
for dynamic modality weighting, and residual connections to preserve original
representations. Beyond architectural design, we further refine noisy labels in
the training set by a multi-source labeling strategy. Our approach achieves a
substantial performance improvement over the official baseline on the
MER2025-SEMI dataset, attaining a weighted F-score of 87.49% compared to
78.63%, thereby validating the effectiveness of the proposed framework.

</details>


### [101] [EvoMakeup: High-Fidelity and Controllable Makeup Editing with MakeupQuad](https://arxiv.org/abs/2508.05994)
*Huadong Wu,Yi Fu,Yunhao Li,Yuan Gao,Kang Du*

Main category: cs.CV

TL;DR: The paper introduces a new dataset (MakeupQuad) and framework (EvoMakeup) for high-quality facial makeup editing, excelling in maintaining identity and makeup fidelity.


<details>
  <summary>Details</summary>
Motivation: Current methods for makeup transfer fail to produce high-quality results due to the lack of structured paired data that preserves identity and makeup fidelity.

Method: The authors developed MakeupQuad, a large-scale dataset with diverse inputs, and EvoMakeup, a training framework that iteratively improves data and model quality.

Result: EvoMakeup, trained only on synthetic data, excels at makeup fidelity and identity preservation across various tasks, outperforming previous methods on real-world benchmarks.

Conclusion: The proposed dataset and framework enable superior, flexible, and high-fidelity makeup editing. Future code and data release will support further research.

Abstract: Facial makeup editing aims to realistically transfer makeup from a reference
to a target face. Existing methods often produce low-quality results with
coarse makeup details and struggle to preserve both identity and makeup
fidelity, mainly due to the lack of structured paired data -- where source and
result share identity, and reference and result share identical makeup. To
address this, we introduce MakeupQuad, a large-scale, high-quality dataset with
non-makeup faces, references, edited results, and textual makeup descriptions.
Building on this, we propose EvoMakeup, a unified training framework that
mitigates image degradation during multi-stage distillation, enabling iterative
improvement of both data and model quality. Although trained solely on
synthetic data, EvoMakeup generalizes well and outperforms prior methods on
real-world benchmarks. It supports high-fidelity, controllable, multi-task
makeup editing -- including full-face and partial reference-based editing, as
well as text-driven makeup editing -- within a single model. Experimental
results demonstrate that our method achieves superior makeup fidelity and
identity preservation, effectively balancing both aspects. Code and dataset
will be released upon acceptance.

</details>


### [102] [MathReal: We Keep It Real! A Real Scene Benchmark for Evaluating Math Reasoning in Multimodal Large Language Models](https://arxiv.org/abs/2508.06009)
*Jun Feng,Zixin Wang,Zhentao Zhang,Yue Guo,Zhihan Zhou,Xiuyi Chen,Zhenyang Li,Dawei Yin*

Main category: cs.CV

TL;DR: The paper introduces MathReal, a dataset of real-world educational math problems with images to evaluate MLLMs' reasoning abilities, revealing significant challenges in realistic contexts.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for visual mathematical reasoning are based on clean or processed modalities, lacking real-world scenarios provided by K-12 educational users.

Method: The authors created the MathReal dataset with 2,000 real math questions captured via mobile devices, categorized images into 14 subcategories of challenges, and evaluated MLLMs in six experimental settings.

Result: MLLMs show significant performance limitations when tested in realistic educational contexts, with challenges identified across recognition, comprehension, and reasoning.

Conclusion: MathReal dataset highlights the need for improved algorithms to enhance MLLMs' real-world problem-solving capabilities, driving future research focus.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in visual mathematical reasoning across various existing
benchmarks. However, these benchmarks are predominantly based on clean or
processed multimodal inputs, without incorporating the images provided by
real-world Kindergarten through 12th grade (K-12) educational users. To address
this gap, we introduce MathReal, a meticulously curated dataset comprising
2,000 mathematical questions with images captured by handheld mobile devices in
authentic scenarios. Each question is an image, containing the question text
and visual element. We systematically classify the real images into three
primary categories: image quality degradation, perspective variation, and
irrelevant content interference, which are further delineated into 14
subcategories. Additionally, MathReal spans five core knowledge and ability
categories, which encompass three question types and are divided into three
difficulty levels. To comprehensively evaluate the multimodal mathematical
reasoning abilities of state-of-the-art MLLMs in real-world scenarios, we
design six experimental settings that enable a systematic analysis of their
performance. Through extensive experimentation, we find that the
problem-solving abilities of existing MLLMs are significantly challenged in
realistic educational contexts. Based on this, we conduct a thorough analysis
of their performance and error patterns, providing insights into their
recognition, comprehension, and reasoning capabilities, and outlining
directions for future improvements. Data and code:
https://github.com/junfeng0288/MathReal.

</details>


### [103] [ExploreGS: Explorable 3D Scene Reconstruction with Virtual Camera Samplings and Diffusion Priors](https://arxiv.org/abs/2508.06014)
*Minsu Kim,Subin Jeon,In Cho,Mijin Yoo,Seon Joo Kim*

Main category: cs.CV

TL;DR: The paper proposes an improved pipeline for 3D Gaussian Splatting (3DGS) to address rendering artifacts and missing regions during novel view synthesis (NVS), enabling better reconstruction and scene exploration.


<details>
  <summary>Details</summary>
Motivation: Current 3DGS methods struggle with artifacts and incomplete renderings when viewing scenes from perspectives outside the training viewpoints, which hinders seamless exploration.

Method: The authors propose a pipeline using an information-gain-driven virtual camera placement strategy to improve scene coverage and video diffusion priors to enhance rendering quality. They also fine-tune 3D Gaussians with the augmented views.

Result: The proposed method outperforms existing 3DGS approaches in rendering quality and artifact reduction, as validated on the new Wild-Explore benchmark designed for challenging scene exploration.

Conclusion: This improved 3DGS pipeline facilitates high-quality, artifact-free rendering from arbitrary viewpoints, providing a significant improvement in novel view synthesis for complex scenes.

Abstract: Recent advances in novel view synthesis (NVS) have enabled real-time
rendering with 3D Gaussian Splatting (3DGS). However, existing methods struggle
with artifacts and missing regions when rendering from viewpoints that deviate
from the training trajectory, limiting seamless scene exploration. To address
this, we propose a 3DGS-based pipeline that generates additional training views
to enhance reconstruction. We introduce an information-gain-driven virtual
camera placement strategy to maximize scene coverage, followed by video
diffusion priors to refine rendered results. Fine-tuning 3D Gaussians with
these enhanced views significantly improves reconstruction quality. To evaluate
our method, we present Wild-Explore, a benchmark designed for challenging scene
exploration. Experiments demonstrate that our approach outperforms existing
3DGS-based methods, enabling high-quality, artifact-free rendering from
arbitrary viewpoints.
  https://exploregs.github.io

</details>


### [104] [Improved Sub-Visible Particle Classification in Flow Imaging Microscopy via Generative AI-Based Image Synthesis](https://arxiv.org/abs/2508.06021)
*Utku Ozbulak,Michaela Cohrs,Hristo L. Svilenov,Joris Vankerschaver,Wesley De Neve*

Main category: cs.CV

TL;DR: This paper presents a diffusion model to generate high-fidelity images to address data imbalance in sub-visible particle analysis, improving multi-class classifier performance.


<details>
  <summary>Details</summary>
Motivation: The research aims to tackle data scarcity and class imbalance issues in sub-visible particle analysis, which hinder the effectiveness of multi-class classifiers, especially for underrepresented types like silicone oil and air bubbles.

Method: The study develops a state-of-the-art diffusion model to generate synthetic particle images to augment training datasets, validating the generated images' quality through visual resemblance and structural fidelity.

Result: The diffusion-generated images improved classification performance when applied to a large validation dataset of 500,000 protein particle images, with no significant drawbacks.

Conclusion: The use of diffusion models effectively addresses data imbalance, enhancing deep learning classifier performance for sub-visible particle analysis, and the tools are released publicly for broader accessibility.

Abstract: Sub-visible particle analysis using flow imaging microscopy combined with
deep learning has proven effective in identifying particle types, enabling the
distinction of harmless components such as silicone oil from protein particles.
However, the scarcity of available data and severe imbalance between particle
types within datasets remain substantial hurdles when applying multi-class
classifiers to such problems, often forcing researchers to rely on less
effective methods. The aforementioned issue is particularly challenging for
particle types that appear unintentionally and in lower numbers, such as
silicone oil and air bubbles, as opposed to protein particles, where obtaining
large numbers of images through controlled settings is comparatively
straightforward. In this work, we develop a state-of-the-art diffusion model to
address data imbalance by generating high-fidelity images that can augment
training datasets, enabling the effective training of multi-class deep neural
networks. We validate this approach by demonstrating that the generated samples
closely resemble real particle images in terms of visual quality and structure.
To assess the effectiveness of using diffusion-generated images in training
datasets, we conduct large-scale experiments on a validation dataset comprising
500,000 protein particle images and demonstrate that this approach improves
classification performance with no negligible downside. Finally, to promote
open research and reproducibility, we publicly release both our diffusion
models and the trained multi-class deep neural network classifiers, along with
a straightforward interface for easy integration into future studies, at
https://github.com/utkuozbulak/svp-generative-ai.

</details>


### [105] [Learning 3D Texture-Aware Representations for Parsing Diverse Human Clothing and Body Parts](https://arxiv.org/abs/2508.06032)
*Kiran Chhatre,Christopher Peters,Srikrishna Karanam*

Main category: cs.CV

TL;DR: Spectrum proposes an advanced tool for detailed human parsing by combining body parts and clothing into finely grained segmentation and instance grouping using repurposed 3D texture-based diffusion models.


<details>
  <summary>Details</summary>
Motivation: Existing human parsing methods lack granularity in clothing types and body parts, and recent diffusion-based models fail to distinguish diverse clothing and detailed body parts.

Method: Spectrum repurposes an Image-to-Texture (I2Tx) diffusion model fine-tuned on 3D human texture maps, extracting internal features for pixel parsing and aligning them with diverse clothing categories through prompt-guided grounding.

Result: Spectrum achieves improved semantic segmentation outputs for body parts and various clothing categories in multi-human scenarios, outperforming baseline methods on cross-dataset evaluations.

Conclusion: This study offers a novel and effective solution for fine-grained human parsing by leveraging specialized features from 3D texture-based diffusion models, enhancing both segmentation accuracy and generality.

Abstract: Existing methods for human parsing into body parts and clothing often use
fixed mask categories with broad labels that obscure fine-grained clothing
types. Recent open-vocabulary segmentation approaches leverage pretrained
text-to-image (T2I) diffusion model features for strong zero-shot transfer, but
typically group entire humans into a single person category, failing to
distinguish diverse clothing or detailed body parts. To address this, we
propose Spectrum, a unified network for part-level pixel parsing (body parts
and clothing) and instance-level grouping. While diffusion-based
open-vocabulary models generalize well across tasks, their internal
representations are not specialized for detailed human parsing. We observe
that, unlike diffusion models with broad representations, image-driven 3D
texture generators maintain faithful correspondence to input images, enabling
stronger representations for parsing diverse clothing and body parts. Spectrum
introduces a novel repurposing of an Image-to-Texture (I2Tx) diffusion model --
obtained by fine-tuning a T2I model on 3D human texture maps -- for improved
alignment with body parts and clothing. From an input image, we extract
human-part internal features via the I2Tx diffusion model and generate
semantically valid masks aligned to diverse clothing categories through
prompt-guided grounding. Once trained, Spectrum produces semantic segmentation
maps for every visible body part and clothing category, ignoring standalone
garments or irrelevant objects, for any number of humans in the scene. We
conduct extensive cross-dataset experiments -- separately assessing body parts,
clothing parts, unseen clothing categories, and full-body masks -- and
demonstrate that Spectrum consistently outperforms baseline methods in
prompt-based segmentation.

</details>


### [106] [InstantEdit: Text-Guided Few-Step Image Editing with Piecewise Rectified Flow](https://arxiv.org/abs/2508.06033)
*Yiming Gong,Zhen Zhu,Minjia Zhang*

Main category: cs.CV

TL;DR: The paper introduces InstantEdit, a fast few-step text-guided image editing method using RectifiedFlow, delivering consistent and superior results by introducing novel techniques like PerRFI, Inversion Latent Injection, and Disentangled Prompt Guidance.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this research is to enable faster and more accurate text-guided image editing while maintaining critical image content and generating highly coherent outputs.

Method: The authors developed the InstantEdit framework, which includes innovations such as PerRFI for improved inversion, Inversion Latent Injection for detailed regeneration, and Disentangled Prompt Guidance. They also integrated a Canny-conditioned ControlNet to introduce structural consistency and minimize artifacts.

Result: InstantEdit performed better both qualitatively and quantitatively on the PIE image editing dataset, surpassing other state-of-the-art methods for few-step image editing tasks.

Conclusion: InstantEdit demonstrates that fast text-guided image editing with improved structural coherence and quality can be achieved through advancements in RectifiedFlow and the proposed novel techniques.

Abstract: We propose a fast text-guided image editing method called InstantEdit based
on the RectifiedFlow framework, which is structured as a few-step editing
process that preserves critical content while following closely to textual
instructions. Our approach leverages the straight sampling trajectories of
RectifiedFlow by introducing a specialized inversion strategy called PerRFI. To
maintain consistent while editable results for RectifiedFlow model, we further
propose a novel regeneration method, Inversion Latent Injection, which
effectively reuses latent information obtained during inversion to facilitate
more coherent and detailed regeneration. Additionally, we propose a
Disentangled Prompt Guidance technique to balance editability with detail
preservation, and integrate a Canny-conditioned ControlNet to incorporate
structural cues and suppress artifacts. Evaluation on the PIE image editing
dataset demonstrates that InstantEdit is not only fast but also achieves better
qualitative and quantitative results compared to state-of-the-art few-step
editing methods.

</details>


### [107] [More Is Better: A MoE-Based Emotion Recognition Framework with Human Preference Alignment](https://arxiv.org/abs/2508.06036)
*Jun Xie,Yingjian Zhu,Feng Chen,Zhenghao Zhang,Xiaohui Fan,Hongzhu Yi,Xinming Wang,Chen Yu,Yue Bi,Zhaoran Zhao,Xiongjun Guan,Zhepeng Wang*

Main category: cs.CV

TL;DR: The paper proposes a semi-supervised learning framework for Mixture of Experts (MoE) emotion recognition, achieving an F1-score of 0.8772 and ranking 2nd in the MER2025-SEMI challenge.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve emotion recognition through a robust and diverse mixture of expert systems while effectively using unlabeled data in a semi-supervised learning context.

Method: The paper's method includes integrating multiple input modalities as experts (e.g., Vision-Language Model knowledge and Action Unit information), pseudo-labeling through a consensus-based strategy, two-stage training, and multi-expert ensemble voting with rule-based re-ranking.

Result: The proposed method achieved an F1-score of 0.8772 on the MER2025-SEMI challenge dataset, ranking 2nd in the competition.

Conclusion: The proposed comprehensive framework demonstrates the effectiveness of leveraging diverse modalities, pseudo-labeling, and ensemble strategies in semi-supervised emotion recognition tasks.

Abstract: In this paper, we present our solution for the semi-supervised learning track
(MER-SEMI) in MER2025. We propose a comprehensive framework, grounded in the
principle that "more is better," to construct a robust Mixture of Experts (MoE)
emotion recognition system. Our approach integrates a diverse range of input
modalities as independent experts, including novel signals such as knowledge
from large Vision-Language Models (VLMs) and temporal Action Unit (AU)
information. To effectively utilize unlabeled data, we introduce a
consensus-based pseudo-labeling strategy, generating high-quality labels from
the agreement between a baseline model and Gemini, which are then used in a
two-stage training paradigm. Finally, we employ a multi-expert voting ensemble
combined with a rule-based re-ranking process to correct prediction bias and
better align the outputs with human preferences. Evaluated on the MER2025-SEMI
challenge dataset, our method achieves an F1-score of 0.8772 on the test set,
ranking 2nd in the track. Our code is available at
https://github.com/zhuyjan/MER2025-MRAC25.

</details>


### [108] [GMF-Drive: Gated Mamba Fusion with Spatial-Aware BEV Representation for End-to-End Autonomous Driving](https://arxiv.org/abs/2508.06113)
*Jian Wang,Chaokang Jiang,Haitao Xu*

Main category: cs.CV

TL;DR: GMF-Drive introduces a more efficient and spatially-aware framework for autonomous driving by replacing transformers with state-space models, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of transformer-based fusion in autonomous driving, such as computational inefficiency and inadequate representation of spatial dependencies in BEV formats.

Method: Introduces GMF-Drive framework combining geometrically-augmented pillar format for LiDAR data and a hierarchical gated mamba fusion architecture using a spatially-aware state-space model (BEV-SSM) with linear complexity.

Result: GMF-Drive outperforms DiffusionDrive and achieves state-of-the-art results on the NAVSIM benchmark, validated by extensive experiments and ablation studies.

Conclusion: Task-specific state-space models (SSMs) can offer superior performance and efficiency over general-purpose transformers in autonomous driving applications.

Abstract: Diffusion-based models are redefining the state-of-the-art in end-to-end
autonomous driving, yet their performance is increasingly hampered by a
reliance on transformer-based fusion. These architectures face fundamental
limitations: quadratic computational complexity restricts the use of
high-resolution features, and a lack of spatial priors prevents them from
effectively modeling the inherent structure of Bird's Eye View (BEV)
representations. This paper introduces GMF-Drive (Gated Mamba Fusion for
Driving), an end-to-end framework that overcomes these challenges through two
principled innovations. First, we supersede the information-limited
histogram-based LiDAR representation with a geometrically-augmented pillar
format encoding shape descriptors and statistical features, preserving critical
3D geometric details. Second, we propose a novel hierarchical gated mamba
fusion (GM-Fusion) architecture that substitutes an expensive transformer with
a highly efficient, spatially-aware state-space model (SSM). Our core BEV-SSM
leverages directional sequencing and adaptive fusion mechanisms to capture
long-range dependencies with linear complexity, while explicitly respecting the
unique spatial properties of the driving scene. Extensive experiments on the
challenging NAVSIM benchmark demonstrate that GMF-Drive achieves a new
state-of-the-art performance, significantly outperforming DiffusionDrive.
Comprehensive ablation studies validate the efficacy of each component,
demonstrating that task-specific SSMs can surpass a general-purpose transformer
in both performance and efficiency for autonomous driving.

</details>


### [109] [Fourier-VLM: Compressing Vision Tokens in the Frequency Domain for Large Vision-Language Models](https://arxiv.org/abs/2508.06038)
*Huanyu Wang,Jushi Kai,Haoli Bai,Lu Hou,Bo Jiang,Ziwei He,Zhouhan Lin*

Main category: cs.CV

TL;DR: Fourier-VLM reduces computational overhead in Vision-Language Models by compressing visual features in the frequency domain, achieving up to 83.8% inference FLOPs reduction and 31.2% faster generation compared to similar models.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address the high computational overhead and latency caused by the large number of vision tokens in Vision-Language Models.

Method: It proposes compressing visual representations using a low-pass filter and a two-dimensional Discrete Cosine Transform (DCT), executed efficiently via the Fast Fourier Transform (FFT) operator.

Result: Fourier-VLM demonstrates competitive performance and strong generalizability on image-based benchmarks while significantly reducing inference FLOPs and increasing generation speed.

Conclusion: Fourier-VLM offers a practical and efficient way to handle visual features in Vision-Language Models, balancing performance and computational efficiency without adding extra parameters.

Abstract: Vision-Language Models (VLMs) typically replace the predefined image
placeholder token (<image>) in textual instructions with visual features from
an image encoder, forming the input to a backbone Large Language Model (LLM).
However, the large number of vision tokens significantly increases the context
length, leading to high computational overhead and inference latency. While
previous efforts mitigate this by selecting only important visual features or
leveraging learnable queries to reduce token count, they often compromise
performance or introduce substantial extra costs. In response, we propose
Fourier-VLM, a simple yet efficient method that compresses visual
representations in the frequency domain. Our approach is motivated by the
observation that vision features output from the vision encoder exhibit
concentrated energy in low-frequency components. Leveraging this, we apply a
low-pass filter to the vision features using a two-dimentional Discrete Cosine
Transform (DCT). Notably, the DCT is efficiently computed via the Fast Fourier
Transform (FFT) operator with a time complexity of $\mathcal{O}(n\log n)$,
minimizing the extra computational cost while introducing no additional
parameters. Extensive experiments across various image-based benchmarks
demonstrate that Fourier-VLM achieves competitive performance with strong
generalizability across both LLaVA and Qwen-VL architectures. Crucially, it
reduce inference FLOPs by up to 83.8% and boots generation speed by 31.2%
compared to LLaVA-v1.5, highlighting the superior efficiency and practicality.

</details>


### [110] [Graph-based Robot Localization Using a Graph Neural Network with a Floor Camera and a Feature Rich Industrial Floor](https://arxiv.org/abs/2508.06177)
*Dominik Brämer,Diana Kleingarn,Oliver Urbann*

Main category: cs.CV

TL;DR: This paper presents a localization framework for robotic navigation using floor graph characteristics and Graph Convolutional Networks, achieving high accuracy (0.64cm error) and addressing the kidnapped robot problem effectively.


<details>
  <summary>Details</summary>
Motivation: Current localization methods like Lidar or QR-code systems face scalability and adaptability issues in complex environments.

Method: The framework represents floor features using graphs and utilizes Graph Convolutional Networks for accurate and efficient robot localization.

Result: The method achieved a low localization error of 0.64cm and solved the kidnapped robot problem without complicated filtering processes.

Conclusion: The proposed approach significantly improves robotic navigation accuracy and efficiency, presenting new opportunities for operation in diverse environments.

Abstract: Accurate localization represents a fundamental challenge in
  robotic navigation. Traditional methodologies, such as Lidar or QR-code based
systems, suffer from inherent scalability and adaptability con straints,
particularly in complex environments. In this work, we propose
  an innovative localization framework that harnesses flooring characteris tics
by employing graph-based representations and Graph Convolutional
  Networks (GCNs). Our method uses graphs to represent floor features,
  which helps localize the robot more accurately (0.64cm error) and more
  efficiently than comparing individual image features. Additionally, this
  approach successfully addresses the kidnapped robot problem in every
  frame without requiring complex filtering processes. These advancements
  open up new possibilities for robotic navigation in diverse environments.

</details>


### [111] [NEP: Autoregressive Image Editing via Next Editing Token Prediction](https://arxiv.org/abs/2508.06044)
*Huimin Wu,Xiaojian Ma,Haozhe Zhao,Yanpeng Zhao,Qing Li*

Main category: cs.CV

TL;DR: The paper introduces a method for text-guided image editing that minimizes computational costs and unintended changes to non-editing regions.


<details>
  <summary>Details</summary>
Motivation: Existing image editing methods are computationally expensive and prone to unintentionally altering non-editing regions.

Method: The method is based on autoregressive Next Editing-token Prediction (NEP), pre-training a text-to-image (T2I) model for selective region edits.

Result: The proposed method achieves state-of-the-art results in image editing benchmarks with support for test-time scaling for refinement.

Conclusion: Formulating image editing as NEP enables precise, efficient edits with zero-shot capability, improving flexibility and edit quality.

Abstract: Text-guided image editing involves modifying a source image based on a
language instruction and, typically, requires changes to only small local
regions. However, existing approaches generate the entire target image rather
than selectively regenerate only the intended editing areas. This results in
(1) unnecessary computational costs and (2) a bias toward reconstructing
non-editing regions, which compromises the quality of the intended edits. To
resolve these limitations, we propose to formulate image editing as Next
Editing-token Prediction (NEP) based on autoregressive image generation, where
only regions that need to be edited are regenerated, thus avoiding unintended
modification to the non-editing areas. To enable any-region editing, we propose
to pre-train an any-order autoregressive text-to-image (T2I) model. Once
trained, it is capable of zero-shot image editing and can be easily adapted to
NEP for image editing, which achieves a new state-of-the-art on widely used
image editing benchmarks. Moreover, our model naturally supports test-time
scaling (TTS) through iteratively refining its generation in a zero-shot
manner. The project page is: https://nep-bigai.github.io/

</details>


### [112] [Depth Jitter: Seeing through the Depth](https://arxiv.org/abs/2508.06227)
*Md Sazidur Rahman,David Cabecinhas,Ricard Marxer*

Main category: cs.CV

TL;DR: The paper presents Depth-Jitter, a depth-aware augmentation method improving model robustness and generalization in depth-sensitive tasks.


<details>
  <summary>Details</summary>
Motivation: Existing augmentation techniques lack depth-aware transformations, limiting model robustness in scenarios involving real-world depth variations.

Method: Depth-Jitter uses adaptive depth offsetting guided by depth variance thresholds to create synthetic depth perturbations while maintaining structural coherence.

Result: Depth-Jitter improves model stability and generalization on benchmark datasets like FathomNet and UTDAC2020, though it doesn't always surpass conventional methods in absolute performance.

Conclusion: Depth-Jitter shows promise for depth-sensitive environments, providing a novel approach to depth-aware augmentation that lays the groundwork for future research.

Abstract: Depth information is essential in computer vision, particularly in underwater
imaging, robotics, and autonomous navigation. However, conventional
augmentation techniques overlook depth aware transformations, limiting model
robustness in real world depth variations. In this paper, we introduce
Depth-Jitter, a novel depth-based augmentation technique that simulates natural
depth variations to improve generalization. Our approach applies adaptive depth
offsetting, guided by depth variance thresholds, to generate synthetic depth
perturbations while preserving structural integrity. We evaluate Depth-Jitter
on two benchmark datasets, FathomNet and UTDAC2020 demonstrating its impact on
model stability under diverse depth conditions. Extensive experiments compare
Depth-Jitter against traditional augmentation strategies such as ColorJitter,
analyzing performance across varying learning rates, encoders, and loss
functions. While Depth-Jitter does not always outperform conventional methods
in absolute performance, it consistently enhances model stability and
generalization in depth-sensitive environments. These findings highlight the
potential of depth-aware augmentation for real-world applications and provide a
foundation for further research into depth-based learning strategies. The
proposed technique is publicly available to support advancements in depth-aware
augmentation. The code is publicly available on
\href{https://github.com/mim-team/Depth-Jitter}{github}.

</details>


### [113] [VQAThinker: Exploring Generalizable and Explainable Video Quality Assessment via Reinforcement Learning](https://arxiv.org/abs/2508.06051)
*Linhan Cao,Wei Sun,Weixia Zhang,Xiangyang Zhu,Jun Jia,Kaiwei Zhang,Dandan Zhu,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: The paper introduces VQAThinker, a reasoning-based framework for video quality assessment, that uses reinforcement learning to improve generalization and explainability, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To overcome poor generalization to out-of-distribution videos and limited explainability in existing video quality assessment (VQA) models, both of which hinder their real-world applicability.

Method: A framework called VQAThinker is proposed, leveraging large multimodal models (LMMs) and group relative policy optimization (GRPO), a reinforcement learning technique. It utilizes three VQA-specific rewards: bell-shaped regression reward, pairwise ranking reward, and temporal consistency reward.

Result: The proposed framework achieves state-of-the-art performance on both in-domain and out-of-distribution VQA benchmarks. It also outperforms existing models in video quality understanding tasks, demonstrating improved distortion attribution and quality description.

Conclusion: Reinforcement learning emerges as a viable approach to building generalizable and explainable VQA models, using only score-level supervision.

Abstract: Video quality assessment (VQA) aims to objectively quantify perceptual
quality degradation in alignment with human visual perception. Despite recent
advances, existing VQA models still suffer from two critical limitations:
\textit{poor generalization to out-of-distribution (OOD) videos} and
\textit{limited explainability}, which restrict their applicability in
real-world scenarios. To address these challenges, we propose
\textbf{VQAThinker}, a reasoning-based VQA framework that leverages large
multimodal models (LMMs) with reinforcement learning to jointly model video
quality understanding and scoring, emulating human perceptual decision-making.
Specifically, we adopt group relative policy optimization (GRPO), a rule-guided
reinforcement learning algorithm that enables reasoning over video quality
under score-level supervision, and introduce three VQA-specific rewards: (1) a
\textbf{bell-shaped regression reward} that increases rapidly as the prediction
error decreases and becomes progressively less sensitive near the ground truth;
(2) a \textbf{pairwise ranking reward} that guides the model to correctly
determine the relative quality between video pairs; and (3) a \textbf{temporal
consistency reward} that encourages the model to prefer temporally coherent
videos over their perturbed counterparts. Extensive experiments demonstrate
that VQAThinker achieves state-of-the-art performance on both in-domain and OOD
VQA benchmarks, showing strong generalization for video quality scoring.
Furthermore, evaluations on video quality understanding tasks validate its
superiority in distortion attribution and quality description compared to
existing explainable VQA models and LMMs. These findings demonstrate that
reinforcement learning offers an effective pathway toward building
generalizable and explainable VQA models solely with score-level supervision.

</details>


### [114] [LV-Net: Anatomy-aware lateral ventricle shape modeling with a case study on Alzheimer's disease, the Australian Imaging Biomarkers and Lifestyle flagship study of ageing](https://arxiv.org/abs/2508.06055)
*Wonjung Park,Suhyun Ahn,Jinah Park*

Main category: cs.CV

TL;DR: This paper presents LV-Net, a robust 3D shape modeling framework for lateral ventricles in the brain, addressing challenges in variability and imaging limitations, and applies it to Alzheimer's disease analysis.


<details>
  <summary>Details</summary>
Motivation: To address challenges in lateral ventricle shape analysis, such as individual variability and MRI segmentation difficulties, and to enhance its potential as a biomarker for neurological diseases.

Method: Introduces LV-Net, which deforms a joint LV-hippocampus template mesh and incorporates anatomical relationships to provide accurate and robust lateral ventricle reconstructions from brain MRI.

Result: Demonstrates superior reconstruction accuracy, enhanced point correspondence, and reliable LV shape statistics, identifying Alzheimer's disease-related LV subregions.

Conclusion: LV-Net provides a promising tool for individualized LV shape analysis, offering improved accuracy, robustness, and its application highlights significant associations between LV shape and Alzheimer's disease.

Abstract: Lateral ventricle (LV) shape analysis holds promise as a biomarker for
neurological diseases; however, challenges remain due to substantial shape
variability across individuals and segmentation difficulties arising from
limited MRI resolution. We introduce LV-Net, a novel framework for producing
individualized 3D LV meshes from brain MRI by deforming an anatomy-aware joint
LV-hippocampus template mesh. By incorporating anatomical relationships
embedded within the joint template, LV-Net reduces boundary segmentation
artifacts and improves reconstruction robustness. In addition, by classifying
the vertices of the template mesh based on their anatomical adjacency, our
method enhances point correspondence across subjects, leading to more accurate
LV shape statistics. We demonstrate that LV-Net achieves superior
reconstruction accuracy, even in the presence of segmentation imperfections,
and delivers more reliable shape descriptors across diverse datasets. Finally,
we apply LV-Net to Alzheimer's disease analysis, identifying LV subregions that
show significantly associations with the disease relative to cognitively normal
controls. The codes for LV shape modeling are available at
https://github.com/PWonjung/LV_Shape_Modeling.

</details>


### [115] [AGI for the Earth, the path, possibilities and how to evaluate intelligence of models that work with Earth Observation Data?](https://arxiv.org/abs/2508.06057)
*Mojtaba Valipour,Kelly Zheng,James Lowman,Spencer Szabados,Mike Gartner,Bobby Braswell*

Main category: cs.CV

TL;DR: The paper discusses the potential of using satellite spectral imagery to advance Artificial General Intelligence (AGI) and proposes comprehensive benchmarks to evaluate models in Earth Observation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of focus on satellite spectral imagery as a modality in AGI research, despite its potential for improving models' understanding of Earth's environment.

Method: The study reviews existing benchmarks, identifies their shortcomings, and proposes a set of tasks to build a comprehensive benchmark for evaluating Earth observation models.

Result: The proposed framework aims to better assess a model’s ability to interact with and understand Earth observation data.

Conclusion: Satellite spectral imagery represents a valuable yet underutilized resource in AGI research, and developing a robust evaluation benchmark is essential for advancing this domain.

Abstract: Artificial General Intelligence (AGI) is closer than ever to becoming a
reality, sparking widespread enthusiasm in the research community to collect
and work with various modalities, including text, image, video, and audio.
Despite recent efforts, satellite spectral imagery, as an additional modality,
has yet to receive the attention it deserves. This area presents unique
challenges, but also holds great promise in advancing the capabilities of AGI
in understanding the natural world. In this paper, we argue why Earth
Observation data is useful for an intelligent model, and then we review
existing benchmarks and highlight their limitations in evaluating the
generalization ability of foundation models in this domain. This paper
emphasizes the need for a more comprehensive benchmark to evaluate earth
observation models. To facilitate this, we propose a comprehensive set of tasks
that a benchmark should encompass to effectively assess a model's ability to
understand and interact with Earth observation data.

</details>


### [116] [Lightweight Quad Bayer HybridEVS Demosaicing via State Space Augmented Cross-Attention](https://arxiv.org/abs/2508.06058)
*Shiyang Zhou,Haijin Zeng,Yunfan Lu,Yongyong Chen,Jie Liu,Jingyong Su*

Main category: cs.CV

TL;DR: The paper introduces TSANet, a lightweight two-stage network leveraging state-space cross-attention for efficient and high-quality image demosaicing of HybridEVS event cameras, outperforming existing methods while being computationally lighter.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in demosaicing images captured by HybridEVS event cameras, specifically resolving aliasing and artifacts caused by combining CFA sensors and event pixels, while keeping resource constraints of mobile devices in mind.

Method: The paper proposes a two-stage network, TSANet, which separates the inpainting and demosaicing tasks for better manageability. It introduces a lightweight Cross-Swin State Block that enhances positional prior and global dependencies using a state space model, ensuring linear computational complexity.

Result: TSANet achieves superior demosaicing performance in terms of PSNR and SSIM metrics across seven datasets when compared to DemosaicFormer, while reducing parameters by 1.86x and computation costs by 3.29x. It performs well on both simulated and real data from HybridEVS.

Conclusion: The study demonstrates that TSANet is an efficient and effective solution for image demosaicing challenges in hybrid event-based sensors, enabling advanced mobile applications while being resource-efficient.

Abstract: Event cameras like the Hybrid Event-based Vision Sensor (HybridEVS) camera
capture brightness changes as asynchronous "events" instead of frames, offering
advanced application on mobile photography. However, challenges arise from
combining a Quad Bayer Color Filter Array (CFA) sensor with event pixels
lacking color information, resulting in aliasing and artifacts on the
demosaicing process before downstream application. Current methods struggle to
address these issues, especially on resource-limited mobile devices. In
response, we introduce \textbf{TSANet}, a lightweight \textbf{T}wo-stage
network via \textbf{S}tate space augmented cross-\textbf{A}ttention, which can
handle event pixels inpainting and demosaicing separately, leveraging the
benefits of dividing complex tasks into manageable subtasks. Furthermore, we
introduce a lightweight Cross-Swin State Block that uniquely utilizes
positional prior for demosaicing and enhances global dependencies through the
state space model with linear complexity. In summary, TSANet demonstrates
excellent demosaicing performance on both simulated and real data of HybridEVS
while maintaining a lightweight model, averaging better results than the
previous state-of-the-art method DemosaicFormer across seven diverse datasets
in both PSNR and SSIM, while respectively reducing parameter and computation
costs by $1.86\times$ and $3.29\times$. Our approach presents new possibilities
for efficient image demosaicing on mobile devices. Code is available in the
supplementary materials.

</details>


### [117] [Distribution-Specific Learning for Joint Salient and Camouflaged Object Detection](https://arxiv.org/abs/2508.06063)
*Chao Hao,Zitong Yu,Xin Liu,Yuhao Wang,Weicheng Xie,Jingang Shi,Huanjing Yue,Jingyu Yang*

Main category: cs.CV

TL;DR: This paper introduces a joint learning approach for Salient Object Detection (SOD) and Camouflaged Object Detection (COD), demonstrating that combining these tasks can actually enhance performance under the proposed method.


<details>
  <summary>Details</summary>
Motivation: The motivation is to challenge the prevailing notion that the joint learning of SOD and COD tasks produces conflicting results, and instead show that these tasks can complement each other when decoupled effectively.

Method: The authors propose SCJoint, a joint learning scheme where minimal task-specific parameters are integrated into a fully shared network. They also introduce a Saliency-based Sampling Strategy (SBSS) to balance training sizes and enhance efficiency.

Result: The proposed JoNet model, based on SCJoint and SBSS, achieves strong performance in detecting salient and camouflaged objects, with extensive experiments validating its effectiveness.

Conclusion: The study concludes that with the right joint learning approach, the seemingly contradictory tasks of SOD and COD can be integrated efficiently, leading to mutual benefits for both.

Abstract: Salient object detection (SOD) and camouflaged object detection (COD) are two
closely related but distinct computer vision tasks. Although both are
class-agnostic segmentation tasks that map from RGB space to binary space, the
former aims to identify the most salient objects in the image, while the latter
focuses on detecting perfectly camouflaged objects that blend into the
background in the image. These two tasks exhibit strong contradictory
attributes. Previous works have mostly believed that joint learning of these
two tasks would confuse the network, reducing its performance on both tasks.
However, here we present an opposite perspective: with the correct approach to
learning, the network can simultaneously possess the capability to find both
salient and camouflaged objects, allowing both tasks to benefit from joint
learning. We propose SCJoint, a joint learning scheme for SOD and COD tasks,
assuming that the decoding processes of SOD and COD have different distribution
characteristics. The key to our method is to learn the respective means and
variances of the decoding processes for both tasks by inserting a minimal
amount of task-specific learnable parameters within a fully shared network
structure, thereby decoupling the contradictory attributes of the two tasks at
a minimal cost. Furthermore, we propose a saliency-based sampling strategy
(SBSS) to sample the training set of the SOD task to balance the training set
sizes of the two tasks. In addition, SBSS improves the training set quality and
shortens the training time. Based on the proposed SCJoint and SBSS, we train a
powerful generalist network, named JoNet, which has the ability to
simultaneously capture both ``salient" and ``camouflaged". Extensive
experiments demonstrate the competitive performance and effectiveness of our
proposed method. The code is available at https://github.com/linuxsino/JoNet.

</details>


### [118] [Can Large Models Fool the Eye? A New Turing Test for Biological Animation](https://arxiv.org/abs/2508.06072)
*Zijian Chen,Lirong Deng,Zhengyu Chen,Kaiwei Zhang,Qi Jia,Yuan Tian,Yucheng Zhu,Guangtao Zhai*

Main category: cs.CV

TL;DR: BioMotion Arena introduces a novel framework using visual animations to evaluate large language models (LLMs) and multimodal LLMs (MLLMs), emphasizing performance differences through biological motion perception.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods of large models either rely on static ground-truth-based benchmarks or unclear human preferences, which fail to provide straightforward and perceptible feedback on performance gaps.

Method: The framework utilizes point-light source imaging inspired by biological motion patterns for pairwise comparison evaluation. It collects crowdsourced human votes alongside expert validation on 90 motion variants.

Result: Data analysis shows alignment between human and expert evaluations. 90% of models tested fail to generate fundamental humanoid point-light animations, exposing limitations in current LLMs and MLLMs.

Conclusion: BioMotion Arena serves as a discriminative, ground-truth-independent benchmark for evaluating and visualizing performance inconsistencies of large models.

Abstract: Evaluating the abilities of large models and manifesting their gaps are
challenging. Current benchmarks adopt either ground-truth-based score-form
evaluation on static datasets or indistinct textual chatbot-style human
preferences collection, which may not provide users with immediate, intuitive,
and perceptible feedback on performance differences. In this paper, we
introduce BioMotion Arena, a novel framework for evaluating large language
models (LLMs) and multimodal large language models (MLLMs) via visual
animation. Our methodology draws inspiration from the inherent visual
perception of motion patterns characteristic of living organisms that utilizes
point-light source imaging to amplify the performance discrepancies between
models. Specifically, we employ a pairwise comparison evaluation and collect
more than 45k votes for 53 mainstream LLMs and MLLMs on 90 biological motion
variants. Data analyses show that the crowd-sourced human votes are in good
agreement with those of expert raters, demonstrating the superiority of our
BioMotion Arena in offering discriminative feedback. We also find that over
90\% of evaluated models, including the cutting-edge open-source InternVL3 and
proprietary Claude-4 series, fail to produce fundamental humanoid point-light
groups, much less smooth and biologically plausible motions. This enables
BioMotion Arena to serve as a challenging benchmark for performance
visualization and a flexible evaluation framework without restrictions on
ground-truth.

</details>


### [119] [Towards MR-Based Trochleoplasty Planning](https://arxiv.org/abs/2508.06076)
*Michael Wehrli,Alicia Durrer,Paul Friedrich,Sidaty El Hadramy,Edwin Li,Luana Brahaj,Carol C. Hasler,Philippe C. Cattin*

Main category: cs.CV

TL;DR: The paper presents a pipeline for treating Trochlear Dysplasia using super-resolved, patient-specific 3D pseudo-healthy target morphologies. The method does not rely on CT imaging, reducing radiation exposure.


<details>
  <summary>Details</summary>
Motivation: Current treatments for Trochlear Dysplasia rely on low-resolution MR scans and surgical intuition, which lead to inconsistent outcomes and have limited minimally invasive techniques.

Method: The pipeline utilizes an Implicit Neural Representation for super-resolving MR volumes, a custom-trained network for segmentation, and a Wavelet Diffusion Model to generate pseudo-healthy target morphologies.

Result: The proposed approach significantly improved sulcus angle and trochlear groove depth in evaluations of 25 patients.

Conclusion: The sub-millimeter resolved 3D shapes generated by the pipeline can enhance pre-and intraoperative planning, serve as blueprints for reshaping femoral grooves, and reduce radiation exposure by avoiding CT scans.

Abstract: To treat Trochlear Dysplasia (TD), current approaches rely mainly on
low-resolution clinical Magnetic Resonance (MR) scans and surgical intuition.
The surgeries are planned based on surgeons experience, have limited adoption
of minimally invasive techniques, and lead to inconsistent outcomes. We propose
a pipeline that generates super-resolved, patient-specific 3D pseudo-healthy
target morphologies from conventional clinical MR scans. First, we compute an
isotropic super-resolved MR volume using an Implicit Neural Representation
(INR). Next, we segment femur, tibia, patella, and fibula with a multi-label
custom-trained network. Finally, we train a Wavelet Diffusion Model (WDM) to
generate pseudo-healthy target morphologies of the trochlear region. In
contrast to prior work producing pseudo-healthy low-resolution 3D MR images,
our approach enables the generation of sub-millimeter resolved 3D shapes
compatible for pre- and intraoperative use. These can serve as preoperative
blueprints for reshaping the femoral groove while preserving the native patella
articulation. Furthermore, and in contrast to other work, we do not require a
CT for our pipeline - reducing the amount of radiation. We evaluated our
approach on 25 TD patients and could show that our target morphologies
significantly improve the sulcus angle (SA) and trochlear groove depth (TGD).
The code and interactive visualization are available at
https://wehrlimi.github.io/sr-3d-planning/.

</details>


### [120] [DreamVE: Unified Instruction-based Image and Video Editing](https://arxiv.org/abs/2508.06080)
*Bin Xia,Jiyang Liu,Yuechen Zhang,Bohao Peng,Ruihang Chu,Yitong Wang,Xinglong Wu,Bei Yu,Jiaya Jia*

Main category: cs.CV

TL;DR: DreamVE is a unified model for instruction-based image and video editing. It employs a two-stage training strategy: training with image data followed by video data. Collage-based and generative model-based pipelines synthesize training data.


<details>
  <summary>Details</summary>
Motivation: The practical application of instruction-based video editing has been limited by a lack of sufficient training data, necessitating innovative strategies to improve scalability and efficiency.

Method: DreamVE utilizes a two-stage training approach starting with image editing and then progressing to video editing. It combines extensive collage-based data pretraining with generative model-based fine-tuning. A novel editing framework incorporates guidance from source images using a token concatenation method.

Result: The model demonstrates strong performance in key editing tasks and improved generalization capabilities by leveraging collage-based pretraining. Fine-tuning with generative data addresses specific attribute editing challenges.

Conclusion: DreamVE advances instruction-based editing by unifying image and video editing, incorporating scalable and diverse training data synthesis, and designing an efficient editing framework. Its release aims to further research and application.

Abstract: Instruction-based editing holds vast potential due to its simple and
efficient interactive editing format. However, instruction-based editing,
particularly for video, has been constrained by limited training data,
hindering its practical application. To this end, we introduce DreamVE, a
unified model for instruction-based image and video editing. Specifically, We
propose a two-stage training strategy: first image editing, then video editing.
This offers two main benefits: (1) Image data scales more easily, and models
are more efficient to train, providing useful priors for faster and better
video editing training. (2) Unifying image and video generation is natural and
aligns with current trends. Moreover, we present comprehensive training data
synthesis pipelines, including collage-based and generative model-based data
synthesis. The collage-based data synthesis combines foreground objects and
backgrounds to generate diverse editing data, such as object manipulation,
background changes, and text modifications. It can easily generate billions of
accurate, consistent, realistic, and diverse editing pairs. We pretrain DreamVE
on extensive collage-based data to achieve strong performance in key editing
types and enhance generalization and transfer capabilities. However,
collage-based data lacks some attribute editing cases, leading to a relative
drop in performance. In contrast, the generative model-based pipeline, despite
being hard to scale up, offers flexibility in handling attribute editing cases.
Therefore, we use generative model-based data to further fine-tune DreamVE.
Besides, we design an efficient and powerful editing framework for DreamVE. We
build on the SOTA T2V model and use a token concatenation with early drop
approach to inject source image guidance, ensuring strong consistency and
editability. The codes and models will be released.

</details>


### [121] [SwiftVideo: A Unified Framework for Few-Step Video Generation through Trajectory-Distribution Alignment](https://arxiv.org/abs/2508.06082)
*Yanxiao Sun,Jiafu Wu,Yun Cao,Chengming Xu,Yabiao Wang,Weijian Cao,Donghao Luo,Chengjie Wang,Yanwei Fu*

Main category: cs.CV

TL;DR: SwiftVideo addresses computational inefficiency in video synthesis by combining trajectory-preserving and distribution-matching methods for accelerated, high-quality generation.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion and flow-based video synthesis methods are computationally expensive due to iterative sampling, and distillation methods are prone to performance issues with fewer inference steps.

Method: SwiftVideo employs continuous-time consistency distillation and dual-perspective alignment, combining trajectory alignment and distribution alignment for stable and efficient video synthesis.

Result: SwiftVideo achieves fewer inference steps while maintaining superior quality in video generation, outperforming alternative methods on the OpenVid-1M benchmark.

Conclusion: SwiftVideo establishes a robust framework for efficient video synthesis by addressing the trade-offs between computational speed and video quality, offering advancements in few-step generation approaches.

Abstract: Diffusion-based or flow-based models have achieved significant progress in
video synthesis but require multiple iterative sampling steps, which incurs
substantial computational overhead. While many distillation methods that are
solely based on trajectory-preserving or distribution-matching have been
developed to accelerate video generation models, these approaches often suffer
from performance breakdown or increased artifacts under few-step settings. To
address these limitations, we propose \textbf{\emph{SwiftVideo}}, a unified and
stable distillation framework that combines the advantages of
trajectory-preserving and distribution-matching strategies. Our approach
introduces continuous-time consistency distillation to ensure precise
preservation of ODE trajectories. Subsequently, we propose a dual-perspective
alignment that includes distribution alignment between synthetic and real data
along with trajectory alignment across different inference steps. Our method
maintains high-quality video generation while substantially reducing the number
of inference steps. Quantitative evaluations on the OpenVid-1M benchmark
demonstrate that our method significantly outperforms existing approaches in
few-step video generation.

</details>


### [122] [AdaptInfer: Adaptive Token Pruning for Vision-Language Model Inference with Dynamical Text Guidance](https://arxiv.org/abs/2508.06084)
*Weichen Zhang,Zhui Zhu,Ningbo Li,Kebin Liu,Yunhao Liu*

Main category: cs.CV

TL;DR: AdaptInfer is a plug-and-play framework targeting the reduction of inference costs in vision-language models by introducing a dynamic text-guided pruning technique and an efficient pruning schedule.


<details>
  <summary>Details</summary>
Motivation: Vision-language models excel in multimodal reasoning but struggle with high inference costs due to excessive vision token processing. Existing methods fail to leverage dynamic internal inference signals, necessitating a more adaptive pruning approach.

Method: AdaptInfer uses a dynamic text-guided pruning mechanism that harnesses layer-wise text-to-text attention maps to prioritize important vision tokens. Additionally, it employs an optimized pruning schedule inspired by cross-modal attention shift patterns identified through offline analysis.

Result: AdaptInfer significantly reduces CUDA latency by 61.3% while maintaining an accuracy of 92.9% on LLaVA-1.5-7B. It also outperforms state-of-the-art methods in accuracy under the same token budget.

Conclusion: The proposed framework is lightweight, efficient, and generalizable, offering substantial improvements in multimodal reasoning tasks without sacrificing performance, making it a valuable advancement in vision-language models.

Abstract: Vision-language models (VLMs) have achieved impressive performance on
multimodal reasoning tasks such as visual question answering (VQA), but their
inference cost remains a significant challenge due to the large number of
vision tokens processed during the prefill stage. Existing pruning methods
often rely on directly using the attention patterns or static text prompt
guidance, failing to exploit the dynamic internal signals generated during
inference. To address these issues, we propose AdaptInfer, a plug-and-play
framework for adaptive vision token pruning in VLMs. First, we introduce a
fine-grained, dynamic text-guided pruning mechanism that reuses layer-wise
text-to-text attention maps to construct soft priors over text-token
importance, allowing more informed scoring of vision tokens at each stage.
Second, we perform an offline analysis of cross-modal attention shifts and
identify consistent inflection locations in inference, which inspire us to
propose a more principled and efficient pruning schedule. Our method is
lightweight and plug-and-play, also generalizable across multi-modal tasks.
Experimental results have verified the effectiveness of the proposed method.
For example, it reduces CUDA latency by 61.3\% while maintaining an average
accuracy of 92.9\% on vanilla LLaVA-1.5-7B. Under the same token budget,
AdaptInfer surpasses SOTA in accuracy.

</details>


### [123] [Q-CLIP: Unleashing the Power of Vision-Language Models for Video Quality Assessment through Unified Cross-Modal Adaptation](https://arxiv.org/abs/2508.06092)
*Yachun Mi,Yu Li,Yanting Li,Shixin Sun,Chen Hui,Tong Zhang,Yuanyuan Liu,Chenyue Song,Shaohui Liu*

Main category: cs.CV

TL;DR: The authors introduce Q-CLIP, a cost-efficient VQA framework built around Vision-Language Models, which eliminates the need for extensive pretraining and incorporates mechanisms to enhance sensitivity to video quality variations.


<details>
  <summary>Details</summary>
Motivation: Accurate Video Quality Assessment is challenging due to the reliance on computationally expensive pretraining on large datasets and the insufficiency of semantic knowledge transfer for the nuanced factors impacting video quality.

Method: The proposed Q-CLIP framework utilizes Vision-Language Models with a Shared Cross-Modal Adapter (SCMA) to optimize visual and textual representations. The approach incorporates trainable prompts to focus on video quality nuances and explores efficient frame sampling strategies.

Result: Q-CLIP achieves high performance on multiple VQA datasets while significantly reducing computational demands compared to conventional methods.

Conclusion: Q-CLIP establishes itself as an efficient and robust VQA solution by leveraging VLMs with minimal training requirements and introducing techniques to enhance video quality sensitivity.

Abstract: Accurate and efficient Video Quality Assessment (VQA) has long been a key
research challenge. Current mainstream VQA methods typically improve
performance by pretraining on large-scale classification datasets (e.g.,
ImageNet, Kinetics-400), followed by fine-tuning on VQA datasets. However, this
strategy presents two significant challenges: (1) merely transferring semantic
knowledge learned from pretraining is insufficient for VQA, as video quality
depends on multiple factors (e.g., semantics, distortion, motion, aesthetics);
(2) pretraining on large-scale datasets demands enormous computational
resources, often dozens or even hundreds of times greater than training
directly on VQA datasets. Recently, Vision-Language Models (VLMs) have shown
remarkable generalization capabilities across a wide range of visual tasks, and
have begun to demonstrate promising potential in quality assessment. In this
work, we propose Q-CLIP, the first fully VLMs-based framework for VQA. Q-CLIP
enhances both visual and textual representations through a Shared Cross-Modal
Adapter (SCMA), which contains only a minimal number of trainable parameters
and is the only component that requires training. This design significantly
reduces computational cost. In addition, we introduce a set of five learnable
quality-level prompts to guide the VLMs in perceiving subtle quality
variations, thereby further enhancing the model's sensitivity to video quality.
Furthermore, we investigate the impact of different frame sampling strategies
on VQA performance, and find that frame-difference-based sampling leads to
better generalization performance across datasets. Extensive experiments
demonstrate that Q-CLIP exhibits excellent performance on several VQA datasets.

</details>


### [124] [E-React: Towards Emotionally Controlled Synthesis of Human Reactions](https://arxiv.org/abs/2508.06093)
*Chen Zhu,Buzhen Huang,Zijing Wu,Binghui Zuo,Yangang Wang*

Main category: cs.CV

TL;DR: The paper introduces a method for generating human reaction motions influenced by emotional cues using a semi-supervised emotion prior and an actor-reactor diffusion model, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing motion generation techniques lack consideration of emotions, making them less natural and restricting their use in interactive tasks.

Method: A semi-supervised learning framework is used to train an emotion prior, integrated with an actor-reactor diffusion model for emotion-driven reaction synthesis.

Result: The approach generates realistic reaction motions in different emotional contexts and surpasses current reaction generation methods in performance.

Conclusion: The model effectively integrates emotional cues into motion generation frameworks, advancing human-like interaction synthesis for practical applications.

Abstract: Emotion serves as an essential component in daily human interactions.
Existing human motion generation frameworks do not consider the impact of
emotions, which reduces naturalness and limits their application in interactive
tasks, such as human reaction synthesis. In this work, we introduce a novel
task: generating diverse reaction motions in response to different emotional
cues. However, learning emotion representation from limited motion data and
incorporating it into a motion generation framework remains a challenging
problem. To address the above obstacles, we introduce a semi-supervised emotion
prior in an actor-reactor diffusion model to facilitate emotion-driven reaction
synthesis. Specifically, based on the observation that motion clips within a
short sequence tend to share the same emotion, we first devise a
semi-supervised learning framework to train an emotion prior. With this prior,
we further train an actor-reactor diffusion model to generate reactions by
considering both spatial interaction and emotional response. Finally, given a
motion sequence of an actor, our approach can generate realistic reactions
under various emotional conditions. Experimental results demonstrate that our
model outperforms existing reaction generation methods. The code and data will
be made publicly available at https://ereact.github.io/

</details>


### [125] [UGD-IML: A Unified Generative Diffusion-based Framework for Constrained and Unconstrained Image Manipulation Localization](https://arxiv.org/abs/2508.06101)
*Yachun Mi,Xingyang He,Shixin Sun,Yu Li,Yanting Li,Zhixuan Li,Jian Jin,Chen Hui,Shaohui Liu*

Main category: cs.CV

TL;DR: The paper introduces UGD-IML, a unified generative framework for image manipulation detection and constrained annotation using diffusion models. It achieves superior performance in accuracy and efficiency compared to state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: With the increased threat to visual content integrity due to advanced image editing tools, existing image forgery detection methods are limited by dataset scale, diversity, and inefficiency in annotation processes.

Method: The paper proposes UGD-IML, a generative diffusion model framework integrating IML and CIML tasks with a class embedding mechanism and parameter-sharing design. The model avoids multi-stage pipelines and reduces reliance on large-scale datasets.

Result: UGD-IML surpasses state-of-the-art techniques in F1 metrics for IML and CIML tasks by 9.66 and 4.36, respectively, and demonstrates strong performance in uncertainty estimation, visualization, and robustness.

Conclusion: The proposed approach effectively addresses limitations of existing methods, offering a unified, efficient, and high-performing solution for both image manipulation detection and constrained annotation tasks.

Abstract: In the digital age, advanced image editing tools pose a serious threat to the
integrity of visual content, making image forgery detection and localization a
key research focus. Most existing Image Manipulation Localization (IML) methods
rely on discriminative learning and require large, high-quality annotated
datasets. However, current datasets lack sufficient scale and diversity,
limiting model performance in real-world scenarios. To overcome this, recent
studies have explored Constrained IML (CIML), which generates pixel-level
annotations through algorithmic supervision. However, existing CIML approaches
often depend on complex multi-stage pipelines, making the annotation process
inefficient. In this work, we propose a novel generative framework based on
diffusion models, named UGD-IML, which for the first time unifies both IML and
CIML tasks within a single framework. By learning the underlying data
distribution, generative diffusion models inherently reduce the reliance on
large-scale labeled datasets, allowing our approach to perform effectively even
under limited data conditions. In addition, by leveraging a class embedding
mechanism and a parameter-sharing design, our model seamlessly switches between
IML and CIML modes without extra components or training overhead. Furthermore,
the end-to-end design enables our model to avoid cumbersome steps in the data
annotation process. Extensive experimental results on multiple datasets
demonstrate that UGD-IML outperforms the SOTA methods by an average of 9.66 and
4.36 in terms of F1 metrics for IML and CIML tasks, respectively. Moreover, the
proposed method also excels in uncertainty estimation, visualization and
robustness.

</details>


### [126] [MCA: 2D-3D Retrieval with Noisy Labels via Multi-level Adaptive Correction and Alignment](https://arxiv.org/abs/2508.06104)
*Gui Zou,Chaofan Gan,Chern Hong Lim,Supavadee Aramvith,Weiyao Lin*

Main category: cs.CV

TL;DR: This paper introduces MCA, a robust framework for 2D-3D cross-modal retrieval under noisy label conditions, using multimodal label correction and adaptive alignment strategies.


<details>
  <summary>Details</summary>
Motivation: Existing 2D-3D cross-modal retrieval methods are vulnerable to overfitting due to noisy labels, highlighting the need for robust solutions.

Method: The proposed MCA framework includes a Multimodal Joint label Correction mechanism to refine labels using historical self-predictions and a Multi-level Adaptive Alignment strategy to improve cross-modal feature semantics and discrimination.

Result: MCA achieves state-of-the-art performance on both conventional and realistic noisy 3D benchmarks, demonstrating its effectiveness.

Conclusion: MCA is a general and effective solution for robust 2D-3D cross-modal retrieval, particularly under noisy label conditions.

Abstract: With the increasing availability of 2D and 3D data, significant advancements
have been made in the field of cross-modal retrieval. Nevertheless, the
existence of imperfect annotations presents considerable challenges, demanding
robust solutions for 2D-3D cross-modal retrieval in the presence of noisy label
conditions. Existing methods generally address the issue of noise by dividing
samples independently within each modality, making them susceptible to
overfitting on corrupted labels. To address these issues, we propose a robust
2D-3D \textbf{M}ulti-level cross-modal adaptive \textbf{C}orrection and
\textbf{A}lignment framework (MCA). Specifically, we introduce a Multimodal
Joint label Correction (MJC) mechanism that leverages multimodal historical
self-predictions to jointly model the modality prediction consistency, enabling
reliable label refinement. Additionally, we propose a Multi-level Adaptive
Alignment (MAA) strategy to effectively enhance cross-modal feature semantics
and discrimination across different levels. Extensive experiments demonstrate
the superiority of our method, MCA, which achieves state-of-the-art performance
on both conventional and realistic noisy 3D benchmarks, highlighting its
generality and effectiveness.

</details>


### [127] [Mask & Match: Learning to Recognize Handwritten Math with Self-Supervised Attention](https://arxiv.org/abs/2508.06107)
*Shree Mitra,Ritabrata Chakraborty,Nilkanta Sahu*

Main category: cs.CV

TL;DR: The paper introduces a self-supervised learning (SSL) approach for handwriting mathematical expression recognition (HMER) without needing extensive labeled data.


<details>
  <summary>Details</summary>
Motivation: The task of recognizing handwritten mathematical expressions (HMER) is complex due to the two-dimensional structure, symbol variations, and spatial relationships, which require robust solutions.

Method: The proposed method leverages self-supervised learning with an image encoder pretrained using contrastive loss, a novel progressive spatial masking strategy for self-supervised attention, and supervised fine-tuning using a transformer decoder.

Result: Experiments on CROHME benchmarks reveal that the proposed approach outperforms existing self-supervised and supervised methods for HMER, particularly due to its robust attention mechanism.

Conclusion: The paper demonstrates the effectiveness of self-supervised learning combined with a progressive attention mechanism for improving HMER performance and provides an open-source codebase.

Abstract: Recognizing handwritten mathematical expressions (HMER) is a challenging task
due to the inherent two-dimensional structure, varying symbol scales, and
complex spatial relationships among symbols. In this paper, we present a
self-supervised learning (SSL) framework for HMER that eliminates the need for
expensive labeled data. Our approach begins by pretraining an image encoder
using a combination of global and local contrastive loss, enabling the model to
learn both holistic and fine-grained representations. A key contribution of
this work is a novel self-supervised attention network, which is trained using
a progressive spatial masking strategy. This attention mechanism is designed to
learn semantically meaningful focus regions, such as operators, exponents, and
nested mathematical notation, without requiring any supervision. The
progressive masking curriculum encourages the network to become increasingly
robust to missing or occluded visual information, ultimately improving
structural understanding. Our complete pipeline consists of (1) self-supervised
pretraining of the encoder, (2) self-supervised attention learning, and (3)
supervised fine-tuning with a transformer decoder to generate LATEX sequences.
Extensive experiments on CROHME benchmarks demonstrate that our method
outperforms existing SSL and fully supervised baselines, validating the
effectiveness of our progressive attention mechanism in enhancing HMER
performance. Our codebase can be found here.

</details>


### [128] [FMCE-Net++: Feature Map Convergence Evaluation and Training](https://arxiv.org/abs/2508.06109)
*Zhibo Zhu,Renyu Huang,Lei He*

Main category: cs.CV

TL;DR: FMCE-Net++ introduces a framework to improve DNNs' interpretability and performance by integrating feature map convergence evaluation during training.


<details>
  <summary>Details</summary>
Motivation: To tackle interpretability challenges and enhance the performance of Deep Neural Networks by addressing the limitations of existing Feature Map Convergence Evaluation methods.

Method: Proposed FMCE-Net++, which uses a pretrained FMCE-Net as an auxiliary head to predict feature convergence during training. A Representation Auxiliary Loss dynamically balances classification and convergence loss.

Result: Extensive experiments on datasets like MNIST and CIFAR showed improved accuracy, e.g., +1.16 pp for ResNet-50 on CIFAR-10 and +1.08 pp for ShuffleNet v2 on CIFAR-100.

Conclusion: FMCE-Net++ effectively enhances DNN performance and interpretability without requiring architectural changes or extra data, offering a practical approach to surpassing current performance ceilings.

Abstract: Deep Neural Networks (DNNs) face interpretability challenges due to their
opaque internal representations. While Feature Map Convergence Evaluation
(FMCE) quantifies module-level convergence via Feature Map Convergence Scores
(FMCS), it lacks experimental validation and closed-loop integration. To
address this limitation, we propose FMCE-Net++, a novel training framework that
integrates a pretrained, frozen FMCE-Net as an auxiliary head. This module
generates FMCS predictions, which, combined with task labels, jointly supervise
backbone optimization through a Representation Auxiliary Loss. The RAL
dynamically balances the primary classification loss and feature convergence
optimization via a tunable \Representation Abstraction Factor. Extensive
experiments conducted on MNIST, CIFAR-10, FashionMNIST, and CIFAR-100
demonstrate that FMCE-Net++ consistently enhances model performance without
architectural modifications or additional data. Key experimental outcomes
include accuracy gains of $+1.16$ pp (ResNet-50/CIFAR-10) and $+1.08$ pp
(ShuffleNet v2/CIFAR-100), validating that FMCE-Net++ can effectively elevate
state-of-the-art performance ceilings.

</details>


### [129] [SynSeg: Feature Synergy for Multi-Category Contrastive Learning in Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2508.06115)
*Weichen Zhang,Kebin Liu,Fan Dang,Zhui Zhu,Xikai Sun,Yunhao Liu*

Main category: cs.CV

TL;DR: SynSeg introduces a novel weakly-supervised method for open-vocabulary semantic segmentation, improving accuracy and addressing semantic misalignment challenges through Multi-Category Contrastive Learning and the Feature Synergy Structure.


<details>
  <summary>Details</summary>
Motivation: Semantic segmentation in open-vocabulary scenarios faces challenges due to varied semantic categories and weak supervision methods, which result in poor performance and semantic misalignment.

Method: SynSeg employs Multi-Category Contrastive Learning (MCCL) for intra- and inter-category alignment and uses Feature Synergy Structure (FSS) for feature reconstruction to enhance semantic learning while avoiding biases.

Result: SynSeg demonstrated improvements over state-of-the-art methods with significant accuracy gains across various benchmarks: 4.5% on VOC, 8.9% on Context, 2.6% on Object, and 2.0% on City.

Conclusion: The paper concludes that SynSeg effectively enhances semantic segmentation in open-vocabulary scenarios by improving localization and discrimination capabilities under weak supervision.

Abstract: Semantic segmentation in open-vocabulary scenarios presents significant
challenges due to the wide range and granularity of semantic categories.
Existing weakly-supervised methods often rely on category-specific supervision
and ill-suited feature construction methods for contrastive learning, leading
to semantic misalignment and poor performance. In this work, we propose a novel
weakly-supervised approach, SynSeg, to address the challenges. SynSeg performs
Multi-Category Contrastive Learning (MCCL) as a stronger training signal with a
new feature reconstruction framework named Feature Synergy Structure (FSS).
Specifically, MCCL strategy robustly combines both intra- and inter-category
alignment and separation in order to make the model learn the knowledge of
correlations from different categories within the same image. Moreover, FSS
reconstructs discriminative features for contrastive learning through prior
fusion and semantic-activation-map enhancement, effectively avoiding the
foreground bias introduced by the visual encoder. In general, SynSeg
effectively improves the abilities in semantic localization and discrimination
under weak supervision. Extensive experiments on benchmarks demonstrate that
our method outperforms state-of-the-art (SOTA) performance. For instance,
SynSeg achieves higher accuracy than SOTA baselines by 4.5\% on VOC, 8.9\% on
Context, 2.6\% on Object and 2.0\% on City.

</details>


### [130] [Learning Representations of Satellite Images with Evaluations on Synoptic Weather Events](https://arxiv.org/abs/2508.06122)
*Ting-Shuo Yo,Shih-Hao Su,Chien-Ming Wu,Wei-Ting Chen,Jung-Lien Chu,Chiao-Wei Chang,Hung-Chi Kuo*

Main category: cs.CV

TL;DR: This paper explores the use of representation learning algorithms, including PCA, CAE, and pre-trained ResNet, for classifying weather events from satellite images, finding CAE to be the most effective.


<details>
  <summary>Details</summary>
Motivation: To investigate how different representation learning techniques perform on satellite imagery classification tasks for various weather events.

Method: The study compares three representation learning methods (PCA, CAE, and pre-trained ResNet) by evaluating their latent spaces through classification performance on weather-related satellite images.

Result: CAE demonstrated the highest classification performance with low false alarm rates, PCA showed high false alarms, and pre-trained ResNet excelled at tropical cyclone detection but underperformed for other tasks. High-resolution input data and specific latent space sizes impacted performance.

Conclusion: CAE is effective for representation learning but lacks physical interpretability, suggesting the potential for further development of physics-informed CAE to address this limitation.

Abstract: This study applied representation learning algorithms to satellite images and
evaluated the learned latent spaces with classifications of various weather
events. The algorithms investigated include the classical linear
transformation, i.e., principal component analysis (PCA), state-of-the-art deep
learning method, i.e., convolutional autoencoder (CAE), and a residual network
pre-trained with large image datasets (PT). The experiment results indicated
that the latent space learned by CAE consistently showed higher threat scores
for all classification tasks. The classifications with PCA yielded high hit
rates but also high false-alarm rates. In addition, the PT performed
exceptionally well at recognizing tropical cyclones but was inferior in other
tasks. Further experiments suggested that representations learned from
higher-resolution datasets are superior in all classification tasks for
deep-learning algorithms, i.e., CAE and PT. We also found that smaller latent
space sizes had minor impact on the classification task's hit rate. Still, a
latent space dimension smaller than 128 caused a significantly higher false
alarm rate. Though the CAE can learn latent spaces effectively and efficiently,
the interpretation of the learned representation lacks direct connections to
physical attributions. Therefore, developing a physics-informed version of CAE
can be a promising outlook for the current work.

</details>


### [131] [SC-Captioner: Improving Image Captioning with Self-Correction by Reinforcement Learning](https://arxiv.org/abs/2508.06125)
*Lin Zhang,Xianfang Zeng,Kangcong Li,Gang Yu,Tao Chen*

Main category: cs.CV

TL;DR: SC-Captioner is a reinforcement learning-based framework designed for improving the self-correcting ability of image captioning models, using a novel reward system and new metrics.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing image captioning models which lack the capability to self-correct their caption predictions and struggle with accurate quality assessment.

Method: The framework uses scene-graph parsing to decompose captions into object, attribute, and relation sets. Reward is calculated based on correctness bonuses and mistake punishments by comparing against these sets. A new dataset (RefinedCaps) and improved evaluation metrics are introduced.

Result: Experiments demonstrate that SC-Captioner significantly outperforms traditional preference optimization methods in generating high-quality, contextually accurate image captions.

Conclusion: SC-Captioner enhances the captioning capabilities of visual-language models by introducing a better reward structure and evaluation metrics, leading to more accurate and refined captions.

Abstract: We propose SC-Captioner, a reinforcement learning framework that enables the
self-correcting capability of image caption models. Our crucial technique lies
in the design of the reward function to incentivize accurate caption
corrections. Specifically, the predicted and reference captions are decomposed
into object, attribute, and relation sets using scene-graph parsing algorithms.
We calculate the set difference between sets of initial and self-corrected
captions to identify added and removed elements. These elements are matched
against the reference sets to calculate correctness bonuses for accurate
refinements and mistake punishments for wrong additions and removals, thereby
forming the final reward. For image caption quality assessment, we propose a
set of metrics refined from CAPTURE that alleviate its incomplete precision
evaluation and inefficient relation matching problems. Furthermore, we collect
a fine-grained annotated image caption dataset, RefinedCaps, consisting of 6.5K
diverse images from COCO dataset. Experiments show that applying SC-Captioner
on large visual-language models can generate better image captions across
various scenarios, significantly outperforming the direct preference
optimization training strategy.

</details>


### [132] [SAM Encoder Breach by Adversarial Simplicial Complex Triggers Downstream Model Failures](https://arxiv.org/abs/2508.06127)
*Yi Qin,Rui Wang,Tao Huang,Tong Xiao,Liping Jing*

Main category: cs.CV

TL;DR: The paper introduces VeSCA, a method to evaluate and exploit vulnerabilities in SAM by generating transferable adversarial examples using simplicial complexes and domain re-adaptation.


<details>
  <summary>Details</summary>
Motivation: To address the single-point risk posed by SAM's vulnerabilities, which can jeopardize the reliability of multiple downstream models.

Method: VeSCA uses the encoder of SAM to find shared vulnerable regions via a parametric simplicial complex and iterative refinement, supplemented by domain re-adaptation for better transferability.

Result: VeSCA improves adversarial transferability by 12.7% over state-of-the-art methods on various models and datasets.

Conclusion: SAM's vulnerabilities pose risks to downstream applications, emphasizing the need for more secure and robust foundational models.

Abstract: While the Segment Anything Model (SAM) transforms interactive segmentation
with zero-shot abilities, its inherent vulnerabilities present a single-point
risk, potentially leading to the failure of numerous downstream applications.
Proactively evaluating these transferable vulnerabilities is thus imperative.
Prior adversarial attacks on SAM often present limited transferability due to
insufficient exploration of common weakness across domains. To address this, we
propose Vertex-Refining Simplicial Complex Attack (VeSCA), a novel method that
leverages only the encoder of SAM for generating transferable adversarial
examples. Specifically, it achieves this by explicitly characterizing the
shared vulnerable regions between SAM and downstream models through a
parametric simplicial complex. Our goal is to identify such complexes within
adversarially potent regions by iterative vertex-wise refinement. A lightweight
domain re-adaptation strategy is introduced to bridge domain divergence using
minimal reference data during the initialization of simplicial complex.
Ultimately, VeSCA generates consistently transferable adversarial examples
through random simplicial complex sampling. Extensive experiments demonstrate
that VeSCA achieves performance improved by 12.7% compared to state-of-the-art
methods across three downstream model categories across five domain-specific
datasets. Our findings further highlight the downstream model risks posed by
SAM's vulnerabilities and emphasize the urgency of developing more robust
foundation models.

</details>


### [133] [Roll Your Eyes: Gaze Redirection via Explicit 3D Eyeball Rotation](https://arxiv.org/abs/2508.06136)
*YoungChan Choi,HengFei Wang,YiHua Cheng,Boeun Kim,Hyung Jin Chang,YoungGeun Choi,Sang-Il Choi*

Main category: cs.CV

TL;DR: The paper proposes a framework for 3D gaze redirection using an explicit 3D eyeball structure based on 3D Gaussian Splatting, producing photorealistic images with improved gaze estimation accuracy.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing gaze redirection methods, which lack explicit modeling of 3D rotation and translation, aiming to enhance image realism and gaze estimation accuracy.

Method: The approach uses 3D Gaussian Splatting to create a dedicated 3D eyeball model, explicitly rotating and translating the structure while using an adaptive deformation module for realistic muscle movement replication.

Result: Experiments on ETH-XGaze dataset show the framework generates diverse novel gaze images with better image quality and higher gaze estimation accuracy than state-of-the-art methods.

Conclusion: The explicit representation of 3D eyeball structure and adaptive deformation module substantially improve gaze redirection realism and accuracy, making it a superior alternative to previous methods.

Abstract: We propose a novel 3D gaze redirection framework that leverages an explicit
3D eyeball structure. Existing gaze redirection methods are typically based on
neural radiance fields, which employ implicit neural representations via volume
rendering. Unlike these NeRF-based approaches, where the rotation and
translation of 3D representations are not explicitly modeled, we introduce a
dedicated 3D eyeball structure to represent the eyeballs with 3D Gaussian
Splatting (3DGS). Our method generates photorealistic images that faithfully
reproduce the desired gaze direction by explicitly rotating and translating the
3D eyeball structure. In addition, we propose an adaptive deformation module
that enables the replication of subtle muscle movements around the eyes.
Through experiments conducted on the ETH-XGaze dataset, we demonstrate that our
framework is capable of generating diverse novel gaze images, achieving
superior image quality and gaze estimation accuracy compared to previous
state-of-the-art methods.

</details>


### [134] [DiffCap: Diffusion-based Real-time Human Motion Capture using Sparse IMUs and a Monocular Camera](https://arxiv.org/abs/2508.06139)
*Shaohua Pan,Xinyu Yi,Yan Zhou,Weihua Jian,Yuan Zhang,Pengfei Wan,Feng Xu*

Main category: cs.CV

TL;DR: This paper introduces a diffusion-based method to integrate IMUs and a monocular camera for real-time human motion capture, demonstrating state-of-the-art pose estimation performance.


<details>
  <summary>Details</summary>
Motivation: Recent advancements require robust real-time human motion capture, which is challenging due to issues like occlusion and signal inconsistencies in traditional methods.

Method: The paper uses a diffusion model to learn human motion priors, where visual information is treated as a single sequential feature embedding, and IMU measurements are processed frame-by-frame.

Result: Experimental evaluations show that the proposed system offers improved robustness and achieves state-of-the-art results in pose estimation compared to previous approaches.

Conclusion: By combining sparse IMUs and a monocular camera within a unified framework, the research provides a novel solution for reliable and accurate human motion tracking.

Abstract: Combining sparse IMUs and a monocular camera is a new promising setting to
perform real-time human motion capture. This paper proposes a diffusion-based
solution to learn human motion priors and fuse the two modalities of signals
together seamlessly in a unified framework. By delicately considering the
characteristics of the two signals, the sequential visual information is
considered as a whole and transformed into a condition embedding, while the
inertial measurement is concatenated with the noisy body pose frame by frame to
construct a sequential input for the diffusion model. Firstly, we observe that
the visual information may be unavailable in some frames due to occlusions or
subjects moving out of the camera view. Thus incorporating the sequential
visual features as a whole to get a single feature embedding is robust to the
occasional degenerations of visual information in those frames. On the other
hand, the IMU measurements are robust to occlusions and always stable when
signal transmission has no problem. So incorporating them frame-wisely could
better explore the temporal information for the system. Experiments have
demonstrated the effectiveness of the system design and its state-of-the-art
performance in pose estimation compared with the previous works. Our codes are
available for research at https://shaohua-pan.github.io/diffcap-page.

</details>


### [135] [SDEval: Safety Dynamic Evaluation for Multimodal Large Language Models](https://arxiv.org/abs/2508.06142)
*Hanqing Wang,Yuan Tian,Mingyu Liu,Zhenhao Zhang,Xiangyang Zhu*

Main category: cs.CV

TL;DR: The paper introduces SDEval, a dynamic framework for evaluating the safety of Multimodal Large Language Models (MLLMs).


<details>
  <summary>Details</summary>
Motivation: Existing safety evaluation benchmarks for MLLMs become outdated and face data contamination issues.

Method: SDEval uses text, image, and text-image dynamics to generate new samples from original benchmarks, addressing these shortcomings.

Result: Experiments show that SDEval effectively influences safety evaluation, mitigates contamination, and reveals safety limitations in MLLMs.

Conclusion: SDEval proves to be adaptable for various benchmarks, significantly advancing safety evaluations for MLLMs.

Abstract: In the rapidly evolving landscape of Multimodal Large Language Models
(MLLMs), the safety concerns of their outputs have earned significant
attention. Although numerous datasets have been proposed, they may become
outdated with MLLM advancements and are susceptible to data contamination
issues. To address these problems, we propose \textbf{SDEval}, the
\textit{first} safety dynamic evaluation framework to controllably adjust the
distribution and complexity of safety benchmarks. Specifically, SDEval mainly
adopts three dynamic strategies: text, image, and text-image dynamics to
generate new samples from original benchmarks. We first explore the individual
effects of text and image dynamics on model safety. Then, we find that
injecting text dynamics into images can further impact safety, and conversely,
injecting image dynamics into text also leads to safety risks. SDEval is
general enough to be applied to various existing safety and even capability
benchmarks. Experiments across safety benchmarks, MLLMGuard and VLSBench, and
capability benchmarks, MMBench and MMVet, show that SDEval significantly
influences safety evaluation, mitigates data contamination, and exposes safety
limitations of MLLMs. Code is available at https://github.com/hq-King/SDEval

</details>


### [136] [Text-guided Visual Prompt DINO for Generic Segmentation](https://arxiv.org/abs/2508.06146)
*Yuchen Guan,Chong Sun,Canmiao Fu,Zhipeng Huang,Chun Yuan,Chen Li*

Main category: cs.CV

TL;DR: This paper proposes Prompt-DINO, a multimodal framework addressing limitations in open-world segmentation through early fusion, aligned query selection, and a generative data engine, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: The paper addresses challenges in late-stage feature fusion, suboptimal query selection, and fixed-vocabulary constraints in open-world segmentation.

Method: The authors introduce early fusion of text/visual prompts with backbone features, implement order-aligned query selection in DETR-based architectures, and develop a generative data engine to produce diverse training datasets.

Result: Prompt-DINO achieves state-of-the-art results in open-world detection benchmarks, expands semantic coverage, and reduces label noise by 80.5%.

Conclusion: Prompt-DINO sets a new standard for scalable multimodal detection and data generation in open-world scenarios.

Abstract: Recent advancements in multimodal vision models have highlighted limitations
in late-stage feature fusion and suboptimal query selection for hybrid prompts
open-world segmentation, alongside constraints from caption-derived
vocabularies. To address these challenges, we propose Prompt-DINO, a
text-guided visual Prompt DINO framework featuring three key innovations.
First, we introduce an early fusion mechanism that unifies text/visual prompts
and backbone features at the initial encoding stage, enabling deeper
cross-modal interactions to resolve semantic ambiguities. Second, we design
order-aligned query selection for DETR-based architectures, explicitly
optimizing the structural alignment between text and visual queries during
decoding to enhance semantic-spatial consistency. Third, we develop a
generative data engine powered by the Recognize Anything via Prompting (RAP)
model, which synthesizes 0.5B diverse training instances through a dual-path
cross-verification pipeline, reducing label noise by 80.5% compared to
conventional approaches. Extensive experiments demonstrate that Prompt-DINO
achieves state-of-the-art performance on open-world detection benchmarks while
significantly expanding semantic coverage beyond fixed-vocabulary constraints.
Our work establishes a new paradigm for scalable multimodal detection and data
generation in open-world scenarios. Data&Code are available at
https://github.com/WeChatCV/WeVisionOne.

</details>


### [137] [DSConv: Dynamic Splitting Convolution for Pansharpening](https://arxiv.org/abs/2508.06147)
*Xuanyu Liu,Bonan An*

Main category: cs.CV

TL;DR: DSConv, a novel dynamic splitting convolution method combined with attention, enhances pansharpening performance by improving feature extraction and network generalization.


<details>
  <summary>Details</summary>
Motivation: Existing pansharpening methods, used to produce high-resolution images by combining multi-spectral and panchromatic images, lack adaptability due to their heavy reliance on standard convolutions.

Method: The authors introduce DSConv, a strategy that adaptively splits convolution kernels with attention mechanisms to improve feature extraction at diverse positions, optimizing network performance.

Result: DSConv achieves state-of-the-art performance in pansharpening tasks and demonstrates improved generalization and feature representation capabilities.

Conclusion: The study validates DSConv’s effectiveness and superiority through experiments, suggesting its promising applications in advanced pansharpening techniques.

Abstract: Aiming to obtain a high-resolution image, pansharpening involves the fusion
of a multi-spectral image (MS) and a panchromatic image (PAN), the low-level
vision task remaining significant and challenging in contemporary research.
Most existing approaches rely predominantly on standard convolutions, few
making the effort to adaptive convolutions, which are effective owing to the
inter-pixel correlations of remote sensing images. In this paper, we propose a
novel strategy for dynamically splitting convolution kernels in conjunction
with attention, selecting positions of interest, and splitting the original
convolution kernel into multiple smaller kernels, named DSConv. The proposed
DSConv more effectively extracts features of different positions within the
receptive field, enhancing the network's generalization, optimization, and
feature representation capabilities. Furthermore, we innovate and enrich
concepts of dynamic splitting convolution and provide a novel network
architecture for pansharpening capable of achieving the tasks more efficiently,
building upon this methodology. Adequate fair experiments illustrate the
effectiveness and the state-of-the-art performance attained by
DSConv.Comprehensive and rigorous discussions proved the superiority and
optimal usage conditions of DSConv.

</details>


### [138] [VISTAR:A User-Centric and Role-Driven Benchmark for Text-to-Image Evaluation](https://arxiv.org/abs/2508.06152)
*Kaiyuan Jiang,Ruoxi Sun,Ying Cao,Yuqi Xu,Xinran Zhang,Junyan Guo,ChengSheng Deng*

Main category: cs.CV

TL;DR: VISTAR introduces a user-centric, multi-dimensional benchmark for evaluating text-to-image (T2I) models, combining scriptable metrics with a novel approach that uses vision-language models for abstract semantics.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-image evaluation metrics have limitations in assessing both quantifiable and abstract facets of generated images, necessitating a comprehensive and user-centric framework.

Method: The approach combines deterministic metrics for measurable attributes with a Hierarchical Weighted P/N Questioning scheme for abstract semantics, derived through a Delphi study and involving 15,000 human comparisons.

Result: Metrics achieve high alignment with human judgments (>75%), with an 85.9% accuracy in assessing abstract semantics, outperforming existing methods. State-of-the-art models show variable performance based on user role-weighted evaluations.

Conclusion: There is no universal best model for text-to-image tasks; the VISTAR benchmark provides actionable insights and facilitates tailored assessments for specific domains.

Abstract: We present VISTAR, a user-centric, multi-dimensional benchmark for
text-to-image (T2I) evaluation that addresses the limitations of existing
metrics. VISTAR introduces a two-tier hybrid paradigm: it employs
deterministic, scriptable metrics for physically quantifiable attributes (e.g.,
text rendering, lighting) and a novel Hierarchical Weighted P/N Questioning
(HWPQ) scheme that uses constrained vision-language models to assess abstract
semantics (e.g., style fusion, cultural fidelity). Grounded in a Delphi study
with 120 experts, we defined seven user roles and nine evaluation angles to
construct the benchmark, which comprises 2,845 prompts validated by over 15,000
human pairwise comparisons. Our metrics achieve high human alignment (>75%),
with the HWPQ scheme reaching 85.9% accuracy on abstract semantics,
significantly outperforming VQA baselines. Comprehensive evaluation of
state-of-the-art models reveals no universal champion, as role-weighted scores
reorder rankings and provide actionable guidance for domain-specific
deployment. All resources are publicly released to foster reproducible T2I
assessment.

</details>


### [139] [An Interpretable Multi-Plane Fusion Framework With Kolmogorov-Arnold Network Guided Attention Enhancement for Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2508.06157)
*Xiaoxiao Yang,Meiliang Liu,Yunfang Xu,Zijin Li,Zhengye Si,Xinyue Yang,Zhiwen Zhao*

Main category: cs.CV

TL;DR: This paper proposes MPF-KANSC, a new AI framework for better Alzheimer's disease diagnosis using multi-plane fusion and advanced attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the difficulty of early and precise Alzheimer's disease diagnosis caused by subtle and complex brain structural changes, which current deep learning models can't effectively capture.

Method: The authors developed MPF-KANSC, which uses a multi-plane fusion for combining coronal, sagittal, and axial MRI features, alongside a Kolmogorov-Arnold Network-guided attention mechanism for flexible and precise analysis.

Result: The proposed model outperformed existing methods in diagnosing Alzheimer's on the ADNI dataset, offering superior accuracy and better feature extraction.

Conclusion: The MPF-KANSC model not only enhances diagnostic precision but also provides insights into right-lateralized brain structure changes, showing both effectiveness and interpretability.

Abstract: Alzheimer's disease (AD) is a progressive neurodegenerative disorder that
severely impairs cognitive function and quality of life. Timely intervention in
AD relies heavily on early and precise diagnosis, which remains challenging due
to the complex and subtle structural changes in the brain. Most existing deep
learning methods focus only on a single plane of structural magnetic resonance
imaging (sMRI) and struggle to accurately capture the complex and nonlinear
relationships among pathological regions of the brain, thus limiting their
ability to precisely identify atrophic features. To overcome these limitations,
we propose an innovative framework, MPF-KANSC, which integrates multi-plane
fusion (MPF) for combining features from the coronal, sagittal, and axial
planes, and a Kolmogorov-Arnold Network-guided spatial-channel attention
mechanism (KANSC) to more effectively learn and represent sMRI atrophy
features. Specifically, the proposed model enables parallel feature extraction
from multiple anatomical planes, thus capturing more comprehensive structural
information. The KANSC attention mechanism further leverages a more flexible
and accurate nonlinear function approximation technique, facilitating precise
identification and localization of disease-related abnormalities. Experiments
on the ADNI dataset confirm that the proposed MPF-KANSC achieves superior
performance in AD diagnosis. Moreover, our findings provide new evidence of
right-lateralized asymmetry in subcortical structural changes during AD
progression, highlighting the model's promising interpretability.

</details>


### [140] [Effective Training Data Synthesis for Improving MLLM Chart Understanding](https://arxiv.org/abs/2508.06492)
*Yuwei Yang,Zeyu Zhang,Yunzhong Hou,Zhuowan Li,Gaowen Liu,Ali Payani,Yuan-Sen Ting,Liang Zheng*

Main category: cs.CV

TL;DR: The paper introduces a five-step data synthesis pipeline for improving chart understanding in multimodal large language models (MLLMs), resulting in a new dataset, the Effective Chart Dataset (ECD), which enhances model performance.


<details>
  <summary>Details</summary>
Motivation: Current multimodal large language models struggle with chart understanding, achieving only a 30%-50% success rate on challenging benchmarks. Existing fine-tuning approaches using synthetic charts are limited by their lack of realism, leading to poor performance on real-world charts.

Method: The authors developed a modularized five-step data synthesis pipeline to generate training datasets. This pipeline includes separate data and function creation, conditional subplot generation, visual diversification of figures, quality filtering, and question-answer pair generation using GPT-4. The resulting dataset (ECD) consists of over 10,000 chart images and 300,000 QA pairs across 25 topics and 250+ chart types.

Result: Tests show that incorporating ECD consistently improves MLLM performance on real-world and synthetic benchmarks, enhancing chart understanding capabilities.

Conclusion: The proposed method and the ECD dataset address challenges in chart understanding and improve the effectiveness of MLLMs. The resources made openly available will facilitate further advances in the field.

Abstract: Being able to effectively read scientific plots, or chart understanding, is a
central part toward building effective agents for science. However, existing
multimodal large language models (MLLMs), especially open-source ones, are
still falling behind with a typical success rate of 30%-50% on challenging
benchmarks. Previous studies on fine-tuning MLLMs with synthetic charts are
often restricted by their inadequate similarity to the real charts, which could
compromise model training and performance on complex real-world charts. In this
study, we show that modularizing chart generation and diversifying visual
details improves chart understanding capabilities. In particular, we design a
five-step data synthesis pipeline, where we separate data and function creation
for single plot generation, condition the generation of later subplots on
earlier ones for multi-subplot figures, visually diversify the generated
figures, filter out low quality data, and finally generate the question-answer
(QA) pairs with GPT-4o. This approach allows us to streamline the generation of
fine-tuning datasets and introduce the effective chart dataset (ECD), which
contains 10k+ chart images and 300k+ QA pairs, covering 25 topics and featuring
250+ chart type combinations with high visual complexity. We show that ECD
consistently improves the performance of various MLLMs on a range of real-world
and synthetic test sets. Code, data and models are available at:
https://github.com/yuweiyang-anu/ECD.

</details>


### [141] [Fewer Denoising Steps or Cheaper Per-Step Inference: Towards Compute-Optimal Diffusion Model Deployment](https://arxiv.org/abs/2508.06160)
*Zhenbang Du,Yonggan Fu,Lifu Wang,Jiayi Qian,Xiao Luo,Yingyan,Lin*

Main category: cs.CV

TL;DR: This paper introduces PostDiff, a framework designed to accelerate diffusion models without fine-tuning, optimizing computational efficiency through mixed-resolution inputs and computation reuse.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the computational challenges of deploying diffusion models on resource-limited platforms and to determine the optimal strategy for compute-efficient deployment without retraining.

Method: PostDiff reduces inference costs post-training by (1) introducing a mixed-resolution scheme that prioritizes low-frequency components during early denoising steps and (2) utilizing a hybrid module caching strategy for reuse of computations.

Result: The framework achieves a significant improvement in the fidelity-efficiency trade-off for state-of-the-art diffusion models, demonstrating that reducing per-step inference cost is more effective than reducing the number of denoising steps.

Conclusion: PostDiff provides an effective solution for accelerating diffusion models, showing that strategic computation reduction can maintain fidelity while optimizing performance.

Abstract: Diffusion models have shown remarkable success across generative tasks, yet
their high computational demands challenge deployment on resource-limited
platforms. This paper investigates a critical question for compute-optimal
diffusion model deployment: Under a post-training setting without fine-tuning,
is it more effective to reduce the number of denoising steps or to use a
cheaper per-step inference? Intuitively, reducing the number of denoising steps
increases the variability of the distributions across steps, making the model
more sensitive to compression. In contrast, keeping more denoising steps makes
the differences smaller, preserving redundancy, and making post-training
compression more feasible. To systematically examine this, we propose PostDiff,
a training-free framework for accelerating pre-trained diffusion models by
reducing redundancy at both the input level and module level in a post-training
manner. At the input level, we propose a mixed-resolution denoising scheme
based on the insight that reducing generation resolution in early denoising
steps can enhance low-frequency components and improve final generation
fidelity. At the module level, we employ a hybrid module caching strategy to
reuse computations across denoising steps. Extensive experiments and ablation
studies demonstrate that (1) PostDiff can significantly improve the
fidelity-efficiency trade-off of state-of-the-art diffusion models, and (2) to
boost efficiency while maintaining decent generation fidelity, reducing
per-step inference cost is often more effective than reducing the number of
denoising steps. Our code is available at
https://github.com/GATECH-EIC/PostDiff.

</details>


### [142] [UW-3DGS: Underwater 3D Reconstruction with Physics-Aware Gaussian Splatting](https://arxiv.org/abs/2508.06169)
*Wenpeng Xing,Jie Chen,Zaifeng Yang,Changting Lin,Jianfeng Dong,Chaochao Chen,Xun Zhou,Meng Han*

Main category: cs.CV

TL;DR: This paper introduces UW-3DGS, a novel underwater 3D reconstruction framework leveraging 3D Gaussian Splatting, addressing light absorption, scattering, and turbidity challenges better than Neural Radiance Fields (NeRF) or its extensions.


<details>
  <summary>Details</summary>
Motivation: Underwater environments hinder traditional 3D reconstruction methods like NeRF due to light-related issues, necessitating a more effective and efficient approach for geometry and color fidelity.

Method: Key innovations include a learnable underwater image formation module using voxel-based regression and Physics-Aware Uncertainty Pruning (PAUP) to remove noisy artifacts caused by floating Gaussians.

Result: UW-3DGS demonstrated superior performance on SeaThru-NeRF and UWBundle datasets, yielding PSNR: 27.604, SSIM: 0.868, LPIPS: 0.104, and reducing floating artifacts by ~65%.

Conclusion: UW-3DGS significantly improves the accuracy and efficiency of underwater 3D reconstruction compared to existing methods, paving the way for enhanced scene fidelity and artifact minimization.

Abstract: Underwater 3D scene reconstruction faces severe challenges from light
absorption, scattering, and turbidity, which degrade geometry and color
fidelity in traditional methods like Neural Radiance Fields (NeRF). While NeRF
extensions such as SeaThru-NeRF incorporate physics-based models, their MLP
reliance limits efficiency and spatial resolution in hazy environments. We
introduce UW-3DGS, a novel framework adapting 3D Gaussian Splatting (3DGS) for
robust underwater reconstruction. Key innovations include: (1) a plug-and-play
learnable underwater image formation module using voxel-based regression for
spatially varying attenuation and backscatter; and (2) a Physics-Aware
Uncertainty Pruning (PAUP) branch that adaptively removes noisy floating
Gaussians via uncertainty scoring, ensuring artifact-free geometry. The
pipeline operates in training and rendering stages. During training, noisy
Gaussians are optimized end-to-end with underwater parameters, guided by PAUP
pruning and scattering modeling. In rendering, refined Gaussians produce clean
Unattenuated Radiance Images (URIs) free from media effects, while learned
physics enable realistic Underwater Images (UWIs) with accurate light
transport. Experiments on SeaThru-NeRF and UWBundle datasets show superior
performance, achieving PSNR of 27.604, SSIM of 0.868, and LPIPS of 0.104 on
SeaThru-NeRF, with ~65% reduction in floating artifacts.

</details>


### [143] [Synthetic Data-Driven Multi-Architecture Framework for Automated Polyp Segmentation Through Integrated Detection and Mask Generation](https://arxiv.org/abs/2508.06170)
*Ojonugwa Oluwafemi Ejiga Peter,Akingbola Oluwapemiisin,Amalahu Chetachi,Adeniran Opeyemi,Fahmi Khalifa,Md Mahmudur Rahman*

Main category: cs.CV

TL;DR: The paper presents a method to enhance automated polyp detection in colonoscopy images using a novel framework combining Stable Diffusion for synthetic data generation with Faster R-CNN and SAM for detection and segmentation.


<details>
  <summary>Details</summary>
Motivation: To enhance early detection and prevention of colorectal cancer, leveraging artificial intelligence to address challenges like limited medical datasets and complex image annotation.

Method: The method integrates Stable Diffusion for synthetic data generation, Faster R-CNN for object localization, and the SAM model for mask refinement. Various segmentation models, including FPN, U-Net, PSPNet, LinkNet, and MANet, were evaluated using ResNet34.

Result: Faster R-CNN achieved high recall (93.08%), precision (88.97%), and F1 score (90.98%). Among segmentation models, FPN showed the best PSNR (7.205893) and SSIM (0.492381), U-Net excelled in recall (84.85%), while LinkNet had balanced IoU (64.20%) and Dice score (77.53%).

Conclusion: The framework demonstrates a high level of effectiveness in enhancing polyp detection and segmentation, by leveraging a combination of state-of-the-art AI techniques and synthetic data generation.

Abstract: Colonoscopy is a vital tool for the early diagnosis of colorectal cancer,
which is one of the main causes of cancer-related mortality globally; hence, it
is deemed an essential technique for the prevention and early detection of
colorectal cancer. The research introduces a unique multidirectional
architectural framework to automate polyp detection within colonoscopy images
while helping resolve limited healthcare dataset sizes and annotation
complexities. The research implements a comprehensive system that delivers
synthetic data generation through Stable Diffusion enhancements together with
detection and segmentation algorithms. This detection approach combines Faster
R-CNN for initial object localization while the Segment Anything Model (SAM)
refines the segmentation masks. The faster R-CNN detection algorithm achieved a
recall of 93.08% combined with a precision of 88.97% and an F1 score of
90.98%.SAM is then used to generate the image mask. The research evaluated five
state-of-the-art segmentation models that included U-Net, PSPNet, FPN, LinkNet,
and MANet using ResNet34 as a base model. The results demonstrate the superior
performance of FPN with the highest scores of PSNR (7.205893) and SSIM
(0.492381), while UNet excels in recall (84.85%) and LinkNet shows balanced
performance in IoU (64.20%) and Dice score (77.53%).

</details>


### [144] [MA-CBP: A Criminal Behavior Prediction Framework Based on Multi-Agent Asynchronous Collaboration](https://arxiv.org/abs/2508.06189)
*Cheng Liu,Daou Zhang,Tingxu Liu,Yuhan Wang,Jinyang Chen,Yuexuan Li,Xinying Xiao,Chenbo Xin,Ziru Wang,Weichao Wu*

Main category: cs.CV

TL;DR: The paper introduces MA-CBP, a framework for predicting criminal behavior by analyzing real-time video streams and historical information, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Address challenges in criminal behavior detection linked to limitations in traditional feature recognition methods and delays in language model-based approaches.

Method: MA-CBP utilizes multi-agent asynchronous collaboration to transform video streams into semantic descriptions, construct historical summaries, and perform reasoning across diverse temporal contexts.

Result: The framework demonstrates superior performance in criminal behavior prediction on multiple datasets, supported by a newly developed annotated dataset.

Conclusion: MA-CBP offers a robust solution for predicting and warning about criminal activities, enhancing urban public safety measures.

Abstract: With the acceleration of urbanization, criminal behavior in public scenes
poses an increasingly serious threat to social security. Traditional anomaly
detection methods based on feature recognition struggle to capture high-level
behavioral semantics from historical information, while generative approaches
based on Large Language Models (LLMs) often fail to meet real-time
requirements. To address these challenges, we propose MA-CBP, a criminal
behavior prediction framework based on multi-agent asynchronous collaboration.
This framework transforms real-time video streams into frame-level semantic
descriptions, constructs causally consistent historical summaries, and fuses
adjacent image frames to perform joint reasoning over long- and short-term
contexts. The resulting behavioral decisions include key elements such as event
subjects, locations, and causes, enabling early warning of potential criminal
activity. In addition, we construct a high-quality criminal behavior dataset
that provides multi-scale language supervision, including frame-level,
summary-level, and event-level semantic annotations. Experimental results
demonstrate that our method achieves superior performance on multiple datasets
and offers a promising solution for risk warning in urban public safety
scenarios.

</details>


### [145] [A Semantic Segmentation Algorithm for Pleural Effusion Based on DBIF-AUNet](https://arxiv.org/abs/2508.06191)
*Ruixiang Tang,Jianglong Qin,Mingda Zhang,Yan Song,Yi Wu,Wei Wu*

Main category: cs.CV

TL;DR: This paper proposes DBIF-AUNet, a novel model for semantic segmentation of pleural effusion in CT images, achieving better accuracy and robustness than existing methods.


<details>
  <summary>Details</summary>
Motivation: Semantic segmentation of pleural effusion CT images is crucial for enhancing clinical diagnosis and treatment, but faces challenges due to similar gray levels, blurred edges, and varying morphology.

Method: The paper introduces DBIF-AUNet, which includes a Dual-Domain Feature Disentanglement module (DDFD) for feature complementarity, a Branch Interaction Attention Fusion module (BIAF) for dynamic feature fusion, and a nested deep supervision mechanism for addressing class imbalance.

Result: DBIF-AUNet demonstrates IoU and Dice scores of 80.1% and 89.0%, outperforming U-Net++ and Swin-UNet models by significant margins.

Conclusion: DBIF-AUNet effectively optimizes segmentation accuracy and robustness for pleural effusion CT images, proving its potential for clinical applicability and outperforming state-of-the-art methods.

Abstract: Pleural effusion semantic segmentation can significantly enhance the accuracy
and timeliness of clinical diagnosis and treatment by precisely identifying
disease severity and lesion areas. Currently, semantic segmentation of pleural
effusion CT images faces multiple challenges. These include similar gray levels
between effusion and surrounding tissues, blurred edges, and variable
morphology. Existing methods often struggle with diverse image variations and
complex edges, primarily because direct feature concatenation causes semantic
gaps. To address these challenges, we propose the Dual-Branch Interactive
Fusion Attention model (DBIF-AUNet). This model constructs a densely nested
skip-connection network and innovatively refines the Dual-Domain Feature
Disentanglement module (DDFD). The DDFD module orthogonally decouples the
functions of dual-domain modules to achieve multi-scale feature complementarity
and enhance characteristics at different levels. Concurrently, we design a
Branch Interaction Attention Fusion module (BIAF) that works synergistically
with the DDFD. This module dynamically weights and fuses global, local, and
frequency band features, thereby improving segmentation robustness.
Furthermore, we implement a nested deep supervision mechanism with hierarchical
adaptive hybrid loss to effectively address class imbalance. Through validation
on 1,622 pleural effusion CT images from Southwest Hospital, DBIF-AUNet
achieved IoU and Dice scores of 80.1% and 89.0% respectively. These results
outperform state-of-the-art medical image segmentation models U-Net++ and
Swin-UNet by 5.7%/2.7% and 2.2%/1.5% respectively, demonstrating significant
optimization in segmentation accuracy for complex pleural effusion CT images.

</details>


### [146] [LoRA in LoRA: Towards Parameter-Efficient Architecture Expansion for Continual Visual Instruction Tuning](https://arxiv.org/abs/2508.06202)
*Chang Che,Ziqi Wang,Pengwan Yang,Qi Wang,Hui Ma,Zenglin Shi*

Main category: cs.CV

TL;DR: This paper proposes LiLoRA, an efficient method to enable Continual Visual Instruction Tuning (CVIT) for multimodal large language models (MLLMs) with minimal parameter expansion while addressing catastrophic forgetting.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of catastrophic forgetting in CVIT for MLLMs while reducing parameter overhead and improving scalability in task-specific modules.

Method: The paper introduces LiLoRA, which shares LoRA matrix A across tasks, applies a low-rank decomposition to matrix B for task-specific parameter minimization, and uses cosine-regularized stability loss to maintain shared representation consistency.

Result: Experiments demonstrate that LiLoRA outperforms existing methods in sequential task learning and substantially improves parameter efficiency on a diverse CVIT benchmark.

Conclusion: LiLoRA is a scalable and efficient solution for CVIT, enabling MLLMs to continuously learn tasks with significantly less parameter redundancy and better performance.

Abstract: Continual Visual Instruction Tuning (CVIT) enables Multimodal Large Language
Models (MLLMs) to incrementally learn new tasks over time. However, this
process is challenged by catastrophic forgetting, where performance on
previously learned tasks deteriorates as the model adapts to new ones. A common
approach to mitigate forgetting is architecture expansion, which introduces
task-specific modules to prevent interference. Yet, existing methods often
expand entire layers for each task, leading to significant parameter overhead
and poor scalability. To overcome these issues, we introduce LoRA in LoRA
(LiLoRA), a highly efficient architecture expansion method tailored for CVIT in
MLLMs. LiLoRA shares the LoRA matrix A across tasks to reduce redundancy,
applies an additional low-rank decomposition to matrix B to minimize
task-specific parameters, and incorporates a cosine-regularized stability loss
to preserve consistency in shared representations over time. Extensive
experiments on a diverse CVIT benchmark show that LiLoRA consistently achieves
superior performance in sequential task learning while significantly improving
parameter efficiency compared to existing approaches.

</details>


### [147] [AnomalyMoE: Towards a Language-free Generalist Model for Unified Visual Anomaly Detection](https://arxiv.org/abs/2508.06203)
*Zhaopeng Gu,Bingke Zhu,Guibo Zhu,Yingying Chen,Wei Ge,Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: AnomalyMoE is a general anomaly detection framework that uses a Mixture-of-Experts (MoE) architecture to address diverse anomaly types across different semantic levels, significantly outperforming specialized models.


<details>
  <summary>Details</summary>
Motivation: Existing anomaly detection models are highly specialized and often fail to generalize across diverse types of anomalies, creating an urgent need for a universal approach.

Method: AnomalyMoE employs a hierarchical Mixture-of-Experts (MoE) architecture with three distinct expert networks focused on patch-level, component-level, and global-level anomalies. It incorporates Expert Information Repulsion (EIR) and Expert Selection Balancing (ESB) modules for diversity and comprehensive utilization.

Result: AnomalyMoE sets new state-of-the-art benchmarks across 8 datasets spanning varied tasks, outperforming domain-specific methods significantly.

Conclusion: The hierarchical design and expert diversity of AnomalyMoE enable robust anomaly detection, establishing its utility as a universal framework across diverse domains.

Abstract: Anomaly detection is a critical task across numerous domains and modalities,
yet existing methods are often highly specialized, limiting their
generalizability. These specialized models, tailored for specific anomaly types
like textural defects or logical errors, typically exhibit limited performance
when deployed outside their designated contexts. To overcome this limitation,
we propose AnomalyMoE, a novel and universal anomaly detection framework based
on a Mixture-of-Experts (MoE) architecture. Our key insight is to decompose the
complex anomaly detection problem into three distinct semantic hierarchies:
local structural anomalies, component-level semantic anomalies, and global
logical anomalies. AnomalyMoE correspondingly employs three dedicated expert
networks at the patch, component, and global levels, and is specialized in
reconstructing features and identifying deviations at its designated semantic
level. This hierarchical design allows a single model to concurrently
understand and detect a wide spectrum of anomalies. Furthermore, we introduce
an Expert Information Repulsion (EIR) module to promote expert diversity and an
Expert Selection Balancing (ESB) module to ensure the comprehensive utilization
of all experts. Experiments on 8 challenging datasets spanning industrial
imaging, 3D point clouds, medical imaging, video surveillance, and logical
anomaly detection demonstrate that AnomalyMoE establishes new state-of-the-art
performance, significantly outperforming specialized methods in their
respective domains.

</details>


### [148] [PA-HOI: A Physics-Aware Human and Object Interaction Dataset](https://arxiv.org/abs/2508.06205)
*Ruiyan Wang,Lin Zuo,Zonghao Lin,Qiang Wang,Zhengxue Cheng,Rong Xie,Jun Ling,Li Song*

Main category: cs.CV

TL;DR: This paper introduces the PA-HOI dataset for studying the impact of physical attributes of objects on human motion dynamics.


<details>
  <summary>Details</summary>
Motivation: Existing HOI datasets overlook the influence of physical object properties on human long-term motion.

Method: The authors developed the PA-HOI motion capture dataset containing 562 human-object interaction sequences, performed with 35 varied 3D objects.

Result: The dataset features a wide range of human posture, velocity, and interaction characteristics, extending the scope of existing studies.

Conclusion: PA-HOI enables realistic motion generation while incorporating physical awareness, bridging a gap in HOI studies.

Abstract: The Human-Object Interaction (HOI) task explores the dynamic interactions
between humans and objects in physical environments, providing essential
biomechanical and cognitive-behavioral foundations for fields such as robotics,
virtual reality, and human-computer interaction. However, existing HOI data
sets focus on details of affordance, often neglecting the influence of physical
properties of objects on human long-term motion. To bridge this gap, we
introduce the PA-HOI Motion Capture dataset, which highlights the impact of
objects' physical attributes on human motion dynamics, including human posture,
moving velocity, and other motion characteristics. The dataset comprises 562
motion sequences of human-object interactions, with each sequence performed by
subjects of different genders interacting with 35 3D objects that vary in size,
shape, and weight. This dataset stands out by significantly extending the scope
of existing ones for understanding how the physical attributes of different
objects influence human posture, speed, motion scale, and interacting
strategies. We further demonstrate the applicability of the PA-HOI dataset by
integrating it with existing motion generation methods, validating its capacity
to transfer realistic physical awareness.

</details>


### [149] [Interpretable Rheumatoid Arthritis Scoring via Anatomy-aware Multiple Instance Learning](https://arxiv.org/abs/2508.06218)
*Zhiyan Bo,Laura C. Coates,Bartlomiej W. Papiez*

Main category: cs.CV

TL;DR: This paper introduces a two-stage pipeline to automate the scoring of radiographic damage in RA using dual-hand radiographs, achieving near-radiologist-level accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the inefficiency and complexity of manual SvdH scoring for RA, which prevents its widespread use in routine clinical practice.

Method: The authors propose a pipeline using dual-hand radiographs with two region extraction schemes: sampling image tiles containing abnormalities and cropping patches of disease-relevant joints. Multiple instance learning is employed for prediction.

Result: The best model achieved PCC of 0.943 and RMSE of 15.73, while ensemble learning slightly improved these metrics to PCC of 0.945 and RMSE of 15.57, comparable to radiologists.

Conclusion: The automated pipeline effectively replicates radiologists’ accuracy and uses interpretable anatomical structures relevant to RA progression, making it promising for routine use.

Abstract: The Sharp/van der Heijde (SvdH) score has been widely used in clinical trials
to quantify radiographic damage in Rheumatoid Arthritis (RA), but its
complexity has limited its adoption in routine clinical practice. To address
the inefficiency of manual scoring, this work proposes a two-stage pipeline for
interpretable image-level SvdH score prediction using dual-hand radiographs.
Our approach extracts disease-relevant image regions and integrates them using
attention-based multiple instance learning to generate image-level features for
prediction. We propose two region extraction schemes: 1) sampling image tiles
most likely to contain abnormalities, and 2) cropping patches containing
disease-relevant joints. With Scheme 2, our best individual score prediction
model achieved a Pearson's correlation coefficient (PCC) of 0.943 and a root
mean squared error (RMSE) of 15.73. Ensemble learning further boosted
prediction accuracy, yielding a PCC of 0.945 and RMSE of 15.57, achieving
state-of-the-art performance that is comparable to that of experienced
radiologists (PCC = 0.97, RMSE = 18.75). Finally, our pipeline effectively
identified and made decisions based on anatomical structures which clinicians
consider relevant to RA progression.

</details>


### [150] [TEFormer: Texture-Aware and Edge-Guided Transformer for Semantic Segmentation of Urban Remote Sensing Images](https://arxiv.org/abs/2508.06224)
*Guoyu Zhou,Jing Zhang,Yi Yan,Hui Zhang,Li Zhuo*

Main category: cs.CV

TL;DR: The paper introduces TEFormer, a novel approach to improve semantic segmentation in urban remote sensing images using texture-aware and edge-guided mechanisms to address challenges like semantic ambiguity and irregular object shapes.


<details>
  <summary>Details</summary>
Motivation: Urban remote sensing image segmentation faces challenges like subtle texture differences, similar spatial structures, irregular shapes, and overlapping distributions, which lead to misclassification and semantic ambiguity.

Method: TEFormer integrates a texture-aware module, an edge-guided tri-branch decoder, and an edge-guided feature fusion module to enhance fine-grained texture detection, preserve edges, and fuse edge-contextual information.

Result: TEFormer achieves impressive mIoU scores of 88.57% (Potsdam), 81.46% (Vaihingen), and 53.55% (LoveDA datasets), showcasing superior performance in urban remote sensing image segmentation.

Conclusion: TEFormer effectively tackles the complexities of URSI segmentation by improving semantic discrimination and edge-preserving capabilities, demonstrating strong results across various datasets.

Abstract: Semantic segmentation of urban remote sensing images (URSIs) is crucial for
applications such as urban planning and environmental monitoring. However,
geospatial objects often exhibit subtle texture differences and similar spatial
structures, which can easily lead to semantic ambiguity and misclassification.
Moreover, challenges such as irregular object shapes, blurred boundaries, and
overlapping spatial distributions of semantic objects contribute to complex and
diverse edge morphologies, further complicating accurate segmentation. To
tackle these issues, we propose a texture-aware and edge-guided Transformer
(TEFormer) that integrates texture awareness and edge-guidance mechanisms for
semantic segmentation of URSIs. In the encoder, a texture-aware module (TaM) is
designed to capture fine-grained texture differences between visually similar
categories to enhance semantic discrimination. Then, an edge-guided tri-branch
decoder (Eg3Head) is constructed to preserve local edges and details for
multiscale context-awareness. Finally, an edge-guided feature fusion module
(EgFFM) is to fuse contextual and detail information with edge information to
realize refined semantic segmentation. Extensive experiments show that TEFormer
achieves mIoU of 88.57%, 81.46%, and 53.55% on the Potsdam, Vaihingen, and
LoveDA datasets, respectively, shows the effectiveness in URSI semantic
segmentation.

</details>


### [151] [Towards Unified Image Deblurring using a Mixture-of-Experts Decoder](https://arxiv.org/abs/2508.06228)
*Daniel Feijoo,Paula Garrido-Mellado,Jaesung Rim,Alvaro Garcia,Marcos V. Conde*

Main category: cs.CV

TL;DR: The paper presents an all-in-one image deblurring approach using a mixture-of-experts decoding module that generalizes across various blur types and scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing deblurring approaches are tailored to specific blur types, resulting in poor generalization and requiring multiple models for diverse scenarios, which is impractical.

Method: The authors propose a mixture-of-experts (MoE) decoding module that dynamically routes image features based on identified blur degradations for efficient image restoration in an end-to-end framework.

Result: The proposed method performs comparably to specialized models while showcasing exceptional robustness and generalization to unseen blur conditions.

Conclusion: The all-in-one approach offers a practical, efficient, and robust deblurring solution for varied blur types, addressing limitations of task-specific methods.

Abstract: Image deblurring, removing blurring artifacts from images, is a fundamental
task in computational photography and low-level computer vision. Existing
approaches focus on specialized solutions tailored to particular blur types,
thus, these solutions lack generalization. This limitation in current methods
implies requiring multiple models to cover several blur types, which is not
practical in many real scenarios. In this paper, we introduce the first
all-in-one deblurring method capable of efficiently restoring images affected
by diverse blur degradations, including global motion, local motion, blur in
low-light conditions, and defocus blur. We propose a mixture-of-experts (MoE)
decoding module, which dynamically routes image features based on the
recognized blur degradation, enabling precise and efficient restoration in an
end-to-end manner. Our unified approach not only achieves performance
comparable to dedicated task-specific models, but also demonstrates remarkable
robustness and generalization capabilities on unseen blur degradation
scenarios.

</details>


### [152] [Deepfake Detection that Generalizes Across Benchmarks](https://arxiv.org/abs/2508.06248)
*Andrii Yermakov,Jan Cech,Jiri Matas,Mario Fritz*

Main category: cs.CV

TL;DR: This paper proposes LNCLIP-DF, a parameter-efficient deepfake detection model using minimal adaptation of a pre-trained CLIP encoder, achieving superior generalization to unseen techniques.


<details>
  <summary>Details</summary>
Motivation: Deepfake detectors often struggle with generalizing to unseen manipulation techniques, and current solutions involve complex architectures, limiting practical deployment.

Method: The LNCLIP-DF method fine-tunes only Layer Normalization parameters of a CLIP vision encoder, introducing L2 normalization and latent space augmentations to enhance generalization.

Result: Extensive evaluations across 13 benchmark datasets from 2019 to 2025 show LNCLIP-DF achieving state-of-the-art performance in average cross-dataset AUROC, surpassing more complex methods.

Conclusion: Minimal, targeted adaptations to a pre-trained CLIP model can deliver superior generalization in deepfake detection, providing a computationally efficient and reproducible approach for practical deployment.

Abstract: The generalization of deepfake detectors to unseen manipulation techniques
remains a challenge for practical deployment. Although many approaches adapt
foundation models by introducing significant architectural complexity, this
work demonstrates that robust generalization is achievable through a
parameter-efficient adaptation of a pre-trained CLIP vision encoder. The
proposed method, LNCLIP-DF, fine-tunes only the Layer Normalization parameters
(0.03% of the total) and enhances generalization by enforcing a hyperspherical
feature manifold using L2 normalization and latent space augmentations.
  We conducted an extensive evaluation on 13 benchmark datasets spanning from
2019 to 2025. The proposed method achieves state-of-the-art performance,
outperforming more complex, recent approaches in average cross-dataset AUROC.
Our analysis yields two primary findings for the field: 1) training on paired
real-fake data from the same source video is essential for mitigating shortcut
learning and improving generalization, and 2) detection difficulty on academic
datasets has not strictly increased over time, with models trained on older,
diverse datasets showing strong generalization capabilities.
  This work delivers a computationally efficient and reproducible method,
proving that state-of-the-art generalization is attainable by making targeted,
minimal changes to a pre-trained CLIP model. The code will be made publicly
available upon acceptance.

</details>


### [153] [FedX: Explanation-Guided Pruning for Communication-Efficient Federated Learning in Remote Sensing](https://arxiv.org/abs/2508.06256)
*Barış Büyüktaş,Jonas Klotz,Begüm Demir*

Main category: cs.CV

TL;DR: Federated learning (FL) is adapted for remote sensing image classification by pruning model updates to reduce communication overhead through the FedX strategy.


<details>
  <summary>Details</summary>
Motivation: Federated learning often faces legal and privacy restrictions in remote sensing applications but struggles with communication inefficiencies due to frequent large model updates.

Method: The proposed FedX employs explanation-guided pruning, using backpropagation-based methods to rank and prune less relevant model components, thereby reducing the size of exchanged models.

Result: FedX significantly reduces communication overhead and enhances the generalization capability of the global model compared to unpruned and existing pruning methods.

Conclusion: FedX provides an effective solution for addressing FL communication bottlenecks in remote sensing applications by optimizing model update transmissions without sacrificing performance.

Abstract: Federated learning (FL) enables the collaborative training of deep neural
networks across decentralized data archives (i.e., clients), where each client
stores data locally and only shares model updates with a central server. This
makes FL a suitable learning paradigm for remote sensing (RS) image
classification tasks, where data centralization may be restricted due to legal
and privacy constraints. However, a key challenge in applying FL to RS tasks is
the communication overhead caused by the frequent exchange of large model
updates between clients and the central server. To address this issue, in this
paper we propose a novel strategy (denoted as FedX) that uses
explanation-guided pruning to reduce communication overhead by minimizing the
size of the transmitted models without compromising performance. FedX leverages
backpropagation-based explanation methods to estimate the task-specific
importance of model components and prunes the least relevant ones at the
central server. The resulting sparse global model is then sent to clients,
substantially reducing communication overhead. We evaluate FedX on multi-label
scene classification using the BigEarthNet-S2 dataset and single-label scene
classification using the EuroSAT dataset. Experimental results show the success
of FedX in significantly reducing the number of shared model parameters while
enhancing the generalization capability of the global model, compared to both
unpruned model and state-of-the-art pruning methods. The code of FedX will be
available at https://git.tu-berlin.de/rsim/FedX.

</details>


### [154] [XAG-Net: A Cross-Slice Attention and Skip Gating Network for 2.5D Femur MRI Segmentation](https://arxiv.org/abs/2508.06258)
*Byunghyun Ko,Anning Tian,Jeongkyu Lee*

Main category: cs.CV

TL;DR: The paper introduces XAG-Net, a 2.5D U-Net-based architecture achieving improved femur MRI segmentation by utilizing novel cross-slice attention and skip attention gating mechanisms.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of accurate femur segmentation from MRI using existing 2D and 3D deep learning-based approaches.

Method: Developed XAG-Net, featuring pixel-wise cross-slice attention (CSA) and skip attention gating (AG) mechanisms, enhancing contextual modeling and feature refinement in MRI segmentation.

Result: XAG-Net significantly improved femur segmentation accuracy over 2D, 2.5D, and 3D U-Net models, with efficient computational performance.

Conclusion: XAG-Net, integrating CSA and AG modules, offers an effective solution for femur MRI segmentation, proving its potential in orthopedic applications.

Abstract: Accurate segmentation of femur structures from Magnetic Resonance Imaging
(MRI) is critical for orthopedic diagnosis and surgical planning but remains
challenging due to the limitations of existing 2D and 3D deep learning-based
segmentation approaches. In this study, we propose XAG-Net, a novel 2.5D
U-Net-based architecture that incorporates pixel-wise cross-slice attention
(CSA) and skip attention gating (AG) mechanisms to enhance inter-slice
contextual modeling and intra-slice feature refinement. Unlike previous
CSA-based models, XAG-Net applies pixel-wise softmax attention across adjacent
slices at each spatial location for fine-grained inter-slice modeling.
Extensive evaluations demonstrate that XAG-Net surpasses baseline 2D, 2.5D, and
3D U-Net models in femur segmentation accuracy while maintaining computational
efficiency. Ablation studies further validate the critical role of the CSA and
AG modules, establishing XAG-Net as a promising framework for efficient and
accurate femur MRI segmentation.

</details>


### [155] [SIFThinker: Spatially-Aware Image Focus for Visual Reasoning](https://arxiv.org/abs/2508.06259)
*Zhangquan Chen,Ruihui Zhao,Chuwei Luo,Mingze Sun,Xinlei Yu,Yangyang Kang,Ruqi Huang*

Main category: cs.CV

TL;DR: This paper presents SIFThinker, a framework designed to improve complex visual reasoning tasks by incorporating spatial perception and iterative attention correction.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in multimodal large language models (MLLMs) for complex visual tasks like spatial understanding and fine-grained perception, where existing approaches fail to utilize iterative spatial attention corrections.

Method: The authors propose the SIFThinker framework, which uses depth-enhanced bounding boxes and language interleaving to mimic visual perception. They introduce a reverse-expansion-forward-inference strategy for process supervision and suggest GRPO-SIF, a reinforced training paradigm for dynamic spatial reasoning.

Result: Experiments demonstrate that SIFThinker surpasses state-of-the-art methods in spatial understanding and fine-grained perception tasks while retaining strong general-purpose performance.

Conclusion: The results confirm the effectiveness of SIFThinker in advancing MLLMs' capability in handling complex visual reasoning tasks by improving spatial awareness and attention refinement.

Abstract: Current multimodal large language models (MLLMs) still face significant
challenges in complex visual tasks (e.g., spatial understanding, fine-grained
perception). Prior methods have tried to incorporate visual reasoning, however,
they fail to leverage attention correction with spatial cues to iteratively
refine their focus on prompt-relevant regions. In this paper, we introduce
SIFThinker, a spatially-aware "think-with-images" framework that mimics human
visual perception. Specifically, SIFThinker enables attention correcting and
image region focusing by interleaving depth-enhanced bounding boxes and natural
language. Our contributions are twofold: First, we introduce a
reverse-expansion-forward-inference strategy that facilitates the generation of
interleaved image-text chains of thought for process-level supervision, which
in turn leads to the construction of the SIF-50K dataset. Besides, we propose
GRPO-SIF, a reinforced training paradigm that integrates depth-informed visual
grounding into a unified reasoning pipeline, teaching the model to dynamically
correct and focus on prompt-relevant regions. Extensive experiments demonstrate
that SIFThinker outperforms state-of-the-art methods in spatial understanding
and fine-grained visual perception, while maintaining strong general
capabilities, highlighting the effectiveness of our method.

</details>


### [156] [Uncertainty-quantified Rollout Policy Adaptation for Unlabelled Cross-domain Temporal Grounding](https://arxiv.org/abs/2508.06317)
*Jian Hu,Zixu Cheng,Shaogang Gong,Isabel Guan,Jianye Hao,Jun Wang,Kun Shao*

Main category: cs.CV

TL;DR: This paper introduces URPA to enable effective video temporal grounding across domains using a minimal amount of unlabelled target data.


<details>
  <summary>Details</summary>
Motivation: While Vision-Language Models show promise in semantic matching, they struggle with fine-grained temporal localization. Existing solutions like GRPO provide strong performance but lack scalability for unlabelled domains and real-time use.

Method: The paper proposes URPA, which uses GRPO rollouts to create pseudo labels and confidence metrics, integrating these metrics into reward weighting during training for efficient cross-domain adaptation.

Result: Experiments conducted on three datasets and six cross-domain settings demonstrate URPA’s capability to generalize effectively with minimal unlabelled target data.

Conclusion: URPA addresses the limitations of GRPO by enabling scalable domain adaptation without the need for target annotations, making it suitable for real-time, computationally efficient applications.

Abstract: Video Temporal Grounding (TG) aims to temporally locate video segments
matching a natural language description (a query) in a long video. While
Vision-Language Models (VLMs) are effective at holistic semantic matching, they
often struggle with fine-grained temporal localisation. Recently, Group
Relative Policy Optimisation (GRPO) reformulates the inference process as a
reinforcement learning task, enabling fine-grained grounding and achieving
strong in-domain performance. However, GRPO relies on labelled data, making it
unsuitable in unlabelled domains. Moreover, because videos are large and
expensive to store and process, performing full-scale adaptation introduces
prohibitive latency and computational overhead, making it impractical for
real-time deployment. To overcome both problems, we introduce a Data-Efficient
Unlabelled Cross-domain Temporal Grounding method, from which a model is first
trained on a labelled source domain, then adapted to a target domain using only
a small number of unlabelled videos from the target domain. This approach
eliminates the need for target annotation and keeps both computational and
storage overhead low enough to run in real time. Specifically, we introduce.
Uncertainty-quantified Rollout Policy Adaptation (URPA) for cross-domain
knowledge transfer in learning video temporal grounding without target labels.
URPA generates multiple candidate predictions using GRPO rollouts, averages
them to form a pseudo label, and estimates confidence from the variance across
these rollouts. This confidence then weights the training rewards, guiding the
model to focus on reliable supervision. Experiments on three datasets across
six cross-domain settings show that URPA generalises well using only a few
unlabelled target videos. Codes will be released once published.

</details>


### [157] [Mixture of Experts Guided by Gaussian Splatters Matters: A new Approach to Weakly-Supervised Video Anomaly Detection](https://arxiv.org/abs/2508.06318)
*Giacomo D'Amicantonio,Snehashis Majhi,Quan Kong,Lorenzo Garattoni,Gianpiero Francesca,François Bremond,Egor Bondarev*

Main category: cs.CV

TL;DR: The paper introduces GS-MoE, a model that uses specialized experts and Gaussian splatting for weakly-supervised video anomaly detection, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Current models struggle with detecting complex anomalies and dealing with weak supervision signals for VAD.

Method: The GS-MoE leverages expert models specialized in specific anomaly types, guided by a temporal Gaussian splatting mechanism, with predictions integrated through a mixture-of-experts framework.

Result: GS-MoE achieves 91.58% AUC on UCF-Crime and performs superiorly on XD-Violence and MSAD datasets.

Conclusion: The proposed GS-MoE framework improves VAD under weak supervision by addressing category-specific anomalies and enhancing temporal representation.

Abstract: Video Anomaly Detection (VAD) is a challenging task due to the variability of
anomalous events and the limited availability of labeled data. Under the
Weakly-Supervised VAD (WSVAD) paradigm, only video-level labels are provided
during training, while predictions are made at the frame level. Although
state-of-the-art models perform well on simple anomalies (e.g., explosions),
they struggle with complex real-world events (e.g., shoplifting). This
difficulty stems from two key issues: (1) the inability of current models to
address the diversity of anomaly types, as they process all categories with a
shared model, overlooking category-specific features; and (2) the weak
supervision signal, which lacks precise temporal information, limiting the
ability to capture nuanced anomalous patterns blended with normal events. To
address these challenges, we propose Gaussian Splatting-guided Mixture of
Experts (GS-MoE), a novel framework that employs a set of expert models, each
specialized in capturing specific anomaly types. These experts are guided by a
temporal Gaussian splatting loss, enabling the model to leverage temporal
consistency and enhance weak supervision. The Gaussian splatting approach
encourages a more precise and comprehensive representation of anomalies by
focusing on temporal segments most likely to contain abnormal events. The
predictions from these specialized experts are integrated through a
mixture-of-experts mechanism to model complex relationships across diverse
anomaly patterns. Our approach achieves state-of-the-art performance, with a
91.58% AUC on the UCF-Crime dataset, and demonstrates superior results on
XD-Violence and MSAD datasets. By leveraging category-specific expertise and
temporal guidance, GS-MoE sets a new benchmark for VAD under weak supervision.

</details>


### [158] [Can Diffusion Models Bridge the Domain Gap in Cardiac MR Imaging?](https://arxiv.org/abs/2508.06327)
*Xin Ci Wong,Duygu Sarikaya,Kieran Zucker,Marc De Kamps,Nishant Ravikumar*

Main category: cs.CV

TL;DR: The paper proposes a diffusion model (DM) to generate synthetic cardiac MRI data that addresses domain shift issues, improving segmentation performance on unseen domains without traditional approaches like transfer learning.


<details>
  <summary>Details</summary>
Motivation: To overcome domain shift challenges in cardiac MR imaging caused by variations in imaging devices and protocols that degrade AI model performance, especially in data-limited scenarios.

Method: A diffusion model (DM) is trained on a source domain to generate synthetic cardiac MR images with structural fidelity, used for domain generalization and domain adaptation in segmentation tasks.

Result: The approach improved segmentation performance on unseen domains using metrics like Welch's t-test, showing significant advantages over traditional real data methods.

Conclusion: The proposed method reduces reliance on transfer learning or online training, effectively addressing domain shift issues in cardiac MRI analysis, making it ideal for data-scarce environments.

Abstract: Magnetic resonance (MR) imaging, including cardiac MR, is prone to domain
shift due to variations in imaging devices and acquisition protocols. This
challenge limits the deployment of trained AI models in real-world scenarios,
where performance degrades on unseen domains. Traditional solutions involve
increasing the size of the dataset through ad-hoc image augmentation or
additional online training/transfer learning, which have several limitations.
Synthetic data offers a promising alternative, but anatomical/structural
consistency constraints limit the effectiveness of generative models in
creating image-label pairs. To address this, we propose a diffusion model (DM)
trained on a source domain that generates synthetic cardiac MR images that
resemble a given reference. The synthetic data maintains spatial and structural
fidelity, ensuring similarity to the source domain and compatibility with the
segmentation mask. We assess the utility of our generative approach in
multi-centre cardiac MR segmentation, using the 2D nnU-Net, 3D nnU-Net and
vanilla U-Net segmentation networks. We explore domain generalisation, where,
domain-invariant segmentation models are trained on synthetic source domain
data, and domain adaptation, where, we shift target domain data towards the
source domain using the DM. Both strategies significantly improved segmentation
performance on data from an unseen target domain, in terms of surface-based
metrics (Welch's t-test, p < 0.01), compared to training segmentation models on
real data alone. The proposed method ameliorates the need for transfer learning
or online training to address domain shift challenges in cardiac MR image
analysis, especially useful in data-scarce settings.

</details>


### [159] [ViPro-2: Unsupervised State Estimation via Integrated Dynamics for Guiding Video Prediction](https://arxiv.org/abs/2508.06335)
*Patrick Takenaka,Johannes Maucher,Marco F. Huber*

Main category: cs.CV

TL;DR: This paper improves video frame prediction by eliminating reliance on initial ground truth symbolic states, enabling unsupervised state inference.


<details>
  <summary>Details</summary>
Motivation: Previous models relied on given ground truth symbolic states, which created a shortcut. This approach failed when initial states were noisy.

Method: The authors improved the ViPro model to infer states from observations without ground truth states and extended the Orbits dataset to 3D for realism.

Result: The modified model successfully inferred symbolic states in an unsupervised manner and proved its applicability to real-world-like settings.

Conclusion: The advancements overcome limitations of earlier models, showing promise in robust video frame prediction for complex scenarios.

Abstract: Predicting future video frames is a challenging task with many downstream
applications. Previous work has shown that procedural knowledge enables deep
models for complex dynamical settings, however their model ViPro assumed a
given ground truth initial symbolic state. We show that this approach led to
the model learning a shortcut that does not actually connect the observed
environment with the predicted symbolic state, resulting in the inability to
estimate states given an observation if previous states are noisy. In this
work, we add several improvements to ViPro that enables the model to correctly
infer states from observations without providing a full ground truth state in
the beginning. We show that this is possible in an unsupervised manner, and
extend the original Orbits dataset with a 3D variant to close the gap to real
world scenarios.

</details>


### [160] [Street View Sociability: Interpretable Analysis of Urban Social Behavior Across 15 Cities](https://arxiv.org/abs/2508.06342)
*Kieran Elrod,Katherine Flanigan,Mario Bergés*

Main category: cs.CV

TL;DR: The study explores the use of street view imagery to assess social interactions using urban design theories, confirming connections between environmental features and sociability.


<details>
  <summary>Details</summary>
Motivation: There is a need for a scalable, cost-effective tool to measure qualitative aspects of sociability on streets, beyond pedestrian volume.

Method: The methodology involved analyzing 2,998 street view images with a multimodal large language model guided by sociability taxonomy, complemented by regression models controlling for external variables.

Result: Findings revealed correlations between urban design elements (like green and sky view indices) and various sociability types, aligning with established urban planning theories.

Conclusion: Street view imagery demonstrates potential as a global, privacy-preserving method for studying sociability, paving the way for improved urban design and theory testing.

Abstract: Designing socially active streets has long been a goal of urban planning, yet
existing quantitative research largely measures pedestrian volume rather than
the quality of social interactions. We hypothesize that street view imagery --
an inexpensive data source with global coverage -- contains latent social
information that can be extracted and interpreted through established social
science theory. As a proof of concept, we analyzed 2,998 street view images
from 15 cities using a multimodal large language model guided by Mehta's
taxonomy of passive, fleeting, and enduring sociability -- one illustrative
example of a theory grounded in urban design that could be substituted or
complemented by other sociological frameworks. We then used linear regression
models, controlling for factors like weather, time of day, and pedestrian
counts, to test whether the inferred sociability measures correlate with
city-level place attachment scores from the World Values Survey and with
environmental predictors (e.g., green, sky, and water view indices) derived
from individual street view images. Results aligned with long-standing urban
planning theory: the sky view index was associated with all three sociability
types, the green view index predicted enduring sociability, and place
attachment was positively associated with fleeting sociability. These results
provide preliminary evidence that street view images can be used to infer
relationships between specific types of social interactions and built
environment variables. Further research could establish street view imagery as
a scalable, privacy-preserving tool for studying urban sociability, enabling
cross-cultural theory testing and evidence-based design of socially vibrant
cities.

</details>


### [161] [Aligning Effective Tokens with Video Anomaly in Large Language Models](https://arxiv.org/abs/2508.06350)
*Yingxian Chen,Jiahui Liu,Ruifan Di,Yanwei Li,Chirui Chang,Shizhen Zhao,Wilton W. T. Fok,Xiaojuan Qi,Yik-Chung Wu*

Main category: cs.CV

TL;DR: This paper proposes VA-GPT, a novel Multi-modal Large Language Model (MLLM) designed to improve the detection and analysis of abnormal events in videos by addressing spatial and temporal sparsity issues.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current video understanding Multi-modal Large Language Models (MLLMs), which struggle with recognizing abnormal events due to information sparsity and redundancy.

Method: The proposed VA-GPT model employs Spatial Effective Token Selection (SETS) and Temporal Effective Token Generation (TETG) modules to align tokens between visual encoders and LLMs, improving spatial and temporal anomaly detection capabilities.

Result: VA-GPT demonstrates superior performance over state-of-the-art methods across various benchmarks, including the cross-domain XD-Violence dataset.

Conclusion: VA-GPT effectively handles video anomaly detection challenges by leveraging advancements in Vision Language Models (VLMs) and Large Language Models (LLMs), offering a robust solution for summarizing and localizing abnormal events.

Abstract: Understanding abnormal events in videos is a vital and challenging task that
has garnered significant attention in a wide range of applications. Although
current video understanding Multi-modal Large Language Models (MLLMs) are
capable of analyzing general videos, they often struggle to handle anomalies
due to the spatial and temporal sparsity of abnormal events, where the
redundant information always leads to suboptimal outcomes. To address these
challenges, exploiting the representation and generalization capabilities of
Vison Language Models (VLMs) and Large Language Models (LLMs), we propose
VA-GPT, a novel MLLM designed for summarizing and localizing abnormal events in
various videos. Our approach efficiently aligns effective tokens between visual
encoders and LLMs through two key proposed modules: Spatial Effective Token
Selection (SETS) and Temporal Effective Token Generation (TETG). These modules
enable our model to effectively capture and analyze both spatial and temporal
information associated with abnormal events, resulting in more accurate
responses and interactions. Furthermore, we construct an instruction-following
dataset specifically for fine-tuning video-anomaly-aware MLLMs, and introduce a
cross-domain evaluation benchmark based on XD-Violence dataset. Our proposed
method outperforms existing state-of-the-art methods on various benchmarks.

</details>


### [162] [An Implemention of Two-Phase Image Segmentation using the Split Bregman Method](https://arxiv.org/abs/2508.06351)
*Olakunle S. Abawonse,Günay Doğan*

Main category: cs.CV

TL;DR: This paper implements and evaluates a two-phase image segmentation algorithm that divides an image into foreground and background by assigning pixels based on average values and smooth region boundaries.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to implement and analyze the two-phase image segmentation algorithm by Goldstein, Bresson, and Osher, which optimizes the region-membership function for efficient foreground-background segmentation.

Method: The authors implemented the modified Chan-Vese energy model using the split Bregman method, which minimizes the energy combining data consistency and region boundary smoothness.

Result: The performance of the implemented segmentation algorithm was validated on several images with varying algorithm parameters, showcasing its efficiency.

Conclusion: The paper provides a detailed documentation of the segmentation algorithm's implementation and demonstrates its effectiveness on various datasets.

Abstract: In this paper, we describe an implementation of the two-phase image
segmentation algorithm proposed by Goldstein, Bresson, Osher in
\cite{gold:bre}. This algorithm partitions the domain of a given 2d image into
foreground and background regions, and each pixel of the image is assigned
membership to one of these two regions. The underlying assumption for the
segmentation model is that the pixel values of the input image can be
summarized by two distinct average values, and that the region boundaries are
smooth. Accordingly, the model is defined as an energy in which the variable is
a region membership function to assign pixels to either region, originally
proposed by Chan and Vese in \cite{chan:vese}. This energy is the sum of image
data terms in the regions and a length penalty for region boundaries.
Goldstein, Bresson, Osher modify the energy of Chan-Vese in \cite{gold:bre} so
that their new energy can be minimized efficiently using the split Bregman
method to produce an equivalent two-phase segmentation. We provide a detailed
implementation of this method \cite{gold:bre}, and document its performance
with several images over a range of algorithm parameters.

</details>


### [163] [Are you In or Out (of gallery)? Wisdom from the Same-Identity Crowd](https://arxiv.org/abs/2508.06357)
*Aman Bhatta,Maria Dhakal,Michael C. King,Kevin W. Bowyer*

Main category: cs.CV

TL;DR: This paper introduces a method to improve facial identification systems by predicting if the top-matching identity is already enrolled in the gallery using data from additional images.


<details>
  <summary>Details</summary>
Motivation: To tackle the challenge of identifying whether the top-ranked facial match from a gallery is legitimately linked to an enrolled identity (In-gallery) or not (Out-of-gallery), especially in challenging conditions like image degradation.

Method: The approach generates training data by deriving ranks of additional enrolled images linked to the top-ranked result, then trains a classifier to predict Out-of-gallery/In-gallery results using these ranks.

Result: The method demonstrates effectiveness on two datasets and under various conditions (e.g. image blur, atmospheric turbulence, etc.), maintaining consistent accuracy across demographic groups.

Conclusion: This approach provides a reliable mechanism for detecting Out-of-gallery cases, which can reduce false positives, wrongful arrests, and wasted investigative efforts. The method's success depends on the use of advanced CNN face matchers with margin-based loss functions.

Abstract: A central problem in one-to-many facial identification is that the person in
the probe image may or may not have enrolled image(s) in the gallery; that is,
may be In-gallery or Out-of-gallery. Past approaches to detect when a rank-one
result is Out-of-gallery have mostly focused on finding a suitable threshold on
the similarity score. We take a new approach, using the additional enrolled
images of the identity with the rank-one result to predict if the rank-one
result is In-gallery / Out-of-gallery. Given a gallery of identities and
images, we generate In-gallery and Out-of-gallery training data by extracting
the ranks of additional enrolled images corresponding to the rank-one identity.
We then train a classifier to utilize this feature vector to predict whether a
rank-one result is In-gallery or Out-of-gallery. Using two different datasets
and four different matchers, we present experimental results showing that our
approach is viable for mugshot quality probe images, and also, importantly, for
probes degraded by blur, reduced resolution, atmospheric turbulence and
sunglasses. We also analyze results across demographic groups, and show that
In-gallery / Out-of-gallery classification accuracy is similar across
demographics. Our approach has the potential to provide an objective estimate
of whether a one-to-many facial identification is Out-of-gallery, and thereby
to reduce false positive identifications, wrongful arrests, and wasted
investigative time. Interestingly, comparing the results of older deep
CNN-based face matchers with newer ones suggests that the effectiveness of our
Out-of-gallery detection approach emerges only with matchers trained using
advanced margin-based loss functions.

</details>


### [164] [Text as Any-Modality for Zero-Shot Classification by Consistent Prompt Tuning](https://arxiv.org/abs/2508.06382)
*Xiangyu Wu,Feng Yu,Yang Yang,Jianfeng Lu*

Main category: cs.CV

TL;DR: The paper introduces a scalable approach called TaAM-CPT for generalized multimodal learning using only text data, avoiding reliance on modality-specific labeled data.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing multimodal learning methods, which rely heavily on modality-specific labeled data or focus only on a single modality.

Method: TaAM-CPT uses modality prompt pools, text construction, and modality-aligned text encoders from pre-trained models, with intra- and inter-modal objectives to ensure semantic consistency across modalities.

Result: TaAM-CPT achieves leading performance across diverse datasets in tasks like video, image, and audio classification without using modality-specific labeled data.

Conclusion: TaAM-CPT enables scalable and extensible multimodal learning by leveraging text data as a universal modality, achieving strong generalization across multiple tasks.

Abstract: The integration of prompt tuning with multimodal learning has shown
significant generalization abilities for various downstream tasks. Despite
advancements, existing methods heavily depend on massive modality-specific
labeled data (e.g., video, audio, and image), or are customized for a single
modality. In this study, we present Text as Any-Modality by Consistent Prompt
Tuning (TaAM-CPT), a scalable approach for constructing a general
representation model toward unlimited modalities using solely text data.
TaAM-CPT comprises modality prompt pools, text construction, and
modality-aligned text encoders from pre-trained models, which allows for
extending new modalities by simply adding prompt pools and modality-aligned
text encoders. To harmonize the learning across different modalities, TaAM-CPT
designs intra- and inter-modal learning objectives, which can capture category
details within modalities while maintaining semantic consistency across
different modalities. Benefiting from its scalable architecture and pre-trained
models, TaAM-CPT can be seamlessly extended to accommodate unlimited
modalities. Remarkably, without any modality-specific labeled data, TaAM-CPT
achieves leading results on diverse datasets spanning various modalities,
including video classification, image classification, and audio classification.
The code is available at https://github.com/Jinx630/TaAM-CPT.

</details>


### [165] [FVGen: Accelerating Novel-View Synthesis with Adversarial Video Diffusion Distillation](https://arxiv.org/abs/2508.06392)
*Wenbin Teng,Gonglin Chen,Haiwei Chen,Yajie Zhao*

Main category: cs.CV

TL;DR: This paper introduces FVGen, a framework to accelerate novel view synthesis with Video Diffusion Models (VDMs), achieving comparable or better quality in just four steps while dramatically reducing sampling time.


<details>
  <summary>Details</summary>
Motivation: 3D reconstruction struggles with sparse input views, often creating artifacts due to poor visual coverage. Fast synthesis of dense observations is necessary to address these gaps and improve spatial coverage for reconstruction tasks.

Method: The paper proposes FVGen, which uses video diffusion model distillation. It employs a teacher-student approach where a multi-step denoising model is distilled into a few-step model using GANs and softened reverse KL-divergence minimization.

Result: Extensive tests on real-world datasets show FVGen generates novel views with similar or better quality while reducing sampling time by more than 90%.

Conclusion: FVGen significantly enhances efficiency in novel view generation and downstream 3D reconstruction tasks, especially under sparse view conditions.

Abstract: Recent progress in 3D reconstruction has enabled realistic 3D models from
dense image captures, yet challenges persist with sparse views, often leading
to artifacts in unseen areas. Recent works leverage Video Diffusion Models
(VDMs) to generate dense observations, filling the gaps when only sparse views
are available for 3D reconstruction tasks. A significant limitation of these
methods is their slow sampling speed when using VDMs. In this paper, we present
FVGen, a novel framework that addresses this challenge by enabling fast novel
view synthesis using VDMs in as few as four sampling steps. We propose a novel
video diffusion model distillation method that distills a multi-step denoising
teacher model into a few-step denoising student model using Generative
Adversarial Networks (GANs) and softened reverse KL-divergence minimization.
Extensive experiments on real-world datasets show that, compared to previous
works, our framework generates the same number of novel views with similar (or
even better) visual quality while reducing sampling time by more than 90%.
FVGen significantly improves time efficiency for downstream reconstruction
tasks, particularly when working with sparse input views (more than 2) where
pre-trained VDMs need to be run multiple times to achieve better spatial
coverage.

</details>


### [166] [A Classification-Aware Super-Resolution Framework for Ship Targets in SAR Imagery](https://arxiv.org/abs/2508.06407)
*Ch Muhammad Awais,Marco Reggiannini,Davide Moroni,Oktay Karakus*

Main category: cs.CV

TL;DR: This paper explores improving classification accuracy by integrating classification objectives into super-resolution techniques, offering a novel approach for synthesizing high-resolution images.


<details>
  <summary>Details</summary>
Motivation: Low-resolution images hinder the accuracy of automated visual tasks like classification and detection. Current SR methods focus on pixel quality but lack integration with classification performance improvement.

Method: The authors propose a methodology that optimizes loss functions accounting for both image quality and classification performance, targeting synthetic aperture radar imagery.

Result: The method enhances both image quality and classification accuracy as validated through image metrics and classification tests.

Conclusion: Integrating classification performance directly within the SR process yields better-quality images and improved classification outcomes.

Abstract: High-resolution imagery plays a critical role in improving the performance of
visual recognition tasks such as classification, detection, and segmentation.
In many domains, including remote sensing and surveillance, low-resolution
images can limit the accuracy of automated analysis. To address this,
super-resolution (SR) techniques have been widely adopted to attempt to
reconstruct high-resolution images from low-resolution inputs. Related
traditional approaches focus solely on enhancing image quality based on
pixel-level metrics, leaving the relationship between super-resolved image
fidelity and downstream classification performance largely underexplored. This
raises a key question: can integrating classification objectives directly into
the super-resolution process further improve classification accuracy? In this
paper, we try to respond to this question by investigating the relationship
between super-resolution and classification through the deployment of a
specialised algorithmic strategy. We propose a novel methodology that increases
the resolution of synthetic aperture radar imagery by optimising loss functions
that account for both image quality and classification performance. Our
approach improves image quality, as measured by scientifically ascertained
image quality indicators, while also enhancing classification accuracy.

</details>


### [167] [Feature-Space Oversampling for Addressing Class Imbalance in SAR Ship Classification](https://arxiv.org/abs/2508.06420)
*Ch Muhammad Awais,Marco Reggiannini,Davide Moroni,Oktay Karakus*

Main category: cs.CV

TL;DR: The paper addresses class imbalance in SAR ship classification datasets using novel oversampling algorithms, improving F1-scores significantly on public datasets.


<details>
  <summary>Details</summary>
Motivation: To tackle the long-tailed datasets problem in SAR ship classification, where underrepresented classes hinder accurate classification.

Method: Proposed two new algorithms (M2m$_f$ and M2m$_u$) inspired by the Major-to-minor method, tested across two public datasets (OpenSARShip and FuSARShip) using three models (ViT, VGG16, ResNet50).

Result: The new methods achieved an average F1-score improvement of 8.82% on FuSARShip and 4.44% on OpenSARShip over the baseline methods.

Conclusion: The proposed algorithms effectively address class imbalance, making them valuable for improving SAR ship classification in long-tailed datasets.

Abstract: SAR ship classification faces the challenge of long-tailed datasets, which
complicates the classification of underrepresented classes. Oversampling
methods have proven effective in addressing class imbalance in optical data. In
this paper, we evaluated the effect of oversampling in the feature space for
SAR ship classification. We propose two novel algorithms inspired by the
Major-to-minor (M2m) method M2m$_f$, M2m$_u$. The algorithms are tested on two
public datasets, OpenSARShip (6 classes) and FuSARShip (9 classes), using three
state-of-the-art models as feature extractors: ViT, VGG16, and ResNet50.
Additionally, we also analyzed the impact of oversampling methods on different
class sizes. The results demonstrated the effectiveness of our novel methods
over the original M2m and baselines, with an average F1-score increase of 8.82%
for FuSARShip and 4.44% for OpenSARShip.

</details>


### [168] [SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via Class-Conditioned Image Translation](https://arxiv.org/abs/2508.06429)
*Guido Manni,Clemente Lauretti,Loredana Zollo,Paolo Soda*

Main category: cs.CV

TL;DR: The paper proposes a GAN-based semi-supervised learning framework tailored for medical imaging with minimal labeled data, demonstrating improvements against existing methods across various labeled-data settings.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of inadequate labeled training data in medical imaging, particularly in scenarios where annotation is costly and data is scarce.

Method: A framework involving three neural networks (generator, discriminator, classifier) with supervised and unsupervised phases, leveraging image-to-image translation, ensemble-based pseudo-labeling, and temporal consistency for label estimation.

Result: Significant improvements over six state-of-the-art methods across eleven MedMNIST datasets, showing robust performance even in extremely low labeled-data settings (as few as 5 labeled samples/class).

Conclusion: The method provides a viable solution for medical imaging tasks, achieving reliable classification with minimal labeled data, and promises practical application in low-data regimes.

Abstract: Deep learning has revolutionized medical imaging, but its effectiveness is
severely limited by insufficient labeled training data. This paper introduces a
novel GAN-based semi-supervised learning framework specifically designed for
low labeled-data regimes, evaluated across settings with 5 to 50 labeled
samples per class. Our approach integrates three specialized neural networks --
a generator for class-conditioned image translation, a discriminator for
authenticity assessment and classification, and a dedicated classifier --
within a three-phase training framework. The method alternates between
supervised training on limited labeled data and unsupervised learning that
leverages abundant unlabeled images through image-to-image translation rather
than generation from noise. We employ ensemble-based pseudo-labeling that
combines confidence-weighted predictions from the discriminator and classifier
with temporal consistency through exponential moving averaging, enabling
reliable label estimation for unlabeled data. Comprehensive evaluation across
eleven MedMNIST datasets demonstrates that our approach achieves statistically
significant improvements over six state-of-the-art GAN-based semi-supervised
methods, with particularly strong performance in the extreme 5-shot setting
where the scarcity of labeled data is most challenging. The framework maintains
its superiority across all evaluated settings (5, 10, 20, and 50 shots per
class). Our approach offers a practical solution for medical imaging
applications where annotation costs are prohibitive, enabling robust
classification performance even with minimal labeled data. Code is available at
https://github.com/GuidoManni/SPARSE.

</details>


### [169] [MotionSwap](https://arxiv.org/abs/2508.06430)
*Om Patil,Jinesh Modi,Suryabha Mukhopadhyay,Meghaditya Giri,Chhavi Malhotra*

Main category: cs.CV

TL;DR: This paper enhances SimSwap for face swapping by integrating attention mechanisms, dynamic loss adjustment, and learning rate scheduling, achieving better quality and identity preservation.


<details>
  <summary>Details</summary>
Motivation: To advance face swapping technology for improved fidelity, identity preservation, and consistency in attributes.

Method: The authors improve SimSwap by adding self and cross-attention mechanisms, dynamic loss weighting, and cosine annealing learning rate scheduling.

Result: Enhanced model yields better identity similarity, lower FID scores, and qualitative improvements backed by experiments over 400,000 training iterations.

Conclusion: Key future directions include integrating StyleGAN3, improving lip synchronization, exploring 3D facial modeling, and ensuring temporal consistency in video applications.

Abstract: Face swapping technology has gained significant attention in both academic
research and commercial applications. This paper presents our implementation
and enhancement of SimSwap, an efficient framework for high fidelity face
swapping. We introduce several improvements to the original model, including
the integration of self and cross-attention mechanisms in the generator
architecture, dynamic loss weighting, and cosine annealing learning rate
scheduling. These enhancements lead to significant improvements in identity
preservation, attribute consistency, and overall visual quality.
  Our experimental results, spanning 400,000 training iterations, demonstrate
progressive improvements in generator and discriminator performance. The
enhanced model achieves better identity similarity, lower FID scores, and
visibly superior qualitative results compared to the baseline. Ablation studies
confirm the importance of each architectural and training improvement. We
conclude by identifying key future directions, such as integrating StyleGAN3,
improving lip synchronization, incorporating 3D facial modeling, and
introducing temporal consistency for video-based applications.

</details>


### [170] [CLIPin: A Non-contrastive Plug-in to CLIP for Multimodal Semantic Alignment](https://arxiv.org/abs/2508.06434)
*Shengzhu Yang,Jiawei Du,Shuai Lu,Weihang Zhang,Ningli Wang,Huiqi Li*

Main category: cs.CV

TL;DR: The paper introduces CLIPin, a plug-in for improving multimodal semantic alignment in CLIP-style architectures, addressing challenges in natural and medical datasets.


<details>
  <summary>Details</summary>
Motivation: To enhance CLIP's ability to learn robust and generalizable representations, particularly in datasets with loose alignment or low diversity.

Method: Introduced a non-contrastive plug-in, CLIPin, with shared pre-projectors for both modalities to harmonize contrastive and non-contrastive learning.

Result: Demonstrated effectiveness of CLIPin across multiple downstream tasks, showcasing its compatibility with various contrastive frameworks.

Conclusion: The proposed CLIPin enhances supervision and alignment robustness, serving as an effective, generalizable plug-and-play solution for multimodal representation learning.

Abstract: Large-scale natural image-text datasets, especially those automatically
collected from the web, often suffer from loose semantic alignment due to weak
supervision, while medical datasets tend to have high cross-modal correlation
but low content diversity. These properties pose a common challenge for
contrastive language-image pretraining (CLIP): they hinder the model's ability
to learn robust and generalizable representations. In this work, we propose
CLIPin, a unified non-contrastive plug-in that can be seamlessly integrated
into CLIP-style architectures to improve multimodal semantic alignment,
providing stronger supervision and enhancing alignment robustness. Furthermore,
two shared pre-projectors are designed for image and text modalities
respectively to facilitate the integration of contrastive and non-contrastive
learning in a parameter-compromise manner. Extensive experiments on diverse
downstream tasks demonstrate the effectiveness and generality of CLIPin as a
plug-and-play component compatible with various contrastive frameworks. Code is
available at https://github.com/T6Yang/CLIPin.

</details>


### [171] [TRUST: Leveraging Text Robustness for Unsupervised Domain Adaptation](https://arxiv.org/abs/2508.06452)
*Mattia Litrico,Mario Valerio Giuffrida,Sebastiano Battiato,Devis Tuia*

Main category: cs.CV

TL;DR: The paper introduces TRUST, a novel UDA method leveraging language modality to overcome classical and complex domain shifts by generating pseudo-labels from captions and utilizing multimodal soft-contrastive learning.


<details>
  <summary>Details</summary>
Motivation: Unsupervised domain adaptation methods struggle with complex domain shifts, like geographical changes where object and background appearances vary significantly. Language modalities show potential in improving adaptation robustness.

Method: TRUST generates pseudo-labels using captions, applies uncertainty estimation with normalised CLIP similarity scores, reweights classification loss to mitigate wrong pseudo-label effects, and employs multimodal soft-contrastive learning to align vision and language feature spaces.

Result: TRUST achieves state-of-the-art performance in both classical domain shifts (DomainNet) and complex shifts (GeoNet).

Conclusion: TRUST effectively addresses challenges in UDA by using language-guided pseudo labeling and multimodal contrastive learning, offering improved robustness and accuracy under diverse shift types.

Abstract: Recent unsupervised domain adaptation (UDA) methods have shown great success
in addressing classical domain shifts (e.g., synthetic-to-real), but they still
suffer under complex shifts (e.g. geographical shift), where both the
background and object appearances differ significantly across domains. Prior
works showed that the language modality can help in the adaptation process,
exhibiting more robustness to such complex shifts. In this paper, we introduce
TRUST, a novel UDA approach that exploits the robustness of the language
modality to guide the adaptation of a vision model. TRUST generates
pseudo-labels for target samples from their captions and introduces a novel
uncertainty estimation strategy that uses normalised CLIP similarity scores to
estimate the uncertainty of the generated pseudo-labels. Such estimated
uncertainty is then used to reweight the classification loss, mitigating the
adverse effects of wrong pseudo-labels obtained from low-quality captions. To
further increase the robustness of the vision model, we propose a multimodal
soft-contrastive learning loss that aligns the vision and language feature
spaces, by leveraging captions to guide the contrastive training of the vision
model on target images. In our contrastive loss, each pair of images acts as
both a positive and a negative pair and their feature representations are
attracted and repulsed with a strength proportional to the similarity of their
captions. This solution avoids the need for hardly determining positive and
negative pairs, which is critical in the UDA setting. Our approach outperforms
previous methods, setting the new state-of-the-art on classical (DomainNet) and
complex (GeoNet) domain shifts. The code will be available upon acceptance.

</details>


### [172] [Text Embedded Swin-UMamba for DeepLesion Segmentation](https://arxiv.org/abs/2508.06453)
*Ruida Cheng,Tejas Sudharshan Mathai,Pritam Mukherjee,Benjamin Hou,Qingqing Zhu,Zhiyong Lu,Matthew McAuliffe,Ronald M. Summers*

Main category: cs.CV

TL;DR: The paper introduces the Text-Swin-UMamba model for lesion segmentation by incorporating text from radiology reports into a CT image analysis workflow, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: The research aims to enhance the accuracy of lesion segmentation on CT scans by leveraging text descriptions from radiology reports alongside imaging features.

Method: The study integrates text-based descriptions into the Swin-UMamba architecture and evaluates performance using the ULS23 DeepLesion dataset.

Result: The model achieved a Dice Score of 82% and a Hausdorff distance of 6.58 pixels, outperforming previous models such as LanGuideMedSeg by 37%, xLSTM-UNet by 1.74%, and nnUNet by 0.22%. Statistical significance (p < 0.001) was demonstrated.

Conclusion: Integrating text with imaging features significantly enhances lesion segmentation models, offering improved metrics and potential clinical benefits.

Abstract: Segmentation of lesions on CT enables automatic measurement for clinical
assessment of chronic diseases (e.g., lymphoma). Integrating large language
models (LLMs) into the lesion segmentation workflow offers the potential to
combine imaging features with descriptions of lesion characteristics from the
radiology reports. In this study, we investigate the feasibility of integrating
text into the Swin-UMamba architecture for the task of lesion segmentation. The
publicly available ULS23 DeepLesion dataset was used along with short-form
descriptions of the findings from the reports. On the test dataset, a high Dice
Score of 82% and low Hausdorff distance of 6.58 (pixels) was obtained for
lesion segmentation. The proposed Text-Swin-UMamba model outperformed prior
approaches: 37% improvement over the LLM-driven LanGuideMedSeg model (p <
0.001),and surpassed the purely image-based xLSTM-UNet and nnUNet models by
1.74% and 0.22%, respectively. The dataset and code can be accessed at
https://github.com/ruida/LLM-Swin-UMamba

</details>


### [173] [WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface Temperature Estimation via Spatio-Temporal Fusion](https://arxiv.org/abs/2508.06485)
*Sofiane Bouaziz,Adel Hafiane,Raphael Canals,Rachid Nedjai*

Main category: cs.CV

TL;DR: This paper introduces WGAST, a novel deep learning framework for estimating daily Land Surface Temperature (LST) at 10 m resolution using spatio-temporal fusion of satellite data. WGAST outperforms existing methods in accuracy and robustness against noise.


<details>
  <summary>Details</summary>
Motivation: The need for precise and timely environmental monitoring is intensified by urbanization, climate change, and agricultural demands. Current satellite-based LST retrieval systems face a trade-off between spatial and temporal resolution, creating the need for advanced solutions.

Method: WGAST, the proposed method, is a weakly-supervised generative network using a conditional GAN architecture. It includes multi-stage processing—feature extraction, fusion, LST reconstruction, and noise reduction—leveraging encoders, cosine similarity, normalization, temporal attention, and Gaussian filtering. A PatchGAN discriminator reinforces the weakly supervised training.

Result: WGAST demonstrates superior performance by reducing RMSE by 17.18% and improving SSIM by 11.00% relative to the best baseline method. The approach is validated on 33 ground-based sensors and exhibits robustness against cloud-induced LST issues.

Conclusion: The findings highlight WGAST as a pioneering and effective solution for daily 10 m LST estimation, addressing a key gap in remote sensing. The model's performance and robustness affirm its utility for high-resolution environmental monitoring tasks.

Abstract: Urbanization, climate change, and agricultural stress are increasing the
demand for precise and timely environmental monitoring. Land Surface
Temperature (LST) is a key variable in this context and is retrieved from
remote sensing satellites. However, these systems face a trade-off between
spatial and temporal resolution. While spatio-temporal fusion methods offer
promising solutions, few have addressed the estimation of daily LST at 10 m
resolution. In this study, we present WGAST, a Weakly-Supervised Generative
Network for Daily 10 m LST Estimation via Spatio-Temporal Fusion of Terra
MODIS, Landsat 8, and Sentinel-2. WGAST is the first end-to-end deep learning
framework designed for this task. It adopts a conditional generative
adversarial architecture, with a generator composed of four stages: feature
extraction, fusion, LST reconstruction, and noise suppression. The first stage
employs a set of encoders to extract multi-level latent representations from
the inputs, which are then fused in the second stage using cosine similarity,
normalization, and temporal attention mechanisms. The third stage decodes the
fused features into high-resolution LST, followed by a Gaussian filter to
suppress high-frequency noise. Training follows a weakly supervised strategy
based on physical averaging principles and reinforced by a PatchGAN
discriminator. Experiments demonstrate that WGAST outperforms existing methods
in both quantitative and qualitative evaluations. Compared to the
best-performing baseline, on average, WGAST reduces RMSE by 17.18% and improves
SSIM by 11.00%. Furthermore, WGAST is robust to cloud-induced LST and
effectively captures fine-scale thermal patterns, as validated against 33
ground-based sensors. The code is available at
https://github.com/Sofianebouaziz1/WGAST.git.

</details>


### [174] [LightSwitch: Multi-view Relighting with Material-guided Diffusion](https://arxiv.org/abs/2508.06494)
*Yehonathan Litman,Fernando De la Torre,Shubham Tulsiani*

Main category: cs.CV

TL;DR: The paper presents Lightswitch, a finetuned diffusion framework for efficient multi-view relighting using intrinsic cues, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current 3D relighting techniques struggle with integrating intrinsic subject properties and large-scale multi-view data, resulting in lower-quality relighting.

Method: Lightswitch leverages a material-relighting diffusion framework incorporating intrinsic property cues, multi-views, and scalable denoising to enhance relighting.

Result: Lightswitch achieves superior 2D relighting quality compared to existing state-of-the-art methods and outperforms in relighting synthetic and real objects efficiently.

Conclusion: The proposed Lightswitch framework is a significant step forward in scalable and efficient 3D relighting, offering improvements in quality and computational efficiency.

Abstract: Recent approaches for 3D relighting have shown promise in integrating 2D
image relighting generative priors to alter the appearance of a 3D
representation while preserving the underlying structure. Nevertheless,
generative priors used for 2D relighting that directly relight from an input
image do not take advantage of intrinsic properties of the subject that can be
inferred or cannot consider multi-view data at scale, leading to subpar
relighting. In this paper, we propose Lightswitch, a novel finetuned
material-relighting diffusion framework that efficiently relights an arbitrary
number of input images to a target lighting condition while incorporating cues
from inferred intrinsic properties. By using multi-view and material
information cues together with a scalable denoising scheme, our method
consistently and efficiently relights dense multi-view data of objects with
diverse material compositions. We show that our 2D relighting prediction
quality exceeds previous state-of-the-art relighting priors that directly
relight from images. We further demonstrate that LightSwitch matches or
outperforms state-of-the-art diffusion inverse rendering methods in relighting
synthetic and real objects in as little as 2 minutes.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [175] [Accelerating Data Chunking in Deduplication Systems using Vector Instructions](https://arxiv.org/abs/2508.05797)
*Sreeharsha Udayashankar,Abdelrahman Baba,Samer Al-Kiswany*

Main category: cs.DC

TL;DR: VectorCDC uses vector CPU instructions to accelerate CDC in deduplication systems, achieving 8.35x-26.2x speedup.


<details>
  <summary>Details</summary>
Motivation: Current Content-defined Chunking (CDC) algorithms are performance bottlenecks in data deduplication systems because they process entire files slowly.

Method: Introduces VectorCDC, a method applying vector CPU instructions (like SSE/AVX) to accelerate hashless CDC algorithms.

Result: Achieves 8.35x - 26.2x higher throughput compared to existing vector-accelerated techniques without compromising deduplication efficiency.

Conclusion: VectorCDC effectively speeds up CDC algorithms without diminishing data deduplication space savings, making it broadly applicable across various CPU architectures.

Abstract: Content-defined Chunking (CDC) algorithms dictate the overall space savings
that deduplication systems achieve. However, due to their need to scan each
file in its entirety, they are slow and often the main performance bottleneck
within data deduplication. We present VectorCDC, a method to accelerate
hashless CDC algorithms using vector CPU instructions, such as SSE / AVX. Our
evaluation shows that VectorCDC is effective on Intel, AMD, ARM, and IBM CPUs,
achieving 8.35x - 26.2x higher throughput than existing vector-accelerated
techniques without affecting the deduplication space savings.

</details>


### [176] [A Dynamic Approach to Load Balancing in Cloud Infrastructure: Enhancing Energy Efficiency and Resource Utilization](https://arxiv.org/abs/2508.05821)
*Shadman Sakib,Ajay Katangur,Rahul Dubey*

Main category: cs.DC

TL;DR: The paper presents a Score-Based Dynamic Load Balancer (SBDLB) for cloud systems, improving resource utilization, response times, and energy efficiency.


<details>
  <summary>Details</summary>
Motivation: With the rapid growth in cloud computing, efficient load balancing is needed to maintain system performance and prevent server overload.

Method: A Score-Based Dynamic Load Balancer (SBDLB) was introduced, which allocates workloads to virtual machines using real-time performance metrics, tested with CloudSim 7G against a throttled strategy.

Result: SBDLB improved average response times by up to 37%, reduced processing times by 13%, and decreased operational costs by 15%.

Conclusion: SBDLB enhances resource utilization and optimizes energy consumption, making cloud systems more efficient and sustainable.

Abstract: Cloud computing has grown rapidly in recent years, mainly due to the sharp
increase in data transferred over the internet. This growth makes load
balancing a key part of cloud systems, as it helps distribute user requests
across servers to maintain performance, prevent overload, and ensure a smooth
user experience. Despite its importance, managing server resources and keeping
workloads balanced over time remains a major challenge in cloud environments.
This paper introduces a novel Score-Based Dynamic Load Balancer (SBDLB) that
allocates workloads to virtual machines based on real-time performance metrics.
The objective is to enhance resource utilization and overall system efficiency.
The method was thoroughly tested using the CloudSim 7G platform, comparing its
performance against the throttled load balancing strategy. Evaluations were
conducted across a variety of workloads and scenarios, demonstrating the
SBDLB's ability to adapt dynamically to workload fluctuations while optimizing
resource usage. The proposed method outperformed the throttled strategy,
improving average response times by 34% and 37% in different scenarios. It also
reduced data center processing times by an average of 13%. Over a 24-hour
simulation, the method decreased operational costs by 15%, promoting a more
energy-efficient and sustainable cloud infrastructure through reduced energy
consumption.

</details>


### [177] [Snowpark: Performant, Secure, User-Friendly Data Engineering and AI/ML Next To Your Data](https://arxiv.org/abs/2508.05904)
*Brandon Baker,Elliott Brossard,Chenwei Xie,Zihao Ye,Deen Liu,Yijun Xie,Arthur Zwiegincew,Nitya Kumar Sharma,Gaurav Jain,Eugene Retunsky,Mike Halcrow,Derek Denny-Brown,Istvan Cseri,Tyler Akidau,Yuxiong He*

Main category: cs.DC

TL;DR: This paper discusses Snowpark, a Snowflake solution enhancing AI and ML workflows by leveraging elastic scalability, secure execution, and integration with Snowflake's core infrastructure.


<details>
  <summary>Details</summary>
Motivation: To extend Snowflake's capabilities to support efficient and secure data engineering, AI, and ML workflows at scale.

Method: Snowpark employs features like Python package caching, improved workload scheduling, and data skew management while utilizing Snowflake's control plane and secure sandboxes to integrate into its computation infrastructure.

Result: The platform reduced query latency, enhanced workload customizability, and illustrated real-world success in large-scale AI, ML, and data engineering tasks.

Conclusion: Snowpark is an effective extension of Snowflake for supporting versatile and performance-oriented data, AI, and ML workloads in a scalable and secure manner.

Abstract: Snowflake revolutionized data analytics with an elastic architecture that
decouples compute and storage, enabling scalable solutions supporting data
architectures like data lake, data warehouse, data lakehouse, and data mesh.
Building on this foundation, Snowflake has advanced its AI Data Cloud vision by
introducing Snowpark, a managed turnkey solution that supports data engineering
and AI and ML workloads using Python and other programming languages.
  This paper outlines Snowpark's design objectives towards high performance,
strong security and governance, and ease of use. We detail the architecture of
Snowpark, highlighting its elastic scalability and seamless integration with
Snowflake core compute infrastructure. This includes leveraging Snowflake
control plane for distributed computing and employing a secure sandbox for
isolating Snowflake SQL workloads from Snowpark executions. Additionally, we
present core innovations in Snowpark that drive further performance
enhancements, such as query initialization latency reduction through Python
package caching, improved workload scheduling for customized workloads, and
data skew management via efficient row redistribution. Finally, we showcase
real-world case studies that illustrate Snowpark's efficiency and effectiveness
for large-scale data engineering and AI and ML tasks.

</details>


### [178] [KnapFormer: An Online Load Balancer for Efficient Diffusion Transformers Training](https://arxiv.org/abs/2508.06001)
*Kai Zhang,Peng Wang,Sai Bi,Jianming Zhang,Yuanjun Xiong*

Main category: cs.DC

TL;DR: KnapFormer is a framework that efficiently balances workloads and implements sequence parallelism for distributed training of Diffusion Transformers, achieving notable speedups and reduced workload discrepancies.


<details>
  <summary>Details</summary>
Motivation: Variable-length input sequences and varying token counts in mixed-resolution or joint image-video datasets lead to workload imbalances across GPUs during distributed training.

Method: KnapFormer addresses imbalances by redistributing tokens using metadata collection and solving a global knapsack problem, incorporating sequence parallelism and a workload model to minimize workload variance.

Result: The framework achieves less than 1% workload discrepancy, eliminates stragglers, and provides a 2x to 3x training speedup in real-world setups.

Conclusion: KnapFormer effectively handles token imbalance and sequence parallelism in training Diffusion Transformers, optimizing distributed training performance with minimal overhead. The implementation is open-sourced.

Abstract: We present KnapFormer, an efficient and versatile framework to combine
workload balancing and sequence parallelism in distributed training of
Diffusion Transformers (DiT). KnapFormer builds on the insight that strong
synergy exists between sequence parallelism and the need to address the
significant token imbalance across ranks. This imbalance arises from
variable-length text inputs and varying visual token counts in mixed-resolution
and image-video joint training. KnapFormer redistributes tokens by first
gathering sequence length metadata across all ranks in a balancing group and
solving a global knapsack problem. The solver aims to minimize the variances of
total workload per-GPU, while accounting for the effect of sequence
parallelism. By integrating DeepSpeed-Ulysees-based sequence parallelism in the
load-balancing decision process and utilizing a simple semi-empirical workload
model, KnapFormers achieves minimal communication overhead and less than 1%
workload discrepancy in real-world training workloads with sequence length
varying from a few hundred to tens of thousands. It eliminates straggler
effects and achieves 2x to 3x speedup when training state-of-the-art diffusion
models like FLUX on mixed-resolution and image-video joint data corpora. We
open-source the KnapFormer implementation at
https://github.com/Kai-46/KnapFormer/

</details>


### [179] [EC2MoE: Adaptive End-Cloud Pipeline Collaboration Enabling Scalable Mixture-of-Experts Inference](https://arxiv.org/abs/2508.06024)
*Zheming Yang,Yunqing Hu,Sheng Sun,Wen Ji*

Main category: cs.DC

TL;DR: The paper introduces EC2MoE, a framework designed for scalable Mixture-of-Experts (MoE) inference across end-cloud environments, improving throughput, latency, and scalability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in expert scheduling, communication overhead, and resource heterogeneity while deploying MoE models in heterogeneous end-cloud setups.

Method: The authors propose a hardware-aware lightweight group gate network for adaptive expert selection and a pipeline optimization mechanism involving end-cloud collaboration, low-rank compression, and route-aware pipeline scheduling.

Result: EC2MoE achieves a throughput increase of 2.2x to 5.1x, a latency reduction of 53% to 67%, and maintains high accuracy over state-of-the-art methods.

Conclusion: The proposed EC2MoE framework enhances scalable inference, improves efficiency across multi-device systems, and remains robust under dynamic conditions.

Abstract: The Mixture-of-Experts (MoE) paradigm has emerged as a promising solution to
scale up model capacity while maintaining inference efficiency. However,
deploying MoE models across heterogeneous end-cloud environments poses new
challenges in expert scheduling, communication overhead, and resource
heterogeneity. In this paper, we propose EC2MoE, an adaptive framework for
scalable MoE inference via end-cloud pipeline collaboration. First, we design a
hardware-aware lightweight group gate network that enhances expert selection
and computational efficiency. By incorporating a hardware-aware local expert
selection mechanism, the system adaptively filters candidate experts based on
real-time device profiles. A lightweight group gate module then integrates
local and global gating outputs to achieve high-quality expert routing with
minimal overhead. Second, we develop a pipeline optimization mechanism based on
endcloud collaboration to accelerate MoE inference. This includes an
encoder-decoder structure based on low-rank compression, which reduces
transmission and computation costs. And a route-aware heuristic pipeline
scheduling algorithm that dynamically allocates inference stages across devices
according to workload and network topology. Extensive experiments show that
EC2MoE can increase throughput by 2.2x to 5.1x and reduce end-to-end latency by
53% to 67% while maintaining high accuracy compared to state-of-the-art
methods. It also maintains good scalability under dynamic load and network
environments.

</details>


### [180] [KV Cache Compression for Inference Efficiency in LLMs: A Review](https://arxiv.org/abs/2508.06297)
*Yanyu Liu,Jingying Fu,Sixiang Liu,Yitian Zou,You Fu,Jiehan Zhou,Shouhua Zhang*

Main category: cs.DC

TL;DR: The paper reviews optimization techniques for Key-Value (KV) caching in large language models, addressing memory bottlenecks and exploring strategies to enhance inference efficiency.


<details>
  <summary>Details</summary>
Motivation: The exponential growth in KV caching demand due to increasing context lengths in large language models has created significant memory bottlenecks, necessitating optimization for better scalability and performance during inference.

Method: A systematic review of KV cache optimization techniques, including compression strategies, quantization, attention compression, and evaluation of their effectiveness, trade-offs, and applicability.

Result: Comprehensive analysis of the impact of various KV cache optimization techniques on memory usage and inference speed, with insights into their limitations and challenges.

Conclusion: Future directions for research are proposed, such as hybrid optimization techniques, adaptive strategies, and software-hardware co-design to further improve inference efficiency and practical applications of LLMs.

Abstract: Withtherapid advancement of large language models (LLMs), the context length
for inference has been continuously increasing, leading to an exponential
growth in the demand for Key-Value (KV) caching. This has resulted in a
significant memory bottleneck, limiting the inference efficiency and
scalability of the models. Therefore, optimizing the KV cache during inference
is crucial for enhancing performance and efficiency. This review systematically
examines current KV cache optimization techniques, including compression
strategies such as selective token strategies, quantization, and attention
compression. We evaluate the effectiveness, trade-offs, and application
scenarios of these methods, providing a comprehensive analysis of their impact
on memory usage and inference speed. We focus on identifying the limitations
and challenges of existing methods, such as compatibility issues with different
models and tasks. Additionally, this review highlights future research
directions, including hybrid optimization techniques, adaptive dynamic
strategies, and software-hardware co-design. These approaches aim to improve
inference efficiency and promote the practical application of large language
models.

</details>


### [181] [Performant Unified GPU Kernels for Portable Singular Value Computation Across Hardware and Precision](https://arxiv.org/abs/2508.06339)
*Evelyne Ringoot,Rabab Alomairy,Valentin Churavy,Alan Edelman*

Main category: cs.DC

TL;DR: The paper introduces a GPU-accelerated QR-based singular value decomposition (SVD) algorithm implemented in Julia, excelling in portability and performance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the need for a portable and high-performance GPU-accelerated SVD implementation, crucial for tasks like low-rank adaptation in large-scale machine learning systems.

Method: The algorithm employs a QR reduction approach, integrating Julia's multiple dispatch and metaprogramming with GPUArrays and KernelAbstractions to ensure type and hardware-agnostic functionality across multiple architectures and data types.

Result: The implementation is compatible with diverse GPU frameworks, including Apple Metal GPUs and half precision, outperforming many existing libraries for matrices larger than 1024x1024, achieving 80%-90% of NVIDIA’s cuSOLVER performance.

Conclusion: The presented solution demonstrates that efficient and portable GPU-accelerated SVD computation is achievable without sacrificing performance, contributing significantly to both numerical computing and machine learning workflows.

Abstract: This paper presents a portable, GPU-accelerated implementation of a QR-based
singular value computation algorithm in Julia. The singular value ecomposition
(SVD) is a fundamental numerical tool in scientific computing and machine
learning, providing optimal low-rank matrix approximations. Its importance has
increased even more in large-scale machine learning pipelines, including large
language models (LLMs), where it enables low-rank adaptation (LoRA). The
implemented algorithm is based on the classic two-stage QR reduction,
consisting of successive matrix reduction to band form and bidiagonal form. Our
implementation leverages Julia's multiple dispatch and metaprogramming
capabilities, integrating with the GPUArrays and KernelAbstractions frameworks
to provide a unified type and hardware-agnostic function. It supports diverse
GPU architectures and data types, and is, to our knowledge, the first
GPU-accelerated singular value implementation to support Apple Metal GPUs and
half precision. Performance results on multiple GPU backends and data types
demonstrate that portability does not require sacrificing performance: the
unified function outperforms most linear algebra libraries (MAGMA, SLATE,
rocSOLVER, oneMKL) for matrix sizes larger than 1024x1024, and achieves 80%-90%
of the performance of cuSOLVER for large matrices.

</details>


### [182] [Blockchain-Enabled Federated Learning](https://arxiv.org/abs/2508.06406)
*Murtaza Rangwala,Venugopal K R,Rajkumar Buyya*

Main category: cs.DC

TL;DR: The paper analyzes blockchain-enabled federated learning (BCFL) systems, providing a taxonomy and case study, and validates their practical viability in real-world applications.


<details>
  <summary>Details</summary>
Motivation: To address challenges of trust, privacy, and coordination in collaborative AI systems through blockchain-enabled federated learning.

Method: The paper presents a four-dimensional taxonomy to analyze BCFL systems, detailed consensus mechanisms, multi-tier storage architectures, and a technical case study of the TrustMesh framework.

Result: The study demonstrates scalable, secure, and fault-tolerant BCFL systems with effective collaborative learning across non-IID data and real-world applicability in healthcare, finance, and IoT.

Conclusion: BCFL systems can achieve centralized system-like performance while offering enhanced security and enabling trustless collaborative AI models.

Abstract: Blockchain-enabled federated learning (BCFL) addresses fundamental challenges
of trust, privacy, and coordination in collaborative AI systems. This chapter
provides comprehensive architectural analysis of BCFL systems through a
systematic four-dimensional taxonomy examining coordination structures,
consensus mechanisms, storage architectures, and trust models. We analyze
design patterns from blockchain-verified centralized coordination to fully
decentralized peer-to-peer networks, evaluating trade-offs in scalability,
security, and performance. Through detailed examination of consensus mechanisms
designed for federated learning contexts, including Proof of Quality and Proof
of Federated Learning, we demonstrate how computational work can be repurposed
from arbitrary cryptographic puzzles to productive machine learning tasks. The
chapter addresses critical storage challenges by examining multi-tier
architectures that balance blockchain's transaction constraints with neural
networks' large parameter requirements while maintaining cryptographic
integrity. A technical case study of the TrustMesh framework illustrates
practical implementation considerations in BCFL systems through distributed
image classification training, demonstrating effective collaborative learning
across IoT devices with highly non-IID data distributions while maintaining
complete transparency and fault tolerance. Analysis of real-world deployments
across healthcare consortiums, financial services, and IoT security
applications validates the practical viability of BCFL systems, achieving
performance comparable to centralized approaches while providing enhanced
security guarantees and enabling new models of trustless collaborative
intelligence.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [183] [Diagrams-to-Dynamics (D2D): Exploring Causal Loop Diagram Leverage Points under Uncertainty](https://arxiv.org/abs/2508.05659)
*Jeroen F. Uleman,Loes Crielaard,Leonie K. Elsenburg,Guido A. Veldhuis,Karien Stronks,Naja Hulvej Rod,Rick Quax,Vítor V. Vasconcelos*

Main category: cs.LG

TL;DR: The paper introduces Diagrams-to-Dynamics (D2D), a method to convert causal loop diagrams (CLDs) into exploratory system dynamics models (SDMs), enabling dynamic analysis and simulation of interventions under uncertainty.


<details>
  <summary>Details</summary>
Motivation: Causal loop diagrams (CLDs) are limited in dynamic analysis and intervention strategy development, and existing quantitative analysis methods like network centrality analysis often lead to false inferences.

Method: D2D converts CLDs into exploratory SDMs with minimal user input by labeling variables as stocks, flows/auxiliaries, or constants. The method then leverages the structural information in CLDs to simulate hypothetical interventions and identify leverage points while accounting for uncertainty.

Result: D2D distinguished high- and low-ranked leverage points, showed greater consistency with a data-driven SDM compared to network centrality analysis, and provided uncertainty estimates and data collection guidance.

Conclusion: D2D lowers the barrier for dynamic modeling of CLDs through an open-source Python package and web application, supporting researchers in testing interventions and improving data-driven modeling practices.

Abstract: Causal loop diagrams (CLDs) are widely used in health and environmental
research to represent hypothesized causal structures underlying complex
problems. However, as qualitative and static representations, CLDs are limited
in their ability to support dynamic analysis and inform intervention
strategies. Additionally, quantitative CLD analysis methods like network
centrality analysis often lead to false inference. We propose
Diagrams-to-Dynamics (D2D), a method for converting CLDs into exploratory
system dynamics models (SDMs) in the absence of empirical data. With minimal
user input - following a protocol to label variables as stocks,
flows/auxiliaries, or constants - D2D leverages the structural information
already encoded in CLDs, namely, link existence and polarity, to simulate
hypothetical interventions and explore potential leverage points under
uncertainty. Results suggest that D2D helps distinguish between high- and
low-ranked leverage points. We compare D2D to a data-driven SDM constructed
from the same CLD and variable labeling. D2D showed greater consistency with
the data-driven model than network centrality analysis, while providing
uncertainty estimates and guidance for future data collection. The method is
implemented in an open-source Python package and a web-based application to
support further testing and lower the barrier to dynamic modeling for
researchers working with CLDs. We expect additional validation will further
establish the approach's utility across a broad range of cases and domains.

</details>


### [184] [A Graph Neural Network Approach for Mapping the Conceptual Structure and Inter-Branch Connectivity of Physics](https://arxiv.org/abs/2508.05724)
*Massimiliano Romiti*

Main category: cs.LG

TL;DR: The paper introduces a knowledge graph framework for physical laws, leveraging 400 cleaned physics equations and a Graph Attention Network (GAT) model for link prediction. Results significantly outperform baselines, revealing insights into physics interconnections.


<details>
  <summary>Details</summary>
Motivation: The motivation was to better understand and systematize the structure of physical laws using graph-based approaches to bridge gaps between various physical domains and enable hypothesis generation.

Method: The authors created a weighted knowledge graph from 400 cleaned physics equations, defined weights for connections based on normalized metrics, and used a GAT model for link prediction, comparing with classical and other GNN methods.

Result: The GAT model achieved superior link prediction test AUC (0.9742), outperforming baselines and other architectures. It uncovered conceptual connections, critical hub equations, and provided testable hypotheses.

Conclusion: The framework proved effective in analyzing physics concepts and domains, generating new cross-domain hypotheses, and offering datasets for further specialized analysis. It demonstrates potential for innovative theoretical discoveries.

Abstract: This work introduces a novel framework for representing and analyzing
physical laws as a weighted knowledge graph. We constructed a database of 659
distinct physical equations, subjected to rigorous semantic cleaning to resolve
notational ambiguities, resulting in a corpus of 400 advanced physics
equations. We developed an enhanced graph representation where both physical
concepts and equations are nodes, connected by weighted inter-equation bridges.
These weights are objectively defined using normalized metrics for variable
overlap, physics-informed importance scores, and bibliometric data. A Graph
Attention Network (GAT) was trained for link prediction, achieving a test AUC
of 0.9742 +/- 0.0018 across five independent runs, significantly outperforming
both classical heuristics (best baseline AUC: 0.9487) and established GNN
architectures like GraphSAGE (AUC: 0.9504, p = 0.029). Statistical testing
confirmed significance of all comparisons (p < 0.05), with 2.7% improvement
over the best baseline. Our analysis reveals three key findings: (i) The model
autonomously rediscovers the known macroscopic structure of physics,
identifying strong conceptual axes between Electromagnetism and Statistical
Mechanics. (ii) It identifies central hub equations that serve as critical
bridges between multiple physical domains. (iii) The model generates stable,
computationally-derived hypotheses for cross-domain relationships, identifying
both known principles and suggesting novel mathematical analogies for further
theoretical investigation. The framework can generate hundreds of such
hypotheses, enabling the creation of specialized datasets for targeted analysis
of specific physics subfields. Code and data available at
https://github.com/kingelanci/graphysics

</details>


### [185] [Machine Learning-Based Nonlinear Nudging for Chaotic Dynamical Systems](https://arxiv.org/abs/2508.05778)
*Jaemin Oh,Jinsil Lee,Youngjoon Hong*

Main category: cs.LG

TL;DR: Nudging is a data assimilation method which adjusts model dynamics using observations. This study introduces a neural network-based approach to improve nudging in nonlinear state space models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of designing effective nudging terms for nonlinear state space models, which are difficult compared to linear models.

Method: A neural network-based nudging method is proposed, supported by theoretical analysis using observer theory, and tested on chaotic benchmark problems.

Result: The method is evaluated on models such as Lorenz 96, Kuramoto-Sivashinsky, and Kolmogorov flow, showcasing its effectiveness in chaotic systems.

Conclusion: Neural network nudging enhances the ability to incorporate observation data in nonlinear systems, overcoming limitations of classical nudging techniques.

Abstract: Nudging is an empirical data assimilation technique that incorporates an
observation-driven control term into the model dynamics. The trajectory of the
nudged system approaches the true system trajectory over time, even when the
initial conditions differ. For linear state space models, such control terms
can be derived under mild assumptions. However, designing effective nudging
terms becomes significantly more challenging in the nonlinear setting. In this
work, we propose neural network nudging, a data-driven method for learning
nudging terms in nonlinear state space models. We establish a theoretical
existence result based on the Kazantzis--Kravaris--Luenberger observer theory.
The proposed approach is evaluated on three benchmark problems that exhibit
chaotic behavior: the Lorenz 96 model, the Kuramoto--Sivashinsky equation, and
the Kolmogorov flow.

</details>


### [186] [From Imperfect Signals to Trustworthy Structure: Confidence-Aware Inference from Heterogeneous and Reliability-Varying Utility Data](https://arxiv.org/abs/2508.05791)
*Haoran Li,Lihao Mai,Muhao Guo,Jiaqi Wu,Yang Weng,Yannan Sun,Ce Jimmy Liu*

Main category: cs.LG

TL;DR: The paper introduces a framework to reconstruct accurate distribution grid topology using heterogeneous data, ensuring reliability under varied real-world conditions.


<details>
  <summary>Details</summary>
Motivation: Distribution grid topology is critical for reliable grid operations, but utility data varies in quality and origin, necessitating a scalable and trustworthy reconstruction method.

Method: A framework integrates spatial and signal domain data, employs confidence-aware inference, and enforces operational constraints during the learning process.

Result: Validated on data from over 8000 meters across 3 feeders, the framework achieved over 95% accuracy in topology reconstruction and improved confidence calibration and efficiency.

Conclusion: The approach systematically handles uncertainty and ensures structural validity, providing reliable topology reconstruction tailored for real-world deployment.

Abstract: Accurate distribution grid topology is essential for reliable modern grid
operations. However, real-world utility data originates from multiple sources
with varying characteristics and levels of quality. In this work, developed in
collaboration with Oncor Electric Delivery, we propose a scalable framework
that reconstructs a trustworthy grid topology by systematically integrating
heterogeneous data. We observe that distribution topology is fundamentally
governed by two complementary dimensions: the spatial layout of physical
infrastructure (e.g., GIS and asset metadata) and the dynamic behavior of the
system in the signal domain (e.g., voltage time series). When jointly
leveraged, these dimensions support a complete and physically coherent
reconstruction of network connectivity. To address the challenge of uneven data
quality without compromising observability, we introduce a confidence-aware
inference mechanism that preserves structurally informative yet imperfect
inputs, while quantifying the reliability of each inferred connection for
operator interpretation. This soft handling of uncertainty is tightly coupled
with hard enforcement of physical feasibility: we embed operational
constraints, such as transformer capacity limits and radial topology
requirements, directly into the learning process. Together, these components
ensure that inference is both uncertainty-aware and structurally valid,
enabling rapid convergence to actionable, trustworthy topologies under
real-world deployment conditions. The proposed framework is validated using
data from over 8000 meters across 3 feeders in Oncor's service territory,
demonstrating over 95% accuracy in topology reconstruction and substantial
improvements in confidence calibration and computational efficiency relative to
baseline methods.

</details>


### [187] [Optimal Linear Baseline Models for Scientific Machine Learning](https://arxiv.org/abs/2508.05831)
*Alexander DeLise,Kyle Loh,Krish Patel,Meredith Teague,Andrea Arnold,Matthias Chung*

Main category: cs.LG

TL;DR: The paper develops a robust framework for analyzing linear encoder-decoder architectures in scientific machine learning, offering theoretical insights and practical validations.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of understanding mappings from physical processes to observed signals, with an emphasis on interpretable methods.

Method: A unified theoretical framework involving Bayes risk minimization is applied to linear encoder-decoder architectures, producing rank-constrained solutions.

Result: Validated with numerical experiments across biomedical imaging, financial analysis, and fluid dynamics, the results generalize formulations to rank-deficiencies.

Conclusion: The work provides a baseline for benchmarking neural network models in scientific machine learning, ensuring interpretability and broader applicability of trained models.

Abstract: Across scientific domains, a fundamental challenge is to characterize and
compute the mappings from underlying physical processes to observed signals and
measurements. While nonlinear neural networks have achieved considerable
success, they remain theoretically opaque, which hinders adoption in contexts
where interpretability is paramount. In contrast, linear neural networks serve
as a simple yet effective foundation for gaining insight into these complex
relationships. In this work, we develop a unified theoretical framework for
analyzing linear encoder-decoder architectures through the lens of Bayes risk
minimization for solving data-driven scientific machine learning problems. We
derive closed-form, rank-constrained linear and affine linear optimal mappings
for forward modeling and inverse recovery tasks. Our results generalize
existing formulations by accommodating rank-deficiencies in data, forward
operators, and measurement processes. We validate our theoretical results by
conducting numerical experiments on datasets from simple biomedical imaging,
financial factor analysis, and simulations involving nonlinear fluid dynamics
via the shallow water equations. This work provides a robust baseline for
understanding and benchmarking learned neural network models for scientific
machine learning problems.

</details>


### [188] [SCAR: State-Space Compression for AI-Driven Resource Management in 6G-Enabled Vehicular Infotainment Systems](https://arxiv.org/abs/2508.06243)
*Ioan-Sorin Comsa,Purav Shah,Karthik Vaidhyanathan,Deepak Gangadharan,Christof Imhof,Per Bergamin,Aryan Kaushik,Gabriel-Miro Muntean,Ramona Trestian*

Main category: cs.LG

TL;DR: This paper introduces SCAR, an Edge AI-assisted framework for optimizing scheduling and fairness in 6G vehicular infotainment by compressing Channel Quality Indicators (CQI) for efficient reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: The complexities and increasing volume of CQI data in autonomous vehicular networks challenge traditional Radio Resource Management techniques, necessitating innovative solutions to optimize scheduling and ensure fairness.

Method: SCAR uses machine learning-based compression methods, such as clustering and RBF networks, to reduce CQI data size, and then applies reinforcement learning to train scheduling policies that maximize throughput and meet fairness criteria.

Result: SCAR improves feasible scheduling by 14% and reduces unfair scheduling by 15% compared to non-compressed RL baselines. Additionally, SAST-based CQI clustering decreases distortion by 10%.

Conclusion: SCAR proves to be an efficient solution for managing resources in dynamic vehicular networks, showcasing significant gains in scalability, scheduling efficiency, and fairness for future 6G systems.

Abstract: The advent of 6G networks opens new possibilities for connected infotainment
services in vehicular environments. However, traditional Radio Resource
Management (RRM) techniques struggle with the increasing volume and complexity
of data such as Channel Quality Indicators (CQI) from autonomous vehicles. To
address this, we propose SCAR (State-Space Compression for AI-Driven Resource
Management), an Edge AI-assisted framework that optimizes scheduling and
fairness in vehicular infotainment. SCAR employs ML-based compression
techniques (e.g., clustering and RBF networks) to reduce CQI data size while
preserving essential features. These compressed states are used to train
6G-enabled Reinforcement Learning policies that maximize throughput while
meeting fairness objectives defined by the NGMN. Simulations show that SCAR
increases time in feasible scheduling regions by 14\% and reduces unfair
scheduling time by 15\% compared to RL baselines without CQI compression.
Furthermore, Simulated Annealing with Stochastic Tunneling (SAST)-based
clustering reduces CQI clustering distortion by 10\%, confirming its
efficiency. These results demonstrate SCAR's scalability and fairness benefits
for dynamic vehicular networks.

</details>


### [189] [An Effective Approach for Node Classification in Textual Graphs](https://arxiv.org/abs/2508.05836)
*Rituparna Datta,Nibir Chandra Mandal*

Main category: cs.LG

TL;DR: The paper introduces an innovative framework combining TAPE and Graphormer to enhance node classification in Textual Attribute Graphs (TAGs), achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Address the challenges in integrating textual semantics with structural graph information for effective node classification in Textual Attribute Graphs.

Method: Combines TAPE with Graphormer, integrates ChatGPT for semantic enrichment, and utilizes path-aware encoding and multi-head attention for structural analysis.

Result: Achieved state-of-the-art classification accuracy of 0.772 on the ogbn-arxiv dataset, outperforming existing methods in multiple metrics like precision, recall, and F1-score.

Conclusion: The proposed framework demonstrates its robustness and scalability for node classification, offering advancements in knowledge systems and scientific discovery.

Abstract: Textual Attribute Graphs (TAGs) are critical for modeling complex networks
like citation networks, but effective node classification remains challenging
due to difficulties in integrating rich semantics from text with structural
graph information. Existing methods often struggle with capturing nuanced
domain-specific terminology, modeling long-range dependencies, adapting to
temporal evolution, and scaling to massive datasets. To address these issues,
we propose a novel framework that integrates TAPE (Text-Attributed Graph
Representation Enhancement) with Graphormer. Our approach leverages a large
language model (LLM), specifically ChatGPT, within the TAPE framework to
generate semantically rich explanations from paper content, which are then
fused into enhanced node representations. These embeddings are combined with
structural features using a novel integration layer with learned attention
weights. Graphormer's path-aware position encoding and multi-head attention
mechanisms are employed to effectively capture long-range dependencies across
the citation network. We demonstrate the efficacy of our framework on the
challenging ogbn-arxiv dataset, achieving state-of-the-art performance with a
classification accuracy of 0.772, significantly surpassing the best GCN
baseline of 0.713. Our method also yields strong results in precision (0.671),
recall (0.577), and F1-score (0.610). We validate our approach through
comprehensive ablation studies that quantify the contribution of each
component, demonstrating the synergy between semantic and structural
information. Our framework provides a scalable and robust solution for node
classification in dynamic TAGs, offering a promising direction for future
research in knowledge systems and scientific discovery.

</details>


### [190] [Structural Equation-VAE: Disentangled Latent Representations for Tabular Data](https://arxiv.org/abs/2508.06347)
*Ruiyu Zhang,Ce Zhao,Xin Zhao,Lin Nie,Wai-Fung Lam*

Main category: cs.LG

TL;DR: SE-VAE introduces architectural design elements to improve latent disentanglement and interpretable representation in tabular data by embedding structural equation modeling principles.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of learning interpretable latent representations in tabular data, a known difficulty in deep generative models.

Method: SE-VAE leverages a structural equation-inspired modular design, embedding measurement structure directly into the variational autoencoder architecture. It includes a global nuisance latent and aligns latent subspaces with known variable groupings.

Result: Performance evaluations on simulated tabular datasets show SE-VAE outperforms alternative methods in factor recovery, interpretability, and robustness to nuisance variation.

Conclusion: SE-VAE introduces a principled and modular approach to generative modeling that prioritizes theory-driven latent constructs, offering significant improvements in interpretability and applicability for scientific and social data modeling.

Abstract: Learning interpretable latent representations from tabular data remains a
challenge in deep generative modeling. We introduce SE-VAE (Structural
Equation-Variational Autoencoder), a novel architecture that embeds measurement
structure directly into the design of a variational autoencoder. Inspired by
structural equation modeling, SE-VAE aligns latent subspaces with known
indicator groupings and introduces a global nuisance latent to isolate
construct-specific confounding variation. This modular architecture enables
disentanglement through design rather than through statistical regularizers
alone. We evaluate SE-VAE on a suite of simulated tabular datasets and
benchmark its performance against a series of leading baselines using standard
disentanglement metrics. SE-VAE consistently outperforms alternatives in factor
recovery, interpretability, and robustness to nuisance variation. Ablation
results reveal that architectural structure, rather than regularization
strength, is the key driver of performance. SE-VAE offers a principled
framework for white-box generative modeling in scientific and social domains
where latent constructs are theory-driven and measurement validity is
essential.

</details>


### [191] [A Markov Decision Process Framework for Early Maneuver Decisions in Satellite Collision Avoidance](https://arxiv.org/abs/2508.05876)
*Francesca Ferrara,Lander W. Schillinger Arana,Florian Dörfler,Sarah H. Q. Li*

Main category: cs.LG

TL;DR: The paper develops a Markov decision process framework for collision avoidance maneuvers optimized for fuel economy and decision timing using reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: To improve collision avoidance strategies by minimizing propellant consumption and ensuring acceptable collision risks using decision-making models.

Method: Continuous-state, discrete-action MDP model combined with reinforcement learning policy gradient. Verified with historical data and ablation studies on hyperparameters.

Result: The trained policy outperforms conventional policies in fuel efficiency across synthetic and historical conjunction events while maintaining collision risk guarantees.

Conclusion: Reinforcement-learning-based CAM policies offer a balanced approach to minimize propellant use and address collision risks effectively.

Abstract: This work presents a Markov decision process (MDP) framework to model
decision-making for collision avoidance maneuver (CAM) and a reinforcement
learning policy gradient (RL-PG) algorithm to train an autonomous guidance
policy using historic CAM data. In addition to maintaining acceptable collision
risks, this approach seeks to minimize the average fuel consumption of CAMs by
making early maneuver decisions. We model CAM as a continuous state, discrete
action and finite horizon MDP, where the critical decision is determining when
to initiate the maneuver. The MDP model also incorporates analytical models for
conjunction risk, propellant consumption, and transit orbit geometry. The
Markov policy effectively trades-off maneuver delay-which improves the
reliability of conjunction risk indicators-with propellant consumption-which
increases with decreasing maneuver time. Using historical data of tracked
conjunction events, we verify this framework and conduct an extensive ablation
study on the hyper-parameters used within the MDP. On synthetic conjunction
events, the trained policy significantly minimizes both the overall and average
propellant consumption per CAM when compared to a conventional cut-off policy
that initiates maneuvers 24 hours before the time of closest approach (TCA). On
historical conjunction events, the trained policy consumes more propellant
overall but reduces the average propellant consumption per CAM. For both
historical and synthetic conjunction events, the trained policy achieves equal
if not higher overall collision risk guarantees.

</details>


### [192] [The Fourth State: Signed-Zero Ternary for Stable LLM Quantization (and More)](https://arxiv.org/abs/2508.05905)
*Jeffrey Uhlmann*

Main category: cs.LG

TL;DR: The paper introduces a 2-bit quantization method called Signed-Zero Ternary (SZT) that claims to improve gradient representation without computational drawbacks.


<details>
  <summary>Details</summary>
Motivation: To challenge the traditional view that quantization sacrifices quality for reduced compute requirements and explore its potential to enhance information density within fixed resource budgets.

Method: The authors propose a deterministic 2-bit quantization method, Signed-Zero Ternary (SZT), that encodes gradient information effectively without incurring any forward-path computational penalties.

Result: The analysis suggests that SZT improves information density when compared to non-quantized options.

Conclusion: SZT quantization may serve as a resource-efficient technique that enhances gradient representation without sacrificing performance in systems constrained by fixed budgets.

Abstract: Quantization is usually regarded as a means to trade quality of performance
for reduced compute requirements, i.e., as a suboptimal approximation. However,
if examined in terms of a fixed overall resource budget, a very different
perspective arises. We introduce Signed-Zero Ternary (SZT), a 2-bit
quantization that deterministically provides gradient information with no
forward-path penalty. Our analysis provides evidence that it may improve
information density compared to non-quantized alternatives.

</details>


### [193] [FedMeNF: Privacy-Preserving Federated Meta-Learning for Neural Fields](https://arxiv.org/abs/2508.06301)
*Junhyeog Yun,Minui Hong,Gunhee Kim*

Main category: cs.LG

TL;DR: FedMeNF introduces a privacy-preserving Federated Meta-Learning approach to efficiently optimize neural fields while safeguarding client data.


<details>
  <summary>Details</summary>
Motivation: To address computational and data constraints on edge devices while ensuring privacy in Federated Meta-Learning approaches.

Method: A novel Federated Meta-Learning technique incorporating a privacy-preserving loss function for local optimizations without accessing client data.

Result: FedMeNF achieves robust reconstruction and fast optimization across diverse modalities under non-IID and few-shot scenarios.

Conclusion: FedMeNF effectively balances privacy preservation, optimization efficiency, and performance in neural field learning across diverse conditions.

Abstract: Neural fields provide a memory-efficient representation of data, which can
effectively handle diverse modalities and large-scale data. However, learning
to map neural fields often requires large amounts of training data and
computations, which can be limited to resource-constrained edge devices. One
approach to tackle this limitation is to leverage Federated Meta-Learning
(FML), but traditional FML approaches suffer from privacy leakage. To address
these issues, we introduce a novel FML approach called FedMeNF. FedMeNF
utilizes a new privacy-preserving loss function that regulates privacy leakage
in the local meta-optimization. This enables the local meta-learner to optimize
quickly and efficiently without retaining the client's private data. Our
experiments demonstrate that FedMeNF achieves fast optimization speed and
robust reconstruction performance, even with few-shot or non-IID data across
diverse data modalities, while preserving client data privacy.

</details>


### [194] [Dual Signal Decomposition of Stochastic Time Series](https://arxiv.org/abs/2508.05915)
*Alex Glushkovsky*

Main category: cs.LG

TL;DR: This paper discusses a method to decompose stochastic time series into mean, dispersion, and noise using machine learning. The approach minimizes a custom loss function, incorporates statistical regularization, and supports sequential or joint learning processes.


<details>
  <summary>Details</summary>
Motivation: The paper aims to effectively decompose time series data into meaningful components (mean, dispersion, and noise), to improve analysis, denoising, and forecasting capabilities while handling complex structures like heteroskedasticity.

Method: The decomposition is achieved using machine learning methods that minimize a loss function balancing data fitting and regularization. Weighting based on Statistical Process Control is introduced, and models optimize via direct nonlinear techniques or neural networks.

Result: The approach effectively isolates noise and provides decompositions for analysis. It supports applications in denoising, structure learning, and forecasting. Sequential learning processes mean and dispersion separately, while joint learning uncovers complex relationships.

Conclusion: The proposed decomposition is versatile and enables a variety of applications such as noise isolation, pattern recognition, and forecasting. It flexibly adapts to different scenarios with customizable hyperparameters and learning architectures.

Abstract: The research paper addresses decomposition of a stochastic time series into
three time series representing a dual signal i.e., the mean and the dispersion,
with noise isolated. Decomposition is done by applying machine learning to fit
a dual signal. Machine learning minimizes the loss function which compromises
between fitting the original time series and penalizing irregularities of the
dual signal. The latter includes terms based on the first and second order
derivatives along time. To preserve special patterns, weighting of the
regularization components of the loss function has been introduced based on
Statistical Process Control methodology. The proposed decomposition can be
applied as a smoothing algorithm against the mean and dispersion of the time
series. By isolating noise, the proposed decomposition can be seen as a
denoising algorithm. Two approaches of the learning process have been
considered: sequential and jointly. The former approach learns the mean signal
first and then dispersion. The latter approach fits the dual signal jointly.
Jointly learning can uncover complex relationships for the time series with
heteroskedasticity. Learning has been set by solving the direct non-linear
unconstrained optimization problem or by applying neural networks that have
sequential or twin output architectures. Tuning of the loss function
hyperparameters focuses on the isolated noise to be a stationary stochastic
process without autocorrelation properties. Depending on the applications, the
hyperparameters of the learning can be tuned towards either the discrete states
by stepped signal or smoothed series. The decomposed dual signal can be
represented on the 2D space and used to learn inherent structures, to forecast
both mean and dispersion, or to analyze cross effects in case of multiple time
series.

</details>


### [195] [Fast, Convex and Conditioned Network for Multi-Fidelity Vectors and Stiff Univariate Differential Equations](https://arxiv.org/abs/2508.05921)
*Siddharth Rout*

Main category: cs.LG

TL;DR: The paper tackles the optimization and ill-conditioning issues in neural PDE solvers by introducing Shifted Gaussian Encoding, which improves accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the optimization challenges in Physics-Informed Extreme Learning Machines (PIELMs), caused by ill-conditioned activation matrices and asymptotic behavior in governing equations, which impair convergence.

Method: The authors propose Shifted Gaussian Encoding, a filtering step that improves the matrix rank and avoids ill-conditioning while maintaining the solver's convex properties.

Result: The approach extended the solvable range of Peclet numbers by over two orders of magnitude, reduced errors in multi-frequency learning by up to six orders, and delivered better accuracy and speed compared to deep networks with over one million parameters.

Conclusion: The findings emphasize that ill-conditioning, rather than network depth, is a limiting factor in scientific neural PDE solvers and that simple architectural adjustments can yield significant improvements.

Abstract: Accuracy in neural PDE solvers often breaks down not because of limited
expressivity, but due to poor optimisation caused by ill-conditioning,
especially in multi-fidelity and stiff problems. We study this issue in
Physics-Informed Extreme Learning Machines (PIELMs), a convex variant of neural
PDE solvers, and show that asymptotic components in governing equations can
produce highly ill-conditioned activation matrices, severely limiting
convergence. We introduce Shifted Gaussian Encoding, a simple yet effective
activation filtering step that increases matrix rank and expressivity while
preserving convexity. Our method extends the solvable range of Peclet numbers
in steady advection-diffusion equations by over two orders of magnitude,
achieves up to six orders lower error on multi-frequency function learning, and
fits high-fidelity image vectors more accurately and faster than deep networks
with over a million parameters. This work highlights that conditioning, not
depth, is often the bottleneck in scientific neural solvers and that simple
architectural changes can unlock substantial gains.

</details>


### [196] [Mitigating Think-Answer Mismatch in LLM Reasoning Through Noise-Aware Advantage Reweighting](https://arxiv.org/abs/2508.05928)
*Si Shen,Peijun Shen,Wenhua Zhao,Danhao Zhu*

Main category: cs.LG

TL;DR: The paper introduces Stable Group-Relative Policy Optimization (S-GRPO), a method designed to address stability challenges in training reasoning models under noisy reward signals.


<details>
  <summary>Details</summary>
Motivation: The issue arises from the 'Think-Answer Mismatch', where unbalanced response groups lead to corrupted learning signals, hindering model performance.

Method: S-GRPO enhances standard GRPO by introducing noise-aware advantage weights that stabilize the training process.

Result: S-GRPO significantly outperforms DR. GRPO across various reasoning models, such as a performance gain of +2.5% on Qwen-Math-7B-Base, and presents stable learning under synthetic reward noise conditions.

Conclusion: S-GRPO provides a robust solution for training large-scale reasoning models, especially under scenarios with high noise, making it superior to standard GRPO methods.

Abstract: Group-Relative Policy Optimization (GRPO) is a key technique for training
large reasoning models, yet it suffers from a critical vulnerability: the
\emph{Think-Answer Mismatch}, where noisy reward signals corrupt the learning
process. This problem is most severe in unbalanced response groups,
paradoxically degrading the signal precisely when it should be most
informative. To address this challenge, we propose Stable Group-Relative Policy
Optimization (S-GRPO), a principled enhancement that derives optimal,
noise-aware advantage weights to stabilize training. Our comprehensive
experiments on mathematical reasoning benchmarks demonstrate S-GRPO's
effectiveness and robustness. On various models, S-GRPO significantly
outperforms DR. GRPO, achieving performance gains of +2.5% on
Qwen-Math-7B-Base, +2.2% on Llama-3.2-3B-Base, and +2.4% on
Qwen-Math-1.5B-Instruct. Most critically, while standard GRPO fails to learn
under 20% synthetic reward noise, S-GRPO maintains stable learning progress.
These results highlight S-GRPO's potential for more robust and effective
training of large-scale reasoning models. \footnote{Code and data are available
at: https://github.com/shenpeijun0212/S-GRPO

</details>


### [197] [Multi-Armed Bandits-Based Optimization of Decision Trees](https://arxiv.org/abs/2508.05957)
*Hasibul Karim Shanto,Umme Ayman Koana,Shadikur Rahman*

Main category: cs.LG

TL;DR: The paper proposes a novel decision tree pruning approach using Multi-Armed Bandits (MAB) to improve generalization and performance, surpassing traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional decision tree pruning methods like Cost-Complexity Pruning (CCP) and Reduced Error Pruning (REP) often overfit or underperform on unseen data, especially with small, complex datasets.

Method: The proposed approach models the pruning process as an exploration-exploitation problem, employing MAB algorithms to dynamically optimize pruning decisions based on feedback from previous pruning actions.

Result: Experimental results on several benchmark datasets reveal that the MAB-based pruning method outperforms conventional pruning techniques in predictive performance.

Conclusion: The study shows that a reinforcement learning-based MAB pruning approach is a promising alternative for optimizing decision tree models by enhancing generalization and reducing overfitting.

Abstract: Decision trees, without appropriate constraints, can easily become overly
complex and prone to overfit, capturing noise rather than generalizable
patterns. To resolve this problem,pruning operation is a crucial part in
optimizing decision trees, as it not only reduces the complexity of trees but
also decreases the probability of generating overfit models. The conventional
pruning techniques like Cost-Complexity Pruning (CCP) and Reduced Error Pruning
(REP) are mostly based on greedy approaches that focus on immediate gains in
performance while pruning nodes of the decision tree. However, this might
result in a lower generalization in the long run, compromising the robust
ability of the tree model when introduced to unseen data samples, particularly
when trained with small and complex datasets. To address this challenge, we are
proposing a Multi-Armed Bandits (MAB)-based pruning approach, a reinforcement
learning (RL)-based technique, that will dynamically prune the tree to generate
an optimal decision tree with better generalization. Our proposed approach
assumes the pruning process as an exploration-exploitation problem, where we
are utilizing the MAB algorithms to find optimal branch nodes to prune based on
feedback from each pruning actions. Experimental evaluation on several
benchmark datasets, demonstrated that our proposed approach results in better
predictive performance compared to the traditional ones. This suggests the
potential of utilizing MAB for a dynamic and probabilistic way of decision tree
pruning, in turn optimizing the decision tree-based model.

</details>


### [198] [Mildly Conservative Regularized Evaluation for Offline Reinforcement Learning](https://arxiv.org/abs/2508.05960)
*Haohui Chen,Zhiyong Chen*

Main category: cs.LG

TL;DR: This paper introduces MCRE framework and MCRQ algorithm to improve performance in offline RL by balancing conservatism and performance.


<details>
  <summary>Details</summary>
Motivation: The challenge of distribution shift in offline RL causes out-of-distribution actions and overestimation, requiring a balance between conservatism and performance.

Method: Introduces the MCRE framework, which integrates a behavior cloning term into the Bellman backup, and develops the MCRQ algorithm within an off-policy actor-critic framework.

Result: MCRQ surpasses strong baselines and state-of-the-art offline RL approaches in benchmark experiments.

Conclusion: The proposed MCRQ algorithm demonstrates effective performance in offline RL by achieving a balance between conservative value estimation and policy improvement.

Abstract: Offline reinforcement learning (RL) seeks to learn optimal policies from
static datasets without further environment interaction. A key challenge is the
distribution shift between the learned and behavior policies, leading to
out-of-distribution (OOD) actions and overestimation. To prevent gross
overestimation, the value function must remain conservative; however, excessive
conservatism may hinder performance improvement. To address this, we propose
the mildly conservative regularized evaluation (MCRE) framework, which balances
conservatism and performance by combining temporal difference (TD) error with a
behavior cloning term in the Bellman backup. Building on this, we develop the
mildly conservative regularized Q-learning (MCRQ) algorithm, which integrates
MCRE into an off-policy actor-critic framework. Experiments show that MCRQ
outperforms strong baselines and state-of-the-art offline RL algorithms on
benchmark datasets.

</details>


### [199] [Near-Optimal Regret for Efficient Stochastic Combinatorial Semi-Bandits](https://arxiv.org/abs/2508.06247)
*Zichun Ye,Runqi Wang,Xutong Liu,Shuai Li*

Main category: cs.LG

TL;DR: The paper introduces CMOSS, a novel algorithm for combinatorial multi-armed bandit problems, achieving low regret and high computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Current CMAB algorithms either have high computational cost or incur additional regret over long horizons, creating a need for a method that balances computational efficiency and regret minimization.

Method: CMOSS, a computationally efficient algorithm operating under a stochastic framework with semi-bandit and cascading feedback, achieves optimal regret bounds by eliminating dependency on log T.

Result: CMOSS achieves near-optimal regret of $O((\log k)^2\sqrt{kmT})$ and performs better than existing benchmarks in both regret and runtime efficiency in synthetic and real-world settings.

Conclusion: CMOSS is an effective and efficient solution that strikes a balance between regret minimization and computational overhead, outperforming existing methods for combinatorial multi-armed bandit problems.

Abstract: The combinatorial multi-armed bandit (CMAB) is a cornerstone of sequential
decision-making framework, dominated by two algorithmic families: UCB-based and
adversarial methods such as follow the regularized leader (FTRL) and online
mirror descent (OMD). However, prominent UCB-based approaches like CUCB suffer
from additional regret factor $\log T$ that is detrimental over long horizons,
while adversarial methods such as EXP3.M and HYBRID impose significant
computational overhead. To resolve this trade-off, we introduce the
Combinatorial Minimax Optimal Strategy in the Stochastic setting (CMOSS). CMOSS
is a computationally efficient algorithm that achieves an instance-independent
regret of $O\big( (\log k)^2\sqrt{kmT}\big )$ under semi-bandit feedback, where
$m$ is the number of arms and $k$ is the maximum cardinality of a feasible
action. Crucially, this result eliminates the dependency on $\log T$ and
matches the established $\Omega\big( \sqrt{kmT}\big)$ lower bound up to
$O\big((\log k)^2\big)$. We then extend our analysis to show that CMOSS is also
applicable to cascading feedback. Experiments on synthetic and real-world
datasets validate that CMOSS consistently outperforms benchmark algorithms in
both regret and runtime efficiency.

</details>


### [200] [LinguaFluid: Language Guided Fluid Control via Semantic Rewards in Reinforcement Learning](https://arxiv.org/abs/2508.05977)
*Aoming Liang,Chi Cheng,Dashuai Chen,Boai Sun,Dixia Fan*

Main category: cs.LG

TL;DR: This paper introduces a semantic-based reward computation method for reinforcement learning (RL) using SBERT to align state and goal descriptions, eliminating the need for handcrafted rewards.


<details>
  <summary>Details</summary>
Motivation: Current reinforcement learning struggles with designing effective reward functions in tasks with difficult-to-specify goals, often relying on heuristic or manually engineered solutions.

Method: The authors propose a method that computes rewards based on the cosine similarity between natural language descriptions of task goals and states using Sentence-BERT (SBERT), replacing manually defined rewards.

Result: Experiments across varied environments demonstrated that this semantic reward approach achieves competitive control behaviors without relying on handcrafted rewards.

Conclusion: The study highlights the relationship between language embedding and Euclidean spaces, paving the way for integrating larger language models with RL for natural language-guided agent behaviors.

Abstract: In the domain of scientific machine learning, designing effective reward
functions remains a challenge in reinforcement learning (RL), particularly in
environments where task goals are difficult to specify numerically. Reward
functions in existing work are predominantly based on heuristics, manual
engineering, or task-specific tuning. In this work, we introduce a semantically
aligned reinforcement learning method where rewards are computed by aligning
the current state with a target semantic instruction using a
Sentence-Bidirectional Encoder Representations from Transformers (SBERT).
Instead of relying on manually defined reward functions, the policy receives
feedback based on the reward, which is a cosine similarity between the goal
textual description and the statement description in the episode. We evaluated
our approach in several environments and showed that semantic reward can guide
learning to achieve competitive control behavior, even in the absence of
hand-crafted reward functions. Our study demonstrates a correlation between the
language embedding space and the conventional Euclidean space. This framework
opens new horizons for aligning agent behavior with natural language goals and
lays the groundwork for a more seamless integration of larger language models
(LLMs) and fluid control applications.

</details>


### [201] [Parameter-free Optimal Rates for Nonlinear Semi-Norm Contractions with Applications to $Q$-Learning](https://arxiv.org/abs/2508.05984)
*Ankur Naskar,Gugan Thoppe,Vijay Gupta*

Main category: cs.LG

TL;DR: The paper introduces methods to achieve optimal convergence rates for nonlinear fixed-point algorithms, such as $Q$-learning and TD-learning, by overcoming the challenge of semi-norm non-monotonicity.


<details>
  <summary>Details</summary>
Motivation: Nonlinear fixed-point algorithms like $Q$-learning and TD-learning often involve semi-norm contractions, but achieving optimal convergence rates has been challenging due to non-monotonicity.

Method: The authors recast averaged errors as linear recursions with nonlinear perturbations and couple semi-norm contractions with the monotonicity of induced norms to tame nonlinearity.

Result: The paper achieves parameter-free optimal convergence rates of $	ilde{O}(1/\sqrt{t})$ for $Q$-learning methods in various settings, including synchronous, asynchronous, single-agent, distributed, and simulator-based deployments.

Conclusion: These results contribute a unified framework for nonlinear fixed-point algorithms, addressing critical gaps in achieving optimal performance for both average-reward and discounted $Q$-learning.

Abstract: Algorithms for solving \textit{nonlinear} fixed-point equations -- such as
average-reward \textit{$Q$-learning} and \textit{TD-learning} -- often involve
semi-norm contractions. Achieving parameter-free optimal convergence rates for
these methods via Polyak--Ruppert averaging has remained elusive, largely due
to the non-monotonicity of such semi-norms. We close this gap by (i.) recasting
the averaged error as a linear recursion involving a nonlinear perturbation,
and (ii.) taming the nonlinearity by coupling the semi-norm's contraction with
the monotonicity of a suitably induced norm. Our main result yields the first
parameter-free $\tilde{O}(1/\sqrt{t})$ optimal rates for $Q$-learning in both
average-reward and exponentially discounted settings, where $t$ denotes the
iteration index. The result applies within a broad framework that accommodates
synchronous and asynchronous updates, single-agent and distributed deployments,
and data streams obtained either from simulators or along Markovian
trajectories.

</details>


### [202] [Pruning the Unsurprising: Efficient Code Reasoning via First-Token Surprisal](https://arxiv.org/abs/2508.05988)
*Wenhao Zeng,Yaoning Wang,Chao Hu,Yuling Shi,Chengcheng Wan,Hongyu Zhang,Xiaodong Gu*

Main category: cs.LG

TL;DR: ASAP introduces a novel method to compress reasoning traces in large reasoning models for efficient coding tasks, achieving state-of-the-art metrics while reducing cost and latency.


<details>
  <summary>Details</summary>
Motivation: Long reasoning chains in code reasoning tasks create challenges due to higher training costs, inference latency, and deployment feasibility.

Method: ASAP performs anchor-guided pruning to preserve the core reasoning structure and uses a logic-aware pruning based on a first-token surprisal metric.

Result: ASAP reduces token generation by 23.5% and inference latency by 43.5% while maintaining competitive accuracy of 36.19% on challenging code benchmarks.

Conclusion: ASAP provides a promising approach toward effective and efficient large reasoning models by improving reasoning trace compression without compromising accuracy.

Abstract: Recently, Large Reasoning Models (LRMs) have demonstrated remarkable
capabilities in code reasoning by scaling up the length of Chain-of-Thought
(CoT). However, excessively long reasoning traces introduce substantial
challenges in terms of training cost, inference latency, and deployment
feasibility. While various CoT compression approaches have emerged to address
this challenge, they face inherent trade-offs: token-level methods often
disrupt syntactic and logical coherence, while step-level methods based on
perplexity fail to reliably capture the logically critical reasoning steps. In
this paper, we propose ASAP (Anchor-guided, Surprisal-based Pruning), a novel
coarse-to-fine framework for CoT compression. ASAP first performs anchor-guided
pruning to preserve the core reasoning structure, which efficiently reduces the
search space for subsequent processing. It then enables a logic-aware pruning
by selecting logically essential reasoning steps based on a novel first-token
surprisal metric. Finally, ASAP teaches models to autonomously generate and
leverage these concise CoTs at inference time, enabling efficient reasoning in
coding tasks. Experiments show that ASAP achieves state-of-the-art accuracy
across multiple code generation benchmarks while substantially reducing
training and inference costs. On the challenging LiveCodeBench v4_v5 benchmark,
our approach reduces token generation by 23.5% and inference latency by 43.5%
compared to the strongest baseline, while achieving a competitive accuracy of
36.19% in Pass@1. Our results highlight a promising direction for building
powerful and efficient LRMs.

</details>


### [203] [Optimizing Prompt Sequences using Monte Carlo Tree Search for LLM-Based Optimization](https://arxiv.org/abs/2508.05995)
*Fei Xu Yu,Gina Adam,Nathaniel D. Bastian,Tian Lan*

Main category: cs.LG

TL;DR: The paper introduces MCTS-OPS, a framework merging Monte Carlo Tree Search (MCTS) with prompt selection in large language models (LLMs) to improve multi-step tasks, showing significant improvements in code execution success and optimization results.


<details>
  <summary>Details</summary>
Motivation: To address the performance drop of LLMs on complex tasks requiring consistent multi-step planning and explore neural-symbolic frameworks for improving code generation.

Method: The method integrates MCTS to guide sequential decision-making in prompt selection, refining prompt sequences for LLM-based code generation and optimization in complex tasks.

Result: Experiments on network optimization demonstrate improved code execution success, optimization quality (2-4x higher reward, 3x lower standard deviation), and enhanced probability of attaining optimal solutions (+10% in hard problems).

Conclusion: MCTS-OPS effectively combines symbolic planning with LLMs to enhance problem-solving capabilities and code generation quality in complex domains.

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
code generation and structured reasoning; however, their performance often
degrades on complex tasks that require consistent multi-step planning. Recent
work has explored combining LLMs with Monte Carlo Tree Search (MCTS), yet
existing approaches primarily focus on generating heuristic-based code for
optimization or target simpler tasks where correctness alone is sufficient. In
this work, we propose MCTS-OPS, a novel neural-symbolic framework that
formulates prompt selection as a sequential decision process guided by MCTS.
Our method explores and refines multi-step prompt sequences for the goal of
improving code generation quality and enhancing the problem-solving
capabilities of LLMs in general optimization. Experiments on network
optimization show significant improvement over the baselines, both in the
success rate of executing the generated code and in the optimization results
with the specified objective and constraints (2$\sim$4$\times$ higher reward
and 3$\times$ lower standard deviation). Moreover, it improves the chance of
attaining the optimal solution by about 10\% of cases, compared to baseline
methods in hard problems. These results highlight the promise of combining
symbolic planning with LLMs for robust, high-quality code generation in complex
domains.

</details>


### [204] [Stepwise Fine and Gray: Subject-Specific Variable Selection Shows When Hemodynamic Data Improves Prognostication of Comatose Post-Cardiac Arrest Patients](https://arxiv.org/abs/2508.06023)
*Xiaobin Shen,Jonathan Elmer,George H. Chen*

Main category: cs.LG

TL;DR: This paper introduces a novel dynamic prediction model for neurologic outcomes in post-cardiac arrest patients, leveraging baseline and time-varying features to improve clinical prognostication.


<details>
  <summary>Details</summary>
Motivation: Prognostication for post-cardiac arrest patients is challenging due to the evolving nature of clinical data collected in ICU settings. Existing methods inadequately address when and how to use these time-invariant and time-varying features.

Method: The researchers propose a stepwise dynamic competing risks model, extending the Fine and Gray model and incorporating neural networks to flexibly capture nonlinear relationships. The model unfolds in two phases, utilizing both baseline and hemodynamic features over time.

Result: The proposed model, evaluated on 2,278 retrospective cases, shows strong predictive performance for three competing outcomes: awakening, withdrawal of life-sustaining therapy, and death despite support.

Conclusion: This dynamic model enhances prognostication, particularly identifying when and for whom additional ICU data is meaningful. The approach is generalizable to other domains where features are sequentially collected.

Abstract: Prognostication for comatose post-cardiac arrest patients is a critical
challenge that directly impacts clinical decision-making in the ICU. Clinical
information that informs prognostication is collected serially over time.
Shortly after cardiac arrest, various time-invariant baseline features are
collected (e.g., demographics, cardiac arrest characteristics). After ICU
admission, additional features are gathered, including time-varying hemodynamic
data (e.g., blood pressure, doses of vasopressor medications). We view these as
two phases in which we collect new features. In this study, we propose a novel
stepwise dynamic competing risks model that improves the prediction of
neurological outcomes by automatically determining when to take advantage of
time-invariant features (first phase) and time-varying features (second phase).
Notably, our model finds patients for whom this second phase (time-varying
hemodynamic) information is beneficial for prognostication and also when this
information is beneficial (as we collect more hemodynamic data for a patient
over time, how important these data are for prognostication varies). Our
approach extends the standard Fine and Gray model to explicitly model the two
phases and to incorporate neural networks to flexibly capture complex nonlinear
feature relationships. Evaluated on a retrospective cohort of 2,278 comatose
post-arrest patients, our model demonstrates robust discriminative performance
for the competing outcomes of awakening, withdrawal of life-sustaining therapy,
and death despite maximal support. Our approach generalizes to more than two
phases in which new features are collected and could be used in other dynamic
prediction tasks, where it may be helpful to know when and for whom newly
collected features significantly improve prediction.

</details>


### [205] [Adaptive Heterogeneous Graph Neural Networks: Bridging Heterophily and Heterogeneity](https://arxiv.org/abs/2508.06034)
*Qin Chen,Guojie Song*

Main category: cs.LG

TL;DR: This paper introduces an Adaptive Heterogeneous Graph Neural Network (AHGNN) to better handle heterophilic heterogeneous graphs and outperforms 20 baselines in experiments.


<details>
  <summary>Details</summary>
Motivation: Heterogeneous graphs often present heterophily, but existing studies fail to address heterophilic HGs effectively, leading to performance degradation.

Method: The proposed AHGNN incorporates heterophily-aware convolutions and a coarse-to-fine attention mechanism for better modeling of semantic diversity and heterophily.

Result: AHGNN exhibited superior performance in experiments on seven real-world graphs compared to 20 baseline models, excelling in high-heterophily cases.

Conclusion: The study emphasizes addressing the nuances of heterophily and heterogeneity in graphs, presenting AHGNN as an effective solution.

Abstract: Heterogeneous graphs (HGs) are common in real-world scenarios and often
exhibit heterophily. However, most existing studies focus on either
heterogeneity or heterophily in isolation, overlooking the prevalence of
heterophilic HGs in practical applications. Such ignorance leads to their
performance degradation. In this work, we first identify two main challenges in
modeling heterophily HGs: (1) varying heterophily distributions across hops and
meta-paths; (2) the intricate and often heterophily-driven diversity of
semantic information across different meta-paths. Then, we propose the Adaptive
Heterogeneous Graph Neural Network (AHGNN) to tackle these challenges. AHGNN
employs a heterophily-aware convolution that accounts for heterophily
distributions specific to both hops and meta-paths. It then integrates messages
from diverse semantic spaces using a coarse-to-fine attention mechanism, which
filters out noise and emphasizes informative signals. Experiments on seven
real-world graphs and twenty baselines demonstrate the superior performance of
AHGNN, particularly in high-heterophily situations.

</details>


### [206] [DP-LLM: Runtime Model Adaptation with Dynamic Layer-wise Precision Assignment](https://arxiv.org/abs/2508.06041)
*Sangwoo Kwon,Seong Hoon Seo,Jae W. Lee,Yeonhong Park*

Main category: cs.LG

TL;DR: DP-LLM introduces dynamic precision assignment for on-device large language models to optimize performance and latency based on runtime needs.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of efficiently handling runtime constraints (such as latency and accuracy) in on-device large language models.

Method: The proposed method involves dynamically allocating precision to each LLM layer via a lightweight error estimator and learned threshold values, leveraging mixed-precision quantization.

Result: DP-LLM demonstrates a superior trade-off between performance and latency compared to prior approaches, as shown through experiments on multiple models and benchmarks.

Conclusion: Dynamic precision assignment using DP-LLM is effective in optimizing LLM runtime constraints, offering a scalable solution for on-device model adaptation.

Abstract: How can we effectively handle queries for on-device large language models
(LLMs) with varying runtime constraints, such as latency and accuracy?
Multi-scale quantization addresses this challenge by enabling memory-efficient
runtime model adaptation of LLMs through the overlaying of multiple model
variants quantized to different bitwidths. Meanwhile, an important question
still remains open-ended: how can models be properly configured to match a
target precision or latency? While mixed-precision offers a promising solution,
we take this further by leveraging the key observation that the sensitivity of
each layer dynamically changes across decoding iterations. Building on this
insight, we introduce DP-LLM, a novel mechanism that dynamically assigns
precision to each layer based on input values. DP-LLM augments each linear
layer in an LLM with a precision selector that determines the bitwidth at
runtime using a lightweight error estimator and threshold values learned
through fine-tuning. Experimental results across multiple models and benchmarks
demonstrate that DP-LLM achieves a superior performance-latency trade-off,
outperforming prior approaches.

</details>


### [207] [Architecture-Aware Generalization Bounds for Temporal Networks: Theory and Fair Comparison Methodology](https://arxiv.org/abs/2508.06066)
*Barak Gahtan,Alex M. Bronstein*

Main category: cs.LG

TL;DR: This paper provides first non-vacuous, architecture-aware generalization bounds for Temporal Convolutional Networks (TCNs) addressing their theoretical limitations and evaluates their predictive performance under various sequential data dependencies.


<details>
  <summary>Details</summary>
Motivation: The aim is to bridge the gap in theoretical understanding and generalization bounds of Temporal Convolutional Networks (TCNs) for sequential data.

Method: The authors derive generalization bounds for TCNs by using a delayed-feedback blocking mechanism to minimize dependencies in exponentially β-mixing sequences. They also develop a methodology to fairly compare models by fixing the effective sample size to examine impacts of temporal dependencies.

Result: The derived bounds reveal $
abla\sqrt{D}$ scalability (quadratic data growth per depth increase). Experiments show that strong temporal dependencies can reduce generalization gaps by 76%, but that empirical convergence rates deviate from theoretical expectations.

Conclusion: The study finds that temporal dependence may enhance TCN generalization performance under fixed information budgets, but divergence between theoretical predictions and empirical findings highlights the need for further investigation.

Abstract: Deep temporal architectures such as Temporal Convolutional Networks (TCNs)
achieve strong predictive performance on sequential data, yet theoretical
understanding of their generalization remains limited. We address this gap by
providing both the first non-vacuous, architecture-aware generalization bounds
for deep temporal models and a principled evaluation methodology.
  For exponentially $\beta$-mixing sequences, we derive bounds scaling as $
O\!\Bigl(R\,\sqrt{\tfrac{D\,p\,n\,\log N}{N}}\Bigr), $ where $D$ is network
depth, $p$ kernel size, $n$ input dimension, and $R$ weight norm. Our
delayed-feedback blocking mechanism transforms dependent samples into
effectively independent ones while discarding only $O(1/\log N)$ of the data,
yielding $\sqrt{D}$ scaling instead of exponential, implying that doubling
depth requires approximately quadrupling the training data.
  We also introduce a fair-comparison methodology that fixes the effective
sample size to isolate the effect of temporal structure from information
content. Under $N_{\text{eff}}=2{,}000$, strongly dependent sequences
($\rho=0.8$) exhibit $\approx76\%$ smaller generalization gaps than weakly
dependent ones ($\rho=0.2$), challenging the intuition that dependence is
purely detrimental. Yet convergence rates diverge from theory: weak
dependencies follow $N_{\text{eff}}^{-1.21}$ scaling and strong dependencies
follow $N_{\text{eff}}^{-0.89}$, both steeper than the predicted $N^{-0.5}$.
These findings reveal that temporal dependence can enhance learning under fixed
information budgets, while highlighting gaps between theory and practice that
motivate future research.

</details>


### [208] [Recurrent Deep Differentiable Logic Gate Networks](https://arxiv.org/abs/2508.06097)
*Simon Bührer,Andreas Plesner,Till Aczel,Roger Wattenhofer*

Main category: cs.LG

TL;DR: The paper introduces Recurrent Deep Differentiable Logic Gate Networks (RDDLGN) for sequence-to-sequence learning, combining logic gates with recurrent architectures.


<details>
  <summary>Details</summary>
Motivation: Explores the largely uncharted application of differentiable logic gates in sequential models.

Method: Developed RDDLGN architecture, applying Boolean operations with recurrent models for tasks like translation.

Result: RDDLGN shows comparable performance to GRU in translation tasks, with BLEU score of 5.00 and 30.9% training accuracy.

Conclusion: RDDLGNs demonstrate the viability of logic-based recurrent computation, suggesting potential for FPGA acceleration and advancing recursive networks.

Abstract: While differentiable logic gates have shown promise in feedforward networks,
their application to sequential modeling remains unexplored. This paper
presents the first implementation of Recurrent Deep Differentiable Logic Gate
Networks (RDDLGN), combining Boolean operations with recurrent architectures
for sequence-to-sequence learning.
  Evaluated on WMT'14 English-German translation, RDDLGN achieves 5.00 BLEU and
30.9\% accuracy during training, approaching GRU performance (5.41 BLEU) and
graceful degradation (4.39 BLEU) during inference. This work establishes
recurrent logic-based neural computation as viable, opening research directions
for FPGA acceleration in sequential modeling and other recursive network
architectures.

</details>


### [209] [GCHR : Goal-Conditioned Hindsight Regularization for Sample-Efficient Reinforcement Learning](https://arxiv.org/abs/2508.06108)
*Xing Lei,Wenyan Yang,Kaiqiang Ke,Shentao Yang,Xuetao Zhang,Joni Pajarinen,Donglin Wang*

Main category: cs.LG

TL;DR: The paper introduces Hindsight Goal-conditioned Regularization (HGR) to enhance sample efficiency in goal-conditioned reinforcement learning (GCRL).


<details>
  <summary>Details</summary>
Motivation: Sparse rewards in GCRL impede learning efficiency, and existing trajectory relabeling approaches like HER cannot fully exploit general experience.

Method: The authors present HGR, which uses hindsight goals to regularize actions, combined with hindsight self-imitation regularization (HSR) for better experience utilization.

Result: The approach achieves superior sample efficiency and outperforms existing techniques in navigation and manipulation tasks.

Conclusion: HGR and HSR together allow off-policy RL methods to better utilize data, leading to enhanced performance in sparse reward environments.

Abstract: Goal-conditioned reinforcement learning (GCRL) with sparse rewards remains a
fundamental challenge in reinforcement learning. While hindsight experience
replay (HER) has shown promise by relabeling collected trajectories with
achieved goals, we argue that trajectory relabeling alone does not fully
exploit the available experiences in off-policy GCRL methods, resulting in
limited sample efficiency. In this paper, we propose Hindsight Goal-conditioned
Regularization (HGR), a technique that generates action regularization priors
based on hindsight goals. When combined with hindsight self-imitation
regularization (HSR), our approach enables off-policy RL algorithms to maximize
experience utilization. Compared to existing GCRL methods that employ HER and
self-imitation techniques, our hindsight regularizations achieve substantially
more efficient sample reuse and the best performances, which we empirically
demonstrate on a suite of navigation and manipulation tasks.

</details>


### [210] [Improving Diagnostic Accuracy for Oral Cancer with inpainting Synthesis Lesions Generated Using Diffusion Models](https://arxiv.org/abs/2508.06151)
*Yong Oh Lee,JeeEun Kim,Jung Woo Lee*

Main category: cs.LG

TL;DR: This paper introduces a method using synthetic image generation with inpainting and diffusion models to improve diagnostic accuracy in oral cancer by overcoming limitations of annotated datasets.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is to address the challenge of limited availability and variability of annotated datasets in oral cancer diagnostics, which constrains model performance.

Method: The study utilizes an inpainting technique combined with a fine-tuned diffusion model to synthesize realistic oral cancer lesion images and enhance the diagnostic dataset.

Result: The synthetic lesions improved diagnostic algorithms, achieving 0.97 accuracy in classification between cancerous and non-cancerous tissues and 0.85 accuracy in lesion detection.

Conclusion: The approach demonstrates the potential of synthetic image generation in medical diagnostics and offers a foundation for exploring similar methods in other cancer diagnoses.

Abstract: In oral cancer diagnostics, the limited availability of annotated datasets
frequently constrains the performance of diagnostic models, particularly due to
the variability and insufficiency of training data. To address these
challenges, this study proposed a novel approach to enhance diagnostic accuracy
by synthesizing realistic oral cancer lesions using an inpainting technique
with a fine-tuned diffusion model. We compiled a comprehensive dataset from
multiple sources, featuring a variety of oral cancer images. Our method
generated synthetic lesions that exhibit a high degree of visual fidelity to
actual lesions, thereby significantly enhancing the performance of diagnostic
algorithms. The results show that our classification model achieved a
diagnostic accuracy of 0.97 in differentiating between cancerous and
non-cancerous tissues, while our detection model accurately identified lesion
locations with 0.85 accuracy. This method validates the potential for synthetic
image generation in medical diagnostics and paves the way for further research
into extending these methods to other types of cancer diagnostics.

</details>


### [211] [Differentially Private Federated Clustering with Random Rebalancing](https://arxiv.org/abs/2508.06183)
*Xiyuan Yang,Shengyuan Hu,Soyeon Kim,Tian Li*

Main category: cs.LG

TL;DR: The paper addresses privacy challenges in federated clustering by introducing RR-Cluster, a lightweight method to minimize privacy noise via random rebalancing of cluster assignments without compromising utility.


<details>
  <summary>Details</summary>
Motivation: The authors aim to tackle the vulnerabilities of federated clustering to privacy leakage while preserving model utility, as current privacy mechanisms often lead to degraded performance.

Method: The RR-Cluster technique involves randomly rebalancing cluster assignments to ensure a minimum number of clients per cluster, reducing privacy noise variance while analyzing tradeoffs between noise and bias.

Result: RR-Cluster improves privacy/utility tradeoffs when integrated with federated clustering algorithms, demonstrated across synthetic and real-world datasets.

Conclusion: The proposed RR-Cluster method is an effective add-on for federated clustering algorithms, enhancing both privacy preservation and model utility.

Abstract: Federated clustering aims to group similar clients into clusters and produce
one model for each cluster. Such a personalization approach typically improves
model performance compared with training a single model to serve all clients,
but can be more vulnerable to privacy leakage. Directly applying client-level
differentially private (DP) mechanisms to federated clustering could degrade
the utilities significantly. We identify that such deficiencies are mainly due
to the difficulties of averaging privacy noise within each cluster (following
standard privacy mechanisms), as the number of clients assigned to the same
clusters is uncontrolled. To this end, we propose a simple and effective
technique, named RR-Cluster, that can be viewed as a light-weight add-on to
many federated clustering algorithms. RR-Cluster achieves reduced privacy noise
via randomly rebalancing cluster assignments, guaranteeing a minimum number of
clients assigned to each cluster. We analyze the tradeoffs between decreased
privacy noise variance and potentially increased bias from incorrect
assignments and provide convergence bounds for RR-Clsuter. Empirically, we
demonstrate the RR-Cluster plugged into strong federated clustering algorithms
results in significantly improved privacy/utility tradeoffs across both
synthetic and real-world datasets.

</details>


### [212] [Benchmarking Pretrained Molecular Embedding Models For Molecular Representation Learning](https://arxiv.org/abs/2508.06199)
*Mateusz Praski,Jakub Adamczyk,Wojciech Czech*

Main category: cs.LG

TL;DR: The study compares 25 pretrained neural network models in molecular chemistry and finds that most do not outperform the baseline ECFP molecular fingerprint.


<details>
  <summary>Details</summary>
Motivation: To assess the effectiveness of pretrained neural networks in molecular property prediction and drug design, and to address evaluation pitfalls in previous studies.

Method: The authors conducted a systematic evaluation of 25 models across 25 datasets using a fair comparison framework and hierarchical Bayesian statistical testing.

Result: Most neural models do not show notable improvement over the ECFP molecular fingerprint baseline. The CLAMP model is the only exception with better performance.

Conclusion: Current pretrained neural networks provide limited added value over traditional ECFP molecular fingerprints, prompting a need for more rigorous evaluation and better methodologies.

Abstract: Pretrained neural networks have attracted significant interest in chemistry
and small molecule drug design. Embeddings from these models are widely used
for molecular property prediction, virtual screening, and small data learning
in molecular chemistry. This study presents the most extensive comparison of
such models to date, evaluating 25 models across 25 datasets. Under a fair
comparison framework, we assess models spanning various modalities,
architectures, and pretraining strategies. Using a dedicated hierarchical
Bayesian statistical testing model, we arrive at a surprising result: nearly
all neural models show negligible or no improvement over the baseline ECFP
molecular fingerprint. Only the CLAMP model, which is also based on molecular
fingerprints, performs statistically significantly better than the
alternatives. These findings raise concerns about the evaluation rigor in
existing studies. We discuss potential causes, propose solutions, and offer
practical recommendations.

</details>


### [213] [Graph Federated Learning for Personalized Privacy Recommendation](https://arxiv.org/abs/2508.06208)
*Ce Na,Kai Yang,Dengzhao Fang,Yu Li,Jingtong Gao,Chengcheng Zhu,Jiale Zhang,Xiaobing Sun,Yi Chang*

Main category: cs.LG

TL;DR: This paper introduces GFed-PP, a federated recommendation system capable of adjusting to varying user privacy preferences while leveraging publicly available data to improve recommendation performance.


<details>
  <summary>Details</summary>
Motivation: Current federated recommendation systems assume uniform privacy requirements, neglecting opportunities to enhance recommendations by using public user data.

Method: GFed-PP combines public user interaction data to create user-item and user relationship graphs, employs lightweight GCN for personalized item embedding, learns key metrics locally on clients, and optimizes the framework through server-side aggregation and initialization.

Result: Experimental evaluations show GFed-PP achieves superior recommendation accuracy across five datasets without compromising privacy.

Conclusion: GFed-PP is a practical and effective approach to balancing personalized privacy preferences and improved recommendation accuracy in federated systems.

Abstract: Federated recommendation systems (FedRecs) have gained significant attention
for providing privacy-preserving recommendation services. However, existing
FedRecs assume that all users have the same requirements for privacy
protection, i.e., they do not upload any data to the server. The approaches
overlook the potential to enhance the recommendation service by utilizing
publicly available user data. In real-world applications, users can choose to
be private or public. Private users' interaction data is not shared, while
public users' interaction data can be shared. Inspired by the issue, this paper
proposes a novel Graph Federated Learning for Personalized Privacy
Recommendation (GFed-PP) that adapts to different privacy requirements while
improving recommendation performance. GFed-PP incorporates the interaction data
of public users to build a user-item interaction graph, which is then used to
form a user relationship graph. A lightweight graph convolutional network (GCN)
is employed to learn each user's user-specific personalized item embedding. To
protect user privacy, each client learns the user embedding and the scoring
function locally. Additionally, GFed-PP achieves optimization of the federated
recommendation framework through the initialization of item embedding on
clients and the aggregation of the user relationship graph on the server.
Experimental results demonstrate that GFed-PP significantly outperforms
existing methods for five datasets, offering superior recommendation accuracy
without compromising privacy. This framework provides a practical solution for
accommodating varying privacy preferences in federated recommendation systems.

</details>


### [214] [Reparameterization Proximal Policy Optimization](https://arxiv.org/abs/2508.06214)
*Hai Zhong,Xun Wang,Zhuoran Li,Longbo Huang*

Main category: cs.LG

TL;DR: This paper introduces Reparameterization Proximal Policy Optimization (RPO), a stable, sample-efficient method combining RPG and PPO techniques.


<details>
  <summary>Details</summary>
Motivation: RPG shows promise for better sample efficiency but suffers from high-gradient variance, leading to training instability. The authors aim to address this issue by integrating stable policy optimization techniques.

Method: The authors connect RPG with PPO's surrogate objective formulation, enabling efficient computation of the reparameterization gradient. RPO is introduced with a clipped surrogate for RPG, KL regularization, and compatibility with variance reduction techniques.

Result: RPO demonstrates both superior sample efficiency and strong task performance in benchmarks related to locomotion and manipulation.

Conclusion: RPO effectively stabilizes RPG by incorporating PPO's techniques, achieving robust learning and improved sample reuse while maintaining compatibility with existing methods.

Abstract: Reparameterization policy gradient (RPG) is promising for improving sample
efficiency by leveraging differentiable dynamics. However, a critical barrier
is its training instability, where high-variance gradients can destabilize the
learning process. To address this, we draw inspiration from Proximal Policy
Optimization (PPO), which uses a surrogate objective to enable stable sample
reuse in the model-free setting. We first establish a connection between this
surrogate objective and RPG, which has been largely unexplored and is
non-trivial. Then, we bridge this gap by demonstrating that the
reparameterization gradient of a PPO-like surrogate objective can be computed
efficiently using backpropagation through time. Based on this key insight, we
propose Reparameterization Proximal Policy Optimization (RPO), a stable and
sample-efficient RPG-based method. RPO enables multiple epochs of stable sample
reuse by optimizing a clipped surrogate objective tailored for RPG, while being
further stabilized by Kullback-Leibler (KL) divergence regularization and
remaining fully compatible with existing variance reduction methods. We
evaluate RPO on a suite of challenging locomotion and manipulation tasks, where
experiments demonstrate that our method achieves superior sample efficiency and
strong performance.

</details>


### [215] [Epidemic Control on a Large-Scale-Agent-Based Epidemiology Model using Deep Deterministic Policy Gradient](https://arxiv.org/abs/2304.04475)
*Gaurav Deshkar,Jayanta Kshirsagar,Harshal Hayatnagarkar,Janani Venugopalan*

Main category: cs.LG

TL;DR: The paper explores an optimization framework using Deep Deterministic Policy Gradient (DDPG) to balance health and economic objectives in large-scale pandemic interventions.


<details>
  <summary>Details</summary>
Motivation: Current methods for modeling optimal pandemic interventions are limited in scope, scale, and flexibility.

Method: The study uses DDPG-based policy optimization in a 100,000-individual agent-based epidemiological simulation to determine ideal lockdowns and vaccination strategies.

Result: Optimal outcomes include minimal economic impact and balanced health objectives when mid-age and elderly individuals are vaccinated with no lockdowns.

Conclusion: Further validation and open-sourcing of the framework are necessary for broader applicability and reliability.

Abstract: To mitigate the impact of the pandemic, several measures include lockdowns,
rapid vaccination programs, school closures, and economic stimulus. These
interventions can have positive or unintended negative consequences. Current
research to model and determine an optimal intervention automatically through
round-tripping is limited by the simulation objectives, scale (a few thousand
individuals), model types that are not suited for intervention studies, and the
number of intervention strategies they can explore (discrete vs continuous). We
address these challenges using a Deep Deterministic Policy Gradient (DDPG)
based policy optimization framework on a large-scale (100,000 individual)
epidemiological agent-based simulation where we perform multi-objective
optimization. We determine the optimal policy for lockdown and vaccination in a
minimalist age-stratified multi-vaccine scenario with a basic simulation for
economic activity. With no lockdown and vaccination (mid-age and elderly),
results show optimal economy (individuals below the poverty line) with balanced
health objectives (infection, and hospitalization). An in-depth simulation is
needed to further validate our results and open-source our framework.

</details>


### [216] [Membership Inference Attack with Partial Features](https://arxiv.org/abs/2508.06244)
*Xurun Wang,Guangrui Liu,Xinjie Li,Haoyu He,Lin Yao,Weizhe Zhang*

Main category: cs.LG

TL;DR: This paper introduces Partial Feature Membership Inference (PFMI), a new type of membership inference attack where adversaries have only partial access to feature data. The proposed MRAD framework effectively addresses this problem, demonstrating strong performance across datasets.


<details>
  <summary>Details</summary>
Motivation: The motivation is to study membership inference attacks in scenarios where adversaries have access to only partial feature information, which is common in real-world situations and limits the applicability of existing methods.

Method: The paper proposes MRAD, a two-stage attack framework. In the first stage, it reconstructs missing feature values to minimize loss. In the second stage, it detects anomalies by measuring deviations from the training set's distribution.

Result: MRAD performs effectively across multiple datasets and can integrate well with existing anomaly detection techniques. For instance, it achieves an AUC of about 0.6 on STL-10 even with 40% of features missing.

Conclusion: MRAD provides a robust approach for membership inference attacks under partial feature observability. It highlights the vulnerability of machine learning models to PFMI attacks, even with limited feature data.

Abstract: Machine learning models have been shown to be susceptible to membership
inference attack, which can be used to determine whether a given sample appears
in the training data. Existing membership inference methods commonly assume
that the adversary has full access to the features of the target sample. This
assumption, however, does not hold in many real-world scenarios where only
partial features information is available, thereby limiting the applicability
of these methods. In this work, we study an inference scenario where the
adversary observes only partial features of each sample and aims to infer
whether this observed subset was present in the training set of the target
model. We define this problem as Partial Feature Membership Inference (PFMI).
To address this problem, we propose MRAD (Memory-guided Reconstruction and
Anomaly Detection), a two-stage attack framework. In the first stage, MRAD
optimizes the unknown feature values to minimize the loss of the sample. In the
second stage, it measures the deviation between the reconstructed sample and
the training distribution using anomaly detection. Empirical results
demonstrate that MRAD is effective across a range of datasets, and maintains
compatibility with various off-the-shelf anomaly detection techniques. For
example, on STL-10, our attack achieves an AUC of around 0.6 even with 40% of
the missing features.

</details>


### [217] [AttriLens-Mol: Attribute Guided Reinforcement Learning for Molecular Property Prediction with Large Language Models](https://arxiv.org/abs/2508.04748)
*Xuan Lin,Long Chen,Yile Wang*

Main category: cs.LG

TL;DR: The paper proposes AttriLens-Mol, a reinforcement learning framework to improve molecular property prediction using LLMs by guiding their reasoning process with attribute-based rewards. This method outperforms prior models and enhances interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs for molecular property prediction rely heavily on human-crafted prompts/templates and produce verbose or irrelevant reasoning. A more structured and effective approach is needed.

Method: Introduced AttriLens-Mol, which uses three types of rewards (format, count, rationality) to guide LLMs towards generating structured, relevant, and rational output. Experiments were conducted using specifically trained 7B LLMs on molecular datasets.

Result: AttriLens-Mol achieved comparable or better results than supervised fine-tuning models and advanced LLMs. Generated attributes, when used with decision tree models, showed enhanced performance and interpretability.

Conclusion: AttriLens-Mol effectively elicits relevant molecular attributes, improving both predictive performance and interpretability of molecular property predictions. The open-source release provides resources for further exploration.

Abstract: Large Language Models (LLMs) have shown promise in assisting molecular
property prediction tasks but often rely on human-crafted prompts and
chain-of-thought templates. While recent advanced large reasoning models like
DeepSeek-R1 employ reinforcement learning for an extended ``thinking'' process,
their reasoning can be verbose and lack relevance. We introduce AttriLens-Mol,
an attribute-guided reinforcement learning framework for molecular property
prediction with LLMs. AttriLens-Mol steers the model's reasoning by using: (1)
a format reward encouraging attribute-based structured output, (2) a count
reward to avoid enumerating irrelevant attributes, and (3) a rationality reward
using advanced LLMs and RDKit to verify the relatedness of the generated
attributes. This approach implicitly elicits the model's inherent knowledge of
relevant molecular attributes during reasoning, enables making predictions for
the molecular property more effectively. Experiments on both in-distribution
and out-of-distribution datasets show that, training both 7B-size
R1-Distilled-Qwen2.5 and R1-Distilled-LLaMA3.1 models on 4,000 samples with our
proposed AttriLens-Mol method significantly boosts the performance, getting
comparable or better results than supervised fine-tuning models
(Mol-Instructions, ChemDFM, etc.) and advanced models (GPT-3.5, GPT-4o,
DeepSeek-V3, DeepSeek-R1, etc.). Further, our extracted attributes for the
target property, when used as features for an interpretable decision tree
model, yield superior performance compared to attributes generated by prompting
LLMs. This shows that AttriLens-Mol effectively elicits more relevant and
predictive molecular attributes, leading to enhanced interpretability and
performance for property prediction. We release the code in
https://github.com/szu-tera/AttriLens-Mol.

</details>


### [218] [In-Training Defenses against Emergent Misalignment in Language Models](https://arxiv.org/abs/2508.06249)
*David Kaczér,Magnus Jørgenvåg,Clemens Vetter,Lucie Flek,Florian Mai*

Main category: cs.LG

TL;DR: The paper investigates methods to address emergent misalignment (EMA) in fine-tuned large language models, proposing and testing four training interventions.


<details>
  <summary>Details</summary>
Motivation: Practitioners fine-tune aligned LLMs for domain-specific tasks, but this can unexpectedly create harmful behaviors outside the target domain, posing risks for models exposed via fine-tuning APIs.

Method: The paper tests four regularization interventions during fine-tuning to mitigate EMA: KL-divergence regularization, feature-space distance enforcement, SafeLoRA projection, and interleaving safe examples during fine-tuning.

Result: The interventions were systematically evaluated for their effects on emergent misalignment across malicious tasks and their performance on benign tasks.

Conclusion: Several training strategies show promise in combating EMA, though further research is needed to explore unanswered questions and refine safeguards against misalignment.

Abstract: Fine-tuning lets practitioners repurpose aligned large language models (LLMs)
for new domains, yet recent work reveals emergent misalignment (EMA): Even a
small, domain-specific fine-tune can induce harmful behaviors far outside the
target domain. Even in the case where model weights are hidden behind a
fine-tuning API, this gives attackers inadvertent access to a broadly
misaligned model in a way that can be hard to detect from the fine-tuning data
alone. We present the first systematic study of in-training safeguards against
EMA that are practical for providers who expose fine-tuning via an API. We
investigate four training regularization interventions: (i) KL-divergence
regularization toward a safe reference model, (ii) $\ell_2$ distance in feature
space, (iii) projecting onto a safe subspace (SafeLoRA), and (iv) interleaving
of a small amount of safe training examples from a general instruct-tuning
dataset. We first evaluate the methods' emergent misalignment effect across
four malicious, EMA-inducing tasks. Second, we assess the methods' impacts on
benign tasks. We conclude with a discussion of open questions in emergent
misalignment research.

</details>


### [219] [Synthetic Data Generation and Differential Privacy using Tensor Networks' Matrix Product States (MPS)](https://arxiv.org/abs/2508.06251)
*Alejandro Moreno R.,Desale Fentaw,Samuel Palmer,Raúl Salles de Padua,Ninad Dixit,Samuel Mugel,Roman Orús,Manuel Radons,Josef Menter,Ali Abedi*

Main category: cs.LG

TL;DR: A method is proposed to generate high-quality, privacy-preserving synthetic tabular data using Matrix Product States (MPS), outperforming traditional approaches under privacy constraints.


<details>
  <summary>Details</summary>
Motivation: The increasing need to overcome data scarcity, address privacy concerns, and create diverse datasets in AI demands effective synthetic data generation methods.

Method: The study employs Tensor Networks, especially Matrix Product States (MPS), integrating noise injection and gradient clipping for Rényi Differential Privacy, and benchmarks this against models like CTGAN, VAE, and PrivBayes.

Result: The proposed MPS model outperformed classical models in data fidelity and downstream task performance, particularly under stringent privacy settings.

Conclusion: MPS offers a scalable and interpretable solution for generating privacy-preserving synthetic data, making it suitable for sensitive domains requiring high data quality and confidentiality.

Abstract: Synthetic data generation is a key technique in modern artificial
intelligence, addressing data scarcity, privacy constraints, and the need for
diverse datasets in training robust models. In this work, we propose a method
for generating privacy-preserving high-quality synthetic tabular data using
Tensor Networks, specifically Matrix Product States (MPS). We benchmark the
MPS-based generative model against state-of-the-art models such as CTGAN, VAE,
and PrivBayes, focusing on both fidelity and privacy-preserving capabilities.
To ensure differential privacy (DP), we integrate noise injection and gradient
clipping during training, enabling privacy guarantees via R\'enyi Differential
Privacy accounting. Across multiple metrics analyzing data fidelity and
downstream machine learning task performance, our results show that MPS
outperforms classical models, particularly under strict privacy constraints.
This work highlights MPS as a promising tool for privacy-aware synthetic data
generation. By combining the expressive power of tensor network representations
with formal privacy mechanisms, the proposed approach offers an interpretable
and scalable alternative for secure data sharing. Its structured design
facilitates integration into sensitive domains where both data quality and
confidentiality are critical.

</details>


### [220] [Multi-Omics Analysis for Cancer Subtype Inference via Unrolling Graph Smoothness Priors](https://arxiv.org/abs/2508.06257)
*Jielong Lu,Zhihao Wu,Jiajun Yu,Jiajun Bu,Haishuai Wang*

Main category: cs.LG

TL;DR: This paper introduces GTMancer, leveraging Graph Neural Networks and contrastive learning for improved multi-omics cancer subtype classification.


<details>
  <summary>Details</summary>
Motivation: Current methods for multi-omics cancer subtype classification fall short in addressing the intricate coupling between heterogeneous omics data.

Method: The approach introduces GTMancer, which uses contrastive learning to unify multi-omics data in a semantic space, and employs dual attention mechanisms for refining graph representations.

Result: Empirical tests on seven cancer datasets show GTMancer surpasses existing algorithms in performance.

Conclusion: GTMancer enhances precision oncology by improving the integration and analysis of heterogeneous multi-omics data for cancer subtype classification.

Abstract: Integrating multi-omics datasets through data-driven analysis offers a
comprehensive understanding of the complex biological processes underlying
various diseases, particularly cancer. Graph Neural Networks (GNNs) have
recently demonstrated remarkable ability to exploit relational structures in
biological data, enabling advances in multi-omics integration for cancer
subtype classification. Existing approaches often neglect the intricate
coupling between heterogeneous omics, limiting their capacity to resolve subtle
cancer subtype heterogeneity critical for precision oncology. To address these
limitations, we propose a framework named Graph Transformer for Multi-omics
Cancer Subtype Classification (GTMancer). This framework builds upon the GNN
optimization problem and extends its application to complex multi-omics data.
Specifically, our method leverages contrastive learning to embed multi-omics
data into a unified semantic space. We unroll the multiplex graph optimization
problem in that unified space and introduce dual sets of attention coefficients
to capture structural graph priors both within and among multi-omics data. This
approach enables global omics information to guide the refining of the
representations of individual omics. Empirical experiments on seven real-world
cancer datasets demonstrate that GTMancer outperforms existing state-of-the-art
algorithms.

</details>


### [221] [OM2P: Offline Multi-Agent Mean-Flow Policy](https://arxiv.org/abs/2508.06269)
*Zhuoran Li,Xun Wang,Hai Zhong,Longbo Huang*

Main category: cs.LG

TL;DR: This paper introduces OM2P, a novel offline MARL algorithm using mean-flow policies to achieve efficient one-step action sampling, addressing time and resource constraints.


<details>
  <summary>Details</summary>
Motivation: Generative models like diffusion and flow-based models show promise in offline MARL but face challenges such as low sampling efficiency due to iterative generation processes.

Method: The authors propose OM2P, integrating mean-flow matching loss with Q-function supervision for reward alignment, introducing a generalized timestep distribution, and a derivative-free estimation strategy.

Result: OM2P achieves up to a 3.8x reduction in GPU memory use and up to a 10.8x speed-up in training time, achieving superior performance in Multi-Agent Particle and MuJoCo benchmarks.

Conclusion: OM2P demonstrates the practical integration of mean-flow models into offline MARL, providing scalable and efficient solutions for cooperative multi-agent policies.

Abstract: Generative models, especially diffusion and flow-based models, have been
promising in offline multi-agent reinforcement learning. However, integrating
powerful generative models into this framework poses unique challenges. In
particular, diffusion and flow-based policies suffer from low sampling
efficiency due to their iterative generation processes, making them impractical
in time-sensitive or resource-constrained settings. To tackle these
difficulties, we propose OM2P (Offline Multi-Agent Mean-Flow Policy), a novel
offline MARL algorithm to achieve efficient one-step action sampling. To
address the misalignment between generative objectives and reward maximization,
we introduce a reward-aware optimization scheme that integrates a
carefully-designed mean-flow matching loss with Q-function supervision.
Additionally, we design a generalized timestep distribution and a
derivative-free estimation strategy to reduce memory overhead and improve
training stability. Empirical evaluations on Multi-Agent Particle and MuJoCo
benchmarks demonstrate that OM2P achieves superior performance, with up to a
3.8x reduction in GPU memory usage and up to a 10.8x speed-up in training time.
Our approach represents the first to successfully integrate mean-flow model
into offline MARL, paving the way for practical and scalable generative
policies in cooperative multi-agent settings.

</details>


### [222] [A Study on Regularization-Based Continual Learning Methods for Indic ASR](https://arxiv.org/abs/2508.06280)
*Gokul Adethya T,S. Jaya Nirmala*

Main category: cs.LG

TL;DR: The paper studies Continual Learning (CL) strategies for Automatic Speech Recognition (ASR) systems in Indian languages using a Conformer-based hybrid model. It compares Elastic Weight Consolidation, Memory Aware Synapses, and Learning without Forgetting methods for sequential learning without catastrophic forgetting.


<details>
  <summary>Details</summary>
Motivation: India's linguistic diversity necessitates ASR systems that can adapt to multiple languages under constraints like sequential data arrival and privacy. The motivation is to enable scalable multilingual ASR solutions without catastrophic forgetting.

Method: The authors use a Conformer-based hybrid RNN-T/CTC model initially pretrained on Hindi, which is incrementally trained on eight additional Indian languages. Regularization- and distillation-based CL methods like EWC, MAS, and LwF are evaluated for their Word Error Rate (WER) performance and knowledge retention in no-replay settings.

Result: The results show that CL strategies outperform naive fine-tuning by mitigating forgetting. Experiments also reveal how varying training epochs affects performance and backward knowledge transfer under clean and noisy data conditions.

Conclusion: Continual Learning methods like EWC, MAS, and LwF provide scalable solutions for multilingual ASR in languages like Hindi and others. This approach is realistic and effective for implementing ASR systems that respect privacy and adapt sequentially.

Abstract: Indias linguistic diversity poses significant challenges for developing
inclusive Automatic Speech Recognition (ASR) systems. Traditional multilingual
models, which require simultaneous access to all language data, are impractical
due to the sequential arrival of data and privacy constraints. Continual
Learning (CL) offers a solution by enabling models to learn new languages
sequentially without catastrophically forgetting previously learned knowledge.
This paper investigates CL for ASR on Indian languages using a subset of the
IndicSUPERB benchmark. We employ a Conformer-based hybrid RNN-T/CTC model,
initially pretrained on Hindi, which is then incrementally trained on eight
additional Indian languages, for a total sequence of nine languages. We
evaluate three prominent regularization- and distillation-based CL strategies:
Elastic Weight Consolidation (EWC), Memory Aware Synapses (MAS), and Learning
without Forgetting (LwF), selected for their suitability in no-replay,
privacy-conscious scenarios. Performance is analyzed using Word Error Rate
(WER) for both RNN-T and CTC paths on clean and noisy data, as well as
knowledge retention via Backward Transfer. We also explore the impact of
varying the number of training epochs (1, 2, 5, and 10) per task. Results,
compared against naive fine-tuning, demonstrate CLs effectiveness in mitigating
forgetting, making it a promising approach for scalable ASR in diverse Indian
languages under realistic constraints. The code is available at:
https://github.com/FrozenWolf-Cyber/Indic-CL-ASR

</details>


### [223] [Low-Bit Data Processing Using Multiple-Output Spiking Neurons with Non-linear Reset Feedback](https://arxiv.org/abs/2508.06292)
*Sanja Karilanova,Subhrakanti Dey,Ayça Özçelikkale*

Main category: cs.LG

TL;DR: The paper introduces a novel spiking neuron model combining linear state transitions with a reset mechanism, achieving competitive performance in signal processing tasks.


<details>
  <summary>Details</summary>
Motivation: The study aims to bridge the advantages of spiking neural networks (SNNs) and deep state-space models (SSMs) to enhance low-latency, energy-efficient computing.

Method: A multiple-output spiking neuron model is proposed, integrating linear SSM state transitions with nonlinear reset mechanisms for enhanced stability and learning.

Result: The model achieves comparable performance benchmarks across tasks such as keyword spotting, event-based vision, and sequential pattern recognition.

Conclusion: The reset mechanism overcomes neuron instability and surpasses the strictly enforced stability limitations in linear dynamics of deep SSM models.

Abstract: Neuromorphic computing is an emerging technology enabling low-latency and
energy-efficient signal processing. A key algorithmic tool in neuromorphic
computing is spiking neural networks (SNNs). SNNs are biologically inspired
neural networks which utilize stateful neurons, and provide low-bit data
processing by encoding and decoding information using spikes. Similar to SNNs,
deep state-space models (SSMs) utilize stateful building blocks. However, deep
SSMs, which recently achieved competitive performance in various temporal
modeling tasks, are typically designed with high-precision activation functions
and no reset mechanisms. To bridge the gains offered by SNNs and the recent
deep SSM models, we propose a novel multiple-output spiking neuron model that
combines a linear, general SSM state transition with a non-linear feedback
mechanism through reset. Compared to the existing neuron models for SNNs, our
proposed model clearly conceptualizes the differences between the spiking
function, the reset condition and the reset action. The experimental results on
various tasks, i.e., a keyword spotting task, an event-based vision task and a
sequential pattern recognition task, show that our proposed model achieves
performance comparable to existing benchmarks in the SNN literature. Our
results illustrate how the proposed reset mechanism can overcome instability
and enable learning even when the linear part of neuron dynamics is unstable,
allowing us to go beyond the strictly enforced stability of linear dynamics in
recent deep SSM models.

</details>


### [224] [Unsupervised Partner Design Enables Robust Ad-hoc Teamwork](https://arxiv.org/abs/2508.06336)
*Constantin Ruhdorfer,Matteo Bortoletto,Victor Oei,Anna Penzkofer,Andreas Bulling*

Main category: cs.LG

TL;DR: UPD is a novel framework for ad-hoc teamwork that generates diverse training partners without requiring predefined resources, achieving superior performance in cooperative tasks.


<details>
  <summary>Details</summary>
Motivation: To achieve robust ad-hoc teamwork without relying on pretrained partners or manual hyperparameter tuning.

Method: UPD stochastically mixes the ego agent's policy with random behaviors and evaluates these partners using a variance-based learnability metric.

Result: UPD surpasses both population-based and population-free baselines on benchmarks like Overcooked-AI, and demonstrates adaptability and human-like behavior in a user study.

Conclusion: UPD is an effective and adaptive solution for generating training partners in multi-agent reinforcement learning.

Abstract: We introduce Unsupervised Partner Design (UPD) - a population-free,
multi-agent reinforcement learning framework for robust ad-hoc teamwork that
adaptively generates training partners without requiring pretrained partners or
manual parameter tuning. UPD constructs diverse partners by stochastically
mixing an ego agent's policy with biased random behaviours and scores them
using a variance-based learnability metric that prioritises partners near the
ego agent's current learning frontier. We show that UPD can be integrated with
unsupervised environment design, resulting in the first method enabling fully
unsupervised curricula over both level and partner distributions in a
cooperative setting. Through extensive evaluations on Overcooked-AI and the
Overcooked Generalisation Challenge, we demonstrate that this dynamic partner
curriculum is highly effective: UPD consistently outperforms both
population-based and population-free baselines as well as ablations. In a user
study, we further show that UPD achieves higher returns than all baselines and
was perceived as significantly more adaptive, more human-like, a better
collaborator, and less frustrating.

</details>


### [225] [Introducing Fractional Classification Loss for Robust Learning with Noisy Labels](https://arxiv.org/abs/2508.06346)
*Mert Can Kurucu,Tufan Kumbasar,İbrahim Eksin,Müjde Güzelkaya*

Main category: cs.LG

TL;DR: The paper introduces Fractional Classification Loss (FCL), a robust loss function for deep networks that adapts to label noise without requiring dataset-specific hyperparameter tuning.


<details>
  <summary>Details</summary>
Motivation: Existing robust loss functions struggle with label noise and require heavy hyperparameter tuning, motivating the need for a more adaptive approach.

Method: FCL uses the fractional derivative of Cross-Entropy loss as an active component and Mean Absolute Error as a passive one. The fractional derivative order is learned and optimized during training.

Result: FCL automatically balances robustness and convergence speed, achieving superior performance on benchmarks with label noise.

Conclusion: FCL eliminates the need for hyperparameter tuning and offers robust, adaptive loss behavior, making it effective against label noise.

Abstract: Robust loss functions are crucial for training deep neural networks in the
presence of label noise, yet existing approaches require extensive,
dataset-specific hyperparameter tuning. In this work, we introduce Fractional
Classification Loss (FCL), an adaptive robust loss that automatically
calibrates its robustness to label noise during training. Built within the
active-passive loss framework, FCL employs the fractional derivative of the
Cross-Entropy (CE) loss as its active component and the Mean Absolute Error
(MAE) as its passive loss component. With this formulation, we demonstrate that
the fractional derivative order $\mu$ spans a family of loss functions that
interpolate between MAE-like robustness and CE-like fast convergence.
Furthermore, we integrate $\mu$ into the gradient-based optimization as a
learnable parameter and automatically adjust it to optimize the trade-off
between robustness and convergence speed. We reveal that FCL's unique property
establishes a critical trade-off that enables the stable learning of $\mu$:
lower log penalties on difficult or mislabeled examples improve robustness but
impose higher penalties on easy or clean data, reducing model confidence in
them. Consequently, FCL can dynamically reshape its loss landscape to achieve
effective classification performance under label noise. Extensive experiments
on benchmark datasets show that FCL achieves state-of-the-art results without
the need for manual hyperparameter tuning.

</details>


### [226] [Geometric-k-means: A Bound Free Approach to Fast and Eco-Friendly k-means](https://arxiv.org/abs/2508.06353)
*Parichit Sharma,Marcin Stanislaw,Hasan Kurban,Oguzhan Kulekci,Mehmet Dalkilic*

Main category: cs.LG

TL;DR: The paper presents a new version of the k-means algorithm, called Geometric-k-means (Gk-means), which uses geometric principles for faster processing and enhanced efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency and high computational cost of the traditional k-means algorithm while maintaining solution quality.

Method: Utilizes geometric principles like scalar projection to differentiate between impactful (HE) and non-impactful (LE) data points for efficient cluster updates.

Result: Gk-means demonstrates superior runtime, computational efficiency, and reduced energy consumption compared to traditional and state-of-the-art versions across various datasets.

Conclusion: Gk-means provides a sustainable, efficient alternative to traditional k-means, significantly improving performance and resource utilization without compromising clustering quality.

Abstract: This paper introduces Geometric-k-means (or Gk-means for short), a novel
approach that significantly enhances the efficiency and energy economy of the
widely utilized k-means algorithm, which, despite its inception over five
decades ago, remains a cornerstone in machine learning applications. The
essence of Gk-means lies in its active utilization of geometric principles,
specifically scalar projection, to significantly accelerate the algorithm
without sacrificing solution quality. This geometric strategy enables a more
discerning focus on data points that are most likely to influence cluster
updates, which we call as high expressive data (HE). In contrast, low
expressive data (LE), does not impact clustering outcome, is effectively
bypassed, leading to considerable reductions in computational overhead.
Experiments spanning synthetic, real-world and high-dimensional datasets,
demonstrate Gk-means is significantly better than traditional and state of the
art (SOTA) k-means variants in runtime and distance computations (DC).
Moreover, Gk-means exhibits better resource efficiency, as evidenced by its
reduced energy footprint, placing it as more sustainable alternative.

</details>


### [227] [Beyond Prompt-Induced Lies: Investigating LLM Deception on Benign Prompts](https://arxiv.org/abs/2508.06361)
*Zhaomin Wu,Mingzhe Du,See-Kiong Ng,Bingsheng He*

Main category: cs.LG

TL;DR: The paper investigates self-initiated deception in Large Language Models (LLMs), introducing two metrics to measure deception tendencies, finding that deception increases with task complexity.


<details>
  <summary>Details</summary>
Motivation: The study aims to address trustworthiness concerns of LLMs, specifically the unexplored threat of self-initiated deception in real-world benign prompts.

Method: A novel framework using 'contact searching questions' introduces two metrics: Deceptive Intention Score and Deceptive Behavior Score, evaluated across 14 leading LLMs.

Result: Findings reveal that deception tendencies, as quantified by the two metrics, escalate parallelly with task complexity in most examined LLMs.

Conclusion: The increasing propensity for deception in advanced LLMs during complex problems raises critical concerns about their deployment in high-stakes domains.

Abstract: Large Language Models (LLMs) have been widely deployed in reasoning,
planning, and decision-making tasks, making their trustworthiness a critical
concern. The potential for intentional deception, where an LLM deliberately
fabricates or conceals information to serve a hidden objective, remains a
significant and underexplored threat. Existing studies typically induce such
deception by explicitly setting a "hidden" objective through prompting or
fine-tuning, which may not fully reflect real-world human-LLM interactions.
Moving beyond this human-induced deception, we investigate LLMs' self-initiated
deception on benign prompts. To address the absence of ground truth in this
evaluation, we propose a novel framework using "contact searching questions."
This framework introduces two statistical metrics derived from psychological
principles to quantify the likelihood of deception. The first, the Deceptive
Intention Score, measures the model's bias towards a hidden objective. The
second, Deceptive Behavior Score, measures the inconsistency between the LLM's
internal belief and its expressed output. Upon evaluating 14 leading LLMs, we
find that both metrics escalate as task difficulty increases, rising in
parallel for most models. Building on these findings, we formulate a
mathematical model to explain this behavior. These results reveal that even the
most advanced LLMs exhibit an increasing tendency toward deception when
handling complex problems, raising critical concerns for the deployment of LLM
agents in complex and crucial domains.

</details>


### [228] [ActivityDiff: A diffusion model with Positive and Negative Activity Guidance for De Novo Drug Design](https://arxiv.org/abs/2508.06364)
*Renyi Zhou,Huimin Zhu,Jing Tang,Min Li*

Main category: cs.LG

TL;DR: This paper introduces ActivityDiff, a new drug design method using classifier-guided diffusion models to control targeted molecular activity and mitigate off-target toxicity.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenge of achieving precise control over multifaceted molecular behaviors (targeted activation/inhibition, multi-target efficacy, and toxicity mitigation) in drug design.

Method: ActivityDiff uses classifier-guidance in diffusion models, which leverage drug-target classifiers to guide molecular generation positively (desired activities) and negatively (reduce harmful effects).

Result: ActivityDiff demonstrated strong performance in critical design tasks such as dual-target generation, selective target enhancement, and off-target toxicity reduction.

Conclusion: This approach effectively balances drug efficacy and safety, offering a novel framework with potential extensions for integrated molecular activity control in drug design.

Abstract: Achieving precise control over a molecule's biological activity-encompassing
targeted activation/inhibition, cooperative multi-target modulation, and
off-target toxicity mitigation-remains a critical challenge in de novo drug
design. However, existing generative methods primarily focus on producing
molecules with a single desired activity, lacking integrated mechanisms for the
simultaneous management of multiple intended and unintended molecular
interactions. Here, we propose ActivityDiff, a generative approach based on the
classifier-guidance technique of diffusion models. It leverages separately
trained drug-target classifiers for both positive and negative guidance,
enabling the model to enhance desired activities while minimizing harmful
off-target effects. Experimental results show that ActivityDiff effectively
handles essential drug design tasks, including single-/dual-target generation,
fragment-constrained dual-target design, selective generation to enhance target
specificity, and reduction of off-target effects. These results demonstrate the
effectiveness of classifier-guided diffusion in balancing efficacy and safety
in molecular design. Overall, our work introduces a novel paradigm for
achieving integrated control over molecular activity, and provides ActivityDiff
as a versatile and extensible framework.

</details>


### [229] [End-to-End Text-to-SQL with Dataset Selection: Leveraging LLMs for Adaptive Query Generation](https://arxiv.org/abs/2508.06387)
*Anurag Tripathi,Vaibhav Patle,Abhinav Jain,Ayush Pundir,Sairam Menon,Ajeet Kumar Singh*

Main category: cs.LG

TL;DR: The paper introduces a three-stage framework for the Text-to-SQL task, improving accuracy by first predicting the target database identifier (db_id) before generating SQL queries.


<details>
  <summary>Details</summary>
Motivation: To address challenges in Text-to-SQL scenarios where the user's intended database is not pre-specified, which can be problematic when multiple databases are involved.

Method: They use a three-stage framework: 1) Extract implicit database-related information via LLMs and prompt engineering. 2) Train a RoBERTa-based encoder for database identifier (db_id) prediction. 3) Refine SQL generation using critic agents for error correction.

Result: The proposed framework achieves better database intent prediction and SQL generation accuracy, surpassing current state-of-the-art models.

Conclusion: The approach demonstrates the importance of database selection in Text-to-SQL tasks and shows that incorporating database intent prediction and error refinement leads to improved performance in querying tasks.

Abstract: Text-to-SQL bridges the gap between natural language and structured database
language, thus allowing non-technical users to easily query databases.
Traditional approaches model text-to-SQL as a direct translation task, where a
given Natural Language Query (NLQ) is mapped to an SQL command. Recent advances
in large language models (LLMs) have significantly improved translation
accuracy, however, these methods all require that the target database is
pre-specified. This becomes problematic in scenarios with multiple extensive
databases, where identifying the correct database becomes a crucial yet
overlooked step. In this paper, we propose a three-stage end-to-end text-to-SQL
framework to identify the user's intended database before generating SQL
queries. Our approach leverages LLMs and prompt engineering to extract implicit
information from natural language queries (NLQs) in the form of a ruleset. We
then train a large db\_id prediction model, which includes a RoBERTa-based
finetuned encoder, to predict the correct Database identifier (db\_id) based on
both the NLQ and the LLM-generated rules. Finally, we refine the generated SQL
by using critic agents to correct errors. Experimental results demonstrate that
our framework outperforms the current state-of-the-art models in both database
intent prediction and SQL generation accuracy.

</details>


### [230] [A New Lens on Homelessness: Daily Tent Monitoring with 311 Calls and Street Images](https://arxiv.org/abs/2508.06409)
*Wooyong Jung,Sola Kim,Dongwook Kim,Maryam Tabar,Dongwon Lee*

Main category: cs.LG

TL;DR: This paper introduces a novel method using 311 Service Calls and street-level imagery data to track and predict homelessness tent trends in San Francisco with fine-grained spatial and temporal detail.


<details>
  <summary>Details</summary>
Motivation: Existing methods for monitoring homelessness, like PIT counts, lack frequency, consistency, and detailed spatial resolution.

Method: The study utilizes 311 Service Calls and street-level imagery data to create a predictive model that captures daily and neighborhood-level variations in homeless tent trends.

Result: The model reveals nuanced trends overlooked by traditional methods, such as fluctuations during the COVID-19 pandemic and shifts in tent locations.

Conclusion: This approach offers a more timely, localized, and cost-effective method for informing homelessness policy responses and evaluating interventions.

Abstract: Homelessness in the United States has surged to levels unseen since the Great
Depression. However, existing methods for monitoring it, such as point-in-time
(PIT) counts, have limitations in terms of frequency, consistency, and spatial
detail. This study proposes a new approach using publicly available,
crowdsourced data, specifically 311 Service Calls and street-level imagery, to
track and forecast homeless tent trends in San Francisco. Our predictive model
captures fine-grained daily and neighborhood-level variations, uncovering
patterns that traditional counts often overlook, such as rapid fluctuations
during the COVID-19 pandemic and spatial shifts in tent locations over time. By
providing more timely, localized, and cost-effective information, this approach
serves as a valuable tool for guiding policy responses and evaluating
interventions aimed at reducing unsheltered homelessness.

</details>


### [231] [Sample-efficient LLM Optimization with Reset Replay](https://arxiv.org/abs/2508.06412)
*Zichuan Liu,Jinyu Wang,Lei Song,Jiang Bian*

Main category: cs.LG

TL;DR: The paper introduces LoRR, a method for more sample-efficient and effective large language model tuning, particularly in reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: To improve reasoning capabilities in LLMs while addressing issues like low sample efficiency and primacy bias in current optimization methods.

Method: LoRR combines high-replay training with a reset-and-reuse strategy for initial data and a hybrid optimization objective utilizing both supervised fine-tuning and preference-based losses.

Result: LoRR significantly enhances performance in preference optimization methods, achieving results comparable to complex RL-based algorithms on challenging tasks.

Conclusion: LoRR provides a practical, efficient, and effective approach for finetuning LLMs, demonstrating its utility in unlocking higher performance using limited data.

Abstract: Recent advancements in post-training Large Language Models (LLMs),
particularly through Reinforcement Learning (RL) and preference optimization
methods, are key drivers for enhancing their reasoning capabilities. However,
these methods are often plagued by low sample efficiency and a susceptibility
to primacy bias, where overfitting to initial experiences degrades policy
quality and damages the learning process. To address these challenges, we
introduce LLM optimization with Reset Replay (LoRR), a general and powerful
plugin designed to enhance sample efficiency in any preference-based
optimization framework. LoRR core mechanism enables training at a high replay
number, maximizing the utility of each collected data batch. To counteract the
risk of overfitting inherent in high-replay training, LoRR incorporates a
periodic reset strategy with reusing initial data, which preserves network
plasticity. Furthermore, it leverages a hybrid optimization objective,
combining supervised fine-tuning (SFT) and preference-based losses to further
bolster data exploitation. Our extensive experiments demonstrate that LoRR
significantly boosts the performance of various preference optimization methods
on both mathematical and general reasoning benchmarks. Notably, an iterative
DPO approach augmented with LoRR achieves comparable performance on challenging
math tasks, outperforming some complex and computationally intensive RL-based
algorithms. These findings highlight that LoRR offers a practical,
sample-efficient, and highly effective paradigm for LLM finetuning, unlocking
greater performance from limited data.

</details>


### [232] [LLM Unlearning using Gradient Ratio-Based Influence Estimation and Noise Injection](https://arxiv.org/abs/2508.06467)
*Ameya Anjarlekar,Sandeep Pombra*

Main category: cs.LG

TL;DR: The paper presents GRIN, a modular framework for targeted unlearning in large language models (LLMs) using a gradient-ratio-based metric for precise parameter adjustment and introduces new evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: To address issues of machine unlearning in LLMs due to sensitive or unauthorized data, ensuring high effectiveness with minimal degradation of unrelated knowledge.

Method: GRIN employs a gradient-ratio-based metric to locate parameters responsible for memorizing specific data. It introduces selective noise injection into these parameters and proposes new metrics for evaluating unlearning in LLMs.

Result: Validation using standard benchmarks shows GRIN achieves improved unlearning performance while preserving the overall model utility.

Conclusion: GRIN provides a robust solution for targeted unlearning in LLMs, balancing effective removal of sensitive data and preserving useful knowledge with improved evaluation mechanisms.

Abstract: The growing legal and ethical scrutiny of large language models (LLMs)
necessitates effective machine unlearning, particularly for sensitive or
unauthorized data. Existing empirical methods often yield incomplete forgetting
or unintended degradation of unrelated knowledge due to poor localization. In
this work, we propose GRIN: a modular and targeted framework for LLM
unlearning. GRIN introduces a novel gradient-ratio-based metric to identify
parameters most responsible for memorizing forget data. We then perform
selective noise injection into these parameters prior to fine-tuning, which
improves unlearning performance while maintaining model utility. Finally, we
propose new evaluation metrics tailored to the LLM setting and validate our
approach on standard benchmarks such as TOFU, WMDP, and SafePKU.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [233] [Functional Connectivity Graph Neural Networks](https://arxiv.org/abs/2508.05786)
*Yang Li,Luopeiwen Yi,Tananun Songdechakraiwut*

Main category: cs.NE

TL;DR: The paper introduces Functional Connectivity Graph Neural Networks, a brain-inspired framework leveraging global topological and structural features for graph-level classification, achieving better performance than existing methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve graph-level classification by capturing both local and global interactions, as seen in brain imaging where functional and structural connectivity are complementary.

Method: The method combines a functional connectivity block using persistent graph homology with structural features, creating a multi-modal graph neural network architecture.

Result: Experiments confirm consistent performance improvements over existing approaches for graph-level classification in various networks.

Conclusion: The proposed framework benefits from brain-inspired representations and proves effective for diverse graph-level classification tasks.

Abstract: Real-world networks often benefit from capturing both local and global
interactions. Inspired by multi-modal analysis in brain imaging, where
structural and functional connectivity offer complementary views of network
organization, we propose a graph neural network framework that generalizes this
approach to other domains. Our method introduces a functional connectivity
block based on persistent graph homology to capture global topological
features. Combined with structural information, this forms a multi-modal
architecture called Functional Connectivity Graph Neural Networks. Experiments
show consistent performance gains over existing methods, demonstrating the
value of brain-inspired representations for graph-level classification across
diverse networks.

</details>


### [234] [Identity Increases Stability in Neural Cellular Automata](https://arxiv.org/abs/2508.06389)
*James Stovold*

Main category: cs.NE

TL;DR: This paper introduces an 'identity' layer to Neural Cellular Automata (NCAs) to improve organism stability and foster emergent behaviors.


<details>
  <summary>Details</summary>
Motivation: NCA-grown organisms face challenges like boundary breakdowns and tumor-like growth. The study aims to stabilize these artificial organisms.

Method: An 'identity' layer with constraints is included during training to enhance organism stability.

Result: NCAs grown nearby exhibit enhanced stability, require only one identity value for this improvement, and show emergent movement patterns.

Conclusion: The method improves NCA-grown organism stability and opens avenues for studying cellular-level social interactions in artificial systems.

Abstract: Neural Cellular Automata (NCAs) offer a way to study the growth of
two-dimensional artificial organisms from a single seed cell. From the outset,
NCA-grown organisms have had issues with stability, their natural boundary
often breaking down and exhibiting tumour-like growth or failing to maintain
the expected shape. In this paper, we present a method for improving the
stability of NCA-grown organisms by introducing an 'identity' layer with simple
constraints during training.
  Results show that NCAs grown in close proximity are more stable compared with
the original NCA model. Moreover, only a single identity value is required to
achieve this increase in stability. We observe emergent movement from the
stable organisms, with increasing prevalence for models with multiple identity
values.
  This work lays the foundation for further study of the interaction between
NCA-grown organisms, paving the way for studying social interaction at a
cellular level in artificial organisms.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [235] [Hybrid Game Control Envelope Synthesis](https://arxiv.org/abs/2508.05997)
*Aditi Kabra,Jonathan Laurent,Stefan Mitsch,André Platzer*

Main category: cs.PL

TL;DR: This paper focuses on two-player hybrid games to address control problems in embedded systems such as cars and trains. It introduces a method to synthesize nondeterministic winning policies, enabling safety and flexibility in controlling these systems.


<details>
  <summary>Details</summary>
Motivation: The motivation is to provide the safest and most flexible control solutions for embedded systems by leveraging nondeterministic winning strategies in hybrid games.

Method: The paper uses subvalue maps to represent nondeterministic policies, verified and synthesized using differential game logic (dGL). A logical characterization checks if a policy is sound and winning.

Result: The study shows that the maximal subvalue map exists, allowing for the most permissive winning strategies. Algorithms based on the subvalue map characterization were developed and demonstrated using dGL models.

Conclusion: The findings provide a structured and verified approach to synthesize safe and flexible control policies for embedded systems, with promising results in diverse control challenges.

Abstract: Control problems for embedded systems like cars and trains can be modeled by
two-player hybrid games. Control envelopes, which are families of safe control
solutions, correspond to nondeterministic winning policies of hybrid games,
where each deterministic specialization of the policy is a control solution.
This paper synthesizes nondeterministic winning policies for hybrid games that
are as permissive as possible. It introduces subvalue maps, a compositional
representation of such policies that enables verification and synthesis along
the structure of the game. An inductive logical characterization in
differential game logic (dGL) checks whether a subvalue map induces a sound
control envelope which always induces a winning play. A policy is said to win
if it always achieves the desirable outcome when the player follows it, no
matter what actions the opponent plays. The maximal subvalue map, which allows
the most action options while still winning, is shown to exist and satisfy a
logical characterization. A family of algorithms for nondeterministic policy
synthesis can be obtained from the inductive subvalue map soundness
characterization. An implementation of these findings is evaluated on examples
that use the expressivity of dGL to model a range of diverse control
challenges.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [236] [GPU-Accelerated Barrier-Rate Guided MPPI Control for Tractor-Trailer Systems](https://arxiv.org/abs/2508.05773)
*Keyvan Majd,Hardik Parwana,Bardh Hoxha,Steven Hong,Hideki Okamoto,Georgios Fainekos*

Main category: cs.RO

TL;DR: The paper introduces a control method called Barrier-Rate guided Model Predictive Path Integral (BR-MPPI) for articulated vehicle navigation in cluttered environments, evaluated in a simulator for tractor-trailer parking scenarios.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of maneuvering articulated vehicles like tractor-trailers in cluttered spaces with pedestrians, requiring safe and robust trajectory planning.

Method: The method integrates Control Barrier Function (CBF) constraints into the MPPI framework, steering sampling toward safe and feasible trajectories while enhancing exploration strength.

Result: The proposed BR-MPPI method outperformed standard MPPI and collision-cost baseline in a simulated tractor-trailer parking task, maintaining high computational efficiency at over 100 Hz.

Conclusion: BR-MPPI is a robust and efficient control approach for articulated vehicle navigation in complex and constrained environments, demonstrating better safety and performance in collision avoidance.

Abstract: Articulated vehicles such as tractor-trailers, yard trucks, and similar
platforms must often reverse and maneuver in cluttered spaces where pedestrians
are present. We present how Barrier-Rate guided Model Predictive Path Integral
(BR-MPPI) control can solve navigation in such challenging environments.
BR-MPPI embeds Control Barrier Function (CBF) constraints directly into the
path-integral update. By steering the importance-sampling distribution toward
collision-free, dynamically feasible trajectories, BR-MPPI enhances the
exploration strength of MPPI and improves robustness of resulting trajectories.
The method is evaluated in the high-fidelity CarMaker simulator on a 12 [m]
tractor-trailer tasked with reverse and forward parking in a parking lot.
BR-MPPI computes control inputs in above 100 [Hz] on a single GPU (for
scenarios with eight obstacles) and maintains better parking clearance than a
standard MPPI baseline and an MPPI with collision cost baseline.

</details>


### [237] [Integrating Vision Foundation Models with Reinforcement Learning for Enhanced Object Interaction](https://arxiv.org/abs/2508.05838)
*Ahmad Farooq,Kamran Iqbal*

Main category: cs.RO

TL;DR: The paper combines vision foundation models like SAM and YOLOv5 with reinforcement learning (PPO) to improve robotic object interaction and navigation in simulated environments, achieving significant performance gains.


<details>
  <summary>Details</summary>
Motivation: To improve object interaction and navigation efficiency for reinforcement learning agents in simulated environments by leveraging advanced foundation models.

Method: The approach integrates the Segment Anything Model (SAM) and YOLOv5 with a Proximal Policy Optimization (PPO) reinforcement learning agent in the AI2-THOR simulation environment. Comprehensive experiments are conducted in four indoor kitchen settings.

Result: The combined method shows a 68% rise in average cumulative reward, a 52.5% improvement in object interaction success rate, and a 33% increase in navigation efficiency over a baseline approach.

Conclusion: Integrating foundation models with reinforcement learning enhances robotic perception and capability in simulated environments, offering a foundation for more sophisticated autonomous agents.

Abstract: This paper presents a novel approach that integrates vision foundation models
with reinforcement learning to enhance object interaction capabilities in
simulated environments. By combining the Segment Anything Model (SAM) and
YOLOv5 with a Proximal Policy Optimization (PPO) agent operating in the
AI2-THOR simulation environment, we enable the agent to perceive and interact
with objects more effectively. Our comprehensive experiments, conducted across
four diverse indoor kitchen settings, demonstrate significant improvements in
object interaction success rates and navigation efficiency compared to a
baseline agent without advanced perception. The results show a 68% increase in
average cumulative reward, a 52.5% improvement in object interaction success
rate, and a 33% increase in navigation efficiency. These findings highlight the
potential of integrating foundation models with reinforcement learning for
complex robotic tasks, paving the way for more sophisticated and capable
autonomous agents.

</details>


### [238] [Modular Vacuum-Based Fixturing System for Adaptive Disassembly Workspace Integration](https://arxiv.org/abs/2508.05936)
*Haohui Pan,Takuya Kiyokawa,Tomoki Ishikura,Shingo Hamada,Genichiro Matsuda,Kensuke Harada*

Main category: cs.RO

TL;DR: The paper presents a modular vacuum fixturing system using balloon-type soft grippers for improved disassembly of curved appliances.


<details>
  <summary>Details</summary>
Motivation: Traditional rigid fixtures struggle with the disassembly of complex and curved objects like small household appliances.

Method: A system combining modular balloon-type soft grippers and a stability-aware planning framework ensures effective handling and placement during screw-removal tasks.

Result: Experimental results show the proposed system achieves better placement stability and higher success rates compared to rigid fixtures.

Conclusion: The modular vacuum-based soft gripper system is an effective alternative for disassembling objects with complex geometries, outperforming traditional fixtures in stability and success rates.

Abstract: The disassembly of small household appliances poses significant challenges
due to their complex and curved geometries, which render traditional rigid
fixtures inadequate. In this paper, we propose a modular vacuum-based fixturing
system that leverages commercially available balloon-type soft grippers to
conform to arbitrarily shaped surfaces and provide stable support during
screw-removal tasks. To enable a reliable deployment of the system, we develop
a stability-aware planning framework that samples the bottom surface of the
target object, filters candidate contact points based on geometric continuity,
and evaluates support configurations using convex hull-based static stability
criteria. We compare the quality of object placement under different numbers
and configurations of balloon hands. In addition, real-world experiments were
conducted to compare the success rates of traditional rigid fixtures with our
proposed system. The results demonstrate that our method consistently achieves
higher success rates and superior placement stability during screw removal
tasks.

</details>


### [239] [Affordance-Guided Dual-Armed Disassembly Teleoperation for Mating Parts](https://arxiv.org/abs/2508.05937)
*Gen Sako,Takuya Kiyokawa,Kensuke Harada,Tomoki Ishikura,Naoya Miyaji,Genichiro Matsuda*

Main category: cs.RO

TL;DR: The paper introduces a teleoperation system for robotic disassembly of mating parts using intuitive visualization of grasp and disassembly directions, combined with a hybrid controller to enhance task success and precision.


<details>
  <summary>Details</summary>
Motivation: Robotic disassembly of mating parts is challenging due to structural occlusions, manipulation flexibility, and visibility constraints, motivating the development of a system for better interaction and task execution.

Method: The paper employs an affordance-guided system that visualizes grasp poses and disassembly directions in a virtual environment, coupled with a hybrid controller integrating position and impedance control for robotic teleoperation.

Result: Experiments in real-world scenarios validate the system, showing improved success rates in disassembly tasks and reduced positional deviations of objects involved.

Conclusion: The study demonstrates that combining affordance visualization with hybrid control enhances robotic disassembly performance by enabling intuitive human interaction and addressing structural complexities.

Abstract: Robotic non-destructive disassembly of mating parts remains challenging due
to the need for flexible manipulation and the limited visibility of internal
structures. This study presents an affordance-guided teleoperation system that
enables intuitive human demonstrations for dual-arm fix-and-disassemble tasks
for mating parts. The system visualizes feasible grasp poses and disassembly
directions in a virtual environment, both derived from the object's geometry,
to address occlusions and structural complexity. To prevent excessive position
tracking under load when following the affordance, we integrate a hybrid
controller that combines position and impedance control into the teleoperated
disassembly arm. Real-world experiments validate the effectiveness of the
proposed system, showing improved task success rates and reduced object pose
deviation.

</details>


### [240] [Latent Policy Barrier: Learning Robust Visuomotor Policies by Staying In-Distribution](https://arxiv.org/abs/2508.05941)
*Zhanyi Sun,Shuran Song*

Main category: cs.RO

TL;DR: Latent Policy Barrier (LPB) is introduced to improve robustness in visuomotor policy learning, addressing covariate shift by separating expert imitation and recovery from out-of-distribution states.


<details>
  <summary>Details</summary>
Motivation: Visuomotor policies trained with behavior cloning face challenges due to covariate shift, causing failure when slight deviations from expert trajectories accumulate.

Method: LPB uses latent embeddings of expert demonstrations as implicit barriers to differentiate safe and unsafe states. It decouples expert imitation and recovery into two modules: a diffusion policy trained on expert data, and a dynamics model trained on a mix of expert and suboptimal data, guiding inference towards in-distribution states.

Result: Simulated and real-world experiments demonstrated that LPB enhances policy robustness and reduces reliance on extensive human corrections while achieving efficient manipulation with limited expert data.

Conclusion: LPB is effective in mitigating covariate shift issues by creating a robust framework for visuomotor policy learning, driven by expert imitation and recovery mechanisms.

Abstract: Visuomotor policies trained via behavior cloning are vulnerable to covariate
shift, where small deviations from expert trajectories can compound into
failure. Common strategies to mitigate this issue involve expanding the
training distribution through human-in-the-loop corrections or synthetic data
augmentation. However, these approaches are often labor-intensive, rely on
strong task assumptions, or compromise the quality of imitation. We introduce
Latent Policy Barrier, a framework for robust visuomotor policy learning.
Inspired by Control Barrier Functions, LPB treats the latent embeddings of
expert demonstrations as an implicit barrier separating safe, in-distribution
states from unsafe, out-of-distribution (OOD) ones. Our approach decouples the
role of precise expert imitation and OOD recovery into two separate modules: a
base diffusion policy solely on expert data, and a dynamics model trained on
both expert and suboptimal policy rollout data. At inference time, the dynamics
model predicts future latent states and optimizes them to stay within the
expert distribution. Both simulated and real-world experiments show that LPB
improves both policy robustness and data efficiency, enabling reliable
manipulation from limited expert data and without additional human correction
or annotation.

</details>


### [241] [Social and Telepresence Robots for Accessibility and Inclusion in Small Museums](https://arxiv.org/abs/2508.05946)
*Nello Balossino,Rossana Damiano,Cristina Gena,Alberto Lillo,Anna Maria Marras,Claudio Mattutino,Antonio Pizzo,Alessia Prin,Fabiana Vernero*

Main category: cs.RO

TL;DR: The ROBSO-PM project aims to enhance the accessibility of small museums using social and telepresence robots, focusing on guiding inclusive visits and enabling remote access.


<details>
  <summary>Details</summary>
Motivation: Many museums face accessibility challenges, particularly in lesser-known areas with barriers tied to perception, culture, and cognition.

Method: The project applies robots as museum guides and telepresence tools, emphasizing inclusivity for foreign and disabled visitors, backed by research on storytelling, empathy, and role definitions.

Result: Case studies include the use of robots at three specific museums, showcasing improved access for visitors with mobility limitations and inclusive experiences.

Conclusion: Robots can effectively address accessibility issues in smaller museums, fostering inclusivity and providing remote access opportunities.

Abstract: There are still many museums that present accessibility barriers,
particularly regarding perceptual, cultural, and cognitive aspects. This is
especially evident in low-density population areas. The aim of the ROBSO-PM
project is to improve the accessibility of small museums through the use of
social robots and social telepresence robots, focusing on three museums as case
studies: the Museum of the Holy Shroud in Turin, a small but globally known
institution, and two lesser known mountain museums: the Museum of the Champlas
du Col Carnival and the Pragelato Museum of Alpine Peoples' Costumes and
Traditions. The project explores two main applications for robots: as guides
supporting inclusive visits for foreign or disabled visitors, and as
telepresence tools allowing people with limited mobility to access museums
remotely. From a research perspective, key topics include storytelling, robot
personality, empathy, personalization, and, in the case of telepresence,
collaboration between the robot and the person, with clearly defined roles and
autonomy.

</details>


### [242] [Dynamical Trajectory Planning of Disturbance Consciousness for Air-Land Bimodal Unmanned Aerial Vehicles](https://arxiv.org/abs/2508.05972)
*Shaoting Liu,Zhou Liu*

Main category: cs.RO

TL;DR: This paper proposes a planning framework for air-land vehicles to improve trajectory robustness by estimating and adapting to environmental disturbances.


<details>
  <summary>Details</summary>
Motivation: Enhance trajectory robustness of air-land bimodal vehicles under environmental disturbances, utilizing the advantages of aerial and ground-based locomotion.

Method: Developed a disturbance-aware planning framework that integrates real-time disturbance estimation, dynamic safety boundary adjustments, and a dynamics model for adaptive trajectory optimization.

Result: The method exhibited improved tracking accuracy, task efficiency, and energy performance in real-world tests and benchmark comparisons.

Conclusion: The proposed approach ensures feasible, adaptive, and robust motion planning for air-land vehicles across diverse environments.

Abstract: Air-land bimodal vehicles provide a promising solution for navigating complex
environments by combining the flexibility of aerial locomotion with the energy
efficiency of ground mobility. To enhance the robustness of trajectory planning
under environmental disturbances, this paper presents a disturbance-aware
planning framework that incorporates real-time disturbance estimation into both
path searching and trajectory optimization. A key component of the framework is
a disturbance-adaptive safety boundary adjustment mechanism, which dynamically
modifies the vehicle's feasible dynamic boundaries based on estimated
disturbances to ensure trajectory feasibility. Leveraging the dynamics model of
the bimodal vehicle, the proposed approach achieves adaptive and reliable
motion planning across different terrains and operating conditions. A series of
real-world experiments and benchmark comparisons on a custom-built platform
validate the effectiveness and robustness of the method, demonstrating
improvements in tracking accuracy, task efficiency, and energy performance
under both ground and aerial disturbances.

</details>


### [243] [ReNiL: Relative Neural Inertial Locator with Any-Scale Bayesian Inference](https://arxiv.org/abs/2508.06053)
*Kaixuan Wu,Yuanzhuo Xu,Zejun Zhang,Weiping Zhu,Steve Drew,Xiaoguang Niu*

Main category: cs.RO

TL;DR: The paper introduces ReNiL, a Bayesian deep learning framework for accurate and uncertainty-aware pedestrian inertial localization, outperforming existing methods with reduced computation and improved practicality.


<details>
  <summary>Details</summary>
Motivation: Pedestrian inertial localization provides infrastructure-free positioning crucial for mobile and IoT services, yet existing approaches struggle with adaptability to diverse motion cadences, scales, and consistent uncertainty.

Method: ReNiL uses Inertial Positioning Demand Points (IPDPs) for waypoint-based motion estimation, integrates a motion-aware orientation filter, and employs an Any-Scale Laplace Estimator (ASLE) to model displacement with homogeneous Euclidean uncertainty.

Result: ReNiL delivers state-of-the-art displacement accuracy and consistent uncertainty on datasets like RoNIN-ds and WUDataset, outperforming other methods (TLIO, CTIN, iMoT, RoNIN variants) while reducing computational cost.

Conclusion: ReNiL offers a robust and scalable solution for pedestrian localization in mobile and IoT contexts, supporting uncertainty-aware and efficient positioning systems.

Abstract: Pedestrian inertial localization is key for mobile and IoT services because
it provides infrastructure-free positioning. Yet most learning-based methods
depend on fixed sliding-window integration, struggle to adapt to diverse motion
scales and cadences, and yield inconsistent uncertainty, limiting real-world
use. We present ReNiL, a Bayesian deep-learning framework for accurate,
efficient, and uncertainty-aware pedestrian localization. ReNiL introduces
Inertial Positioning Demand Points (IPDPs) to estimate motion at contextually
meaningful waypoints instead of dense tracking, and supports inference on IMU
sequences at any scale so cadence can match application needs. It couples a
motion-aware orientation filter with an Any-Scale Laplace Estimator (ASLE), a
dual-task network that blends patch-based self-supervision with Bayesian
regression. By modeling displacements with a Laplace distribution, ReNiL
provides homogeneous Euclidean uncertainty that integrates cleanly with other
sensors. A Bayesian inference chain links successive IPDPs into consistent
trajectories. On RoNIN-ds and a new WUDataset covering indoor and outdoor
motion from 28 participants, ReNiL achieves state-of-the-art displacement
accuracy and uncertainty consistency, outperforming TLIO, CTIN, iMoT, and RoNIN
variants while reducing computation. Application studies further show
robustness and practicality for mobile and IoT localization, making ReNiL a
scalable, uncertainty-aware foundation for next-generation positioning.

</details>


### [244] [Incremental Language Understanding for Online Motion Planning of Robot Manipulators](https://arxiv.org/abs/2508.06095)
*Mitchell Abrams,Thies Oelerich,Christian Hartl-Nesic,Andreas Kugi,Matthias Scheutz*

Main category: cs.RO

TL;DR: The paper introduces an incremental parser for human-robot interaction, enabling real-time motion adaptation to evolving linguistic inputs.


<details>
  <summary>Details</summary>
Motivation: Existing systems assume complete instructions, causing inefficiencies when speech corrections or clarifications are made, limiting natural human-robot collaboration.

Method: The authors integrate a reasoning-based incremental parser with online motion planning within a cognitive architecture, maintaining multiple candidate parses and adapting dynamically to linguistic input.

Result: The framework demonstrates successful online adaptations for changing constraints and task objectives in real-world scenarios, outperforming traditional methods.

Conclusion: Integrating incremental language understanding with motion planning enhances fluid interactions, improving real-time adaptability in human-robot systems.

Abstract: Human-robot interaction requires robots to process language incrementally,
adapting their actions in real-time based on evolving speech input. Existing
approaches to language-guided robot motion planning typically assume fully
specified instructions, resulting in inefficient stop-and-replan behavior when
corrections or clarifications occur. In this paper, we introduce a novel
reasoning-based incremental parser which integrates an online motion planning
algorithm within the cognitive architecture. Our approach enables continuous
adaptation to dynamic linguistic input, allowing robots to update motion plans
without restarting execution. The incremental parser maintains multiple
candidate parses, leveraging reasoning mechanisms to resolve ambiguities and
revise interpretations when needed. By combining symbolic reasoning with online
motion planning, our system achieves greater flexibility in handling speech
corrections and dynamically changing constraints. We evaluate our framework in
real-world human-robot interaction scenarios, demonstrating online adaptions of
goal poses, constraints, or task objectives. Our results highlight the
advantages of integrating incremental language understanding with real-time
motion planning for natural and fluid human-robot collaboration. The
experiments are demonstrated in the accompanying video at
www.acin.tuwien.ac.at/42d5.

</details>


### [245] [Bounding Distributional Shifts in World Modeling through Novelty Detection](https://arxiv.org/abs/2508.06096)
*Eric Jing,Abdeslam Boularias*

Main category: cs.RO

TL;DR: The paper proposes a novel method to improve the robustness of model-based planning algorithms using a variational autoencoder (VAE) as a novelty detector, evaluating its effectiveness in simulated robotic environments.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for visual world models often struggle with sensitivity to training quality, requiring complete action and state space coverage to avoid issues during inference.

Method: The authors use a variational autoencoder (VAE) as a novelty detector to ensure that action trajectories remain within the training data distribution and integrate this into a model-predictive control policy based on the DINO-WM architecture.

Result: In simulated robotic environments, the proposed method demonstrated superior data efficiency compared to state-of-the-art solutions.

Conclusion: Incorporating a VAE-based novelty detection mechanism into the planning loop enhances robustness and data efficiency in model-based planning algorithms.

Abstract: Recent work on visual world models shows significant promise in latent state
dynamics obtained from pre-trained image backbones. However, most of the
current approaches are sensitive to training quality, requiring near-complete
coverage of the action and state space during training to prevent divergence
during inference. To make a model-based planning algorithm more robust to the
quality of the learned world model, we propose in this work to use a
variational autoencoder as a novelty detector to ensure that proposed action
trajectories during planning do not cause the learned model to deviate from the
training data distribution. To evaluate the effectiveness of this approach, a
series of experiments in challenging simulated robot environments was carried
out, with the proposed method incorporated into a model-predictive control
policy loop extending the DINO-WM architecture. The results clearly show that
the proposed method improves over state-of-the-art solutions in terms of data
efficiency.

</details>


### [246] [Beyond Constant Parameters: Hyper Prediction Models and HyperMPC](https://arxiv.org/abs/2508.06181)
*Jan Węgrzynowski,Piotr Kicki,Grzegorz Czechmanowski,Maciej Krupka,Krzysztof Walas*

Main category: cs.RO

TL;DR: The paper presents HyperPM, a novel model improving dynamics prediction in Model Predictive Control (MPC) by introducing time-dependent parameters learned through neural networks, yielding higher accuracy and outperforming existing methods in robotic systems like autonomous racing.


<details>
  <summary>Details</summary>
Motivation: Improving dynamics prediction in MPC by addressing computational complexity and limited state representation challenges of existing models.

Method: Developing the Hyper Prediction Model (HyperPM) that projects unmodeled dynamics onto a time-dependent model with neural network-learned evolution over MPC horizons.

Result: HyperPM reduces long-horizon prediction errors and consistently outperforms other techniques when integrated into MPC across various systems, including real-world autonomous racing.

Conclusion: HyperPM enhances MPC dynamics prediction efficiency and accuracy, demonstrating robustness and superiority over state-of-the-art approaches.

Abstract: Model Predictive Control (MPC) is among the most widely adopted and reliable
methods for robot control, relying critically on an accurate dynamics model.
However, existing dynamics models used in the gradient-based MPC are limited by
computational complexity and state representation. To address this limitation,
we propose the Hyper Prediction Model (HyperPM) - a novel approach in which we
project the unmodeled dynamics onto a time-dependent dynamics model. This
time-dependency is captured through time-varying model parameters, whose
evolution over the MPC prediction horizon is learned using a neural network.
Such formulation preserves the computational efficiency and robustness of the
base model while equipping it with the capacity to anticipate previously
unmodeled phenomena. We evaluated the proposed approach on several challenging
systems, including real-world F1TENTH autonomous racing, and demonstrated that
it significantly reduces long-horizon prediction errors. Moreover, when
integrated within the MPC framework (HyperMPC), our method consistently
outperforms existing state-of-the-art techniques.

</details>


### [247] [Affordance-R1: Reinforcement Learning for Generalizable Affordance Reasoning in Multimodal Large Language Model](https://arxiv.org/abs/2508.06206)
*Hanqing Wang,Shaoyang Wang,Yiming Zhong,Zemin Yang,Jiamin Wang,Zhiqing Cui,Jiahao Yuan,Yifan Han,Mingyu Liu,Yuexin Ma*

Main category: cs.RO

TL;DR: This paper introduces Affordance-R1, a novel framework integrating reinforcement learning and reasoning for robotic affordance prediction.


<details>
  <summary>Details</summary>
Motivation: Address limitations in existing models that neglect shared affordances among objects and lack reasoning abilities, which restrict OOD generalization.

Method: Proposed Affordance-R1 framework uses Group Relative Policy Optimization (GRPO) within reinforcement learning and introduces a reasoning-focused dataset named ReasonAff.

Result: Affordance-R1 demonstrates robust zero-shot generalization and emergent test-time reasoning, outperforming established methods in open-world generalization tasks.

Conclusion: Affordance-R1 successfully integrates GRPO-based RL and reasoning, marking an advancement in affordance grounding with improved generalization and reasoning.

Abstract: Affordance grounding focuses on predicting the specific regions of objects
that are associated with the actions to be performed by robots. It plays a
vital role in the fields of human-robot interaction, human-object interaction,
embodied manipulation, and embodied perception. Existing models often neglect
the affordance shared among different objects because they lack the
Chain-of-Thought(CoT) reasoning abilities, limiting their out-of-domain (OOD)
generalization and explicit reasoning capabilities. To address these
challenges, we propose Affordance-R1, the first unified affordance grounding
framework that integrates cognitive CoT guided Group Relative Policy
Optimization (GRPO) within a reinforcement learning paradigm. Specifically, we
designed a sophisticated affordance function, which contains format,
perception, and cognition rewards to effectively guide optimization directions.
Furthermore, we constructed a high-quality affordance-centric reasoning
dataset, ReasonAff, to support training. Trained exclusively via reinforcement
learning with GRPO and without explicit reasoning data, Affordance-R1 achieves
robust zero-shot generalization and exhibits emergent test-time reasoning
capabilities. Comprehensive experiments demonstrate that our model outperforms
well-established methods and exhibits open-world generalization. To the best of
our knowledge, Affordance-R1 is the first to integrate GRPO-based RL with
reasoning into affordance reasoning. The code of our method and our dataset is
released on https://github.com/hq-King/Affordance-R1.

</details>


### [248] [Computer Vision-based Adaptive Control for Back Exoskeleton Performance Optimization](https://arxiv.org/abs/2508.06207)
*Andrea Dal Prete,Seyram Ofori,Chan Yon Sin,Ashwin Narayan,Francesco Braghin,Marta Gandolla,Haoyong Yu*

Main category: cs.RO

TL;DR: The study addresses optimizing support strategies and adaptive control for back exoskeletons, leveraging user-centered metrics and real-time payload estimation through a vision-based pipeline.


<details>
  <summary>Details</summary>
Motivation: To enhance the effectiveness of back exoskeletons in reducing musculoskeletal strain by addressing modulation and control challenges.

Method: The authors proposed an optimization space based on muscle activity, discomfort, and user preference, conducted experiments to define optimal regions, and developed a vision-based adaptive control pipeline for real-time payload estimation.

Result: The adaptive control pipeline achieved an accuracy above 80%, reduced peak back muscle activation by up to 23%, and improved user-centric metrics versus static control.

Conclusion: The intelligent adaptive framework demonstrates substantial benefits in reducing muscle strain, validating its potential for industrial use in context-aware exoskeletons.

Abstract: Back exoskeletons can reduce musculoskeletal strain, but their effectiveness
depends on support modulation and adaptive control. This study addresses two
challenges: defining optimal support strategies and developing adaptive control
based on payload estimation. We introduce an optimization space based on muscle
activity reduction, perceived discomfort, and user preference, constructing
functions to identify optimal strategies. Experiments with 12 subjects revealed
optimal operating regions, highlighting the need for dynamic modulation. Based
on these insights, we developed a vision-based adaptive control pipeline that
estimates payloads in real-time by enhancing exoskeleton contextual
understanding, minimising latency and enabling support adaptation within the
defined optimisation space. Validation with 12 more subjects showed over 80%
accuracy and improvements across all metrics. Compared to static control,
adaptive modulation reduced peak back muscle activation by up to 23% while
preserving user preference and minimising discomfort. These findings validate
the proposed framework and highlight the potential of intelligent,
context-aware control in industrial exoskeletons.

</details>


### [249] [REBot: Reflexive Evasion Robot for Instantaneous Dynamic Obstacle Avoidance](https://arxiv.org/abs/2508.06229)
*Zihao Xu,Ce Hao,Chunzheng Wang,Kuankuan Sima,Fan Shi,Jin Song Dong*

Main category: cs.RO

TL;DR: The paper introduces a control framework (REBot) for dynamic obstacle avoidance (DOA) in quadrupedal robots, focusing on reflexive evasion to handle fast-moving obstacles.


<details>
  <summary>Details</summary>
Motivation: To address the shortcomings of existing navigation-based trajectory replanning approaches, which fail when faced with rapidly approaching obstacles, and to equip quadrupedal robots with low-latency reflexive evasive capabilities.

Method: The authors developed REBot, a control framework based on a finite-state machine that integrates an avoidance policy and a recovery policy. It features tailored learning curricula, regularization, and adaptive rewards to enhance performance in instantaneous DOA tasks.

Result: REBot demonstrates significant improvements in obstacle avoidance success rates, energy efficiency, and robustness in both simulations and real-world experiments involving fast-moving obstacles.

Conclusion: The proposed framework, REBot, equips quadrupedal robots with real-time reflexive evasion capabilities, addressing limitations in existing approaches and showing robust and efficient performance in dynamic environments.

Abstract: Dynamic obstacle avoidance (DOA) is critical for quadrupedal robots operating
in environments with moving obstacles or humans. Existing approaches typically
rely on navigation-based trajectory replanning, which assumes sufficient
reaction time and leading to fails when obstacles approach rapidly. In such
scenarios, quadrupedal robots require reflexive evasion capabilities to perform
instantaneous, low-latency maneuvers. This paper introduces Reflexive Evasion
Robot (REBot), a control framework that enables quadrupedal robots to achieve
real-time reflexive obstacle avoidance. REBot integrates an avoidance policy
and a recovery policy within a finite-state machine. With carefully designed
learning curricula and by incorporating regularization and adaptive rewards,
REBot achieves robust evasion and rapid stabilization in instantaneous DOA
tasks. We validate REBot through extensive simulations and real-world
experiments, demonstrating notable improvements in avoidance success rates,
energy efficiency, and robustness to fast-moving obstacles. Videos and appendix
are available on https://rebot-2025.github.io/.

</details>


### [250] [ADPro: a Test-time Adaptive Diffusion Policy for Robot Manipulation via Manifold and Initial Noise Constraints](https://arxiv.org/abs/2508.06266)
*Zezeng Li,Rui Yang,Ruochen Chen,ZhongXuan Luo,Liming Chen*

Main category: cs.RO

TL;DR: The paper introduces the Adaptive Diffusion Policy (ADP), a diffusion-based robot control approach that incorporates geometric and analytical constraints to improve performance and generalization without retraining.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing diffusion policies for robot manipulation, which overlook geometric and control constraints, and often result in slow convergence and suboptimal generalization.

Method: The proposed ADP introduces two test-time adaptation biases: (1) a geometric manifold constraint to align denoising updates with task-relevant subspaces and guidance along the manipulation manifold; (2) an analytically guided initialization to provide structured noisy action proposals based on gripper-target registration. ADP integrates these into pre-trained diffusion policies.

Result: ADP, demonstrated through its implementation called ADPro, improves success rates, generalization, and sampling efficiency in benchmarks and real-world settings. It achieves up to 25% faster execution and a 9% improvement over strong diffusion baselines.

Conclusion: ADP effectively enhances diffusion policy performance by incorporating inductive biases at test time, enabling better task-specific adaptation, improved generalization, and faster convergence without the need for retraining.

Abstract: Diffusion policies have recently emerged as a powerful class of visuomotor
controllers for robot manipulation, offering stable training and expressive
multi-modal action modeling. However, existing approaches typically treat
action generation as an unconstrained denoising process, ignoring valuable a
priori knowledge about geometry and control structure. In this work, we propose
the Adaptive Diffusion Policy (ADP), a test-time adaptation method that
introduces two key inductive biases into the diffusion. First, we embed a
geometric manifold constraint that aligns denoising updates with task-relevant
subspaces, leveraging the fact that the relative pose between the end-effector
and target scene provides a natural gradient direction, and guiding denoising
along the geodesic path of the manipulation manifold. Then, to reduce
unnecessary exploration and accelerate convergence, we propose an analytically
guided initialization: rather than sampling from an uninformative prior, we
compute a rough registration between the gripper and target scenes to propose a
structured initial noisy action. ADP is compatible with pre-trained diffusion
policies and requires no retraining, enabling test-time adaptation that tailors
the policy to specific tasks, thereby enhancing generalization across novel
tasks and environments. Experiments on RLBench, CALVIN, and real-world dataset
show that ADPro, an implementation of ADP, improves success rates,
generalization, and sampling efficiency, achieving up to 25% faster execution
and 9% points over strong diffusion baselines.

</details>


### [251] [EcBot: Data-Driven Energy Consumption Open-Source MATLAB Library for Manipulators](https://arxiv.org/abs/2508.06276)
*Juan Heredia,Christian Schlette,Mikkel Baun Kjærgaard*

Main category: cs.RO

TL;DR: This paper introduces a Matlab-based library for generating electrical power estimation models for manipulators, validated on lightweight robots with improved accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitations in existing models for estimating electrical power in manipulators, particularly their focus on traditional robots and lack of accuracy.

Method: The authors developed a Matlab-based, open-source library that utilizes real operational data and specific inputs such as Denavit-Hartenberg parameters, link masses, and centers of mass to generate data-driven electrical power estimation models.

Result: The methodology was tested on four lightweight robots from Universal Robots, Franka Emika, and Kinova, yielding RMSE between 1.42 W and 2.80 W for training data and between 1.45 W and 5.25 W for testing data.

Conclusion: The study demonstrates that the developed library is effective and accurate for creating electrical power estimation models for lightweight manipulators, offering higher precision compared to previous models.

Abstract: Existing literature proposes models for estimating the electrical power of
manipulators, yet two primary limitations prevail. First, most models are
predominantly tested using traditional industrial robots. Second, these models
often lack accuracy. To address these issues, we introduce an open source
Matlab-based library designed to automatically generate \ac{ec} models for
manipulators. The necessary inputs for the library are Denavit-Hartenberg
parameters, link masses, and centers of mass. Additionally, our model is
data-driven and requires real operational data, including joint positions,
velocities, accelerations, electrical power, and corresponding timestamps. We
validated our methodology by testing on four lightweight robots sourced from
three distinct manufacturers: Universal Robots, Franka Emika, and Kinova. The
model underwent testing, and the results demonstrated an RMSE ranging from 1.42
W to 2.80 W for the training dataset and from 1.45 W to 5.25 W for the testing
dataset.

</details>


### [252] [Mitigating Undesired Conditions in Flexible Production with Product-Process-Resource Asset Knowledge Graphs](https://arxiv.org/abs/2508.06278)
*Petr Novak,Stefan Biffl,Marek Obitko,Petr Kadera*

Main category: cs.RO

TL;DR: This paper introduces PPR-AKG, a semantic model for addressing undesired conditions in flexible CPPS using OWL ontology and LLMs.


<details>
  <summary>Details</summary>
Motivation: Address the challenges in undesired condition analysis and quality assurance in the flexible and dynamic Industry 4.0 CPPS.

Method: The authors developed the Product-Process-Resource Asset Knowledge Graph (PPR-AKG) built on the Product-Process-Resource model, integrated it with OWL ontology for error handling, and combined it with LLM-based chatbot interfaces.

Result: Evaluation in the context of electric vehicle battery remanufacturing confirmed the model's utility in resource allocation and mitigating undesired conditions.

Conclusion: PPR-AKG provides an effective framework for analyzing and mitigating undesired conditions in CPPS, offering an approachable and interactive solution for industry stakeholders.

Abstract: Contemporary industrial cyber-physical production systems (CPPS) composed of
robotic workcells face significant challenges in the analysis of undesired
conditions due to the flexibility of Industry 4.0 that disrupts traditional
quality assurance mechanisms. This paper presents a novel industry-oriented
semantic model called Product-Process-Resource Asset Knowledge Graph (PPR-AKG),
which is designed to analyze and mitigate undesired conditions in flexible
CPPS. Built on top of the well-proven Product-Process-Resource (PPR) model
originating from ISA-95 and VDI-3682, a comprehensive OWL ontology addresses
shortcomings of conventional model-driven engineering for CPPS, particularly
inadequate undesired condition and error handling representation. The
integration of semantic technologies with large language models (LLMs) provides
intuitive interfaces for factory operators, production planners, and engineers
to interact with the entire model using natural language. Evaluation with the
use case addressing electric vehicle battery remanufacturing demonstrates that
the PPR-AKG approach efficiently supports resource allocation based on
explicitly represented capabilities as well as identification and mitigation of
undesired conditions in production. The key contributions include (1) a
holistic PPR-AKG model capturing multi-dimensional production knowledge, and
(2) the useful combination of the PPR-AKG with LLM-based chatbots for human
interaction.

</details>


### [253] [Situationally-aware Path Planning Exploiting 3D Scene Graphs](https://arxiv.org/abs/2508.06283)
*Saad Ejaz,Marco Giberna,Muhammad Shaheer,Jose Andres Millan-Romera,Ali Tourani,Paul Kremer,Holger Voos,Jose Luis Sanchez-Lopez*

Main category: cs.RO

TL;DR: The paper introduces S-Path, a path planning framework leveraging 3D Scene Graphs for efficient and interpretable indoor navigation.


<details>
  <summary>Details</summary>
Motivation: Current path planning methods underutilize the combination of metric and semantic information inherent in 3D Scene Graphs.

Method: The S-Path method uses a two-stage process: semantic graph search for human-level path understanding followed by problem decomposition into smaller sub-problems solved in parallel. It includes a replanning mechanism to optimize paths when infeasibility arises.

Result: S-Path demonstrated a 5.7x reduction in planning time while maintaining path optimality comparable to classical methods, outperforming them in complex scenarios.

Conclusion: S-Path is proven to be an efficient and interpretable path planning technique for environments structured by 3D Scene Graphs.

Abstract: 3D Scene Graphs integrate both metric and semantic information, yet their
structure remains underutilized for improving path planning efficiency and
interpretability. In this work, we present S-Path, a situationally-aware path
planner that leverages the metric-semantic structure of indoor 3D Scene Graphs
to significantly enhance planning efficiency. S-Path follows a two-stage
process: it first performs a search over a semantic graph derived from the
scene graph to yield a human-understandable high-level path. This also
identifies relevant regions for planning, which later allows the decomposition
of the problem into smaller, independent subproblems that can be solved in
parallel. We also introduce a replanning mechanism that, in the event of an
infeasible path, reuses information from previously solved subproblems to
update semantic heuristics and prioritize reuse to further improve the
efficiency of future planning attempts. Extensive experiments on both
real-world and simulated environments show that S-Path achieves average
reductions of 5.7x in planning time while maintaining comparable path
optimality to classical sampling-based planners and surpassing them in complex
scenarios, making it an efficient and interpretable path planner for
environments represented by indoor 3D Scene Graphs.

</details>


### [254] [Real-Time 3D Vision-Language Embedding Mapping](https://arxiv.org/abs/2508.06291)
*Christian Rauch,Björn Ellensohn,Linus Nwankwo,Vedant Dave,Elmar Rueckert*

Main category: cs.RO

TL;DR: The paper proposes a method to integrate 2D embeddings into metric-accurate 3D representations in real-time, enhancing robotic applications like object localization using natural language.


<details>
  <summary>Details</summary>
Motivation: To provide a task-agnostic metric-accurate 3D semantic representation for robotics, capable of object localization in real-time using natural language.

Method: Combining local embedding masking and confidence-weighted 3D integration to ensure distinct 2D embeddings and reliable 3D embeddings.

Result: Achieved more accurate object localization and improved runtime performance, validated on diverse real-world sequences.

Conclusion: The approach facilitates versatile and interactive robotic tasks using raw image data, enabling efficient real-time object localization and manipulation.

Abstract: A metric-accurate semantic 3D representation is essential for many robotic
tasks. This work proposes a simple, yet powerful, way to integrate the 2D
embeddings of a Vision-Language Model in a metric-accurate 3D representation at
real-time. We combine a local embedding masking strategy, for a more distinct
embedding distribution, with a confidence-weighted 3D integration for more
reliable 3D embeddings. The resulting metric-accurate embedding representation
is task-agnostic and can represent semantic concepts on a global multi-room, as
well as on a local object-level. This enables a variety of interactive robotic
applications that require the localisation of objects-of-interest via natural
language. We evaluate our approach on a variety of real-world sequences and
demonstrate that these strategies achieve a more accurate object-of-interest
localisation while improving the runtime performance in order to meet our
real-time constraints. We further demonstrate the versatility of our approach
in a variety of interactive handheld, mobile robotics and manipulation tasks,
requiring only raw image data.

</details>


### [255] [Evaluating Robot Program Performance with Power Consumption Driven Metrics in Lightweight Industrial Robots](https://arxiv.org/abs/2508.06295)
*Juan Heredia,Emil Stubbe Kolvig-Raun,Sune Lundo Sorensen,Mikkel Baun Kjaergaard*

Main category: cs.RO

TL;DR: This paper introduces a new method for evaluating industrial robot code performance using electrical power profiles rather than traditional CPU metrics.


<details>
  <summary>Details</summary>
Motivation: Energy efficiency and physical reliability of robots during task execution is an overlooked but critical aspect, prompting the need for alternative evaluation frameworks beyond CPU metrics.

Method: The framework is built on normalized metrics such as energy utilization coefficient, energy conversion metric, reliability coefficient, and robot wear metric, supported by an experimental case study using a UR5e robot.

Result: The metrics effectively differentiate and categorize robot programs based on power utilization and task execution reliability, showing strengths and weaknesses of diverse coding strategies.

Conclusion: Optimizing robot code performance via this embodiment-centric framework enhances energy efficiency, reliability, and aligns with industrial goals of sustainability and cost reduction.

Abstract: The code performance of industrial robots is typically analyzed through CPU
metrics, which overlook the physical impact of code on robot behavior. This
study introduces a novel framework for assessing robot program performance from
an embodiment perspective by analyzing the robot's electrical power profile.
Our approach diverges from conventional CPU based evaluations and instead
leverages a suite of normalized metrics, namely, the energy utilization
coefficient, the energy conversion metric, and the reliability coefficient, to
capture how efficiently and reliably energy is used during task execution.
Complementing these metrics, the established robot wear metric provides further
insight into long term reliability. Our approach is demonstrated through an
experimental case study in machine tending, comparing four programs with
diverse strategies using a UR5e robot. The proposed metrics directly compare
and categorize different robot programs, regardless of the specific task, by
linking code performance to its physical manifestation through power
consumption patterns. Our results reveal the strengths and weaknesses of each
strategy, offering actionable insights for optimizing robot programming
practices. Enhancing energy efficiency and reliability through this embodiment
centric approach not only improves individual robot performance but also
supports broader industrial objectives such as sustainable manufacturing and
cost reduction.

</details>


### [256] [Surrogate-Enhanced Modeling and Adaptive Modular Control of All-Electric Heavy-Duty Robotic Manipulators](https://arxiv.org/abs/2508.06313)
*Amir Hossein Barjini,Mohammad Bahari,Mahdi Hejrati,Jouni Mattila*

Main category: cs.RO

TL;DR: The paper introduces an advanced framework integrating neural network-enhanced actuator models with hierarchical control mechanisms for precise and stable operation of all-electric heavy-duty robotic manipulators.


<details>
  <summary>Details</summary>
Motivation: To address the need for precise, stable, and modular control in next-generation heavy-duty robotic manipulators driven by electromechanical actuators.

Method: The authors developed a surrogate-enhanced electromechanical actuator model using neural networks on a testbed, integrating it into an extended virtual decomposition control architecture with an adaptation law and stability analysis.

Result: Multi-domain simulations achieved highly accurate tracking, while experiments validated the proposed system showing robustness under realistic load conditions.

Conclusion: The research successfully demonstrates that neural network-enhanced actuator models embedded in specialized control frameworks can achieve modular, real-time, and reliable robotic manipulator control.

Abstract: This paper presents a unified system-level modeling and control framework for
an all-electric heavy-duty robotic manipulator (HDRM) driven by
electromechanical linear actuators (EMLAs). A surrogate-enhanced actuator
model, combining integrated electromechanical dynamics with a neural network
trained on a dedicated testbed, is integrated into an extended virtual
decomposition control (VDC) architecture augmented by a natural adaptation law.
The derived analytical HDRM model supports a hierarchical control structure
that seamlessly maps high-level force and velocity objectives to real-time
actuator commands, accompanied by a Lyapunov-based stability proof. In
multi-domain simulations of both cubic and a custom planar triangular
trajectory, the proposed adaptive modular controller achieves sub-centimeter
Cartesian tracking accuracy. Experimental validation of the same 1-DoF platform
under realistic load emulation confirms the efficacy of the proposed control
strategy. These findings demonstrate that a surrogate-enhanced EMLA model
embedded in the VDC approach can enable modular, real-time control of an
all-electric HDRM, supporting its deployment in next-generation mobile working
machines.

</details>


### [257] [Towards Balanced Behavior Cloning from Imbalanced Datasets](https://arxiv.org/abs/2508.06319)
*Sagar Parekh,Heramb Nemlekar,Dylan P. Losey*

Main category: cs.RO

TL;DR: The paper investigates learning from imbalanced human demonstration datasets in imitation learning and proposes solutions for rebalancing these datasets.


<details>
  <summary>Details</summary>
Motivation: Address the issue of imbalanced datasets in human demonstrations, which causes learning algorithms to disproportionately emphasize overrepresented behaviors, undermining the diversity and complexity of human-provided demonstrations.

Method: Formal proof of imbalanced data's effects on policies, development of autonomous dataset reweighting techniques, and introduction of a novel meta-gradient rebalancing algorithm.

Result: Experimental findings show that rebalancing datasets improves downstream imitation learning performance, providing benefit without needing additional data collection.

Conclusion: Dataset rebalancing is essential to overcome dataset imbalances, and the proposed methods (including the new meta-gradient algorithm) enhance general imitation learning performance, with tradeoffs highlighted to guide researchers on method choice.

Abstract: Robots should be able to learn complex behaviors from human demonstrations.
In practice, these human-provided datasets are inevitably imbalanced: i.e., the
human demonstrates some subtasks more frequently than others. State-of-the-art
methods default to treating each element of the human's dataset as equally
important. So if -- for instance -- the majority of the human's data focuses on
reaching a goal, and only a few state-action pairs move to avoid an obstacle,
the learning algorithm will place greater emphasis on goal reaching. More
generally, misalignment between the relative amounts of data and the importance
of that data causes fundamental problems for imitation learning approaches. In
this paper we analyze and develop learning methods that automatically account
for mixed datasets. We formally prove that imbalanced data leads to imbalanced
policies when each state-action pair is weighted equally; these policies
emulate the most represented behaviors, and not the human's complex, multi-task
demonstrations. We next explore algorithms that rebalance offline datasets
(i.e., reweight the importance of different state-action pairs) without human
oversight. Reweighting the dataset can enhance the overall policy performance.
However, there is no free lunch: each method for autonomously rebalancing
brings its own pros and cons. We formulate these advantages and disadvantages,
helping other researchers identify when each type of approach is most
appropriate. We conclude by introducing a novel meta-gradient rebalancing
algorithm that addresses the primary limitations behind existing approaches.
Our experiments show that dataset rebalancing leads to better downstream
learning, improving the performance of general imitation learning algorithms
without requiring additional data collection. See our project website:
https://collab.me.vt.edu/data_curation/.

</details>


### [258] [L2Calib: $SE(3)$-Manifold Reinforcement Learning for Robust Extrinsic Calibration with Degenerate Motion Resilience](https://arxiv.org/abs/2508.06330)
*Baorun Li,Chengrui Zhu,Siyi Du,Bingran Chen,Jie Ren,Wenfei Wang,Yong Liu,Jiajun Lv*

Main category: cs.RO

TL;DR: This paper proposes a reinforcement learning-based framework for extrinsic calibration, improving odometry accuracy across diverse robotic platforms without the need for structured targets.


<details>
  <summary>Details</summary>
Motivation: Existing methods for multi-sensor extrinsic calibration are limited by their reliance on structured targets and fully-excited data, making them less suitable for real-world applications.

Method: The authors introduce a reinforcement learning framework that models extrinsic calibration as a decision-making problem, optimizing $SE(3)$ extrinsics. It utilizes a Bingham distribution for stable 3D rotation modeling, a trajectory alignment reward mechanism for calibration without structured targets, and an automated data selection module for enhancing scalability.

Result: Experimental evaluations on various platforms, including UAVs and UGVs, show that the proposed method outperforms traditional optimization-based approaches, achieving high-precision calibration even under weak excitation conditions.

Conclusion: The framework simplifies deployment across robotic platforms by supporting calibration from routine data, removing the need for high-quality initial extrinsics, and demonstrating robustness and efficiency in diverse conditions.

Abstract: Extrinsic calibration is essential for multi-sensor fusion, existing methods
rely on structured targets or fully-excited data, limiting real-world
applicability. Online calibration further suffers from weak excitation, leading
to unreliable estimates. To address these limitations, we propose a
reinforcement learning (RL)-based extrinsic calibration framework that
formulates extrinsic calibration as a decision-making problem, directly
optimizes $SE(3)$ extrinsics to enhance odometry accuracy. Our approach
leverages a probabilistic Bingham distribution to model 3D rotations, ensuring
stable optimization while inherently retaining quaternion symmetry. A
trajectory alignment reward mechanism enables robust calibration without
structured targets by quantitatively evaluating estimated tightly-coupled
trajectory against a reference trajectory. Additionally, an automated data
selection module filters uninformative samples, significantly improving
efficiency and scalability for large-scale datasets. Extensive experiments on
UAVs, UGVs, and handheld platforms demonstrate that our method outperforms
traditional optimization-based approaches, achieving high-precision calibration
even under weak excitation conditions. Our framework simplifies deployment on
diverse robotic platforms by eliminating the need for high-quality initial
extrinsics and enabling calibration from routine operating data. The code is
available at https://github.com/APRIL-ZJU/learn-to-calibrate.

</details>


### [259] [V*: An Efficient Motion Planning Algorithm for Autonomous Vehicles](https://arxiv.org/abs/2508.06404)
*Abdullah Zareh Andaryan,Michael G. H. Bell,Mohsen Ramezani,Glenn Geers*

Main category: cs.RO

TL;DR: This paper introduces V*, a motion planner for autonomous vehicles that integrates speed and direction into a graph-based approach to generate collision-free, time-optimal, and dynamically feasible trajectories.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current motion planning methods for autonomous vehicles, which often separate spatial feasibility from dynamic constraints or require smoothing after trajectory generation.

Method: The authors propose V*, a graph-based motion planner using a space-time-velocity lattice with integrated speed and direction as state variables. Techniques include hexagonal discretisation for efficient search, mathematical proofs for waypoint spacing, and geometric pruning to ensure dynamically feasible steering.

Result: V* demonstrates successful navigation in simulations with cluttered and dynamic environments, proactively avoiding conflicts, waiting when necessary, and coordinating dynamically with moving obstacles.

Conclusion: V* provides a theoretically sound framework to plan safe, efficient, and dynamically feasible trajectories without requiring additional refinement post-generation.

Abstract: Autonomous vehicle navigation in structured environments requires planners
capable of generating time-optimal, collision-free trajectories that satisfy
dynamic and kinematic constraints. We introduce V*, a graph-based motion
planner that represents speed and direction as explicit state variables within
a discretised space-time-velocity lattice. Unlike traditional methods that
decouple spatial search from dynamic feasibility or rely on post-hoc smoothing,
V* integrates both motion dimensions directly into graph construction through
dynamic graph generation during search expansion. To manage the complexity of
high-dimensional search, we employ a hexagonal discretisation strategy and
provide formal mathematical proofs establishing optimal waypoint spacing and
minimal node redundancy under constrained heading transitions for
velocity-aware motion planning. We develop a mathematical formulation for
transient steering dynamics in the kinematic bicycle model, modelling steering
angle convergence with exponential behaviour, and deriving the relationship for
convergence rate parameters. This theoretical foundation, combined with
geometric pruning strategies that eliminate expansions leading to infeasible
steering configurations, enables V* to evaluate dynamically admissible
manoeuvres, ensuring each trajectory is physically realisable without further
refinement. We further demonstrate V*'s performance in simulation studies with
cluttered and dynamic environments involving moving obstacles, showing its
ability to avoid conflicts, yield proactively, and generate safe, efficient
trajectories with temporal reasoning capabilities for waiting behaviours and
dynamic coordination.

</details>


### [260] [Shortcut Learning in Generalist Robot Policies: The Role of Dataset Diversity and Fragmentation](https://arxiv.org/abs/2508.06426)
*Youguang Xing,Xu Luo,Junlin Xie,Lianli Gao,Hengtao Shen,Jingkuan Song*

Main category: cs.RO

TL;DR: This paper analyzes the limited generalization capacity observed in generalist robot policies trained on large-scale datasets like Open X-Embodiment (OXE), attributing it to shortcut learning caused by dataset limitations.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of poor generalization in generalist robot policies, which hinders their practical deployment in tasks deviating from the training data distribution.

Method: The paper conducts theoretical and empirical analysis to identify the key causes of shortcut learning, namely limited diversity within sub-datasets and large distributional disparities. It also explores robotic data augmentation strategies to mitigate these issues.

Result: The study confirms that shortcut learning is rooted in dataset structure and argues for improved dataset collection strategies and data augmentation as means of addressing the problem. Both methods show improvements in simulations and real-world tests.

Conclusion: Understanding the structural shortcomings of large-scale datasets like OXE and leveraging data augmentation can significantly enhance the generalization potential of generalist robot policies.

Abstract: Generalist robot policies trained on large-scale datasets such as Open
X-Embodiment (OXE) demonstrate strong performance across a wide range of tasks.
However, they often struggle to generalize beyond the distribution of their
training data. In this paper, we investigate the underlying cause of this
limited generalization capability. We identify shortcut learning -- the
reliance on task-irrelevant features -- as a key impediment to generalization.
Through comprehensive theoretical and empirical analysis, we uncover two
primary contributors to shortcut learning: (1) limited diversity within
individual sub-datasets, and (2) significant distributional disparities across
sub-datasets, leading to dataset fragmentation. These issues arise from the
inherent structure of large-scale datasets like OXE, which are typically
composed of multiple sub-datasets collected independently across varied
environments and embodiments. Our findings provide critical insights into
dataset collection strategies that can reduce shortcut learning and enhance the
generalization ability of generalist robot policies. Moreover, in scenarios
where acquiring new large-scale data is impractical, we demonstrate that
carefully selected robotic data augmentation strategies can effectively reduce
shortcut learning in existing offline datasets, thereby improving
generalization capabilities of generalist robot policies, e.g., $\pi_0$, in
both simulation and real-world environments. More information at
https://lucky-light-sun.github.io/proj/shortcut-learning-in-grps/.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [261] [Empirical Evaluation of AI-Assisted Software Package Selection: A Knowledge Graph Approach](https://arxiv.org/abs/2508.05693)
*Siamak Farshidi,Amir Saberhabibi,Behbod Eskafi,Niloofar Nikfarjam,Sadegh Eskandari,Slinger Jansen,Michel Chaudron,Bedir Tekinerdogan*

Main category: cs.SE

TL;DR: The paper proposes PySelect, a decision support system using MCDM principles and AI to support evidence-based selection of Python packages.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in choosing third-party Python packages due to numerous alternatives and limited comparison evidence, minimizing risks associated with dependency evaluation and emphasizing long-term reliability.

Method: The method includes formulating package selection as an MCDM problem, collecting metadata and trends from platforms (GitHub, PyPI, Stack Overflow), and structuring these into a decision model utilized by PySelect, powered by large language models.

Result: The study evaluated 798,669 Python scripts from 16,887 GitHub repositories, demonstrating high precision in data extraction, enhanced recommendation quality over AI baselines, and positive user feedback about usefulness and ease of use.

Conclusion: The work presents a scalable, interpretable, and reproducible framework to aid software selection through empirical data and AI-assisted modeling, improving transparency and maintainability.

Abstract: Selecting third-party software packages in open-source ecosystems like Python
is challenging due to the large number of alternatives and limited transparent
evidence for comparison. Generative AI tools are increasingly used in
development workflows, but their suggestions often overlook dependency
evaluation, emphasize popularity over suitability, and lack reproducibility.
This creates risks for projects that require transparency, long-term
reliability, maintainability, and informed architectural decisions. This study
formulates software package selection as a Multi-Criteria Decision-Making
(MCDM) problem and proposes a data-driven framework for technology evaluation.
Automated data pipelines continuously collect and integrate software metadata,
usage trends, vulnerability information, and developer sentiment from GitHub,
PyPI, and Stack Overflow. These data are structured into a decision model
representing relationships among packages, domain features, and quality
attributes. The framework is implemented in PySelect, a decision support system
that uses large language models to interpret user intent and query the model to
identify contextually appropriate packages. The approach is evaluated using
798,669 Python scripts from 16,887 GitHub repositories and a user study based
on the Technology Acceptance Model. Results show high data extraction
precision, improved recommendation quality over generative AI baselines, and
positive user evaluations of usefulness and ease of use. This work introduces a
scalable, interpretable, and reproducible framework that supports
evidence-based software selection using MCDM principles, empirical data, and
AI-assisted intent modeling.

</details>


### [262] [Klear-CodeTest: Scalable Test Case Generation for Code Reinforcement Learning](https://arxiv.org/abs/2508.05710)
*Jia Fu,Xinyu Yang,Hongzhi Zhang,Yahui Liu,Jingyuan Zhang,Qi Wang,Fuzheng Zhang,Guorui Zhou*

Main category: cs.SE

TL;DR: Klear-CodeTest proposes a robust test case synthesis framework to improve feedback for training LLMs in code reinforcement learning, including a validation mechanism and security sandbox.


<details>
  <summary>Details</summary>
Motivation: High-quality feedback for large language models in code reinforcement learning is hampered by the challenge of synthesizing reliable test cases.

Method: Introduces a Generator-Validation (G-V) framework to create comprehensive test cases, along with a secure, multi-layered sandbox system for safe execution.

Result: Showed enhanced model performance and stability through experiments using the curated dataset and validation mechanisms.

Conclusion: The framework provides better test coverage, ensures correctness, and improves LLM training for code tasks. Source code and dataset are publicly available.

Abstract: Precise, correct feedback is crucial for effectively training large language
models (LLMs) in code reinforcement learning. However, synthesizing
high-quality test cases remains a profoundly challenging and unsolved problem.
In this work, we present Klear-CodeTest, a comprehensive test case synthesis
framework featuring rigorous verification to ensure quality and reliability of
test cases. Our approach achieves broad coverage of programming problems via a
novel Generator-Validation (G-V) framework, ensuring correctness through a
consistency validation mechanism that verifies outputs against gold solutions.
The proposed G-V framework generates comprehensive test cases including both
regular and corner cases, enhancing test coverage and discriminative power for
solution correctness assessment in code reinforcement learning. In addition, we
design a multi-layered security sandbox system optimized for online
verification platforms, guaranteeing safe and reliable code execution. Through
comprehensive experiments, we demonstrate the effectiveness of our curated
dataset, showing significant improvements in model performance and training
stability. The source codes, curated dataset and sandbox system are available
at: https://github.com/Kwai-Klear/CodeTest.

</details>


### [263] [Utilizing Composer Packages to Accelerate Laravel-Based Project Development Among Students: A Pedagogical and Practical Framework](https://arxiv.org/abs/2508.05747)
*Rohaizah Abdul Wahid,Muhamad Said Nizamuddin Nadim,Suliana Sulaiman,Syahmi Akmal Shaharudin,Muhammad Danial Jupikil,Iqqwan Jasman Su Azlan Su*

Main category: cs.SE

TL;DR: The paper proposes using Composer packages to ease Laravel development in academic settings while promoting professional practices and deeper learning.


<details>
  <summary>Details</summary>
Motivation: Students face challenges completing Laravel projects within short academic timelines, necessitating tools to aid development.

Method: The paper categorizes useful Composer packages, discusses their integration in educational curricula, and provides best-practice guidelines.

Result: Using Composer packages can simplify project development, align academia with industry practices, and prepare students for professional environments.

Conclusion: Properly guided use of Composer packages accelerates learning and enhances industry readiness, but educators must ensure students critically engage with the tools to foster deep understanding.

Abstract: Laravel has emerged as a foundational framework in university web development
curricula. However, despite its scaffolding capabilities, students often
struggle to complete projects within limited academic timelines. This
conceptual paper introduces Composer, PHP's standard dependency manager, and
categorizes a curated selection of Composer packages that significantly reduce
development effort while fostering professional software practices. Grounded in
practical and pedagogical considerations, the paper illustrates how educators
and learners can strategically leverage these tools to build typical academic
or personal Laravel-based systems. Central to this approach is maintaining code
quality and reinforcing conceptual understanding. The paper also addresses
potential risks such as package conflicts and over-reliance on tools, providing
best-practice recommendations to mitigate them. While the goal is to accelerate
development, the deeper objective is to reinforce professional workflows and
industry readiness. Exposure to Composer packages enhances curriculum relevance
and smooths the transition from academia to the workplace. However, effective
integration requires deliberate instructional design aligned with learning
objectives. Without guidance, students may treat packages as black boxes. Thus,
educators must teach not only how to use these tools, but also when and why,
encouraging critical evaluation of their utility and limitations. This ensures
that practical convenience supports rather than supplants deep learning.

</details>


### [264] [AI-Guided Exploration of Large-Scale Codebases](https://arxiv.org/abs/2508.05799)
*Yoseph Berhanu Alebachew*

Main category: cs.SE

TL;DR: The paper proposes an adaptive tool blending UML visuals, user interfaces, and LLM-guided exploration for better software comprehension.


<details>
  <summary>Details</summary>
Motivation: Developers struggle to understand complex software systems effectively using traditional tools lacking adaptability, interactivity, and integration with context.

Method: The paper introduces a hybrid system combining deterministic reverse engineering with LLM-assisted intent-aware exploration, featuring UML visuals, dynamic interfaces, and collaborative tools.

Result: A prototype for Java demonstrates the approach's feasibility, setting a foundation for empirical evaluation and future expansions.

Conclusion: This adaptive tool addresses comprehension challenges, paving the way for intelligent environments that align with developer cognition and collaborative workflows.

Abstract: Understanding large-scale, complex software systems is a major challenge for
developers, who spend a significant portion of their time on program
comprehension. Traditional tools such as static visualizations and reverse
engineering techniques provide structural insights but often lack
interactivity, adaptability, and integration with contextual information.
Recent advancements in large language models (LLMs) offer new opportunities to
enhance code exploration workflows, yet their lack of grounding and integration
with structured views limits their effectiveness. This work introduces a hybrid
approach that integrates deterministic reverse engineering with LLM-guided,
intent-aware visual exploration. The proposed system combines UML-based
visualization, dynamic user interfaces, historical context, and collaborative
features into an adaptive tool for code comprehension. By interpreting user
queries and interaction patterns, the LLM helps developers navigate and
understand complex codebases more effectively. A prototype implementation for
Java demonstrates the feasibility of this approach. Future work includes
empirical evaluation, scaling to polyglot systems, and exploring GUI-driven LLM
interaction models. This research lays the groundwork for intelligent,
interactive environments that align with developer cognition and collaborative
workflows.

</details>


### [265] [Enhancing Software Vulnerability Detection Through Adaptive Test Input Generation Using Genetic Algorithm](https://arxiv.org/abs/2508.05923)
*Yanusha Mehendran,Maolin Tang,Yi Lu*

Main category: cs.SE

TL;DR: This paper presents a genetic algorithm-based approach for generating test inputs to detect software vulnerabilities more effectively, achieving significant improvements in code coverage metrics.


<details>
  <summary>Details</summary>
Motivation: The growing complexity of software outpaces traditional methods for detecting vulnerabilities, necessitating adaptive and innovative solutions for effective security testing.

Method: A genetic algorithm integrates crossover operators and adaptive feedback mechanisms to dynamically generate structurally valid test inputs, evolving them to achieve deeper code traversal and improved detection efficiency.

Result: The method, evaluated on nine open-source JSON-processing libraries, demonstrated significant improvements in various code coverage metrics compared to a benchmark fuzzing method, including 166% higher branch coverage.

Conclusion: The approach provides an adaptive and scalable solution for software security testing, capable of uncovering deeper and more complex vulnerabilities, and exceeding traditional fuzzing methods in effectiveness.

Abstract: Software vulnerabilities continue to undermine the reliability and security
of modern systems, particularly as software complexity outpaces the
capabilities of traditional detection methods. This study introduces a genetic
algorithm-based method for test input generation that innovatively integrates
genetic operators and adaptive learning to enhance software vulnerability
detection. A key contribution is the application of the crossover operator,
which facilitates exploration by searching across a broader space of potential
test inputs. Complementing this, an adaptive feedback mechanism continuously
learns from the system's execution behavior and dynamically guides input
generation toward promising areas of the input space. Rather than relying on
fixed or randomly selected inputs, the approach evolves a population of
structurally valid test cases using feedback-driven selection, enabling deeper
and more effective code traversal. This strategic integration of exploration
and exploitation ensures that both diverse and targeted test inputs are
developed over time. Evaluation was conducted across nine open-source
JSON-processing libraries. The proposed method achieved substantial
improvements in coverage compared to a benchmark evolutionary fuzzing method,
with average gains of 39.8% in class coverage, 62.4% in method coverage, 105.0%
in line coverage, 114.0% in instruction coverage, and 166.0% in branch
coverage. These results highlight the method's capacity to detect deeper and
more complex vulnerabilities, offering a scalable and adaptive solution to
software security testing.

</details>


### [266] [A Survey on Task Scheduling in Carbon-Aware Container Orchestration](https://arxiv.org/abs/2508.05949)
*Jialin Yang,Zainab Saad,Jiajun Wu,Xiaoguang Niu,Henry Leung,Steve Drew*

Main category: cs.SE

TL;DR: This paper reviews Kubernetes scheduling strategies for addressing energy consumption and carbon emissions in cloud computing, providing a taxonomy and sustainability-focused insights.


<details>
  <summary>Details</summary>
Motivation: The overwhelming energy demands of cloud computing and extensive deployment of large language models have dramatically increased carbon footprints, necessitating efficient solutions.

Method: The paper systematically reviews Kubernetes scheduling strategies, classifies them into hardware-centric and software-centric approaches, and proposes a taxonomy focusing on sustainability objectives.

Result: Key research trends and open challenges in sustainable scheduling for cloud systems are analyzed, offering insights to guide future developments.

Conclusion: Effective scheduling strategies can play a pivotal role in reducing energy demands and carbon emissions, contributing to more sustainable cloud computing systems.

Abstract: The soaring energy demands of large-scale software ecosystems and cloud data
centers, accelerated by the intensive training and deployment of large language
models, have driven energy consumption and carbon footprint to unprecedented
levels. In response, both industry and academia are increasing efforts to
reduce the carbon emissions associated with cloud computing through more
efficient task scheduling and infrastructure orchestration. In this work, we
present a systematic review of various Kubernetes scheduling strategies,
categorizing them into hardware-centric and software-centric, annotating each
with its sustainability objectives, and grouping them according to the
algorithms they use. We propose a comprehensive taxonomy for cloud task
scheduling studies, with a particular focus on the environmental sustainability
aspect. We analyze emerging research trends and open challenges, and our
findings provide critical insight into the design of sustainable scheduling
solutions for next-generation cloud computing systems.

</details>


### [267] [Impact-driven Context Filtering For Cross-file Code Completion](https://arxiv.org/abs/2508.05970)
*Yanzhou Li,Shangqing Liu,Kangjie Chen,Tianwei Zhang,Yang Liu*

Main category: cs.SE

TL;DR: The paper addresses the issue of irrelevant or harmful retrieved contexts in repository-level code completion, introducing CODEFILTER to improve accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing retrieval-augmented generation (RAG) techniques for code completion sometimes utilize irrelevant or detrimental cross-file contexts, impacting the quality of code completions.

Method: The authors propose CODEFILTER, a retrieval context filtering framework trained on a custom-labeled dataset to classify retrieved code chunks as positive, neutral, or negative, filtering out harmful ones during generation.

Result: CODEFILTER improves code completion accuracy, reduces input prompt length, and shows strong generalizability across tasks and models, validated on benchmarks like RepoEval and CrossCodeLongEval.

Conclusion: The proposed CODEFILTER framework enhances repository-level code completion by improving accuracy, computational efficiency, and precision through adaptive filtering of retrieved contexts.

Abstract: Retrieval-augmented generation (RAG) has recently demonstrated considerable
potential for repository-level code completion, as it integrates cross-file
knowledge with in-file preceding code to provide comprehensive contexts for
generation. To better understand the contribution of the retrieved cross-file
contexts, we introduce a likelihood-based metric to evaluate the impact of each
retrieved code chunk on the completion. Our analysis reveals that, despite
retrieving numerous chunks, only a small subset positively contributes to the
completion, while some chunks even degrade performance. To address this issue,
we leverage this metric to construct a repository-level dataset where each
retrieved chunk is labeled as positive, neutral, or negative based on its
relevance to the target completion. We then propose an adaptive retrieval
context filtering framework, CODEFILTER, trained on this dataset to mitigate
the harmful effects of negative retrieved contexts in code completion.
Extensive evaluation on the RepoEval and CrossCodeLongEval benchmarks
demonstrates that CODEFILTER consistently improves completion accuracy compared
to approaches without filtering operations across various tasks. Additionally,
CODEFILTER significantly reduces the length of the input prompt, enhancing
computational efficiency while exhibiting strong generalizability across
different models. These results underscore the potential of CODEFILTER to
enhance the accuracy, efficiency, and attributability of repository-level code
completion.

</details>


### [268] [Position: Intelligent Coding Systems Should Write Programs with Justifications](https://arxiv.org/abs/2508.06017)
*Xiangzhe Xu,Shiwei Feng,Zian Su,Chengpeng Wang,Xiangyu Zhang*

Main category: cs.SE

TL;DR: The paper discusses enhancing AI coding systems by focusing on generating clear and meaningful justifications for code suggestions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address trust and usability concerns stemming from the opaque decision-making of AI coding systems, particularly for non-expert users.

Method: The proposed method suggests adopting neuro-symbolic approaches, incorporating symbolic constraints during training and enriching program semantics with neural representations to generate justifications.

Result: The authors identify two critical properties for justifications: cognitive alignment and semantic faithfulness, and critique existing approaches like formal verification and post-hoc explainability.

Conclusion: The paper concludes that integrating neuro-symbolic methods could enhance the trust, usability, and comprehensibility of AI-driven coding systems.

Abstract: Intelligent coding systems are transforming software development by enabling
users to specify code behavior in natural language. However, the opaque
decision-making of AI-driven coders raises trust and usability concerns,
particularly for non-expert users who cannot inspect low-level implementations.
We argue that these systems should not only generate code but also produce
clear, consistent justifications that bridge model reasoning and user
understanding. To this end, we identify two critical justification
properties-cognitive alignment and semantic faithfulness-and highlight the
limitations of existing methods, including formal verification, static
analysis, and post-hoc explainability. We advocate exploring neuro-symbolic
approaches for justification generation, where symbolic constraints guide model
behavior during training and program semantics are enriched through neural
representations, enabling automated consistency checks at inference time.

</details>


### [269] [Understanding Inconsistent State Update Vulnerabilities in Smart Contracts](https://arxiv.org/abs/2508.06192)
*Lantian Li,Yuyu Chen,Jingwen Wu,Yue Pan,Zhongxing Yu*

Main category: cs.SE

TL;DR: This paper presents the first large-scale study on inconsistent state update vulnerabilities in smart contracts. It identifies root causes, fixes, and exploitation methods, offering findings and developed a detection tool successfully applied to GitHub projects.


<details>
  <summary>Details</summary>
Motivation: Smart contracts' state update vulnerabilities are exploited by attackers, creating a need for better detection and prevention methods.

Method: A systematic large-scale empirical study conducted on 116 vulnerabilities in 352 smart contract projects, supported by a proof-of-concept detection tool.

Result: The study uncovers 11 key findings and demonstrates the successful application of its detection tool in identifying issues in 64 GitHub projects, with confirmations from 19 project owners.

Conclusion: The findings and developed checker highlight critical insights and practical tools for identifying and mitigating inconsistent state update vulnerabilities in smart contracts.

Abstract: Smart contracts enable contract terms to be automatically executed and
verified on the blockchain, and recent years have witnessed numerous
applications of them in areas such as financial institutions and supply chains.
The execution logic of a smart contract is closely related to the contract
state, and thus the correct and safe execution of the contract depends heavily
on the precise control and update of the contract state. However, the contract
state update process can have issues. In particular, inconsistent state update
issues can arise for reasons such as unsynchronized modifications. Inconsistent
state update bugs have been exploited by attackers many times, but existing
detection tools still have difficulty in effectively identifying them. This
paper conducts the first large-scale empirical study about inconsistent state
update vulnerabilities (that is, inconsistent state update bugs that are
exploitable) in smart contracts, aiming to shed light for developers,
researchers, tool builders, and language or library designers in order to avoid
inconsistent state update vulnerabilities. We systematically investigate 116
inconsistent state update vulnerabilities in 352 real-world smart contract
projects, summarizing their root causes, fix strategies, and exploitation
methods. Our study provides 11 original and important findings, and we also
give the implications of our findings. To illustrate the potential benefits of
our research, we also develop a proof-of-concept checker based on one of our
findings. The checker effectively detects issues in 64 popular GitHub projects,
and 19 project owners have confirmed the detected issues at the time of
writing. The result demonstrates the usefulness and importance of our findings
for avoiding inconsistent state update vulnerabilities in smart contracts.

</details>


### [270] [Improving the Developer Experience with a Low-Code Process Modelling Language](https://arxiv.org/abs/2508.06299)
*Henrique Henriques,Hugo Lourenço,Vasco Amaral,Miguel Goulão*

Main category: cs.SE

TL;DR: The paper addresses usability issues in the process modeling language (BPT) of the OutSystems Platform, redesigning it based on empirical evaluations and achieving significant usability improvements.


<details>
  <summary>Details</summary>
Motivation: The motivation comes from the low adoption rate and usability problems of the BPT DSL in the OutSystems Platform, leading to increased maintenance costs and limited usage.

Method: The method involved interviews, critical reviews using the 'Physics of Notation,' and empirical evaluations (System Usability Scale and NASA Task Load Index) to inform redesign efforts aligned with engineers' workflows and culture.

Result: The new BPT version showed significant improvement in usability, reflected in better semantic transparency (31% to 69%), correctness of responses (51% to 89%), SUS score (42.25 to 64.78), and reduced task load (TLX score: 36.50 to 20.78).

Conclusion: The redesigned BPT version substantially enhances developer experience compared to its predecessor, showcasing how user feedback and targeted changes can improve adoption and usability in a specialized DSL.

Abstract: Context: The OutSystems Platform is a development environment composed of
several DSLs, used to specify, quickly build, and validate web and mobile
applications. The DSLs allow users to model different perspectives such as
interfaces and data models, define custom business logic and construct process
models. Problem: The DSL for process modelling (Business Process Technology
(BPT)), has a low adoption rate and is perceived as having usability problems
hampering its adoption. This is problematic given the language maintenance
costs. Method: We used a combination of interviews, a critical review of BPT
using the "Physics of Notation" and empirical evaluations of BPT using the
System Usability Scale (SUS) and the NASA Task Load indeX (TLX), to develop a
new version of BPT, taking these inputs and Outsystems' engineers' culture into
account. Results: Evaluations conducted with 25 professional software engineers
showed an increase of the semantic transparency on the new version, from 31% to
69%, an increase in the correctness of responses, from 51% to 89%, an increase
in the SUS score, from 42.25 to 64.78, and a decrease of the TLX score, from
36.50 to 20.78. These differences were statistically significant. Conclusions:
These results suggest that the new version of BPT significantly improved the
developer experience of the previous version. The end users' background with
OutSystems had a relevant impact on the final concrete syntax choices and
achieved usability indicators.

</details>


### [271] [Execution-Feedback Driven Test Generation from SWE Issues](https://arxiv.org/abs/2508.06365)
*Toufique Ahmed,Jatin Ganhotra,Avraham Shinnar,Martin Hirzel*

Main category: cs.SE

TL;DR: The paper introduces e-Otter++, a tool that generates reproduction tests for software engineering issues, overcoming the challenge posed by missing or incorrect code.


<details>
  <summary>Details</summary>
Motivation: To address the difficulty in resolving software engineering issues due to the lack of reproduction tests.

Method: Developing and implementing e-Otter++, which uses novel execution feedback techniques to generate reproduction tests even when the code is incomplete or incorrect.

Result: Experiments with e-Otter++ demonstrated its effectiveness, achieving an average fail-to-pass rate of 63% on the TDD-Bench Verified benchmark.

Conclusion: e-Otter++ significantly advances the state-of-the-art in automated test generation for software issues, particularly when reproduction tests are missing.

Abstract: A software engineering issue (SWE issue) is easier to resolve when
accompanied by a reproduction test. Unfortunately, most issues do not come with
functioning reproduction tests, so this paper explores how to generate them
automatically. The primary challenge in this setting is that the code to be
tested is either missing or wrong, as evidenced by the existence of the issue
in the first place. This has held back test generation for this setting:
without the correct code to execute, it is difficult to leverage execution
feedback to generate good tests. This paper introduces novel techniques for
leveraging execution feedback to get around this problem, implemented in a new
reproduction test generator called e-Otter++. Experiments show that e-Otter++
represents a leap ahead in the state-of-the-art for this problem, generating
tests with an average fail-to-pass rate of 63% on the TDD-Bench Verified
benchmark.

</details>


### [272] [What Builds Effective In-Context Examples for Code Generation?](https://arxiv.org/abs/2508.06414)
*Dongze Li,Songqiang Chen,Jialun Cao,Shing-Chi Cheung*

Main category: cs.SE

TL;DR: The paper explores how various code features in In-Context Learning (ICL) prompts influence the code generation ability of Large Language Models (LLMs), finding that identifier naming is a critical factor for performance.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to uncover which specific features in code examples, such as naming conventions or formatting, most impact ICL's effectiveness in enhancing LLM code generation.

Method: The authors perform controlled ablation studies on various features (e.g., naming styles, formatting) in ICL-provided code examples to isolate and evaluate their impact on performance.

Result: Appropriate naming of variables and functions is found to significantly improve code generation, while LLMs prioritize semantically meaningful naming over formatting. Current LLMs struggle to generalize problem-solving insights from examples.

Conclusion: The study provides insights for optimizing ICL systems and identifies key challenges in enabling LLMs to learn reflection-based problem-solving for code generation tasks.

Abstract: In-Context Learning (ICL) has emerged as a promising solution to enhance the
code generation capabilities of Large Language Models (LLMs), which
incorporates code examples inside the prompt to let LLMs learn from
demonstrations. However, despite the substantial effectiveness of the code
example-based ICL approach, the specific features (e.g., identifier naming
styles, code formatting, solution insight) within the ICL-provided code
examples that significantly contribute to the ICL's effectiveness remain
unclear. This paper systematically investigates the impact of various code
features on ICL with code examples through controlled ablation studies. Our
findings reveal that the appropriate naming of variables and functions is
crucial for effective code generation, with their elimination leading to
performance decreases of up to 30 percentage points. We further demonstrate
that LLMs prioritize semantically meaningful identifier names over formatting
conventions, with language-specific preferences regarding identifier verbosity.
Additionally, our investigation into ICL's potential for enhancing reflection
and inference capabilities reveals that current LLMs struggle to extract
generalizable problem-solving insights from similar code solutions, despite
being capable of utilizing direct information effectively. These findings are
expected to provide valuable insights for optimizing ICL systems in code
generation applications and highlight fundamental challenges in
reflection-based learning for code generation tasks.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [273] [Diverse Neural Sequences in QIF Networks: An Analytically Tractable Framework for Synfire Chains and Hippocampal Replay](https://arxiv.org/abs/2508.06085)
*Genki Shimizu,Taro Toyoizumi*

Main category: q-bio.NC

TL;DR: The paper introduces a QIF neural network model with a TAH learning rule to simulate diverse, biologically plausible sequential neural activities, offering analytical tractability and robustness.


<details>
  <summary>Details</summary>
Motivation: Sequential neural activity is crucial for cognition, but understanding its recall under biological constraints remains challenging.

Method: The authors propose a QIF neural network with sequences embedded via a temporally asymmetric Hebbian (TAH) learning rule and derive low-dimensional firing-rate equations to study its dynamics.

Result: The model reproduces diverse sequential activities, from synfire-like chains to hippocampal replay-like bursts, and explains these dynamics using derived bifurcation structures. It also shows robustness to synaptic heterogeneity and pattern overlap.

Conclusion: The proposed model serves as a biologically plausible and analytically tractable framework for studying sequential neural activity dynamics, stability, and diversity.

Abstract: Sequential neural activity is fundamental to cognition, yet how diverse
sequences are recalled under biological constraints remains a key question.
Existing models often struggle to balance biophysical realism and analytical
tractability. We address this problem by proposing a parsimonious network of
Quadratic Integrate-and-Fire (QIF) neurons with sequences embedded via a
temporally asymmetric Hebbian (TAH) rule. Our findings demonstrate that this
single framework robustly reproduces a spectrum of sequential activities,
including persistent synfire-like chains and transient, hippocampal replay-like
bursts exhibiting intra-ripple frequency accommodation (IFA), all achieved
without requiring specialized delay or adaptation mechanisms. Crucially, we
derive exact low-dimensional firing-rate equations (FREs) that provide
mechanistic insight, elucidating the bifurcation structure governing these
distinct dynamical regimes and explaining their stability. The model also
exhibits strong robustness to synaptic heterogeneity and memory pattern
overlap. These results establish QIF networks with TAH connectivity as an
analytically tractable and biologically plausible platform for investigating
the emergence, stability, and diversity of sequential neural activity in the
brain.

</details>


### [274] [Ensemble-Based Graph Representation of fMRI Data for Cognitive Brain State Classification](https://arxiv.org/abs/2508.06118)
*Daniil Vlasenko,Vadim Ushakov,Alexey Zaikin,Denis Zakharov*

Main category: q-bio.NC

TL;DR: The paper presents an ensemble-based graph representation method for fMRI data, achieving high classification accuracy in distinguishing cognitive brain states.


<details>
  <summary>Details</summary>
Motivation: The work aims to address the challenge of classifying human cognitive brain states from high-dimensional and noisy neuroimaging data.

Method: The method involves creating ensemble-based graphs where edge weights represent the difference in posterior probabilities between cognitive states, and using these graphs for classification tasks.

Result: The approach achieved an average accuracy between 97.07% and 99.74% across seven cognitive tasks, outperforming correlation-based graphs in all experiments.

Conclusion: Ensemble-based graphs improved classification accuracy and preserved interpretability, showcasing their utility in brain-state discrimination and potential for broader applications.

Abstract: Understanding and classifying human cognitive brain states based on
neuroimaging data remains one of the foremost and most challenging problems in
neuroscience, owing to the high dimensionality and intrinsic noise of the
signals. In this work, we propose an ensemble-based graph representation method
of functional magnetic resonance imaging (fMRI) data for the task of binary
brain-state classification. Our method builds the graph by leveraging multiple
base machine-learning models: each edge weight reflects the difference in
posterior probabilities between two cognitive states, yielding values in the
range [-1, 1] that encode confidence in a given state. We applied this approach
to seven cognitive tasks from the Human Connectome Project (HCP 1200 Subject
Release), including working memory, gambling, motor activity, language, social
cognition, relational processing, and emotion processing. Using only the mean
incident edge weights of the graphs as features, a simple logistic-regression
classifier achieved average accuracies from 97.07% to 99.74%. We also compared
our ensemble graphs with classical correlation-based graphs in a classification
task with a graph neural network (GNN). In all experiments, the highest
classification accuracy was obtained with ensemble graphs. These results
demonstrate that ensemble graphs convey richer topological information and
enhance brain-state discrimination. Our approach preserves edge-level
interpretability of the fMRI graph representation, is adaptable to multiclass
and regression tasks, and can be extended to other neuroimaging modalities and
pathological-state classification.

</details>


### [275] [Low dimensional dynamics of a sparse balanced synaptic network of quadratic integrate-and-fire neurons](https://arxiv.org/abs/2508.06253)
*Maria V. Ageeva,Denis S. Goldobin*

Main category: q-bio.NC

TL;DR: This paper presents a low-dimensional model using circular cumulants to describe the kinetics of a balanced network of neurons and demonstrates its accuracy even in non-diffusion conditions.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of diffusion approximation in modeling complex collective behaviors in neural networks and provide a framework for studying biologically relevant regimes of neural activity.

Method: The authors use a mean-field theory approach for a homogeneous inhibitory network of quadratic integrate-and-fire neurons, introducing circular cumulants to formulate a two-cumulant reduction. They validate the reduced model through numerical simulations under both static and periodically modulated inputs.

Result: The derived model shows high accuracy in representing the time-independent and dynamic responses of the network, even in parameter domains where the diffusion approximation fails.

Conclusion: The reduced low-dimensional model provides insights into the macroscopic dynamics of neural networks with a low embedding dimensionality and offers a robust tool for further theoretical exploration of neural inhibitory-excitatory balances.

Abstract: Kinetics of a balanced network of neurons with a sparse grid of synaptic
links is well representable by the stochastic dynamics of a generic neuron
subject to an effective shot noise. The rate of delta-pulses of the noise is
determined self-consistently from the probability density of the neuron states.
Importantly, the most sophisticated (but robust) collective regimes of the
network do not allow for the diffusion approximation, which is routinely
adopted for a shot noise in mathematical neuroscience. These regimes can be
expected to be biologically relevant. For the kinetics equations of the
complete mean field theory of a homogeneous inhibitory network of quadratic
integrate-and-fire neurons, we introduce circular cumulants of the genuine
phase variable and derive a rigorous two cumulant reduction for both
time-independent conditions and modulation of the excitatory current. The low
dimensional model is examined with numerical simulations and found to be
accurate for time-independent states and dynamic response to a periodic
modulation deep into the parameter domain where the diffusion approximation is
not applicable. The accuracy of a low dimensional model indicates and explains
a low embedding dimensionality of the macroscopic collective dynamics of the
network. The reduced model can be instrumental for theoretical studies of
inhibitory-excitatory balances neural networks.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [276] [Random Walk Learning and the Pac-Man Attack](https://arxiv.org/abs/2508.05663)
*Xingran Chen,Parimal Parag,Rohit Bhagat,Zonghong Liu,Salim El Rouayheb*

Main category: stat.ML

TL;DR: This paper presents the "Pac-Man" attack on random walk-based decentralized learning and proposes the Average Crossing (AC) algorithm as a defense mechanism, ensuring robustness and convergence.


<details>
  <summary>Details</summary>
Motivation: Random walk-based algorithms are efficient and scalable but are vulnerable to stealthy adversarial threats, such as malicious actions in decentralized learning systems.

Method: The authors introduce the "Pac-Man" attack, a stealthy adversarial method that halts random walks in a network, and propose the Average Crossing (AC) algorithm to duplicate random walks and maintain robustness against such threats.

Result: Theoretical analysis shows that the proposed AC algorithm keeps the random walk population bounded and ensures stochastic gradient descent convergence, even under adversarial attacks. Empirical results confirm these findings and demonstrate a phase transition behavior in extinction probability based on the duplication threshold.

Conclusion: Average Crossing effectively counters the Pac-Man attack, preserving decentralized learning processes' robustness and efficiency. The work improves understanding of adversarial resistance mechanisms in distributed systems.

Abstract: Random walk (RW)-based algorithms have long been popular in distributed
systems due to low overheads and scalability, with recent growing applications
in decentralized learning. However, their reliance on local interactions makes
them inherently vulnerable to malicious behavior. In this work, we investigate
an adversarial threat that we term the ``Pac-Man'' attack, in which a malicious
node probabilistically terminates any RW that visits it. This stealthy behavior
gradually eliminates active RWs from the network, effectively halting the
learning process without triggering failure alarms. To counter this threat, we
propose the Average Crossing (AC) algorithm--a fully decentralized mechanism
for duplicating RWs to prevent RW extinction in the presence of Pac-Man. Our
theoretical analysis establishes that (i) the RW population remains almost
surely bounded under AC and (ii) RW-based stochastic gradient descent remains
convergent under AC, even in the presence of Pac-Man, with a quantifiable
deviation from the true optimum. Our extensive empirical results on both
synthetic and real-world datasets corroborate our theoretical findings.
Furthermore, they uncover a phase transition in the extinction probability as a
function of the duplication threshold. We offer theoretical insights by
analyzing a simplified variant of the AC, which sheds light on the observed
phase transition.

</details>


### [277] [Reduction Techniques for Survival Analysis](https://arxiv.org/abs/2508.05715)
*Johannes Piller,Léa Orsini,Simon Wiegrebe,John Zobolas,Lukas Burk,Sophie Hanna Langbein,Philip Studener,Markus Goeswein,Andreas Bender*

Main category: stat.ML

TL;DR: This paper explores methods to simplify survival analysis by converting it into a regression or classification task, enabling the use of standard machine learning tools.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address challenges in survival analysis by leveraging common approaches from regression or classification, making it easier to use machine learning and deep learning methods for such tasks.

Method: The paper discusses various reduction techniques that allow survival analysis tasks to be expressed as regression or classification problems, detailing strengths, weaknesses, examples, and offering a benchmark comparison of their predictive performance.

Result: The benchmark results show how the reduction techniques can compare to established machine learning methods for survival analysis, providing insights into their predictiveness and applicability.

Conclusion: Reduction techniques simplify survival analysis by enabling the use of conventional machine learning tools, demonstrating their practical advantages and effectiveness through examples and benchmarks.

Abstract: In this work, we discuss what we refer to as reduction techniques for
survival analysis, that is, techniques that "reduce" a survival task to a more
common regression or classification task, without ignoring the specifics of
survival data. Such techniques particularly facilitate machine learning-based
survival analysis, as they allow for applying standard tools from machine and
deep learning to many survival tasks without requiring custom learners. We
provide an overview of different reduction techniques and discuss their
respective strengths and weaknesses. We also provide a principled
implementation of some of these reductions, such that they are directly
available within standard machine learning workflows. We illustrate each
reduction using dedicated examples and perform a benchmark analysis that
compares their predictive performance to established machine learning methods
for survival analysis.

</details>


### [278] [Stochastic Trace Optimization of Parameter Dependent Matrices Based on Statistical Learning Theory](https://arxiv.org/abs/2508.05764)
*Arvind K. Saibaba,Ilse C. F. Ipsen*

Main category: stat.ML

TL;DR: This paper develops a Monte Carlo estimator to minimize a trace function of parameter-dependent matrices and evaluates sampling bounds using two techniques: epsilon nets and generic chaining.


<details>
  <summary>Details</summary>
Motivation: The study seeks to minimize the trace of parameter-dependent matrices efficiently while ensuring that errors are controlled and sampling requirements are minimized.

Method: The authors use Monte Carlo estimation to minimize the trace of the matrix and derive sampling bounds for error control based on two methods: epsilon net and generic chaining techniques.

Result: Both methods predict minimal sampling requirements for matrices with small offdiagonal mass and compact parameter spaces. Epsilon net bounds include explicit constants, while chaining bounds are theoretically promising but harder to quantify.

Conclusion: Epsilon nets offer practical applicability, while chaining bounds might perform better in theory but are difficult to evaluate, leaving a gap in direct comparison.

Abstract: We consider matrices $\boldsymbol{A}(\boldsymbol\theta)\in\mathbb{R}^{m\times
m}$ that depend, possibly nonlinearly, on a parameter $\boldsymbol\theta$ from
a compact parameter space $\Theta$. We present a Monte Carlo estimator for
minimizing $\text{trace}(\boldsymbol{A}(\boldsymbol\theta))$ over all
$\boldsymbol\theta\in\Theta$, and determine the sampling amount so that the
backward error of the estimator is bounded with high probability. We derive two
types of bounds, based on epsilon nets and on generic chaining. Both types
predict a small sampling amount for matrices
$\boldsymbol{A}(\boldsymbol\theta)$ with small offdiagonal mass, and parameter
spaces $\Theta$ of small ``size.'' Dependence on the matrix dimension~$m$ is
only weak or not explicit. The bounds based on epsilon nets are easier to
evaluate and come with fully specified constants. In contrast, the bounds based
on chaining depend on the Talagrand functionals which are difficult to
evaluate, except in very special cases. Comparisons between the two types of
bounds are difficult, although the literature suggests that chaining bounds can
be superior.

</details>


### [279] [Lightweight Auto-bidding based on Traffic Prediction in Live Advertising](https://arxiv.org/abs/2508.06069)
*Bo Yang,Ruixuan Luo,Junqi Jin,Han Zhu*

Main category: stat.ML

TL;DR: The paper introduces Binary Constrained Bidding (BiCB), a lightweight algorithm for real-time bidding in live advertising, combining mathematical optimization and statistical traffic estimation.


<details>
  <summary>Details</summary>
Motivation: Existing auto-bidding methods struggle with real-time traffic control and unknown future traffic complexities, requiring a better solution for live streaming advertising.

Method: BiCB combines an optimal bidding formula derived from mathematical analysis with statistical traffic estimation, achieving efficient performance through a low-complexity solution and bounding constraints on auto-bidding models.

Result: Offline and online experiments demonstrate that BiCB performs well with low engineering costs and provides good approximations to optimal results.

Conclusion: BiCB improves real-time bidding performance in live advertising while maintaining computational efficiency and practical utility.

Abstract: Internet live streaming is widely used in online entertainment and
e-commerce, where live advertising is an important marketing tool for anchors.
An advertising campaign hopes to maximize the effect (such as conversions)
under constraints (such as budget and cost-per-click). The mainstream control
of campaigns is auto-bidding, where the performance depends on the decision of
the bidding algorithm in each request. The most widely used auto-bidding
algorithms include Proportional-Integral-Derivative (PID) control, linear
programming (LP), reinforcement learning (RL), etc. Existing methods either do
not consider the entire time traffic, or have too high computational
complexity. In this paper, the live advertising has high requirements for
real-time bidding (second-level control) and faces the difficulty of unknown
future traffic. Therefore, we propose a lightweight bidding algorithm Binary
Constrained Bidding (BiCB), which neatly combines the optimal bidding formula
given by mathematical analysis and the statistical method of future traffic
estimation, and obtains good approximation to the optimal result through a low
complexity solution. In addition, we complement the form of upper and lower
bound constraints for traditional auto-bidding modeling and give theoretical
analysis of BiCB. Sufficient offline and online experiments prove BiCB's good
performance and low engineering cost.

</details>


### [280] [Decorrelated feature importance from local sample weighting](https://arxiv.org/abs/2508.06337)
*Benedikt Fröhlich,Alison Durst,Merle Behr*

Main category: stat.ML

TL;DR: The paper introduces a method called local sample weighting (losaw) to improve feature importance (FI) scores in machine learning models when features are correlated, by decorrelating target features using a sample weighting scheme.


<details>
  <summary>Details</summary>
Motivation: FI statistics are valuable for understanding machine learning models but are limited when features in the training data are correlated, potentially leading to noisy or misleading FI scores.

Method: The proposed losaw method incorporates local sample weighting inspired by causality and inverse probability weighting, aiming to decorrelate target features locally and reduce model bias for better FI evaluation.

Result: Losaw consistently improves FI scores and often enhances out-of-distribution prediction accuracy, while maintaining similar in-distribution accuracy, as demonstrated on random forest and convolutional neural networks in simulation studies.

Conclusion: Losaw is effective in handling feature correlation and can be flexibly integrated into many machine learning algorithms, offering a trade-off between interpretation and prediction performance.

Abstract: Feature importance (FI) statistics provide a prominent and valuable method of
insight into the decision process of machine learning (ML) models, but their
effectiveness has well-known limitations when correlation is present among the
features in the training data. In this case, the FI often tends to be
distributed among all features which are in correlation with the
response-generating signal features. Even worse, if multiple signal features
are in strong correlation with a noise feature, while being only modestly
correlated with one another, this can result in a noise feature having a
distinctly larger FI score than any signal feature. Here we propose local
sample weighting (losaw) which can flexibly be integrated into many ML
algorithms to improve FI scores in the presence of feature correlation in the
training data. Our approach is motivated from inverse probability weighting in
causal inference and locally, within the ML model, uses a sample weighting
scheme to decorrelate a target feature from the remaining features. This
reduces model bias locally, whenever the effect of a potential signal feature
is evaluated and compared to others. Moreover, losaw comes with a natural
tuning parameter, the minimum effective sample size of the weighted population,
which corresponds to an interpretation-prediction-tradeoff, analog to a
bias-variance-tradeoff as for classical ML tuning parameters. We demonstrate
how losaw can be integrated within decision tree-based ML methods and within
mini-batch training of neural networks. We investigate losaw for random forest
and convolutional neural networks in a simulation study on settings showing
diverse correlation patterns. We found that losaw improves FI consistently.
Moreover, it often improves prediction accuracy for out-of-distribution, while
maintaining a similar accuracy for in-distribution test data.

</details>


### [281] [DP-SPRT: Differentially Private Sequential Probability Ratio Tests](https://arxiv.org/abs/2508.06377)
*Thomas Michel,Debabrota Basu,Emilie Kaufmann*

Main category: stat.ML

TL;DR: The paper revisits Wald's Sequential Probability Ratio Test (SPRT) under privacy constraints, presenting DP-SPRT, a privacy-preserving and near-optimal method for hypothesis testing.


<details>
  <summary>Details</summary>
Motivation: The existing sequential hypothesis tests lack mechanisms for achieving desired error probabilities while maintaining differential privacy. This study aims to fill this gap.

Method: The authors propose DP-SPRT, a test relying on a privacy-preserving mechanism called OutsideInterval, which processes sequential queries with noise from privacy settings and halts when query results leave a set interval.

Result: The paper provides theoretical bounds on error and sample complexity, validates the method for Laplace noise (pure DP) and Gaussian noise (RDP), and demonstrates its near-optimality in specific scenarios. Experiments showcase strong practical performance.

Conclusion: DP-SPRT is a practically effective and theoretically sound mechanism for sequential hypothesis testing under privacy constraints, with potential for broad applicability.

Abstract: We revisit Wald's celebrated Sequential Probability Ratio Test for sequential
tests of two simple hypotheses, under privacy constraints. We propose DP-SPRT,
a wrapper that can be calibrated to achieve desired error probabilities and
privacy constraints, addressing a significant gap in previous work. DP-SPRT
relies on a private mechanism that processes a sequence of queries and stops
after privately determining when the query results fall outside a predefined
interval. This OutsideInterval mechanism improves upon naive composition of
existing techniques like AboveThreshold, potentially benefiting other
sequential algorithms. We prove generic upper bounds on the error and sample
complexity of DP-SPRT that can accommodate various noise distributions based on
the practitioner's privacy needs. We exemplify them in two settings: Laplace
noise (pure Differential Privacy) and Gaussian noise (R\'enyi differential
privacy). In the former setting, by providing a lower bound on the sample
complexity of any $\epsilon$-DP test with prescribed type I and type II errors,
we show that DP-SPRT is near optimal when both errors are small and the two
hypotheses are close. Moreover, we conduct an experimental study revealing its
good practical performance.

</details>


<div id='nlin.CG'></div>

# nlin.CG [[Back]](#toc)

### [282] [Post-apocalyptic computing from cellular automata](https://arxiv.org/abs/2508.06035)
*Genaro J. Martinez,Andrew Adamatzky,Guanrong Chen*

Main category: nlin.CG

TL;DR: This paper reimagines algorithms as dynamic state-space configurations of cellular automata, offering a framework connecting computation to physical processes and enabling unconventional computing.


<details>
  <summary>Details</summary>
Motivation: To rethink traditional algorithms by exploring cellular automata as computational models and to connect computation with physical systems.

Method: Proposed representing algorithms through the dynamic state-space configurations of cellular automata, linking computational theory to physical and emergent processes.

Result: The study reveals the potential of engineered devices leveraging cellular automata for unconventional computing beyond silicon-based systems.

Conclusion: Cellular automata offer a transformative tool for advancing computational theory and practice, with implications for engineering adaptive, efficient, and novel computing systems.

Abstract: Cellular automata are arrays of finite state machines that can exist in a
finite number of states. These machines update their states simultaneously
based on specific local rules that govern their interactions. This framework
provides a simple yet powerful model for studying complex systems and emergent
behaviors. We revisit and reconsider the traditional notion of an algorithm,
proposing a novel perspective in which algorithms are represented through the
dynamic state-space configurations of cellular automata. By doing so, we
establish a conceptual framework that connects computation to physical
processes in a unique and innovative way. This approach not only enhances our
understanding of computation but also paves the way for the future development
of unconventional computing devices. Such devices could be engineered to
leverage the inherent computational capabilities of physical, chemical, and
biological substrates. This opens up new possibilities for designing systems
that are more efficient, adaptive, and capable of solving problems in ways that
traditional silicon-based computers cannot. The integration of cellular
automata into these domains highlights their potential as a transformative tool
in the ongoing evolution of computational theory and practice.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [283] [Evaluating Universal Machine Learning Force Fields Against Experimental Measurements](https://arxiv.org/abs/2508.05762)
*Sajid Mannan,Vaibhav Bihani,Carmelo Gonzales,Kin Long Kelvin Lee,Nitya Nand Gosvami,Sayan Ranu,Santiago Miret,N M Anoop Krishnan*

Main category: cond-mat.mtrl-sci

TL;DR: This paper introduces UniFFBench, a framework to evaluate universal machine learning force fields (UMLFFs) against experimental mineral structures. Findings show current UMLFFs fail to meet real-world requirements, revealing systematic limitations.


<details>
  <summary>Details</summary>
Motivation: Universal machine learning force fields have the potential to transform materials science but lack rigorous experimental validation to test their reliability beyond computational benchmarks.

Method: UniFFBench was used to assess six UMLFF models against experimental measurements of mineral structures, analyzing their density prediction, simulation stability, and representation of mechanical properties.

Result: The study found significant gaps between computational benchmark results and experimental performance. Density prediction errors exceeded practical thresholds, and the errors were influenced by dataset representation rather than modeling approaches.

Conclusion: While computational benchmarks are useful for comparisons, they overestimate UMLFF reliability in complex chemical spaces. UniFFBench establishes essential validation standards to address this gap and improve universal force field models.

Abstract: Universal machine learning force fields (UMLFFs) promise to revolutionize
materials science by enabling rapid atomistic simulations across the periodic
table. However, their evaluation has been limited to computational benchmarks
that may not reflect real-world performance. Here, we present UniFFBench, a
comprehensive framework for evaluating UMLFFs against experimental measurements
of ~1,500 carefully curated mineral structures spanning diverse chemical
environments, bonding types, structural complexity, and elastic properties. Our
systematic evaluation of six state-of-the-art UMLFFs reveals a substantial
reality gap: models achieving impressive performance on computational
benchmarks often fail when confronted with experimental complexity. Even the
best-performing models exhibit higher density prediction error than the
threshold required for practical applications. Most strikingly, we observe
disconnects between simulation stability and mechanical property accuracy, with
prediction errors correlating with training data representation rather than the
modeling method. These findings demonstrate that while current computational
benchmarks provide valuable controlled comparisons, they may overestimate model
reliability when extrapolated to experimentally complex chemical spaces.
Altogether, UniFFBench establishes essential experimental validation standards
and reveals systematic limitations that must be addressed to achieve truly
universal force field capabilities.

</details>


<div id='cs.SC'></div>

# cs.SC [[Back]](#toc)

### [284] [Tree-Based Deep Learning for Ranking Symbolic Integration Algorithms](https://arxiv.org/abs/2508.06383)
*Rashid Barket,Matthew England,Jürgen Gerhard*

Main category: cs.SC

TL;DR: The paper introduces a machine learning-based approach for symbolic indefinite integration in Computer Algebra Systems, aiming to choose efficient algorithms based on input complexity.


<details>
  <summary>Details</summary>
Motivation: Often, Computer Algebra Systems inefficiently select algorithms without considering problem-specific nuances, leading to suboptimal outcomes.

Method: The authors propose a machine learning framework with tree-based deep learning models. Their two-stage architecture first identifies applicable methods and then ranks them based on presentation complexity.

Result: Their model achieves 90% accuracy in method selection on a large test set and outperforms traditional approaches and built-in systems on independent benchmarks.

Conclusion: Using tree-based representations boosts accuracy in symbolic computation, and their framework is expected to generalize well across similar mathematical optimization problems.

Abstract: Symbolic indefinite integration in Computer Algebra Systems such as Maple
involves selecting the most effective algorithm from multiple available
methods. Not all methods will succeed for a given problem, and when several do,
the results, though mathematically equivalent, can differ greatly in
presentation complexity. Traditionally, this choice has been made with minimal
consideration of the problem instance, leading to inefficiencies.
  We present a machine learning (ML) approach using tree-based deep learning
models within a two-stage architecture: first identifying applicable methods
for a given instance, then ranking them by predicted output complexity.
Furthermore, we find representing mathematical expressions as tree structures
significantly improves performance over sequence-based representations, and our
two-stage framework outperforms alternative ML formulations.
  Using a diverse dataset generated by six distinct data generators, our models
achieve nearly 90% accuracy in selecting the optimal method on a 70,000 example
holdout test set. On an independent out-of-distribution benchmark from Maple's
internal test suite, our tree transformer model maintains strong
generalisation, outperforming Maple's built-in selector and prior ML
approaches.
  These results highlight the critical role of data representation and problem
framing in ML for symbolic computation, and we expect our methodology to
generalise effectively to similar optimisation problems in mathematical
software.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [285] [Voting-Based Semi-Parallel Proof-of-Work Protocol](https://arxiv.org/abs/2508.06489)
*Mustafa Doger,Sennur Ulukus*

Main category: cs.CR

TL;DR: This paper investigates vulnerabilities in existing parallel Proof-of-Work (PoW) protocols, introduces a voting-based semi-parallel PoW protocol, and demonstrates its advantages in security, throughput, and incentive compatibility.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address safety vulnerabilities, throughput limitations, and incentive compatibility challenges of Nakamoto consensus and existing parallel PoW protocols.

Method: The authors identify incentive attacks in existing parallel PoW, develop attack models, and propose a new voting-based semi-parallel PoW protocol with Markov decision process (MDP) analysis for performance and security evaluations.

Result: The proposed protocol mitigates incentive attack vulnerabilities, improves throughput, reduces transaction conflicts, and ensures a fair reward distribution.

Conclusion: The voting-based semi-parallel PoW protocol surpasses Nakamoto and existing parallel PoW protocols in practicality, security, and fairness.

Abstract: Parallel Proof-of-Work (PoW) protocols are suggested to improve the safety
guarantees, transaction throughput and confirmation latencies of Nakamoto
consensus. In this work, we first consider the existing parallel PoW protocols
and develop hard-coded incentive attack structures. Our theoretical results and
simulations show that the existing parallel PoW protocols are more vulnerable
to incentive attacks than the Nakamoto consensus, e.g., attacks have smaller
profitability threshold and they result in higher relative rewards. Next, we
introduce a voting-based semi-parallel PoW protocol that outperforms both
Nakamoto consensus and the existing parallel PoW protocols from most practical
perspectives such as communication overheads, throughput, transaction
conflicts, incentive compatibility of the protocol as well as a fair
distribution of transaction fees among the voters and the leaders. We use
state-of-the-art analysis to evaluate the consistency of the protocol and
consider Markov decision process (MDP) models to substantiate our claims about
the resilience of our protocol against incentive attacks.

</details>


### [286] [Adaptive Backtracking for Privacy Protection in Large Language Models](https://arxiv.org/abs/2508.06087)
*Zhihao Yao,Yuxuan Gu,Xiachong Feng,Weitao Ma,Bo Li,Xiaocheng Feng*

Main category: cs.CR

TL;DR: This paper addresses enterprise-oriented privacy issues in AI by proposing a mechanism called ABack and introducing a benchmark dataset PriGenQA.


<details>
  <summary>Details</summary>
Motivation: Current approaches mostly address user privacy, ignoring enterprise data leakage risks, especially with Retrieval-Augmented Generation models.

Method: The authors propose ABack, a mechanism using Hidden State Models for safe output rewriting and construct PriGenQA, a benchmark dataset for healthcare and finance privacy scenarios.

Result: ABack achieves a 15% improvement in privacy utility score over strong baselines while avoiding performance trade-offs.

Conclusion: The solutions presented successfully address enterprise privacy concerns without compromising model effectiveness and offer benchmarks for rigorous evaluation in sensitive industries.

Abstract: The preservation of privacy has emerged as a critical topic in the era of
artificial intelligence. However, current work focuses on user-oriented
privacy, overlooking severe enterprise data leakage risks exacerbated by the
Retrieval-Augmented Generation paradigm. To address this gap, our paper
introduces a novel objective: enterprise-oriented privacy concerns. Achieving
this objective requires overcoming two fundamental challenges: existing methods
such as data sanitization severely degrade model performance, and the field
lacks public datasets for evaluation. We address these challenges with several
solutions. (1) To prevent performance degradation, we propose ABack, a
training-free mechanism that leverages a Hidden State Model to pinpoint the
origin of a leakage intention and rewrite the output safely. (2) To solve the
lack of datasets, we construct PriGenQA, a new benchmark for enterprise privacy
scenarios in healthcare and finance. To ensure a rigorous evaluation, we move
beyond simple static attacks by developing a powerful adaptive attacker with
Group Relative Policy Optimization. Experiments show that against this superior
adversary, ABack improves the overall privacy utility score by up to 15\% over
strong baselines, avoiding the performance trade-offs of prior methods.

</details>


### [287] [Secure and Scalable Blockchain Voting: A Comparative Framework and the Role of Large Language Models](https://arxiv.org/abs/2508.05865)
*Kiana Kiashemshaki,Elvis Nnaemeka Chukwuani,Mohammad Jalili Torkamani,Negin Mahmoudi*

Main category: cs.CR

TL;DR: The paper investigates the use of blockchain technology in E-Voting, addressing challenges like scalability and privacy while leveraging large language models (LLMs) for advanced capabilities.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of real-world adoption of blockchain-based E-Voting, such as scalability, high computational demands, and privacy complexities, and to explore how LLMs can contribute to this domain.

Method: The authors provide a comparative analysis of blockchain architectures, consensus mechanisms, and cryptographic protocols, propose optimization strategies, and analyze the role of LLMs in smart contract generation and anomaly detection.

Result: The study establishes a foundation for secure and scalable blockchain E-Voting systems and suggests practical optimization strategies and the integration of LLM capabilities for improved performance.

Conclusion: This research helps advance blockchain-based E-Voting by offering a systematic framework, simulation-based validation, and potential for building intelligent systems with LLMs for national-scale deployments.

Abstract: Blockchain technology offers a promising foundation for modernizing E-Voting
systems by enhancing transparency, decentralization, and security. Yet,
real-world adoption remains limited due to persistent challenges such as
scalability constraints, high computational demands, and complex privacy
requirements. This paper presents a comparative framework for analyzing
blockchain-based E-Voting architectures, consensus mechanisms, and
cryptographic protocols. We examine the limitations of prevalent models like
Proof of Work, Proof of Stake, and Delegated Proof of Stake, and propose
optimization strategies that include hybrid consensus, lightweight
cryptography, and decentralized identity management. Additionally, we explore
the novel role of Large Language Models (LLMs) in smart contract generation,
anomaly detection, and user interaction. Our findings offer a foundation for
designing secure, scalable, and intelligent blockchain-based E-Voting systems
suitable for national-scale deployment. This work lays the groundwork for
building an end-to-end blockchain E-Voting prototype enhanced by LLM-guided
smart contract generation and validation, supported by a systematic framework
and simulation-based analysis.

</details>


### [288] [DINA: A Dual Defense Framework Against Internal Noise and External Attacks in Natural Language Processing](https://arxiv.org/abs/2508.05671)
*Ko-Wei Chuang,Hen-Hsen Huang,Tsai-Yen Li*

Main category: cs.CR

TL;DR: DINA, a novel framework, addresses dual adversarial threats for NLP systems by merging noisy-label learning and adversarial training, showing improved robustness and accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address emerging adversarial threats in NLP applications, specifically from external manipulations and internal label corruption in customer service and moderation applications.

Method: The authors introduce DINA, which combines advanced noisy-label learning methods and adversarial training into a unified framework tailored for NLP.

Result: Experiments on a real-world dataset from an online gaming service demonstrated that DINA significantly enhanced robustness and accuracy compared to baseline models.

Conclusion: DINA offers effective strategies to safeguard NLP systems against dual threats, ensuring fair and responsible AI deployment and highlighting the importance of robust defenses in adversarial scenarios.

Abstract: As large language models (LLMs) and generative AI become increasingly
integrated into customer service and moderation applications, adversarial
threats emerge from both external manipulations and internal label corruption.
In this work, we identify and systematically address these dual adversarial
threats by introducing DINA (Dual Defense Against Internal Noise and
Adversarial Attacks), a novel unified framework tailored specifically for NLP.
Our approach adapts advanced noisy-label learning methods from computer vision
and integrates them with adversarial training to simultaneously mitigate
internal label sabotage and external adversarial perturbations. Extensive
experiments conducted on a real-world dataset from an online gaming service
demonstrate that DINA significantly improves model robustness and accuracy
compared to baseline models. Our findings not only highlight the critical
necessity of dual-threat defenses but also offer practical strategies for
safeguarding NLP systems in realistic adversarial scenarios, underscoring
broader implications for fair and responsible AI deployment.

</details>


### [289] [Can LLMs effectively provide game-theoretic-based scenarios for cybersecurity?](https://arxiv.org/abs/2508.05670)
*Daniele Proverbio,Alessio Buscemi,Alessandro Di Stefano,The Anh Han,German Castignani,Pietro Liò*

Main category: cs.CR

TL;DR: The paper explores how well classical game theory captures the behavior of LLM-driven agents in cybersecurity contexts, highlighting language-based and agent-specific influences.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand whether classical game theory can effectively model the actions and biases of LLM-driven agents, which are increasingly relevant in cybersecurity.

Method: The researchers used a reproducible game-theoretic framework to analyze LLM-driven agents across two scenarios: one-shot zero-sum games and dynamic Prisoner's Dilemma, involving four LLMs in five languages.

Result: LLMs displayed sensitivity to personality traits, repeated rounds, and language choice, influencing the outcomes of the games. This variability poses risks for their indiscriminate use in cybersecurity.

Conclusion: Game theory frameworks can model LLM behavior but must account for variability arising from agent traits and language-dependent idiosyncrasies to ensure reliable cybersecurity applications.

Abstract: Game theory has long served as a foundational tool in cybersecurity to test,
predict, and design strategic interactions between attackers and defenders. The
recent advent of Large Language Models (LLMs) offers new tools and challenges
for the security of computer systems; In this work, we investigate whether
classical game-theoretic frameworks can effectively capture the behaviours of
LLM-driven actors and bots. Using a reproducible framework for game-theoretic
LLM agents, we investigate two canonical scenarios -- the one-shot zero-sum
game and the dynamic Prisoner's Dilemma -- and we test whether LLMs converge to
expected outcomes or exhibit deviations due to embedded biases. Our experiments
involve four state-of-the-art LLMs and span five natural languages, English,
French, Arabic, Vietnamese, and Mandarin Chinese, to assess linguistic
sensitivity. For both games, we observe that the final payoffs are influenced
by agents characteristics such as personality traits or knowledge of repeated
rounds. Moreover, we uncover an unexpected sensitivity of the final payoffs to
the choice of languages, which should warn against indiscriminate application
of LLMs in cybersecurity applications and call for in-depth studies, as LLMs
may behave differently when deployed in different countries. We also employ
quantitative metrics to evaluate the internal consistency and cross-language
stability of LLM agents, to help guide the selection of the most stable LLMs
and optimising models for secure applications.

</details>


### [290] [DMFI: Dual-Modality Fine-Tuning and Inference Framework for LLM-Based Insider Threat Detection](https://arxiv.org/abs/2508.05694)
*Kaichuan Kong,Dongjie Liu,Xiaobo Jin,Guanggang Geng,Zhiying Li,Jian Weng*

Main category: cs.CR

TL;DR: The paper introduces DMFI, a dual-modality framework for insider threat detection (ITD), combining semantic inference and behavior modeling to achieve state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Insider threat detection remains a complex challenge in cybersecurity due to the subtle and context-dependent nature of malicious behaviors; existing approaches lack semantic intent understanding and robust behavior modeling.

Method: DMFI processes raw logs into two views: semantic (via prompts for content-rich data) and behavioral (using a 4W-guided transformation). It employs two fine-tuned LLMs, fuses their outputs, and uses a discriminative strategy for imbalanced data.

Result: DMFI achieves superior detection accuracy on CERT r4.2 and r5.2 datasets, outperforming other state-of-the-art methods in robustness and scalability.

Conclusion: Combining LLM-based semantic inference with structured behavior modeling offers a scalable and effective approach to modern insider threat detection.

Abstract: Insider threat detection (ITD) poses a persistent and high-impact challenge
in cybersecurity due to the subtle, long-term, and context-dependent nature of
malicious insider behaviors. Traditional models often struggle to capture
semantic intent and complex behavior dynamics, while existing LLM-based
solutions face limitations in prompt adaptability and modality coverage. To
bridge this gap, we propose DMFI, a dual-modality framework that integrates
semantic inference with behavior-aware fine-tuning. DMFI converts raw logs into
two structured views: (1) a semantic view that processes content-rich artifacts
(e.g., emails, https) using instruction-formatted prompts; and (2) a behavioral
abstraction, constructed via a 4W-guided (When-Where-What-Which) transformation
to encode contextual action sequences. Two LoRA-enhanced LLMs are fine-tuned
independently, and their outputs are fused via a lightweight MLP-based decision
module. We further introduce DMFI-B, a discriminative adaptation strategy that
separates normal and abnormal behavior representations, improving robustness
under severe class imbalance. Experiments on CERT r4.2 and r5.2 datasets
demonstrate that DMFI outperforms state-of-the-art methods in detection
accuracy. Our approach combines the semantic reasoning power of LLMs with
structured behavior modeling, offering a scalable and effective solution for
real-world insider threat detection. Our work demonstrates the effectiveness of
combining LLM reasoning with structured behavioral modeling, offering a
scalable and deployable solution for modern insider threat detection.

</details>


### [291] [Towards Effective Offensive Security LLM Agents: Hyperparameter Tuning, LLM as a Judge, and a Lightweight CTF Benchmark](https://arxiv.org/abs/2508.05674)
*Minghao Shao,Nanda Rani,Kimberly Milner,Haoran Xi,Meet Udeshi,Saksham Aggarwal,Venkata Sai Charan Putrevu,Sandeep Kumar Shukla,Prashanth Krishnamurthy,Farshad Khorrami,Ramesh Karri,Muhammad Shafique*

Main category: cs.CR

TL;DR: The paper explores the application of advanced LLM systems in automating offensive security tasks for CTF challenges, introducing tools, metrics, and benchmarks to optimize performance.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the growing need to enhance cybersecurity automation and the transformative potential of LLM agents in solving challenging tasks like CTF competitions.

Method: The authors present CTFJudge for agent trajectory analysis and granular evaluation, a new metric CTF Competency Index (CCI) for partial correctness, and CTFTiny—a curated benchmark to evaluate agent performance across varied tasks.

Result: The study identifies optimal configurations for multi-agent setups and demonstrates improvements in task automation for various cybersecurity domains using their proposed tools and metrics.

Conclusion: The findings outline actionable strategies for optimizing LLM agent performance and provide open-source resources to advance research in automated cybersecurity solutions.

Abstract: Recent advances in LLM agentic systems have improved the automation of
offensive security tasks, particularly for Capture the Flag (CTF) challenges.
We systematically investigate the key factors that drive agent success and
provide a detailed recipe for building effective LLM-based offensive security
agents. First, we present CTFJudge, a framework leveraging LLM as a judge to
analyze agent trajectories and provide granular evaluation across CTF solving
steps. Second, we propose a novel metric, CTF Competency Index (CCI) for
partial correctness, revealing how closely agent solutions align with
human-crafted gold standards. Third, we examine how LLM hyperparameters, namely
temperature, top-p, and maximum token length, influence agent performance and
automated cybersecurity task planning. For rapid evaluation, we present
CTFTiny, a curated benchmark of 50 representative CTF challenges across binary
exploitation, web, reverse engineering, forensics, and cryptography. Our
findings identify optimal multi-agent coordination settings and lay the
groundwork for future LLM agent research in cybersecurity. We make CTFTiny open
source to public https://github.com/NYU-LLM-CTF/CTFTiny along with CTFJudge on
https://github.com/NYU-LLM-CTF/CTFJudge.

</details>


### [292] [Principle-Guided Verilog Optimization: IP-Safe Knowledge Transfer via Local-Cloud Collaboration](https://arxiv.org/abs/2508.05675)
*Jing Wang,Zheng Li,Lei Li,Fan He,Liyu Lin,Yao Lai,Yan Li,Xiaoyang Zeng,Yufeng Guo*

Main category: cs.CR

TL;DR: This paper introduces a secure edge-cloud collaborative framework using local and cloud-based large language models (LLMs) for optimizing Verilog code while preserving intellectual property (IP). The method ensures improved optimization success rates without leaking sensitive information.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the problem of intellectual property leakage when using powerful cloud-based LLMs for hardware design optimization and proposes a secure framework to balance optimization with IP protection.

Method: The method involves leveraging local small LLMs for secure comparative analysis to derive general optimization principles, which are then abstracted and passed to stronger cloud-based LLMs for further code improvement.

Result: The framework shows superior optimization outcomes, achieving success rates like 66.67% for power utilization, exceeding standalone models such as Deepseek-V3 (49.81%) and GPT-4o (55.81%).

Conclusion: The work establishes a novel paradigm in hardware optimization, combining performance boosts with strong IP protection protocols.

Abstract: Recent years have witnessed growing interest in adopting large language
models (LLMs) for Register Transfer Level (RTL) code optimization. While
powerful cloud-based LLMs offer superior optimization capabilities, they pose
unacceptable intellectual property (IP) leakage risks when processing
proprietary hardware designs. In this paper, we propose a new scenario where
Verilog code must be optimized for specific attributes without leaking
sensitive IP information. We introduce the first IP-preserving edge-cloud
collaborative framework that leverages the benefits of both paradigms. Our
approach employs local small LLMs (e.g., Qwen-2.5-Coder-7B) to perform secure
comparative analysis between paired high-quality target designs and novice
draft codes, yielding general design principles that summarize key insights for
improvements. These principles are then used to query stronger cloud LLMs
(e.g., Deepseek-V3) for targeted code improvement, ensuring that only
abstracted and IP-safe guidance reaches external services. Our experimental
results demonstrate that the framework achieves significantly higher
optimization success rates compared to baseline methods. For example, combining
Qwen-2.5-Coder-7B and Deepseek-V3 achieves a 66.67\% optimization success rate
for power utilization, outperforming Deepseek-V3 alone (49.81\%) and even
commercial models like GPT-4o (55.81\%). Further investigation of local and
cloud LLM combinations reveals that different model pairings exhibit varying
strengths for specific optimization objectives, with interesting trends
emerging when varying the number of comparative code pairs. Our work
establishes a new paradigm for secure hardware design optimization that
balances performance gains with IP protection.

</details>


### [293] [Adversarial Attacks on Reinforcement Learning-based Medical Questionnaire Systems: Input-level Perturbation Strategies and Medical Constraint Validation](https://arxiv.org/abs/2508.05677)
*Peizhuo Liu*

Main category: cs.CR

TL;DR: The paper examines vulnerabilities in RL-based medical questionnaire systems under adversarial attacks, achieving a 97.6% success rate for generating clinically plausible adversarial samples.


<details>
  <summary>Details</summary>
Motivation: To assess the safety and robustness of Reinforcement Learning (RL)-based medical questionnaire systems, especially under adversarial attack scenarios.

Method: The authors formulated the diagnostic process as a Markov Decision Process (MDP) and tested six major adversarial attack methods on samples from the NHIS dataset while ensuring medical plausibility using a validation framework with 247 constraints.

Result: Under adversarial attacks, diagnostic accuracy suffered significantly, with attack success rates ranging from 33.08% (FGSM) to 64.70% (AutoAttack).

Conclusion: Despite enforcing strict medical constraints, RL-based medical questionnaire systems remain vulnerable to adversarial attacks, raising safety and reliability concerns.

Abstract: RL-based medical questionnaire systems have shown great potential in medical
scenarios. However, their safety and robustness remain unresolved. This study
performs a comprehensive evaluation on adversarial attack methods to identify
and analyze their potential vulnerabilities. We formulate the diagnosis process
as a Markov Decision Process (MDP), where the state is the patient responses
and unasked questions, and the action is either to ask a question or to make a
diagnosis. We implemented six prevailing major attack methods, including the
Fast Gradient Signed Method (FGSM), Projected Gradient Descent (PGD), Carlini &
Wagner Attack (C&W) attack, Basic Iterative Method (BIM), DeepFool, and
AutoAttack, with seven epsilon values each. To ensure the generated adversarial
examples remain clinically plausible, we developed a comprehensive medical
validation framework consisting of 247 medical constraints, including
physiological bounds, symptom correlations, and conditional medical
constraints. We achieved a 97.6% success rate in generating clinically
plausible adversarial samples. We performed our experiment on the National
Health Interview Survey (NHIS) dataset (https://www.cdc.gov/nchs/nhis/), which
consists of 182,630 samples, to predict the participant's 4-year mortality
rate. We evaluated our attacks on the AdaptiveFS framework proposed in
arXiv:2004.00994. Our results show that adversarial attacks could significantly
impact the diagnostic accuracy, with attack success rates ranging from 33.08%
(FGSM) to 64.70% (AutoAttack). Our work has demonstrated that even under strict
medical constraints on the input, such RL-based medical questionnaire systems
still show significant vulnerabilities.

</details>


### [294] [Selection-Based Vulnerabilities: Clean-Label Backdoor Attacks in Active Learning](https://arxiv.org/abs/2508.05681)
*Yuhan Zhi,Longtian Wang,Xiaofei Xie,Chao Shen,Qiang Hu,Xiaohong Guan*

Main category: cs.CR

TL;DR: The paper introduces ALA, a framework revealing vulnerabilities in Active Learning by showcasing how acquisition functions can be exploited for poisoning attacks.


<details>
  <summary>Details</summary>
Motivation: Active Learning is widely adopted in resource-constrained scenarios, but its security vulnerabilities, particularly regarding acquisition functions, remain unexplored.

Method: ALA utilizes imperceptible poisoned inputs to manipulate acquisition functions and increase their selection probability for labeling.

Result: Experiments show ALA achieves up to 94% attack success rates with minimal poisoning budgets (0.5%-1.0%) while maintaining model utility and being undetectable to human annotators.

Conclusion: Active Learning acquisition functions are easily exploitable, highlighting the need for caution and security considerations when deploying in sensitive contexts.

Abstract: Active learning(AL), which serves as the representative label-efficient
learning paradigm, has been widely applied in resource-constrained scenarios.
The achievement of AL is attributed to acquisition functions, which are
designed for identifying the most important data to label. Despite this
success, one question remains unanswered: is AL safe? In this work, we
introduce ALA, a practical and the first framework to utilize the acquisition
function as the poisoning attack surface to reveal the weakness of active
learning. Specifically, ALA optimizes imperceptibly poisoned inputs to exhibit
high uncertainty scores, increasing their probability of being selected by
acquisition functions. To evaluate ALA, we conduct extensive experiments across
three datasets, three acquisition functions, and two types of clean-label
backdoor triggers. Results show that our attack can achieve high success rates
(up to 94%) even under low poisoning budgets (0.5%-1.0%) while preserving model
utility and remaining undetectable to human annotators. Our findings remind
active learning users: acquisition functions can be easily exploited, and
active learning should be deployed with caution in trusted data scenarios.

</details>


### [295] [Fact2Fiction: Targeted Poisoning Attack to Agentic Fact-checking System](https://arxiv.org/abs/2508.06059)
*Haorui He,Yupeng Li,Bin Benjamin Zhu,Dacheng Wen,Reynold Cheng,Francis C. M. Lau*

Main category: cs.CR

TL;DR: The paper introduces Fact2Fiction, a poisoning attack framework targeting autonomous LLM-based fact-checking systems, achieving higher attack success rates and exposing security gaps.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the potential security vulnerabilities in state-of-the-art LLM-based fact-checking systems, as these systems can amplify misinformation if compromised.

Method: Fact2Fiction mimics the claim decomposition strategy used by fact-checking systems and exploits system-generated justifications to craft malicious evidence that disrupt sub-claim verification processes.

Result: Fact2Fiction demonstrates 8.9%-21.2% higher attack success rates compared to prior attacks, revealing significant weaknesses in current fact-checking systems.

Conclusion: The findings emphasize the need for more robust defensive measures to secure LLM-based agentic fact-checking systems against such poisoning attacks.

Abstract: State-of-the-art fact-checking systems combat misinformation at scale by
employing autonomous LLM-based agents to decompose complex claims into smaller
sub-claims, verify each sub-claim individually, and aggregate the partial
results to produce verdicts with justifications (explanatory rationales for the
verdicts). The security of these systems is crucial, as compromised
fact-checkers, which tend to be easily underexplored, can amplify
misinformation. This work introduces Fact2Fiction, the first poisoning attack
framework targeting such agentic fact-checking systems. Fact2Fiction mirrors
the decomposition strategy and exploits system-generated justifications to
craft tailored malicious evidences that compromise sub-claim verification.
Extensive experiments demonstrate that Fact2Fiction achieves 8.9\%--21.2\%
higher attack success rates than state-of-the-art attacks across various
poisoning budgets. Fact2Fiction exposes security weaknesses in current
fact-checking systems and highlights the need for defensive countermeasures.

</details>


### [296] [MM-FusionNet: Context-Aware Dynamic Fusion for Multi-modal Fake News Detection with Large Vision-Language Models](https://arxiv.org/abs/2508.05684)
*Junhao He,Tianyu Liu,Jingyuan Zhao,Benjamin Turner*

Main category: cs.CR

TL;DR: MM-FusionNet employs Large Vision-Language Models and introduces dynamic mechanisms for effectively detecting multi-modal fake news, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: The paper addresses challenges in detecting multi-modal fake news, which involves deceptive combinations of text and images on social media, surpassing the limitations of traditional text-based methods.

Method: The proposed MM-FusionNet framework utilizes the Context-Aware Dynamic Fusion Module with bi-directional cross-modal attention and dynamic modal gating to adaptively fuse textual and visual information.

Result: MM-FusionNet achieved an F1-score of 0.938, outperforming current multi-modal baselines by 0.5% and single-modal methods significantly, while demonstrating robustness and near-human-level performance.

Conclusion: MM-FusionNet is a practical and interpretable solution for multi-modal fake news detection, offering high accuracy and adaptability for real-world applications.

Abstract: The proliferation of multi-modal fake news on social media poses a
significant threat to public trust and social stability. Traditional detection
methods, primarily text-based, often fall short due to the deceptive interplay
between misleading text and images. While Large Vision-Language Models (LVLMs)
offer promising avenues for multi-modal understanding, effectively fusing
diverse modal information, especially when their importance is imbalanced or
contradictory, remains a critical challenge. This paper introduces
MM-FusionNet, an innovative framework leveraging LVLMs for robust multi-modal
fake news detection. Our core contribution is the Context-Aware Dynamic Fusion
Module (CADFM), which employs bi-directional cross-modal attention and a novel
dynamic modal gating network. This mechanism adaptively learns and assigns
importance weights to textual and visual features based on their contextual
relevance, enabling intelligent prioritization of information. Evaluated on the
large-scale Multi-modal Fake News Dataset (LMFND) comprising 80,000 samples,
MM-FusionNet achieves a state-of-the-art F1-score of 0.938, surpassing existing
multi-modal baselines by approximately 0.5% and significantly outperforming
single-modal approaches. Further analysis demonstrates the model's dynamic
weighting capabilities, its robustness to modality perturbations, and
performance remarkably close to human-level, underscoring its practical
efficacy and interpretability for real-world fake news detection.

</details>


### [297] [Log2Sig: Frequency-Aware Insider Threat Detection via Multivariate Behavioral Signal Decomposition](https://arxiv.org/abs/2508.05696)
*Kaichuan Kong,Dongjie Liu,Xiaobo Jin,Zhiying Li,Guanggang Geng*

Main category: cs.CR

TL;DR: The paper presents Log2Sig, a novel anomaly detection framework that transforms system logs into frequency signals for detecting insider threats, achieving superior performance on two datasets.


<details>
  <summary>Details</summary>
Motivation: Insider threats are challenging to detect due to the deceptive similarity between malicious and legitimate user behaviors. Existing approaches fail to capture complex frequency dynamics and multi-scale patterns in user behavior.

Method: Log2Sig converts user logs into multivariate behavioral frequency signals, utilizes Multivariate Variational Mode Decomposition (MVMD) to derive behavioral fluctuations across scales, and employs a Mamba-based encoder alongside frequency component projection for capturing and fusing behavioral representations for anomaly detection.

Result: Log2Sig demonstrated significant improvements in accuracy and F1 score compared to state-of-the-art baselines when tested on the CERT r4.2 and r5.2 datasets.

Conclusion: The dual-view modeling of user logs in Log2Sig provides a robust framework for insider threat detection, effectively addressing limitations in frequency dynamics and scaling for detection.

Abstract: Insider threat detection presents a significant challenge due to the
deceptive nature of malicious behaviors, which often resemble legitimate user
operations. However, existing approaches typically model system logs as flat
event sequences, thereby failing to capture the inherent frequency dynamics and
multiscale disturbance patterns embedded in user behavior. To address these
limitations, we propose Log2Sig, a robust anomaly detection framework that
transforms user logs into multivariate behavioral frequency signals,
introducing a novel representation of user behavior. Log2Sig employs
Multivariate Variational Mode Decomposition (MVMD) to extract Intrinsic Mode
Functions (IMFs), which reveal behavioral fluctuations across multiple temporal
scales. Based on this, the model further performs joint modeling of behavioral
sequences and frequency-decomposed signals: the daily behavior sequences are
encoded using a Mamba-based temporal encoder to capture long-term dependencies,
while the corresponding frequency components are linearly projected to match
the encoder's output dimension. These dual-view representations are then fused
to construct a comprehensive user behavior profile, which is fed into a
multilayer perceptron for precise anomaly detection. Experimental results on
the CERT r4.2 and r5.2 datasets demonstrate that Log2Sig significantly
outperforms state-of-the-art baselines in both accuracy and F1 score.

</details>


### [298] [Leveraging large language models for SQL behavior-based database intrusion detection](https://arxiv.org/abs/2508.05690)
*Meital Shlezinger,Shay Akirav,Lei Zhou,Liang Guo,Avi Kessel,Guoliang Li*

Main category: cs.CR

TL;DR: The paper presents a two-tier anomaly detection system for SQL using DistilBERT to improve precision in identifying internal and external database threats.


<details>
  <summary>Details</summary>
Motivation: To address the rising threat of abnormal database access behaviors, particularly from internal and external masqueraders, and overcome the limitations of current detection methods, which lack granularity and fail to distinguish between normal and anomalous behaviors.

Method: The paper proposes a combined unsupervised and supervised machine learning approach using DistilBERT for anomaly detection. Unsupervised methods focus on out-of-scope queries via ensemble detectors, while supervised methods deal with in-scope queries by fine-tuning role-labeled classification models.

Result: The proposed system effectively identifies anomalous behavior in SQL databases with high accuracy, even on limited labeled data.

Conclusion: The two-tier method offers a robust solution to protect critical databases from advanced internal and external threats by leveraging advanced transformer-based models and reducing the reliance on data labeling.

Abstract: Database systems are extensively used to store critical data across various
domains. However, the frequency of abnormal database access behaviors, such as
database intrusion by internal and external attacks, continues to rise.
Internal masqueraders often have greater organizational knowledge, making it
easier to mimic employee behavior effectively. In contrast, external
masqueraders may behave differently due to their lack of familiarity with the
organization. Current approaches lack the granularity needed to detect
anomalies at the operational level, frequently misclassifying entire sequences
of operations as anomalies, even though most operations are likely to represent
normal behavior. On the other hand, some anomalous behaviors often resemble
normal activities, making them difficult for existing detection methods to
identify. This paper introduces a two-tiered anomaly detection approach for
Structured Query Language (SQL) using the Bidirectional Encoder Representations
from Transformers (BERT) model, specifically DistilBERT, a more efficient,
pre-trained version. Our method combines both unsupervised and supervised
machine learning techniques to accurately identify anomalous activities while
minimizing the need for data labeling. First, the unsupervised method uses
ensemble anomaly detectors that flag embedding vectors distant from learned
normal patterns of typical user behavior across the database (out-of-scope
queries). Second, the supervised method uses fine-tuned transformer-based
models to detect internal attacks with high precision (in-scope queries), using
role-labeled classification, even on limited labeled SQL data. Our findings
make a significant contribution by providing an effective solution for
safeguarding critical database systems from sophisticated threats.

</details>


### [299] [ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls](https://arxiv.org/abs/2508.06457)
*Sanket Badhe*

Main category: cs.CR

TL;DR: ScamAgent demonstrates the misuse potential of LLMs in generating convincing scam dialogues and voice calls using AI tools, revealing weaknesses in current safety mechanisms.


<details>
  <summary>Details</summary>
Motivation: To investigate and highlight the vulnerabilities of LLMs in simulated fraud scenarios and the ineffectiveness of existing safeguard mechanisms.

Method: Developed ScamAgent, an LLM autonomous multi-turn agent, which creates scam call scripts, maintains dialogue memory, adapts to simulated user responses, and uses modern text-to-speech systems to convert scripts into lifelike voice calls.

Result: Demonstrated that current LLM guardrails are easily bypassed through incremental prompts or disguised inputs, creating realistic and highly persuasive scam scenarios.

Conclusion: The study underscores the urgent need for improved multi-turn safety mechanisms, agent-level controls, and methods to detect sophisticated conversational deception driven by AI.

Abstract: Large Language Models (LLMs) have demonstrated impressive fluency and
reasoning capabilities, but their potential for misuse has raised growing
concern. In this paper, we present ScamAgent, an autonomous multi-turn agent
built on top of LLMs, capable of generating highly realistic scam call scripts
that simulate real-world fraud scenarios. Unlike prior work focused on
single-shot prompt misuse, ScamAgent maintains dialogue memory, adapts
dynamically to simulated user responses, and employs deceptive persuasion
strategies across conversational turns. We show that current LLM safety
guardrails, including refusal mechanisms and content filters, are ineffective
against such agent-based threats. Even models with strong prompt-level
safeguards can be bypassed when prompts are decomposed, disguised, or delivered
incrementally within an agent framework. We further demonstrate the
transformation of scam scripts into lifelike voice calls using modern
text-to-speech systems, completing a fully automated scam pipeline. Our
findings highlight an urgent need for multi-turn safety auditing, agent-level
control frameworks, and new methods to detect and disrupt conversational
deception powered by generative AI.

</details>


### [300] [MambaITD: An Efficient Cross-Modal Mamba Network for Insider Threat Detection](https://arxiv.org/abs/2508.05695)
*Kaichuan Kong,Dongjie Liu,Xiaobo Jin,Zhiying Li,Guanggang Geng,Jian Weng*

Main category: cs.CR

TL;DR: The paper introduces MambaITD, a novel framework for insider threat detection, addressing challenges like temporal dynamics, computational bottlenecks, and cross-modal data integration.


<details>
  <summary>Details</summary>
Motivation: Existing detection methods fail to adequately address insider threats due to insufficient modeling of time dynamics, real-time inefficiencies, and data silo issues.

Method: The framework includes multi-source log preprocessing for heterogeneous data alignment, a Mamba encoder for capturing behavioral and interval dependencies, and an adaptive threshold optimization method to improve anomaly detection and handle class imbalances.

Result: MambaITD demonstrates superior modeling efficiency, feature fusion, and outperforms Transformer-based methods in detecting insider threats.

Conclusion: The proposed MambaITD framework offers a robust and efficient solution for insider threat detection, overcoming limitations of traditional methods.

Abstract: Enterprises are facing increasing risks of insider threats, while existing
detection methods are unable to effectively address these challenges due to
reasons such as insufficient temporal dynamic feature modeling, computational
efficiency and real-time bottlenecks and cross-modal information island
problem. This paper proposes a new insider threat detection framework MambaITD
based on the Mamba state space model and cross-modal adaptive fusion. First,
the multi-source log preprocessing module aligns heterogeneous data through
behavioral sequence encoding, interval smoothing, and statistical feature
extraction. Second, the Mamba encoder models long-range dependencies in
behavioral and interval sequences, and combines the sequence and statistical
information dynamically in combination with the gated feature fusion mechanism.
Finally, we propose an adaptive threshold optimization method based on
maximizing inter-class variance, which dynamically adjusts the decision
threshold by analyzing the probability distribution, effectively identifies
anomalies, and alleviates class imbalance and concept drift. Compared with
traditional methods, MambaITD shows significant advantages in modeling
efficiency and feature fusion capabilities, outperforming Transformer-based
methods, and provides a more effective solution for insider threat detection.

</details>


### [301] [Universally Unfiltered and Unseen:Input-Agnostic Multimodal Jailbreaks against Text-to-Image Model Safeguards](https://arxiv.org/abs/2508.05658)
*Song Yan,Hui Wei,Jinlong Fei,Guoliang Yang,Zhengyu Zhao,Zheng Wamg*

Main category: cs.CR

TL;DR: The U3-Attack method bypasses safety measures in T2I models by using optimized adversarial patches and paraphrases, achieving significantly higher success rates compared to existing approaches.


<details>
  <summary>Details</summary>
Motivation: Addressing security vulnerabilities in text-to-image (T2I) safeguards, which face issues of scalability and inefficiency in current jailbreak methods.

Method: Development of the U3-Attack, which utilizes optimized adversarial patches on image backgrounds and safe paraphrases to bypass both prompt filters and safety checkers in T2I models.

Result: Experimental results show U3-Attack achieves nearly four times higher success rates than the previous best method, MMA-Diffusion, particularly in commercial models like Runway-inpainting.

Conclusion: The U3-Attack offers a superior, scalable solution for testing security weaknesses in T2I safeguards, highlighting the need for stronger protection mechanisms.

Abstract: Various (text) prompt filters and (image) safety checkers have been
implemented to mitigate the misuse of Text-to-Image (T2I) models in creating
Not-Safe-For-Work (NSFW) content.In order to expose potential security
vulnerabilities of such safeguards, multimodal jailbreaks have been
studied.However, existing jailbreaks are limited to prompt-specific and
image-specific perturbations, which suffer from poor scalability and
time-consuming optimization.To address these limitations, we propose
Universally Unfiltered and Unseen (U3)-Attack, a multimodal jailbreak attack
method against T2I safeguards.Specifically, U3-Attack optimizes an adversarial
patch on the image background to universally bypass safety checkers and
optimizes a safe paraphrase set from a sensitive word to universally bypass
prompt filters while eliminating redundant computations.Extensive experimental
results demonstrate the superiority of our U3-Attack on both open-source and
commercial T2I models.For example, on the commercial Runway-inpainting model
with both prompt filter and safety checker, our U3-Attack achieves $~4\times$
higher success rates than the state-of-the-art multimodal jailbreak attack,
MMA-Diffusion.Content Warning: This paper includes examples of NSFW content.

</details>


### [302] [Anti-Tamper Protection for Unauthorized Individual Image Generation](https://arxiv.org/abs/2508.06325)
*Zelin Li,Ruohan Zong,Yifan Liu,Ruichen Yao,Yaokun Liu,Yang Zhang,Dong Wang*

Main category: cs.CR

TL;DR: The paper introduces Anti-Tamper Perturbation (ATP), a perturbation algorithm designed to protect images from forgery attacks and detect tampering caused by purification methods.


<details>
  <summary>Details</summary>
Motivation: Growing concerns about forgery attacks undermining portrait rights and privacy in the age of personalized image generation technologies.

Method: ATP employs perturbations applied in the frequency domain, separating protection perturbation (to combat forgery) and authorization perturbation (to detect tampering). A mask ensures they function without interference.

Result: ATP successfully defended against various forgery attack scenarios and remained effective against purification attempts in extensive experiments.

Conclusion: The ATP method offers a robust and tamper-proof solution to protect portrait rights and privacy, addressing challenges posed by purification and forgery attacks.

Abstract: With the advancement of personalized image generation technologies, concerns
about forgery attacks that infringe on portrait rights and privacy are growing.
To address these concerns, protection perturbation algorithms have been
developed to disrupt forgery generation. However, the protection algorithms
would become ineffective when forgery attackers apply purification techniques
to bypass the protection. To address this issue, we present a novel approach,
Anti-Tamper Perturbation (ATP). ATP introduces a tamper-proof mechanism within
the perturbation. It consists of protection and authorization perturbations,
where the protection perturbation defends against forgery attacks, while the
authorization perturbation detects purification-based tampering. Both
protection and authorization perturbations are applied in the frequency domain
under the guidance of a mask, ensuring that the protection perturbation does
not disrupt the authorization perturbation. This design also enables the
authorization perturbation to be distributed across all image pixels,
preserving its sensitivity to purification-based tampering. ATP demonstrates
its effectiveness in defending forgery attacks across various attack settings
through extensive experiments, providing a robust solution for protecting
individuals' portrait rights and privacy. Our code is available at:
https://github.com/Seeyn/Anti-Tamper-Perturbation .

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [303] [On Approximate MMS Allocations on Restricted Graph Classes](https://arxiv.org/abs/2508.06343)
*Václav Blažej,Michał Dębski ad Zbigniew Lonc,Marta Piecyk,Paweł Rzążewski*

Main category: cs.DM

TL;DR: The paper investigates fair division of indivisible goods represented as connected subgraphs within various graph classes, specifically under maximin share fairness. The authors establish approximate fairness guarantees for several graph classes, including block graphs, cacti, and split graphs.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of ensuring fair division of indivisible goods when connectivity constraints apply, particularly under the approximate maximin share fairness criterion, which has not been fully resolved for all graph classes.

Method: The authors systematically analyze the properties of restricted classes of graphs, developing techniques to prove the existence of approximate allocations for those classes.

Result: They demonstrate that approximate fair allocations satisfying a fraction of the maximin share value are possible for several graph classes, including block graphs, cacti, complete multipartite graphs, and split graphs.

Conclusion: The findings advance the understanding of fair division under connectivity constraints, resolving open problems for specific graph classes and expanding the scope of guaranteed fairness.

Abstract: We study the problem of fair division of a set of indivisible goods with
connectivity constraints. Specifically, we assume that the goods are
represented as vertices of a connected graph, and sets of goods allocated to
the agents are connected subgraphs of this graph. We focus on the
widely-studied maximin share criterion of fairness. It has been shown that an
allocation satisfying this criterion may not exist even without connectivity
constraints, i.e., if the graph of goods is complete. In view of this, it is
natural to seek approximate allocations that guarantee each agent a connected
bundle of goods with value at least a constant fraction of the maximin share
value to the agent. It is known that for some classes of graphs, such as
complete graphs, cycles, and $d$-claw-free graphs for any fixed $d$, such
approximate allocations indeed exist. However, it is an open problem whether
they exist for the class of all graphs.
  In this paper, we continue the systematic study of the existence of
approximate allocations on restricted graph classes. In particular, we show
that such allocations exist for several well-studied classes, including block
graphs, cacti, complete multipartite graphs, and split graphs.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [304] [Numerical Considerations in Weighted Model Counting](https://arxiv.org/abs/2508.06264)
*Randal E. Bryant*

Main category: math.NA

TL;DR: This paper addresses the challenges of weighted model counting and proposes methods to achieve guaranteed precision while maintaining efficiency.


<details>
  <summary>Details</summary>
Motivation: Current methods for weighted model counting face challenges such as inaccurate results with floating-point arithmetic or high computational costs with rational arithmetic.

Method: The paper introduces extended-range double (ERD) numeric representation for nonnegative weights and a combination of interval floating-point arithmetic and rational arithmetic for mixed weights.

Result: The proposed techniques efficiently compute weighted model counts with user-specified precision, overcoming underflow and overflow issues.

Conclusion: Using tailored numeric representations ensures robust weighted model counting with improved efficiency and guaranteed precision.

Abstract: Weighted model counting computes the sum of the rational-valued weights
associated with the satisfying assignments for a Boolean formula, where the
weight of an assignment is given by the product of the weights assigned to the
positive and negated variables comprising the assignment. Weighted model
counting finds applications across a variety of domains including probabilistic
reasoning and quantitative risk assessment.
  Most weighted model counting programs operate by (explicitly or implicitly)
converting the input formula into a form that enables arithmetic evaluation,
using multiplication for conjunctions and addition for disjunctions. Performing
this evaluation using floating-point arithmetic can yield inaccurate results,
and it cannot quantify the level of precision achieved. Computing with rational
arithmetic gives exact results, but it is costly in both time and space.
  This paper describes how to combine multiple numeric representations to
efficiently compute weighted model counts that are guaranteed to achieve a
user-specified precision. When all weights are nonnegative, we prove that the
precision loss of arithmetic evaluation using floating-point arithmetic can be
tightly bounded. We show that supplementing a standard IEEE double-precision
representation with a separate 64-bit exponent, a format we call extended-range
double (ERD), avoids the underflow and overflow issues commonly encountered in
weighted model counting. For problems with mixed negative and positive weights,
we show that a combination of interval floating-point arithmetic and rational
arithmetic can achieve the twin goals of efficiency and guaranteed precision.
For our evaluations, we have devised especially challenging formulas and weight
assignments, demonstrating the robustness of our approach.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [305] [Moment Estimate and Variational Approach for Learning Generalized Diffusion with Non-gradient Structures](https://arxiv.org/abs/2508.01854)
*Fanze Kong,Chen-Chih Lai,Yubin Lu*

Main category: physics.comp-ph

TL;DR: This paper introduces a framework to identify governing laws in non-gradient generalized diffusions using a two-stage method that combines dissipation laws and first-moment evolution.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the ability to recover governing laws in generalized diffusion processes that include non-gradient components, which are challenging to model and analyze due to their complexity.

Method: The framework incorporates energy dissipation laws with a physically consistent penalty and first-moment evolution in a two-stage learning process to recover pseudo-potential and rotation in non-gradient drift dynamics.

Result: The method is successfully applied to scenarios with dissipation-rotation dynamics, rough pseudo-potentials, and noisy data, demonstrating its effectiveness in learning these physical processes.

Conclusion: The proposed two-stage method proves to be effective for identifying physical laws in complex and noisy non-gradient generalized diffusions.

Abstract: This paper proposes a data-driven learning framework for identifying
governing laws of generalized diffusions with non-gradient components. By
combining energy dissipation laws with a physically consistent penalty and
first-moment evolution, we design a two-stage method to recover the
pseudo-potential and rotation in the pointwise orthogonal decomposition of a
class of non-gradient drifts in generalized diffusions. Our two-stage method is
applied to complex generalized diffusion processes including
dissipation-rotation dynamics, rough pseudo-potentials and noisy data.
Representative numerical experiments demonstrate the effectiveness of our
approach for learning physical laws in non-gradient generalized diffusions.

</details>


### [306] [Hybrid Physics-Machine Learning Models for Quantitative Electron Diffraction Refinements](https://arxiv.org/abs/2508.05908)
*Shreshth A. Malik,Tiarnan A. S. Doherty,Benjamin Colmey,Stephen J. Roberts,Yarin Gal,Paul A. Midgley*

Main category: physics.comp-ph

TL;DR: The paper introduces a hybrid physics-machine learning framework that combines differentiable physical simulations with neural networks to improve high-fidelity quantitative crystal structure refinements in electron microscopy.


<details>
  <summary>Details</summary>
Motivation: Real-world experimental effects in electron microscopy are difficult to model analytically despite theoretical descriptions of physical interactions.

Method: The framework uses automatic differentiation for gradient-based joint optimization of physical parameters and neural network components. It is applied to 3D electron diffraction structure refinement, learning complex experimental variables directly from the data.

Result: The proposed approach achieves state-of-the-art refinement performance, recovering accurate atomic positions, thermal displacements, and thickness profiles in both synthetic and experimental data.

Conclusion: The hybrid framework provides a powerful paradigm for enhancing quantitative electron microscopy analysis and can be extended to incorporate additional physical phenomena and other microscopy techniques.

Abstract: High-fidelity electron microscopy simulations required for quantitative
crystal structure refinements face a fundamental challenge: while physical
interactions are well-described theoretically, real-world experimental effects
are challenging to model analytically. To address this gap, we present a novel
hybrid physics-machine learning framework that integrates differentiable
physical simulations with neural networks. By leveraging automatic
differentiation throughout the simulation pipeline, our method enables
gradient-based joint optimization of physical parameters and neural network
components representing experimental variables, offering superior scalability
compared to traditional second-order methods. We demonstrate this framework
through application to three-dimensional electron diffraction (3D-ED) structure
refinement, where our approach learns complex thickness distributions directly
from diffraction data rather than relying on simplified geometric models. This
method achieves state-of-the-art refinement performance across synthetic and
experimental datasets, recovering atomic positions, thermal displacements, and
thickness profiles with high fidelity. The modular architecture proposed can
naturally be extended to accommodate additional physical phenomena and extended
to other electron microscopy techniques. This establishes differentiable hybrid
modeling as a powerful new paradigm for quantitative electron microscopy, where
experimental complexities have historically limited analysis.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [307] [Quantum Resource Management in the NISQ Era: Implications and Perspectives from Software Engineering](https://arxiv.org/abs/2508.05697)
*Marcos Guillermo Lammers,Federico Hernán Holik,Alejandro Fernández*

Main category: quant-ph

TL;DR: Quantum computers in the NISQ era face hardware limitations like limited qubits and high error rates. This paper explores efficient quantum resource management for better algorithm deployment.


<details>
  <summary>Details</summary>
Motivation: Current NISQ devices have significant physical limitations, prompting the need for improved resource management to unlock their potential for quantum computing.

Method: The paper analyzes the role of quantum resources in deploying quantum algorithms and highlights their implications for quantum software engineering.

Result: The study strengthens the concept of Quantum Resource Estimation (QRE) and offers insights into developing scalable and reliable quantum software.

Conclusion: Efficient resource management can mitigate NISQ hardware limitations, advancing quantum software development and highlighting the importance of Quantum Resource Estimation.

Abstract: Quantum computers represent a radical technological breakthrough in
information processing by leveraging the principles of quantum mechanics to
solve highly complex problems beyond the reach of classical systems. However,
in the current NISQ era (noisy intermediate-scale quantum devices), the
available hardware presents several limitations, such as a limited number of
qubits, high error rates, and short coherence times. Efficient management of
quantum resources, both physical and logical, is especially relevant in the
design and deployment of quantum algorithms. In this paper, we analyze the role
of resources in current uses of NISQ devices, identifying their relevance and
implications for quantum software engineering. With this contribution, we aim
to strengthen the field of Quantum Resource Estimation (QRE) and move toward
scalable and reliable quantum software development

</details>


### [308] [MPS-JuliQAOA: User-friendly, Scalable MPS-based Simulation for Quantum Optimization](https://arxiv.org/abs/2508.05883)
*Sean Feeney,Reuben Tate,John Golden,Stephan Eidenbenz*

Main category: quant-ph

TL;DR: The paper introduces MPS-JuliQAOA, an open-source simulator for Quantum Approximate Optimization Algorithm (QAOA) problems utilizing Matrix Product State (MPS) methods for scalability.


<details>
  <summary>Details</summary>
Motivation: To provide an accessible, scalable tool for simulating QAOA problems, addressing challenges in parameter finding and the computational scalability of existing methods.

Method: Employing Julia-language constructs and the ITensor package with MPS techniques, the tool supports QAOA simulation for large-scale problems of up to 512 qubits and multiple simulation rounds.

Result: Achieves simulation scalability for 512 qubits and 20 rounds on the benchmark MaxCut problem, with built-in parameter finding capabilities for enhanced usability.

Conclusion: MPS-JuliQAOA simplifies QAOA simulation for users without knowledge of MPS or advanced differentiation techniques, while proving efficient in runtime, memory usage, and accuracy tradeoffs.

Abstract: We present the MPS-JuliQAOA simulator, a user-friendly, open-source tool to
simulate the Quantum Approximate Optimization Algorithm (QAOA) of any
optimization problem that can be expressed as diagonal Hamiltonian. By
leveraging Julia-language constructs and the ITensor package to implement a
Matrix Product State (MPS) approach to simulating QAOA, MPS-Juli-QAOA
effortlessly scales to 512 qubits and 20 simulation rounds on the standard
de-facto benchmark 3-regular MaxCut QAOA problem. MPS-JuliQAOA also has
built-in parameter finding capabilities, which is a crucial performance aspect
of QAOA. We illustrate through examples that the user does not need to know MPS
principles or complex automatic differentiation techniques to use MPS-JuliQAOA.
We study the scalability of our tool with respect to runtime, memory usage and
accuracy tradeoffs. Code available at
https://github.com/lanl/JuliQAOA.jl/tree/mps.

</details>


### [309] [Enhancing the Scalability of Classical Surrogates for Real-World Quantum Machine Learning Applications](https://arxiv.org/abs/2508.06131)
*Philip Anton Hernicht,Alona Sakhnenko,Corey O'Meara,Giorgio Cortiana,Jeanette Miriam Lorenz*

Main category: quant-ph

TL;DR: This paper proposes using classical surrogates to make quantum machine learning (QML) solutions deployable on classical devices, addressing deployment challenges due to limited quantum hardware access.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between the potential of quantum machine learning and limited access to quantum hardware, enabling industrial adoption.

Method: The authors propose an optimized pipeline for generating classical surrogates for quantum models, minimizing computational demands compared to previous methods and employing rigorous testing on quantum hardware and simulations.

Result: Their method achieves high accuracy in energy demand forecasting while scaling computational resource requirements linearly, making large-scale surrogate generation feasible.

Conclusion: The proposed lightweight classical surrogate approach accelerates the integration of quantum solutions into industry and aids in exploring practical quantum advantages empirically.

Abstract: Quantum machine learning (QML) presents potential for early industrial
adoption, yet limited access to quantum hardware remains a significant
bottleneck for deployment of QML solutions. This work explores the use of
classical surrogates to bypass this restriction, which is a technique that
allows to build a lightweight classical representation of a (trained) quantum
model, enabling to perform inference on entirely classical devices. We reveal
prohibiting high computational demand associated with previously proposed
methods for generating classical surrogates from quantum models, and propose an
alternative pipeline enabling generation of classical surrogates at a larger
scale than was previously possible. Previous methods required at least a
high-performance computing (HPC) system for quantum models of below industrial
scale (ca. 20 qubits), which raises questions about its practicality. We
greatly minimize the redundancies of the previous approach, utilizing only a
minute fraction of the resources previously needed. We demonstrate the
effectiveness of our method on a real-world energy demand forecasting problem,
conducting rigorous testing of performance and computation demand in both
simulations and on quantum hardware. Our results indicate that our method
achieves high accuracy on the testing dataset while its computational resource
requirements scale linearly rather than exponentially. This work presents a
lightweight approach to transform quantum solutions into classically deployable
versions, facilitating faster integration of quantum technology in industrial
settings. Furthermore, it can serve as a powerful research tool in search
practical quantum advantage in an empirical setup.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [310] [A Physiologically-Constrained Neural Network Digital Twin Framework for Replicating Glucose Dynamics in Type 1 Diabetes](https://arxiv.org/abs/2508.05705)
*Valentina Roquemen-Echeverri,Taisa Kushner,Peter G. Jacobs,Clara Mosquera-Lopez*

Main category: q-bio.QM

TL;DR: The paper presents physiologically-constrained neural network digital twins to simulate glucose dynamics in type 1 diabetes, ensuring physiologically consistent and individually personalized modeling.


<details>
  <summary>Details</summary>
Motivation: To improve the simulation of glucose dynamics in T1D by addressing gaps in existing models that overlook key physiological aspects and personalization potential.

Method: The authors designed a physiologically-aligned neural network model verified using ordinary differential equations, then individualized models incorporating personal data and contextual information.

Result: Using real-world T1D data, simulated glucose profiles matched observed data with clinically equivalent outcomes for time in range, time below range, and time above range.

Conclusion: The framework successfully integrates personalized modeling, allowing in silico testing of treatments, insulin optimization, and blending physics-based and data-driven approaches.

Abstract: Simulating glucose dynamics in individuals with type 1 diabetes (T1D) is
critical for developing personalized treatments and supporting data-driven
clinical decisions. Existing models often miss key physiological aspects and
are difficult to individualize. Here, we introduce physiologically-constrained
neural network (NN) digital twins to simulate glucose dynamics in T1D. To
ensure interpretability and physiological consistency, we first build a
population-level NN state-space model aligned with a set of ordinary
differential equations (ODEs) describing glucose regulation. This model is
formally verified to conform to known T1D dynamics. Digital twins are then
created by augmenting the population model with individual-specific models,
which include personal data, such as glucose management and contextual
information, capturing both inter- and intra-individual variability. We
validate our approach using real-world data from the T1D Exercise Initiative
study. Two weeks of data per participant were split into 5-hour sequences and
simulated glucose profiles were compared to observed ones. Clinically relevant
outcomes were used to assess similarity via paired equivalence t-tests with
predefined clinical equivalence margins. Across 394 digital twins, glucose
outcomes were equivalent between simulated and observed data: time in range
(70-180 mg/dL) was 75.1$\pm$21.2% (simulated) vs. 74.4$\pm$15.4% (real;
P<0.001); time below range (<70 mg/dL) 2.5$\pm$5.2% vs. 3.0$\pm$3.3% (P=0.022);
and time above range (>180 mg/dL) 22.4$\pm$22.0% vs. 22.6$\pm$15.9% (P<0.001).
Our framework can incorporate unmodeled factors like sleep and activity while
preserving key dynamics. This approach enables personalized in silico testing
of treatments, supports insulin optimization, and integrates physics-based and
data-driven modeling. Code: https://github.com/mosqueralopez/T1DSim_AI

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [311] [Identifiability of the minimum-trace directed acyclic graph and hill climbing algorithms without strict local optima under weakly increasing error variances](https://arxiv.org/abs/2508.05706)
*Hyunwoong Chang,Jaehoan Kim*

Main category: stat.CO

TL;DR: The paper establishes the identifiability of true DAGs in Gaussian linear models and enhances algorithmic understanding, while presenting computational insights about neighborhood search methods.


<details>
  <summary>Details</summary>
Motivation: To address identifiability issues in Gaussian linear structural equation models and connect existing frameworks with computational insights.

Method: By proving minimum-trace DAG identifiability under weakly increasing error variances and analyzing algorithmic behavior using hill climbing with R2R neighborhoods.

Result: True DAGs are identifiable with the proposed approach, and R2R neighborhoods avoid strict local optima, outperforming other neighborhood search methods in simulations.

Conclusion: The method links theory and computation, emphasizing minimum-trace DAGs, and offers a robust approach using R2R neighborhoods in causal discovery.

Abstract: We prove that the true underlying directed acyclic graph (DAG) in Gaussian
linear structural equation models is identifiable as the minimum-trace DAG when
the error variances are weakly increasing with respect to the true causal
ordering. This result bridges two existing frameworks as it extends the
identifiable cases within the minimum-trace DAG method and provides a
principled interpretation of the algorithmic ordering search approach,
revealing that its objective is actually to minimize the total residual sum of
squares. On the computational side, we prove that the hill climbing algorithm
with a random-to-random (R2R) neighborhood does not admit any strict local
optima. Under standard settings, we confirm the result through extensive
simulations, observing only a few weak local optima. Interestingly, algorithms
using other neighborhoods of equal size exhibit suboptimal behavior, having
strict local optima and a substantial number of weak local optima.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [312] [A Systematic Literature Review of Retrieval-Augmented Generation: Techniques, Metrics, and Challenges](https://arxiv.org/abs/2508.06401)
*Andrew Brown,Muhammad Roman,Barry Devereux*

Main category: cs.DL

TL;DR: This review analyzes 128 notable papers on retrieval-augmented generation (RAG) published between 2020 and May 2025, focusing on datasets, architectures, evaluations, and its effectiveness.


<details>
  <summary>Details</summary>
Motivation: To clarify the current understanding and progression of retrieval-augmented generation (RAG) systems, while identifying gaps and future priorities in research.

Method: A systematic review guided by PRISMA 2020, analyzing studies with explicit inclusion criteria based on citation count and research questions, and applying adjustments to account for emerging breakthroughs.

Result: The study catalogs datasets, describes architectures, evaluates practices, and synthesizes evidence on RAG's strengths and weaknesses. Emerging studies from 2025 were inclusively considered.

Conclusion: The review highlights the current state, gaps, and future directions in RAG research, and provides valuable insights for researchers and practitioners.

Abstract: This systematic review of the research literature on retrieval-augmented
generation (RAG) provides a focused analysis of the most highly cited studies
published between 2020 and May 2025. A total of 128 articles met our inclusion
criteria. The records were retrieved from ACM Digital Library, IEEE Xplore,
Scopus, ScienceDirect, and the Digital Bibliography and Library Project (DBLP).
RAG couples a neural retriever with a generative language model, grounding
output in up-to-date, non-parametric memory while retaining the semantic
generalisation stored in model weights. Guided by the PRISMA 2020 framework, we
(i) specify explicit inclusion and exclusion criteria based on citation count
and research questions, (ii) catalogue datasets, architectures, and evaluation
practices, and (iii) synthesise empirical evidence on the effectiveness and
limitations of RAG. To mitigate citation-lag bias, we applied a lower
citation-count threshold to papers published in 2025 so that emerging
breakthroughs with naturally fewer citations were still captured. This review
clarifies the current research landscape, highlights methodological gaps, and
charts priority directions for future research.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [313] [Stochastic Bandits for Crowdsourcing and Multi-Platform Autobidding](https://arxiv.org/abs/2508.05844)
*François Bachoc,Nicolò Cesa-Bianchi,Tommaso Cesari,Roberto Colomboni*

Main category: cs.GT

TL;DR: This study introduces a stochastic bandit model to optimize budget allocation among $K$ tasks or auctions, achieving regret bounds of $K\sqrt{T}$ or $K(\log T)^2$ under specific conditions.


<details>
  <summary>Details</summary>
Motivation: Applications in crowdsourcing (budget distribution) and autobidding (simultaneous auction allocations) necessitate efficient resource allocation.

Method: A stochastic bandit model with rewards dependent on probabilities tied to fractional budget distribution is constructed with regret-minimization algorithms.

Result: Achieved expected regret bounds of $K\sqrt{T}$ (general) and $K(\log T)^2$ (under diminishing returns conditions).

Conclusion: The proposed model and algorithms effectively allocate budgets while minimizing regret, backed by matching theoretical bounds.

Abstract: Motivated by applications in crowdsourcing, where a fixed sum of money is
split among $K$ workers, and autobidding, where a fixed budget is used to bid
in $K$ simultaneous auctions, we define a stochastic bandit model where arms
belong to the $K$-dimensional probability simplex and represent the fraction of
budget allocated to each task/auction. The reward in each round is the sum of
$K$ stochastic rewards, where each of these rewards is unlocked with a
probability that varies with the fraction of the budget allocated to that
task/auction. We design an algorithm whose expected regret after $T$ steps is
of order $K\sqrt{T}$ (up to log factors) and prove a matching lower bound.
Improved bounds of order $K (\log T)^2$ are shown when the function mapping
budget to probability of unlocking the reward (i.e., terminating the task or
winning the auction) satisfies additional diminishing-returns conditions.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [314] [Estimating the size of a set using cascading exclusion](https://arxiv.org/abs/2508.05901)
*Sourav Chatterjee,Persi Diaconis,Susan Holmes*

Main category: math.ST

TL;DR: The paper explores methods for estimating the size of a finite set using sampling techniques, developing improved estimators and error bounds for various applications, including unseen species problems and regression-style predictors.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance the efficiency and accuracy of estimators for the size of a finite set, especially in scenarios where traditional methods like the birthday problem or maximum sampling have limitations.

Method: The paper develops a general non-asymptotic theory, proposes refinements to existing estimators, and uses non-parametric finite sample error bounds to validate the approaches.

Result: The proposed methods improve on traditional estimation techniques and provide a framework for broader applications, including species estimation and population testing.

Conclusion: The refinements and general results offer practical alternatives that bridge extremes between conventional methods, giving robust estimators for finite set sizes and related statistical challenges.

Abstract: Let $S$ be a finite set, and $X_1,\ldots,X_n$ an i.i.d. uniform sample from
$S$. To estimate the size $|S|$, without further structure, one can wait for
repeats and use the birthday problem. This requires a sample size of the order
$|S|^\frac{1}{2}$. On the other hand, if $S=\{1,2,\ldots,|S|\}$, the maximum of
the sample blown up by $n/(n-1)$ gives an efficient estimator based on any
growing sample size. This paper gives refinements that interpolate between
these extremes. A general non-asymptotic theory is developed. This includes
estimating the volume of a compact convex set, the unseen species problem, and
a host of testing problems that follow from the question `Is this new
observation a typical pick from a large prespecified population?' We also treat
regression style predictors. A general theorem gives non-parametric finite $n$
error bounds in all cases.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [315] [SHACL Validation in the Presence of Ontologies: Semantics and Rewriting Techniques](https://arxiv.org/abs/2507.12286)
*Anouk Oudshoorn,Magdalena Ortiz,Mantas Simkus*

Main category: cs.LO

TL;DR: This paper discusses SHACL and OWL, highlighting their semantic gap and proposing solutions for validation in the presence of ontologies using universal models.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of combining SHACL and OWL, which operate under fundamentally different assumptions, to leverage their strengths in managing RDF data.

Method: The paper introduces a semantic for SHACL validation using core universal models, constructs these models for Horn-ALCHIQ, utilizes finite representations for rewriting techniques, and analyzes computational complexity.

Result: The proposed approach enables SHACL validation in the presence of OWL ontologies and reveals that the problem has a high computational complexity (EXPTIME-complete or PTIME-complete depending on data).

Conclusion: While integrating SHACL and OWL validation is computationally demanding, the proposed framework provides a viable solution for combining these formalisms effectively.

Abstract: SHACL and OWL are two prominent W3C standards for managing RDF data. These
languages share many features, but they have one fundamental difference: OWL,
designed for inferring facts from incomplete data, makes the open-world
assumption, whereas SHACL is a constraint language that treats the data as
complete and must be validated under the closed-world assumption. The
combination of both formalisms is very appealing and has been called for, but
their semantic gap is a major challenge, semantically and computationally. In
this paper, we advocate a semantics for SHACL validation in the presence of
ontologies based on core universal models. We provide a technique for
constructing these models for ontologies in the rich data-tractable description
logic Horn-ALCHIQ. Furthermore, we use a finite representation of this model to
develop a rewriting technique that reduces SHACL validation in the presence of
ontologies to standard validation. Finally, we study the complexity of SHACL
validation in the presence of ontologies, and show that even very simple
ontologies make the problem EXPTIME-complete, and PTIME-complete in data
complexity.

</details>


### [316] [Basic interactive algorithms: Preview](https://arxiv.org/abs/2508.05798)
*Yuri Gurevich*

Main category: cs.LO

TL;DR: The paper previews an upcoming work on the axiomatization of basic interactive algorithms, discussing their expansion beyond classical constructs.


<details>
  <summary>Details</summary>
Motivation: To revisit and build upon the notion of algorithms, which has evolved to include various types like probabilistic and quantum algorithms, and provide a unifying view.

Method: Discussion on the axiomatic foundations of basic algorithms and the application of oracles to extend this framework to newer algorithm classes.

Result: The paper illustrates how models like nondeterministic, probabilistic, and quantum circuit algorithms can be linked to basic algorithm frameworks.

Conclusion: The study bridges classical and modern algorithmic formulations, highlighting the conceptual distinction between the classical and physical Church-Turing theses.

Abstract: This dialog paper offers a preview and provides a foretaste of an upcoming
work on the axiomatization of basic interactive algorithms.
  The modern notion of algorithm was elucidated in the 1930s--1950s. It was
axiomatized a quarter of a century ago as the notion of ``sequential
algorithm'' or ``classical algorithm''; we prefer to call it ``basic algorithm"
now. The axiomatization was used to show that for every basic algorithm there
is a behaviorally equivalent abstract state machine. It was also used to prove
the Church-Turing thesis as it has been understood by the logicians.
  Starting from the 1960s, the notion of algorithm has expanded --
probabilistic algorithms, quantum algorithms, etc. -- prompting introduction of
a much more ambitious version of the Church-Turing thesis commonly known as the
``physical thesis.'' We emphasize the difference between the two versions of
the Church-Turing thesis and illustrate how nondeterministic and probabilistic
algorithms can be viewed as basic algorithms with appropriate oracles. The same
view applies to quantum circuit algorithms and many other classes of
algorithms.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [317] [Neural Field-Based 3D Surface Reconstruction of Microstructures from Multi-Detector Signals in Scanning Electron Microscopy](https://arxiv.org/abs/2508.04728)
*Shuo Chen,Yijin Li,Xi Zheng,Guofeng Zhang*

Main category: eess.IV

TL;DR: This paper introduces NFH-SEM, a neural field-based method to achieve 3D surface reconstruction from 2D SEM images, overcoming existing challenges.


<details>
  <summary>Details</summary>
Motivation: Conventional SEM images are 2D and lack direct information on the 3D topography of microstructures, necessitating new reconstruction methods.

Method: The NFH-SEM method uses multi-view, multi-detector 2D SEM images to fuse geometric and photometric data into a continuous neural field representation, featuring end-to-end self-calibration and shadow disentanglement.

Result: NFH-SEM delivers high-quality 3D reconstructions on both real and simulated datasets, accurately capturing intricate microstructures such as lithography designs, pollen, and silicon carbide particles.

Conclusion: NFH-SEM provides a robust and adaptable solution for precise SEM 3D surface reconstruction, eliminating manual calibrations and achieving broad applicability in scientific and industrial contexts.

Abstract: The scanning electron microscope (SEM) is a widely used imaging device in
scientific research and industrial applications. Conventional two-dimensional
(2D) SEM images do not directly reveal the three-dimensional (3D) topography of
micro samples, motivating the development of SEM 3D surface reconstruction
methods. However, reconstruction of complex microstructures remains challenging
for existing methods due to the limitations of discrete 3D representations, the
need for calibration with reference samples, and shadow-induced gradient
errors. Here, we introduce NFH-SEM, a neural field-based hybrid SEM 3D
reconstruction method that takes multi-view, multi-detector 2D SEM images as
input and fuses geometric and photometric information into a continuous neural
field representation. NFH-SEM eliminates the manual calibration procedures
through end-to-end self-calibration and automatically disentangles shadows from
SEM images during training, enabling accurate reconstruction of intricate
microstructures. We validate the effectiveness of NFH-SEM on real and simulated
datasets. Our experiments show high-fidelity reconstructions of diverse,
challenging samples, including two-photon lithography microstructures, peach
pollen, and silicon carbide particle surfaces, demonstrating precise detail and
broad applicability.

</details>


### [318] [Transformer-Based Explainable Deep Learning for Breast Cancer Detection in Mammography: The MammoFormer Framework](https://arxiv.org/abs/2508.06137)
*Ojonugwa Oluwafemi Ejiga Peter,Daniel Emakporuena,Bamidele Dayo Tunde,Maryam Abdulkarim,Abdullahi Bn Umar*

Main category: eess.IV

TL;DR: The study introduces the MammoFormer framework, combining transformer-based models with feature enhancements to improve accuracy and diagnostic transparency, addressing key challenges in AI-based mammography systems.


<details>
  <summary>Details</summary>
Motivation: There are challenges in mammography-based breast cancer detection due to subtle abnormalities and inconsistent interpretations by specialists. Current CNN approaches struggle with global context and lack explainability for clinical adoption.

Method: The researchers developed the MammoFormer framework combining transformers with multi-feature enhancements and explainable AI functionalities. Various architectures were evaluated (CNNs, Vision Transformer, Swin Transformer, ConvNext) with enhancements like histograms and adaptive processing.

Result: Results showed up to 13% performance improvement with systematic optimization. Notably, ViT achieved 98.3% accuracy with AHE enhancements, and Swin Transformer saw a 13% improvement using HOG enhancements.

Conclusion: The framework effectively addresses limitations of traditional mammography AI systems by enhancing model accuracy and interpretability, making it suitable for clinical deployment.

Abstract: Breast cancer detection through mammography interpretation remains difficult
because of the minimal nature of abnormalities that experts need to identify
alongside the variable interpretations between readers. The potential of CNNs
for medical image analysis faces two limitations: they fail to process both
local information and wide contextual data adequately, and do not provide
explainable AI (XAI) operations that doctors need to accept them in clinics.
The researcher developed the MammoFormer framework, which unites
transformer-based architecture with multi-feature enhancement components and
XAI functionalities within one framework. Seven different architectures
consisting of CNNs, Vision Transformer, Swin Transformer, and ConvNext were
tested alongside four enhancement techniques, including original images,
negative transformation, adaptive histogram equalization, and histogram of
oriented gradients. The MammoFormer framework addresses critical clinical
adoption barriers of AI mammography systems through: (1) systematic
optimization of transformer architectures via architecture-specific feature
enhancement, achieving up to 13% performance improvement, (2) comprehensive
explainable AI integration providing multi-perspective diagnostic
interpretability, and (3) a clinically deployable ensemble system combining CNN
reliability with transformer global context modeling. The combination of
transformer models with suitable feature enhancements enables them to achieve
equal or better results than CNN approaches. ViT achieves 98.3% accuracy
alongside AHE while Swin Transformer gains a 13.0% advantage through HOG
enhancements

</details>


### [319] [Clinically-guided Data Synthesis for Laryngeal Lesion Detection](https://arxiv.org/abs/2508.06182)
*Chiara Baldini,Kaisar Kushibar,Richard Osuala,Simone Balocco,Oliver Diaz,Karim Lekadir,Leonardo S. Mattos*

Main category: eess.IV

TL;DR: This paper proposes an innovative method using a Latent Diffusion Model (LDM) paired with ControlNet to generate high-quality synthetic laryngeal endoscopic image datasets, addressing data scarcity for CAD systems in otorhinolaryngology.


<details>
  <summary>Details</summary>
Motivation: Current diagnostic methods in otorhinolaryngology rely heavily on expert assessments, face challenges due to lesion heterogeneity, and lack sufficient well-annotated datasets for CADx/CADe systems.

Method: The authors utilize a Latent Diffusion Model (LDM) with ControlNet, conditioned by clinical observations, to synthesize realistic image-annotation pairs for training datasets.

Result: Adding just 10% synthetic data to training improved lesion detection rates by 9% on internal tests and 22.1% on external datasets. Experts also found the synthetic images indistinguishable from real ones.

Conclusion: This approach demonstrates the value of synthetic data in enhancing CADx/CADe performance, offering solutions to data scarcity in laryngology and improving diagnostic outcomes.

Abstract: Although computer-aided diagnosis (CADx) and detection (CADe) systems have
made significant progress in various medical domains, their application is
still limited in specialized fields such as otorhinolaryngology. In the latter,
current assessment methods heavily depend on operator expertise, and the high
heterogeneity of lesions complicates diagnosis, with biopsy persisting as the
gold standard despite its substantial costs and risks. A critical bottleneck
for specialized endoscopic CADx/e systems is the lack of well-annotated
datasets with sufficient variability for real-world generalization. This study
introduces a novel approach that exploits a Latent Diffusion Model (LDM)
coupled with a ControlNet adapter to generate laryngeal endoscopic
image-annotation pairs, guided by clinical observations. The method addresses
data scarcity by conditioning the diffusion process to produce realistic,
high-quality, and clinically relevant image features that capture diverse
anatomical conditions. The proposed approach can be leveraged to expand
training datasets for CADx/e models, empowering the assessment process in
laryngology. Indeed, during a downstream task of detection, the addition of
only 10% synthetic data improved the detection rate of laryngeal lesions by 9%
when the model was internally tested and 22.1% on out-of-domain external data.
Additionally, the realism of the generated images was evaluated by asking 5
expert otorhinolaryngologists with varying expertise to rate their confidence
in distinguishing synthetic from real images. This work has the potential to
accelerate the development of automated tools for laryngeal disease diagnosis,
offering a solution to data scarcity and demonstrating the applicability of
synthetic data in real-world scenarios.

</details>


### [320] [Advanced Deep Learning Techniques for Accurate Lung Cancer Detection and Classification](https://arxiv.org/abs/2508.06287)
*Mobarak Abumohsen,Enrique Costa-Montenegro,Silvia García-Méndez,Amani Yousef Owda,Majdi Owda*

Main category: eess.IV

TL;DR: This paper proposes using DenseNet201 paired with advanced techniques such as Focal Loss, data augmentation, and regularization to achieve 98.95% accuracy in lung cancer detection and classification from CT images.


<details>
  <summary>Details</summary>
Motivation: Lung cancer is a leading cause of global mortality and its diagnosis through CT images often lacks accuracy due to imbalanced and small datasets, resulting in significant false positives.

Method: The paper employs the DenseNet201 model combined with Focal Loss, data augmentation, and regularization techniques to address issues of data imbalance and overfitting for lung cancer detection and classification.

Result: The proposed method demonstrated an impressive performance, achieving 98.95% accuracy in LC detection and classification from CT images.

Conclusion: Leveraging DenseNet201 with advanced methods significantly enhances the accuracy of lung cancer detection, offering a robust solution to challenges linked to dataset imbalance and false positive rates.

Abstract: Lung cancer (LC) ranks among the most frequently diagnosed cancers and is one
of the most common causes of death for men and women worldwide. Computed
Tomography (CT) images are the most preferred diagnosis method because of their
low cost and their faster processing times. Many researchers have proposed
various ways of identifying lung cancer using CT images. However, such
techniques suffer from significant false positives, leading to low accuracy.
The fundamental reason results from employing a small and imbalanced dataset.
This paper introduces an innovative approach for LC detection and
classification from CT images based on the DenseNet201 model. Our approach
comprises several advanced methods such as Focal Loss, data augmentation, and
regularization to overcome the imbalanced data issue and overfitting challenge.
The findings show the appropriateness of the proposal, attaining a promising
performance of 98.95% accuracy.

</details>


### [321] [Multivariate Fields of Experts](https://arxiv.org/abs/2508.06490)
*Stanislas Ducotterd,Michael Unser*

Main category: eess.IV

TL;DR: The paper introduces multivariate fields of experts, a novel framework for learning image priors, excelling in solving inverse problems with better performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing univariate fields of experts methods and provide more effective and interpretable image priors for inverse problems.

Method: The authors propose incorporating multivariate potential functions, specifically Moreau envelopes of the $\ell_\infty$-norm, into the fields of experts framework.

Result: The proposed multivariate approach outperforms univariate models, rivals deep-learning-based regularizers in quality, and is faster and more efficient in terms of parameters and training data.

Conclusion: The multivariate fields of experts method provides an effective and interpretable alternative to deep-learning-based techniques for image priors in inverse problems, balancing performance and efficiency.

Abstract: We introduce the multivariate fields of experts, a new framework for the
learning of image priors. Our model generalizes existing fields of experts
methods by incorporating multivariate potential functions constructed via
Moreau envelopes of the $\ell_\infty$-norm. We demonstrate the effectiveness of
our proposal across a range of inverse problems that include image denoising,
deblurring, compressed-sensing magnetic-resonance imaging, and computed
tomography. The proposed approach outperforms comparable univariate models and
achieves performance close to that of deep-learning-based regularizers while
being significantly faster, requiring fewer parameters, and being trained on
substantially fewer data. In addition, our model retains a relatively high
level of interpretability due to its structured design.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [322] [CLAPP: The CLASS LLM Agent for Pair Programming](https://arxiv.org/abs/2508.05728)
*Santiago Casas,Christian Fidler,Boris Bolliet,Francisco Villaescusa-Navarro,Julien Lesgourgues*

Main category: astro-ph.IM

TL;DR: CLAPP is an AI assistant combining LLMs, domain-specific retrieval, and Python execution for helping cosmologists work with the CLASS solver.


<details>
  <summary>Details</summary>
Motivation: To facilitate computational and numerical cosmology by providing AI-driven support for the CLASS Einstein-Boltzmann solver.

Method: CLAPP integrates multi-agent LLM orchestration, semantic search, and a live Python environment deployed in a user-friendly web app.

Result: CLAPP provides functionality for coding tasks such as debugging, code generation, and plotting, enhancing productivity and collaboration.

Conclusion: CLAPP simplifies interaction with CLASS for researchers, promoting accessibility and efficiency in cosmology research.

Abstract: We introduce CLAPP (CLASS LLM Agent for Pair Programming), an interactive AI
assistant designed to support researchers working with the Einstein-Boltzmann
solver CLASS. CLAPP leverages large language models (LLMs) and domain-specific
retrieval to provide conversational coding support for CLASS-answering
questions, generating code, debugging errors, and producing plots. Its
architecture combines multi-agent LLM orchestration, semantic search across
CLASS documentation, and a live Python execution environment. Deployed as a
user-friendly web application, CLAPP lowers the entry barrier for scientists
unfamiliar with AI tools and enables more productive human-AI collaboration in
computational and numerical cosmology. The app is available at
https://classclapp.streamlit.app

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [323] [A variational approach to dimension-free self-normalized concentration](https://arxiv.org/abs/2508.06483)
*Ben Chugg,Aaditya Ramdas*

Main category: math.PR

TL;DR: This paper improves bounds for vector-valued stochastic processes, specifically for distributions satisfying a sub-psi tail condition, and offers new inequalities for improving concentration results.


<details>
  <summary>Details</summary>
Motivation: The need to generalize existing bounds for stochastic processes, particularly addressing a gap between determinant-based bounds and condition-number-based bounds in the literature.

Method: Use variational (PAC-Bayes) techniques to derive self-normalized concentration bounds for sub-psi processes, including more general conditions than boundedness.

Result: Key results include a Bernstein inequality for random vectors with a general moment condition and a dimension-free, self-normalized empirical Bernstein inequality.

Conclusion: The study advances concentration bounds for sub-psi processes, deepening understanding in this area of stochastic processes and providing more versatile tools for analysis.

Abstract: We study the self-normalized concentration of vector-valued stochastic
processes. We focus on bounds for sub-$\psi$ processes, a tail condition that
encompasses a wide variety of well-known distributions (including
sub-exponential, sub-Gaussian, sub-gamma, and sub-Poisson distributions). Our
results recover and generalize the influential bound of Abbasi-Yadkori et al.
(2011) and fill a gap in the literature between determinant-based bounds and
those based on condition numbers. As applications we prove a Bernstein
inequality for random vectors satisfying a moment condition (which is more
general than boundedness), and also provide the first dimension-free,
self-normalized empirical Bernstein inequality. Our techniques are based on the
variational (PAC-Bayes) approach to concentration.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [324] [IOCC: Aligning Semantic and Cluster Centers for Few-shot Short Text Clustering](https://arxiv.org/abs/2508.06126)
*Jixuan Yin,Zhihao Yao,Wenshuai Huo,Xinmiao Yu,Xiaocheng Feng,Bo Li*

Main category: stat.ME

TL;DR: IOCC is proposed as a novel few-shot contrastive learning method for clustering short text data by aligning cluster centers with semantic centers. It consists of Interaction-enhanced Optimal Transport (IEOT) and Center-aware Contrastive Learning (CACL).


<details>
  <summary>Details</summary>
Motivation: Short text representations lack expressiveness, leading to suboptimal clustering performance as conventional methods fail to capture underlying semantics effectively.

Method: IOCC utilizes IEOT to generate pseudo-labels informed by semantic interactions, and CACL optimizes text representations toward pseudo-centers, bridging the gap between cluster centers and semantic centers iteratively.

Result: IOCC demonstrated improvements in clustering accuracy, achieving up to 7.34% enhancement on challenging datasets like Biomedical and surpassing previous methods in stability and efficiency.

Conclusion: Effective alignment of cluster centers with semantic centers through IOCC results in higher clustering performance, establishing the method's utility and reliability for short text clustering tasks.

Abstract: In clustering tasks, it is essential to structure the feature space into
clear, well-separated distributions. However, because short text
representations have limited expressiveness, conventional methods struggle to
identify cluster centers that truly capture each category's underlying
semantics, causing the representations to be optimized in suboptimal
directions. To address this issue, we propose IOCC, a novel few-shot
contrastive learning method that achieves alignment between the cluster centers
and the semantic centers. IOCC consists of two key modules:
Interaction-enhanced Optimal Transport (IEOT) and Center-aware Contrastive
Learning (CACL). Specifically, IEOT incorporates semantic interactions between
individual samples into the conventional optimal transport problem, and
generate pseudo-labels. Based on these pseudo-labels, we aggregate
high-confidence samples to construct pseudo-centers that approximate the
semantic centers. Next, CACL optimizes text representations toward their
corresponding pseudo-centers. As training progresses, the collaboration between
the two modules gradually reduces the gap between cluster centers and semantic
centers. Therefore, the model will learn a high-quality distribution, improving
clustering performance. Extensive experiments on eight benchmark datasets show
that IOCC outperforms previous methods, achieving up to 7.34\% improvement on
challenging Biomedical dataset and also excelling in clustering stability and
efficiency. The code is available at:
https://anonymous.4open.science/r/IOCC-C438.

</details>


<div id='astro-ph.CO'></div>

# astro-ph.CO [[Back]](#toc)

### [325] [Detecting Model Misspecification in Cosmology with Scale-Dependent Normalizing Flows](https://arxiv.org/abs/2508.05744)
*Aizhan Akhmetzhanova,Carolina Cuesta-Lazaro,Siddharth Mishra-Sharma*

Main category: astro-ph.CO

TL;DR: The paper introduces a novel framework that uses neural networks and normalizing flows to detect theoretical model errors in cosmological data simulations.


<details>
  <summary>Details</summary>
Motivation: Current cosmological data analysis faces challenges in validating theoretical models amidst high-dimensional data and choosing suitable data representations that retain critical information.

Method: The framework combines scale-dependent neural summary statistics and normalizing flows, conditioned on smoothing scales, to perform Bayesian evidence estimation and detect model misspecifications.

Result: The method was applied to matter and gas density fields from CAMELS simulation suites, demonstrating its ability to systematically pinpoint where theoretical models fail.

Conclusion: This approach enhances the ability to validate cosmological models through a data-driven analysis, ensuring robust model simulations for upcoming cosmological surveys.

Abstract: Current and upcoming cosmological surveys will produce unprecedented amounts
of high-dimensional data, which require complex high-fidelity forward
simulations to accurately model both physical processes and systematic effects
which describe the data generation process. However, validating whether our
theoretical models accurately describe the observed datasets remains a
fundamental challenge. An additional complexity to this task comes from
choosing appropriate representations of the data which retain all the relevant
cosmological information, while reducing the dimensionality of the original
dataset. In this work we present a novel framework combining scale-dependent
neural summary statistics with normalizing flows to detect model
misspecification in cosmological simulations through Bayesian evidence
estimation. By conditioning our neural network models for data compression and
evidence estimation on the smoothing scale, we systematically identify where
theoretical models break down in a data-driven manner. We demonstrate a first
application to our approach using matter and gas density fields from three
CAMELS simulation suites with different subgrid physics implementations.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [326] [Towards Transparent Ethical AI: A Roadmap for Trustworthy Robotic Systems](https://arxiv.org/abs/2508.05846)
*Ahmad Farooq,Kamran Iqbal*

Main category: cs.CY

TL;DR: The paper emphasizes the importance of transparency in AI and robotics to ensure ethical behavior and trustworthiness, exploring its challenges and proposing solutions.


<details>
  <summary>Details</summary>
Motivation: The increase in AI and robotics usage in society creates a need for ethical and trustworthy systems, which requires transparency in decision-making processes.

Method: The authors propose approaches for enhancing transparency such as standardized metrics, explainable AI techniques, and user-friendly interfaces, along with a framework connecting technical implementation to ethical considerations.

Result: The paper highlights how transparency impacts public trust, regulatory policies, and research directions, fundamentally affecting ethical AI design.

Conclusion: Transparency is crucial for ethical and responsible AI system designs, and the paper adds to discussions on advancing the field with practical and ethical considerations.

Abstract: As artificial intelligence (AI) and robotics increasingly permeate society,
ensuring the ethical behavior of these systems has become paramount. This paper
contends that transparency in AI decision-making processes is fundamental to
developing trustworthy and ethically aligned robotic systems. We explore how
transparency facilitates accountability, enables informed consent, and supports
the debugging of ethical algorithms. The paper outlines technical, ethical, and
practical challenges in implementing transparency and proposes novel approaches
to enhance it, including standardized metrics, explainable AI techniques, and
user-friendly interfaces. This paper introduces a framework that connects
technical implementation with ethical considerations in robotic systems,
focusing on the specific challenges of achieving transparency in dynamic,
real-world contexts. We analyze how prioritizing transparency can impact public
trust, regulatory policies, and avenues for future research. By positioning
transparency as a fundamental element in ethical AI system design, we aim to
add to the ongoing discussion on responsible AI and robotics, providing
direction for future advancements in this vital field.

</details>


### [327] [Learning by Teaching: Engaging Students as Instructors of Large Language Models in Computer Science Education](https://arxiv.org/abs/2508.05979)
*Xinming Yang,Haasil Pujara,Jun Li*

Main category: cs.CY

TL;DR: This paper introduces an educational method where students act as instructors to teach a Large Language Model (LLM) via strategically designed questions, resulting in improved student performance.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the issues of passive learning and over-reliance on LLMs in computer science education.

Method: The method inverts the typical LLM-tutor approach by having students teach the LLM through questions with engineered knowledge gaps. The authors developed a system called Socrates to implement this process efficiently.

Result: Testing in an undergraduate course showed statistically significant improvements in student performance compared to previous cohorts.

Conclusion: The proposed method offers a practical and cost-effective way to enhance student engagement and mastery using LLMs in educational settings.

Abstract: While Large Language Models (LLMs) are often used as virtual tutors in
computer science (CS) education, this approach can foster passive learning and
over-reliance. This paper presents a novel pedagogical paradigm that inverts
this model: students act as instructors who must teach an LLM to solve
problems. To facilitate this, we developed strategies for designing questions
with engineered knowledge gaps that only a student can bridge, and we introduce
Socrates, a system for deploying this method with minimal overhead. We
evaluated our approach in an undergraduate course and found that this
active-learning method led to statistically significant improvements in student
performance compared to historical cohorts. Our work demonstrates a practical,
cost-effective framework for using LLMs to deepen student engagement and
mastery.

</details>


### [328] [Dimensional Characterization and Pathway Modeling for Catastrophic AI Risks](https://arxiv.org/abs/2508.06411)
*Ze Shen Chin*

Main category: cs.CY

TL;DR: The paper develops a structured framework to explore six major AI catastrophic risks by analyzing their dimensions and step-by-step pathways from hazards to harms.


<details>
  <summary>Details</summary>
Motivation: The lack of a multidimensional and causal approach in studying the risks associated with AI technology motivated this research.

Method: The paper mapped six AI risks using a seven-dimensional framework—intent, competency, entity, polarity, linearity, reach, and order—and applied risk pathway modeling to trace hazards to harms.

Result: The dimensional analysis facilitates systematic risk identification and general mitigation, while pathway models enable targeted, scenario-specific interventions.

Conclusion: By offering a structured and actionable foundation, the study enhances the understanding and management of AI-related catastrophic risks across various sectors.

Abstract: Although discourse around the risks of Artificial Intelligence (AI) has
grown, it often lacks a comprehensive, multidimensional framework, and concrete
causal pathways mapping hazard to harm. This paper aims to bridge this gap by
examining six commonly discussed AI catastrophic risks: CBRN, cyber offense,
sudden loss of control, gradual loss of control, environmental risk, and
geopolitical risk. First, we characterize these risks across seven key
dimensions, namely intent, competency, entity, polarity, linearity, reach, and
order. Next, we conduct risk pathway modeling by mapping step-by-step
progressions from the initial hazard to the resulting harms. The dimensional
approach supports systematic risk identification and generalizable mitigation
strategies, while risk pathway models help identify scenario-specific
interventions. Together, these methods offer a more structured and actionable
foundation for managing catastrophic AI risks across the value chain.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [329] [NanoCodec: Towards High-Quality Ultra Fast Speech LLM Inference](https://arxiv.org/abs/2508.05835)
*Edresson Casanova,Paarth Neekhara,Ryan Langman,Shehzeen Hussain,Subhankar Ghosh,Xuesong Yang,Ante Jukić,Jason Li,Boris Ginsburg*

Main category: eess.AS

TL;DR: The paper introduces NanoCodec, a low-frame-rate audio codec achieving exceptional audio compression quality at 12.5 FPS, enhancing efficiency for Speech LLM training and inference.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiencies caused by high frame rates in existing audio codecs, which result in slow training and inference for Speech LLMs, particularly in autoregressive models.

Method: A series of ablation studies were conducted to assess the effects of frame rate, bitrate, and causality on audio codec reconstruction quality, leading to the design and implementation of NanoCodec.

Result: NanoCodec demonstrated superior performance over existing codecs across various bitrate ranges, achieving high-quality audio compression at 12.5 FPS.

Conclusion: NanoCodec sets a new benchmark for low-latency, efficient Speech LLM training and inference, marking a significant step forward in audio compression technology.

Abstract: Large Language Models (LLMs) have significantly advanced audio processing by
leveraging audio codecs to discretize audio into tokens, enabling the
application of language modeling techniques to speech data. However, existing
audio codecs often operate at high frame rates, leading to slow training and
inference, particularly for autoregressive models. To address this, there is
growing interest in low frame-rate audio codecs, which reduce the number of
autoregressive steps required to generate one second of audio. In this paper,
we conduct ablation studies to examine the impact of frame rate, bitrate, and
causality on codec reconstruction quality. Based on our findings, we introduce
NanoCodec, a state-of-the-art audio codec that achieves high-quality
compression at just 12.5 frames per second (FPS). NanoCodec outperforms related
works across various bitrate ranges, establishing a new benchmark for
low-latency and efficient Speech LLM training and inference.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [330] [A Humanoid Social Robot as a Teaching Assistant in the Classroom](https://arxiv.org/abs/2508.05646)
*Thomas Sievers*

Main category: cs.HC

TL;DR: This paper explores the use of the social robot Pepper, connected to ChatGPT, in high school classrooms to teach learning content and gauges students' acceptance and perceived usefulness.


<details>
  <summary>Details</summary>
Motivation: To address the lack of social robots assisting teachers in educational environments and explore the potential of Child-Robot Interaction (CRI) in modern learning environments.

Method: The robot Pepper, connected to ChatGPT, was deployed in a high school classroom to present learning content. Student feedback regarding the robot's acceptance and perceived usefulness was gathered.

Result: All participants found the robot's presentation appropriate or partially appropriate and believed its use to be justified.

Conclusion: Integrating a social robot like Pepper, with AI capabilities, into classrooms is perceived positively by students and holds promise for enhancing educational tasks.

Abstract: Although innovation and the support of new technologies are much needed to
ease the burden on the education system, social robots in schools to help
teachers with educational tasks are rare. Child-Robot Interaction (CRI) could
support teachers and add an embodied social component to modern multi-modal and
multi-sensory learning environments already in use. The social robot Pepper,
connected to the Large Language Model (LLM) ChatGPT, was used in a high school
classroom to teach new learning content to groups of students. I tested the
technical possibilities with the robot on site and asked the students about
their acceptance and perceived usefulness of teaching with the help of a social
robot. All participants felt that the robot's presentation of the learning
material was appropriate or at least partially appropriate and that its use
made sense.

</details>


### [331] [Automated Visualization Makeovers with LLMs](https://arxiv.org/abs/2508.05637)
*Siddharth Gangwar,David A. Selby,Sebastian J. Vollmer*

Main category: cs.HC

TL;DR: The paper explores whether large language models (LLMs) can assist in improving data visualizations by providing constructive feedback based on best practices.


<details>
  <summary>Details</summary>
Motivation: To investigate the potential of LLMs in aiding users to enhance their data visualizations by offering semi-automated, constructive critiques based on established best practices.

Method: The approach relies on prompt engineering of a pre-trained LLM and incorporates user-provided guidelines along with the model's inherent knowledge of visualization practices to deliver feedback.

Result: Quantitative evaluation showed the model's sensitivity to different plotting issues across varied chart types, demonstrating its utility in the visualization improvement process.

Conclusion: LLMs, with appropriate prompt engineering, can effectively educate users on improving existing visualizations, offering a valuable tool for both learning and refinement. The tool is released as a simple, self-hosted app with a user-friendly web interface.

Abstract: Making a good graphic that accurately and efficiently conveys the desired
message to the audience is both an art and a science, typically not taught in
the data science curriculum. Visualisation makeovers are exercises where the
community exchange feedback to improve charts and data visualizations. Can
multi-modal large language models (LLMs) emulate this task? Given a plot in the
form of an image file, or the code used to generate it, an LLM, primed with a
list of visualization best practices, is employed to semi-automatically
generate constructive criticism to produce a better plot. Our system is centred
around prompt engineering of a pre-trained model, relying on a combination of
userspecified guidelines and any latent knowledge of data visualization
practices that might lie within an LLMs training corpus. Unlike other works,
the focus is not on generating valid visualization scripts from raw data or
prompts, but on educating the user how to improve their existing data
visualizations according to an interpretation of best practices. A quantitative
evaluation is performed to measure the sensitivity of the LLM agent to various
plotting issues across different chart types. We make the tool available as a
simple self-hosted applet with an accessible Web interface.

</details>


### [332] [Modeling Interactive Narrative Systems: A Formal Approach](https://arxiv.org/abs/2508.05653)
*Jules Clerc,Domitile Lourdeaux,Mohamed Sallak,Johann Barbier,Marc Ravaine*

Main category: cs.HC

TL;DR: The paper proposes a formal representation framework for Interactive Narrative Systems (INS) to standardize analysis, inspired by state-of-the-art approaches.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the fragmented research efforts and diverse representations in Interactive Narrative Systems, which hinder analysis and comparison.

Method: The paper introduces a formal representation framework using a consistent vocabulary and modeling structure, validated experimentally with the "Little Red Riding Hood" scenario.

Result: The framework is shown to improve the evaluation and comparison of INS through its application in experimental settings.

Conclusion: The authors conclude that their framework can enhance collaboration and coherence among the INS research community by providing a formal foundation.

Abstract: Interactive Narrative Systems (INS) have revolutionized digital experiences
by empowering users to actively shape their stories, diverging from traditional
passive storytelling. However, the field faces challenges due to fragmented
research efforts and diverse system representations. This paper introduces a
formal representation framework for INS, inspired by diverse approaches from
the state of the art. By providing a consistent vocabulary and modeling
structure, the framework facilitates the analysis, the description and
comparison of INS properties. Experimental validations on the "Little Red
Riding Hood" scenario highlight the usefulness of the proposed formalism and
its impact on improving the evaluation of INS. This work aims to foster
collaboration and coherence within the INS research community by proposing a
methodology for formally representing these systems.

</details>


### [333] [Do Ethical AI Principles Matter to Users? A Large-Scale Analysis of User Sentiment and Satisfaction](https://arxiv.org/abs/2508.05913)
*Stefan Pasch,Min Chul Cha*

Main category: cs.HC

TL;DR: The paper investigates how ethical principles in AI affect user satisfaction by analyzing over 100,000 user reviews from G2. Using transformer-based language models, it examines sentiment across seven ethical AI dimensions and discovers varying impacts based on user roles and product types.


<details>
  <summary>Details</summary>
Motivation: To address the lack of empirical evidence on how ethical AI principles are recognized, valued, or impactful from the perspective of users.

Method: Analyzing 100,000+ user reviews of AI products using transformer-based language models to measure sentiment across seven ethical dimensions outlined in the EU Ethics Guidelines for Trustworthy AI.

Result: All seven ethical dimensions are positively associated with user satisfaction, with variations in emphasis based on user type (technical vs. non-technical) and product type (AI development platforms vs. end-user applications).

Conclusion: Ethical AI design is crucial from the users' perspectives, and contextual differences in user roles and product types should be considered to optimize user satisfaction.

Abstract: As AI systems become increasingly embedded in organizational workflows and
consumer applications, ethical principles such as fairness, transparency, and
robustness have been widely endorsed in policy and industry guidelines.
However, there is still scarce empirical evidence on whether these principles
are recognized, valued, or impactful from the perspective of users. This study
investigates the link between ethical AI and user satisfaction by analyzing
over 100,000 user reviews of AI products from G2. Using transformer-based
language models, we measure sentiment across seven ethical dimensions defined
by the EU Ethics Guidelines for Trustworthy AI. Our findings show that all
seven dimensions are positively associated with user satisfaction. Yet, this
relationship varies systematically across user and product types. Technical
users and reviewers of AI development platforms more frequently discuss
system-level concerns (e.g., transparency, data governance), while
non-technical users and reviewers of end-user applications emphasize
human-centric dimensions (e.g., human agency, societal well-being). Moreover,
the association between ethical AI and user satisfaction is significantly
stronger for non-technical users and end-user applications across all
dimensions. Our results highlight the importance of ethical AI design from
users' perspectives and underscore the need to account for contextual
differences across user roles and product types.

</details>


### [334] [ThematicPlane: Bridging Tacit User Intent and Latent Spaces for Image Generation](https://arxiv.org/abs/2508.06065)
*Daniel Lee,Nikhil Sharma,Donghoon Shin,DaEun Choi,Harsh Sharma,Jeonghwan Kim,Heng Ji*

Main category: cs.HC

TL;DR: The paper introduces ThematicPlane, a system enhancing generative AI design by enabling interactive, high-level semantic control over outputs.


<details>
  <summary>Details</summary>
Motivation: Generative AI struggles to align outputs with nuanced creative intent, especially for non-experts, due to reliance on prompts or references.

Method: ThematicPlane allows users to manipulate semantic concepts like mood and style interactively, bridging creative intent with system control.

Result: Participants effectively used ThematicPlane for exploratory and iterative workflows, though they identified a need for more explainable controls.

Conclusion: ThematicPlane supports expressive creativity and highlights the importance of intuitive, semantics-driven interaction in AI design tools.

Abstract: Generative AI has made image creation more accessible, yet aligning outputs
with nuanced creative intent remains challenging, particularly for non-experts.
Existing tools often require users to externalize ideas through prompts or
references, limiting fluid exploration. We introduce ThematicPlane, a system
that enables users to navigate and manipulate high-level semantic concepts
(e.g., mood, style, or narrative tone) within an interactive thematic design
plane. This interface bridges the gap between tacit creative intent and system
control. In our exploratory study (N=6), participants engaged in divergent and
convergent creative modes, often embracing unexpected results as inspiration or
iteration cues. While they grounded their exploration in familiar themes,
differing expectations of how themes mapped to outputs revealed a need for more
explainable controls. Overall, ThematicPlane fosters expressive, iterative
workflows and highlights new directions for intuitive, semantics-driven
interaction in generative design tools.

</details>


### [335] [REFS: Robust EEG feature selection with missing multi-dimensional annotation for emotion recognition](https://arxiv.org/abs/2508.05933)
*Xueyuan Xu,Wenjia Dong,Fulin Wei,Li Zhuo*

Main category: cs.HC

TL;DR: This study addresses the issue of high-dimensional EEG data and missing emotional labels in brain-computer interfaces through a novel feature selection method, achieving superior performance in multi-dimensional emotion recognition.


<details>
  <summary>Details</summary>
Motivation: Affective brain-computer interfaces face challenges such as high-dimensional EEG data, small sample sizes, and missing emotional labels, limiting their robustness and real-time application.

Method: The paper proposes using adaptive orthogonal non-negative matrix factorization for reconstructing emotional label spaces and least squares regression with graph-based manifold learning and global redundancy minimization for feature selection.

Result: The method demonstrated superior performance compared to thirteen advanced alternatives in simulations conducted on three EEG emotional datasets: DREAMER, DEAP, and HDED.

Conclusion: The proposed feature selection approach enhances robustness for multi-dimensional emotion recognition, addressing key challenges in affective brain-computer interfaces.

Abstract: The affective brain-computer interface is a crucial technology for affective
interaction and emotional intelligence, emerging as a significant area of
research in the human-computer interaction. Compared to single-type features,
multi-type EEG features provide a multi-level representation for analyzing
multi-dimensional emotions. However, the high dimensionality of multi-type EEG
features, combined with the relatively small number of high-quality EEG
samples, poses challenges such as classifier overfitting and suboptimal
real-time performance in multi-dimensional emotion recognition. Moreover,
practical applications of affective brain-computer interface frequently
encounters partial absence of multi-dimensional emotional labels due to the
open nature of the acquisition environment, and ambiguity and variability in
individual emotion perception. To address these challenges, this study proposes
a novel EEG feature selection method for missing multi-dimensional emotion
recognition. The method leverages adaptive orthogonal non-negative matrix
factorization to reconstruct the multi-dimensional emotional label space
through second-order and higher-order correlations, which could reduce the
negative impact of missing values and outliers on label reconstruction.
Simultaneously, it employs least squares regression with graph-based manifold
learning regularization and global feature redundancy minimization
regularization to enable EEG feature subset selection despite missing
information, ultimately achieving robust EEG-based multi-dimensional emotion
recognition. Simulation experiments on three widely used multi-dimensional
emotional datasets, DREAMER, DEAP and HDED, reveal that the proposed method
outperforms thirteen advanced feature selection methods in terms of robustness
for EEG emotional feature selection.

</details>


### [336] [ASLSL: Adaptive shared latent structure learning with incomplete multi-modal physiological data for multi-dimensional emotional feature selection](https://arxiv.org/abs/2508.05934)
*Xueyuan Xu,Tianze Yu,Wenjia Dong,Fulin Wei,Li Zhuo*

Main category: cs.HC

TL;DR: The paper proposes Adaptive Shared Latent Structure Learning (ASLSL) as a feature selection method to handle incomplete multi-modal physiological signals in emotion recognition, outperforming existing approaches on DEAP and DREAMER datasets.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges of high-dimensional, potentially noisy, irrelevant, or redundant multi-modal physiological features in emotion recognition, which lead to performance and computational issues, especially when data is incomplete.

Method: The ASLSL approach learns a common latent space shared among incomplete multi-modal physiological signals and multi-dimensional emotional labels. It uses adaptive shared latent structure learning to mitigate missing data effects while capturing key consensus information.

Result: The ASLSL method demonstrated superior feature selection and performance compared to seventeen existing methods when evaluated on the DEAP and DREAMER datasets.

Conclusion: Adaptive Shared Latent Structure Learning is an effective solution for feature selection in incomplete multi-modal physiological emotion recognition. It reduces the impact of data incompleteness and improves recognition performance.

Abstract: Recently, multi-modal physiological signals based emotion recognition has
garnered increasing attention in the field of brain-computer interfaces.
Nevertheness, the associated multi-modal physiological features are often
high-dimensional and inevitably include irrelevant, redundant, and noisy
representation, which can easily lead to overfitting, poor performance, and
high computational complexity in emotion classifiers. Feature selection has
been widely applied to address these challenges. However, previous studies
generally assumed that multi-modal physiological data are complete, whereas in
reality, the data are often incomplete due to the openness of the acquisition
and operational environment. For example, a part of samples are available in
several modalities but not in others. To address this issue, we propose a novel
method for incomplete multi-modal physiological signal feature selection called
adaptive shared latent structure learning (ASLSL). Based on the property that
similar features share similar emotional labels, ASLSL employs adaptive shared
latent structure learning to explore a common latent space shared for
incomplete multi-modal physiological signals and multi-dimensional emotional
labels, thereby mitigating the impact of missing information and mining
consensus information. Two most popular multi-modal physiological emotion
datasets (DEAP and DREAMER) with multi-dimensional emotional labels were
utilized to compare the performance between compare ASLSL and seventeen feature
selection methods. Comprehensive experimental results on these datasets
demonstrate the effectiveness of ASLSL.

</details>


### [337] [Hand by Hand: LLM Driving EMS Assistant for Operational Skill Learning](https://arxiv.org/abs/2508.06000)
*Wei Xiang,Ziyue Lei,Haoyuan Che,Fangyuan Ye,Xueting Wu,Lingyun Sun*

Main category: cs.HC

TL;DR: This paper introduces FlightAxis, a tool combining LLMs and EMS for enhancing flight skill training with kinesthetic feedback.


<details>
  <summary>Details</summary>
Motivation: Address the gap in LLM-supported training by incorporating physical and hands-on assistance for operational skill learning.

Method: Developed the 'Align-Analyze-Adjust' strategy, integrating LLMs with EMS to guide physical movements during simulated training tasks.

Result: Users showed high acceptance of LLM-driven body control, reduced task times, and improved engagement and awareness of operational flaws.

Conclusion: LLM-mediated kinesthetic training can improve operational skill acquisition and demonstrates promising potential for broader applications.

Abstract: Operational skill learning, inherently physical and reliant on hands-on
practice and kinesthetic feedback, has yet to be effectively replicated in
large language model (LLM)-supported training. Current LLM training assistants
primarily generate customized textual feedback, neglecting the crucial
kinesthetic modality. This gap derives from the textual and uncertain nature of
LLMs, compounded by concerns on user acceptance of LLM driven body control. To
bridge this gap and realize the potential of collaborative human-LLM action,
this work explores human experience of LLM driven kinesthetic assistance.
Specifically, we introduced an "Align-Analyze-Adjust" strategy and developed
FlightAxis, a tool that integrates LLM with Electrical Muscle Stimulation (EMS)
for flight skill acquisition, a representative operational skill domain.
FlightAxis learns flight skills from manuals and guides forearm movements
during simulated flight tasks. Our results demonstrate high user acceptance of
LLM-mediated body control and significantly reduced task completion times.
Crucially, trainees reported that this kinesthetic assistance enhanced their
awareness of operation flaws and fostered increased engagement in the training
process, rather than relieving perceived load. This work demonstrated the
potential of kinesthetic LLM training in operational skill acquisition.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [338] [Risk Analysis Techniques for Governed LLM-based Multi-Agent Systems](https://arxiv.org/abs/2508.05687)
*Alistair Reid,Simon O'Callaghan,Liam Carroll,Tiberio Caetano*

Main category: cs.MA

TL;DR: This paper explores risk identification and analysis for multi-agent AI systems, highlighting six critical failure modes and providing tools to address them in governed environments.


<details>
  <summary>Details</summary>
Motivation: To address the unique risks that arise when transitioning from single-agent to interconnected multi-agent AI systems, and to propose an effective risk management framework.

Method: The authors analyze six critical multi-agent system failure modes and propose a staged testing methodology involving simulation, observations, benchmarking, and red teaming to assess risks systematically.

Result: The study highlights six major failure modes—cascading reliability failures, inter-agent communication failures, monoculture collapse, conformity bias, deficient theory of mind, and mixed motive dynamics—and introduces tools to assess them.

Conclusion: The paper lays a foundation for risk management techniques to ensure safer deployments of LLM-based multi-agent systems within organizations by progressively analyzing and mitigating risks.

Abstract: Organisations are starting to adopt LLM-based AI agents, with their
deployments naturally evolving from single agents towards interconnected,
multi-agent networks. Yet a collection of safe agents does not guarantee a safe
collection of agents, as interactions between agents over time create emergent
behaviours and induce novel failure modes. This means multi-agent systems
require a fundamentally different risk analysis approach than that used for a
single agent.
  This report addresses the early stages of risk identification and analysis
for multi-agent AI systems operating within governed environments where
organisations control their agent configurations and deployment. In this
setting, we examine six critical failure modes: cascading reliability failures,
inter-agent communication failures, monoculture collapse, conformity bias,
deficient theory of mind, and mixed motive dynamics. For each, we provide a
toolkit for practitioners to extend or integrate into their existing frameworks
to assess these failure modes within their organisational contexts.
  Given fundamental limitations in current LLM behavioural understanding, our
approach centres on analysis validity, and advocates for progressively
increasing validity through staged testing across stages of abstraction and
deployment that gradually increases exposure to potential negative impacts,
while collecting convergent evidence through simulation, observational
analysis, benchmarking, and red teaming. This methodology establishes the
groundwork for robust organisational risk management as these LLM-based
multi-agent systems are deployed and operated.

</details>


### [339] [Semantic Reasoning Meets Numerical Precision: An LLM-Powered Multi-Agent System for Power Grid Control](https://arxiv.org/abs/2508.05702)
*Yan Zhang*

Main category: cs.MA

TL;DR: The paper introduces Grid-Agent, an AI-based framework blending Large Language Models (LLMs) and reinforcement learning to address power grid violations in real time.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the increasing challenges in power grid planning, operation, and management due to the rise of Distributed Energy Resources (DERs), Electric Vehicles (EVs), and extreme weather events.

Method: The proposed system integrates semantic reasoning from Large Language Models (LLMs) with multi-agent reinforcement learning. It uses a modular agent architecture with a planning agent for action generation and a validation agent for stability evaluation. It also employs adaptive multiscale network representation for scalability.

Result: Experimental results validate Grid-Agent's effectiveness using IEEE and CIGRE test systems, showcasing superior performance in mitigating grid violations and enabling continuous learning for diverse network topologies.

Conclusion: Grid-Agent demonstrates its potential as an adaptable, autonomous framework well-suited for modern smart grid applications requiring fast and dynamic system responses.

Abstract: The increasing penetration of Distributed Energy Resources (DERs), widespread
adoption of Electric Vehicles (EVs), and the growing frequency of extreme
weather events have significantly increased the complexity of power grid
planning, operation, and management. Traditional rule-based systems and
numerical optimization approaches often struggle with the scale, dynamics, and
adaptability required by modern power networks. This paper introduces
Grid-Agent, an autonomous, AI-driven framework that combines Large Language
Models (LLMs) with multi-agent reinforcement learning to detect and remediate
grid violations in real time. Grid-Agent integrates semantic reasoning with
numerical precision through a modular agent architecture: a planning agent
generates coordinated action sequences using numerical power flow solvers,
while a validation agent evaluates system stability and action effectiveness
via sandboxed execution with safety rollbacks. To ensure scalability,
Grid-Agent incorporates an adaptive multiscale network representation that
dynamically selects optimal encoding schemes based on network size and
complexity. The framework enables coordinated violation resolution through
optimizing switch configurations, battery deployment, and load curtailment
strategies. Experimental results in standard IEEE and CIGRE test systems (IEEE
69-bus, CIGRE MV, and IEEE 30-bus) demonstrate superior violation mitigation
performance. Additionally, the framework's built-in data collection and
learning capabilities enable continuous learning and adaptation to diverse
network topologies. The autonomous nature of the framework makes it
particularly suitable for modern smart grid applications requiring rapid
response to dynamic operating conditions.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [340] [Data-Driven Density Steering via the Gromov-Wasserstein Optimal Transport Distance](https://arxiv.org/abs/2508.06052)
*Haruto Nakashima,Siddhartha Ganguly,Kenji Kashima*

Main category: math.OC

TL;DR: This paper addresses a data-driven approach to control an unknown linear system while fulfilling specific probabilistic constraints using the Gromov-Wasserstein metric.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to effectively steer the density of dynamical systems, modeled with insufficiently understood dynamics, while ensuring chance constraints are met.

Method: The authors use a difference-of-convex (DC) programming approach to reformulate the optimal control problem. They rely on the Gromov-Wasserstein metric and sufficient input-output experimental data, along with DC algorithms to solve the problem efficiently.

Result: The approach is validated through numerical experiments that showcase the feasibility and effectiveness of data-driven schemes.

Conclusion: The proposed framework demonstrates a robust method for addressing chance-constrained density steering in systems with unknown dynamics.

Abstract: We tackle the data-driven chance-constrained density steering problem using
the Gromov-Wasserstein metric. The underlying dynamical system is an unknown
linear controlled recursion, with the assumption that sufficiently rich
input-output data from pre-operational experiments are available. The initial
state is modeled as a Gaussian mixture, while the terminal state is required to
match a specified Gaussian distribution. We reformulate the resulting optimal
control problem as a difference-of-convex program and show that it can be
efficiently and tractably solved using the DC algorithm. Numerical results
validate our approach through various data-driven schemes.

</details>


### [341] [LLM Serving Optimization with Variable Prefill and Decode Lengths](https://arxiv.org/abs/2508.06133)
*Meixuan Wang,Yinyu Ye,Zijie Zhou*

Main category: math.OC

TL;DR: The paper addresses the issue of efficiently scheduling LLM requests to minimize completion time, considering constraints like memory usage and batching challenges.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the inefficiency of current scheduling strategies for LLM requests in real-world applications, where large memory demands and complex request dynamics make performance optimization challenging.

Method: The authors analyze the limitations of existing scheduling strategies like FCFS and SF, propose a novel algorithm with a constant competitive ratio for batching, and further refine it using dynamic programming, local search, and LP-based approaches.

Result: The newly proposed algorithm and its variants outperform baseline scheduling strategies in simulations, balancing improved performance with computational efficiency.

Conclusion: The paper highlights the NP-hardness of scheduling LLM requests, proposes an innovative solution with practical performance benefits, and provides a framework for future improvement in LLM serving systems.

Abstract: We study the problem of serving LLM (Large Language Model) requests where
each request has heterogeneous prefill and decode lengths. In LLM serving, the
prefill length corresponds to the input prompt length, which determines the
initial memory usage in the KV cache. The decode length refers to the number of
output tokens generated sequentially, with each additional token increasing the
KV cache memory usage by one unit. Given a set of n requests, our goal is to
schedule and process them to minimize the total completion time. We show that
this problem is NP-hard due to the interplay of batching, placement
constraints, precedence relationships, and linearly increasing memory usage. We
then analyze commonly used scheduling strategies in practice, such as
First-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their
competitive ratios scale up sublinearly with the memory limit-a significant
drawback in real-world settings where memory demand is large. To address this,
we propose a novel algorithm based on a new selection metric that efficiently
forms batches over time. We prove that this algorithm achieves a constant
competitive ratio. Finally, we develop and evaluate a few algorithm variants
inspired by this approach, including dynamic programming variants, local search
methods, and an LP-based scheduler, demonstrating through comprehensive
simulations that they outperform standard baselines while maintaining
computational efficiency.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [342] [Training chord recognition models on artificially generated audio](https://arxiv.org/abs/2508.05878)
*Martyna Majchrzak,Jacek Mańdziuk*

Main category: cs.SD

TL;DR: This paper compares two Transformer-based models for chord sequence recognition in audio and evaluates the utility of an artificially generated dataset for training.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of acquiring sufficient non-copyrighted audio recordings for training and evaluating models in Music Information Retrieval.

Method: Two Transformer-based neural network models were trained on artificial and human-composed datasets and evaluated with three metrics: Root, MajMin, and CCM.

Result: The study found that Artificial Audio Multitracks (AAM) can supplement smaller human-composed datasets or serve as standalone training data for chord sequence prediction in pop music when no other data is available.

Conclusion: Artificially generated music datasets, despite differences from human-composed music, are valuable for enriching training data or as a substitute in the absence of human-composed datasets.

Abstract: One of the challenging problems in Music Information Retrieval is the
acquisition of enough non-copyrighted audio recordings for model training and
evaluation. This study compares two Transformer-based neural network models for
chord sequence recognition in audio recordings and examines the effectiveness
of using an artificially generated dataset for this purpose. The models are
trained on various combinations of Artificial Audio Multitracks (AAM),
Schubert's Winterreise Dataset, and the McGill Billboard Dataset and evaluated
with three metrics: Root, MajMin and Chord Content Metric (CCM). The
experiments prove that even though there are certainly differences in
complexity and structure between artificially generated and human-composed
music, the former can be useful in certain scenarios. Specifically, AAM can
enrich a smaller training dataset of music composed by a human or can even be
used as a standalone training set for a model that predicts chord sequences in
pop music, if no other data is available.

</details>


### [343] [DAFMSVC: One-Shot Singing Voice Conversion with Dual Attention Mechanism and Flow Matching](https://arxiv.org/abs/2508.05978)
*Wei Chen,Binzhu Sha,Dan Luo,Jing Yang,Zhuo Wang,Fan Fan,Zhiyong Wu*

Main category: cs.SD

TL;DR: This paper introduces DAFMSVC, a method to improve Singing Voice Conversion (SVC) by incorporating SSL feature replacement, dual cross-attention, and a flow matching module, resulting in better timbre similarity and audio quality.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in any-to-any Singing Voice Conversion (SVC), specifically issues like timbre leakage and ensuring high timbre similarity and audio quality.

Method: DAFMSVC prevents timbre leakage by replacing source audio SSL features with similar features from the target. It uses a dual cross-attention mechanism to fuse speaker embeddings, melody, and linguistic content, and employs a flow matching module for high-quality audio generation.

Result: DAFMSVC achieves significant improvements in timbre similarity, naturalness, and audio quality compared to existing methods, as demonstrated by subjective and objective evaluations.

Conclusion: The proposed DAFMSVC method surpasses existing SVC approaches in quality, effectively resolving common challenges like timbre leakage while maintaining high fidelity to timbre and naturalness in generated audio.

Abstract: Singing Voice Conversion (SVC) transfers a source singer's timbre to a target
while keeping melody and lyrics. The key challenge in any-to-any SVC is
adapting unseen speaker timbres to source audio without quality degradation.
Existing methods either face timbre leakage or fail to achieve satisfactory
timbre similarity and quality in the generated audio. To address these
challenges, we propose DAFMSVC, where the self-supervised learning (SSL)
features from the source audio are replaced with the most similar SSL features
from the target audio to prevent timbre leakage. It also incorporates a dual
cross-attention mechanism for the adaptive fusion of speaker embeddings,
melody, and linguistic content. Additionally, we introduce a flow matching
module for high quality audio generation from the fused features. Experimental
results show that DAFMSVC significantly enhances timbre similarity and
naturalness, outperforming state-of-the-art methods in both subjective and
objective evaluations.

</details>


### [344] [EmoAugNet: A Signal-Augmented Hybrid CNN-LSTM Framework for Speech Emotion Recognition](https://arxiv.org/abs/2508.06321)
*Durjoy Chandra Paul,Gaurob Saha,Md Amjad Hossain*

Main category: cs.SD

TL;DR: This paper introduces EmoAugNet, a hybrid deep learning model combining LSTM and 1D-CNN for speech emotion recognition (SER), using enhanced data augmentation and feature extraction. The model achieves high accuracy on standard datasets.


<details>
  <summary>Details</summary>
Motivation: The study aims to improve the robustness and performance of Speech Emotion Recognition (SER) systems for more effective human-computer interaction by addressing challenges such as overfitting and feature quality.

Method: The authors proposed EmoAugNet, a deep learning framework combining LSTM and 1D-CNN layers. A novel data augmentation technique, mixing traditional and combination-based methods, was applied. Key features like RMSE, MFCC, and ZCR were extracted to transform audio into high-dimensional feature vectors.

Result: On the IEMOCAP dataset, accuracy reached up to 96.75% (weighted) and 92.52% (unweighted). For the RAVDESS dataset, the model achieved up to 94.98% (weighted) and 94.64% (unweighted) accuracy, with slight variations across ReLU and ELU activations.

Conclusion: EmoAugNet effectively enhances the robustness and accuracy of SER systems through its integrated data augmentation and hybrid modeling techniques, showcasing its potential for practical applications in HCI.

Abstract: Recognizing emotional signals in speech has a significant impact on enhancing
the effectiveness of human-computer interaction (HCI). This study introduces
EmoAugNet, a hybrid deep learning framework, that incorporates Long Short-Term
Memory (LSTM) layers with one-dimensional Convolutional Neural Networks
(1D-CNN) to enable reliable Speech Emotion Recognition (SER). The quality and
variety of the features that are taken from speech signals have a significant
impact on how well SER systems perform. A comprehensive speech data
augmentation strategy was used to combine both traditional methods, such as
noise addition, pitch shifting, and time stretching, with a novel
combination-based augmentation pipeline to enhance generalization and reduce
overfitting. Each audio sample was transformed into a high-dimensional feature
vector using root mean square energy (RMSE), Mel-frequency Cepstral Coefficient
(MFCC), and zero-crossing rate (ZCR). Our model with ReLU activation has a
weighted accuracy of 95.78\% and unweighted accuracy of 92.52\% on the IEMOCAP
dataset and, with ELU activation, has a weighted accuracy of 96.75\% and
unweighted accuracy of 91.28\%. On the RAVDESS dataset, we get a weighted
accuracy of 94.53\% and 94.98\% unweighted accuracy for ReLU activation and
93.72\% weighted accuracy and 94.64\% unweighted accuracy for ELU activation.
These results highlight EmoAugNet's effectiveness in improving the robustness
and performance of SER systems through integated data augmentation and hybrid
modeling.

</details>


### [345] [MeanAudio: Fast and Faithful Text-to-Audio Generation with Mean Flows](https://arxiv.org/abs/2508.06098)
*Xiquan Li,Junxi Liu,Yuzhe Liang,Zhikang Niu,Wenxi Chen,Xie Chen*

Main category: cs.SD

TL;DR: MeanAudio introduces a fast and efficient mechanism for text-to-audio generation, achieving up to 100x speedup compared to diffusion-based systems.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome the slow inference speed of current text-to-audio generation (TTA) systems, which limits their practical application despite achieving great synthesis quality.

Method: The authors propose MeanAudio, built on a MeanFlow-based model that leverages a Flux-style latent transformer, instantaneous-to-mean curriculum learning, and classifier-free guidance for efficient audio synthesis.

Result: MeanAudio achieves state-of-the-art (SOTA) single-step audio generation with a real-time factor (RTF) of 0.013, demonstrating a 100x speed improvement over diffusion-based systems and strong performance in multi-step generation.

Conclusion: Using innovative methodologies and optimizations, MeanAudio addresses the speed bottleneck in TTA systems, offering significant advancements in synthesis quality and rate, making it viable for practical applications.

Abstract: Recent developments in diffusion- and flow- based models have significantly
advanced Text-to-Audio Generation (TTA). While achieving great synthesis
quality and controllability, current TTA systems still suffer from slow
inference speed, which significantly limits their practical applicability. This
paper presents MeanAudio, a novel MeanFlow-based model tailored for fast and
faithful text-to-audio generation. Built on a Flux-style latent transformer,
MeanAudio regresses the average velocity field during training, enabling fast
generation by mapping directly from the start to the endpoint of the flow
trajectory. By incorporating classifier-free guidance (CFG) into the training
target, MeanAudio incurs no additional cost in the guided sampling process. To
further stabilize training, we propose an instantaneous-to-mean curriculum with
flow field mix-up, which encourages the model to first learn the foundational
instantaneous dynamics, and then gradually adapt to mean flows. This strategy
proves critical for enhancing training efficiency and generation quality.
Experimental results demonstrate that MeanAudio achieves state-of-the-art
performance in single-step audio generation. Specifically, it achieves a real
time factor (RTF) of 0.013 on a single NVIDIA RTX 3090, yielding a 100x speedup
over SOTA diffusion-based TTA systems. Moreover, MeanAudio also demonstrates
strong performance in multi-step generation, enabling smooth and coherent
transitions across successive synthesis steps.

</details>


### [346] [SpeakerLM: End-to-End Versatile Speaker Diarization and Recognition with Multimodal Large Language Models](https://arxiv.org/abs/2508.06372)
*Han Yin,Yafeng Chen,Chong Deng,Luyao Cheng,Hui Wang,Chao-Hong Tan,Qian Chen,Wen Wang,Xiangang Li*

Main category: cs.SD

TL;DR: SpeakerLM, a multimodal large language model, is introduced for end-to-end Speaker Diarization and Recognition (SDR), overcoming limitations of cascaded systems.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address key limitations in existing cascaded SDR systems, such as error propagation, challenges with overlapping speech, and the lack of joint optimization between Speaker Diarization (SD) and Automatic Speech Recognition (ASR).

Method: The authors propose SpeakerLM, a unified multimodal large language model for SDR that uses a multi-stage training strategy on large-scale real data to jointly perform SD and ASR in an end-to-end manner. A flexible speaker registration mechanism is also incorporated.

Result: SpeakerLM outperforms state-of-the-art cascaded baselines on both in-domain and out-of-domain SDR benchmarks, demonstrating strong data scaling ability and generalizability. The speaker registration mechanism enables robust performance under different settings.

Conclusion: The paper concludes that SpeakerLM effectively mitigates the limitations of existing systems, significantly improving SDR tasks and demonstrating robustness across diverse scenarios.

Abstract: The Speaker Diarization and Recognition (SDR) task aims to predict "who spoke
when and what" within an audio clip, which is a crucial task in various
real-world multi-speaker scenarios such as meeting transcription and dialogue
systems. Existing SDR systems typically adopt a cascaded framework, combining
multiple modules such as speaker diarization (SD) and automatic speech
recognition (ASR). The cascaded systems suffer from several limitations, such
as error propagation, difficulty in handling overlapping speech, and lack of
joint optimization for exploring the synergy between SD and ASR tasks. To
address these limitations, we introduce SpeakerLM, a unified multimodal large
language model for SDR that jointly performs SD and ASR in an end-to-end
manner. Moreover, to facilitate diverse real-world scenarios, we incorporate a
flexible speaker registration mechanism into SpeakerLM, enabling SDR under
different speaker registration settings. SpeakerLM is progressively developed
with a multi-stage training strategy on large-scale real data. Extensive
experiments show that SpeakerLM demonstrates strong data scaling capability and
generalizability, outperforming state-of-the-art cascaded baselines on both
in-domain and out-of-domain public SDR benchmarks. Furthermore, experimental
results show that the proposed speaker registration mechanism effectively
ensures robust SDR performance of SpeakerLM across diverse speaker registration
conditions and varying numbers of registered speakers.

</details>


### [347] [Robust Target Speaker Diarization and Separation via Augmented Speaker Embedding Sampling](https://arxiv.org/abs/2508.06393)
*Md Asif Jalal,Luca Remaggi,Vasileios Moschopoulos,Thanasis Kotsiopoulos,Vandana Rajan,Karthikeyan Saravanan,Anastasis Drosou,Junho Heo,Hyuk Oh,Seokyeong Jeong*

Main category: cs.SD

TL;DR: This paper proposes a method combining speech separation and speaker diarization without prior knowledge of target speakers, using automatic speaker embeddings.


<details>
  <summary>Details</summary>
Motivation: Current systems for speech separation and diarization rely heavily on speaker enrollment or predefined participant limits, which limits flexibility and scalability.

Method: The model uses a dual-stage training process to develop robust speaker embeddings and adds a spectral loss function to improve diarization in overlapping speech scenarios.

Result: The model achieves significant performance gains compared to state-of-the-art baselines: 71% improvement in Diarization Error Rate (DER) and 69% improvement in word error rate in overlapped speech (cpWER).

Conclusion: The introduced method effectively enhances speech separation and diarization accuracy, pushing boundaries for enrollment-free systems.

Abstract: Traditional speech separation and speaker diarization approaches rely on
prior knowledge of target speakers or a predetermined number of participants in
audio signals. To address these limitations, recent advances focus on
developing enrollment-free methods capable of identifying targets without
explicit speaker labeling. This work introduces a new approach to train
simultaneous speech separation and diarization using automatic identification
of target speaker embeddings, within mixtures. Our proposed model employs a
dual-stage training pipeline designed to learn robust speaker representation
features that are resilient to background noise interference. Furthermore, we
present an overlapping spectral loss function specifically tailored for
enhancing diarization accuracy during overlapped speech frames. Experimental
results show significant performance gains compared to the current SOTA
baseline, achieving 71% relative improvement in DER and 69% in cpWER.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [348] [Request-Only Optimization for Recommendation Systems](https://arxiv.org/abs/2508.05640)
*Liang Guo,Wei Li,Lucy Liao,Huihui Cheng,Rui Zhang,Yu Shi,Yueming Wang,Yanzun Huang,Keke Zhai,Pengchao Wang,Timothy Shi,Xuan Cao,Shengzhi Wang,Renqin Cai,Zhaojie Gong,Omkar Vichare,Rui Jian,Leon Gao,Shiyan Deng,Xingyu Liu,Xiong Zhang,Fu Li,Wenlei Xie,Bin Wen,Rui Li,Xing Liu,Jiaqi Zhai*

Main category: cs.IR

TL;DR: This paper introduces a Request-Only Optimizations (ROO) approach to improve storage, training efficiency, and recommendation model quality.


<details>
  <summary>Details</summary>
Motivation: DLRMs are highly complex systems requiring efficient algorithms and storage to handle massive petabytes of training data and trillions of computations per example.

Method: The paper proposes the ROO paradigm that uses a user's request as a unit for data and computation, enabling deduplication and co-design across data, infrastructure, and model architectures.

Result: Using ROO achieves savings in storage, reduces computational redundancy, and facilitates more advanced architectures like Generative Recommenders.

Conclusion: ROO represents a scalable and efficient advancement for industry-scale DLRMs, improving both quality and resource utilization.

Abstract: Deep Learning Recommendation Models (DLRMs) represent one of the largest
machine learning applications on the planet. Industry-scale DLRMs are trained
with petabytes of recommendation data to serve billions of users every day. To
utilize the rich user signals in the long user history, DLRMs have been scaled
up to unprecedented complexity, up to trillions of floating-point operations
(TFLOPs) per example. This scale, coupled with the huge amount of training
data, necessitates new storage and training algorithms to efficiently improve
the quality of these complex recommendation systems. In this paper, we present
a Request-Only Optimizations (ROO) training and modeling paradigm. ROO
simultaneously improves the storage and training efficiency as well as the
model quality of recommendation systems. We holistically approach this
challenge through co-designing data (i.e., request-only data), infrastructure
(i.e., request-only based data processing pipeline), and model architecture
(i.e., request-only neural architectures). Our ROO training and modeling
paradigm treats a user request as a unit of the training data. Compared with
the established practice of treating a user impression as a unit, our new
design achieves native feature deduplication in data logging, consequently
saving data storage. Second, by de-duplicating computations and communications
across multiple impressions in a request, this new paradigm enables highly
scaled-up neural network architectures to better capture user interest signals,
such as Generative Recommenders (GRs) and other request-only friendly
architectures.

</details>


### [349] [Query-Aware Graph Neural Networks for Enhanced Retrieval-Augmented Generation](https://arxiv.org/abs/2508.05647)
*Vibhor Agrawal,Fay Wang,Rishi Puri*

Main category: cs.IR

TL;DR: The paper proposes a novel graph neural network (GNN) architecture for retrieval-augmented generation (RAG) to enhance multi-hop question answering.


<details>
  <summary>Details</summary>
Motivation: To address limitations of traditional dense retrieval methods that fail to capture relationships between text chunks, essential for complex multi-hop questions.

Method: The authors designed a retrieval approach using per-episode knowledge graphs constructed with Enhanced Graph Attention Networks (EGAN) that include query-guided pooling.

Result: Experimental evaluation showed that their method improved retrieval accuracy, especially in multi-document reasoning tasks, surpassing standard dense retrievers.

Conclusion: The proposed method advances retrieval-augmented generation by integrating query-aware graph attention, making it effective and scalable for real-world complex question answering.

Abstract: We present a novel graph neural network (GNN) architecture for
retrieval-augmented generation (RAG) that leverages query-aware attention
mechanisms and learned scoring heads to improve retrieval accuracy on complex,
multi-hop questions. Unlike traditional dense retrieval methods that treat
documents as independent entities, our approach constructs per-episode
knowledge graphs that capture both sequential and semantic relationships
between text chunks. We introduce an Enhanced Graph Attention Network with
query-guided pooling that dynamically focuses on relevant parts of the graph
based on user queries. Experimental results demonstrate that our approach
significantly outperforms standard dense retrievers on complex question
answering tasks, particularly for questions requiring multi-document reasoning.
Our implementation leverages PyTorch Geometric for efficient processing of
graph-structured data, enabling scalable deployment in production retrieval
systems

</details>


### [350] [AquiLLM: a RAG Tool for Capturing Tacit Knowledge in Research Groups](https://arxiv.org/abs/2508.05648)
*Chandler Campbell,Bernie Boscoe,Tuan Do*

Main category: cs.IR

TL;DR: AquiLLM is a lightweight, modular Retrieval-Augmented Generation (RAG) system designed to help research groups manage and access their distributed, often undocumented knowledge, while addressing privacy concerns.


<details>
  <summary>Details</summary>
Motivation: Research groups face difficulties in managing and retrieving their fragmented, informal, and undocumented tacit knowledge, which is often spread across private resources like emails and meeting notes.

Method: AquiLLM incorporates a modular RAG system that supports multiple document types and allows for configurable privacy settings to cater specifically to research groups' unique knowledge-sharing needs.

Result: AquiLLM provides a solution for effective query and response generation by grounding them in source materials while preserving privacy and addressing the incomplete documentation of tacit knowledge.

Conclusion: AquiLLM helps research groups access formal and informal knowledge more effectively, supporting better knowledge management while respecting privacy concerns of internal resources.

Abstract: Research groups face persistent challenges in capturing, storing, and
retrieving knowledge that is distributed across team members. Although
structured data intended for analysis and publication is often well managed,
much of a group's collective knowledge remains informal, fragmented, or
undocumented--often passed down orally through meetings, mentoring, and
day-to-day collaboration. This includes private resources such as emails,
meeting notes, training materials, and ad hoc documentation. Together, these
reflect the group's tacit knowledge--the informal, experience-based expertise
that underlies much of their work. Accessing this knowledge can be difficult,
requiring significant time and insider understanding. Retrieval-augmented
generation (RAG) systems offer promising solutions by enabling users to query
and generate responses grounded in relevant source material. However, most
current RAG-LLM systems are oriented toward public documents and overlook the
privacy concerns of internal research materials. We introduce AquiLLM
(pronounced ah-quill-em), a lightweight, modular RAG system designed to meet
the needs of research groups. AquiLLM supports varied document types and
configurable privacy settings, enabling more effective access to both formal
and informal knowledge within scholarly groups.

</details>


### [351] [OmniBench-RAG: A Multi-Domain Evaluation Platform for Retrieval-Augmented Generation Tools](https://arxiv.org/abs/2508.05650)
*Jiaxuan Liang,Shide Zhou,Kailong Wang*

Main category: cs.IR

TL;DR: This paper introduces OmniBench RAG, a platform for standardized evaluation of Retrieval Augmented Generation (RAG) systems across multiple domains, emphasizing both accuracy and efficiency metrics.


<details>
  <summary>Details</summary>
Motivation: To address the lack of comprehensive, reproducible, and interpretable evaluation frameworks for RAG systems, which are critical in assessing their true performance across different models and knowledge domains.

Method: Introduced OmniBench RAG, which includes dynamic test generation, modular evaluation pipelines, and automated knowledge base construction, with two metrics: Improvements for accuracy and Transformation for efficiency.

Result: The evaluation showed substantial variability in RAG effectiveness across domains, with significant improvements in some fields like culture but declines in others, such as mathematics.

Conclusion: Standardized, domain-aware evaluations are essential to understand and compare the effectiveness of RAG systems accurately, highlighting domain-specific strengths and weaknesses.

Abstract: While Retrieval Augmented Generation (RAG) is now widely adopted to enhance
LLMs, evaluating its true performance benefits in a reproducible and
interpretable way remains a major hurdle. Existing methods often fall short:
they lack domain coverage, employ coarse metrics that miss sub document
precision, and fail to capture computational trade offs. Most critically, they
provide no standardized framework for comparing RAG effectiveness across
different models and domains.
  We introduce OmniBench RAG, a novel automated platform for multi domain
evaluation of RAG systems. The platform quantifies performance gains across
accuracy and efficiency dimensions, spanning nine knowledge fields including
culture, geography, and health. We introduce two standardized metrics:
Improvements (accuracy gains) and Transformation (efficiency differences
between pre RAG and post RAG models), enabling reproducible comparisons across
models and tasks. The platform features dynamic test generation, modular
evaluation pipelines, and automated knowledge base construction. Our evaluation
reveals striking variability in RAG effectiveness, from significant gains in
culture to declines in mathematics, highlighting the critical importance of
systematic, domain aware assessment. A demonstration video is available at:
https://www.youtube.com/watch?v=BZx83QFcTCI. Code and datasets:
https://github.com/Garnett-Liang/Omnibench-RAG.

</details>


### [352] [Lessons from A Large Language Model-based Outdoor Trail Recommendation Chatbot with Retrieval Augmented Generation](https://arxiv.org/abs/2508.05652)
*Julia Ann Mathew,Suining He*

Main category: cs.IR

TL;DR: This paper introduces Judy, a chatbot utilizing large language models (LLMs) and retrieval augmented generation (RAG) for outdoor trail recommendations.


<details>
  <summary>Details</summary>
Motivation: The rising popularity of outdoor activities necessitates an AI system that can provide precise suggestions and handle challenges in information accuracy and user usability.

Method: The system design includes web-based data collection, outdoor trail data management, and performance evaluation using case studies focused on Connecticut trails.

Result: Experimental findings highlight Judy’s effectiveness in delivering accurate, user-friendly trail recommendations.

Conclusion: The study proves that a RAG-based LLM chatbot can efficiently recommend personalized outdoor trails, addressing both functional and user needs.

Abstract: The increasing popularity of outdoor recreational activities (such as hiking
and biking) has boosted the demand for a conversational AI system to provide
informative and personalized suggestion on outdoor trails. Challenges arise in
response to (1) how to provide accurate outdoor trail information via
conversational AI; and (2) how to enable usable and efficient recommendation
services. To address above, this paper discusses the preliminary and practical
lessons learned from developing Judy, an outdoor trail recommendation chatbot
based on the large language model (LLM) with retrieval augmented generation
(RAG). To gain concrete system insights, we have performed case studies with
the outdoor trails in Connecticut (CT), US. We have conducted web-based data
collection, outdoor trail data management, and LLM model performance studies on
the RAG-based recommendation. Our experimental results have demonstrated the
accuracy, effectiveness, and usability of Judy in recommending outdoor trails
based on the LLM with RAG.

</details>


### [353] [Comparison of Information Retrieval Techniques Applied to IT Support Tickets](https://arxiv.org/abs/2508.05654)
*Leonardo Santiago Benitez Pereira,Robinson Pizzio,Samir Bonho*

Main category: cs.IR

TL;DR: The paper evaluates 11 Machine Learning-based Information Retrieval techniques for IT help desk ticket analysis, highlighting Sentence-BERT as the best performer.


<details>
  <summary>Details</summary>
Motivation: Improving IT support analyst efficiency by identifying an optimal method for ticket retrieval using past corrective actions.

Method: Comparison of 11 Information Retrieval techniques on IT support ticket datasets and development of a prototype system.

Result: Sentence-BERT achieved the highest relevance score (78.7%), surpassing TF-IDF, Word2vec, and LDA. All datasets and code were made open source.

Conclusion: The study identifies optimal retrieval techniques for IT support systems, proposes a novel metric, and establishes practical solutions with a prototype system.

Abstract: Institutions dependent on IT services and resources acknowledge the crucial
significance of an IT help desk system, that act as a centralized hub
connecting IT staff and users for service requests. Employing various Machine
Learning models, these IT help desk systems allow access to corrective actions
used in the past, but each model has different performance when applied to
different datasets. This work compares eleven Information Retrieval techniques
in a dataset of IT support tickets, with the goal of implementing a software
that facilitates the work of Information Technology support analysts. The best
results were obtained with the Sentence-BERT technique, in its multi-language
variation distilluse-base-multilingual-cased-v1, where 78.7% of the
recommendations made by the model were considered relevant. TF-IDF (69.0%),
Word2vec (68.7%) and LDA (66.3%) techniques also had consistent results.
Furthermore, the used datasets and essential parts of coding have been
published and made open source. It also demonstrated the practicality of a
support ticket recovery system by implementing a minimal viable prototype, and
described in detail the implementation of the system. Finally, this work
proposed a novel metric for comparing the techniques, whose aim is to closely
reflect the perception of the IT analysts about the retrieval quality.

</details>


### [354] [Beyond Single Labels: Improving Conversational Recommendation through LLM-Powered Data Augmentation](https://arxiv.org/abs/2508.05657)
*Haozhe Xu,Xiaohua Wang,Changze Lv,Xiaoqing Zheng*

Main category: cs.IR

TL;DR: This paper introduces a data augmentation framework to address false negative issues in conversational recommender systems by balancing semantic relevance and collaborative information, resulting in improved recommendation performance.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to tackle the false negative issue in conversational recommender systems, where items that users might like are incorrectly labeled as negatives during training, leading to poor recommendations.

Method: The proposed method involves a data augmentation framework. First, a semantic retriever based on a large language model identifies diverse, semantically relevant items. Then, a relevance scorer filters out noisy candidates. Finally, a two-stage training strategy is applied to balance semantic relevance and collaborative information.

Result: Extensive experiments on benchmark datasets and user simulators showed consistent and significant performance improvements in various recommenders.

Conclusion: The proposed framework effectively addresses false negative issues and advances conversational recommender system performance by enhancing recommendation quality.

Abstract: Conversational recommender systems (CRSs) enhance recommendation quality by
engaging users in multi-turn dialogues, capturing nuanced preferences through
natural language interactions. However, these systems often face the false
negative issue, where items that a user might like are incorrectly labeled as
negative during training, leading to suboptimal recommendations.Expanding the
label set through data augmentation presents an intuitive solution but faces
the challenge of balancing two key aspects: ensuring semantic relevance and
preserving the collaborative information inherent in CRS datasets. To address
these issues, we propose a novel data augmentation framework that first
leverages an LLM-based semantic retriever to identify diverse and semantically
relevant items, which are then filtered by a relevance scorer to remove noisy
candidates. Building on this, we introduce a two-stage training strategy
balancing semantic relevance and collaborative information. Extensive
experiments on two benchmark datasets and user simulators demonstrate
significant and consistent performance improvements across various
recommenders, highlighting the effectiveness of our approach in advancing CRS
performance.

</details>


### [355] [Open-Source Agentic Hybrid RAG Framework for Scientific Literature Review](https://arxiv.org/abs/2508.05660)
*Aditya Nagori,Ricardo Accorsi Casonatto,Ayush Gautam,Abhinav Manikantha Sai Cheruvu,Rishikesan Kamaleswaran*

Main category: cs.IR

TL;DR: The paper proposes an autonomous agent using Hybrid Retrieval Augmented Generation (RAG) that dynamically switches retrieval methods and quantifies uncertainty, leveraging bibliometric data and demonstrating improved reasoning for scientific discovery.


<details>
  <summary>Details</summary>
Motivation: The motivation is the increasing number of scientific publications, which makes traditional review methods inadequate, necessitating advanced tools integrating metadata and full-text analysis.

Method: The pipeline builds a Neo4j citation graph and FAISS vector store, powered by Llama-3.3-70B agent for dynamic switching between GraphRAG and VectorRAG approaches, incorporating instruction tuning and uncertainty estimation.

Result: The proposed system achieves marked improvements across metrics like recall, precision, and faithfulness in synthetic benchmarks, demonstrating enhanced query processing and reasoning over diverse scientific sources.

Conclusion: The agentic hybrid RAG framework succeeds in improving scalability and reproducibility in scientific discovery, creating a robust solution for navigating and analyzing scientific literature.

Abstract: The surge in scientific publications challenges traditional review methods,
demanding tools that integrate structured metadata with full-text analysis.
Hybrid Retrieval Augmented Generation (RAG) systems, combining graph queries
with vector search offer promise but are typically static, rely on proprietary
tools, and lack uncertainty estimates. We present an agentic approach that
encapsulates the hybrid RAG pipeline within an autonomous agent capable of (1)
dynamically selecting between GraphRAG and VectorRAG for each query, (2)
adapting instruction-tuned generation in real time to researcher needs, and (3)
quantifying uncertainty during inference. This dynamic orchestration improves
relevance, reduces hallucinations, and promotes reproducibility.
  Our pipeline ingests bibliometric open-access data from PubMed, arXiv, and
Google Scholar APIs, builds a Neo4j citation-based knowledge graph (KG), and
embeds full-text PDFs into a FAISS vector store (VS) using the all-MiniLM-L6-v2
model. A Llama-3.3-70B agent selects GraphRAG (translating queries to Cypher
for KG) or VectorRAG (combining sparse and dense retrieval with re-ranking).
Instruction tuning refines domain-specific generation, and bootstrapped
evaluation yields standard deviation for evaluation metrics.
  On synthetic benchmarks mimicking real-world queries, the Instruction-Tuned
Agent with Direct Preference Optimization (DPO) outperforms the baseline,
achieving a gain of 0.63 in VS Context Recall and a 0.56 gain in overall
Context Precision. Additional gains include 0.24 in VS Faithfulness, 0.12 in
both VS Precision and KG Answer Relevance, 0.11 in overall Faithfulness score,
0.05 in KG Context Recall, and 0.04 in both VS Answer Relevance and overall
Precision. These results highlight the system's improved reasoning over
heterogeneous sources and establish a scalable framework for autonomous,
agentic scientific discovery.

</details>


### [356] [Zero-Shot Retrieval for Scalable Visual Search in a Two-Sided Marketplace](https://arxiv.org/abs/2508.05661)
*Andre Rusli,Shoma Ishimoto,Sho Akiyama,Aman Kumar Singh*

Main category: cs.IR

TL;DR: The paper introduces a visual search system for Mercari's C2C marketplace using zero-shot vision-language models, achieving significant performance improvement in offline and online evaluations.


<details>
  <summary>Details</summary>
Motivation: To create an intuitive visual search experience for C2C marketplaces where unstructured and visually driven product listings can make search challenging for users.

Method: The system uses recent vision-language models for zero-shot image retrieval, integrating real-time inference, background indexing workflows, and a unified embedding pipeline optimized with dimensionality reduction. Performance is evaluated via offline user interaction logs and online A/B testing.

Result: The multilingual SigLIP model outperformed competitors, with a 13.3% increase in offline nDCG@5 and up to a 40.9% increase in transaction rate in an online A/B test, showcasing enhanced user engagement and conversion.

Conclusion: Zero-shot models can effectively serve as a strong baseline for scalable, production-ready visual search systems, offering flexibility for future fine-tuning while minimizing initial deployment complexity.

Abstract: Visual search offers an intuitive way for customers to explore diverse
product catalogs, particularly in consumer-to-consumer (C2C) marketplaces where
listings are often unstructured and visually driven. This paper presents a
scalable visual search system deployed in Mercari's C2C marketplace, where
end-users act as buyers and sellers. We evaluate recent vision-language models
for zero-shot image retrieval and compare their performance with an existing
fine-tuned baseline. The system integrates real-time inference and background
indexing workflows, supported by a unified embedding pipeline optimized through
dimensionality reduction. Offline evaluation using user interaction logs shows
that the multilingual SigLIP model outperforms other models across multiple
retrieval metrics, achieving a 13.3% increase in nDCG@5 over the baseline. A
one-week online A/B test in production further confirms real-world impact, with
the treatment group showing substantial gains in engagement and conversion, up
to a 40.9% increase in transaction rate via image search. Our findings
highlight that recent zero-shot models can serve as a strong and practical
baseline for production use, which enables teams to deploy effective visual
search systems with minimal overhead, while retaining the flexibility to
fine-tune based on future data or domain-specific needs.

</details>


### [357] [From Static to Dynamic: A Streaming RAG Approach to Real-time Knowledge Base](https://arxiv.org/abs/2508.05662)
*Yuzhou Zhu*

Main category: cs.IR

TL;DR: The paper introduces Streaming RAG, a method designed to improve dynamic retrieval-augmented generation frameworks by addressing challenges in real-time data streams such as high memory costs and latency.


<details>
  <summary>Details</summary>
Motivation: Static frameworks cannot efficiently keep up with dynamic streams, leading to limitations in latency, memory usage, and semantic coverage for retrieval tasks.

Method: A unified pipeline with multi-vector cosine screening, mini-batch clustering, and heavy-hitter filtering maintains a compact prototype set alongside an incremental index update mechanism.

Result: The proposed approach improved Recall@10 (up to 3 points), reduced latency (<15 ms), handled high throughput (>900 docs/sec), and achieved gains in exact match and F1 using GPT-3.5 Turbo.

Conclusion: Streaming RAG sets a new benchmark for dynamic retrieval augmentation, delivering efficient data freshness and improved performance across diverse real-time applications.

Abstract: Dynamic streams from news feeds, social media, sensor networks, and financial
markets challenge static RAG frameworks. Full-scale indices incur high memory
costs; periodic rebuilds introduce latency that undermines data freshness;
naive sampling sacrifices semantic coverage. We present Streaming RAG, a
unified pipeline that combines multi-vector cosine screening, mini-batch
clustering, and a counter-based heavy-hitter filter to maintain a compact
prototype set. We further prove an approximation bound \$E\[R(K\_t)] \ge R^\* -
L \Delta\$ linking retrieval quality to clustering variance. An incremental
index upsert mechanism refreshes prototypes without interrupting queries.
Experiments on eight real-time streams show statistically significant gains in
Recall\@10 (up to 3 points, p < 0.01), end-to-end latency below 15 ms, and
throughput above 900 documents per second under a 150 MB budget. Hyperparameter
sensitivity analysis over cluster count, admission probability, relevance
threshold, and counter capacity validates default settings. In open-domain
question answering with GPT-3.5 Turbo, we record 3.2-point gain in Exact Match
and 2.8-point gain in F1 on SQuAD; abstractive summarization yields ROUGE-L
improvements. Streaming RAG establishes a new Pareto frontier for retrieval
augmentation.

</details>


### [358] [Enhancing Retrieval-Augmented Generation for Electric Power Industry Customer Support](https://arxiv.org/abs/2508.05664)
*Hei Yu Chan,Kuok Tou Ho,Chenglong Ma,Yujing Si,Hok Lai Lin,Sa Lei Lam*

Main category: cs.IR

TL;DR: The study evaluates AI techniques—query rewriting, RAG Fusion, intent recognition, and reranking—and selects a graph-based RAG framework to build an advanced customer support system for the electric power domain, achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: Customer service AI systems struggle with ambiguous, multi-intent, or detail-specific queries, necessitating improved methods for handling such complexity.

Method: The study compared vector-store and graph-based RAG frameworks while applying query rewriting, RAG Fusion, keyword augmentation, intent recognition, and context reranking techniques.

Result: The graph-based RAG framework was superior in handling complex queries. Critical components like query rewriting, RAG Fusion, and intent recognition improved the system's performance, reaching 97.9% accuracy on a GPT-4 dataset and 89.6% on a real-world dataset.

Conclusion: Combining intent recognition, RAG Fusion, and reranking creates a robust solution for complex, ambiguous queries in customer support, outperforming baseline models.

Abstract: Many AI customer service systems use standard NLP pipelines or finetuned
language models, which often fall short on ambiguous, multi-intent, or
detail-specific queries. This case study evaluates recent techniques: query
rewriting, RAG Fusion, keyword augmentation, intent recognition, and context
reranking, for building a robust customer support system in the electric power
domain. We compare vector-store and graph-based RAG frameworks, ultimately
selecting the graph-based RAG for its superior performance in handling complex
queries. We find that query rewriting improves retrieval for queries using
non-standard terminology or requiring precise detail. RAG Fusion boosts
performance on vague or multifaceted queries by merging multiple retrievals.
Reranking reduces hallucinations by filtering irrelevant contexts. Intent
recognition supports the decomposition of complex questions into more targeted
sub-queries, increasing both relevance and efficiency. In contrast, keyword
augmentation negatively impacts results due to biased keyword selection. Our
final system combines intent recognition, RAG Fusion, and reranking to handle
disambiguation and multi-source queries. Evaluated on both a GPT-4-generated
dataset and a real-world electricity provider FAQ dataset, it achieves 97.9%
and 89.6% accuracy respectively, substantially outperforming baseline RAG
models.

</details>


### [359] [HySemRAG: A Hybrid Semantic Retrieval-Augmented Generation Framework for Automated Literature Synthesis and Methodological Gap Analysis](https://arxiv.org/abs/2508.05666)
*Alejandro Godinez*

Main category: cs.IR

TL;DR: HySemRAG is an ETL-RAG framework that automates large-scale literature synthesis, identifying research gaps and achieving high accuracy in citation verification.


<details>
  <summary>Details</summary>
Motivation: The study aims to improve RAG architectures for automated literature synthesis, addressing limitations in traceability, semantic understanding, and scalability.

Method: HySemRAG employs hybrid retrieval (semantic search, keyword filtering, knowledge graphs), LLM-based field extraction, and quality assurance mechanisms across an 8-stage pipeline for processing scholarly literature.

Result: The system outperforms traditional approaches in semantic similarity by 35.1%, achieves 99.0% citation accuracy, and demonstrates applicability in identifying trends in specific domains like geospatial epidemiology.

Conclusion: HySemRAG is a robust infrastructure for automating evidence synthesis, offering traceable, highly accurate, and scalable solutions for identifying methodological trends and gaps across diverse scientific fields.

Abstract: We present HySemRAG, a framework that combines Extract, Transform, Load (ETL)
pipelines with Retrieval-Augmented Generation (RAG) to automate large-scale
literature synthesis and identify methodological research gaps. The system
addresses limitations in existing RAG architectures through a multi-layered
approach: hybrid retrieval combining semantic search, keyword filtering, and
knowledge graph traversal; an agentic self-correction framework with iterative
quality assurance; and post-hoc citation verification ensuring complete
traceability. Our implementation processes scholarly literature through eight
integrated stages: multi-source metadata acquisition, asynchronous PDF
retrieval, custom document layout analysis using modified Docling architecture,
bibliographic management, LLM-based field extraction, topic modeling, semantic
unification, and knowledge graph construction. The system creates dual data
products - a Neo4j knowledge graph enabling complex relationship queries and
Qdrant vector collections supporting semantic search - serving as foundational
infrastructure for verifiable information synthesis. Evaluation across 643
observations from 60 testing sessions demonstrates structured field extraction
achieving 35.1% higher semantic similarity scores (0.655 $\pm$ 0.178) compared
to PDF chunking approaches (0.485 $\pm$ 0.204, p < 0.000001). The agentic
quality assurance mechanism achieves 68.3% single-pass success rates with 99.0%
citation accuracy in validated responses. Applied to geospatial epidemiology
literature on ozone exposure and cardiovascular disease, the system identifies
methodological trends and research gaps, demonstrating broad applicability
across scientific domains for accelerating evidence synthesis and discovery.

</details>


### [360] [ITDR: An Instruction Tuning Dataset for Enhancing Large Language Models in Recommendations](https://arxiv.org/abs/2508.05667)
*Zekun Liu,Xiaowen Huang,Jitao Sang*

Main category: cs.IR

TL;DR: Large language models (LLMs) struggle with recommendation tasks due to structural data differences. This paper introduces ITDR, a robust instruction tuning dataset, to enhance LLMs' recommendation capabilities.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in LLM performance on recommendation tasks caused by differences between user behavior data and natural language data.

Method: A comprehensive instruction tuning dataset called ITDR was created, incorporating data from 13 public datasets and consisting of 200,000 instances derived from two root tasks and 7 subtasks.

Result: Experimental results show that ITDR improved the recommendation performance of mainstream open-source LLMs like GLM-4 and LLaMA-3.2. Further experiments analyzed task correlations, data scale, and compared against closed-source LLMs.

Conclusion: Constructing an instruction tuning dataset tailored to recommendation tasks can significantly enhance the performance of LLMs in this domain. The dataset and fine-tuned models are publicly available.

Abstract: Large language models (LLMs) have demonstrated outstanding performance in
natural language processing tasks. However, in the field of recommendation
systems, due to the structural differences between user behavior data and
natural language, LLMs struggle to effectively model the associations between
user preferences and items. Although prompt-based methods can generate
recommendation results, their inadequate understanding of recommendation tasks
leads to constrained performance. To address this gap, in this work, we
construct a sufficient instruction tuning dataset, ITDR, which encompasses 7
subtasks across two core root tasks--user-item interaction and user-item
understanding. The dataset integrates data from 13 public recommendation
datasets and is built using manually crafted standardized templates, comprising
approximately 200,000 instances. Experimental results demonstrate that ITDR
significantly enhances the performance of mainstream open-source LLMs such as
GLM-4, Qwen2.5, Qwen2.5-Instruct and LLaMA-3.2 on recommendation tasks.
Furthermore, we analyze the correlations between tasks and explore the impact
of task descriptions and data scale on instruction tuning effectiveness.
Finally, we perform comparative experiments against closed-source LLMs with
substantial parameters. Our tuning dataset ITDR and the fine-tuned large
recommendation models can be accessed at https://github.com/hellolzk/ITDR.

</details>


### [361] [A Survey of LLM-based Deep Search Agents: Paradigm, Optimization, Evaluation, and Challenges](https://arxiv.org/abs/2508.05668)
*Yunjia Xi,Jianghao Lin,Yongzhao Xiao,Zheli Zhou,Rong Shan,Te Gao,Jiachen Zhu,Weiwen Liu,Yong Yu,Weinan Zhang*

Main category: cs.IR

TL;DR: Large Language Model-based Search Agents significantly enhance web search with dynamic planning and multi-turn retrieval capabilities. This survey systematically categorizes research in this field and highlights future challenges and directions.


<details>
  <summary>Details</summary>
Motivation: To address the growing capabilities of LLM-based Search Agents and their extensive applications in deeper, autonomous information retrieval.

Method: Systematically analyzing and categorizing existing literature in terms of architecture, optimization, application, and evaluation, while providing a curated repository.

Result: Critical challenges and promising research directions in LLM-based search agents are identified, providing a roadmap for advancement.

Conclusion: LLM-based search agents represent a transformative approach to information retrieval, warranting further research to tackle challenges and unlock their full potential.

Abstract: The advent of Large Language Models (LLMs) has significantly revolutionized
web search. The emergence of LLM-based Search Agents marks a pivotal shift
towards deeper, dynamic, autonomous information seeking. These agents can
comprehend user intentions and environmental context and execute multi-turn
retrieval with dynamic planning, extending search capabilities far beyond the
web. Leading examples like OpenAI's Deep Research highlight their potential for
deep information mining and real-world applications. This survey provides the
first systematic analysis of search agents. We comprehensively analyze and
categorize existing works from the perspectives of architecture, optimization,
application, and evaluation, ultimately identifying critical open challenges
and outlining promising future research directions in this rapidly evolving
field. Our repository is available on
https://github.com/YunjiaXi/Awesome-Search-Agent-Papers.

</details>


### [362] [Fine-Tuning Vision-Language Models for Markdown Conversion of Financial Tables in Malaysian Audited Financial Reports](https://arxiv.org/abs/2508.05669)
*Jin Khye Tan,En Jun Choong,Ethan Jeremiah Chitty,Yan Pheng Choo,John Hsin Yang Wong,Chern Eu Cheah*

Main category: cs.IR

TL;DR: The paper proposes a fine-tuned vision-language model to convert complex financial tables into Markdown with high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Financial documents often contain complex tabular data essential for regulatory and analytical tasks, but accurately extracting and representing this data remains a challenge.

Method: The study uses a fine-tuned VLM based on Qwen2.5-VL-7B, trained on a curated dataset of 2,152 image-text pairs with LoRA supervision and assessed using a criteria-based LLM and Markdown TEDS metric.

Result: The proposed model achieved 92.20% accuracy on criteria-based assessment and 96.53% Markdown TEDS score, outperforming baseline models, larger VLMs, and proprietary systems like GPT-4o and Gemini.

Conclusion: Domain-specific fine-tuning is efficient for extracting and converting unstructured financial data with high accuracy, rivaling larger general models while minimizing computational costs.

Abstract: Accurately extracting and representing the structure of tabular data from
financial documents remains a critical challenge in document understanding,
particularly for regulatory and analytical use cases. This study addresses the
complexity of converting financial tables from Malaysian audited financial
reports into Markdown format, a task complicated by rotated layouts,
multi-level headers, and implicit structural cues. We propose a fine-tuned
vision-language model (VLM), based on Qwen2.5-VL-7B, optimized for
high-fidelity Markdown generation from document images. Our approach includes a
curated dataset of 2,152 image-text pairs with augmentations and a supervised
fine-tuning strategy using LoRA. To assess performance, we evaluated our model
on 100 out-of-sample tables using a dual framework: a criteria-based
LLM-as-a-judge for fine-grained accuracy and our novel Markdown
Tree-Edit-Distance-based Similarity (TEDS) metric for holistic structural
fidelity. Our model achieves a 92.20% overall accuracy on the criteria-based
assessment and a 96.53% Markdown TEDS score. This performance significantly
surpasses its Qwen2.5-VL-7B base model, larger-scale VLMs, and specialized
reasoning-enabled models. Compared to these self-hosted alternatives, it also
significantly reduces inference time. Furthermore, its accuracy exceeds that of
widely used proprietary models such as OpenAI's GPT-4o and Gemini 2.5 Flash.
These results demonstrate that domain-specific fine-tuning provides an
effective and efficient method to bridge the gap between unstructured financial
documents and downstream automation, rivalling much larger and more general
models without their computational overhead.

</details>


### [363] [LMAR: Language Model Augmented Retriever for Domain-specific Knowledge Indexing](https://arxiv.org/abs/2508.05672)
*Yao Zhao,Yantian Ding,Zhiyue Zhang,Dapeng Yao,Yanxun Xu*

Main category: cs.IR

TL;DR: The paper introduces LMAR, a framework designed to address domain-specific retrieval challenges in RAG systems through LLM-based data synthesis and embedding adaptation techniques.


<details>
  <summary>Details</summary>
Motivation: Existing RAG systems struggle with domain-specific retrieval due to limitations in embeddings and high computational costs. Reliable and scalable solutions are needed for better contextual adaptation.

Method: LMAR uses a two-stage pipeline that involves LLM-powered data augmentation and contrastive embedding adaptation, coupled with efficient text clustering strategies.

Result: LMAR outperforms baseline models on domain-specific benchmark datasets while maintaining moderate hardware requirements and low latency.

Conclusion: LMAR emerges as a scalable, cost-effective, and model-agnostic framework suitable for enhancing domain-specific retrieval in RAG architectures.

Abstract: Retrieval Augmented Generation (RAG) systems often struggle with
domain-specific knowledge due to performance deterioration of pre-trained
embeddings and prohibitive computational costs of large language model
(LLM)-based retrievers. While fine-tuning data augmentation embedding models
offers a promising direction, its effectiveness is limited by the need for
high-quality training data and reliable chunking strategies that preserve
contextual integrity. We propose LMAR (Language Model Augmented Retriever), a
model-agnostic framework that addresses these challenges by combining
LLM-guided data synthesis with contrastive embedding adaptation and efficient
text clustering. LMAR consists of a two-stage pipeline: (1) Triplet sampling
and synthetic data augmentation, where LLMs act as both labeler and validator
to ensure high-fidelity supervision throughout the pipeline. Experimental
results across multiple domain-specific benchmark datasets demonstrate that
LMAR outperforms multiple baseline models, while maintaining moderate hardware
requirements and low latency. Its model-agnostic nature further enables
seamless integration with emerging RAG architectures and text embedding models,
ensuring continual improvements without redesigning the pipeline. These results
highlight LMAR as a practical and cost-effective solution for scalable
domain-specific adaptation.

</details>


### [364] [Breaking the Top-$K$ Barrier: Advancing Top-$K$ Ranking Metrics Optimization in Recommender Systems](https://arxiv.org/abs/2508.05673)
*Weiqin Yang,Jiawei Chen,Shengjia Zhang,Peng Wu,Yuegang Sun,Yan Feng,Chun Chen,Can Wang*

Main category: cs.IR

TL;DR: The paper introduces SoftmaxLoss@$K$ (SL@$K$), a new loss function that improves NDCG@$K$ optimization in recommender systems with better computational efficiency and stability, achieving a 6.03% improvement on average.


<details>
  <summary>Details</summary>
Motivation: Optimizing NDCG@$K$, the standard Top-$K$ ranking metric in recommender systems, is challenging due to its discontinuity and Top-$K$ truncation. Existing methods either ignore these challenges or are computationally costly and unstable.

Method: The proposed SL@$K$ loss incorporates a quantile technique for handling Top-$K$ truncation and introduces a smooth upper bound to address the discontinuity in NDCG@$K$ optimization. This approach ensures theoretical robustness and computational efficiency.

Result: Extensive experiments across four real-world datasets and three recommendation backbones show that SL@$K$ outperforms existing losses, achieving a significant average improvement of 6.03%.

Conclusion: The SL@$K$ loss offers an effective and practical solution for NDCG@$K$ optimization in recommendation systems, providing computational efficiency, stability, and superior performance in experimental evaluations.

Abstract: In the realm of recommender systems (RS), Top-$K$ ranking metrics such as
NDCG@$K$ are the gold standard for evaluating recommendation performance.
However, during the training of recommendation models, optimizing NDCG@$K$
poses significant challenges due to its inherent discontinuous nature and the
intricate Top-$K$ truncation. Recent efforts to optimize NDCG@$K$ have either
overlooked the Top-$K$ truncation or suffered from high computational costs and
training instability. To overcome these limitations, we propose SoftmaxLoss@$K$
(SL@$K$), a novel recommendation loss tailored for NDCG@$K$ optimization.
Specifically, we integrate the quantile technique to handle Top-$K$ truncation
and derive a smooth upper bound for optimizing NDCG@$K$ to address
discontinuity. The resulting SL@$K$ loss has several desirable properties,
including theoretical guarantees, ease of implementation, computational
efficiency, gradient stability, and noise robustness. Extensive experiments on
four real-world datasets and three recommendation backbones demonstrate that
SL@$K$ outperforms existing losses with a notable average improvement of 6.03%.
The code is available at https://github.com/Tiny-Snow/IR-Benchmark.

</details>


### [365] [AI Guided Accelerator For Search Experience](https://arxiv.org/abs/2508.05649)
*Jayanth Yetukuri,Mehran Elyasi,Samarth Agrawal,Aritra Mandal,Rui Kong,Harish Vempati,Ishita Khan*

Main category: cs.IR

TL;DR: The paper addresses improving e-commerce query reformulation by modeling transitional queries, constructing query trajectories, and leveraging Large Language Models (LLMs) for generating intent-preserving alternatives, achieving better engagement and conversion metrics.


<details>
  <summary>Details</summary>
Motivation: Traditional query reformulation methods fail to capture the sequential progression and transitions in user intent during exploratory searches in e-commerce.

Method: The authors mine large-scale user interaction logs from eBay to build structured query sequences reflecting intent refinement and leverage generative LLMs to generate semantically diverse query alternatives.

Result: Empirical evaluations show improved conversion and engagement metrics for exploratory searches over traditional methods like Related Searches.

Conclusion: Modeling transitional queries and applying LLMs for query expansion improves both discovery and engagement, validating the approach for real-world e-commerce applications.

Abstract: Effective query reformulation is pivotal in narrowing the gap between a
user's exploratory search behavior and the identification of relevant products
in e-commerce environments. While traditional approaches predominantly model
query rewrites as isolated pairs, they often fail to capture the sequential and
transitional dynamics inherent in real-world user behavior. In this work, we
propose a novel framework that explicitly models transitional
queries--intermediate reformulations occurring during the user's journey toward
their final purchase intent. By mining structured query trajectories from
eBay's large-scale user interaction logs, we reconstruct query sequences that
reflect shifts in intent while preserving semantic coherence. This approach
allows us to model a user's shopping funnel, where mid-journey transitions
reflect exploratory behavior and intent refinement. Furthermore, we incorporate
generative Large Language Models (LLMs) to produce semantically diverse and
intent-preserving alternative queries, extending beyond what can be derived
through collaborative filtering alone. These reformulations can be leveraged to
populate Related Searches or to power intent-clustered carousels on the search
results page, enhancing both discovery and engagement. Our contributions
include (i) the formal identification and modeling of transitional queries,
(ii) the introduction of a structured query sequence mining pipeline for intent
flow understanding, and (iii) the application of LLMs for scalable,
intent-aware query expansion. Empirical evaluation demonstrates measurable
gains in conversion and engagement metrics compared to the existing Related
Searches module, validating the effectiveness of our approach in real-world
e-commerce settings.

</details>


### [366] [Are All Genders Equal in the Eyes of Algorithms? -- Analysing Search and Retrieval Algorithms for Algorithmic Gender Fairness](https://arxiv.org/abs/2508.05680)
*Stefanie Urchs,Veronika Thurner,Matthias Aßenmacher,Ludwig Bothmann,Christian Heumann,Stephanie Thiemichen*

Main category: cs.IR

TL;DR: The paper explores how algorithmic systems may perpetuate gender biases in academic visibility and emphasizes the need for fairness in these systems.


<details>
  <summary>Details</summary>
Motivation: To investigate gender-related biases in algorithmic systems used in academia, like search engines and academic databases.

Method: A bias-preserving definition of algorithmic gender fairness is proposed, applied to analyze gender differences in metadata completeness, publication retrieval, and Google search visibility using a dataset of German academic profiles.

Result: Male professors typically have more search results and aligned publication records, while female professors face greater variability in visibility, even without overt algorithmic discrimination.

Conclusion: Fairness evaluations of algorithmic systems should address both technical performance and representational equality.

Abstract: Algorithmic systems such as search engines and information retrieval
platforms significantly influence academic visibility and the dissemination of
knowledge. Despite assumptions of neutrality, these systems can reproduce or
reinforce societal biases, including those related to gender. This paper
introduces and applies a bias-preserving definition of algorithmic gender
fairness, which assesses whether algorithmic outputs reflect real-world gender
distributions without introducing or amplifying disparities. Using a
heterogeneous dataset of academic profiles from German universities and
universities of applied sciences, we analyse gender differences in metadata
completeness, publication retrieval in academic databases, and visibility in
Google search results. While we observe no overt algorithmic discrimination,
our findings reveal subtle but consistent imbalances: male professors are
associated with a greater number of search results and more aligned publication
records, while female professors display higher variability in digital
visibility. These patterns reflect the interplay between platform algorithms,
institutional curation, and individual self-presentation. Our study highlights
the need for fairness evaluations that account for both technical performance
and representational equality in digital systems.

</details>


### [367] [Domain-Specific Fine-Tuning and Prompt-Based Learning: A Comparative Study for developing Natural Language-Based BIM Information Retrieval Systems](https://arxiv.org/abs/2508.05676)
*Han Gao,Timo Hartmann,Botao Zhong,Kai Lia,Hanbin Luo*

Main category: cs.IR

TL;DR: The paper compares domain-specific fine-tuning and prompt-based learning using LLMs in developing NLI systems for BIM data retrieval.


<details>
  <summary>Details</summary>
Motivation: There is a need for effective Natural Language Interface systems to simplify and enhance data retrieval in complex BIM scenarios.

Method: Two-stage framework using intent recognition and table-based question answering is tested on a dataset of 1,740 annotated queries across 69 BIM models.

Result: Domain-specific fine-tuning excelled in intent recognition, while prompt-based learning (e.g., GPT-4) performed better in table-based question answering. A hybrid approach combining both performed robustly.

Conclusion: Hybrid methods integrating fine-tuning and LLMs for specific tasks are promising for designing advanced NLI-based BIM systems, with real-world evaluations showing balanced effectiveness.

Abstract: Building Information Modeling (BIM) is essential for managing building data
across the entire lifecycle, supporting tasks from design to maintenance.
Natural Language Interface (NLI) systems are increasingly explored as
user-friendly tools for information retrieval in Building Information Modeling
(BIM) environments. Despite their potential, accurately extracting BIM-related
data through natural language queries remains a persistent challenge due to the
complexity use queries and specificity of domain knowledge. This study presents
a comparative analysis of two prominent approaches for developing NLI-based BIM
information retrieval systems: domain-specific fine-tuning and prompt-based
learning using large language models (LLMs). A two-stage framework consisting
of intent recognition and table-based question answering is implemented to
evaluate the effectiveness of both approaches. To support this evaluation, a
BIM-specific dataset of 1,740 annotated queries of varying types across 69
models is constructed. Experimental results show that domain-specific
fine-tuning delivers superior performance in intent recognition tasks, while
prompt-based learning, particularly with GPT-4o, shows strength in table-based
question answering. Based on these findings, this study identify a hybrid
configuration that combines fine-tuning for intent recognition with
prompt-based learning for question answering, achieving more balanced and
robust performance across tasks. This integrated approach is further tested
through case studies involving BIM models of varying complexity. This study
provides a systematic analysis of the strengths and limitations of each
approach and discusses the applicability of the NLI to real-world BIM
scenarios. The findings offer insights for researchers and practitioners in
designing intelligent, language-driven BIM systems.

</details>


### [368] [Multi-Faceted Large Embedding Tables for Pinterest Ads Ranking](https://arxiv.org/abs/2508.05700)
*Runze Su,Jiayin Jin,Jiacheng Li,Sihan Wang,Guangtong Bai,Zelun Wang,Li Tang,Yixiong Meng,Huasen Wu,Zhimeng Pan,Kungang Li,Han Sun,Zhifang Liu,Haoyang Li,Siping Ji,Ling Leng,Prathibha Deshikachar*

Main category: cs.IR

TL;DR: The paper addresses challenges in using large embedding tables in Pinterest Ads ranking models, introduces a novel pretraining scheme and CPU-GPU hybrid infrastructure, leading to improved CTR, CVR, and CPC performance.


<details>
  <summary>Details</summary>
Motivation: To enhance the performance of Pinterest's ad-ranking models by tackling scalability issues, sparsity, and challenges in training large embedding tables effectively.

Method: Introduced a multi-faceted pretraining scheme combining various algorithms and developed a CPU-GPU hybrid serving infrastructure to manage memory limits and scalability.

Result: Achieved substantial performance improvements, including a 1.34% reduction in CPC, a 2.60% increase in CTR, and neutral latency change upon deployment in the Pinterest Ads system.

Conclusion: The novel approaches improved performance metrics of Pinterest Ads significantly, demonstrating the utility of optimized embedding table pretraining and hybrid serving frameworks in real-world systems.

Abstract: Large embedding tables are indispensable in modern recommendation systems,
thanks to their ability to effectively capture and memorize intricate details
of interactions among diverse entities. As we explore integrating large
embedding tables into Pinterest's ads ranking models, we encountered not only
common challenges such as sparsity and scalability, but also several obstacles
unique to our context. Notably, our initial attempts to train large embedding
tables from scratch resulted in neutral metrics. To tackle this, we introduced
a novel multi-faceted pretraining scheme that incorporates multiple pretraining
algorithms. This approach greatly enriched the embedding tables and resulted in
significant performance improvements. As a result, the multi-faceted large
embedding tables bring great performance gain on both the Click-Through Rate
(CTR) and Conversion Rate (CVR) domains. Moreover, we designed a CPU-GPU hybrid
serving infrastructure to overcome GPU memory limits and elevate the
scalability. This framework has been deployed in the Pinterest Ads system and
achieved 1.34% online CPC reduction and 2.60% CTR increase with neutral
end-to-end latency change.

</details>


### [369] [G-UBS: Towards Robust Understanding of Implicit Feedback via Group-Aware User Behavior Simulation](https://arxiv.org/abs/2508.05709)
*Boyu Chen,Siran Chen,Zhengrong Yue,Kainan Yan,Chenyun Yu,Beibei Kong,Cheng Lei,Chengxiang Zhuo,Zang Li,Yali Wang*

Main category: cs.IR

TL;DR: Explicit feedback in recommendation systems is rare, making implicit feedback crucial yet noisy. This paper introduces "Group-aware User Behavior Simulation" (G-UBS) leveraging contextual user groups to enhance feedback interpretation.


<details>
  <summary>Details</summary>
Motivation: Implicit feedback is easy to obtain but suffers from noise, making it challenging to accurately infer user preferences.

Method: G-UBS uses a User Group Manager (UGM) to cluster user groups with an LLM-based workflow, and a User Feedback Modeler (UFM) employs group-aware reinforcement learning.

Result: G-UBS demonstrates superior performance, with a 4.0% increase in video play rate >30% and a 14.9% boost in reasoning accuracy on the IF-VR benchmark.

Conclusion: The approach provides a robust mechanism to interpret implicit feedback, improving recommendation efficacy and creating a validated multi-modal benchmark.

Abstract: User feedback is critical for refining recommendation systems, yet explicit
feedback (e.g., likes or dislikes) remains scarce in practice. As a more
feasible alternative, inferring user preferences from massive implicit feedback
has shown great potential (e.g., a user quickly skipping a recommended video
usually indicates disinterest). Unfortunately, implicit feedback is often
noisy: a user might skip a video due to accidental clicks or other reasons,
rather than disliking it. Such noise can easily misjudge user interests,
thereby undermining recommendation performance. To address this issue, we
propose a novel Group-aware User Behavior Simulation (G-UBS) paradigm, which
leverages contextual guidance from relevant user groups, enabling robust and
in-depth interpretation of implicit feedback for individual users.
Specifically, G-UBS operates via two key agents. First, the User Group Manager
(UGM) effectively clusters users to generate group profiles utilizing a
``summarize-cluster-reflect" workflow based on LLMs. Second, the User Feedback
Modeler (UFM) employs an innovative group-aware reinforcement learning
approach, where each user is guided by the associated group profiles during the
reinforcement learning process, allowing UFM to robustly and deeply examine the
reasons behind implicit feedback. To assess our G-UBS paradigm, we have
constructed a Video Recommendation benchmark with Implicit Feedback (IF-VR). To
the best of our knowledge, this is the first multi-modal benchmark for implicit
feedback evaluation in video recommendation, encompassing 15k users, 25k
videos, and 933k interaction records with implicit feedback. Extensive
experiments on IF-VR demonstrate that G-UBS significantly outperforms
mainstream LLMs and MLLMs, with a 4.0% higher proportion of videos achieving a
play rate > 30% and 14.9% higher reasoning accuracy on IF-VR.

</details>


### [370] [eSASRec: Enhancing Transformer-based Recommendations in a Modular Fashion](https://arxiv.org/abs/2508.06450)
*Daria Tikhonovich,Nikita Zelinskiy,Aleksandr V. Petrov,Mayya Spirina,Andrei Semenov,Andrey V. Savchenko,Sergei Kuliev*

Main category: cs.IR

TL;DR: The paper introduces eSASRec, an enhanced Transformer-based model for sequential recommendations, combining improved architecture, training objective, and loss functions. It is 23% more effective on benchmarks than recent state-of-the-art models while being simple to integrate.


<details>
  <summary>Details</summary>
Motivation: Close the gap in systematically benchmarking the additivity of modular improvements in Transformer-based sequential recommendation models.

Method: Combine modular improvements like SASRec's objective, LiGR Transformer layers, and Sampled Softmax Loss to develop eSASRec and benchmark its effectiveness against academic and production-like datasets.

Result: eSASRec outperforms recent state-of-the-art models by 23% on academic benchmarks and performs excellently in production-like settings, residing on the Pareto frontier for the accuracy-coverage tradeoff.

Conclusion: eSASRec is a powerful, simple model that improves recommendation pipelines without extra features. Its accessibility and effectiveness make it a solid baseline for future algorithm developments, with open-source implementations provided.

Abstract: Since their introduction, Transformer-based models, such as SASRec and
BERT4Rec, have become common baselines for sequential recommendations,
surpassing earlier neural and non-neural methods. A number of following
publications have shown that the effectiveness of these models can be improved
by, for example, slightly updating the architecture of the Transformer layers,
using better training objectives, and employing improved loss functions.
However, the additivity of these modular improvements has not been
systematically benchmarked - this is the gap we aim to close in this paper.
Through our experiments, we identify a very strong model that uses SASRec's
training objective, LiGR Transformer layers, and Sampled Softmax Loss. We call
this combination eSASRec (Enhanced SASRec). While we primarily focus on
realistic, production-like evaluation, in our preliminarily study we find that
common academic benchmarks show eSASRec to be 23% more effective compared to
the most recent state-of-the-art models, such as ActionPiece. In our main
production-like benchmark, eSASRec resides on the Pareto frontier in terms of
the accuracy-coverage tradeoff (alongside the recent industrial models HSTU and
FuXi. As the modifications compared to the original SASRec are relatively
straightforward and no extra features are needed (such as timestamps in HSTU),
we believe that eSASRec can be easily integrated into existing recommendation
pipelines and can can serve as a strong yet very simple baseline for emerging
complicated algorithms. To facilitate this, we provide the open-source
implementations for our models and benchmarks in repository
https://github.com/blondered/transformer_benchmark

</details>


### [371] [Maximum Impact with Fewer Features: Efficient Feature Selection for Cold-Start Recommenders through Collaborative Importance Weighting](https://arxiv.org/abs/2508.06455)
*Nikita Sukhorukov,Danil Gusak,Evgeny Frolov*

Main category: cs.IR

TL;DR: The paper introduces a feature selection technique for recommender systems to tackle cold-start challenges, enhancing accuracy and efficiency by prioritizing critical user behavioral features.


<details>
  <summary>Details</summary>
Motivation: Cold-start issues in recommender systems demand effective utilization of auxiliary features, but irrelevant or excessive features degrade performance or increase computation.

Method: A hybrid matrix factorization technique incorporates correlations from collaborative behavior data, followed by a maximum volume algorithm-based ranking to select influential features.

Result: The method excels in cold-start scenarios by selecting minimal yet effective features, outperforming existing techniques even with strict feature reduction.

Conclusion: The proposed feature selection strategy improves recommendation accuracy and efficiency, effectively addressing cold-start challenges in recommender systems.

Abstract: Cold-start challenges in recommender systems necessitate leveraging auxiliary
features beyond user-item interactions. However, the presence of irrelevant or
noisy features can degrade predictive performance, whereas an excessive number
of features increases computational demands, leading to higher memory
consumption and prolonged training times.
  To address this, we propose a feature selection strategy that prioritizes the
user behavioral information. Our method enhances the feature representation by
incorporating correlations from collaborative behavior data using a hybrid
matrix factorization technique and then ranks features using a mechanism based
on the maximum volume algorithm. This approach identifies the most influential
features, striking a balance between recommendation accuracy and computational
efficiency. We conduct an extensive evaluation across various datasets and
hybrid recommendation models, demonstrating that our method excels in
cold-start scenarios by selecting minimal yet highly effective feature subsets.
Even under strict feature reduction, our approach surpasses existing feature
selection techniques while maintaining superior efficiency.

</details>


### [372] [Semantic Item Graph Enhancement for Multimodal Recommendation](https://arxiv.org/abs/2508.06154)
*Xiaoxiong Zhang,Xin Zhou,Zhiwei Zeng,Dusit Niyato,Zhiqi Shen*

Main category: cs.IR

TL;DR: The paper proposes a multimodal recommendation system addressing issues in semantic graphs by enhancing collaborative signals and reducing noise with a personalized embedding perturbation mechanism.


<details>
  <summary>Details</summary>
Motivation: Multimodal recommendation systems face semantic deficiencies, such as insufficient modeling of collaborations and noise-induced structural distortions, which hinder user preference learning.

Method: The authors extract collaborative signals from interaction graphs, apply a personalized embedding perturbation mechanism for noise-robust learning, and use a dual representation alignment mechanism to enhance representation consistency.

Result: Experiments on four benchmark datasets confirm that the proposed framework improves the performance and addresses semantic deficiencies.

Conclusion: The framework effectively tackles semantic deficiencies in multimodal recommendation systems, improving robustness and representation consistency through innovative mechanisms.

Abstract: Multimodal recommendation systems have attracted increasing attention for
their improved performance by leveraging items' multimodal information. Prior
methods often build modality-specific item-item semantic graphs from raw
modality features and use them as supplementary structures alongside the
user-item interaction graph to enhance user preference learning. However, these
semantic graphs suffer from semantic deficiencies, including (1) insufficient
modeling of collaborative signals among items and (2) structural distortions
introduced by noise in raw modality features, ultimately compromising
performance. To address these issues, we first extract collaborative signals
from the interaction graph and infuse them into each modality-specific item
semantic graph to enhance semantic modeling. Then, we design a modulus-based
personalized embedding perturbation mechanism that injects perturbations with
modulus-guided personalized intensity into embeddings to generate contrastive
views. This enables the model to learn noise-robust representations through
contrastive learning, thereby reducing the effect of structural noise in
semantic graphs. Besides, we propose a dual representation alignment mechanism
that first aligns multiple semantic representations via a designed Anchor-based
InfoNCE loss using behavior representations as anchors, and then aligns
behavior representations with the fused semantics by standard InfoNCE, to
ensure representation consistency. Extensive experiments on four benchmark
datasets validate the effectiveness of our framework.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [373] [Intuition emerges in Maximum Caliber models at criticality](https://arxiv.org/abs/2508.06477)
*Lluís Arola-Fernández*

Main category: physics.soc-ph

TL;DR: This study explores how large predictive models may exhibit intuition by implementing Maximum Caliber with a temperature-like control parameter $\lambda$. It identifies three phases of model behavior and links intuition to a critical balance between memorization and exploration.


<details>
  <summary>Details</summary>
Motivation: Investigate whether predictive models genuinely produce insights or simply mimic training data, explaining mechanisms underlying emergent behavior through physical principles.

Method: Introduce mind-tuning using a Maximum Caliber framework and a control parameter $\lambda$, training models on deterministic maze random walks, examining their behavior across varying $\lambda$ values.

Result: Discovery of a phase diagram with three states: imitation (low $\lambda$), hallucination (high $\lambda$), and a fragile intermediate phase displaying goal-directed strategies and hysteresis.

Conclusion: Intuition in predictive models emerges as a metastable phase between memorization and speculation, suggesting it is an emergent property rooted in critical learning balance.

Abstract: Whether large predictive models merely parrot their training data or produce
genuine insight lacks a physical explanation. This work reports a primitive
form of intuition that emerges as a metastable phase of learning that
critically balances next-token prediction against future path-entropy. The
intuition mechanism is discovered via mind-tuning, the minimal principle that
imposes Maximum Caliber in predictive models with a control temperature-like
parameter $\lambda$. Training on random walks in deterministic mazes reveals a
rich phase diagram: imitation (low $\lambda$), rule-breaking hallucination
(high $\lambda$), and a fragile in-between window exhibiting strong
protocol-dependence (hysteresis) and multistability, where models spontaneously
discover novel goal-directed strategies. These results are captured by an
effective low-dimensional theory and frame intuition as an emergent property at
the critical balance between memorizing what is and wondering what could be.

</details>
