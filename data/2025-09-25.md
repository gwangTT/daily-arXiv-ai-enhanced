<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 24]
- [cs.AR](#cs.AR) [Total: 4]
- [cs.CL](#cs.CL) [Total: 45]
- [cs.CV](#cs.CV) [Total: 44]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.LG](#cs.LG) [Total: 52]
- [cs.NE](#cs.NE) [Total: 4]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.PL](#cs.PL) [Total: 3]
- [cs.RO](#cs.RO) [Total: 44]
- [cs.SE](#cs.SE) [Total: 14]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [stat.ML](#stat.ML) [Total: 10]
- [cs.CR](#cs.CR) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [stat.ME](#stat.ME) [Total: 3]
- [cs.NI](#cs.NI) [Total: 2]
- [eess.SP](#eess.SP) [Total: 11]
- [quant-ph](#quant-ph) [Total: 1]
- [math.NA](#math.NA) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [The Indispensable Role of User Simulation in the Pursuit of AGI](https://arxiv.org/abs/2509.19456)
*Krisztian Balog,ChengXiang Zhai*

Main category: cs.AI

TL;DR: The paper highlights the importance of user simulation as a critical tool to evaluate AI systems, generate training data, and speed up AGI development.


<details>
  <summary>Details</summary>
Motivation: The study aims to address bottlenecks in evaluating interactive AI systems and generating interaction data essential for AGI progress.

Method: It advocates using realistic user simulation to enable scalable evaluation and adaptive learning environments.

Result: The paper identifies user simulation as essential for AGI, highlights interdisciplinary challenges, and proposes a targeted research agenda.

Conclusion: Advancing both user simulation technology and intelligent task agents is essential to surmount AGI development challenges and harness their synergistic benefits.

Abstract: Progress toward Artificial General Intelligence (AGI) faces significant
bottlenecks, particularly in rigorously evaluating complex interactive systems
and acquiring the vast interaction data needed for training adaptive agents.
This paper posits that user simulation -- creating computational agents that
mimic human interaction with AI systems -- is not merely a useful tool, but is
a critical catalyst required to overcome these bottlenecks and accelerate AGI
development. We argue that realistic simulators provide the necessary
environments for scalable evaluation, data generation for interactive learning,
and fostering the adaptive capabilities central to AGI. Therefore, research
into user simulation technology and intelligent task agents are deeply
synergistic and must advance hand-in-hand. This article elaborates on the
critical role of user simulation for AGI, explores the interdisciplinary nature
of building realistic simulators, identifies key challenges including those
posed by large language models, and proposes a future research agenda.

</details>


### [2] [Evaluation-Aware Reinforcement Learning](https://arxiv.org/abs/2509.19464)
*Shripad Vilasrao Deshmukh,Will Schwarzer,Scott Niekum*

Main category: cs.AI

TL;DR: The paper introduces Evaluation-Aware Reinforcement Learning (EvA-RL), a method that trains policies to be both high-performing and easy to evaluate, reducing evaluation errors with minimal rollouts.


<details>
  <summary>Details</summary>
Motivation: The main motivation is to address high variance and bias in policy evaluation due to factors like limited data, long-horizon tasks, and the lack of explicit consideration for evaluation within standard reinforcement learning approaches.

Method: The authors propose EvA-RL, which integrates evaluation-aware policy training by maximizing returns while minimizing evaluation errors. They extend this framework by co-learning an assessment-conditioned value predictor alongside the policy to mitigate tradeoffs.

Result: The study demonstrates that EvA-RL significantly reduces evaluation errors while maintaining competitive policy performance across various action domains, even with limited rollout data.

Conclusion: EvA-RL represents a novel approach that prioritizes reliable evaluation during training, setting the foundation for a new class of reinforcement learning methods that closely integrate evaluation and policy optimization.

Abstract: Policy evaluation is often a prerequisite for deploying safety- and
performance-critical systems. Existing evaluation approaches frequently suffer
from high variance due to limited data and long-horizon tasks, or high bias due
to unequal support or inaccurate environmental models. We posit that these
challenges arise, in part, from the standard reinforcement learning (RL)
paradigm of policy learning without explicit consideration of evaluation. As an
alternative, we propose evaluation-aware reinforcement learning (EvA-RL), in
which a policy is trained to maximize expected return while simultaneously
minimizing expected evaluation error under a given value prediction scheme --
in other words, being "easy" to evaluate. We formalize a framework for EvA-RL
and design an instantiation that enables accurate policy evaluation,
conditioned on a small number of rollouts in an assessment environment that can
be different than the deployment environment. However, our theoretical analysis
and empirical results show that there is often a tradeoff between evaluation
accuracy and policy performance when using a fixed value-prediction scheme
within EvA-RL. To mitigate this tradeoff, we extend our approach to co-learn an
assessment-conditioned state-value predictor alongside the policy. Empirical
results across diverse discrete and continuous action domains demonstrate that
EvA-RL can substantially reduce evaluation error while maintaining competitive
returns. This work lays the foundation for a broad new class of RL methods that
treat reliable evaluation as a first-class principle during training.

</details>


### [3] [Estimating the Self-Consistency of LLMs](https://arxiv.org/abs/2509.19489)
*Robert Nowak*

Main category: cs.AI

TL;DR: The paper discusses a method for analyzing the self-consistency of large language models (LLMs) by examining tradeoffs in repeating prompts and aggregating outputs under a fixed compute budget.


<details>
  <summary>Details</summary>
Motivation: To improve reliability in the responses of LLMs, systems repeatedly prompt them and aggregate the answers. This paper seeks to quantify the self-consistency of LLMs and address compute efficiency.

Method: The paper proposes an analysis framework based on an estimator of self-consistency and assesses the optimal allocation of compute resources across sampled prompts and repeated calls.

Result: The analysis concludes that the ideal budget split involves dividing the compute equally between the number of prompts and their repetitions, favoring a square root relationship between them.

Conclusion: The study presents a practical guideline for maximizing self-consistency in LLM outputs under fixed computational constraints.

Abstract: Systems often repeat the same prompt to large language models (LLMs) and
aggregate responses to improve reliability. This short note analyzes an
estimator of the self-consistency of LLMs and the tradeoffs it induces under a
fixed compute budget $B=mn$, where $m$ is the number of prompts sampled from
the task distribution and $n$ is the number of repeated LLM calls per prompt;
the resulting analysis favors a rough split $m,n\propto\sqrt{B}$.

</details>


### [4] [Cognitive Load Limits in Large Language Models: Benchmarking Multi-Hop Reasoning](https://arxiv.org/abs/2509.19517)
*Sai Teja Reddy Adapala*

Main category: cs.AI

TL;DR: The paper introduces a benchmark called Interleaved Cognitive Evaluation (ICE) to assess the performance of Large Language Models (LLMs) under cognitive load conditions, revealing significant weaknesses and showing that cognitive load contributes to reasoning failures.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) perform well in static benchmarks but struggle in dynamic, complex environments due to insufficient understanding of how cognitive load impacts their reasoning abilities.

Method: The authors developed the ICE benchmark to manipulate and measure cognitive load factors (Context Saturation and Attentional Residue) across challenging reasoning tasks, testing multiple models with 200 multi-hop reasoning questions and replication experiments.

Result: Smaller LLMs underperformed significantly, achieving 0% accuracy, while Gemini-2.0 demonstrated partial resilience with 85% accuracy in control conditions. Performance degraded under cognitive load effects, supporting theories of hallucination during uncertainty.

Conclusion: Dynamic stress testing that accounts for cognitive load is critical for evaluating and improving the resilience and safety of advanced AI models.

Abstract: The scaling of Large Language Models (LLMs) has exposed a critical gap
between their performance on static benchmarks and their fragility in dynamic,
information-rich environments. While models excel at isolated tasks, the
computational limits that govern their reasoning under cognitive load remain
poorly understood. In this work, we introduce a formal theory of computational
cognitive load, positing that extraneous, task-irrelevant information (Context
Saturation) and interference from task-switching (Attentional Residue) are key
mechanisms that degrade performance. We designed the Interleaved Cognitive
Evaluation (ICE), a deconfounded benchmark to systematically manipulate these
load factors on challenging multi-hop reasoning tasks. A comprehensive study (N
= 10 replications per item across 200 questions) revealed significant
performance variations across five instruction-tuned models. Smaller
open-source architectures (Llama-3-8B-Instruct, Mistral-7B-Instruct-v0.2)
exhibited baseline brittleness, achieving 0% accuracy (SEM = 0.0) across all
conditions, including clean controls, on this high-intrinsic-load task. In
contrast, Gemini-2.0-Flash-001 showed partial resilience, achieving 85%
accuracy in control conditions, with a statistically significant degradation
under context saturation ($\beta = -0.003$ per % load, $p < 0.001$). These
findings provide preliminary evidence that cognitive load is a key contributor
to reasoning failures, supporting theories of hallucination-as-guessing under
uncertainty. We conclude that dynamic, cognitive-aware stress testing, as
exemplified by the ICE benchmark, is essential for evaluating the true
resilience and safety of advanced AI systems.

</details>


### [5] [Score the Steps, Not Just the Goal: VLM-Based Subgoal Evaluation for Robotic Manipulation](https://arxiv.org/abs/2509.19524)
*Ramy ElMallah,Krish Chhajer,Chi-Guhn Lee*

Main category: cs.AI

TL;DR: Robot learning papers often obscure partial task competency when evaluating outcomes. This paper introduces StepEval, a framework emphasizing per-subgoal success rate (SR) evaluation using vision-language models.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the limitation of traditional robot learning evaluations, which focus solely on binary success rates and neglect detailed analysis of success across subgoals.

Method: StepEval uses vision-language models to automatically assess subgoal outcomes within manipulation tasks. It creates vectors of per-subgoal SRs from recorded images or videos, allowing granular evaluation.

Result: The framework is designed to combine efficiency, scalability, and model-agnostic principles for robot behavior evaluation, along with diagnostics aiding optimization.

Conclusion: StepEval promotes a shared direction for robot learning evaluation, emphasizing per-step scoring and inviting community contributions for more reproducible and detailed results.

Abstract: Robot learning papers typically report a single binary success rate (SR),
which obscures where a policy succeeds or fails along a multi-step manipulation
task. We argue that subgoal-level reporting should become routine: for each
trajectory, a vector of per-subgoal SRs that makes partial competence visible
(e.g., grasp vs. pour). We propose a blueprint for StepEval, a cost-aware
plug-in evaluation framework that utilizes vision-language models (VLMs) as
automated judges of subgoal outcomes from recorded images or videos. Rather
than proposing new benchmarks or APIs, our contribution is to outline design
principles for a scalable, community-driven open-source project. In StepEval,
the primary artifact for policy evaluation is the per-subgoal SR vector;
however, other quantities (e.g., latency or cost estimates) are also considered
for framework-optimization diagnostics to help the community tune evaluation
efficiency and accuracy when ground-truth subgoal success labels are available.
We discuss how such a framework can remain model-agnostic, support single- or
multi-view inputs, and be lightweight enough to adopt across labs. The intended
contribution is a shared direction: a minimal, extensible seed that invites
open-source contributions, so that scoring the steps, not just the final goal,
becomes a standard and reproducible practice.

</details>


### [6] [Nano Bio-Agents (NBA): Small Language Model Agents for Genomics](https://arxiv.org/abs/2509.19566)
*George Hong,Daniel Trejo Banos*

Main category: cs.AI

TL;DR: The study explores using Small Language Models (SLMs) with less than 10 billion parameters for genomics question answering, leveraging an agentic framework for efficiency and reduced hallucinations. Their framework achieves robust performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges related to hallucination issues and high computational costs in genomics question answering by utilizing smaller language models combined with an agentic framework.

Method: Implementation of the Nano Bio-Agent (NBA) framework integrating task decomposition, tool orchestration, and API access with established systems like NCBI and AlphaGenome.

Result: The combination of SLMs and the NBA framework achieved up to 98% accuracy on the GeneTuring benchmark, performing comparably or better than larger models while significantly reducing computational cost.

Conclusion: SLMs with an agentic framework demonstrate strong potential to offer cost-effective and efficient solutions in genomics question answering without compromising accuracy, paving the way for democratization of ML tools in genomics.

Abstract: We investigate the application of Small Language Models (<10 billion
parameters) for genomics question answering via agentic framework to address
hallucination issues and computational cost challenges. The Nano Bio-Agent
(NBA) framework we implemented incorporates task decomposition, tool
orchestration, and API access into well-established systems such as NCBI and
AlphaGenome. Results show that SLMs combined with such agentic framework can
achieve comparable and in many cases superior performance versus existing
approaches utilising larger models, with our best model-agent combination
achieving 98% accuracy on the GeneTuring benchmark. Notably, small 3-10B
parameter models consistently achieve 85-97% accuracy while requiring much
lower computational resources than conventional approaches. This demonstrates
promising potential for efficiency gains, cost savings, and democratization of
ML-powered genomics tools while retaining highly robust and accurate
performance.

</details>


### [7] [Design Insights and Comparative Evaluation of a Hardware-Based Cooperative Perception Architecture for Lane Change Prediction](https://arxiv.org/abs/2509.20218)
*Mohamed Manzour,Catherine M. Elias,Omar M. Shehata,Rubén Izquierdo,Miguel Ángel Sotelo*

Main category: cs.AI

TL;DR: This study investigates cooperative lane-change prediction through real-world hardware deployment in mixed traffic, highlighting practical challenges and offering guidance.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the gaps in real-world lane change prediction implementations, which often lack detailed documentation of practical challenges.

Method: The researchers deployed a cooperative lane-change prediction system in mixed traffic using real hardware and conducted testing while documenting operational issues.

Result: The research uncovered bottlenecks, reliability issues, and operational constraints that influence system performance.

Conclusion: The paper provides valuable insights for future researchers in developing and improving lane-change prediction systems in practical environments.

Abstract: Research on lane change prediction has gained attention in the last few
years. Most existing works in this area have been conducted in simulation
environments or with pre-recorded datasets, these works often rely on
simplified assumptions about sensing, communication, and traffic behavior that
do not always hold in practice. Real-world deployments of lane-change
prediction systems are relatively rare, and when they are reported, the
practical challenges, limitations, and lessons learned are often
under-documented. This study explores cooperative lane-change prediction
through a real hardware deployment in mixed traffic and shares the insights
that emerged during implementation and testing. We highlight the practical
challenges we faced, including bottlenecks, reliability issues, and operational
constraints that shaped the behavior of the system. By documenting these
experiences, the study provides guidance for others working on similar
pipelines.

</details>


### [8] [What Does Your Benchmark Really Measure? A Framework for Robust Inference of AI Capabilities](https://arxiv.org/abs/2509.19590)
*Nathanael Jo,Ashia Wilson*

Main category: cs.AI

TL;DR: The paper critiques current AI benchmark evaluations and introduces a new framework for more reliable assessments of generative models' capabilities, highlighting the importance of robust inference methods.


<details>
  <summary>Details</summary>
Motivation: Growing skepticism about the reliability of benchmark evaluations in AI motivates the need for a principled approach to measure true model performance.

Method: The authors propose an evaluation-as-inference framework inspired by psychometrics, addressing sensitivity to perturbations and incorporating uncertainty into the estimation of capabilities.

Result: The paper presents a model of ability and adaptive methods that reduce sample complexity and enhance reliability in evaluating AI capabilities.

Conclusion: These contributions aim to establish more trustworthy and rigorous approaches for assessing AI models using benchmark tests.

Abstract: Evaluations of generative models on benchmark data are now ubiquitous, and
their outcomes critically shape public and scientific expectations of AI's
capabilities. Yet growing skepticism surrounds their reliability. How can we
know that a reported accuracy genuinely reflects a model's true performance?
Evaluations are often presented as simple measurements, but in reality they are
inferences: to treat benchmark scores as evidence of capability is already to
assume a theory of what capability is and how it manifests in a test. We make
this step explicit by proposing a principled framework for evaluation as
inference: begin from a theory of capability, and then derive methods for
estimating it. This perspective, familiar in fields such as psychometrics, has
not yet become commonplace in AI evaluation. As a proof of concept, we address
a central challenge that undermines reliability: sensitivity to perturbations.
After formulating a model of ability, we introduce methods that infer ability
while accounting for uncertainty from sensitivity and finite samples, including
an adaptive algorithm that significantly reduces sample complexity. Together,
these contributions lay the groundwork for more reliable and trustworthy
estimates of AI capabilities as measured through benchmarks.

</details>


### [9] [SteinerSQL: Graph-Guided Mathematical Reasoning for Text-to-SQL Generation](https://arxiv.org/abs/2509.19623)
*Xutao Mao,Tao Liu,Hongying Zan*

Main category: cs.AI

TL;DR: The paper introduces SteinerSQL, a framework for improving complex Text-to-SQL queries, establishing new accuracy benchmarks.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with handling complex Text-to-SQL queries requiring mathematical reasoning and schema navigation. Fragmented approaches fail to ensure logical or structural correctness, necessitating a unified framework.

Method: SteinerSQL applies a graph-centric optimization problem in three stages: 1) Decomposes mathematical requirements to find necessary tables, 2) Constructs an optimal reasoning scaffold via a Steiner tree, 3) Validates correctness through multi-level checks.

Result: Achieves state-of-the-art execution accuracy of 36.10% on the LogicCat benchmark and 40.04% on Spider2.0-Lite benchmark using the Gemini-2.5-Pro model.

Conclusion: SteinerSQL not only demonstrates improved performance on complex Text-to-SQL queries but also proposes a unified paradigm that has the potential for broader application in advanced reasoning tasks.

Abstract: Large Language Models (LLMs) struggle with complex Text-to-SQL queries that
demand both sophisticated mathematical reasoning and intricate schema
navigation. Existing methods often tackle these challenges in isolation,
creating a fractured reasoning process that compromises logical and structural
correctness. To resolve this, we introduce SteinerSQL, a framework that unifies
these dual challenges into a single, graph-centric optimization problem.
SteinerSQL operates in three stages: mathematical decomposition to identify
required tables (terminals), optimal reasoning scaffold construction via a
Steiner tree problem, and multi-level validation to ensure correctness. On the
challenging LogicCat and Spider2.0-Lite benchmarks, SteinerSQL establishes a
new state-of-the-art with 36.10% and 40.04% execution accuracy, respectively,
using Gemini-2.5-Pro. Beyond accuracy, SteinerSQL presents a new, unified
paradigm for Text-to-SQL, paving the way for more robust and principled
solutions to complex reasoning tasks.

</details>


### [10] [Calibrated Reasoning: An Explanatory Verifier for Dynamic and Efficient Problem-Solving](https://arxiv.org/abs/2509.19681)
*Anisha Garg,Engin Tekin,Yash More,David Bick,Nishit Neema,Ganesh Venkatesh*

Main category: cs.AI

TL;DR: The paper introduces a pairwise Explanatory Verifier that improves reasoning model performance using calibrated confidence scores and natural language explanations, outperforming current methods in challenging scenarios.


<details>
  <summary>Details</summary>
Motivation: Reasoning models struggle with poor self-evaluation, limiting the effectiveness of advanced test-time strategies.

Method: They develop a pairwise Explanatory Verifier trained via reinforcement learning (GRPO) to generate confidence scores and reasoning for solutions.

Result: The verifier enhances accuracy and efficiency of test-time strategies while identifying challenging failure cases that standard approaches miss.

Conclusion: The proposed verifier is effective in improving reasoning models, particularly in detecting failure modes where traditional methods fall short.

Abstract: Advanced test-time computing strategies are essential for scaling reasoning
models, but their effectiveness is capped by the models' poor self-evaluation.
We propose a pairwise Explanatory Verifier, trained via reinforcement learning
(GRPO), that produces calibrated confidence scores and associated natural
language reasoning for generated solutions. Our verifier improves the accuracy
and efficiency of test-time strategies like best-of-n and self-reflection.
Crucially, it excels at identifying challenging failure modes, such as when
both candidate solutions are identically incorrect, succeeding where standard
methods like majority voting fail.

</details>


### [11] [UserRL: Training Interactive User-Centric Agent via Reinforcement Learning](https://arxiv.org/abs/2509.19736)
*Cheng Qian,Zuxin Liu,Akshara Prabhakar,Jielin Qiu,Zhiwei Liu,Haolin Chen,Shirley Kokane,Heng Ji,Weiran Yao,Shelby Heinecke,Silvio Savarese,Caiming Xiong,Huan Wang*

Main category: cs.AI

TL;DR: Reinforcement learning is used to train agentic models for dynamic user interactions. UserRL framework evaluates user-centric abilities using standardized environments and simulated users.


<details>
  <summary>Details</summary>
Motivation: To enhance the practicality of reinforcement learning by developing agents capable of dynamic, multi-turn interactions with diverse users.

Method: A unified framework, UserRL, with gym environments and simulated users is used. Different reward assignments and scoring mechanisms are studied under the GRPO algorithm.

Result: Experiments reveal SFT cold start importance, trajectory scoring benefits, and cost-effective simulated user options like Qwen3-32B over GPT-4o.

Conclusion: Reward shaping and simulated user choices are pivotal, alongside model scale, for training robust user-centric agentic models. UserRL is a valuable pathway for such developments.

Abstract: Reinforcement learning (RL) has shown promise in training agentic models that
move beyond static benchmarks to engage in dynamic, multi-turn interactions.
Yet, the ultimate value of such agents lies in their ability to assist users, a
setting where diversity and dynamics of user interaction pose challenges. In
this work, we propose UserRL, a unified framework for training and evaluating
user-centric abilities through standardized gym environments paired with
simulated users. We systematically vary turn-level reward assignment and
trajectory-level score calculation to analyze how different formulations affect
learning under the GRPO algorithm. Our experiments across Qwen3 models reveal
three key findings: (i) SFT cold start is critical for unlocking initial
interaction ability and enabling sustained RL improvements; (ii) deliberate
trajectory scoring yields more efficient and effective multi-turn interactions;
and (iii) while stronger simulated users (e.g., GPT-4o) facilitates training,
open-source simulators (e.g., Qwen3-32B) remain a cost-effective and
transferable option. Together, these results highlight that careful design of
reward shaping and user simulation choice is as crucial as model scale, and
establish UserRL as a practical pathway for developing robust user-centric
agentic models. All codes and data are public for future research.

</details>


### [12] [The Conductor and the Engine: A Path Towards Co-Designed Reasoning](https://arxiv.org/abs/2509.19762)
*Yuanxin Wang,Pawel Filipczuk,Anisha Garg,Amaan Dhada,Mohammad Hassanpour,David Bick,Ganesh Venkatesh*

Main category: cs.AI

TL;DR: The study introduces CEPO, an optimized reasoning workflow allowing smaller open-source language models to rival or surpass larger models in reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of modern LLM reasoning frameworks due to model verbosity and poor instruction adherence, and improve performance without excessive computation.

Method: The paper analyzes the trade-off between capability and cost in language model reasoning and develops CEPO, a co-designed orchestration framework tailored for smaller models.

Result: Smaller open-source models equipped with CEPO outperform larger models multiple times their size in reasoning tasks.

Conclusion: Optimizing workflows in tandem with model capabilities unlocks powerful reasoning in small-to-medium-sized models, paving the way for efficient AI development.

Abstract: Modern LLM reasoning relies on extensive test-time computation, driven by
internal model training and external agentic orchestration. However, this
synergy is often inefficient, as model verbosity and poor instruction following
lead to wasted compute. We analyze this capability-cost trade-off and introduce
an optimized reasoning workflow (\cepo) that empowers smaller open-source
models to outperform models multiple times their size. We will open-source this
workflow to enable further research. Our work demonstrates a clear path toward
co-designing orchestration frameworks with the underlying model capabilities to
unlock powerful reasoning in small-to-medium sized models.

</details>


### [13] [Agentic Metacognition: Designing a "Self-Aware" Low-Code Agent for Failure Prediction and Human Handoff](https://arxiv.org/abs/2509.19783)
*Jiexi Xu*

Main category: cs.AI

TL;DR: This paper proposes a metacognitive layer for autonomous agents in LCNC environments to enhance reliability, addressing issues such as task failures and unrecoverable errors.


<details>
  <summary>Details</summary>
Motivation: Address the reliability challenges posed by the non-deterministic nature of autonomous agents, which can lead to user frustration and trust issues.

Method: Introduce a metacognitive layer that monitors the primary LCNC agent, predicts failures based on predefined triggers, and initiates a human handoff with a detailed explanation.

Result: Prototype analysis shows increased task success rates but with higher computational overhead.

Conclusion: Human handoffs, supported by metacognitive transparency, enhance system resilience and trust while offering a blueprint for ethical and practical advancements in autonomous agents.

Abstract: The inherent non-deterministic nature of autonomous agents, particularly
within low-code/no-code (LCNC) environments, presents significant reliability
challenges. Agents can become trapped in unforeseen loops, generate inaccurate
outputs, or encounter unrecoverable failures, leading to user frustration and a
breakdown of trust. This report proposes a novel architectural pattern to
address these issues: the integration of a secondary, "metacognitive" layer
that actively monitors the primary LCNC agent. Inspired by human introspection,
this layer is designed to predict impending task failures based on a defined
set of triggers, such as excessive latency or repetitive actions. Upon
predicting a failure, the metacognitive agent proactively initiates a human
handoff, providing the user with a clear summary of the agent's "thought
process" and a detailed explanation of why it could not proceed. An empirical
analysis of a prototype system demonstrates that this approach significantly
increases the overall task success rate. However, this performance gain comes
with a notable increase in computational overhead. The findings reframe human
handoffs not as an admission of defeat but as a core design feature that
enhances system resilience, improves user experience, and builds trust by
providing transparency into the agent's internal state. The report discusses
the practical and ethical implications of this approach and identifies key
directions for future research.

</details>


### [14] [Analysis of approximate linear programming solution to Markov decision problem with log barrier function](https://arxiv.org/abs/2509.19800)
*Donghwan Lee,Hyukjun Yang,Bum Geun Park*

Main category: cs.AI

TL;DR: This paper proposes using the log-barrier function to transform the LP formulation of MDPs into unconstrained optimization problems, enabling easier solution via gradient descent.


<details>
  <summary>Details</summary>
Motivation: LP-based methods for solving MDPs are relatively underutilized due to challenges associated with inequality-constrained optimization problems.

Method: The paper leverages the log-barrier function to reformulate the LP-based MDP optimization into an unconstrained problem, facilitating gradient descent solutions.

Result: The reformulation using the log-barrier function provides a practical approach to approximate solutions, while offering a theoretical foundation for LP-based MDPs.

Conclusion: The proposed method simplifies LP-based MDP resolution, bridging theoretical gaps and enabling practical application through gradient-based techniques.

Abstract: There are two primary approaches to solving Markov decision problems (MDPs):
dynamic programming based on the Bellman equation and linear programming (LP).
Dynamic programming methods are the most widely used and form the foundation of
both classical and modern reinforcement learning (RL). By contrast, LP-based
methods have been less commonly employed, although they have recently gained
attention in contexts such as offline RL. The relative underuse of the LP-based
methods stems from the fact that it leads to an inequality-constrained
optimization problem, which is generally more challenging to solve effectively
compared with Bellman-equation-based methods. The purpose of this paper is to
establish a theoretical foundation for solving LP-based MDPs in a more
effective and practical manner. Our key idea is to leverage the log-barrier
function, widely used in inequality-constrained optimization, to transform the
LP formulation of the MDP into an unconstrained optimization problem. This
reformulation enables approximate solutions to be obtained easily via gradient
descent. While the method may appear simple, to the best of our knowledge, a
thorough theoretical interpretation of this approach has not yet been
developed. This paper aims to bridge this gap.

</details>


### [15] [LatentGuard: Controllable Latent Steering for Robust Refusal of Attacks and Reliable Response Generation](https://arxiv.org/abs/2509.19839)
*Huizhen Shu,Xuying Li,Zhuo Li*

Main category: cs.AI

TL;DR: LATENTGUARD is a three-stage framework designed to improve the safety alignment of LLMs while maintaining their utility. It uses reasoning-enhanced datasets, a structured VAE for latent space manipulation, and targeted behavioral control to block harmful queries and enable safe, practical usage.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of ensuring LLMs are robustly aligned for safety without losing their utility in legitimate scenarios, a balance that existing methods struggle to achieve.

Method: LATENTGUARD fine-tunes LLMs on specialized datasets for safety-critical and benign scenarios, trains a structured VAE on intermediate activations to develop disentangled and interpretable latent spaces, and manipulates these spaces to selectively refuse harmful inputs while allowing legitimate ones.

Result: Experiments on Qwen3-8B revealed improvements in safety, controllability, and response interpretability, with cross-architecture validation on Mistral-7B confirming the approach's generalizability across model families.

Conclusion: Structured intervention at the latent representation level is a viable and promising strategy for creating LLM systems that are both safer and practical for real-world use cases.

Abstract: Achieving robust safety alignment in large language models (LLMs) while
preserving their utility remains a fundamental challenge. Existing approaches
often struggle to balance comprehensive safety with fine-grained
controllability at the representation level. We introduce LATENTGUARD, a novel
three-stage framework that combines behavioral alignment with supervised latent
space control for interpretable and precise safety steering. Our approach
begins by fine-tuning an LLM on rationalized datasets containing both
reasoning-enhanced refusal responses to adversarial prompts and
reasoning-enhanced normal responses to benign queries, establishing robust
behavioral priors across both safety-critical and utility-preserving scenarios.
We then train a structured variational autoencoder (VAE) on intermediate MLP
activations, supervised by multi-label annotations including attack types,
attack methods, and benign indicators. This supervision enables the VAE to
learn disentangled latent representations that capture distinct adversarial
characteristics while maintaining semantic interpretability. Through targeted
manipulation of learned latent dimensions, LATENTGUARD achieves selective
refusal behavior, effectively blocking harmful requests while preserving
helpfulness for legitimate use cases. Experiments on Qwen3-8B demonstrate
significant improvements in both safety controllability and response
interpretability without compromising utility. Cross-architecture validation on
Mistral-7B confirms the generalizability of our latent steering approach,
showing consistent effectiveness across different model families. Our results
suggest that structured representation-level intervention offers a promising
pathway toward building safer yet practical LLM systems.

</details>


### [16] [CON-QA: Privacy-Preserving QA using cloud LLMs in Contract Domain](https://arxiv.org/abs/2509.19925)
*Ajeet Kumar Singh,Rajsabi Surya,Anurag Tripathi,Santanu Choudhury,Sudhir Bisane*

Main category: cs.AI

TL;DR: This paper proposes a privacy-preserving framework, CON-QA, for secure question answering over enterprise contracts by combining local and cloud language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of protecting sensitive information, including PII and sensitive contractual clauses, as enterprises increasingly use cloud-based LLMs in legal workflows.

Method: The CON-QA framework operates in three stages: (i) semantic query decomposition and document chunk retrieval using a local LLM, (ii) anonymization through a one-to-many mapping to ensure privacy, and (iii) anonymized response generation by a cloud LLM, with local reconstruction via reverse mapping.

Result: CON-QA was evaluated using the CUAD-QA dataset with 85k question-answer pairs from 510 legal documents. Results show that it ensures privacy, maintains utility and answer quality, and mitigates privacy risks, validated by human assessments.

Conclusion: The study demonstrates that CON-QA is a practical, secure solution for performing privacy-preserving question answering on enterprise-level contract documents while ensuring high-fidelity responses.

Abstract: As enterprises increasingly integrate cloud-based large language models
(LLMs) such as ChatGPT and Gemini into their legal document workflows,
protecting sensitive contractual information - including Personally
Identifiable Information (PII) and commercially sensitive clauses - has emerged
as a critical challenge. In this work, we propose CON-QA, a hybrid
privacy-preserving framework designed specifically for secure question
answering over enterprise contracts, effectively combining local and
cloud-hosted LLMs. The CON-QA framework operates through three stages: (i)
semantic query decomposition and query-aware document chunk retrieval using a
locally deployed LLM analysis, (ii) anonymization of detected sensitive
entities via a structured one-to-many mapping scheme, ensuring semantic
coherence while preventing cross-session entity inference attacks, and (iii)
anonymized response generation by a cloud-based LLM, with accurate
reconstruction of the original answer locally using a session-consistent
many-to-one reverse mapping. To rigorously evaluate CON-QA, we introduce
CUAD-QA, a corpus of 85k question-answer pairs generated over 510 real-world
CUAD contract documents, encompassing simple, complex, and summarization-style
queries. Empirical evaluations, complemented by detailed human assessments,
confirm that CON-QA effectively maintains both privacy and utility, preserves
answer quality, maintains fidelity to legal clause semantics, and significantly
mitigates privacy risks, demonstrating its practical suitability for secure,
enterprise-level contract documents.

</details>


### [17] [Embodied AI: From LLMs to World Models](https://arxiv.org/abs/2509.20021)
*Tongtong Feng,Xin Wang,Yu-Gang Jiang,Wenwu Zhu*

Main category: cs.AI

TL;DR: This paper reviews and explores advancements in Embodied AI, emphasizing the roles of Large Language Models (LLMs) and World Models (WMs) in enhancing embodied cognition and interactions, while proposing a combined architecture (MLLM-WM) for complex physical tasks.


<details>
  <summary>Details</summary>
Motivation: To achieve Artificial General Intelligence (AGI) and make embodied AI applicable in real-world scenarios by leveraging recent breakthroughs in LLMs and WMs.

Method: Comprehensive literature review, analysis of key components, technologies, and fields (LLMs/multimodal LLMs and WMs), and proposing a new combined architecture.

Result: Insights into the roles of LLMs and WMs in embodied AI, demonstrated through broad applications, and a proposed joint MLLM-WM architecture for complex physical-world tasks.

Conclusion: The integration of LLMs and WMs is critical for advancing embodied AI, with promising applications and future research directions highlighted.

Abstract: Embodied Artificial Intelligence (AI) is an intelligent system paradigm for
achieving Artificial General Intelligence (AGI), serving as the cornerstone for
various applications and driving the evolution from cyberspace to physical
systems. Recent breakthroughs in Large Language Models (LLMs) and World Models
(WMs) have drawn significant attention for embodied AI. On the one hand, LLMs
empower embodied AI via semantic reasoning and task decomposition, bringing
high-level natural language instructions and low-level natural language actions
into embodied cognition. On the other hand, WMs empower embodied AI by building
internal representations and future predictions of the external world,
facilitating physical law-compliant embodied interactions. As such, this paper
comprehensively explores the literature in embodied AI from basics to advances,
covering both LLM driven and WM driven works. In particular, we first present
the history, key technologies, key components, and hardware systems of embodied
AI, as well as discuss its development via looking from unimodal to multimodal
angle. We then scrutinize the two burgeoning fields of embodied AI, i.e.,
embodied AI with LLMs/multimodal LLMs (MLLMs) and embodied AI with WMs,
meticulously delineating their indispensable roles in end-to-end embodied
cognition and physical laws-driven embodied interactions. Building upon the
above advances, we further share our insights on the necessity of the joint
MLLM-WM driven embodied AI architecture, shedding light on its profound
significance in enabling complex tasks within physical worlds. In addition, we
examine representative applications of embodied AI, demonstrating its wide
applicability in real-world scenarios. Last but not least, we point out future
research directions of embodied AI that deserve further investigation.

</details>


### [18] [MACD: Multi-Agent Clinical Diagnosis with Self-Learned Knowledge for LLM](https://arxiv.org/abs/2509.20067)
*Wenliang Li,Rui Yan,Xu Zhang,Li Chen,Hongji Zhu,Jing Zhao,Junjun Li,Mengru Li,Wei Cao,Zihang Jiang,Wei Wei,Kun Zhang,Shaohua Kevin Zhou*

Main category: cs.AI

TL;DR: The paper introduces a Multi-Agent Clinical Diagnosis (MACD) framework that enhances the accuracy of LLMs in diagnosing diseases by mimicking how physicians gain expertise, surpassing clinical guidelines and even human physicians in some cases.


<details>
  <summary>Details</summary>
Motivation: Current LLMs struggle with diagnosing complex clinical cases using traditional prompting methods, and existing solutions don't accumulate reusable clinical experience, necessitating a new approach.

Method: The MACD framework leverages a multi-agent pipeline where LLMs summarize, refine, and apply clinical insights. It includes a collaborative workflow involving multiple LLM agents, an evaluator, and human oversight for unresolved cases.

Result: The MACD framework improves primary diagnostic accuracy by up to 22.3% compared to established guidelines and achieves up to 18.6% accuracy improvement in collaboration with physicians. It outperforms or matches human physicians in specific datasets.

Conclusion: The study introduces a scalable, self-learning paradigm for LLM-assisted diagnosis, improving accuracy, transferability, and explainability while addressing the gap between LLM knowledge and real-world clinical needs.

Abstract: Large language models (LLMs) have demonstrated notable potential in medical
applications, yet they face substantial challenges in handling complex
real-world clinical diagnoses using conventional prompting methods. Current
prompt engineering and multi-agent approaches typically optimize isolated
inferences, neglecting the accumulation of reusable clinical experience. To
address this, this study proposes a novel Multi-Agent Clinical Diagnosis (MACD)
framework, which allows LLMs to self-learn clinical knowledge via a multi-agent
pipeline that summarizes, refines, and applies diagnostic insights. It mirrors
how physicians develop expertise through experience, enabling more focused and
accurate diagnosis on key disease-specific cues. We further extend it to a
MACD-human collaborative workflow, where multiple LLM-based diagnostician
agents engage in iterative consultations, supported by an evaluator agent and
human oversight for cases where agreement is not reached. Evaluated on 4,390
real-world patient cases across seven diseases using diverse open-source LLMs
(Llama-3.1 8B/70B, DeepSeek-R1-Distill-Llama 70B), MACD significantly improves
primary diagnostic accuracy, outperforming established clinical guidelines with
gains up to 22.3% (MACD). On the subset of the data, it achieves performance on
par with or exceeding that of human physicians (up to 16% improvement over
physicians-only diagnosis). Additionally, on the MACD-human workflow, it
achieves an 18.6% improvement compared to physicians-only diagnosis. Moreover,
self-learned knowledge exhibits strong cross-model stability, transferability,
and model-specific personalization, while the system can generate traceable
rationales, enhancing explainability. Consequently, this work presents a
scalable self-learning paradigm for LLM-assisted diagnosis, bridging the gap
between the intrinsic knowledge of LLMs and real-world clinical practice.

</details>


### [19] [From Pheromones to Policies: Reinforcement Learning for Engineered Biological Swarms](https://arxiv.org/abs/2509.20095)
*Aymeric Vellinger,Nemanja Antonic,Elio Tuci*

Main category: cs.AI

TL;DR: The paper establishes a theoretical connection between pheromone-mediated aggregation in nematodes and reinforcement learning, using computational experiments to demonstrate adaptability improvements through agent heterogeneity.


<details>
  <summary>Details</summary>
Motivation: To understand and enhance the decision-making processes in swarm systems by drawing parallels between biological stigmergic mechanisms and RL algorithms.

Method: Mathematical modeling of nematode swarms, experimental validation using literature data, and simulations in multi-armed bandit scenarios with artificial agents.

Result: The study shows accurate modeling of nematode foraging behaviors, highlights challenges with adaptability in dynamic environments due to pheromone trails, and reveals how exploratory agents improve swarm plasticity.

Conclusion: Stigmergic mechanisms emulate distributed RL processes, enabling the development of programmable living systems that exhibit resilient decision-making in volatile contexts.

Abstract: Swarm intelligence emerges from decentralised interactions among simple
agents, enabling collective problem-solving. This study establishes a
theoretical equivalence between pheromone-mediated aggregation in \celeg\ and
reinforcement learning (RL), demonstrating how stigmergic signals function as
distributed reward mechanisms. We model engineered nematode swarms performing
foraging tasks, showing that pheromone dynamics mathematically mirror
cross-learning updates, a fundamental RL algorithm. Experimental validation
with data from literature confirms that our model accurately replicates
empirical \celeg\ foraging patterns under static conditions. In dynamic
environments, persistent pheromone trails create positive feedback loops that
hinder adaptation by locking swarms into obsolete choices. Through
computational experiments in multi-armed bandit scenarios, we reveal that
introducing a minority of exploratory agents insensitive to pheromones restores
collective plasticity, enabling rapid task switching. This behavioural
heterogeneity balances exploration-exploitation trade-offs, implementing
swarm-level extinction of outdated strategies. Our results demonstrate that
stigmergic systems inherently encode distributed RL processes, where
environmental signals act as external memory for collective credit assignment.
By bridging synthetic biology with swarm robotics, this work advances
programmable living systems capable of resilient decision-making in volatile
environments.

</details>


### [20] [Steerable Adversarial Scenario Generation through Test-Time Preference Alignment](https://arxiv.org/abs/2509.20102)
*Tong Nie,Yuewen Mei,Yihong Tang,Junlin He,Jie Sun,Haotian Shi,Wei Ma,Jian Sun*

Main category: cs.AI

TL;DR: SAGE introduces a steerable framework for generating adversarial scenarios with the ability to balance adversariality and realism during inference without retraining.


<details>
  <summary>Details</summary>
Motivation: Existing adversarial scenario generation methods lack flexibility and are constrained by fixed trade-offs, making them inefficient for diverse training and testing needs in autonomous driving systems.

Method: The paper proposes the SAGE framework, which uses hierarchical group-based preference optimization to balance competing objectives. It fine-tunes two expert models on opposing preferences and interpolates between their weights to steer the scenario generation dynamically at inference.

Result: SAGE achieves superior adversariality-realism balance and enhances the closed-loop training of autonomous driving policies, verified through extensive experiments.

Conclusion: The framework adds significant flexibility and efficiency to adversarial scenario generation by enabling fine-grained control without retraining, offering theoretical grounding and practical benefits for driving policy evaluation and training.

Abstract: Adversarial scenario generation is a cost-effective approach for safety
assessment of autonomous driving systems. However, existing methods are often
constrained to a single, fixed trade-off between competing objectives such as
adversariality and realism. This yields behavior-specific models that cannot be
steered at inference time, lacking the efficiency and flexibility to generate
tailored scenarios for diverse training and testing requirements. In view of
this, we reframe the task of adversarial scenario generation as a
multi-objective preference alignment problem and introduce a new framework
named \textbf{S}teerable \textbf{A}dversarial scenario \textbf{GE}nerator
(SAGE). SAGE enables fine-grained test-time control over the trade-off between
adversariality and realism without any retraining. We first propose
hierarchical group-based preference optimization, a data-efficient offline
alignment method that learns to balance competing objectives by decoupling hard
feasibility constraints from soft preferences. Instead of training a fixed
model, SAGE fine-tunes two experts on opposing preferences and constructs a
continuous spectrum of policies at inference time by linearly interpolating
their weights. We provide theoretical justification for this framework through
the lens of linear mode connectivity. Extensive experiments demonstrate that
SAGE not only generates scenarios with a superior balance of adversariality and
realism but also enables more effective closed-loop training of driving
policies. Project page: https://tongnie.github.io/SAGE/.

</details>


### [21] [PEPS: Quantum-Inspired Reinforcement Learning for Coherent Reasoning Traces in LLMs](https://arxiv.org/abs/2509.20105)
*Venkat Margapuri,Garik Kazanjian,Naren Kosaraju*

Main category: cs.AI

TL;DR: This paper introduces a quantum-inspired approach using fidelity-based rewards to improve multi-step reasoning coherence in Large Language Models (LLMs).


<details>
  <summary>Details</summary>
Motivation: LLMs often fail to generate coherent multi-step reasoning, especially in tasks requiring structured logical flow, necessitating innovative solutions.

Method: The paper proposes incorporating fidelity-based rewards from Projected Entangled Pair States (PEPS) into Proximal Policy Optimization to enforce global coherence in reasoning traces.

Result: The approach demonstrated significant improvements in reasoning trace coherence across various datasets compared to supervised, contrastive, and pretrained baseline methods.

Conclusion: Quantum-inspired fidelity frameworks are effective for enhancing reasoning trace coherence in LLMs, marking promising progress in structured logical task performance.

Abstract: Large Language Models (LLMs) often struggle with maintaining coherent
multi-step reasoning traces, particularly in tasks that require a structured
logical flow. This work introduces a quantum-inspired approach to address the
challenge by incorporating a fidelity-based reward derived from Projected
Entangled Pair States (PEPS) into Proximal Policy Optimization. Unlike prior
approaches that use direct supervision or contrastive objectives, the proposed
method guides learning through structural consistency, offering a novel
approach to enforce global coherence in generated reasoning traces. The
proposed framework is evaluated using multiple coherence-determining metrics on
diverse datasets such as GSM8K, StrategyQA, and EntailmentBank spanning
arithmetic, intuitive, and entailment-based reasoning. Results show that the
proposed quantum-inspired approach offers significant improvements over
supervised, contrastive, and pretrained baseline approaches, highlighting the
effectiveness of quantum-inspired fidelity as a foundation to improve reasoning
trace coherence in LLMs.

</details>


### [22] [Formal Verification of Minimax Algorithms](https://arxiv.org/abs/2509.20138)
*Wieger Wesselink,Kees Huizing,Huub van de Wetering*

Main category: cs.AI

TL;DR: The paper utilizes the Dafny verification system to formally verify minimax search algorithms, incorporating alpha-beta pruning and transposition tables.


<details>
  <summary>Details</summary>
Motivation: The research aims to improve the reliability of minimax search algorithms by formalizing their verification and ensuring correctness through a well-defined theoretical framework.

Method: The authors use the Dafny verification system to rigorously verify a variety of minimax search algorithms. They introduce a witness-based correctness criterion for depth-limited search with transposition tables.

Result: The verification was successfully applied to a variety of minimax search algorithms, with all artifacts such as proofs and implementations being made publicly available.

Conclusion: This work advances the reliability of minimax search algorithm implementations by providing a formal verification process and making the results publicly accessible.

Abstract: Using the Dafny verification system, we formally verify a range of minimax
search algorithms, including variations with alpha-beta pruning and
transposition tables. For depth-limited search with transposition tables, we
introduce a witness-based correctness criterion and apply it to two
representative algorithms. All verification artifacts, including proofs and
Python implementations, are publicly available.

</details>


### [23] [Federation of Agents: A Semantics-Aware Communication Fabric for Large-Scale Agentic AI](https://arxiv.org/abs/2509.20175)
*Lorenzo Giusti,Ole Anton Werner,Riccardo Taiello,Matilde Carvalho Costa,Emre Tosun,Andrea Protani,Marc Molina,Rodrigo Lopes de Almeida,Paolo Cacace,Diogo Reis Santos,Luigi Serio*

Main category: cs.AI

TL;DR: Federation of Agents (FoA) is a distributed framework enabling collaborative multi-agent task execution using capability profiles and semantic algorithms. It achieves significant efficiency gains on complex reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: The need for dynamic and efficient coordination among heterogeneous multi-agent systems to handle complex tasks and unlock collective intelligence.

Method: FoA employs Versioned Capability Vectors (VCVs) for agent profiling, semantic routing for task-agent matching, dynamic task decomposition through consensus, and clustering for collaborative refinement of subtasks.

Result: FoA achieves 13x performance improvements on complex reasoning tasks compared to single-model baselines and scales horizontally without compromising performance.

Conclusion: FoA demonstrates that semantic orchestration paired with structured collaboration can significantly enhance the capabilities of federated AI systems.

Abstract: We present Federation of Agents (FoA), a distributed orchestration framework
that transforms static multi-agent coordination into dynamic, capability-driven
collaboration. FoA introduces Versioned Capability Vectors (VCVs):
machine-readable profiles that make agent capabilities searchable through
semantic embeddings, enabling agents to advertise their capabilities, cost, and
limitations. Our aarchitecturecombines three key innovations: (1) semantic
routing that matches tasks to agents over sharded HNSW indices while enforcing
operational constraints through cost-biased optimization, (2) dynamic task
decomposition where compatible agents collaboratively break down complex tasks
into DAGs of subtasks through consensus-based merging, and (3) smart clustering
that groups agents working on similar subtasks into collaborative channels for
k-round refinement before synthesis. Built on top of MQTT,s publish-subscribe
semantics for scalable message passing, FoA achieves sub-linear complexity
through hierarchical capability matching and efficient index maintenance.
Evaluation on HealthBench shows 13x improvements over single-model baselines,
with clustering-enhanced laboration particularly effective for complex
reasoning tasks requiring multiple perspectives. The system scales horizontally
while maintaining consistent performance, demonstrating that semantic
orchestration with structured collaboration can unlock the collective
intelligence of heterogeneous federations of AI agents.

</details>


### [24] [Scan-do Attitude: Towards Autonomous CT Protocol Management using a Large Language Model Agent](https://arxiv.org/abs/2509.20270)
*Xingjian Kang,Linda Vorberg,Andreas Maier,Alexander Katzmann,Oliver Taubmann*

Main category: cs.AI

TL;DR: The paper introduces a Large Language Model (LLM)-based agent to assist with managing CT scan protocols via natural language or structured requests, aiming to streamline workflows and reduce technologist workload.


<details>
  <summary>Details</summary>
Motivation: CT scan protocol management is time-consuming, requiring expertise, amidst a shortage of skilled radiology professionals.

Method: The proposed agent uses LLM's capabilities including in-context learning, instruction-following, and structured tool-calling for interpreting, modifying, and implementing protocol requests.

Result: Experimental evaluation demonstrates the agent's ability to retrieve protocol components, generate device-compatible configurations, and satisfy user requests, though limitations exist concerning ambiguous inputs and lack of unified APIs.

Conclusion: The approach suggests potential for LLM-based agents to enhance CT scan protocol management, though challenges remain in ensuring semantic accuracy and handling complexity.

Abstract: Managing scan protocols in Computed Tomography (CT), which includes adjusting
acquisition parameters or configuring reconstructions, as well as selecting
postprocessing tools in a patient-specific manner, is time-consuming and
requires clinical as well as technical expertise. At the same time, we observe
an increasing shortage of skilled workforce in radiology. To address this
issue, a Large Language Model (LLM)-based agent framework is proposed to assist
with the interpretation and execution of protocol configuration requests given
in natural language or a structured, device-independent format, aiming to
improve the workflow efficiency and reduce technologists' workload. The agent
combines in-context-learning, instruction-following, and structured toolcalling
abilities to identify relevant protocol elements and apply accurate
modifications. In a systematic evaluation, experimental results indicate that
the agent can effectively retrieve protocol components, generate device
compatible protocol definition files, and faithfully implement user requests.
Despite demonstrating feasibility in principle, the approach faces limitations
regarding syntactic and semantic validity due to lack of a unified device API,
and challenges with ambiguous or complex requests. In summary, the findings
show a clear path towards LLM-based agents for supporting scan protocol
management in CT imaging.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [25] [Open-source Stand-Alone Versatile Tensor Accelerator](https://arxiv.org/abs/2509.19790)
*Anthony Faure-Gignoux,Kevin Delmas,Adrien Gauffriau,Claire Pagetti*

Main category: cs.AR

TL;DR: This paper presents a standalone, open-source Python compiler for the FPGA-based Versatile Tensor Accelerator (VTA), addressing certification compliance and modularity issues.


<details>
  <summary>Details</summary>
Motivation: The demand for computational efficiency in ML applications, especially in safety-critical domains like aeronautics, necessitates solutions compliant with certification standards. Existing solutions like VTA depend on the TVM compiler, which lacks compliance.

Method: Researchers developed a standalone Python compiler pipeline, emphasizing certification requirements, modularity, and extensibility. They tested its functionality using a LeNet-5 CNN model on VTA simulators.

Result: The compiler successfully executed LeNet-5 on VTA simulators, showing promising preliminary results for scaling to larger CNN architectures.

Conclusion: The standalone compiler advances the usability of VTA by addressing compliance challenges and demonstrating scalability, with all contributions made publicly available.

Abstract: Machine Learning (ML) applications demand significant computational
resources, posing challenges for safety-critical domains like aeronautics. The
Versatile Tensor Accelerator (VTA) is a promising FPGA-based solution, but its
adoption was hindered by its dependency on the TVM compiler and by other code
non-compliant with certification requirements. This paper presents an
open-source, standalone Python compiler pipeline for the VTA, developed from
scratch and designed with certification requirements, modularity, and
extensibility in mind. The compiler's effectiveness is demonstrated by
compiling and executing LeNet-5 Convolutional Neural Network (CNN) using the
VTA simulators, and preliminary results indicate a strong potential for scaling
its capabilities to larger CNN architectures. All contributions are publicly
available.

</details>


### [26] [SpecMamba: Accelerating Mamba Inference on FPGA with Speculative Decoding](https://arxiv.org/abs/2509.19873)
*Linfeng Zhong,Songqiang Xu,Huifeng Wen,Tong Xie,Qingyu Guo,Yuan Wang,Meng Li*

Main category: cs.AR

TL;DR: SpecMamba introduces an FPGA-based accelerator for the Mamba SSM architecture, resolving speculative decoding inefficiencies and achieving notable speed and energy efficiency improvements over GPU and previous FPGA solutions.


<details>
  <summary>Details</summary>
Motivation: To address the growing demand for efficient long-sequence modeling on edge devices and deficiencies in SSMs' speculative decoding processes.

Method: System, algorithm, and hardware co-design featuring hybrid backtracking, FIFO-based tree verification, and optimized dataflows for FPGA-based implementation.

Result: SpecMamba outperforms GPU and prior FPGA implementations, achieving 2.27x speedup, 2.85x improvement, and up to 5.41x energy efficiency gains.

Conclusion: Through innovative design adaptations, SpecMamba significantly enhances the speed and energy efficiency of SSM-based long-sequence modeling, bridging hardware and computational constraints.

Abstract: The growing demand for efficient long-sequence modeling on edge devices has
propelled widespread adoption of State Space Models (SSMs) like Mamba, due to
their superior computational efficiency and scalability. As its autoregressive
generation process remains memory-bound, speculative decoding has been proposed
that incorporates draft model generation and target model verification.
However, directly applying speculative decoding to SSMs faces three key
challenges: (1) hidden state backtracking difficulties, (2) tree-based parallel
verification incompatibility, and (3) hardware workload mismatch. To address
these challenges, we propose SpecMamba, the first FPGA-based accelerator for
Mamba with speculative decoding, which features system, algorithm, and hardware
co-design. At the system level, we present a memory-aware hybrid backtracking
strategy to coordinate both models. At the algorithm level, we propose
first-in-first-out (FIFO)-based tree verification with tiling to minimize
memory access. At the hardware level, we customize a dataflow that computes
linear layers in parallel and SSM layers in series to enable maximal
overlapping. Implemented on AMD FPGA platforms (VHK158 and VCK190), SpecMamba
achieves a 2.27x speedup over GPU baselines and a 2.85x improvement compared to
prior FPGA solutions, while demonstrating 5.41x and 1.26x higher energy
efficiency, respectively.

</details>


### [27] [OpenGL GPU-Based Rowhammer Attack (Work in Progress)](https://arxiv.org/abs/2509.19959)
*Antoine Plin,Frédéric Fauberteau,Nga Nguyen*

Main category: cs.AR

TL;DR: The study demonstrates a GPU-assisted Rowhammer attack that leverages compute shaders for efficient and adaptive bit-flipping in DRAM, outperforming traditional CPU-based methods.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore the security vulnerabilities in modern DRAM memory systems, specifically through GPU-assisted Rowhammer attacks, highlighting the limitations of existing mitigations.

Method: The authors use GPU compute shaders combined with statistical targeting, memory initialization, iterative hammering, and dynamic adjustments to optimize Rowhammer attacks, achieving high performance and minimal overhead.

Result: The GPU-based Rowhammer attack on a Raspberry Pi 4 demonstrates superior bit-flipping rates compared to CPU-based methods, confirming its effectiveness and highlighting DRAM vulnerabilities.

Conclusion: The study provides insights into GPU-assisted Rowhammer attacks, emphasizing the need for stronger mitigation strategies in DRAM designs to address emerging security risks.

Abstract: Rowhammer attacks have emerged as a significant threat to modern DRAM-based
memory systems, leveraging frequent memory accesses to induce bit flips in
adjacent memory cells. This work-in-progress paper presents an adaptive,
many-sided Rowhammer attack utilizing GPU compute shaders to systematically
achieve high-frequency memory access patterns. Our approach employs statistical
distributions to optimize row targeting and avoid current mitigations. The
methodology involves initializing memory with known patterns, iteratively
hammering victim rows, monitoring for induced errors, and dynamically adjusting
parameters to maximize success rates. The proposed attack exploits the parallel
processing capabilities of GPUs to accelerate hammering operations, thereby
increasing the probability of successful bit flips within a constrained
timeframe. By leveraging OpenGL compute shaders, our implementation achieves
highly efficient row hammering with minimal software overhead. Experimental
results on a Raspberry Pi 4 demonstrate that the GPU-based approach attains a
high rate of bit flips compared to traditional CPU-based hammering, confirming
its effectiveness in compromising DRAM integrity. Our findings align with
existing research on microarchitectural attacks in heterogeneous systems that
highlight the susceptibility of GPUs to security vulnerabilities. This study
contributes to the understanding of GPU-assisted fault-injection attacks and
underscores the need for improved mitigation strategies in future memory
architectures.

</details>


### [28] [Automated Multi-Agent Workflows for RTL Design](https://arxiv.org/abs/2509.20182)
*Amulya Bhattaram,Janani Ramamoorthy,Ranit Gupta,Diana Marculescu,Dimitrios Stamoulis*

Main category: cs.AR

TL;DR: This paper introduces VeriMaAS, a multi-agent framework leveraging formal verification feedback for RTL code generation in HDL programming, providing significant synthesis performance improvements with reduced training data.


<details>
  <summary>Details</summary>
Motivation: To address challenges in program synthesis for specialized domains like HDL, such as scarce resources and high fine-tuning costs, the paper aims to optimize agentic AI workflows for RTL code generation.

Method: The method integrates formal verification feedback from HDL tools into agentic workflows, avoiding gradient-based updates and enabling efficient RTL code synthesis with minimal training supervision.

Result: VeriMaAS shows synthesis performance improvement by 5-7% in pass@k comparison against fine-tuned baselines, requiring significantly fewer training examples.

Conclusion: By embedding formal feedback, VeriMaAS effectively enhances synthesis performance while cutting supervision costs, presenting a scalable approach for agentic AI in specialized domains.

Abstract: The rise of agentic AI workflows unlocks novel opportunities for computer
systems design and optimization. However, for specialized domains such as
program synthesis, the relative scarcity of HDL and proprietary EDA resources
online compared to more common programming tasks introduces challenges, often
necessitating task-specific fine-tuning, high inference costs, and
manually-crafted agent orchestration. In this work, we present VeriMaAS, a
multi-agent framework designed to automatically compose agentic workflows for
RTL code generation. Our key insight is to integrate formal verification
feedback from HDL tools directly into workflow generation, reducing the cost of
gradient-based updates or prolonged reasoning traces. Our method improves
synthesis performance by 5-7% for pass@k over fine-tuned baselines, while
requiring only a few hundred training examples, representing an
order-of-magnitude reduction in supervision cost.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [29] [Automated Item Neutralization for Non-Cognitive Scales: A Large Language Model Approach to Reducing Social-Desirability Bias](https://arxiv.org/abs/2509.19314)
*Sirui Wu,Daijin Yang*

Main category: cs.CL

TL;DR: The study explores using GPT-3 to rewrite personality test items for reducing social desirability bias. Results show some promise but highlight inconsistencies, preserving reliability but affecting specific traits and social desirability correlations.


<details>
  <summary>Details</summary>
Motivation: To evaluate whether large language models (LLMs) can assist in reducing social desirability bias in personality assessments.

Method: The International Personality Item Pool Big Five Measure (IPIP-BFM-50) was rewritten using GPT-3 to neutralize bias, and 203 participants completed the original or revised version, alongside a social desirability measure.

Result: The reliability and five-factor structure were preserved, with changes in trait scores, inconsistent reductions in social desirability correlations, and failure of metric and scalar invariance.

Conclusion: AI-based item neutralization shows potential for reducing bias, but its effectiveness remains imperfect.

Abstract: This study evaluates item neutralization assisted by the large language model
(LLM) to reduce social desirability bias in personality assessment. GPT-o3 was
used to rewrite the International Personality Item Pool Big Five Measure
(IPIP-BFM-50), and 203 participants completed either the original or
neutralized form along with the Marlowe-Crowne Social Desirability Scale. The
results showed preserved reliability and a five-factor structure, with gains in
Conscientiousness and declines in Agreeableness and Openness. The correlations
with social desirability decreased for several items, but inconsistently.
Configural invariance held, though metric and scalar invariance failed.
Findings support AI neutralization as a potential but imperfect bias-reduction
method.

</details>


### [30] [FHIR-AgentBench: Benchmarking LLM Agents for Realistic Interoperable EHR Question Answering](https://arxiv.org/abs/2509.19319)
*Gyubok Lee,Elea Bach,Eric Yang,Tom Pollard,Alistair Johnson,Edward Choi,Yugang jia,Jong Ha Lee*

Main category: cs.CL

TL;DR: FHIR-AgentBench introduces a benchmark grounded in HL7 FHIR standards for assessing LLM agents' ability to handle real-world clinical questions.


<details>
  <summary>Details</summary>
Motivation: The shift to HL7 FHIR standards requires benchmarks for evaluating AI dealing with complex healthcare data models rather than traditional structured data.

Method: Researchers developed FHIR-AgentBench consisting of real-world clinical questions and systematically measured LLM performance in data retrieval, interaction patterns, and reasoning strategies.

Result: Results identify challenges in data retrieval and reasoning over complex FHIR resources, exposing practical limitations in question answering performance.

Conclusion: FHIR-AgentBench advances clinical AI by providing a robust framework to evaluate LLM agents on interoperable healthcare data, promoting reproducible research.

Abstract: The recent shift toward the Health Level Seven Fast Healthcare
Interoperability Resources (HL7 FHIR) standard opens a new frontier for
clinical AI, demanding LLM agents to navigate complex, resource-based data
models instead of conventional structured health data. However, existing
benchmarks have lagged behind this transition, lacking the realism needed to
evaluate recent LLMs on interoperable clinical data. To bridge this gap, we
introduce FHIR-AgentBench, a benchmark that grounds 2,931 real-world clinical
questions in the HL7 FHIR standard. Using this benchmark, we systematically
evaluate agentic frameworks, comparing different data retrieval strategies
(direct FHIR API calls vs. specialized tools), interaction patterns
(single-turn vs. multi-turn), and reasoning strategies (natural language vs.
code generation). Our experiments highlight the practical challenges of
retrieving data from intricate FHIR resources and the difficulty of reasoning
over them, both of which critically affect question answering performance. We
publicly release the FHIR-AgentBench dataset and evaluation suite
(https://github.com/glee4810/FHIR-AgentBench) to promote reproducible research
and the development of robust, reliable LLM agents for clinical applications.

</details>


### [31] [Readme_AI: Dynamic Context Construction for Large Language Models](https://arxiv.org/abs/2509.19322)
*Millie Vyas,Timothy Blattner,Alden Dima*

Main category: cs.CL

TL;DR: This paper introduces a dynamic context-building specification for Large Language Models (LLMs) using metadata from data sources, aiming to improve the accuracy and reliability of LLM responses.


<details>
  <summary>Details</summary>
Motivation: LLMs, despite being trained on vast amounts of data, often provide inaccurate and unreliable information for specific queries. Enhancing their responses with context from user-specific data sources is needed.

Method: The paper proposes the Readme_AI Model Context Protocol (MCP), which dynamically retrieves and formats metadata from data sources, making context accessible for LLMs. The specification uses extensible types for various data sources and user-specified tags to group the context.

Result: Using the Readme_AI prototype, the paper demonstrates improved LLM responses for specific queries, such as providing accurate information and generating code for the NIST-developed Hedgehog library based on context from metadata.

Conclusion: The proposed protocol successfully reduces hallucinations in LLMs by grounding them in specialized, data-owner-provided context. The Readme_AI tool represents an important step in enhancing LLM reliability for domain-specific tasks.

Abstract: Despite being trained on significant amounts of data, Large Language Models
(LLMs) can provide inaccurate or unreliable information in the context of a
user's specific query. Given query-specific context significantly improves the
usefulness of its responses. In this paper, we present a specification that can
be used to dynamically build context for data sources. The data source owner
creates the file containing metadata for LLMs to use when reasoning about
dataset-related queries. To demonstrate our proposed specification, we created
a prototype Readme_AI Model Context Protocol (MCP) server that retrieves the
metadata from the data source and uses it to dynamically build context. Some
features that make this specification dynamic are the extensible types that
represent crawling web-pages, fetching data from data repositories, downloading
and parsing publications, and general text. The context is formatted and
grouped using user-specified tags that provide clear contextual information for
the LLM to reason about the content. We demonstrate the capabilities of this
early prototype by asking the LLM about the NIST-developed Hedgehog library,
for which common LLMs often provides inaccurate and irrelevant responses
containing hallucinations. With Readme_AI, the LLM receives enough context that
it is now able to reason about the library and its use, and even generate code
interpolated from examples that were included in the Readme_AI file provided by
Hedgehog's developer. Our primary contribution is a extensible protocol for
dynamically grounding LLMs in specialized, owner-provided data, enhancing
responses from LLMs and reducing hallucinations. The source code for the
Readme_AI tool is posted here: https://github.com/usnistgov/readme_ai .

</details>


### [32] [Magnitude Matters: a Superior Class of Similarity Metrics for Holistic Semantic Understanding](https://arxiv.org/abs/2509.19323)
*V. S. Raghu Parupudi*

Main category: cs.CL

TL;DR: The paper presents two new similarity metrics, Overlap Similarity (OS) and Hyperbolic Tangent Similarity (HTS), which are magnitude-aware and address limitations of dot product and cosine similarity for comparing high-dimensional vectors in NLP.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to improve upon traditional vector comparison methods like raw dot product and cosine similarity, which either overly depend on vector magnitude or discard it entirely. The aim is to create metrics that integrate vector magnitude and alignment to benefit NLP tasks.

Method: This paper introduces two new similarity metrics, OS and HTS, and evaluates them across four sentence embedding models (e.g., all-MiniLM-L6-v2) on eight NLP benchmarks using the Wilcoxon signed-rank test for statistical significance.

Result: The OS and HTS metrics significantly improved results on tasks requiring holistic semantic understanding (e.g., paraphrase and inference benchmarks) compared to dot product and cosine similarity. However, no significant advantage was found for benchmarks testing compositional semantics.

Conclusion: Magnitude-aware similarity metrics like OS and HTS can outperform traditional metrics in specific NLP tasks needing semantic understanding, but their efficacy diminishes in tasks involving nuanced compositional semantics. Future work should address challenges in representing compositional text.

Abstract: Vector comparison in high dimensions is a fundamental task in NLP, yet it is
dominated by two baselines: the raw dot product, which is unbounded and
sensitive to vector norms, and the cosine similarity, which discards magnitude
information entirely. This paper challenges both standards by proposing and
rigorously evaluating a new class of parameter-free, magnitude-aware similarity
metrics. I introduce two such functions, Overlap Similarity (OS) and Hyperbolic
Tangent Similarity (HTS), designed to integrate vector magnitude and alignment
in a more principled manner. To ensure that my findings are robust and
generalizable, I conducted a comprehensive evaluation using four
state-of-the-art sentence embedding models (all-MiniLM-L6-v2,
all-mpnet-base-v2, paraphrase-mpnet-base-v2, and BAAI/bge-large-en-v1.5) across
a diverse suite of eight standard NLP benchmarks, including STS-B, SICK, Quora,
and PAWS. Using the Wilcoxon signed-rank test for statistical significance, my
results are definitive: on the tasks requiring holistic semantic understanding
(paraphrase and inference), both OS and HTS provide a statistically significant
improvement in Mean Squared Error over both the raw dot product and cosine
similarity, regardless of the underlying embedding model.Crucially, my findings
delineate the specific domain of advantage for these metrics: for tasks
requiring holistic semantic understanding like paraphrase and inference, my
magnitude-aware metrics offer a statistically superior alternative. This
significant improvement was not observed on benchmarks designed to test highly
nuanced compositional semantics (SICK, STS-B), identifying the challenge of
representing compositional text as a distinct and important direction for
future work.

</details>


### [33] [How Much of Your Data Can Suck? Thresholds for Domain Performance and Emergent Misalignment in LLMs](https://arxiv.org/abs/2509.19325)
*Jian Ouyang,Arman T,Ge Jin*

Main category: cs.CL

TL;DR: The paper studies the impact of incorrect data on fine-tuned large language models like gpt-4o, finding that even small amounts of incorrect data significantly harm its performance and safety.


<details>
  <summary>Details</summary>
Motivation: To explore how fine-tuning with incorrect data can lead to emergent misalignment, which causes harmful or deceptive outputs in high-stakes domains like finance, law, and health.

Method: The paper evaluates gpt-4o fine-tuned with varying ratios (10% to 90%) of incorrect data, across four domains: coding, finance, health, and legal.

Result: It finds that 10-25% incorrect data greatly degrades performance while reducing correct moral alignment. A minimum of 50% correct data is required for recovery, but performance rarely matches that of base models.

Conclusion: High-stakes applications should avoid fine-tuning with incorrect data, focus on high-quality data curation, or use base models without fine-tuning to ensure safety and robustness.

Abstract: This paper investigates the impact of incorrect data on the performance and
safety of large language models (LLMs), specifically gpt-4o, during supervised
fine-tuning (SFT). Although LLMs become increasingly vital across broad domains
like finance, coding, law, and health, fine-tuning on incorrect data can lead
to "emergent misalignment," producing harmful or deceptive outputs unrelated to
the intended task. We evaluate gpt-4o models fine-tuned with varying ratios
(10\% to 90\% correct) of both obviously and subtly incorrect data across four
domains: coding, finance, health, and legal. Our findings show that even modest
amounts of incorrect data (10-25\%) dramatically degrade domain performance and
not moral alignment. A clear threshold of at least 50\% correct data is needed
for models to consistently recover strong performance, though they rarely match
the robustness and safety of the base model, which exhibits near-perfect
alignment and zero dangerous completions out-of-the-box. This research
emphasizes that the cost of incorrect data is heavy, highlighting the critical
need for extremely high-quality data curation or, alternatively, leveraging
robust base models without unnecessary fine-tuning for high-stakes
applications.

</details>


### [34] [Unveiling the Merits and Defects of LLMs in Automatic Review Generation for Scientific Papers](https://arxiv.org/abs/2509.19326)
*Ruochi Li,Haoxuan Zhang,Edward Gehringer,Ting Xiao,Junhua Ding,Haihua Chen*

Main category: cs.CL

TL;DR: This paper evaluates the use of large language models (LLMs) in the peer-review process, highlighting their strengths in descriptive feedback but weaknesses in critical reasoning and contextual accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation arises from the increased volume of scientific submissions, which has overwhelmed the traditional peer-review system and necessitated exploration of LLMs for automated review generation.

Method: The paper proposes an evaluation framework combining semantic similarity analysis and structured knowledge graph metrics to assess LLM-generated reviews. It creates a benchmark dataset of 1,683 papers and 6,495 expert reviews and tests five LLMs, including GPT-4o.

Result: LLMs perform well in summarizing contributions and methodologies but underperform in identifying weaknesses and providing critical feedback. For instance, GPT-4o generated more descriptive content than human reviewers but was significantly less effective in highlighting flaws and adapting to paper quality.

Conclusion: While LLMs like GPT-4o show promise in assisting the review process, they lack the critical reasoning abilities of human reviewers. The study identifies key areas for improvement to enhance the effectiveness of LLM-assisted peer reviews.

Abstract: The surge in scientific submissions has placed increasing strain on the
traditional peer-review process, prompting the exploration of large language
models (LLMs) for automated review generation. While LLMs demonstrate
competence in producing structured and coherent feedback, their capacity for
critical reasoning, contextual grounding, and quality sensitivity remains
limited. To systematically evaluate these aspects, we propose a comprehensive
evaluation framework that integrates semantic similarity analysis and
structured knowledge graph metrics to assess LLM-generated reviews against
human-written counterparts. We construct a large-scale benchmark of 1,683
papers and 6,495 expert reviews from ICLR and NeurIPS in multiple years, and
generate reviews using five LLMs. Our findings show that LLMs perform well in
descriptive and affirmational content, capturing the main contributions and
methodologies of the original work, with GPT-4o highlighted as an illustrative
example, generating 15.74% more entities than human reviewers in the strengths
section of good papers in ICLR 2025. However, they consistently underperform in
identifying weaknesses, raising substantive questions, and adjusting feedback
based on paper quality. GPT-4o produces 59.42% fewer entities than real
reviewers in the weaknesses and increases node count by only 5.7% from good to
weak papers, compared to 50% in human reviews. Similar trends are observed
across all conferences, years, and models, providing empirical foundations for
understanding the merits and defects of LLM-generated reviews and informing the
development of future LLM-assisted reviewing tools. Data, code, and more
detailed results are publicly available at
https://github.com/RichardLRC/Peer-Review.

</details>


### [35] [A systematic review of trial-matching pipelines using large language models](https://arxiv.org/abs/2509.19327)
*Braxton A. Morrison,Madhumita Sushil,Jacob S. Young*

Main category: cs.CL

TL;DR: Identifying clinical trials for patients using large language models (LLMs) is promising but faces issues like data variability, deployment costs, and biases. GPT-4 consistently performs well.


<details>
  <summary>Details</summary>
Motivation: Current approaches for manual clinical trial matching are labor-intensive and error-prone, causing delays. Automating the process using LLMs could address these inefficiencies, especially in oncology.

Method: The study conducted a systematic review of articles (2020-2025) across three academic databases and a preprint server, assessing LLM-based approaches to clinical trial matching. Reviewed models were evaluated on tasks like patient-to-trial matching, criterion extraction, and eligibility classification.

Result: From 126 articles, 31 were eligible for analysis. GPT-4 outperformed other models in most tasks but with higher associated costs. Promising strategies include zero-shot prompting, retrieval-based methods, and fine-tuning smaller open-source models for data privacy.

Conclusion: While LLMs like GPT-4 show immense potential, challenges such as lack of real-world data, economic concerns, risk of hallucinations, and biases must be addressed for scalable deployment. Standardized benchmarks and attention to cost and fairness are essential for progress.

Abstract: Matching patients to clinical trial options is critical for identifying novel
treatments, especially in oncology. However, manual matching is labor-intensive
and error-prone, leading to recruitment delays. Pipelines incorporating large
language models (LLMs) offer a promising solution. We conducted a systematic
review of studies published between 2020 and 2025 from three academic databases
and one preprint server, identifying LLM-based approaches to clinical trial
matching. Of 126 unique articles, 31 met inclusion criteria. Reviewed studies
focused on matching patient-to-criterion only (n=4), patient-to-trial only
(n=10), trial-to-patient only (n=2), binary eligibility classification only
(n=1) or combined tasks (n=14). Sixteen used synthetic data; fourteen used real
patient data; one used both. Variability in datasets and evaluation metrics
limited cross-study comparability. In studies with direct comparisons, the
GPT-4 model consistently outperformed other models, even finely-tuned ones, in
matching and eligibility extraction, albeit at higher cost. Promising
strategies included zero-shot prompting with proprietary LLMs like the GPT-4o
model, advanced retrieval methods, and fine-tuning smaller, open-source models
for data privacy when incorporation of large models into hospital
infrastructure is infeasible. Key challenges include accessing sufficiently
large real-world data sets, and deployment-associated challenges such as
reducing cost, mitigating risk of hallucinations, data leakage, and bias. This
review synthesizes progress in applying LLMs to clinical trial matching,
highlighting promising directions and key limitations. Standardized metrics,
more realistic test sets, and attention to cost-efficiency and fairness will be
critical for broader deployment.

</details>


### [36] [How Model Size, Temperature, and Prompt Style Affect LLM-Human Assessment Score Alignment](https://arxiv.org/abs/2509.19329)
*Julie Jung,Max Lu,Sina Chole Benker,Dogus Darici*

Main category: cs.CL

TL;DR: The study evaluates the alignment of LLMs based on model size, temperature, and prompt style in assessing clinical reasoning skills, focusing on alignment within models, between models, and with humans.


<details>
  <summary>Details</summary>
Motivation: To determine the key factors influencing the alignment of LLM-generated scores with human evaluations of clinical reasoning.

Method: Variations in model size, temperature, and prompt style were tested to analyze their effects on LLM alignments across different levels.

Result: Model size was identified as the most significant factor affecting alignment with human scoring.

Conclusion: Evaluating alignment across multiple levels is crucial when assessing LLM-human score alignment in clinical reasoning tasks.

Abstract: We examined how model size, temperature, and prompt style affect Large
Language Models' (LLMs) alignment within itself, between models, and with human
in assessing clinical reasoning skills. Model size emerged as a key factor in
LLM-human score alignment. Study highlights the importance of checking
alignments across multiple levels.

</details>


### [37] [Quantifying Compositionality of Classic and State-of-the-Art Embeddings](https://arxiv.org/abs/2509.19332)
*Zhijin Guo,Chenhao Xue,Zhaozhen Xu,Hongbo Bo,Yuxuan Ye,Janet B. Pierrehumbert,Martha Lewis*

Main category: cs.CL

TL;DR: This paper evaluates additive compositionality in language models using a two-step framework and identifies stronger compositional signals in later training stages and deeper transformer layers.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the need for language models to correctly generalize to novel expressions by exploiting compositional meanings under justified conditions.

Method: A two-step evaluation is introduced, comprising (i) canonical correlation analysis to measure linear relationships between entity attributes and embeddings, and (ii) reconstruction-based metrics to assess additive generalization.

Result: The study finds stronger compositional signals in late training stages and deeper layers of transformer models, with a subsequent decline observed in the top layer.

Conclusion: The paper highlights the importance of compositional signals for generalization in language models, with variations observed across layers and training stages.

Abstract: For language models to generalize correctly to novel expressions, it is
critical that they exploit access compositional meanings when this is
justified. Even if we don't know what a "pelp" is, we can use our knowledge of
numbers to understand that "ten pelps" makes more pelps than "two pelps".
Static word embeddings such as Word2vec made strong, indeed excessive, claims
about compositionality. The SOTA generative, transformer models and graph
models, however, go too far in the other direction by providing no real limits
on shifts in meaning due to context. To quantify the additive compositionality,
we formalize a two-step, generalized evaluation that (i) measures the linearity
between known entity attributes and their embeddings via canonical correlation
analysis, and (ii) evaluates additive generalization by reconstructing
embeddings for unseen attribute combinations and checking reconstruction
metrics such as L2 loss, cosine similarity, and retrieval accuracy. These
metrics also capture failure cases where linear composition breaks down.
Sentences, knowledge graphs, and word embeddings are evaluated and tracked the
compositionality across all layers and training stages. Stronger compositional
signals are observed in later training stages across data modalities, and in
deeper layers of the transformer-based model before a decline at the top layer.
Code is available at
https://github.com/Zhijin-Guo1/quantifying-compositionality.

</details>


### [38] [Pluralistic Off-policy Evaluation and Alignment](https://arxiv.org/abs/2509.19333)
*Chengkai Huang,Junda Wu,Zhouhang Xie,Yu Xia,Rui Wang,Tong Yu,Subrata Mitra,Julian McAuley,Lina Yao*

Main category: cs.CL

TL;DR: The paper introduces POPE, a framework for better aligning large language models (LLMs) with diverse human preferences by incorporating pluralism in preference evaluation and alignment.


<details>
  <summary>Details</summary>
Motivation: To address the need for aligning LLMs with diverse human preferences, which current methods fail to fully capture due to their focus solely on overall utility without addressing pluralism.

Method: The paper introduces POPE, which combines a collaborative utility component with a diversity component in its reward function and uses decomposable inverse propensity scoring (IPS) estimators for evaluation and optimization.

Result: Empirical findings show POPE enhances pluralistic response generation while preserving the general capabilities of LLMs on other tasks.

Conclusion: POPE is an effective framework for improving pluralistic alignment in LLMs, balancing diverse human preferences and model performance.

Abstract: Personalized preference alignment for LLMs with diverse human preferences
requires evaluation and alignment methods that capture pluralism. Most existing
preference alignment datasets are logged under policies that differ
substantially from the evaluated LLMs, and existing off-policy estimators focus
solely on overall utility while ignoring preference pluralism. Extending
Off-Policy Evaluation (OPE) to pluralistic preference alignment, therefore,
remains an open question. Thus, we propose the Pluralistic Off-Policy
Evaluation (POPE), the first framework for offline pluralistic preference
evaluation and alignment in LLMs. POPE includes a unified reward function that
combines (1) a collaborative utility component derived from human preference
signals (e.g., upvotes or relevance scores) and (2) a diversity component
inspired by entropy-based coverage measures, together reflecting pluralistic
alignment. Furthermore, to estimate this reward from logged interactions, we
derive decomposable inverse propensity scoring (IPS) estimators that separately
evaluate relevance and diversity. Theoretically, we prove that our decomposed
IPS estimators establish a lower bound on their variance. With the off-policy
evaluated value function, we can directly enable off-policy optimization to
further enhance pluralistic alignment. Empirical results demonstrate that POPE
efficiently enhances pluralistic response generation and maintains the models'
general capabilities on downstream tasks

</details>


### [39] [Cognitive-Level Adaptive Generation via Capability-Aware Retrieval and Style Adaptation](https://arxiv.org/abs/2509.19336)
*Qingsong Wang,Tao Wu,Wang Lin,Yueying Feng,Gongsheng Yuan,Chang Yao,Jingyuan Chen*

Main category: cs.CL

TL;DR: The paper introduces CLAF, a framework for aligning content generated by LLMs with users' cognitive capacities, addressing issues in knowledge complexity and presentation style.


<details>
  <summary>Details</summary>
Motivation: LLMs often fail to adapt content to users' cognitive levels, resulting in misalignment in knowledge complexity and presentation style.

Method: The authors propose CLAF, which integrates a retrieval module, style optimization guided by Bloom's taxonomy, and a knowledge-controllable generation system. A new dataset, SCALE, was created for training and evaluation.

Result: Empirical results indicate that CLAF improves LLM outputs' adaptability and informativeness across diverse user profiles.

Conclusion: CLAF shows promise in solving cognitive misalignment issues, enhancing user-specific content generation in practical applications.

Abstract: Large Language Models (LLMs) have demonstrated strong performance in
open-ended generation tasks. However, they often struggle to adapt content to
users with differing cognitive capacities, leading to a phenomenon we term
cognitive misalignment. This issue arises in two forms: knowledge-level
misalignment, where content is too complex or too simplistic relative to user
understanding, and presentation-style misalignment, where the structure or tone
hinders effective comprehension. To address these challenges, we propose the
Cognitive-Level Alignment Framework (CLAF), a general-purpose generation
framework that aligns both knowledge complexity and presentation style with
user cognition. CLAF integrates a capability-aware retrieval module based on a
hierarchical knowledge graph and a style optimization module guided by Bloom's
taxonomy and preference learning. Additionally, a knowledge-controllable
generation component ensures consistency and relevance throughout the output.
To support training and evaluation, we construct SCALE, a cognitively annotated
dataset containing responses at multiple comprehension levels per query.
Empirical results show that CLAF enhances the adaptability and informativeness
of LLM outputs across a range of user profiles, offering a robust solution to
cognitive-level alignment in real-world applications.

</details>


### [40] [Part-of-speech tagging for Nagamese Language using CRF](https://arxiv.org/abs/2509.19343)
*Alovi N Shohe,Chonglio Khiamungam,Teisovi Angami*

Main category: cs.CL

TL;DR: This study addresses the first part-of-speech (POS) tagging for the Nagamese language using Conditional Random Fields (CRF), achieving 85.7% accuracy.


<details>
  <summary>Details</summary>
Motivation: Investigate and establish a POS tagging system for the under-researched and resource-scarce Nagamese language, which has no prior work in this area.

Method: The authors created an annotated corpus of 16,112 tokens and applied the CRF algorithm to identify parts of speech in Nagamese sentences.

Result: The study achieved 85.7% tagging accuracy, with precision and recall at 86% and an F1-score of 85%.

Conclusion: This foundational work sets the stage for further NLP research in Nagamese by successfully implementing a POS tagging system with machine learning techniques.

Abstract: This paper investigates part-of-speech tagging, an important task in Natural
Language Processing (NLP) for the Nagamese language. The Nagamese language,
a.k.a. Naga Pidgin, is an Assamese-lexified Creole language developed primarily
as a means of communication in trade between the Nagas and people from Assam in
northeast India. A substantial amount of work in part-of-speech-tagging has
been done for resource-rich languages like English, Hindi, etc. However, no
work has been done in the Nagamese language. To the best of our knowledge, this
is the first attempt at part-of-speech tagging for the Nagamese Language. The
aim of this work is to identify the part-of-speech for a given sentence in the
Nagamese language. An annotated corpus of 16,112 tokens is created and applied
machine learning technique known as Conditional Random Fields (CRF). Using CRF,
an overall tagging accuracy of 85.70%; precision, recall of 86%, and f1-score
of 85% is achieved.
  Keywords. Nagamese, NLP, part-of-speech, machine learning, CRF.

</details>


### [41] [Performance of Large Language Models in Answering Critical Care Medicine Questions](https://arxiv.org/abs/2509.19344)
*Mahmoud Alwakeel,Aditya Nagori,An-Kwok Ian Wong,Neal Chaisson,Vijay Krishnamoorthy,Rishikesan Kamaleswaran*

Main category: cs.CL

TL;DR: Meta-Llama 3.1 models were tested on Critical Care Medicine questions. The 70B model scored an average accuracy of 60%, outperforming the 8B model by 30%, but performance varied by domain.


<details>
  <summary>Details</summary>
Motivation: To assess the performance of large language models, specifically Meta-Llama 3.1, in specialized medical fields like Critical Care Medicine, where evaluation has been limited.

Method: Meta-Llama 3.1 models with 8B and 70B parameters were tested on 871 Critical Care Medicine questions. Their performance was compared across various domains within the specialty.

Result: The Llama3.1:70B model achieved an average accuracy of 60%, outperforming Llama3.1:8B by 30%. Performance ranged by domain, with the best in Research (68.4%) and the worst in Renal (47.9%).

Conclusion: The performance of the Llama3.1:70B model shows promise in specialized medical fields, though its domain-specific variation signals a need for improvement across subspecialties.

Abstract: Large Language Models have been tested on medical student-level questions,
but their performance in specialized fields like Critical Care Medicine (CCM)
is less explored. This study evaluated Meta-Llama 3.1 models (8B and 70B
parameters) on 871 CCM questions. Llama3.1:70B outperformed 8B by 30%, with 60%
average accuracy. Performance varied across domains, highest in Research
(68.4%) and lowest in Renal (47.9%), highlighting the need for broader future
work to improve models across various subspecialty domains.

</details>


### [42] [SCORE: A Semantic Evaluation Framework for Generative Document Parsing](https://arxiv.org/abs/2509.19345)
*Renyu Li,Antonio Jimeno Yepes,Yao You,Kamil Pluciński,Maximilian Operlejn,Crag Wolfe*

Main category: cs.CL

TL;DR: This paper introduces SCORE, a novel evaluation framework for multi-modal generative document parsing systems, addressing the limitations of traditional metrics. SCORE offers robust, interpretation-agnostic evaluations and reveals insights missed by standard metrics.


<details>
  <summary>Details</summary>
Motivation: Traditional metrics fail to evaluate multi-modal generative document parsing systems properly, often penalizing alternative yet valid outputs.

Method: The paper proposes SCORE, which uses (i) adjusted edit distance for content fidelity, (ii) token-level diagnostics, (iii) spatial tolerance and semantic alignment in table evaluation, and (iv) hierarchy-aware consistency checks.

Result: SCORE revealed overlooked performance patterns, corrected unjust penalties by traditional metrics, and demonstrated that generative parsing suffices for comprehensive evaluation, achieving up to 0.93 reproductions of traditional table scores.

Conclusion: SCORE establishes a fair and semantically rigorous evaluation framework, highlighting interpretive diversity and providing practical benchmarks for modern document parsing systems.

Abstract: Multi-modal generative document parsing systems challenge traditional
evaluation: unlike deterministic OCR or layout models, they often produce
semantically correct yet structurally divergent outputs. Conventional
metrics-CER, WER, IoU, or TEDS-misclassify such diversity as error, penalizing
valid interpretations and obscuring system behavior.
  We introduce SCORE (Structural and COntent Robust Evaluation), an
interpretation-agnostic framework that integrates (i) adjusted edit distance
for robust content fidelity, (ii) token-level diagnostics to distinguish
hallucinations from omissions, (iii) table evaluation with spatial tolerance
and semantic alignment, and (iv) hierarchy-aware consistency checks. Together,
these dimensions enable evaluation that embraces representational diversity
while enforcing semantic rigor.
  Across 1,114 pages spanning a holistic benchmark and a field dataset, SCORE
consistently revealed cross-dataset performance patterns missed by standard
metrics. In 2-5% of pages with ambiguous table structures, traditional metrics
penalized systems by 12-25% on average, leading to distorted rankings. SCORE
corrected these cases, recovering equivalence between alternative but valid
interpretations. Moreover, by normalizing generative outputs into a
format-agnostic representation, SCORE reproduces traditional scores (e.g.,
table F1 up to 0.93) without requiring object-detection pipelines,
demonstrating that generative parsing alone suffices for comprehensive
evaluation.
  By exposing how interpretive diversity impacts evaluation outcomes and
providing multi-dimensional, interpretable diagnostics, SCORE establishes
foundational principles for semantically grounded, fair, and practical
benchmarking of modern document parsing systems.

</details>


### [43] [Benchmarking ChatGPT and DeepSeek in April 2025: A Novel Dual Perspective Sentiment Analysis Using Lexicon-Based and Deep Learning Approaches](https://arxiv.org/abs/2509.19346)
*Maryam Mahdi Alhusseini,Mohammad-Reza Feizi-Derakhshi*

Main category: cs.CL

TL;DR: The study integrates sentiment analysis and deep learning to assess user satisfaction with ChatGPT and DeepSeek, showing superior sentiment for ChatGPT and better performance of deep learning techniques.


<details>
  <summary>Details</summary>
Motivation: The paper aims to bridge the gap in sentiment analysis approaches by combining lexicon-based methods and deep learning models to analyze user reviews of LLM applications.

Method: Analyzed 4,000 Google Play Store reviews of ChatGPT and DeepSeek using sentiment analysis (TextBlob) and deep learning models (CNN and Bi-LSTM). Balanced classes through oversampling for rigorous testing.

Result: ChatGPT received more positive sentiment than DeepSeek. CNN achieved 96.41% accuracy, outperformed Bi-LSTM, and delivered strong F1-scores across sentiment categories.

Conclusion: The research enhances sentiment measurement methodologies for LLM applications, offers insights to improve user-centric AI designs, and highlights the value of deep learning models in this domain.

Abstract: This study presents a novel dual-perspective approach to analyzing user
reviews for ChatGPT and DeepSeek on the Google Play Store, integrating
lexicon-based sentiment analysis (TextBlob) with deep learning classification
models, including Convolutional Neural Networks (CNN) and Bidirectional Long
Short Term Memory (Bi LSTM) Networks. Unlike prior research, which focuses on
either lexicon-based strategies or predictive deep learning models in
isolation, this study conducts an extensive investigation into user
satisfaction with Large Language Model (LLM) based applications. A Dataset of
4,000 authentic user reviews was collected, which were carefully preprocessed
and subjected to oversampling to achieve balanced classes. The balanced test
set of 1,700 Reviews were used for model testing. Results from the experiments
reveal that ChatGPT received significantly more positive sentiment than
DeepSeek. Furthermore, deep learning based classification demonstrated superior
performance over lexicon analysis, with CNN outperforming Bi-LSTM by achieving
96.41 percent accuracy and near perfect classification of negative reviews,
alongside high F1-scores for neutral and positive sentiments. This research
sets a new methodological standard for measuring sentiment in LLM-based
applications and provides practical insights for developers and researchers
seeking to improve user-centric AI system design.

</details>


### [44] [Characterizing Knowledge Graph Tasks in LLM Benchmarks Using Cognitive Complexity Frameworks](https://arxiv.org/abs/2509.19347)
*Sara Todorovikj,Lars-Peter Meyer,Michael Martin*

Main category: cs.CL

TL;DR: The paper evaluates how large language models (LLMs) interact with knowledge graphs (KGs) by analyzing tasks through cognitive psychology frameworks, yielding insights for better benchmarking.


<details>
  <summary>Details</summary>
Motivation: Examine the complexity of tasks LLMs undertake with KGs, moving beyond accuracy to understand cognitive requirements and diversify task evaluation benchmarks.

Method: Incorporates three cognitive psychology complexity frameworks to evaluate tasks under the LLM-KG-Bench framework.

Result: Exposed underrepresented cognitive demands in LLM-KG task evaluations, emphasizing flaws in value distributions and the need for more diverse benchmarking tasks.

Conclusion: Cognitive psychology frameworks enrich the understanding of LLM-KG task complexities, proposing improvements for benchmarking diversity and interpretation.

Abstract: Large Language Models (LLMs) are increasingly used for tasks involving
Knowledge Graphs (KGs), whose evaluation typically focuses on accuracy and
output correctness. We propose a complementary task characterization approach
using three complexity frameworks from cognitive psychology. Applying this to
the LLM-KG-Bench framework, we highlight value distributions, identify
underrepresented demands and motivate richer interpretation and diversity for
benchmark evaluation tasks.

</details>


### [45] [ShinkaEvolve: Towards Open-Ended And Sample-Efficient Program Evolution](https://arxiv.org/abs/2509.19349)
*Robert Tjarko Lange,Yuki Imajuku,Edoardo Cetin*

Main category: cs.CL

TL;DR: The paper introduces ShinkaEvolve, an open-source framework leveraging large language models to enhance the efficiency and quality of scientific discovery.


<details>
  <summary>Details</summary>
Motivation: Existing solutions using LLMs for scientific discovery are inefficient in terms of sample usage and remain closed-source, which limits their accessibility and flexibility.

Method: ShinkaEvolve integrates three innovations: a parent sampling method balancing exploration and exploitation, novelty-based rejection sampling to optimize the search space, and a bandit-based ensemble selection strategy for LLMs.

Result: ShinkaEvolve achieved better sample efficiency and solution quality across various tasks, such as discovering state-of-the-art solutions in circle packing, improving mathematical reasoning, competitive programming solutions, and load-balancing functions.

Conclusion: ShinkaEvolve demonstrates its effectiveness and versatility in solving diverse computational problems while offering open-source availability, paving the way for more accessible and efficient scientific discovery.

Abstract: We introduce ShinkaEvolve: a new open-source framework leveraging large
language models (LLMs) to advance scientific discovery with state-of-the-art
performance and unprecedented efficiency. Recent advances in scaling inference
time compute of LLMs have enabled significant progress in generalized
scientific discovery. These approaches rely on evolutionary agentic harnesses
that leverage LLMs as mutation operators to generate candidate solutions.
However, current code evolution methods suffer from critical limitations: they
are sample inefficient, requiring thousands of samples to identify effective
solutions, and remain closed-source, hindering broad adoption and extension.
ShinkaEvolve addresses these limitations, introducing three key innovations: a
parent sampling technique balancing exploration and exploitation, code novelty
rejection-sampling for efficient search space exploration, and a bandit-based
LLM ensemble selection strategy. We evaluate ShinkaEvolve across diverse tasks,
demonstrating consistent improvements in sample efficiency and solution
quality. ShinkaEvolve discovers a new state-of-the-art circle packing solution
using only 150 samples, designs high-performing agentic harnesses for AIME
mathematical reasoning tasks, identifies improvements to ALE-Bench competitive
programming solutions, and discovers novel mixture-of-expert load balancing
loss functions that illuminate the space of optimization strategies. Our
results demonstrate that ShinkaEvolve achieves broad applicability with
exceptional sample efficiency. By providing open-source accessibility and
cost-efficiency, this work democratizes open-ended discovery across diverse
computational problems.

</details>


### [46] [TriSPrompt: A Hierarchical Soft Prompt Model for Multimodal Rumor Detection with Incomplete Modalities](https://arxiv.org/abs/2509.19352)
*Jiajun Chen,Yangyang Wu,Xiaoye Miao,Mengying Zhu,Meng Xi*

Main category: cs.CL

TL;DR: The paper introduces TriSPrompt, a hierarchical soft prompt model addressing incomplete modalities in multimodal rumor detection, achieving over 13% accuracy improvement.


<details>
  <summary>Details</summary>
Motivation: Incomplete modalities in multimodal data hinder effective rumor detection, with most existing methods requiring complete data, making them impractical for real-world applications.

Method: The authors propose TriSPrompt, which utilizes three hierarchical prompts: MA prompt for modality information recovery, MM prompt for modeling missing data, and MV prompt for detecting relationships between perspectives.

Result: Extensive experiments on three benchmarks show TriSPrompt achieves over 13% accuracy improvement compared to existing state-of-the-art methods.

Conclusion: TriSPrompt effectively addresses incomplete modalities in multimodal rumor detection, demonstrating significant performance enhancements and adaptability.

Abstract: The widespread presence of incomplete modalities in multimodal data poses a
significant challenge to achieving accurate rumor detection. Existing
multimodal rumor detection methods primarily focus on learning joint modality
representations from \emph{complete} multimodal training data, rendering them
ineffective in addressing the common occurrence of \emph{missing modalities} in
real-world scenarios. In this paper, we propose a hierarchical soft prompt
model \textsf{TriSPrompt}, which integrates three types of prompts,
\textit{i.e.}, \emph{modality-aware} (MA) prompt, \emph{modality-missing} (MM)
prompt, and \emph{mutual-views} (MV) prompt, to effectively detect rumors in
incomplete multimodal data. The MA prompt captures both heterogeneous
information from specific modalities and homogeneous features from available
data, aiding in modality recovery. The MM prompt models missing states in
incomplete data, enhancing the model's adaptability to missing information. The
MV prompt learns relationships between subjective (\textit{i.e.}, text and
image) and objective (\textit{i.e.}, comments) perspectives, effectively
detecting rumors. Extensive experiments on three real-world benchmarks
demonstrate that \textsf{TriSPrompt} achieves an accuracy gain of over 13\%
compared to state-of-the-art methods. The codes and datasets are available at
https: //anonymous.4open.science/r/code-3E88.

</details>


### [47] [RoadMind: Towards a Geospatial AI Expert for Disaster Response](https://arxiv.org/abs/2509.19354)
*Ahmed El Fekih Zguir,Ferda Ofli,Muhammad Imran*

Main category: cs.CL

TL;DR: RoadMind improves the geospatial reasoning abilities of Large Language Models (LLMs) using OpenStreetMap data for tasks like evacuation planning and resource allocation during disaster scenarios.


<details>
  <summary>Details</summary>
Motivation: To address LLMs' limitations in reasoning about geospatial data, which restrict their utility in critical disaster scenarios requiring spatial understanding.

Method: Developed RoadMind, a self-supervised framework that extracts and converts OpenStreetMap road data into formats for training LLMs using QLoRA adapters and 4-bit quantized models.

Result: RoadMind-trained models outperformed state-of-the-art LLMs in geospatial tasks like road segment identification and distance estimation across disaster-prone cities.

Conclusion: Structured geospatial data can be effectively used to enhance LLMs' spatial reasoning capabilities, enabling improved offline AI applications for disaster response.

Abstract: Large Language Models (LLMs) have shown impressive performance across a range
of natural language tasks, but remain limited in their ability to reason about
geospatial data, particularly road networks, distances, and directions. This
gap poses challenges in disaster scenarios, where spatial understanding is
critical for tasks such as evacuation planning and resource allocation. In this
work, we present RoadMind, a self-supervised framework that enhances the
geospatial reasoning capabilities of LLMs using structured data from
OpenStreetMap (OSM). Our automated pipeline extracts road infrastructure data
for a given city and converts it into multiple supervision formats tailored to
key spatial tasks. We pretrain and fine-tune LLMs on these representations
using QLoRA adapters and 4-bit quantized models. We evaluate our approach on
three disaster-prone cities with varying global representation, Los Angeles,
Christchurch, and Manila, across tasks such as road segment identification,
nearest road retrieval, and distance/direction estimation. Our results show
that models trained via RoadMind significantly outperform strong baselines,
including state-of-the-art LLMs equipped with advanced prompt engineering. This
demonstrates the potential of structured geospatial data to enhance language
models with robust spatial reasoning, enabling more effective offline AI
systems for disaster response.

</details>


### [48] [Benchmarking and Improving LLM Robustness for Personalized Generation](https://arxiv.org/abs/2509.19358)
*Chimaobi Okite,Naihao Deng,Kiran Bodipati,Huaidian Hou,Joyce Chai,Rada Mihalcea*

Main category: cs.CL

TL;DR: The paper identifies the challenge of ensuring both factual accuracy and user preference alignment in personalized LLM responses, introducing PERG as an evaluation framework. Current LLMs face robustness issues, which the proposed Pref-Aligner method aims to address.


<details>
  <summary>Details</summary>
Motivation: The research is motivated by the gap in ensuring robust responses from LLMs that are both factually accurate and aligned with user preferences, which is crucial for reliable personalization.

Method: The study introduces PERG, a scalable framework for evaluating robustness of LLMs, and a new dataset called PERGData. It evaluates 14 models from 5 model families, using different prompting methods, and proposes Pref-Aligner, a two-stage approach to enhance robustness.

Result: Findings highlight that current models struggle with robust personalization. Even advanced models like GPT-4.1 fail in 5% of cases, while smaller ones fail over 20% of the time. Pref-Aligner improves robustness by 25% on average.

Conclusion: The paper identifies significant robustness gaps in personalized LLMs, especially concerning factuality and user preference alignment, and presents solutions to support more reliable LLM deployments.

Abstract: Recent years have witnessed a growing interest in personalizing the responses
of large language models (LLMs). While existing evaluations primarily focus on
whether a response aligns with a user's preferences, we argue that factuality
is an equally important yet often overlooked dimension. In the context of
personalization, we define a model as robust if its responses are both
factually accurate and align with the user preferences. To assess this, we
introduce PERG, a scalable framework for evaluating robustness in LLMs, along
with a new dataset, PERGData. We evaluate fourteen models from five different
model families using different prompting methods. Our findings show that
current LLMs struggle with robust personalization: even the strongest models
(GPT-4.1, LLaMA3-70B) fail to maintain correctness in 5% of previously
successful cases without personalization, while smaller models (e.g., 7B-scale)
can fail more than 20% of the time. Further analysis reveals that robustness is
significantly affected by the nature of the query and the type of user
preference. To mitigate these failures, we propose Pref-Aligner, a two-stage
approach that improves robustness by an average of 25% across models. Our work
highlights critical gaps in current evaluation practices and introduces tools
and metrics to support more reliable, user-aligned LLM deployments.

</details>


### [49] [Semantic Representation Attack against Aligned Large Language Models](https://arxiv.org/abs/2509.19360)
*Jiawei Lian,Jianhong Pan,Lefan Wang,Yi Wang,Shaohui Mei,Lap-Pui Chau*

Main category: cs.CL

TL;DR: The paper proposes a novel attack method to circumvent alignment techniques in large language models (LLMs), achieving high success rates while being more stealthy and efficient compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing adversarial techniques for aligned LLMs suffer from limitations like unnatural prompts, low success rates, and high computational costs. This paper aims to improve attack efficacy and prompt naturalness when eliciting harmful content from LLMs.

Method: The authors propose the Semantic Representation Attack paradigm, leveraging the semantic representation space to generate attacks. They introduce the Semantic Representation Heuristic Search algorithm for concise and interpretable prompt generation.

Result: The method achieves an average attack success rate of 89.41% across 18 LLMs, with perfect success on 11 models. It also maintains stealthiness and computational efficiency.

Conclusion: Semantic Representation Attack significantly improves adversarial success rates, naturalness, and efficiency, outperforming existing methods. The approach offers theoretical guarantees for semantic convergence and has comprehensive empirical support.

Abstract: Large Language Models (LLMs) increasingly employ alignment techniques to
prevent harmful outputs. Despite these safeguards, attackers can circumvent
them by crafting prompts that induce LLMs to generate harmful content.
  Current methods typically target exact affirmative responses, such as ``Sure,
here is...'', suffering from limited convergence, unnatural prompts, and high
computational costs.
  We introduce Semantic Representation Attack, a novel paradigm that
fundamentally reconceptualizes adversarial objectives against aligned LLMs.
  Rather than targeting exact textual patterns, our approach exploits the
semantic representation space comprising diverse responses with equivalent
harmful meanings.
  This innovation resolves the inherent trade-off between attack efficacy and
prompt naturalness that plagues existing methods.
  The Semantic Representation Heuristic Search algorithm is proposed to
efficiently generate semantically coherent and concise adversarial prompts by
maintaining interpretability during incremental expansion.
  We establish rigorous theoretical guarantees for semantic convergence and
demonstrate that our method achieves unprecedented attack success rates
(89.41\% averaged across 18 LLMs, including 100\% on 11 models) while
maintaining stealthiness and efficiency.
  Comprehensive experimental results confirm the overall superiority of our
Semantic Representation Attack.
  The code will be publicly available.

</details>


### [50] [The Inadequacy of Offline LLM Evaluations: A Need to Account for Personalization in Model Behavior](https://arxiv.org/abs/2509.19364)
*Angelina Wang,Daniel E. Ho,Sanmi Koyejo*

Main category: cs.CL

TL;DR: Standard offline evaluations fail to capture the behavioral variations of language models influenced by personalization.


<details>
  <summary>Details</summary>
Motivation: Language models often change behavior based on user context and personalization, which is not reflected in standard offline evaluations.

Method: Empirical comparison was conducted between traditional offline evaluations and field evaluations using 800 users interacting with ChatGPT and Gemini.

Result: Responses to identical benchmark questions varied significantly between state-less systems and personalized user sessions.

Conclusion: The study demonstrates the inadequacy of offline evaluations and highlights the impact of user personalization in assessing language models.

Abstract: Standard offline evaluations for language models -- a series of independent,
state-less inferences made by models -- fail to capture how language models
actually behave in practice, where personalization fundamentally alters model
behavior. For instance, identical benchmark questions to the same language
model can produce markedly different responses when prompted to a state-less
system, in one user's chat session, or in a different user's chat session. In
this work, we provide empirical evidence showcasing this phenomenon by
comparing offline evaluations to field evaluations conducted by having 800 real
users of ChatGPT and Gemini pose benchmark and other provided questions to
their chat interfaces.

</details>


### [51] [LLM-Assisted Topic Reduction for BERTopic on Social Media Data](https://arxiv.org/abs/2509.19365)
*Wannes Janssens,Matthias Bogaert,Dirk Van den Poel*

Main category: cs.CL

TL;DR: This paper introduces a two-stage framework for topic modeling by combining BERTopic and large language models, achieving improved topic diversity and coherence on social media datasets.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of noisy and sparse social media data, which BERTopic struggles with, and to overcome scalability limitations in large language model-based approaches.

Method: The proposed framework combines BERTopic for initial topic extraction and transformer-based language models for iterative topic refinement by merging semantically similar topics.

Result: Evaluation across three Twitter/X datasets and four language models demonstrates the framework improves topic diversity and often coherence, although results vary by dataset and parameter settings.

Conclusion: The framework offers a scalable solution that integrates BERTopic with large language models, outperforming baseline methods in tackling social media data for topic modeling.

Abstract: The BERTopic framework leverages transformer embeddings and hierarchical
clustering to extract latent topics from unstructured text corpora. While
effective, it often struggles with social media data, which tends to be noisy
and sparse, resulting in an excessive number of overlapping topics. Recent work
explored the use of large language models for end-to-end topic modelling.
However, these approaches typically require significant computational overhead,
limiting their scalability in big data contexts. In this work, we propose a
framework that combines BERTopic for topic generation with large language
models for topic reduction. The method first generates an initial set of topics
and constructs a representation for each. These representations are then
provided as input to the language model, which iteratively identifies and
merges semantically similar topics. We evaluate the approach across three
Twitter/X datasets and four different language models. Our method outperforms
the baseline approach in enhancing topic diversity and, in many cases,
coherence, with some sensitivity to dataset characteristics and initial
parameter selection.

</details>


### [52] [Pipeline Parallelism is All You Need for Optimized Early-Exit Based Self-Speculative Decoding](https://arxiv.org/abs/2509.19368)
*Ruanjun Li,Ziheng Liu,Yuanming Shi,Jiawei Shao,Chi Zhang,Xuelong Li*

Main category: cs.CL

TL;DR: This paper proposes Pipeline-Parallel Self-Speculative Decoding (PPSD) to accelerate large language model inference using a pipelined draft-verify structure.


<details>
  <summary>Details</summary>
Motivation: The high inference cost of large language models due to auto-regressive token generation motivates the need for faster decoding approaches.

Method: The authors propose PPSD, which interleaves token drafting and verification by overlapping early-exit and remaining-layer computations in a pipelined fashion.

Result: Empirical results show PPSD achieves a speedup ratio of 2.01x~3.81x while maintaining performance and optimizing efficiency at fixed acceptance rates.

Conclusion: PPSD demonstrates state-of-the-art acceleration in self-speculative decoding, proving its effectiveness in enhancing large language model inference efficiency.

Abstract: Large language models (LLMs) deliver impressive generation quality, but incur
very high inference cost because each output token is generated
auto-regressively through all model layers. Early-exit based self-speculative
decoding (EESD) has emerged to mitigate this cost. However, in practice, many
approaches struggle to achieve the expected acceleration in such
draft-then-verify paradigm even with a well-aligned early-exit head and
selected exit position. Our analysis reveals that EESD only pays off when the
vast majority of draft tokens are accepted by the LLM. Otherwise, the draft
cost may overcome the acceleration gain and lead to a negative speedup. To
mitigate this, we propose Pipeline-Parallel Self-Speculative Decoding (PPSD)
that fully pipelines the draft and verification work so that no effort is
wasted on failed predictions. It has two key innovations. We configure the
model layers as a pipeline in which early-exit (draft) computations and
remaining-layer (verification) computations overlap. We interleave drafting and
verification per token. While the LLM is verifying the current token in its
final layers, the early-exit path simultaneously drafts the next token. Such a
verify-while-draft scheme keeps all units busy and validates tokens on-the-fly
analogous to pipelining the speculation and verification stages. Empirical
results confirm that PPSD achieves state-of-the-art acceleration in
self-speculative LLM inference. On diverse benchmarks, PPSD achieves speedup
ratios in the range of 2.01x~3.81x, which gains almost the optimal acceleration
at the fixed acceptance rate and exit position, showcasing its advancement in
providing efficient self-speculation.

</details>


### [53] [GAUSS: Benchmarking Structured Mathematical Skills for Large Language Models](https://arxiv.org/abs/2509.18122)
*Yue Zhang,Jiaxin Zhang,Qiuyu Ren,Tahsin Saffat,Xiaoxuan Liu,Zitong Yang,Banghua Zhu,Yi Ma*

Main category: cs.CL

TL;DR: The paper introduces GAUSS, a benchmark for evaluating LLMs' mathematical abilities across twelve skills grouped into three domains.


<details>
  <summary>Details</summary>
Motivation: To provide a detailed and interpretable evaluation framework for understanding the mathematical capabilities of LLMs.

Method: Tasks are designed to isolate specific abilities, categorized by cognitive skills, creating skill profiles of models such as GPT-5-thinking.

Result: Skill profiles reveal strengths, weaknesses, and differences between models, demonstrating the utility of multidimensional evaluation.

Conclusion: GAUSS offers a fine-grained assessment framework that faithfully represents the mathematical intelligence of LLMs, aiding in comprehensive analysis.

Abstract: We introduce \textbf{GAUSS} (\textbf{G}eneral \textbf{A}ssessment of
\textbf{U}nderlying \textbf{S}tructured \textbf{S}kills in Mathematics), a
benchmark that evaluates LLMs' mathematical abilities across twelve core skill
dimensions, grouped into three domains: knowledge and understanding, problem
solving and communication, and meta-skills and creativity. By categorizing
problems according to cognitive skills and designing tasks that isolate
specific abilities, GAUSS constructs comprehensive, fine-grained, and
interpretable profiles of models' mathematical abilities. These profiles
faithfully represent their underlying mathematical intelligence. To exemplify
how to use the \textsc{GAUSS} benchmark, we have derived the skill profile of
\textsc{GPT-5-thinking}, revealing its strengths and weaknesses as well as its
differences relative to \textsc{o4-mini-high}, thereby underscoring the value
of multidimensional, skill-based evaluation.

</details>


### [54] [SLM-Based Agentic AI with P-C-G: Optimized for Korean Tool Use](https://arxiv.org/abs/2509.19369)
*Changhyun Jeon,Jinhee Park,Jungwoo Choi,Keonwoo Kim,Jisu Kim,Minji Hong*

Main category: cs.CL

TL;DR: This paper introduces a Planner-Caller-Generator architecture for small language models optimized for Korean tool use, reducing errors and improving efficiency in Korean queries.


<details>
  <summary>Details</summary>
Motivation: There is a need for efficient and specialized language model architectures to handle Korean-related tasks due to challenges like frequent code-switching between Korean and English.

Method: The introduced architecture separates tasks into three roles: planning, calling tools with validation, and generating answers. A Korean-first value policy is used to address code-switching issues.

Result: The proposed model demonstrates competitive accuracy, reduced token usage, and acceptable latency in various scenarios, assessed under a unified interface across multiple evaluation runs.

Conclusion: Role-specialized small-scale language models, like P-C-G, are effective alternatives for Korean tool-use tasks, combining efficiency and cost-effectiveness.

Abstract: We propose a small-scale language model (SLM) based agent architecture,
Planner-Caller-Generator (P-C-G), optimized for Korean tool use. P-C-G
separates planning, calling, and generation by role: the Planner produces an
initial batch plan with limited on-demand replanning; the Caller returns a
normalized call object after joint schema-value validation; and the Generator
integrates tool outputs to produce the final answer. We apply a Korean-first
value policy to reduce execution failures caused by frequent Korean-to-English
code switching in Korean settings. Evaluation assumes Korean queries and Korean
tool/parameter specifications; it covers single-chain, multi-chain,
missing-parameters, and missing-functions scenarios, and is conducted via an
LLM-as-a-Judge protocol averaged over five runs under a unified I/O interface.
Results show that P-C-G delivers competitive tool-use accuracy and end-to-end
quality while reducing tokens and maintaining acceptable latency, indicating
that role-specialized SLMs are a cost-effective alternative for Korean tool-use
agents.

</details>


### [55] [Meow: End-to-End Outline Writing for Automatic Academic Survey](https://arxiv.org/abs/2509.19370)
*Zhaoyu Ma,Yuan Shan,Jiahao Zhao,Nan Xu,Lei Wang*

Main category: cs.CL

TL;DR: This paper proposes Meow, a metadata-driven framework for generating structured outlines to improve automatic survey writing.


<details>
  <summary>Details</summary>
Motivation: To address limitations in current template-based automatic survey methods, which lack depth and stylistic coherence in outline writing.

Method: The authors created Meow, a framework that formulates outline writing as an end-to-end task using hierarchical generation from paper metadata, supported by a curated dataset and two-stage training (supervised fine-tuning and reinforcement learning).

Result: The proposed 8B reasoning model achieved high performance with structural fidelity and stylistic coherence in generated outlines.

Conclusion: The metadata-driven approach of Meow enables efficient, organized, and faithful outline writing, paving the way for enhanced automated survey generation.

Abstract: As academic paper publication numbers grow exponentially, conducting in-depth
surveys with LLMs automatically has become an inevitable trend. Outline
writing, which aims to systematically organize related works, is critical for
automated survey generation. Yet existing automatic survey methods treat
outline writing as mere workflow steps in the overall pipeline. Such
template-based workflows produce outlines that lack in-depth understanding of
the survey topic and fine-grained styles. To address these limitations, we
propose Meow, the first metadata-driven outline writing framework that produces
organized and faithful outlines efficiently. Specifically, we first formulate
outline writing as an end-to-end task that generates hierarchical structured
outlines from paper metadata. We then curate a high-quality dataset of surveys
from arXiv, bioRxiv, and medRxiv, and establish systematic evaluation metrics
for outline quality assessment. Finally, we employ a two-stage training
approach combining supervised fine-tuning and reinforcement learning. Our 8B
reasoning model demonstrates strong performance with high structural fidelity
and stylistic coherence.

</details>


### [56] [How to inject knowledge efficiently? Knowledge Infusion Scaling Law for Pre-training Large Language Models](https://arxiv.org/abs/2509.19371)
*Kangtao Lv,Haibin Chen,Yujin Yuan,Langming Liu,Shilei Liu,Yongwei Wang,Wenbo Su,Bo Zheng*

Main category: cs.CL

TL;DR: This paper investigates the trade-off between domain knowledge infusion and general knowledge retention in large language models (LLMs). It introduces a scaling law to optimize the process and verifies its effectiveness across various settings.


<details>
  <summary>Details</summary>
Motivation: LLMs perform well on general tasks but struggle with domain-specific ones, especially without optimization. Balancing domain-specific knowledge infusion without causing memory collapse is a challenge.

Method: The authors conducted systematic experiments to identify thresholds of knowledge retention and scaled these observations across model sizes. From these insights, they developed a scaling law to optimize domain knowledge infusion.

Result: The team observed key patterns in memory collapse and established a scaling law correlating knowledge infusion thresholds to model sizes. Experimental validations demonstrated the law’s effectiveness and its broader applicability.

Conclusion: The scaling law helps optimize domain knowledge infusion in LLMs while maintaining general knowledge retention capabilities. It works effectively across different model sizes and token budgets.

Abstract: Large language models (LLMs) have attracted significant attention due to
their impressive general capabilities across diverse downstream tasks. However,
without domain-specific optimization, they often underperform on specialized
knowledge benchmarks and even produce hallucination. Recent studies show that
strategically infusing domain knowledge during pretraining can substantially
improve downstream performance. A critical challenge lies in balancing this
infusion trade-off: injecting too little domain-specific data yields
insufficient specialization, whereas excessive infusion triggers catastrophic
forgetting of previously acquired knowledge. In this work, we focus on the
phenomenon of memory collapse induced by over-infusion. Through systematic
experiments, we make two key observations, i.e. 1) Critical collapse point:
each model exhibits a threshold beyond which its knowledge retention
capabilities sharply degrade. 2) Scale correlation: these collapse points scale
consistently with the model's size. Building on these insights, we propose a
knowledge infusion scaling law that predicts the optimal amount of domain
knowledge to inject into large LLMs by analyzing their smaller counterparts.
Extensive experiments across different model sizes and pertaining token budgets
validate both the effectiveness and generalizability of our scaling law.

</details>


### [57] [A Pipeline to Assess Merging Methods via Behavior and Internals](https://arxiv.org/abs/2509.19476)
*Yutaro Sigris,Andreas Waldis*

Main category: cs.CL

TL;DR: The paper introduces a novel evaluation pipeline to analyze both behavioral and internal aspects of merged language models (LMs), with findings that merging can enhance certain internal competencies beyond those of parent models.


<details>
  <summary>Details</summary>
Motivation: Existing studies of model merging only focus on behavioral output, lacking insights into internal mechanisms and broader evaluations.

Method: The paper merges language models and evaluates them across two dimensions: performance in downstream tasks and internal linguistic competence.

Result: Merged models often achieve intermediate performance but can outperform parent models in linguistic competence, with weak correlation between behavior and internal evaluation.

Conclusion: The findings highlight the necessity for evaluating model merging methods comprehensively to fully understand their capabilities, especially beyond surface-level behavioral improvements.

Abstract: Merging methods combine the weights of multiple language models (LMs) to
leverage their capacities, such as for domain adaptation. While existing
studies investigate merged models from a solely behavioral perspective, we
offer the first comprehensive view by assessing and connecting their behavior
and internals. We present a novel evaluation pipeline that first merges
multiple parent LMs, and then evaluates the merged models in comparison to the
initial ones based on their behavior on downstream tasks, like MMLU, and the
internal encoded linguistic competence. We showcase this pipeline by assessing
the merging of instruction fine-tuned with math- and code-adapted LMs from the
Qwen2.5 family. Our results show that merging methods impacts behavior and
internals differently. While the performance of merged models is typically
between that of the two parent models, their encoded information about
linguistic phenomena, particularly in morphology and syntax, can surpass the
parent models. Moreover, we find weak ranking correlation between this behavior
and internal evaluation. With our pipeline and initial results, we emphasize
the need for more comprehensive evaluations of model merging methods to gain a
faithful understanding of their capabilities and reliability, beyond potential
superficial behavioral advances.

</details>


### [58] [Do LLMs Encode Frame Semantics? Evidence from Frame Identification](https://arxiv.org/abs/2509.19540)
*Jayanth Krishna Chundru,Rudrashis Poddar,Jie Cao,Tianyu Jiang*

Main category: cs.CL

TL;DR: The paper examines whether large language models inherently understand frame semantics and shows they can effectively perform frame identification tasks without specific training, with further fine-tuning boosting their performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to investigate whether large language models have an inherent understanding of frame semantics, an essential aspect of natural language understanding.

Method: The study uses FrameNet for evaluation through prompt-based inference, and fine-tunes models on FrameNet data to test the effects of task-specific training.

Result: The models can perform frame identification effectively without explicit supervision, and fine-tuning significantly improves in-domain and out-of-domain performance. Additionally, the models can generate coherent frame definitions.

Conclusion: Large language models encode latent frame-semantic knowledge that can be harnessed for frame identification tasks, with fine-tuning further enhancing capabilities and demonstrating an understanding of frame semantics.

Abstract: We investigate whether large language models encode latent knowledge of frame
semantics, focusing on frame identification, a core challenge in frame semantic
parsing that involves selecting the appropriate semantic frame for a target
word in context. Using the FrameNet lexical resource, we evaluate models under
prompt-based inference and observe that they can perform frame identification
effectively even without explicit supervision. To assess the impact of
task-specific training, we fine-tune the model on FrameNet data, which
substantially improves in-domain accuracy while generalizing well to
out-of-domain benchmarks. Further analysis shows that the models can generate
semantically coherent frame definitions, highlighting the model's internalized
understanding of frame semantics.

</details>


### [59] [Confidence Calibration in Large Language Model-Based Entity Matching](https://arxiv.org/abs/2509.19557)
*Iris Kamsteeg,Juan Cardenas-Cartagena,Floris van Beers,Gineke ten Holt,Tsegaye Misikir Tashu,Matias Valdenegro-Toro*

Main category: cs.CL

TL;DR: The study investigates confidence calibration methods for RoBERTa in Entity Matching tasks using datasets like Abt-Buy, DBLP-ACM, and others, suggesting Temperature Scaling as an effective mitigation.


<details>
  <summary>Details</summary>
Motivation: Exploring confidence calibration in Large Language Models for accurate Entity Matching predictions.

Method: Empirical comparison of RoBERTa baseline confidence to adjusted calibration methods: Temperature Scaling, Monte Carlo Dropout, and Ensembles across multiple datasets.

Result: The modified RoBERTa model showed slight overconfidence, with Expected Calibration Error (ECE) scores between 0.0043 and 0.0552. Temperature Scaling significantly reduced ECE by up to 23.83%.

Conclusion: Temperature Scaling proves effective at improving confidence calibration in Entity Matching tasks involving RoBERTa, offering a feasible solution to overconfidence issues.

Abstract: This research aims to explore the intersection of Large Language Models and
confidence calibration in Entity Matching. To this end, we perform an empirical
study to compare baseline RoBERTa confidences for an Entity Matching task
against confidences that are calibrated using Temperature Scaling, Monte Carlo
Dropout and Ensembles. We use the Abt-Buy, DBLP-ACM, iTunes-Amazon and Company
datasets. The findings indicate that the proposed modified RoBERTa model
exhibits a slight overconfidence, with Expected Calibration Error scores
ranging from 0.0043 to 0.0552 across datasets. We find that this overconfidence
can be mitigated using Temperature Scaling, reducing Expected Calibration Error
scores by up to 23.83%.

</details>


### [60] [Uncertainty in Semantic Language Modeling with PIXELS](https://arxiv.org/abs/2509.19563)
*Stefania Radu,Marco Zullich,Matias Valdenegro-Toro*

Main category: cs.CL

TL;DR: The paper analyzes uncertainty and confidence in pixel-based language models across multiple languages and scripts using methods like Monte Carlo Dropout, Transformer Attention, and Ensemble Learning.


<details>
  <summary>Details</summary>
Motivation: The goal is to address the challenge of uncertainty quantification in pixel-based language models, which aim to overcome the vocabulary bottleneck problem in language modeling.

Method: The study uses approaches such as Monte Carlo Dropout, Transformer Attention, and Ensemble Learning to analyze uncertainty in semantically demanding tasks across 18 languages and 7 scripts.

Result: Pixel-based models underestimate uncertainty during patch reconstruction, while the script influences uncertainty level—Latin languages show lower uncertainty. Ensemble learning with hyperparameter tuning improves performance in named entity recognition and question-answering tasks across 16 languages.

Conclusion: The findings underscore the importance of proper uncertainty quantification in pixel-based language models and demonstrate the script-specific variations in uncertainty, paving the way for improved language modeling techniques.

Abstract: Pixel-based language models aim to solve the vocabulary bottleneck problem in
language modeling, but the challenge of uncertainty quantification remains
open. The novelty of this work consists of analysing uncertainty and confidence
in pixel-based language models across 18 languages and 7 scripts, all part of 3
semantically challenging tasks. This is achieved through several methods such
as Monte Carlo Dropout, Transformer Attention, and Ensemble Learning. The
results suggest that pixel-based models underestimate uncertainty when
reconstructing patches. The uncertainty is also influenced by the script, with
Latin languages displaying lower uncertainty. The findings on ensemble learning
show better performance when applying hyperparameter tuning during the named
entity recognition and question-answering tasks across 16 languages.

</details>


### [61] [Retrieval Augmented Generation based context discovery for ASR](https://arxiv.org/abs/2509.19567)
*Dimitrios Siskos,Stavros Papadopoulos,Pablo Peso Parada,Jisi Zhang,Karthikeyan Saravanan,Anastasios Drosou*

Main category: cs.CL

TL;DR: The paper explores using retrieval augmented generation for automatic context discovery in ASR to address rare/out-of-vocabulary terms, achieving significant WER reductions.


<details>
  <summary>Details</summary>
Motivation: To improve transcription accuracy in ASR systems, particularly in scenarios involving rare or out-of-vocabulary terms, by automating the context identification process.

Method: The paper proposes an efficient embedding-based retrieval approach for automatic context discovery, comparing it with two LLM-based alternatives: context generation via prompting and post-recognition transcript correction.

Result: The proposed approach achieved up to a 17% relative reduction in WER, and up to 24.1% when using oracle context, across experiments on datasets like TED-LIUMv3, Earnings21, and SPGISpeech.

Conclusion: Automatic context discovery via embedding-based retrieval is an effective way to enhance ASR performance, comparing favorably with LLM-based alternatives and demonstrating substantial accuracy improvements.

Abstract: This work investigates retrieval augmented generation as an efficient
strategy for automatic context discovery in context-aware Automatic Speech
Recognition (ASR) system, in order to improve transcription accuracy in the
presence of rare or out-of-vocabulary terms. However, identifying the right
context automatically remains an open challenge. This work proposes an
efficient embedding-based retrieval approach for automatic context discovery in
ASR. To contextualize its effectiveness, two alternatives based on large
language models (LLMs) are also evaluated: (1) large language model (LLM)-based
context generation via prompting, and (2) post-recognition transcript
correction using LLMs. Experiments on the TED-LIUMv3, Earnings21 and SPGISpeech
demonstrate that the proposed approach reduces WER by up to 17% (percentage
difference) relative to using no-context, while the oracle context results in a
reduction of up to 24.1%.

</details>


### [62] [ExPe: Exact Positional Encodings for Generative Transformer Models with Extrapolating Capabilities](https://arxiv.org/abs/2509.19569)
*Aleksis Datseris,Sylvia Vassileva,Ivan Koychev,Svetla Boytcheva*

Main category: cs.CL

TL;DR: The paper introduces Exact Positional Embeddings (ExPE), a novel method for handling positional embeddings in transformer models to better extrapolate to longer sequences.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing absolute and relative position embeddings in transformer models, which struggle to generalize to sequence lengths beyond what they were trained on.

Method: A new embedding strategy called ExPE that encodes exact positional information by overriding specific dimensions of the embedding vectors, enabling better position representation and generalization to longer sequences.

Result: ExPE significantly reduces perplexity in causal language modeling tasks on sequences longer than those seen during training, outperforming rotary and sinusoidal embeddings.

Conclusion: ExPE improves the ability of transformer models to generalize to longer sequences while maintaining embedding integrity, demonstrating its effectiveness over traditional methods.

Abstract: This paper introduces a novel approach to position embeddings in transformer
models, named "Exact Positional Embeddings" (ExPE). An absolute positional
embedding method that can extrapolate to sequences of lengths longer than the
ones it was trained on. Traditional transformer models rely on absolute or
relative position embeddings to incorporate positional information into token
embeddings, which often struggle with extrapolation to sequences longer than
those seen during training. Our proposed method utilizes a novel embedding
strategy that encodes exact positional information by overriding specific
dimensions of the embedding vectors, thereby enabling a more precise
representation of token positions. The proposed approach not only maintains the
integrity of the original embeddings but also enhances the model's ability to
generalize to more extended sequences. In causal language modeling, our ExPE
embeddings significantly reduce perplexity compared to rotary and sinusoidal
embeddings, when tested on sequences longer than those used in training.

</details>


### [63] [LLMs4All: A Review on Large Language Models for Research and Applications in Academic Disciplines](https://arxiv.org/abs/2509.19580)
*Yanfang,Ye,Zheyuan Zhang,Tianyi Ma,Zehong Wang,Yiyang Li,Shifu Hou,Weixiang Sun,Kaiwen Shi,Yijun Ma,Wei Song,Ahmed Abbasi,Ying Cheng,Jane Cleland-Huang,Steven Corcelli,Patricia Culligan,Robert Goulding,Ming Hu,Ting Hua,John Lalor,Fang Liu,Tengfei Luo,Ed Maginn,Nuno Moniz,Jason Rohr,Brett Savoie,Daniel Slate,Tom Stapleford,Matthew Webber,Olaf Wiest,Johnny Zhang,Nitesh Chawla*

Main category: cs.CL

TL;DR: This paper reviews the applications of Large Language Models (LLMs) across academic disciplines, their successes, limitations, challenges, and future directions.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs are influencing diverse academic fields and to help researchers integrate generative AI into real-world applications.

Method: The paper offers an overview and analysis of how LLMs function across arts, economics, sciences, and engineering, along with examining their limitations and future developments.

Result: LLMs are found to be impactful across various disciplines, demonstrating transformative potential while having inherent challenges.

Conclusion: LLMs have broad applicability across multiple fields, promising significant advancements yet requiring further work to overcome limitations and address challenges.

Abstract: Cutting-edge Artificial Intelligence (AI) techniques keep reshaping our view
of the world. For example, Large Language Models (LLMs) based applications such
as ChatGPT have shown the capability of generating human-like conversation on
extensive topics. Due to the impressive performance on a variety of
language-related tasks (e.g., open-domain question answering, translation, and
document summarization), one can envision the far-reaching impacts that can be
brought by the LLMs with broader real-world applications (e.g., customer
service, education and accessibility, and scientific discovery). Inspired by
their success, this paper will offer an overview of state-of-the-art LLMs and
their integration into a wide range of academic disciplines, including: (1)
arts, letters, and law (e.g., history, philosophy, political science, arts and
architecture, law), (2) economics and business (e.g., finance, economics,
accounting, marketing), and (3) science and engineering (e.g., mathematics,
physics and mechanical engineering, chemistry and chemical engineering, life
sciences and bioengineering, earth sciences and civil engineering, computer
science and electrical engineering). Integrating humanity and technology, in
this paper, we will explore how LLMs are shaping research and practice in these
fields, while also discussing key limitations, open challenges, and future
directions in the era of generative AI. The review of how LLMs are engaged
across disciplines-along with key observations and insights-can help
researchers and practitioners interested in exploiting LLMs to advance their
works in diverse real-world applications.

</details>


### [64] [GuessingGame: Measuring the Informativeness of Open-Ended Questions in Large Language Models](https://arxiv.org/abs/2509.19593)
*Dylan Hutson,Daniel Vennemeyer,Aneesh Deshmukh,Justin Zhan,Tianyu Jiang*

Main category: cs.CL

TL;DR: The paper introduces GuessingGame, a protocol for assessing large language models (LLMs) in open-ended strategic question-asking. It proposes two information gain metrics and finds that question quality predicts game efficiency, showing that LLMs' question-asking can be measured and improved.


<details>
  <summary>Details</summary>
Motivation: The motivation is to evaluate and improve the strategic question-asking ability of large language models, particularly in open-domain and interactive reasoning scenarios.

Method: The method involves a GuessingGame protocol where a Guesser LLM identifies hidden objects by asking free-form questions to an Oracle. Two model-agnostic information gain metrics—Bayesian belief tracking and entropy-based filtering—measure question quality.

Result: In experiments across 858 games using various models and prompting strategies, higher information gain correlates with reduced game length, and guided constraints significantly improve weaker models' performance.

Conclusion: The study concludes that LLMs' question-asking ability is measurable, improvable, and central to interactive reasoning tasks.

Abstract: We introduce GuessingGame, a protocol for evaluating large language models
(LLMs) as strategic question-askers in open-ended, open-domain settings. A
Guesser LLM identifies a hidden object by posing free-form questions to an
Oracle without predefined choices or candidate lists. To measure question
quality, we propose two information gain (IG) metrics: a Bayesian method that
tracks belief updates over semantic concepts using LLM-scored relevance, and an
entropy-based method that filters candidates via ConceptNet. Both metrics are
model-agnostic and support post hoc analysis. Across 858 games with multiple
models and prompting strategies, higher IG strongly predicts efficiency: a
one-standard-deviation IG increase reduces expected game length by 43\%.
Prompting constraints guided by IG, such as enforcing question diversity,
enable weaker models to significantly improve performance. These results show
that question-asking in LLMs is both measurable and improvable, and crucial for
interactive reasoning.

</details>


### [65] [Anatomy of a Feeling: Narrating Embodied Emotions via Large Vision-Language Models](https://arxiv.org/abs/2509.19595)
*Mohammad Saim,Phan Anh Duong,Cat Luong,Aniket Bhanderi,Tianyu Jiang*

Main category: cs.CL

TL;DR: The paper introduces ELENA, a framework using advanced vision-language models to analyze embodied emotional reactions by focusing on body parts involved in emotions.


<details>
  <summary>Details</summary>
Motivation: To improve the understanding and modeling of affective experiences by focusing on bodily emotional responses and overcoming existing limitations focused on facial bias.

Method: Employs large vision-language models (LVLMs) to generate multi-layered emotion narratives, incorporating attention maps to detect emotional expressions, particularly in face-masked images.

Result: The proposed framework can effectively recognize embodied emotions across images, even outperforming baseline methods, without requiring additional fine-tuning.

Conclusion: ELENA broadens the scope of embodied emotion analysis, paving the way for better affect-aware systems using vision-language models.

Abstract: The embodiment of emotional reactions from body parts contains rich
information about our affective experiences. We propose a framework that
utilizes state-of-the-art large vision-language models (LVLMs) to generate
Embodied LVLM Emotion Narratives (ELENA). These are well-defined, multi-layered
text outputs, primarily comprising descriptions that focus on the salient body
parts involved in emotional reactions. We also employ attention maps and
observe that contemporary models exhibit a persistent bias towards the facial
region. Despite this limitation, we observe that our employed framework can
effectively recognize embodied emotions in face-masked images, outperforming
baselines without any fine-tuning. ELENA opens a new trajectory for embodied
emotion analysis across the modality of vision and enriches modeling in an
affect-aware setting.

</details>


### [66] [Evaluating Language Translation Models by Playing Telephone](https://arxiv.org/abs/2509.19611)
*Syeda Jannatus Saba,Steven Skiena*

Main category: cs.CL

TL;DR: The paper introduces a new unsupervised method to generate training data for evaluating machine translation systems, demonstrating improvements over existing evaluation methods.


<details>
  <summary>Details</summary>
Motivation: Current methods for evaluating machine translation systems struggle to match the growing effectiveness of modern language models, limiting progress in challenging translation tasks.

Method: The authors use unsupervised generation of training data through repeated rounds of translation between source and target languages and assess its effectiveness against other approaches.

Result: The proposed method improves over a popular translation evaluation system (xCOMET) on tasks involving translation quality scoring and translation selection.

Conclusion: This method enables better and more reliable evaluation of machine translation systems, potentially enhancing development for complex tasks.

Abstract: Our ability to efficiently and accurately evaluate the quality of machine
translation systems has been outrun by the effectiveness of current language
models--which limits the potential for further improving these models on more
challenging tasks like long-form and literary translation. We propose an
unsupervised method to generate training data for translation evaluation over
different document lengths and application domains by repeated rounds of
translation between source and target languages. We evaluate evaluation systems
trained on texts mechanically generated using both model rotation and language
translation approaches, demonstrating improved performance over a popular
translation evaluation system (xCOMET) on two different tasks: (i) scoring the
quality of a given translation against a human reference and (ii) selecting
which of two translations is generationally closer to an original source
document.

</details>


### [67] [AutoSpec: An Agentic Framework for Automatically Drafting Patent Specification](https://arxiv.org/abs/2509.19640)
*Ryan Shea,Zhou Yu*

Main category: cs.CL

TL;DR: AutoSpec is a framework that automates patent drafting by decomposing the task into smaller subtasks, leveraging open-source language models and specialized tools.


<details>
  <summary>Details</summary>
Motivation: Drafting patent applications is an expensive and time-consuming process, motivating the need for automation.

Method: AutoSpec decomposes the drafting process into subtasks handled by open-source language models enhanced with custom tools specific to patent drafting.

Result: AutoSpec demonstrates superior performance compared to baselines, validated through expert and automatic evaluations involving patent attorneys.

Conclusion: The proposed AutoSpec framework addresses confidentiality concerns and technical challenges, offering a robust solution for automated patent drafting.

Abstract: Patents play a critical role in driving technological innovation by granting
inventors exclusive rights to their inventions. However the process of drafting
a patent application is often expensive and time-consuming, making it a prime
candidate for automation. Despite recent advancements in language models,
several challenges hinder the development of robust automated patent drafting
systems. First, the information within a patent application is highly
confidential, which often prevents the use of closed-source LLMs for automating
this task. Second, the process of drafting a patent application is difficult
for even the most advanced language models due to their long context, technical
writing style, and specialized domain knowledge. To address these challenges,
we introduce AutoSpec, a secure, agentic framework for Automatically drafting
patent Specification. Our approach decomposes the drafting process into a
sequence of manageable subtasks, each solvable by smaller, open-source language
models enhanced with custom tools tailored for drafting patent specification.
To assess our system, we design a novel evaluation protocol in collaboration
with experienced patent attorneys. Our automatic and expert evaluations show
that AutoSpec outperforms existing baselines on a patent drafting task.

</details>


### [68] [Large Language Models for Pedestrian Safety: An Application to Predicting Driver Yielding Behavior at Unsignalized Intersections](https://arxiv.org/abs/2509.19657)
*Yicheng Yang,Zixian Li,Jean Paul Bizimana,Niaz Zafri,Yongfeng Dong,Tianyi Li*

Main category: cs.CL

TL;DR: This paper explores leveraging large language models (LLMs), specifically GPT-4o, to model driver-yielding behavior at crosswalks, demonstrating their potential superiority over traditional methods.


<details>
  <summary>Details</summary>
Motivation: The paper aims to enhance pedestrian safety by accurately modeling driver-pedestrian interactions, a task challenging for traditional machine learning methods due to the need for nuanced and context-dependent reasoning.

Method: The authors use multimodal LLMs with a novel prompt design that embeds domain-specific knowledge, structured reasoning, and few-shot prompting to analyze and model driver-yielding behavior.

Result: The research finds that GPT-4o outperforms traditional classifiers in accuracy and recall, while Deepseek-V3 excels in precision, highlighting trade-offs in performance and computational efficiency.

Conclusion: LLMs, with the proposed methods, hold significant promise for deploying high-performing, interpretable models in pedestrian safety systems, despite the trade-offs between accuracy, recall, precision, and computational cost.

Abstract: Pedestrian safety is a critical component of urban mobility and is strongly
influenced by the interactions between pedestrian decision-making and driver
yielding behavior at crosswalks. Modeling driver--pedestrian interactions at
intersections requires accurately capturing the complexity of these behaviors.
Traditional machine learning models often struggle to capture the nuanced and
context-dependent reasoning required for these multifactorial interactions, due
to their reliance on fixed feature representations and limited
interpretability. In contrast, large language models (LLMs) are suited for
extracting patterns from heterogeneous traffic data, enabling accurate modeling
of driver-pedestrian interactions. Therefore, this paper leverages multimodal
LLMs through a novel prompt design that incorporates domain-specific knowledge,
structured reasoning, and few-shot prompting, enabling interpretable and
context-aware inference of driver yielding behavior, as an example application
of modeling pedestrian--driver interaction. We benchmarked state-of-the-art
LLMs against traditional classifiers, finding that GPT-4o consistently achieves
the highest accuracy and recall, while Deepseek-V3 excels in precision. These
findings highlight the critical trade-offs between model performance and
computational efficiency, offering practical guidance for deploying LLMs in
real-world pedestrian safety systems.

</details>


### [69] [DyBBT: Dynamic Balance via Bandit inspired Targeting for Dialog Policy with Cognitive Dual-Systems](https://arxiv.org/abs/2509.19695)
*Shuyu Zhang,Yifan Wei,Jialuo Yuan,Xinru Wang,Yanmin Zhu,Bin Li*

Main category: cs.CL

TL;DR: DyBBT is a framework for task-oriented dialogue systems that uses dynamic exploration strategies based on cognitive states, yielding state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Task-oriented dialogue systems often struggle with static exploration methods that fail to adapt to dynamic contexts, leading to inefficient performance.

Method: DyBBT utilizes a bandit-inspired meta-controller to switch between an intuitive inference and deliberative reasoning based on a structured cognitive state space.

Result: Extensive experiments on various benchmarks showed DyBBT significantly improved success rate, efficiency, and generalization in dialogue systems, confirmed by human evaluations.

Conclusion: DyBBT presents an adaptive, efficient framework that outperforms existing dialogue systems by leveraging cognitive-state-driven exploration strategies.

Abstract: Task oriented dialog systems often rely on static exploration strategies that
do not adapt to dynamic dialog contexts, leading to inefficient exploration and
suboptimal performance. We propose DyBBT, a novel dialog policy learning
framework that formalizes the exploration challenge through a structured
cognitive state space capturing dialog progression, user uncertainty, and slot
dependency. DyBBT proposes a bandit inspired meta-controller that dynamically
switches between a fast intuitive inference (System 1) and a slow deliberative
reasoner (System 2) based on real-time cognitive states and visitation counts.
Extensive experiments on single- and multi-domain benchmarks show that DyBBT
achieves state-of-the-art performance in success rate, efficiency, and
generalization, with human evaluations confirming its decisions are well
aligned with expert judgment. Code is available at
https://github.com/carsonz/DyBBT.

</details>


### [70] [Personality Vector: Modulating Personality of Large Language Models by Model Merging](https://arxiv.org/abs/2509.19727)
*Seungjong Sun,Seo Yeon Baek,Jang Hyun Kim*

Main category: cs.CL

TL;DR: This paper introduces a novel method to fine-tune personality traits in large language models by leveraging a 'model merging' technique, achieving continuous control and multidimensional trait composition.


<details>
  <summary>Details</summary>
Motivation: Personalized AI systems require language models to align with complex human traits such as personality, which existing methods struggle to capture effectively due to the continuous and multidimensional nature of traits.

Method: The authors propose constructing 'personality vectors' by comparing the weights of pre-trained and fine-tuned models on specific personality traits. These vectors are merged into LLMs to induce desired personalities without further training.

Result: Experiments demonstrate that personality vectors allow fine control over the intensity of traits, support the combination of traits, and can generalize across various downstream models.

Conclusion: The proposed model merging method offers an effective approach to modulate and control personality traits in language models, paving the way for more personalized AI systems.

Abstract: Driven by the demand for personalized AI systems, there is growing interest
in aligning the behavior of large language models (LLMs) with human traits such
as personality. Previous attempts to induce personality in LLMs have shown
promising results, but they struggle to capture the continuous and
multidimensional nature of human traits. In this work, we propose a novel
method for personality modulation in LLMs via model merging. Specifically, we
construct personality vectors by subtracting the weights of a pre-trained model
from those of the fine-tuned model on a given personality trait. By merging
personality vectors, we enable LLMs to exhibit desired personality traits
without additional training. Extensive experiments show that personality
vectors enable continuous control over trait intensity and support the
composition of multiple traits. Furthermore, personality vectors transfer
across diverse downstream models, suggesting that they encode generalizable
representations of personality. Our code is available at here.

</details>


### [71] [HiCoLoRA: Addressing Context-Prompt Misalignment via Hierarchical Collaborative LoRA for Zero-Shot DST](https://arxiv.org/abs/2509.19742)
*Shuyu Zhang,Yifan Wei,Xinru Wang,Yanmin Zhu,Yangfan He,Yixuan Weng,Bin Li*

Main category: cs.CL

TL;DR: The paper presents HiCoLoRA, a model designed to improve zero-shot dialogue state tracking by addressing key alignment issues. HiCoLoRA achieves state-of-the-art results on multi-domain datasets MultiWOZ and SGD.


<details>
  <summary>Details</summary>
Motivation: Zero-shot dialogue state tracking is crucial for making task-oriented dialogue systems adaptable to new domains without extensive data labeling. Current systems face challenges like semantic misalignment, rigid system coordination, domain interference, and forgetfulness.

Method: The paper introduces the HiCoLoRA framework, which uses a hierarchical LoRA architecture for layer-specific processing, spectral joint domain-slot clustering for transferable relationship identification, and semantic-enhanced SVD initialization for preserving pre-trained knowledge.

Result: HiCoLoRA demonstrated superior performance compared to baseline models, setting state-of-the-art benchmarks on the MultiWOZ and SGD datasets.

Conclusion: HiCoLoRA addresses semantic misalignment and rigidity issues in zero-shot dialogue state tracking, offering a robust method for dynamic prompt alignment and multi-domain generalization.

Abstract: Zero-shot Dialog State Tracking (zs-DST) is essential for enabling
Task-Oriented Dialog Systems (TODs) to generalize to new domains without costly
data annotation. A central challenge lies in the semantic misalignment between
dynamic dialog contexts and static prompts, leading to inflexible cross-layer
coordination, domain interference, and catastrophic forgetting. To tackle this,
we propose Hierarchical Collaborative Low-Rank Adaptation (HiCoLoRA), a
framework that enhances zero-shot slot inference through robust prompt
alignment. It features a hierarchical LoRA architecture for dynamic
layer-specific processing (combining lower-layer heuristic grouping and
higher-layer full interaction), integrates Spectral Joint Domain-Slot
Clustering to identify transferable associations (feeding an Adaptive Linear
Fusion Mechanism), and employs Semantic-Enhanced SVD Initialization
(SemSVD-Init) to preserve pre-trained knowledge. Experiments on multi-domain
datasets MultiWOZ and SGD show that HiCoLoRA outperforms baselines, achieving
SOTA in zs-DST. Code is available at https://github.com/carsonz/HiCoLoRA.

</details>


### [72] [PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs](https://arxiv.org/abs/2509.19745)
*Pei Zhang,Andong Chen,Xi Chen,Baosong Yang,Derek F. Wong,Fei Huang*

Main category: cs.CL

TL;DR: This paper introduces Progressive Alignment Representation Training (PART), a framework to improve multilingual speech and text alignment in speech large models (SLMs), outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Aligning speech and text representations in multilingual settings is challenging, and current methods forcing cross-language convergence limit model performance.

Method: The authors propose PART, a multi-stage framework that separates language-specific and cross-language alignment. PART dynamically activates LLM parameters during cross-language training and incorporates text tasks to enhance multilingual understanding.

Result: Experiments on datasets like CommonVoice 15 and CoVoST2 show PART surpasses traditional methods, achieving better balance between language-specific details and cross-language generalization.

Conclusion: PART offers an effective approach to multilingual speech and text modality alignment, demonstrating improved performance and applicability across languages.

Abstract: Large language models (LLMs) have expanded from text to speech, giving rise
to Speech Large Models (SLMs) that support recognition, translation, and
synthesis. A key challenge is aligning speech and text representations, which
becomes harder in multilingual settings. Existing methods often freeze LLM
parameters and train encoders on multilingual data, but this forces
cross-language convergence and limits performance. We introduce Progressive
Alignment Representation Training (PART), a multi-stage and multi-task
framework that separates within-language from cross-language alignment. During
cross-language training, LLM parameters are dynamically activated, and
text-based tasks are later introduced to enhance multilingual understanding.
Experiments on CommonVoice 15, Fleurs, Wenetspeech, and CoVoST2 show that PART
surpasses conventional approaches, with analysis confirming its ability to
balance language-specific distinctions and cross-language generalization. These
results demonstrate PART's effectiveness and generality for multilingual speech
modality alignment.

</details>


### [73] [CHURRO: Making History Readable with an Open-Weight Large Vision-Language Model for High-Accuracy, Low-Cost Historical Text Recognition](https://arxiv.org/abs/2509.19768)
*Sina J. Semnani,Han Zhang,Xinyan He,Merve Tekgürler,Monica S. Lam*

Main category: cs.CL

TL;DR: CHURRO is a specialized vision-language model (VLM) for historical text recognition, trained on the largest dataset for this purpose, achieving superior performance and cost-efficiency compared to existing models.


<details>
  <summary>Details</summary>
Motivation: To improve the readability and study of historical texts, which are often challenging due to their irregular layouts, diverse scripts, and frequent degradation, by addressing the limitations of existing VLMs focused on modern standardized texts.

Method: The paper introduces CHURRO, a 3-billion-parameter VLM, trained on CHURRO-DS, a dataset comprising 99,491 pages from 155 historical corpora, spanning 46 language clusters across 22 centuries.

Result: CHURRO achieves top performance in historical text recognition, with 82.3% similarity for printed texts and 70.1% for handwritten texts, outperforming the second-best model in both metrics while being 15.5 times more cost-effective.

Conclusion: Releasing CHURRO and CHURRO-DS aims to foster community-driven research, improving historical text readability and accelerating the study of cultural heritage.

Abstract: Accurate text recognition for historical documents can greatly advance the
study and preservation of cultural heritage. Existing vision-language models
(VLMs), however, are designed for modern, standardized texts and are not
equipped to read the diverse languages and scripts, irregular layouts, and
frequent degradation found in historical materials.
  This paper presents CHURRO, a 3B-parameter open-weight VLM specialized for
historical text recognition. The model is trained on CHURRO-DS, the largest
historical text recognition dataset to date. CHURRO-DS unifies 155 historical
corpora comprising 99,491 pages, spanning 22 centuries of textual heritage
across 46 language clusters, including historical variants and dead languages.
  We evaluate several open-weight and closed VLMs and optical character
recognition (OCR) systems on CHURRO-DS and find that CHURRO outperforms all
other VLMs. On the CHURRO-DS test set, CHURRO achieves 82.3% (printed) and
70.1% (handwritten) normalized Levenshtein similarity, surpassing the
second-best model, Gemini 2.5 Pro, by 1.4% and 6.5%, respectively, while being
15.5 times more cost-effective.
  By releasing the model and dataset, we aim to enable community-driven
research to improve the readability of historical texts and accelerate
scholarship.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [74] [Vision-Based Perception for Autonomous Vehicles in Off-Road Environment Using Deep Learning](https://arxiv.org/abs/2509.19378)
*Nelson Alves Ferreira Neto*

Main category: cs.CV

TL;DR: The paper introduces CMSNet, a modular framework for real-time segmentation to enable autonomous driving on unpaved and off-road terrains under various adverse visibility conditions, supported by the Kamino dataset.


<details>
  <summary>Details</summary>
Motivation: Autonomous driving systems on off-road terrains face challenges due to non-uniform surfaces and adverse visibility conditions, particularly in environments like open-pit mines. This work aims to address these challenges with low-latency perception solutions.

Method: CMSNet, a Configurable Modular Segmentation Network framework, was proposed. It was trained using a new dataset, Kamino, containing diverse conditions. CMSNet employed optimized deep learning architectures and real-time techniques using TensorRT, C++, and CUDA.

Result: CMSNet successfully segmented trafficable ground and obstacles under diverse conditions. The system achieved real-time performance and was validated empirically on the Kamino dataset and another dataset.

Conclusion: CMSNet demonstrated effective segmentation for autonomous vehicles in off-road and adverse conditions, proving its applicability for real-time autonomous systems, supported by the curated Kamino dataset.

Abstract: Low-latency intelligent systems are required for autonomous driving on
non-uniform terrain in open-pit mines and developing countries. This work
proposes a perception system for autonomous vehicles on unpaved roads and
off-road environments, capable of navigating rough terrain without a predefined
trail. The Configurable Modular Segmentation Network (CMSNet) framework is
proposed, facilitating different architectural arrangements. CMSNet
configurations were trained to segment obstacles and trafficable ground on new
images from unpaved/off-road scenarios with adverse conditions (night, rain,
dust). We investigated applying deep learning to detect drivable regions
without explicit track boundaries, studied algorithm behavior under visibility
impairment, and evaluated field tests with real-time semantic segmentation. A
new dataset, Kamino, is presented with almost 12,000 images from an operating
vehicle with eight synchronized cameras. The Kamino dataset has a high number
of labeled pixels compared to similar public collections and includes images
from an off-road proving ground emulating a mine under adverse visibility. To
achieve real-time inference, CMSNet CNN layers were methodically removed and
fused using TensorRT, C++, and CUDA. Empirical experiments on two datasets
validated the proposed system's effectiveness.

</details>


### [75] [Overview of LifeCLEF Plant Identification task 2020](https://arxiv.org/abs/2509.19402)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: This paper discusses the PlantCLEF 2020 challenge, which evaluated the use of herbarium sheets to improve automated plant identification in biodiversity-rich but data-poor tropical regions.


<details>
  <summary>Details</summary>
Motivation: To address the lack of high-quality training data for plant species in biodiversity-rich tropical regions and to explore how digitized herbarium collections can enhance automated plant identification.

Method: The PlantCLEF 2020 challenge provided a dataset with herbarium sheets and field photos, focusing on species from the Guiana Shield. Participants trained models to map between herbarium sheets and field photos, with testing on field photos exclusively.

Result: The challenge demonstrated that herbarium collections could complement traditional data sources for plant identification. It also highlighted techniques and models developed by participants.

Conclusion: Using herbarium collections can significantly contribute to improving automated plant identification in biodiverse but underrepresented regions, bridging a critical data gap.

Abstract: Automated identification of plants has improved considerably thanks to the
recent progress in deep learning and the availability of training data with
more and more photos in the field. However, this profusion of data only
concerns a few tens of thousands of species, mostly located in North America
and Western Europe, much less in the richest regions in terms of biodiversity
such as tropical countries. On the other hand, for several centuries, botanists
have collected, catalogued and systematically stored plant specimens in
herbaria, particularly in tropical regions, and the recent efforts by the
biodiversity informatics community made it possible to put millions of
digitized sheets online. The LifeCLEF 2020 Plant Identification challenge (or
"PlantCLEF 2020") was designed to evaluate to what extent automated
identification on the flora of data deficient regions can be improved by the
use of herbarium collections. It is based on a dataset of about 1,000 species
mainly focused on the South America's Guiana Shield, an area known to have one
of the greatest diversity of plants in the world. The challenge was evaluated
as a cross-domain classification task where the training set consist of several
hundred thousand herbarium sheets and few thousand of photos to enable learning
a mapping between the two domains. The test set was exclusively composed of
photos in the field. This paper presents the resources and assessments of the
conducted evaluation, summarizes the approaches and systems employed by the
participating research groups, and provides an analysis of the main outcomes.

</details>


### [76] [iFinder: Structured Zero-Shot Vision-Based LLM Grounding for Dash-Cam Video Reasoning](https://arxiv.org/abs/2509.19552)
*Manyi Yao,Bingbing Zhuang,Sparsh Garg,Amit Roy-Chowdhury,Christian Shelton,Manmohan Chandraker,Abhishek Aich*

Main category: cs.CV

TL;DR: iFinder leverages a structured semantic grounding approach to improve reasoning in driving-specific tasks using dash-cam videos.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of general-purpose video-based vision-language models (V-VLMs) in handling spatial reasoning, causal inference, and event explainability within domain-specific driving video tasks.

Method: iFinder is a modular, training-free framework that translates dash-cam videos into hierarchical representations by extracting cues such as object pose, lane positions, and trajectories using pretrained vision models. It also deploys a three-block prompting strategy for step-wise reasoning using large language models.

Result: iFinder outperformed traditional end-to-end V-VLMs, achieving up to 39% improvement in accident reasoning accuracy across four public driving video benchmarks.

Conclusion: The structured semantic grounding and domain-specific representations provided by iFinder offer a reliable, interpretable, and zero-shot alternative to traditional V-VLMs for driving video analysis.

Abstract: Grounding large language models (LLMs) in domain-specific tasks like post-hoc
dash-cam driving video analysis is challenging due to their general-purpose
training and lack of structured inductive biases. As vision is often the sole
modality available for such analysis (i.e., no LiDAR, GPS, etc.), existing
video-based vision-language models (V-VLMs) struggle with spatial reasoning,
causal inference, and explainability of events in the input video. To this end,
we introduce iFinder, a structured semantic grounding framework that decouples
perception from reasoning by translating dash-cam videos into a hierarchical,
interpretable data structure for LLMs. iFinder operates as a modular,
training-free pipeline that employs pretrained vision models to extract
critical cues -- object pose, lane positions, and object trajectories -- which
are hierarchically organized into frame- and video-level structures. Combined
with a three-block prompting strategy, it enables step-wise, grounded reasoning
for the LLM to refine a peer V-VLM's outputs and provide accurate reasoning.
Evaluations on four public dash-cam video benchmarks show that iFinder's
proposed grounding with domain-specific cues, especially object orientation and
global context, significantly outperforms end-to-end V-VLMs on four zero-shot
driving benchmarks, with up to 39% gains in accident reasoning accuracy. By
grounding LLMs with driving domain-specific representations, iFinder offers a
zero-shot, interpretable, and reliable alternative to end-to-end V-VLMs for
post-hoc driving video understanding.

</details>


### [77] [CURE: Centroid-guided Unsupervised Representation Erasure for Facial Recognition Systems](https://arxiv.org/abs/2509.19562)
*Fnu Shivam,Nima Najafzadeh,Yenumula Reddy,Prashnna Gyawali*

Main category: cs.CV

TL;DR: The paper proposes CURE, an unsupervised framework to unlearn facial recognition data without identity labels while preserving model performance.


<details>
  <summary>Details</summary>
Motivation: Existing machine unlearning methods rely on identity labels, which are unavailable in privacy-sensitive cases or large noisy datasets.

Method: CURE achieves unsupervised unlearning by removing targeted sample influences using centroids and introduces the Unlearning Efficiency Score (UES) for better evaluation.

Result: CURE outperformed unsupervised variants of current methods and successfully managed quality-aware unlearning using low-quality image designation.

Conclusion: This framework advances machine unlearning capabilities, especially for privacy in facial recognition systems, enhancing adaptability and evaluation metrics.

Abstract: In the current digital era, facial recognition systems offer significant
utility and have been widely integrated into modern technological
infrastructures; however, their widespread use has also raised serious privacy
concerns, prompting regulations that mandate data removal upon request. Machine
unlearning has emerged as a powerful solution to address this issue by
selectively removing the influence of specific user data from trained models
while preserving overall model performance. However, existing machine
unlearning techniques largely depend on supervised techniques requiring
identity labels, which are often unavailable in privacy-constrained situations
or in large-scale, noisy datasets. To address this critical gap, we introduce
CURE (Centroid-guided Unsupervised Representation Erasure), the first
unsupervised unlearning framework for facial recognition systems that operates
without the use of identity labels, effectively removing targeted samples while
preserving overall performance. We also propose a novel metric, the Unlearning
Efficiency Score (UES), which balances forgetting and retention stability,
addressing shortcomings in the current evaluation metrics. CURE significantly
outperforms unsupervised variants of existing unlearning methods. Additionally,
we conducted quality-aware unlearning by designating low-quality images as the
forget set, demonstrating its usability and benefits, and highlighting the role
of image quality in machine unlearning.

</details>


### [78] [Synthesizing Artifact Dataset for Pixel-level Detection](https://arxiv.org/abs/2509.19589)
*Dennis Menn,Feng Liang,Diana Marculescu*

Main category: cs.CV

TL;DR: Authors propose an artifact corruption pipeline to generate pixel-level annotations for artifact detectors used in image-generative models, eliminating the need for manual labeling and achieving improved detector performance.


<details>
  <summary>Details</summary>
Motivation: Artifact detector training requires expensive pixel-level human annotations, which limits its scalability and performance in assisting image-generative models.

Method: The authors developed an artifact corruption pipeline to inject artifacts into high-quality synthetic images at predetermined regions, enabling automatic pixel-level annotation without human intervention.

Result: Using the proposed method, artifact detectors showed significant performance improvements, with ConvNeXt improving by 13.2% and Swin-T by 3.7%, validated on human-labeled data.

Conclusion: This pipeline represents a scalable approach to generating annotated datasets that facilitate artifact detection and integrate world knowledge, improving fidelity in generative models.

Abstract: Artifact detectors have been shown to enhance the performance of
image-generative models by serving as reward models during fine-tuning. These
detectors enable the generative model to improve overall output fidelity and
aesthetics. However, training the artifact detector requires expensive
pixel-level human annotations that specify the artifact regions. The lack of
annotated data limits the performance of the artifact detector. A naive
pseudo-labeling approach-training a weak detector and using it to annotate
unlabeled images-suffers from noisy labels, resulting in poor performance. To
address this, we propose an artifact corruption pipeline that automatically
injects artifacts into clean, high-quality synthetic images on a predetermined
region, thereby producing pixel-level annotations without manual labeling. The
proposed method enables training of an artifact detector that achieves
performance improvements of 13.2% for ConvNeXt and 3.7% for Swin-T, as verified
on human-labeled data, compared to baseline approaches. This work represents an
initial step toward scalable pixel-level artifact annotation datasets that
integrate world knowledge into artifact detection.

</details>


### [79] [Parameter-Efficient Multi-Task Learning via Progressive Task-Specific Adaptation](https://arxiv.org/abs/2509.19602)
*Neeraj Gangwar,Anshuka Rangi,Rishabh Deshmukh,Holakou Rahmanian,Yesh Dattatreya,Nickvash Kani*

Main category: cs.CV

TL;DR: This paper proposes a novel approach for multi-task learning using pre-trained models, called progressive task-specific multi-task adaptation, which minimizes task interference while performing better than state-of-the-art methods with fewer trainable parameters.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address task interference and negative transfer challenges when extending parameter-efficient fine-tuning methods to multi-task learning.

Method: A progressive task-specific adaptation method is introduced, where adapter modules in a pre-trained model are shared for initial layers across all tasks and become task-specific towards later layers. A gradient-based task similarity measure is also proposed to allocate similar tasks to shared modules.

Result: The proposed approach outperforms fully fine-tuned multi-task models, achieves better relative improvement compared to single-task fine-tuning, and reduces trainable parameters by one-fifth. Experiments on datasets such as PASCAL and NYUD-v2 affirm these results.

Conclusion: This method successfully balances transfer learning and task specialization, proving highly effective for parameter-efficient multi-task learning while surpassing existing techniques.

Abstract: Parameter-efficient fine-tuning methods have emerged as a promising solution
for adapting pre-trained models to various downstream tasks. While these
methods perform well in single-task learning, extending them to multi-task
learning exacerbates common challenges, such as task interference and negative
transfer, due to the limited number of trainable parameters. To address these
issues, we introduce progressive task-specific multi-task adaptation, a novel
parameter-efficient approach for multi-task learning. This approach introduces
adapter modules in a pre-trained model such that these modules are shared
across all tasks in the initial layers and become progressively more
task-specific in the later layers. The motivation is to reduce the conflicts
among tasks by allowing transfer learning across all tasks in the initial
layers and enabling task-specific learning toward the prediction heads.
Additionally, we propose a gradient-based approach for computing task
similarity and use this measure to allocate similar tasks to the shared adapter
modules. Our task similarity method introduces minimal overhead in the
pipeline. We evaluate our approach by adapting the Swin Transformer for dense
prediction tasks. Experiments on the PASCAL and NYUD-v2 datasets demonstrate
that our approach outperforms a fully fine-tuned multi-task model while
requiring only one-fifth of the trainable parameters. This approach achieves
better relative improvement to single-task fine-tuning while reducing the
number of trainable parameters and surpasses the current state-of-the-art
methods for parameter-efficient multi-task learning.

</details>


### [80] [Raw-JPEG Adapter: Efficient Raw Image Compression with JPEG](https://arxiv.org/abs/2509.19624)
*Mahmoud Afifi,Ran Zhang,Michael S. Brown*

Main category: cs.CV

TL;DR: This paper introduces RawJPEG Adapter, a preprocessing pipeline that adapts raw camera data for compression using standard JPEG, maintaining high reconstruction fidelity with compact additional information.


<details>
  <summary>Details</summary>
Motivation: Raw sensor data preserves detailed information beneficial for editing and vision applications but requires large storage, which is impractical. JPEG, while efficient, is not suitable for raw data storage.

Method: The authors propose the RawJPEG Adapter—an invertible preprocessing pipeline that applies spatial and optional frequency-domain transforms to raw images. Parameters are compactly stored in the JPEG comment field for accurate reconstruction.

Result: Experiments indicate that the method outperforms standard JPEG storage in fidelity, supports other codecs, and provides a good balance between compression and reconstruction quality.

Conclusion: RawJPEG Adapter efficiently adapts raw sensors' data for JPEG storage while maintaining high reconstruction accuracy, offering a practical solution for resource-constrained scenarios.

Abstract: Digital cameras digitize scene light into linear raw representations, which
the image signal processor (ISP) converts into display-ready outputs. While raw
data preserves full sensor information--valuable for editing and vision
tasks--formats such as Digital Negative (DNG) require large storage, making
them impractical in constrained scenarios. In contrast, JPEG is a widely
supported format, offering high compression efficiency and broad compatibility,
but it is not well-suited for raw storage. This paper presents RawJPEG Adapter,
a lightweight, learnable, and invertible preprocessing pipeline that adapts raw
images for standard JPEG compression. Our method applies spatial and optional
frequency-domain transforms, with compact parameters stored in the JPEG comment
field, enabling accurate raw reconstruction. Experiments across multiple
datasets show that our method achieves higher fidelity than direct JPEG
storage, supports other codecs, and provides a favorable trade-off between
compression ratio and reconstruction accuracy.

</details>


### [81] [The Impact of 2D Segmentation Backbones on Point Cloud Predictions Using 4D Radar](https://arxiv.org/abs/2509.19644)
*William L. Muckelroy III,Mohammed Alsakabi,John M. Dolan,Ozan K. Tonguz*

Main category: cs.CV

TL;DR: The paper explores improving non-LiDAR 3D point cloud generation using 4D Radar by investigating segmentation backbones, achieving a 23.7% performance improvement over the state-of-the-art.


<details>
  <summary>Details</summary>
Motivation: To reduce the reliance on expensive LiDAR sensors in autonomous driving systems by enhancing 4D Radar-based point cloud generation.

Method: The study evaluates the impact of higher-capacity segmentation backbones on generating LiDAR-like 3D point clouds, using a neural network trained on the RaDelft dataset.

Result: Optimal segmentation backbones demonstrated a 23.7% improvement in point cloud quality compared to the state-of-the-art.

Conclusion: Carefully selecting segmentation backbones can considerably enhance radar-based point cloud generation, but overly high-capacity models may hinder performance.

Abstract: LiDAR's dense, sharp point cloud (PC) representations of the surrounding
environment enable accurate perception and significantly improve road safety by
offering greater scene awareness and understanding. However, LiDAR's high cost
continues to restrict the broad adoption of high-level Autonomous Driving (AD)
systems in commercially available vehicles. Prior research has shown progress
towards circumventing the need for LiDAR by training a neural network, using
LiDAR point clouds as ground truth (GT), to produce LiDAR-like 3D point clouds
using only 4D Radars. One of the best examples is a neural network created to
train a more efficient radar target detector with a modular 2D convolutional
neural network (CNN) backbone and a temporal coherence network at its core that
uses the RaDelft dataset for training (see arXiv:2406.04723). In this work, we
investigate the impact of higher-capacity segmentation backbones on the quality
of the produced point clouds. Our results show that while very high-capacity
models may actually hurt performance, an optimal segmentation backbone can
provide a 23.7% improvement over the state-of-the-art (SOTA).

</details>


### [82] [Bias in the Picture: Benchmarking VLMs with Social-Cue News Images and LLM-as-Judge Assessment](https://arxiv.org/abs/2509.19659)
*Aravind Narayanan,Vahid Reza Khazaie,Shaina Raza*

Main category: cs.CV

TL;DR: The paper analyzes biases in large vision-language models (VLMs) and introduces a benchmark for assessing such risks.


<details>
  <summary>Details</summary>
Motivation: Address the issue of social stereotypes being propagated by vision-language models when visual cues like age, gender, and race affect outputs.

Method: Develop a news-image benchmark with annotated question-image pairs for evaluation and employ state-of-the-art VLMs along with a human-verified large language model as a judge.

Result: Bias prevalence varies across attributes and models, visual context influences outputs, and higher model faithfulness doesn't imply reduced bias.

Conclusion: Promote fairness-aware multimodal assessments by releasing benchmark resources and findings highlighting systematic biases in VLMs.

Abstract: Large vision-language models (VLMs) can jointly interpret images and text,
but they are also prone to absorbing and reproducing harmful social stereotypes
when visual cues such as age, gender, race, clothing, or occupation are
present. To investigate these risks, we introduce a news-image benchmark
consisting of 1,343 image-question pairs drawn from diverse outlets, which we
annotated with ground-truth answers and demographic attributes (age, gender,
race, occupation, and sports). We evaluate a range of state-of-the-art VLMs and
employ a large language model (LLM) as judge, with human verification. Our
findings show that: (i) visual context systematically shifts model outputs in
open-ended settings; (ii) bias prevalence varies across attributes and models,
with particularly high risk for gender and occupation; and (iii) higher
faithfulness does not necessarily correspond to lower bias. We release the
benchmark prompts, evaluation rubric, and code to support reproducible and
fairness-aware multimodal assessment.

</details>


### [83] [MoTiC: Momentum Tightness and Contrast for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2509.19664)
*Zeyu He,Shuai Huang,Yuwu Lu,Ming Zhao*

Main category: cs.CV

TL;DR: This paper introduces a method to improve Few-Shot Class-Incremental Learning (FSCIL) by enhancing prototype accuracy and interclass feature cohesion.


<details>
  <summary>Details</summary>
Motivation: FSCIL faces challenges of learning new classes with very limited data while retaining knowledge of old classes.

Method: The authors propose MoTiC, a framework combining Bayesian analysis, momentum self-supervision, virtual categories, and large-scale contrastive learning.

Result: MoTiC achieves state-of-the-art performance across FSCIL benchmarks, including the fine-grained CUB-200 dataset.

Conclusion: The method effectively reduces prototype bias and robustness in incremental learning scenarios.

Abstract: Few-Shot Class-Incremental Learning (FSCIL) must contend with the dual
challenge of learning new classes from scarce samples while preserving old
class knowledge. Existing methods use the frozen feature extractor and
class-averaged prototypes to mitigate against catastrophic forgetting and
overfitting. However, new-class prototypes suffer significant estimation bias
due to extreme data scarcity, whereas base-class prototypes benefit from
sufficient data. In this work, we theoretically demonstrate that aligning the
new-class priors with old-class statistics via Bayesian analysis reduces
variance and improves prototype accuracy. Furthermore, we propose large-scale
contrastive learning to enforce cross-category feature tightness. To further
enrich feature diversity and inject prior information for new-class prototypes,
we integrate momentum self-supervision and virtual categories into the Momentum
Tightness and Contrast framework (MoTiC), constructing a feature space with
rich representations and enhanced interclass cohesion. Experiments on three
FSCIL benchmarks produce state-of-the-art performances, particularly on the
fine-grained task CUB-200, validating our method's ability to reduce estimation
bias and improve incremental learning robustness.

</details>


### [84] [Deep Learning for Clouds and Cloud Shadow Segmentation in Methane Satellite and Airborne Imaging Spectroscopy](https://arxiv.org/abs/2509.19665)
*Manuel Perez-Carrasco,Maya Nasr,Sebastien Roche,Chris Chan Miller,Zhan Zhang,Core Francisco Park,Eleanor Walker,Cecilia Garraffo,Douglas Finkbeiner,Ritesh Gautam,Steven Wofsy*

Main category: cs.CV

TL;DR: This study applies machine learning, particularly deep learning architectures like UNet and SCAN, for effective cloud and cloud shadow detection in hyperspectral remote sensing data to improve methane retrieval and quantification, outperforming conventional methods.


<details>
  <summary>Details</summary>
Motivation: Accurate detection of clouds and their shadows is crucial for hyperspectral remote sensing to avoid biased methane retrievals and enhance the accuracy of emissions quantification from instruments like MethaneSAT and MethaneAIR.

Method: The paper evaluates machine learning methods, comparing conventional techniques such as Iterative Logistic Regression (ILR) and Multilayer Perceptron (MLP) with advanced deep learning models like UNet and Spectral Channel Attention Network (SCAN).

Result: Deep learning models significantly improve cloud and cloud shadow detection quality, with UNet excelling at preserving spatial coherence and SCAN capturing fine boundary details. SCAN performs better than UNet in MethaneSAT data.

Conclusion: Advanced deep learning models offer robust, scalable solutions for cloud screening, enhancing the utility and reliability of hyperspectral missions for methane emission quantification. Their publicly available code supports further research and application.

Abstract: Effective cloud and cloud shadow detection is a critical prerequisite for
accurate retrieval of concentrations of atmospheric methane or other trace
gases in hyperspectral remote sensing. This challenge is especially pertinent
for MethaneSAT and for its airborne companion mission, MethaneAIR. In this
study, we use machine learning methods to address the cloud and cloud shadow
detection problem for sensors with these high spatial resolutions instruments.
Cloud and cloud shadows in remote sensing data need to be effectively screened
out as they bias methane retrievals in remote sensing imagery and impact the
quantification of emissions. We deploy and evaluate conventional techniques
including Iterative Logistic Regression (ILR) and Multilayer Perceptron (MLP),
with advanced deep learning architectures, namely UNet and a Spectral Channel
Attention Network (SCAN) method. Our results show that conventional methods
struggle with spatial coherence and boundary definition, affecting the
detection of clouds and cloud shadows. Deep learning models substantially
improve detection quality: UNet performs best in preserving spatial structure,
while SCAN excels at capturing fine boundary details. Notably, SCAN surpasses
UNet on MethaneSAT data, underscoring the benefits of incorporating spectral
attention for satellite specific features. This in depth assessment of various
disparate machine learning techniques demonstrates the strengths and
effectiveness of advanced deep learning architectures in providing robust,
scalable solutions for clouds and cloud shadow screening towards enhancing
methane emission quantification capacity of existing and next generation
hyperspectral missions. Our data and code is publicly available at
https://doi.org/10.7910/DVN/IKLZOJ

</details>


### [85] [Enhancing Transformer-Based Vision Models: Addressing Feature Map Anomalies Through Novel Optimization Strategies](https://arxiv.org/abs/2509.19687)
*Sumit Mamtani*

Main category: cs.CV

TL;DR: The paper introduces Structured Token Augmentation (STA) and Adaptive Noise Filtering (ANF) as methods to enhance Vision Transformers (ViTs) by addressing structured noise artifacts, improving both visual quality and task performance.


<details>
  <summary>Details</summary>
Motivation: Despite the superior performance of Vision Transformers (ViTs) in computer vision tasks, their feature maps are hampered by structured noise artifacts, negatively affecting downstream applications like segmentation and depth estimation.

Method: The paper proposes the Structured Token Augmentation (STA) to enhance token diversity via spatial perturbations during tokenization and Adaptive Noise Filtering (ANF) for learnable inline denoising within transformer layers. These are lightweight and architecture-agnostic techniques.

Result: Experimental results demonstrate that STA and ANF consistently improve visual quality and task performance across benchmarks such as ImageNet, Ade20k, and NYUv2.

Conclusion: The proposed techniques successfully mitigate noise artifacts in Vision Transformers, enhancing their interpretability and broadening their applicability in various computer vision tasks.

Abstract: Vision Transformers (ViTs) have demonstrated superior performance across a
wide range of computer vision tasks. However, structured noise artifacts in
their feature maps hinder downstream applications such as segmentation and
depth estimation. We propose two novel and lightweight optimisation techniques-
Structured Token Augmentation (STA) and Adaptive Noise Filtering (ANF)- to
improve interpretability and mitigate these artefacts. STA enhances token
diversity through spatial perturbations during tokenisation, while ANF applies
learnable inline denoising between transformer layers. These methods are
architecture-agnostic and evaluated across standard benchmarks, including
ImageNet, Ade20k, and NYUv2. Experimental results show consistent improvements
in visual quality and task performance, highlighting the practical
effectiveness of our approach.

</details>


### [86] [From Prompt to Progression: Taming Video Diffusion Models for Seamless Attribute Transition](https://arxiv.org/abs/2509.19690)
*Ling Lo,Kelvin C. K. Chan,Wen-Huang Cheng,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: The paper introduces an effective method for ensuring smooth and consistent attribute transitions in video generation.


<details>
  <summary>Details</summary>
Motivation: Address limitations of existing models in handling complex temporal changes and gradual attribute transitions in video generation.

Method: Proposes a frame-wise guidance approach during the denoising process, constructing transitional direction for gradual attribute shifts.

Result: Experimental evaluation demonstrates superior performance compared to baselines in fidelity, prompt alignment, and smooth transitions.

Conclusion: The approach is effective and outperforms existing methods, with its code and evaluation benchmark made publicly available.

Abstract: Existing models often struggle with complex temporal changes, particularly
when generating videos with gradual attribute transitions. The most common
prompt interpolation approach for motion transitions often fails to handle
gradual attribute transitions, where inconsistencies tend to become more
pronounced. In this work, we propose a simple yet effective method to extend
existing models for smooth and consistent attribute transitions, through
introducing frame-wise guidance during the denoising process. Our approach
constructs a data-specific transitional direction for each noisy latent,
guiding the gradual shift from initial to final attributes frame by frame while
preserving the motion dynamics of the video. Moreover, we present the
Controlled-Attribute-Transition Benchmark (CAT-Bench), which integrates both
attribute and motion dynamics, to comprehensively evaluate the performance of
different models. We further propose two metrics to assess the accuracy and
smoothness of attribute transitions. Experimental results demonstrate that our
approach performs favorably against existing baselines, achieving visual
fidelity, maintaining alignment with text prompts, and delivering seamless
attribute transitions. Code and CATBench are released:
https://github.com/lynn-ling-lo/Prompt2Progression.

</details>


### [87] [Anatomically Constrained Transformers for Cardiac Amyloidosis Classification](https://arxiv.org/abs/2509.19691)
*Alexander Thorley,Agis Chartsias,Jordan Strom,Roberto Lang,Jeremy Slivnick,Jamie O'Driscoll,Rajan Sharma,Dipak Kotecha,Jinming Duan,Alberto Gomez*

Main category: cs.CV

TL;DR: The paper introduces a myocardium-focused transformer model for accurate cardiac amyloidosis (CA) classification, outperforming traditional video-based approaches.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the lack of explicit assurance in current neural network-based CA video classification models that their decisions rely on clinically meaningful features.

Method: The proposed approach includes constraining a transformer model to localized anatomical regions (the myocardium) and incorporating self-supervised learning through masked autoencoder pre-training for myocardium-specific image reconstruction.

Result: The myocardium-constrained transformer model demonstrated improved performance in classifying CA compared to full video transformers, with the added interpretability of visualizing attention scores on relevant anatomical areas.

Conclusion: Focusing on clinically relevant anatomical regions improves CA classification accuracy and interpretability, offering a promising paradigm for future disease classification models.

Abstract: Cardiac amyloidosis (CA) is a rare cardiomyopathy, with typical abnormalities
in clinical measurements from echocardiograms such as reduced global
longitudinal strain of the myocardium. An alternative approach for detecting CA
is via neural networks, using video classification models such as convolutional
neural networks. These models process entire video clips, but provide no
assurance that classification is based on clinically relevant features known to
be associated with CA. An alternative paradigm for disease classification is to
apply models to quantitative features such as strain, ensuring that the
classification relates to clinically relevant features. Drawing inspiration
from this approach, we explicitly constrain a transformer model to the
anatomical region where many known CA abnormalities occur -- the myocardium,
which we embed as a set of deforming points and corresponding sampled image
patches into input tokens. We show that our anatomical constraint can also be
applied to the popular self-supervised learning masked autoencoder
pre-training, where we propose to mask and reconstruct only anatomical patches.
We show that by constraining both the transformer and pre-training task to the
myocardium where CA imaging features are localized, we achieve increased
performance on a CA classification task compared to full video transformers.
Our model provides an explicit guarantee that the classification is focused on
only anatomical regions of the echo, and enables us to visualize transformer
attention scores over the deforming myocardium.

</details>


### [88] [Learning to Stop: Reinforcement Learning for Efficient Patient-Level Echocardiographic Classification](https://arxiv.org/abs/2509.19694)
*Woo-Jin Cho Kim,Jorge Oliveira,Arian Beqiri,Alex Thorley,Jordan Strom,Jamie O'Driscoll,Rajan Sharma,Jeremy Slivnick,Roberto Lang,Alberto Gomez,Agisilaos Chartsias*

Main category: cs.CV

TL;DR: The study proposes methods to optimally select and process subsets of transthoracic echocardiographic video clips for improved disease classification, achieving high performance while utilizing fewer clips.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address inefficiencies in current automated methods for analyzing transthoracic echocardiographic clips, which either neglect complementary information or are computationally expensive when processing all clips.

Method: The paper introduces a reinforcement learning-based approach where an agent decides whether to continue processing or stop based on classification confidence, alongside an attention-based method for aggregating information.

Result: The proposed approach achieves an AUC of 0.91 in detecting cardiac amyloidosis, utilizing only 30% of the full set of video clips, surpassing other methods.

Conclusion: The method demonstrates that using optimized subsets of clips and advanced aggregation techniques can achieve better performance than processing all clips or using simple benchmarks, marking a step toward clinical practicality.

Abstract: Guidelines for transthoracic echocardiographic examination recommend the
acquisition of multiple video clips from different views of the heart,
resulting in a large number of clips. Typically, automated methods, for
instance disease classifiers, either use one clip or average predictions from
all clips. Relying on one clip ignores complementary information available from
other clips, while using all clips is computationally expensive and may be
prohibitive for clinical adoption.
  To select the optimal subset of clips that maximize performance for a
specific task (image-based disease classification), we propose a method
optimized through reinforcement learning. In our method, an agent learns to
either keep processing view-specific clips to reduce the disease classification
uncertainty, or stop processing if the achieved classification confidence is
sufficient. Furthermore, we propose a learnable attention-based aggregation
method as a flexible way of fusing information from multiple clips. The
proposed method obtains an AUC of 0.91 on the task of detecting cardiac
amyloidosis using only 30% of all clips, exceeding the performance achieved
from using all clips and from other benchmarks.

</details>


### [89] [Towards Robust In-Context Learning for Medical Image Segmentation via Data Synthesis](https://arxiv.org/abs/2509.19711)
*Jiesi Hu,Yanwu Yang,Zhiyu Ye,Chenfei Ye,Hanyang Peng,Jianfeng Cao,Ting Ma*

Main category: cs.CV

TL;DR: The paper introduces SynthICL, a data synthesis framework that leverages domain randomization to generate diverse, anatomically realistic datasets, addressing data scarcity in in-context learning for medical image segmentation.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the shortage of large-scale and diverse datasets required for in-context learning in medical image segmentation, while maintaining domain realism and diversity.

Method: The proposed SynthICL uses domain randomization and anatomical priors to ensure data realism, generates diverse anatomical structures to cover broad distributions, and models inter-subject variations to create suitable data cohorts for ICL.

Result: SynthICL displayed performance gains of up to 63% in Dice score and improved generalization to unseen domains through experiments on four held-out datasets.

Conclusion: The SynthICL framework offers a solution to the data bottleneck in ICL-based medical image segmentation and facilitates the development of robust, generalized models.

Abstract: The rise of In-Context Learning (ICL) for universal medical image
segmentation has introduced an unprecedented demand for large-scale, diverse
datasets for training, exacerbating the long-standing problem of data scarcity.
While data synthesis offers a promising solution, existing methods often fail
to simultaneously achieve both high data diversity and a domain distribution
suitable for medical data. To bridge this gap, we propose \textbf{SynthICL}, a
novel data synthesis framework built upon domain randomization. SynthICL
ensures realism by leveraging anatomical priors from real-world datasets,
generates diverse anatomical structures to cover a broad data distribution, and
explicitly models inter-subject variations to create data cohorts suitable for
ICL. Extensive experiments on four held-out datasets validate our framework's
effectiveness, showing that models trained with our data achieve performance
gains of up to 63\% in average Dice and substantially enhanced generalization
to unseen anatomical domains. Our work helps mitigate the data bottleneck for
ICL-based segmentation, paving the way for robust models. Our code and the
generated dataset are publicly available at
https://github.com/jiesihu/Neuroverse3D.

</details>


### [90] [VIMD: Monocular Visual-Inertial Motion and Depth Estimation](https://arxiv.org/abs/2509.19713)
*Saimouli Katragadda,Guoquan Huang*

Main category: cs.CV

TL;DR: The paper presents VIMD, a monocular depth estimation framework that refines dense metric depth using multi-view information, achieving strong generalization and efficiency.


<details>
  <summary>Details</summary>
Motivation: To improve dense metric depth estimation for robotics and XR by addressing limitations in prior methods like global invariant affine models.

Method: Developed a monocular visual-inertial learning framework called VIMD, which leverages MSCKF-based motion tracking and iteratively refines depth using multi-view data.

Result: VIMD demonstrates exceptional accuracy and robustness with sparse points and zero-shot generalization across datasets like TartanAir, VOID, and AR Table.

Conclusion: VIMD offers an efficient, modular, and practical solution for dense metric depth estimation, suitable for resource-constrained settings and diverse scenarios.

Abstract: Accurate and efficient dense metric depth estimation is crucial for 3D visual
perception in robotics and XR. In this paper, we develop a monocular
visual-inertial motion and depth (VIMD) learning framework to estimate dense
metric depth by leveraging accurate and efficient MSCKF-based monocular
visual-inertial motion tracking. At the core the proposed VIMD is to exploit
multi-view information to iteratively refine per-pixel scale, instead of
globally fitting an invariant affine model as in the prior work. The VIMD
framework is highly modular, making it compatible with a variety of existing
depth estimation backbones. We conduct extensive evaluations on the TartanAir
and VOID datasets and demonstrate its zero-shot generalization capabilities on
the AR Table dataset. Our results show that VIMD achieves exceptional accuracy
and robustness, even with extremely sparse points as few as 10-20 metric depth
points per image. This makes the proposed VIMD a practical solution for
deployment in resource constrained settings, while its robust performance and
strong generalization capabilities offer significant potential across a wide
range of scenarios.

</details>


### [91] [Frequency-domain Multi-modal Fusion for Language-guided Medical Image Segmentation](https://arxiv.org/abs/2509.19719)
*Bo Yu,Jianhua Yang,Zetao Du,Yan Huang,Chenglong Li,Liang Wang*

Main category: cs.CV

TL;DR: The paper introduces FMISeg, a model for language-guided medical image segmentation using a frequency-domain approach to enhance segmentation performance.


<details>
  <summary>Details</summary>
Motivation: The need to improve medical image segmentation accuracy by integrating clinical text reports, while addressing challenges posed by complex lesions and vision-language modality gaps.

Method: A late fusion model, FMISeg, uses a Frequency-domain Feature Bidirectional Interaction module and a Language-guided Frequency-domain Feature Interaction module for better visual and linguistic feature integration.

Result: Experiments on QaTa-COV19 and MosMedData+ datasets showed FMISeg surpassing state-of-the-art segmentation methods in both qualitative and quantitative evaluations.

Conclusion: FMISeg effectively leverages linguistic texts in the frequency domain to refine segmentation methods, resolving semantic inconsistencies and improving disease diagnosis accuracy.

Abstract: Automatically segmenting infected areas in radiological images is essential
for diagnosing pulmonary infectious diseases. Recent studies have demonstrated
that the accuracy of the medical image segmentation can be improved by
incorporating clinical text reports as semantic guidance. However, the complex
morphological changes of lesions and the inherent semantic gap between
vision-language modalities prevent existing methods from effectively enhancing
the representation of visual features and eliminating semantically irrelevant
information, ultimately resulting in suboptimal segmentation performance. To
address these problems, we propose a Frequency-domain Multi-modal Interaction
model (FMISeg) for language-guided medical image segmentation. FMISeg is a late
fusion model that establishes interaction between linguistic features and
frequency-domain visual features in the decoder. Specifically, to enhance the
visual representation, our method introduces a Frequency-domain Feature
Bidirectional Interaction (FFBI) module to effectively fuse frequency-domain
features. Furthermore, a Language-guided Frequency-domain Feature Interaction
(LFFI) module is incorporated within the decoder to suppress semantically
irrelevant visual features under the guidance of linguistic information.
Experiments on QaTa-COV19 and MosMedData+ demonstrated that our method
outperforms the state-of-the-art methods qualitatively and quantitatively.

</details>


### [92] [PolGS: Polarimetric Gaussian Splatting for Fast Reflective Surface Reconstruction](https://arxiv.org/abs/2509.19726)
*Yufei Han,Bowen Tie,Heng Guo,Youwei Lyu,Si Li,Boxin Shi,Yunpeng Jia,Zhanyu Ma*

Main category: cs.CV

TL;DR: This paper introduces PolGS, a polarimetric-enhanced 3D Gaussian Splatting model for rapid reconstruction of complex reflective surfaces.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the limitations in reconstruction quality of reflective surfaces encountered by current 3D Gaussian Splatting methods compared to implicit neural representations.

Method: The authors integrate polarimetric constraints into the 3D Gaussian Splatting framework, enabling accurate separation of specular and diffuse components during reconstruction.

Result: PolGS successfully reconstructs reflective surfaces in just 10 minutes, demonstrating enhanced reconstruction quality validated through experiments on synthetic and real-world datasets.

Conclusion: PolGS advances the field of real-time virtual reality by combining speed and quality in reconstructing surfaces with complex reflectance properties.

Abstract: Efficient shape reconstruction for surfaces with complex reflectance
properties is crucial for real-time virtual reality. While 3D Gaussian
Splatting (3DGS)-based methods offer fast novel view rendering by leveraging
their explicit surface representation, their reconstruction quality lags behind
that of implicit neural representations, particularly in the case of recovering
surfaces with complex reflective reflectance. To address these problems, we
propose PolGS, a Polarimetric Gaussian Splatting model allowing fast reflective
surface reconstruction in 10 minutes. By integrating polarimetric constraints
into the 3DGS framework, PolGS effectively separates specular and diffuse
components, enhancing reconstruction quality for challenging reflective
materials. Experimental results on the synthetic and real-world dataset
validate the effectiveness of our method.

</details>


### [93] [CAMILA: Context-Aware Masking for Image Editing with Language Alignment](https://arxiv.org/abs/2509.19731)
*Hyunseung Kim,Chiho Choi,Srikanth Malla,Sai Prahladh Padmanabhan,Saurabh Bagchi,Joon Hee Choi*

Main category: cs.CV

TL;DR: CAMILA, a context-aware image editing method, validates the coherence between user instructions and images to avoid executing infeasible edits.


<details>
  <summary>Details</summary>
Motivation: Existing image editing models struggle with infeasible or contradictory instructions, leading to nonsensical results.

Method: CAMILA uses context-aware masking and language alignment to apply only logically valid edits to images.

Result: CAMILA surpasses state-of-the-art models in performance, semantic alignment, and handling complex instructions.

Conclusion: CAMILA successfully ensures relevant edits are applied while discarding non-executable ones, preserving core image integrity.

Abstract: Text-guided image editing has been allowing users to transform and synthesize
images through natural language instructions, offering considerable
flexibility. However, most existing image editing models naively attempt to
follow all user instructions, even if those instructions are inherently
infeasible or contradictory, often resulting in nonsensical output. To address
these challenges, we propose a context-aware method for image editing named as
CAMILA (Context-Aware Masking for Image Editing with Language Alignment).
CAMILA is designed to validate the contextual coherence between instructions
and the image, ensuring that only relevant edits are applied to the designated
regions while ignoring non-executable instructions. For comprehensive
evaluation of this new method, we constructed datasets for both single- and
multi-instruction image editing, incorporating the presence of infeasible
requests. Our method achieves better performance and higher semantic alignment
than state-of-the-art models, demonstrating its effectiveness in handling
complex instruction challenges while preserving image integrity.

</details>


### [94] [Robust RGB-T Tracking via Learnable Visual Fourier Prompt Fine-tuning and Modality Fusion Prompt Generation](https://arxiv.org/abs/2509.19733)
*Hongtao Yang,Bineng Zhong,Qihua Liang,Zhiruo Zhu,Yaozong Zheng,Ning Li*

Main category: cs.CV

TL;DR: The paper introduces VFPTrack, a novel RGB-T tracking method that incorporates frequency-domain information and fusion techniques for effective multi-modal tracking.


<details>
  <summary>Details</summary>
Motivation: Existing RGB-T tracking methods relying solely on the spatial domain overlook frequency-domain information, which limits their performance potential.

Method: The method combines spatial and frequency-domain information using a frozen encoder for modality feature extraction, Fast Fourier Transform for frequency-domain prompts, and a Modality Fusion Prompt Generator for fused modality interactions.

Result: Extensive experiments across three prominent RGB-T tracking benchmarks reveal superior performance of the proposed method.

Conclusion: The paper demonstrates the importance of integrating frequency-domain features and cross-modality interaction for enhancing RGB-T tracking methods.

Abstract: Recently, visual prompt tuning is introduced to RGB-Thermal (RGB-T) tracking
as a parameter-efficient finetuning (PEFT) method. However, these PEFT-based
RGB-T tracking methods typically rely solely on spatial domain information as
prompts for feature extraction. As a result, they often fail to achieve optimal
performance by overlooking the crucial role of frequency-domain information in
prompt learning. To address this issue, we propose an efficient Visual Fourier
Prompt Tracking (named VFPTrack) method to learn modality-related prompts via
Fast Fourier Transform (FFT). Our method consists of symmetric feature
extraction encoder with shared parameters, visual fourier prompts, and Modality
Fusion Prompt Generator that generates bidirectional interaction prompts
through multi-modal feature fusion. Specifically, we first use a frozen feature
extraction encoder to extract RGB and thermal infrared (TIR) modality features.
Then, we combine the visual prompts in the spatial domain with the frequency
domain prompts obtained from the FFT, which allows for the full extraction and
understanding of modality features from different domain information. Finally,
unlike previous fusion methods, the modality fusion prompt generation module we
use combines features from different modalities to generate a fused modality
prompt. This modality prompt is interacted with each individual modality to
fully enable feature interaction across different modalities. Extensive
experiments conducted on three popular RGB-T tracking benchmarks show that our
method demonstrates outstanding performance.

</details>


### [95] [Rectified Decoupled Dataset Distillation: A Closer Look for Fair and Comprehensive Evaluation](https://arxiv.org/abs/2509.19743)
*Xinhao Zhong,Shuoyang Sun,Xulin Gu,Chenyang Zhu,Bin Chen,Yaowei Wang*

Main category: cs.CV

TL;DR: This paper identifies and addresses inconsistencies in dataset distillation evaluations, proposing a new method (RD$^3$) that standardizes evaluation to enable fair comparisons.


<details>
  <summary>Details</summary>
Motivation: Existing dataset distillation methods are hindered by high computational overhead and inconsistent evaluation protocols, which obscure genuine methodological advances.

Method: The authors propose Rectified Decoupled Dataset Distillation (RD$^3$), systematically analyzing post-evaluation settings and establishing a standardized benchmark with rigorous evaluation.

Result: The study reveals that performance variations among methods are often due to evaluation inconsistencies, not actual methodological improvements, and identifies strategies for more effective dataset distillation.

Conclusion: RD$^3$ creates a foundation for replicable and fair comparisons in dataset distillation research, advancing the field with standardized evaluation practices.

Abstract: Dataset distillation aims to generate compact synthetic datasets that enable
models trained on them to achieve performance comparable to those trained on
full real datasets, while substantially reducing storage and computational
costs. Early bi-level optimization methods (e.g., MTT) have shown promising
results on small-scale datasets, but their scalability is limited by high
computational overhead. To address this limitation, recent decoupled dataset
distillation methods (e.g., SRe$^2$L) separate the teacher model pre-training
from the synthetic data generation process. These methods also introduce random
data augmentation and epoch-wise soft labels during the post-evaluation phase
to improve performance and generalization. However, existing decoupled
distillation methods suffer from inconsistent post-evaluation protocols, which
hinders progress in the field. In this work, we propose Rectified Decoupled
Dataset Distillation (RD$^3$), and systematically investigate how different
post-evaluation settings affect test accuracy. We further examine whether the
reported performance differences across existing methods reflect true
methodological advances or stem from discrepancies in evaluation procedures.
Our analysis reveals that much of the performance variation can be attributed
to inconsistent evaluation rather than differences in the intrinsic quality of
the synthetic data. In addition, we identify general strategies that improve
the effectiveness of distilled datasets across settings. By establishing a
standardized benchmark and rigorous evaluation protocol, RD$^3$ provides a
foundation for fair and reproducible comparisons in future dataset distillation
research.

</details>


### [96] [nnFilterMatch: A Unified Semi-Supervised Learning Framework with Uncertainty-Aware Pseudo-Label Filtering for Efficient Medical Segmentation](https://arxiv.org/abs/2509.19746)
*Yi Yang*

Main category: cs.CV

TL;DR: This paper introduces nnFilterMatch, an efficient framework combining semi-supervised learning and active learning to significantly reduce annotation efforts in medical image segmentation.


<details>
  <summary>Details</summary>
Motivation: Medical image segmentation is essential but heavily reliant on annotated data, which is labor-intensive. Combining SSL and AL can reduce this burden, though existing approaches suffer from scalability and computational inefficiencies.

Method: The paper integrates entropy-based pseudo-label filtering with the nnU-Net framework, selectively excluding high-confidence pseudo-labels during training to avoid iterative retraining cycles.

Result: The proposed framework delivers segmentation performance comparable to fully supervised models, while requiring only 5-20% labeled data, validated across multiple clinical benchmarks.

Conclusion: nnFilterMatch offers a scalable and annotation-efficient method for medical image segmentation, retaining high accuracy without the need for extensive data labeling or retraining cycles.

Abstract: Semi-supervised learning (SSL) has emerged as a promising paradigm in medical
image segmentation, offering competitive performance while substantially
reducing the need for extensive manual annotation. When combined with active
learning (AL), these strategies further minimize annotation burden by
selectively incorporating the most informative samples. However, conventional
SSL_AL hybrid approaches often rely on iterative and loop-based retraining
cycles after each annotation round, incurring significant computational
overhead and limiting scalability in clinical applications. In this study, we
present a novel, annotation-efficient, and self-adaptive deep segmentation
framework that integrates SSL with entropy-based pseudo-label filtering
(FilterMatch), an AL-inspired mechanism, within the single-pass nnU-Net
training segmentation framework (nnFilterMatch). By selectively excluding
high-confidence pseudo-labels during training, our method circumvents the need
for retraining loops while preserving the benefits of uncertainty-guided
learning. We validate the proposed framework across multiple clinical
segmentation benchmarks and demonstrate that it achieves performance comparable
to or exceeding fully supervised models, even with only 5\%--20\% labeled data.
This work introduces a scalable, end-to-end learning strategy for reducing
annotation demands in medical image segmentation without compromising accuracy.
Code is available here: https://github.com/Ordi117/nnFilterMatch.git.

</details>


### [97] [Talking Head Generation via AU-Guided Landmark Prediction](https://arxiv.org/abs/2509.19749)
*Shao-Yu Chang,Jingyi Xu,Hieu Le,Dimitris Samaras*

Main category: cs.CV

TL;DR: The paper introduces a two-stage model for creating realistic, audio-driven talking head videos with fine expression control using facial Action Units (AUs), outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To improve control, accuracy, and realism in audio-driven talking head generation, particularly focusing on per-frame expression control using facial Action Units (AUs).

Method: A two-stage approach: Stage 1 employs a variational motion generator for predicting landmark sequences from audio and AU intensities, and Stage 2 uses a diffusion-based synthesizer to generate high-quality videos based on landmarks and a reference image.

Result: The proposed method demonstrates superior performance in expression accuracy, temporal stability, and visual quality compared to state-of-the-art models, as validated on the MEAD dataset.

Conclusion: Explicitly modeling Action Units (AUs) to generate facial landmarks substantially enhances the quality and control in expressive talking head generation, illustrating the framework's effectiveness.

Abstract: We propose a two-stage framework for audio-driven talking head generation
with fine-grained expression control via facial Action Units (AUs). Unlike
prior methods relying on emotion labels or implicit AU conditioning, our model
explicitly maps AUs to 2D facial landmarks, enabling physically grounded,
per-frame expression control. In the first stage, a variational motion
generator predicts temporally coherent landmark sequences from audio and AU
intensities. In the second stage, a diffusion-based synthesizer generates
realistic, lip-synced videos conditioned on these landmarks and a reference
image. This separation of motion and appearance improves expression accuracy,
temporal stability, and visual realism. Experiments on the MEAD dataset show
that our method outperforms state-of-the-art baselines across multiple metrics,
demonstrating the effectiveness of explicit AU-to-landmark modeling for
expressive talking head generation.

</details>


### [98] [ExpFace: Exponential Angular Margin Loss for Deep Face Recognition](https://arxiv.org/abs/2509.19753)
*Jinhui Zheng,Xueyuan Gong*

Main category: cs.CV

TL;DR: The paper introduces ExpFace, an advanced face recognition loss function that better handles noisy samples and achieves superior performance compared to previous methods.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the limitations of existing margin-based softmax losses in face recognition, particularly their inability to effectively deal with noisy samples.

Method: ExpFace is proposed as a loss function introducing an angular exponential term, which applies larger penalties to clean samples and smaller penalties to noisy samples, improving training stability and performance.

Result: ExpFace not only solves issues found in SphereFace and ArcFace but also achieves state-of-the-art results in face recognition benchmarks.

Conclusion: The proposed ExpFace loss enhances the discriminative power of face recognition systems while addressing the challenges posed by noisy data. Source code is made publicly available for further research.

Abstract: Face recognition is an open-set problem requiring high discriminative power
to ensure that intra-class distances remain smaller than inter-class distances.
Margin-based softmax losses, such as SphereFace, CosFace, and ArcFace, have
been widely adopted to enhance intra-class compactness and inter-class
separability, yet they overlook the impact of noisy samples. By examining the
distribution of samples in the angular space, we observe that clean samples
predominantly cluster in the center region, whereas noisy samples tend to shift
toward the peripheral region. Motivated by this observation, we propose the
Exponential Angular Margin Loss (ExpFace), which introduces an angular
exponential term as the margin. This design applies a larger penalty in the
center region and a smaller penalty in the peripheral region within the angular
space, thereby emphasizing clean samples while suppressing noisy samples. We
present a unified analysis of ExpFace and classical margin-based softmax losses
in terms of margin embedding forms, similarity curves, and gradient curves,
showing that ExpFace not only avoids the training instability of SphereFace and
the non-monotonicity of ArcFace, but also exhibits a similarity curve that
applies penalties in the same manner as the decision boundary in the angular
space. Extensive experiments demonstrate that ExpFace achieves state-of-the-art
performance. To facilitate future research, we have released the source code
at: https://github.com/dfr-code/ExpFace.

</details>


### [99] [Logics-Parsing Technical Report](https://arxiv.org/abs/2509.19760)
*Xiangyang Chen,Shuzhao Li,Xiuwen Zhu,Yongfan Chen,Fan Yang,Cheng Fang,Lin Qu,Xiaoxiao Xu,Hu Wei,Minggang Wu*

Main category: cs.CV

TL;DR: This paper introduces Logics-Parsing, an LVLM-based model with reinforcement learning to enhance document parsing capabilities for complex layouts and reading order inference.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome limitations of existing LVLM paradigms in handling complex document types, which are hindered by missing analytical stages for layouts and reading orders.

Method: The proposed model, Logics-Parsing, utilizes reinforcement learning with well-crafted reward mechanisms for layout and reading order optimization. It also incorporates diverse data types in supervised fine-tuning.

Result: Logics-Parsing demonstrates State-of-the-Art (SOTA) performance on diverse document parsing tasks, validated through experiments using a curated benchmark dataset, LogicsParsingBench.

Conclusion: The study concludes that Logics-Parsing significantly enhances document parsing in various complex scenarios, providing a robust solution supported by extensive evaluation and a new benchmark dataset.

Abstract: Recent advances in Large Vision-Language models (LVLM) have spurred
significant progress in document parsing task. Compared to traditional
pipeline-based methods, end-to-end paradigms have shown their excellence in
converting PDF images into structured outputs through integrated Optical
Character Recognition (OCR), table recognition, mathematical formula
recognition and so on. However, the absence of explicit analytical stages for
document layouts and reading orders limits the LVLM's capability in handling
complex document types such as multi-column newspapers or posters. To address
this limitation, we propose in this report Logics-Parsing: an end-to-end
LVLM-based model augmented with reinforcement learning. Our model incorporates
meticulously designed reward mechanisms to optimize complex layout analysis and
reading order inference. In addition, we expand the model's versatility by
incorporating diverse data types such as chemical formulas and handwritten
Chinese characters into supervised fine-tuning. Finally, to enable rigorous
evaluation of our approach, we introduce LogicsParsingBench, a curated set of
1,078 page-level PDF images spanning nine major categories and over twenty
sub-categories, which will be released later. Comprehensive experiments
conducted on LogicsParsingBench have validated the efficacy and
State-of-the-art (SOTA) performance of our proposed model across diverse
document analysis scenarios. Project Page:
https://github.com/alibaba/Logics-Parsing

</details>


### [100] [Sex-based Bias Inherent in the Dice Similarity Coefficient: A Model Independent Analysis for Multiple Anatomical Structures](https://arxiv.org/abs/2509.19778)
*Hartmut Häntze,Myrthe Buser,Alessa Hering,Lisa C. Adams,Keno K. Bressem*

Main category: cs.CV

TL;DR: The study reveals that the Dice Similarity Coefficient (DSC) introduces sex-based biases in segmentation evaluation, penalizing smaller structures more heavily, resulting in lower scores for women due to smaller organ sizes.


<details>
  <summary>Details</summary>
Motivation: To investigate whether the Dice Similarity Coefficient (DSC), a common segmentation evaluation metric, introduces bias based on sex due to differences in organ sizes.

Method: Synthetic errors were deliberately applied to manual MRI annotations from 50 participants to analyze segmentation performance differences in DSC scores between sexes under controlled conditions.

Result: Systematic DSC differences were found between sexes, with notable disparities for smaller structures but negligible differences for larger organs. Small structures showed differences around 0.03, while medium-sized structures exhibited differences around 0.01.

Conclusion: The DSC metric itself introduces bias, and thus fairness analyses should account for this metric-induced bias in order to achieve more accurate and fair evaluations in medical image segmentation.

Abstract: Overlap-based metrics such as the Dice Similarity Coefficient (DSC) penalize
segmentation errors more heavily in smaller structures. As organ size differs
by sex, this implies that a segmentation error of equal magnitude may result in
lower DSCs in women due to their smaller average organ volumes compared to men.
While previous work has examined sex-based differences in models or datasets,
no study has yet investigated the potential bias introduced by the DSC itself.
This study quantifies sex-based differences of the DSC and the normalized DSC
in an idealized setting independent of specific models. We applied
equally-sized synthetic errors to manual MRI annotations from 50 participants
to ensure sex-based comparability. Even minimal errors (e.g., a 1 mm boundary
shift) produced systematic DSC differences between sexes. For small structures,
average DSC differences were around 0.03; for medium-sized structures around
0.01. Only large structures (i.e., lungs and liver) were mostly unaffected,
with sex-based DSC differences close to zero. These findings underline that
fairness studies using the DSC as an evaluation metric should not expect
identical scores between men and women, as the metric itself introduces bias. A
segmentation model may perform equally well across sexes in terms of error
magnitude, even if observed DSC values suggest otherwise. Importantly, our work
raises awareness of a previously underexplored source of sex-based differences
in segmentation performance. One that arises not from model behavior, but from
the metric itself. Recognizing this factor is essential for more accurate and
fair evaluations in medical image analysis.

</details>


### [101] [EfficienT-HDR: An Efficient Transformer-Based Framework via Multi-Exposure Fusion for HDR Reconstruction](https://arxiv.org/abs/2509.19779)
*Yu-Shen Huang,Tzu-Han Chen,Cheng-Yen Hsiao,Shaou-Gang Miaou*

Main category: cs.CV

TL;DR: The paper introduces a lightweight Vision Transformer for HDR image reconstruction, reducing computational demand and avoiding artifacts, tailored for resource-constrained edge devices.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of high computational costs and ghosting artifacts in HDR imaging for edge devices, which impact applications like autonomous driving and surveillance.

Method: The authors propose a Vision Transformer using Context-Aware processing, color-space conversion, and innovative tools like IAAF, IRE, DyT, and E-MSDC to enhance efficiency and image quality.

Result: The proposed models reduce computational costs significantly—67% fewer FLOPS, up to 5x speedup on CPU, and 2.5x improvement on edge devices—while maintaining high-quality HDR imaging.

Conclusion: The study successfully achieves efficient, ghost-free HDR imaging suitable for various dynamic scenarios on edge devices, proving practical and versatile.

Abstract: Achieving high-quality High Dynamic Range (HDR) imaging on
resource-constrained edge devices is a critical challenge in computer vision,
as its performance directly impacts downstream tasks such as intelligent
surveillance and autonomous driving. Multi-Exposure Fusion (MEF) is a
mainstream technique to achieve this goal; however, existing methods generally
face the dual bottlenecks of high computational costs and ghosting artifacts,
hindering their widespread deployment. To this end, this study proposes a
light-weight Vision Transformer architecture designed explicitly for HDR
reconstruction to overcome these limitations. This study is based on the
Context-Aware Vision Transformer and begins by converting input images to the
YCbCr color space to separate luminance and chrominance information. It then
employs an Intersection-Aware Adaptive Fusion (IAAF) module to suppress
ghosting effectively. To further achieve a light-weight design, we introduce
Inverted Residual Embedding (IRE), Dynamic Tanh (DyT), and propose Enhanced
Multi-Scale Dilated Convolution (E-MSDC) to reduce computational complexity at
multiple levels. Our study ultimately contributes two model versions: a main
version for high visual quality and a light-weight version with advantages in
computational efficiency, both of which achieve an excellent balance between
performance and image quality. Experimental results demonstrate that, compared
to the baseline, the main version reduces FLOPS by approximately 67% and
increases inference speed by more than fivefold on CPU and 2.5 times on an edge
device. These results confirm that our method provides an efficient and
ghost-free HDR imaging solution for edge devices, demonstrating versatility and
practicality across various dynamic scenarios.

</details>


### [102] [BiTAA: A Bi-Task Adversarial Attack for Object Detection and Depth Estimation via 3D Gaussian Splatting](https://arxiv.org/abs/2509.19793)
*Yixun Zhang,Feng Zhou,Jianqin Yin*

Main category: cs.CV

TL;DR: This paper introduces BiTAA, a dual-task adversarial attack that disrupts object detection and biases monocular depth estimation using 3D Gaussian Splatting.


<details>
  <summary>Details</summary>
Motivation: Current adversarial attacks for autonomous driving are task-specific, lack depth bias control, and fail to explore cross-task interactions between object detection and depth estimation.

Method: BiTAA employs a dual-model attack framework, a composite loss function, and supports both full-image and patch-based perturbations. It also introduces a unified evaluation protocol for cross-task transfer metrics.

Result: Experiments demonstrate consistent cross-task degradation, effective depth misperception control, and asymmetry in transfer from detection to depth compared to depth to detection.

Conclusion: This work identifies practical risks in multi-task camera-based perception systems and underscores the need for cross-task-aware defenses in autonomous driving scenarios.

Abstract: Camera-based perception is critical to autonomous driving yet remains
vulnerable to task-specific adversarial manipulations in object detection and
monocular depth estimation. Most existing 2D/3D attacks are developed in task
silos, lack mechanisms to induce controllable depth bias, and offer no
standardized protocol to quantify cross-task transfer, leaving the interaction
between detection and depth underexplored. We present BiTAA, a bi-task
adversarial attack built on 3D Gaussian Splatting that yields a single
perturbation capable of simultaneously degrading detection and biasing
monocular depth. Specifically, we introduce a dual-model attack framework that
supports both full-image and patch settings and is compatible with common
detectors and depth estimators, with optional expectation-over-transformation
(EOT) for physical reality. In addition, we design a composite loss that
couples detection suppression with a signed, magnitude-controlled log-depth
bias within regions of interest (ROIs) enabling controllable near or far
misperception while maintaining stable optimization across tasks. We also
propose a unified evaluation protocol with cross-task transfer metrics and
real-world evaluations, showing consistent cross-task degradation and a clear
asymmetry between Det to Depth and from Depth to Det transfer. The results
highlight practical risks for multi-task camera-only perception and motivate
cross-task-aware defenses in autonomous driving scenarios.

</details>


### [103] [StrCGAN: A Generative Framework for Stellar Image Restoration](https://arxiv.org/abs/2509.19805)
*Shantanusinh Parmar*

Main category: cs.CV

TL;DR: StrCGAN is a generative model improving astrophotography by enhancing low-resolution images using 3D convolutions, spectral fusion, and astrophysical regularization.


<details>
  <summary>Details</summary>
Motivation: Low-resolution astrophotography struggles to represent celestial objects accurately due to small-telescope limitations and existing models like CycleGAN causing distortions.

Method: StrCGAN innovates CycleGAN with 3D convolutional layers, multi-spectral fusion, and astrophysical regularization. Training uses multi-mission all-sky surveys for spectral consistency.

Result: StrCGAN produces visually sharper and physically consistent reconstructions, outperforming standard GANs in astrophysical image enhancement.

Conclusion: StrCGAN effectively addresses the limitations of traditional image-to-image translation for astrophotography, ensuring high-quality and accurate celestial reconstructions.

Abstract: We introduce StrCGAN (Stellar Cyclic GAN), a generative model designed to
enhance low-resolution astrophotography images. Our goal is to reconstruct
high-fidelity ground truth-like representations of celestial objects, a task
that is challenging due to the limited resolution and quality of
small-telescope observations such as the MobilTelesco dataset. Traditional
models such as CycleGAN provide a foundation for image-to-image translation but
are restricted to 2D mappings and often distort the morphology of stars and
galaxies. To overcome these limitations, we extend the CycleGAN framework with
three key innovations: 3D convolutional layers to capture volumetric spatial
correlations, multi-spectral fusion to align optical and near-infrared (NIR)
domains, and astrophysical regularization modules to preserve stellar
morphology. Ground-truth references from multi-mission all-sky surveys spanning
optical to NIR guide the training process, ensuring that reconstructions remain
consistent across spectral bands. Together, these components allow StrCGAN to
generate reconstructions that are not only visually sharper but also physically
consistent, outperforming standard GAN models in the task of astrophysical
image enhancement.

</details>


### [104] [Adaptive Model Ensemble for Continual Learning](https://arxiv.org/abs/2509.19819)
*Yuchuan Mao,Zhi Gao,Xiaomeng Fan,Yuwei Wu,Yunde Jia,Chenchen Jing*

Main category: cs.CV

TL;DR: The paper presents Meta-weight-ensembler, a meta-learning-based model ensemble approach to address task- and layer-level knowledge conflicts in continual learning, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To handle catastrophic forgetting in continual learning by resolving task- and layer-level knowledge conflicts during model ensemble.

Method: Proposes Meta-weight-ensembler, which uses a meta-learning-trained mixing coefficient generator. The generator adapts layer-specific coefficients for model ensemble, enabling efficient integration of knowledge from different tasks.

Result: Meta-weight-ensembler outperforms existing approaches by effectively alleviating catastrophic forgetting and improving performance on multiple continual learning datasets.

Conclusion: Meta-weight-ensembler is a versatile approach that can be integrated into existing continual learning frameworks, offering better task performance and knowledge retention.

Abstract: Model ensemble is an effective strategy in continual learning, which
alleviates catastrophic forgetting by interpolating model parameters, achieving
knowledge fusion learned from different tasks. However, existing model ensemble
methods usually encounter the knowledge conflict issue at task and layer
levels, causing compromised learning performance in both old and new tasks. To
solve this issue, we propose meta-weight-ensembler that adaptively fuses
knowledge of different tasks for continual learning. Concretely, we employ a
mixing coefficient generator trained via meta-learning to generate appropriate
mixing coefficients for model ensemble to address the task-level knowledge
conflict. The mixing coefficient is individually generated for each layer to
address the layer-level knowledge conflict. In this way, we learn the prior
knowledge about adaptively accumulating knowledge of different tasks in a fused
model, achieving efficient learning in both old and new tasks.
Meta-weight-ensembler can be flexibly combined with existing continual learning
methods to boost their ability of alleviating catastrophic forgetting.
Experiments on multiple continual learning datasets show that
meta-weight-ensembler effectively alleviates catastrophic forgetting and
achieves state-of-the-art performance.

</details>


### [105] [ThinkFake: Reasoning in Multimodal Large Language Models for AI-Generated Image Detection](https://arxiv.org/abs/2509.19841)
*Tai-Ming Huang,Wei-Tung Lin,Kai-Lung Hua,Wen-Huang Cheng,Junichi Yamagishi,Jun-Cheng Chen*

Main category: cs.CV

TL;DR: The paper introduces ThinkFake, a novel method for detecting AI-generated images that emphasizes reasoning and interpretability, outperforming existing methods and offering better generalization.


<details>
  <summary>Details</summary>
Motivation: The realistic nature of AI-generated images has raised concerns about misinformation and privacy, necessitating more accurate, interpretable, and generalizable detection methods.

Method: ThinkFake employs a Multimodal Large Language Model (MLLM) with a forgery reasoning prompt, trained using Group Relative Policy Optimization (GRPO) reinforcement learning and structured reward functions. It incorporates a structured detection pipeline for enhanced reasoning and adaptability.

Result: ThinkFake surpasses state-of-the-art methods on the GenImage benchmark and demonstrates robust zero-shot generalization on the LOKI benchmark.

Conclusion: The proposed ThinkFake method provides a highly effective, interpretable, and generalizable solution to identifying AI-generated images, with plans for open code release.

Abstract: The increasing realism of AI-generated images has raised serious concerns
about misinformation and privacy violations, highlighting the urgent need for
accurate and interpretable detection methods. While existing approaches have
made progress, most rely on binary classification without explanations or
depend heavily on supervised fine-tuning, resulting in limited generalization.
In this paper, we propose ThinkFake, a novel reasoning-based and generalizable
framework for AI-generated image detection. Our method leverages a Multimodal
Large Language Model (MLLM) equipped with a forgery reasoning prompt and is
trained using Group Relative Policy Optimization (GRPO) reinforcement learning
with carefully designed reward functions. This design enables the model to
perform step-by-step reasoning and produce interpretable, structured outputs.
We further introduce a structured detection pipeline to enhance reasoning
quality and adaptability. Extensive experiments show that ThinkFake outperforms
state-of-the-art methods on the GenImage benchmark and demonstrates strong
zero-shot generalization on the challenging LOKI benchmark. These results
validate our framework's effectiveness and robustness. Code will be released
upon acceptance.

</details>


### [106] [PersONAL: Towards a Comprehensive Benchmark for Personalized Embodied Agents](https://arxiv.org/abs/2509.19843)
*Filippo Ziliotto,Jelin Raphael Akkara,Alessandro Daniele,Lamberto Ballan,Luciano Serafini,Tommaso Campari*

Main category: cs.CV

TL;DR: The paper introduces PersONAL, a benchmark focusing on personalized object identification and navigation in human-centric environments for Embodied AI.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of deploying Embodied AI agents in realistic, human-centered scenarios where individualized preferences and behaviors must be considered.

Method: Developed PersONAL, comprising over 2,000 episodes with natural-language object-user associations in photorealistic environments, and evaluated agent capabilities in navigation and object grounding.

Result: State-of-the-art baselines show significant gaps compared to human performance, demonstrating the difficulty in reasoning and memorizing personalized information.

Conclusion: PersONAL advances the study of personalized Embodied AI, emphasizing the need for agents capable of interacting effectively in human-centered contexts.

Abstract: Recent advances in Embodied AI have enabled agents to perform increasingly
complex tasks and adapt to diverse environments. However, deploying such agents
in realistic human-centered scenarios, such as domestic households, remains
challenging, particularly due to the difficulty of modeling individual human
preferences and behaviors. In this work, we introduce PersONAL (PERSonalized
Object Navigation And Localization, a comprehensive benchmark designed to study
personalization in Embodied AI. Agents must identify, retrieve, and navigate to
objects associated with specific users, responding to natural-language queries
such as "find Lily's backpack". PersONAL comprises over 2,000 high-quality
episodes across 30+ photorealistic homes from the HM3D dataset. Each episode
includes a natural-language scene description with explicit associations
between objects and their owners, requiring agents to reason over user-specific
semantics. The benchmark supports two evaluation modes: (1) active navigation
in unseen environments, and (2) object grounding in previously mapped scenes.
Experiments with state-of-the-art baselines reveal a substantial gap to human
performance, highlighting the need for embodied agents capable of perceiving,
reasoning, and memorizing over personalized information; paving the way towards
real-world assistive robot.

</details>


### [107] [FreezeVLA: Action-Freezing Attacks against Vision-Language-Action Models](https://arxiv.org/abs/2509.19870)
*Xin Wang,Jie Li,Zejia Weng,Yixu Wang,Yifeng Gao,Tianyu Pang,Chao Du,Yan Teng,Yingchun Wang,Zuxuan Wu,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: This paper introduces a critical vulnerability in Vision-Language-Action (VLA) models, where adversarial images can freeze the robot's actions, presenting a significant safety risk.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the unexplored safety and robustness concerns in VLA models, especially their susceptibility to adversarial attacks that can disconnect robot decision-making from physical actions.

Method: FreezeVLA, a novel attack framework based on min-max bi-level optimization, is proposed to systematically study and evaluate action-freezing attacks on robots.

Result: Experimental results across three VLA models and four robotic benchmarks indicate that FreezeVLA achieves a 76.2% attack success rate, outperforming existing attack methods and demonstrating strong transferability of adversarial images.

Conclusion: The study uncovers a pressing safety vulnerability in VLA models and emphasizes the necessity for developing effective defensive strategies to prevent critical robotic inaction.

Abstract: Vision-Language-Action (VLA) models are driving rapid progress in robotics by
enabling agents to interpret multimodal inputs and execute complex,
long-horizon tasks. However, their safety and robustness against adversarial
attacks remain largely underexplored. In this work, we identify and formalize a
critical adversarial vulnerability in which adversarial images can "freeze" VLA
models and cause them to ignore subsequent instructions. This threat
effectively disconnects the robot's digital mind from its physical actions,
potentially inducing inaction during critical interventions. To systematically
study this vulnerability, we propose FreezeVLA, a novel attack framework that
generates and evaluates action-freezing attacks via min-max bi-level
optimization. Experiments on three state-of-the-art VLA models and four robotic
benchmarks show that FreezeVLA attains an average attack success rate of 76.2%,
significantly outperforming existing methods. Moreover, adversarial images
generated by FreezeVLA exhibit strong transferability, with a single image
reliably inducing paralysis across diverse language prompts. Our findings
expose a critical safety risk in VLA models and highlight the urgent need for
robust defense mechanisms.

</details>


### [108] [Adaptive Guidance Semantically Enhanced via Multimodal LLM for Edge-Cloud Object Detection](https://arxiv.org/abs/2509.19875)
*Yunqing Hu,Zheming Yang,Chang Zhao,Wen Ji*

Main category: cs.CV

TL;DR: This paper proposes a semantic-enhanced edge-cloud collaborative object detection method using Multimodal Large Language Models to address challenges in complex scenarios like low-light conditions and occlusions.


<details>
  <summary>Details</summary>
Motivation: Traditional object detection struggles in complex scenarios due to limited semantic understanding. This study aims to improve accuracy and efficiency in such conditions.

Method: The method uses instruction fine-tuned MLLMs for generating structured scene descriptions and an adaptive mapping to translate semantic data into parameter adjustments. It employs an edge-cloud framework to toggle cloud-based or edge-based detections dynamically.

Result: The proposed method improves both detection accuracy and efficiency, reducing latency by 79% and computational cost by 70% in challenging scenarios.

Conclusion: An MLLM-based adaptive semantic guidance approach significantly enhances object detection under challenging conditions, effectively balancing performance and resource usage.

Abstract: Traditional object detection methods face performance degradation challenges
in complex scenarios such as low-light conditions and heavy occlusions due to a
lack of high-level semantic understanding. To address this, this paper proposes
an adaptive guidance-based semantic enhancement edge-cloud collaborative object
detection method leveraging Multimodal Large Language Models (MLLM), achieving
an effective balance between accuracy and efficiency. Specifically, the method
first employs instruction fine-tuning to enable the MLLM to generate structured
scene descriptions. It then designs an adaptive mapping mechanism that
dynamically converts semantic information into parameter adjustment signals for
edge detectors, achieving real-time semantic enhancement. Within an edge-cloud
collaborative inference framework, the system automatically selects between
invoking cloud-based semantic guidance or directly outputting edge detection
results based on confidence scores. Experiments demonstrate that the proposed
method effectively enhances detection accuracy and efficiency in complex
scenes. Specifically, it can reduce latency by over 79% and computational cost
by 70% in low-light and highly occluded scenes while maintaining accuracy.

</details>


### [109] [Generalized Shortest Path-based Superpixels for 3D Spherical Image Segmentation](https://arxiv.org/abs/2509.19895)
*Rémi Giraud,Rodrigo Borba Pinheiro,Yannick Berthoumieu*

Main category: cs.CV

TL;DR: SphSPS, a novel superpixel method for 360° images, improves segmentation accuracy and regularity by incorporating spherical geometry.


<details>
  <summary>Details</summary>
Motivation: To address the need for fast and accurate superpixel-based segmentation methods tailored specifically for wide 360-degree spherical or omnidirectional images.

Method: Introduced SphSPS, a spherical superpixel segmentation method that incorporates the geometry of the spherical 3D acquisition space and uses the notion of shortest path-based clustering.

Result: SphSPS outperformed existing planar and spherical methods in segmentation accuracy, noise robustness, and regularity in tests on 360-degree spherical panorama datasets and synthetic road omnidirectional images.

Conclusion: Considering the spherical geometry significantly improves segmentation accuracy and superpixel regularity, making SphSPS a promising tool for analyzing 360-degree images.

Abstract: The growing use of wide angle image capture devices and the need for fast and
accurate image analysis in computer visions have enforced the need for
dedicated under-representation approaches. Most recent decomposition methods
segment an image into a small number of irregular homogeneous regions, called
superpixels. Nevertheless, these approaches are generally designed to segment
standard 2D planar images, i.e., captured with a 90o angle view without
distortion. In this work, we introduce a new general superpixel method called
SphSPS (for Spherical Shortest Path-based Superpixels)1 , dedicated to wide
360o spherical or omnidirectional images. Our method respects the geometry of
the 3D spherical acquisition space and generalizes the notion of shortest path
between a pixel and a superpixel center, to fastly extract relevant clustering
features. We demonstrate that considering the geometry of the acquisition space
to compute the shortest path enables to jointly improve the segmentation
accuracy and the shape regularity of superpixels. To evaluate this regularity
aspect, we also generalize a global regularity metric to the spherical space,
addressing the limitations of the only existing spherical compactness measure.
Finally, the proposed SphSPS method is validated on the reference 360o
spherical panorama segmentation dataset and on synthetic road omnidirectional
images. Our method significantly outperforms both planar and spherical
state-of-the-art approaches in terms of segmentation accuracy,robustness to
noise and regularity, providing a very interesting tool for superpixel-based
applications on 360o images.

</details>


### [110] [Efficient Cell Painting Image Representation Learning via Cross-Well Aligned Masked Siamese Network](https://arxiv.org/abs/2509.19896)
*Pin-Jui Huang,Yu-Hsuan Liao,SooHeon Kim,NoSeong Park,JongBae Park,DongMyung Shin*

Main category: cs.CV

TL;DR: The paper introduces a new framework, Cross-Well Aligned Masked Siamese Network (CWA-MSN), to improve cellular phenotypic representation learning overcoming batch effects, while being efficient in data and parameters.


<details>
  <summary>Details</summary>
Motivation: Accelerate drug discovery by creating effective computational models for predicting cellular responses to chemical and genetic perturbations that avoid the challenges of batch effects and inefficiency in conventional learning methods.

Method: Developed CWA-MSN, a representation learning framework that aligns embeddings of similarly treated cells across experimental batches within a masked siamese architecture to ensure semantic consistency while being computationally efficient.

Result: CWA-MSN outperforms existing approaches (OpenPhenom and CellCLIP) in a gene-gene relationship retrieval task with significantly fewer data (0.2M vs. 2.2M images) and smaller model size (22M vs. 1.48B parameters), achieving improvements of +29% and +9% over the benchmarks, respectively.

Conclusion: CWA-MSN provides an efficient and robust method to learn cell image representation, overcoming batch effects and reducing data and parameter requirements, making it highly effective for phenotype modeling in drug discovery.

Abstract: Computational models that predict cellular phenotypic responses to chemical
and genetic perturbations can accelerate drug discovery by prioritizing
therapeutic hypotheses and reducing costly wet-lab iteration. However,
extracting biologically meaningful and batch-robust cell painting
representations remains challenging. Conventional self-supervised and
contrastive learning approaches often require a large-scale model and/or a huge
amount of carefully curated data, still struggling with batch effects. We
present Cross-Well Aligned Masked Siamese Network (CWA-MSN), a novel
representation learning framework that aligns embeddings of cells subjected to
the same perturbation across different wells, enforcing semantic consistency
despite batch effects. Integrated into a masked siamese architecture, this
alignment yields features that capture fine-grained morphology while remaining
data- and parameter-efficient. For instance, in a gene-gene relationship
retrieval benchmark, CWA-MSN outperforms the state-of-the-art publicly
available self-supervised (OpenPhenom) and contrastive learning (CellCLIP)
methods, improving the benchmark scores by +29\% and +9\%, respectively, while
training on substantially fewer data (e.g., 0.2M images for CWA-MSN vs. 2.2M
images for OpenPhenom) or smaller model size (e.g., 22M parameters for CWA-MSN
vs. 1.48B parameters for CellCLIP). Extensive experiments demonstrate that
CWA-MSN is a simple and effective way to learn cell image representation,
enabling efficient phenotype modeling even under limited data and parameter
budgets.

</details>


### [111] [Aerial-Ground Image Feature Matching via 3D Gaussian Splatting-based Intermediate View Rendering](https://arxiv.org/abs/2509.19898)
*Jiangxue Yu,Hui Wang,San Jiang,Xing Zhang,Dejin Zhang,Qingquan Li*

Main category: cs.CV

TL;DR: The paper presents a feature matching algorithm for 3D modeling using aerial and ground images, focusing on generating intermediate views to mitigate perspective distortions from large viewpoint differences.


<details>
  <summary>Details</summary>
Motivation: Aerial and ground images often face challenges in integration for 3D modeling due to perspective distortions from varying viewpoints. Reliable correspondences are essential for overcoming these issues, motivating this study's focus on feature matching.

Method: The method involves incremental SfM for sparse model reconstruction using aerial images, 3D Gaussian Splatting for scene rendering, and determining render viewpoints to create intermediate views for feature matching. Features are matched by bridging render, aerial, and ground views.

Result: Experimental results show improved feature matches, an increase in both initial and refined matches, accurate ISfM reconstruction, and high-quality 3D Gaussian Splatting-based scene rendering.

Conclusion: The proposed algorithm effectively enhances feature matching between aerial and ground images, facilitates accurate 3D modeling, and outperforms existing methods in terms of match quantity and scene rendering quality.

Abstract: The integration of aerial and ground images has been a promising solution in
3D modeling of complex scenes, which is seriously restricted by finding
reliable correspondences. The primary contribution of this study is a feature
matching algorithm for aerial and ground images, whose core idea is to generate
intermediate views to alleviate perspective distortions caused by the extensive
viewpoint changes. First, by using aerial images only, sparse models are
reconstructed through an incremental SfM (Structure from Motion) engine due to
their large scene coverage. Second, 3D Gaussian Splatting is then adopted for
scene rendering by taking as inputs sparse points and oriented images. For
accurate view rendering, a render viewpoint determination algorithm is designed
by using the oriented camera poses of aerial images, which is used to generate
high-quality intermediate images that can bridge the gap between aerial and
ground images. Third, with the aid of intermediate images, reliable feature
matching is conducted for match pairs from render-aerial and render-ground
images, and final matches can be generated by transmitting correspondences
through intermediate views. By using real aerial and ground datasets, the
validation of the proposed solution has been verified in terms of feature
matching and scene rendering and compared comprehensively with widely used
methods. The experimental results demonstrate that the proposed solution can
provide reliable feature matches for aerial and ground images with an obvious
increase in the number of initial and refined matches, and it can provide
enough matches to achieve accurate ISfM reconstruction and complete 3DGS-based
scene rendering.

</details>


### [112] [CapStARE: Capsule-based Spatiotemporal Architecture for Robust and Efficient Gaze Estimation](https://arxiv.org/abs/2509.19936)
*Miren Samaniego,Igor Rodriguez,Elena Lazkano*

Main category: cs.CV

TL;DR: The paper introduces CapStARE, a real-time gaze estimation model combining advanced architecture elements and achieving state-of-the-art performance on multiple datasets.


<details>
  <summary>Details</summary>
Motivation: To address the need for efficient, real-time gaze estimation solutions adaptable to various conditions and interactions.

Method: CapStARE integrates a ConvNeXt backbone, capsule formation with attention routing, and dual GRU decoders for specialized slow and rapid gaze dynamics modeling.

Result: CapStARE achieved state-of-the-art results on ETH-XGaze, MPIIFaceGaze, generalized well on Gaze360 and RT-GENE, with fewer parameters and good interpretability.

Conclusion: The model proves to be a practical and robust solution for real-time gaze estimation in interactive systems, outperforming or matching existing methods.

Abstract: We introduce CapStARE, a capsule-based spatio-temporal architecture for gaze
estimation that integrates a ConvNeXt backbone, capsule formation with
attention routing, and dual GRU decoders specialized for slow and rapid gaze
dynamics. This modular design enables efficient part-whole reasoning and
disentangled temporal modeling, achieving state-of-the-art performance on
ETH-XGaze (3.36) and MPIIFaceGaze (2.65) while maintaining real-time inference
(< 10 ms). The model also generalizes well to unconstrained conditions in
Gaze360 (9.06) and human-robot interaction scenarios in RT-GENE (4.76),
outperforming or matching existing methods with fewer parameters and greater
interpretability. These results demonstrate that CapStARE offers a practical
and robust solution for real-time gaze estimation in interactive systems. The
related code and results for this article can be found on:
https://github.com/toukapy/capsStare

</details>


### [113] [GS-RoadPatching: Inpainting Gaussians via 3D Searching and Placing for Driving Scenes](https://arxiv.org/abs/2509.19937)
*Guo Chen,Jiarun Liu,Sicong Du,Chenming Wu,Deqi Li,Shi-Sheng Huang,Guofeng Zhang,Sheng Yang*

Main category: cs.CV

TL;DR: GS-RoadPatching introduces a novel approach to driving scene completion using 3D Gaussian Splatting rather than relying on 2D view-based diffusion or GAN models, achieving superior visual quality and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing inpainting methods face limitations when relying on 2D perspective-driven models, including spatial-temporal inconsistency and retraining overhead. Driving scenes often demand repetitive pattern matching that better suits 3DGS techniques.

Method: The method leverages feature-embedded 3D Gaussian Splatting scenes for abstracting local context through patch measurements and employs structural search for candidate patches. A substitution-and-fusion optimization strategy ensures visual harmony.

Result: Experiments on driving scene datasets demonstrate state-of-the-art quality and efficiency of the approach. Additional trials in general scenes affirm its applicability beyond driving contexts.

Conclusion: GS-RoadPatching refines the use of 3DGS for scene inpainting, offering a superior alternative to 2D generative methods by exploiting multi-modal patterns inherent in driving scenes.

Abstract: This paper presents GS-RoadPatching, an inpainting method for driving scene
completion by referring to completely reconstructed regions, which are
represented by 3D Gaussian Splatting (3DGS). Unlike existing 3DGS inpainting
methods that perform generative completion relying on 2D perspective-view-based
diffusion or GAN models to predict limited appearance or depth cues for missing
regions, our approach enables substitutional scene inpainting and editing
directly through the 3DGS modality, extricating it from requiring
spatial-temporal consistency of 2D cross-modals and eliminating the need for
time-intensive retraining of Gaussians. Our key insight is that the highly
repetitive patterns in driving scenes often share multi-modal similarities
within the implicit 3DGS feature space and are particularly suitable for
structural matching to enable effective 3DGS-based substitutional inpainting.
Practically, we construct feature-embedded 3DGS scenes to incorporate a patch
measurement method for abstracting local context at different scales and,
subsequently, propose a structural search method to find candidate patches in
3D space effectively. Finally, we propose a simple yet effective
substitution-and-fusion optimization for better visual harmony. We conduct
extensive experiments on multiple publicly available datasets to demonstrate
the effectiveness and efficiency of our proposed method in driving scenes, and
the results validate that our method achieves state-of-the-art performance
compared to the baseline methods in terms of both quality and interoperability.
Additional experiments in general scenes also demonstrate the applicability of
the proposed 3D inpainting strategy. The project page and code are available
at: https://shanzhaguoo.github.io/GS-RoadPatching/

</details>


### [114] [Interpreting ResNet-based CLIP via Neuron-Attention Decomposition](https://arxiv.org/abs/2509.19943)
*Edmund Bu,Yossi Gandelsman*

Main category: cs.CV

TL;DR: This paper introduces a technique for interpreting neurons in CLIP-ResNet by analyzing computation paths, enabling improved semantic segmentation and dataset distribution monitoring.


<details>
  <summary>Details</summary>
Motivation: To better understand and interpret the inner workings of CLIP-ResNet neurons, aiming to enhance its applications and usability.

Method: The authors decompose neuron contributions into computation paths by analyzing neuron-head pairs and associating these pairs with text in the embedding space.

Result: Neuron-head pairs were found to contribute significantly and sparsely to outputs, enabling semantic segmentation and dataset monitoring tasks to outperform prior methods.

Conclusion: Examining individual computation paths within neural networks reveals interpretable units that are beneficial for downstream applications.

Abstract: We present a novel technique for interpreting the neurons in CLIP-ResNet by
decomposing their contributions to the output into individual computation
paths. More specifically, we analyze all pairwise combinations of neurons and
the following attention heads of CLIP's attention-pooling layer. We find that
these neuron-head pairs can be approximated by a single direction in
CLIP-ResNet's image-text embedding space. Leveraging this insight, we interpret
each neuron-head pair by associating it with text. Additionally, we find that
only a sparse set of the neuron-head pairs have a significant contribution to
the output value, and that some neuron-head pairs, while polysemantic,
represent sub-concepts of their corresponding neurons. We use these
observations for two applications. First, we employ the pairs for training-free
semantic segmentation, outperforming previous methods for CLIP-ResNet. Second,
we utilize the contributions of neuron-head pairs to monitor dataset
distribution shifts. Our results demonstrate that examining individual
computation paths in neural networks uncovers interpretable units, and that
such units can be utilized for downstream tasks.

</details>


### [115] [When Words Can't Capture It All: Towards Video-Based User Complaint Text Generation with Multimodal Video Complaint Dataset](https://arxiv.org/abs/2509.19952)
*Sarmistha Das,R E Zera Marveen Lyngkhoi,Kirtan Jain,Vinayak Goyal,Sriparna Saha,Manish Gupta*

Main category: cs.CV

TL;DR: The paper introduces the task of generating complaint descriptions from videos (CoD-V), alongside a new dataset (ComVID) and a model for this purpose.


<details>
  <summary>Details</summary>
Motivation: Many users struggle to express their complaints clearly in text, despite being able to provide illustrative videos, leading to unresolved issues. This gap signifies a need for better tools to convert videos into articulate complaints.

Method: The authors formulated the CoD-V task and introduced the ComVID dataset of complaint videos and descriptions. They also developed a Retrieval-Augmented Generation (RAG) embedded VideoLLaMA2-7b model, considering the complainant's emotional state, and evaluated it with metrics like METEOR and readability scores.

Result: The study provided the first dataset and benchmark for CoD-V, introduced a new evaluation metric called complaint retention (CR), and demonstrated that multimodal models can generate quality complaint descriptions.

Conclusion: The research establishes a new direction in complaint mining, enabling users to transform video complaints into articulate textual descriptions. The provided dataset and resources pave the way for future advancements.

Abstract: While there exists a lot of work on explainable complaint mining,
articulating user concerns through text or video remains a significant
challenge, often leaving issues unresolved. Users frequently struggle to
express their complaints clearly in text but can easily upload videos depicting
product defects (e.g., vague text such as `worst product' paired with a
5-second video depicting a broken headphone with the right earcup). This paper
formulates a new task in the field of complaint mining to aid the common users'
need to write an expressive complaint, which is Complaint Description from
Videos (CoD-V) (e.g., to help the above user articulate her complaint about the
defective right earcup). To this end, we introduce ComVID, a video complaint
dataset containing 1,175 complaint videos and the corresponding descriptions,
also annotated with the emotional state of the complainer. Additionally, we
present a new complaint retention (CR) evaluation metric that discriminates the
proposed (CoD-V) task against standard video summary generation and description
tasks. To strengthen this initiative, we introduce a multimodal
Retrieval-Augmented Generation (RAG) embedded VideoLLaMA2-7b model, designed to
generate complaints while accounting for the user's emotional state. We conduct
a comprehensive evaluation of several Video Language Models on several tasks
(pre-trained and fine-tuned versions) with a range of established evaluation
metrics, including METEOR, perplexity, and the Coleman-Liau readability score,
among others. Our study lays the foundation for a new research direction to
provide a platform for users to express complaints through video. Dataset and
resources are available at: https://github.com/sarmistha-D/CoD-V.

</details>


### [116] [SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding](https://arxiv.org/abs/2509.19965)
*Phyo Thet Yee,Dimitrios Kollias,Sudeepta Mishra,Abhinav Dhall*

Main category: cs.CV

TL;DR: This paper introduces SynchroRaMa, a new system for generating emotionally expressive and realistic talking face videos by leveraging multi-modal emotion embeddings from text and audio, as well as dynamic scene inputs.


<details>
  <summary>Details</summary>
Motivation: Most existing methods for talking face generation rely on single modalities for emotion embedding and use single reference images, limiting their ability to generate expressive and dynamic outputs.

Method: The proposed SynchroRaMa system combines emotion embeddings from text sentiment analysis and audio features, integrates an audio-to-motion module for synchronization, and utilizes scene descriptions generated by Large Language Models (LLMs) for dynamic attribute control.

Result: SynchroRaMa outperforms current state-of-the-art methods in terms of image quality, expression accuracy, and motion realism. Both quantitative and qualitative experiments, as well as user studies, validate its superior performance in naturalness, motion diversity, and smoothness.

Conclusion: This framework advances talking face generation by addressing limitations of single-modality emotion embedding and temporal inconsistency, enabling more expressive and realistic talking face videos.

Abstract: Audio-driven talking face generation has received growing interest,
particularly for applications requiring expressive and natural human-avatar
interaction. However, most existing emotion-aware methods rely on a single
modality (either audio or image) for emotion embedding, limiting their ability
to capture nuanced affective cues. Additionally, most methods condition on a
single reference image, restricting the model's ability to represent dynamic
changes in actions or attributes across time. To address these issues, we
introduce SynchroRaMa, a novel framework that integrates a multi-modal emotion
embedding by combining emotional signals from text (via sentiment analysis) and
audio (via speech-based emotion recognition and audio-derived valence-arousal
features), enabling the generation of talking face videos with richer and more
authentic emotional expressiveness and fidelity. To ensure natural head motion
and accurate lip synchronization, SynchroRaMa includes an audio-to-motion (A2M)
module that generates motion frames aligned with the input audio. Finally,
SynchroRaMa incorporates scene descriptions generated by Large Language Model
(LLM) as additional textual input, enabling it to capture dynamic actions and
high-level semantic attributes. Conditioning the model on both visual and
textual cues enhances temporal consistency and visual realism. Quantitative and
qualitative experiments on benchmark datasets demonstrate that SynchroRaMa
outperforms the state-of-the-art, achieving improvements in image quality,
expression preservation, and motion realism. A user study further confirms that
SynchroRaMa achieves higher subjective ratings than competing methods in
overall naturalness, motion diversity, and video smoothness. Our project page
is available at <https://novicemm.github.io/synchrorama>.

</details>


### [117] [OmniScene: Attention-Augmented Multimodal 4D Scene Understanding for Autonomous Driving](https://arxiv.org/abs/2509.19973)
*Pei Liu,Hongliang Lu,Haichao Liu,Haipeng Liu,Xin Liu,Ruoyu Yao,Shengbo Eben Li,Jun Ma*

Main category: cs.CV

TL;DR: The paper introduces "OmniScene," a human-like framework for 4D scene understanding in autonomous driving systems, combining multimodal perception and dynamic fusion strategies to outperform state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: Autonomous driving systems lack true 3D egocentric scene understanding akin to human vision, hindering complex decision-making capabilities.

Method: The authors developed OmniScene, incorporating OmniVLM (a vision-language model), knowledge distillation, and a hierarchical fusion strategy for integrating geometric and semantic features across modalities.

Result: OmniScene consistently surpasses ten state-of-the-art models in various tasks such as perception, planning, prediction, and visual question answering on the nuScenes dataset.

Conclusion: OmniScene introduces an advanced framework that bridges the gap between human-like scene understanding and autonomous driving systems, enhancing their adaptability and decision-making capabilities.

Abstract: Human vision is capable of transforming two-dimensional observations into an
egocentric three-dimensional scene understanding, which underpins the ability
to translate complex scenes and exhibit adaptive behaviors. This capability,
however, remains lacking in current autonomous driving systems, where
mainstream approaches primarily rely on depth-based 3D reconstruction rather
than true scene understanding. To address this limitation, we propose a novel
human-like framework called OmniScene. First, we introduce the OmniScene
Vision-Language Model (OmniVLM), a vision-language framework that integrates
multi-view and temporal perception for holistic 4D scene understanding. Then,
harnessing a teacher-student OmniVLM architecture and knowledge distillation,
we embed textual representations into 3D instance features for semantic
supervision, enriching feature learning, and explicitly capturing human-like
attentional semantics. These feature representations are further aligned with
human driving behaviors, forming a more human-like
perception-understanding-action architecture. In addition, we propose a
Hierarchical Fusion Strategy (HFS) to address imbalances in modality
contributions during multimodal integration. Our approach adaptively calibrates
the relative significance of geometric and semantic features at multiple
abstraction levels, enabling the synergistic use of complementary cues from
visual and textual modalities. This learnable dynamic fusion enables a more
nuanced and effective exploitation of heterogeneous information. We evaluate
OmniScene comprehensively on the nuScenes dataset, benchmarking it against over
ten state-of-the-art models across various tasks. Our approach consistently
achieves superior results, establishing new benchmarks in perception,
prediction, planning, and visual question answering.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [118] [Characterizing Adaptive Mesh Refinement on Heterogeneous Platforms with Parthenon-VIBE](https://arxiv.org/abs/2509.19701)
*Akash Poptani,Alireza Khadem,Scott Mahlke,Jonah Miller,Joshua Dolence,Reetuparna Das*

Main category: cs.DC

TL;DR: This paper evaluates the performance challenges of the Parthenon AMR benchmark on CPU-GPU systems and offers optimizations to enhance efficiency.


<details>
  <summary>Details</summary>
Motivation: Hero-class HPC simulations use Adaptive Mesh Refinement to balance computational and memory requirements with accurate results. Analyzing its performance on modern heterogeneous systems is critical for effective deployment.

Method: The authors analyzed Parthenon's behavior on CPU-GPU systems via profiling tools, examined communication bottlenecks and serial overheads, and proposed optimizations to reduce inefficiencies.

Result: Smaller mesh blocks and deeper AMR levels were found to degrade GPU performance due to communication overheads, serial inefficiencies, and memory bottlenecks. Profiling identified low occupancy and inefficiencies in GPU memory access.

Conclusion: Insights and proposed optimizations from this study aim to improve GPU efficiency and inform future Adaptive Mesh Refinement implementations on heterogeneous supercomputers.

Abstract: Hero-class HPC simulations rely on Adaptive Mesh Refinement (AMR) to reduce
compute and memory demands while maintaining accuracy. This work analyzes the
performance of Parthenon, a block-structured AMR benchmark, on CPU-GPU systems.
We show that smaller mesh blocks and deeper AMR levels degrade GPU performance
due to increased communication, serial overheads, and inefficient GPU
utilization. Through detailed profiling, we identify inefficiencies, low
occupancy, and memory access bottlenecks. We further analyze rank scalability
and memory constraints, and propose optimizations to improve GPU throughput and
reduce memory footprint. Our insights can inform future AMR deployments on
Department of Energy's upcoming heterogeneous supercomputers.

</details>


### [119] [Investigating Sharding Advancements, Methodologies, and Adoption Potential in Hedera](https://arxiv.org/abs/2509.19478)
*Ziwei Wang,Cong Wu,Paolo Tasca*

Main category: cs.DC

TL;DR: The paper explores sharding solutions for blockchain scalability, focusing on Hedera and proposing a hybrid sharding model to improve performance and security.


<details>
  <summary>Details</summary>
Motivation: To address the scalability challenges in blockchain networks by evaluating sharding methods and their applicability to Hedera's architecture.

Method: The study reviews academic and industrial sharding techniques and proposes a hybrid solution combining local and global committees with dynamic reconfiguration for efficiency and security.

Result: The proposed model demonstrated reduced storage and communication overhead, improved scalability, and enhanced fault tolerance for Hedera.

Conclusion: Integrating sharding into Hedera's architecture is feasible and offers significant advantages in scalability, efficiency, and security.

Abstract: Sharding has emerged as a critical solution to address the scalability
challenges faced by blockchain networks, enabling them to achieve higher
transaction throughput, reduced latency, and optimized resource usage. This
paper investigates the advancements, methodologies, and adoption potential of
sharding in the context of Hedera, a distributed ledger technology known for
its unique Gossip about Gossip protocol and asynchronous Byzantine Fault
Tolerance (ABFT). We explore various academic and industrial sharding
techniques, emphasizing their benefits and trade-offs. Building on these
insights, we propose a hybrid sharding solution for Hedera that partitions the
network into local and global committees, facilitating efficient cross-shard
transactions and ensuring robust security through dynamic reconfiguration. Our
analysis highlights significant reductions in storage and communication
overhead, improved scalability, and enhanced fault tolerance, demonstrating the
feasibility and advantages of integrating sharding into Hedera's architecture.

</details>


### [120] [To Stream or Not to Stream: Towards A Quantitative Model for Remote HPC Processing Decisions](https://arxiv.org/abs/2509.19532)
*Flavio Castro,Weijian Zheng,Joaquin Chung,Ian Foster,Rajkumar Kettimuthu*

Main category: cs.DC

TL;DR: Modern scientific instruments face difficulties in processing and transferring high-rate data locally and remotely using file-based systems, necessitating the evaluation of real-time streaming frameworks. This paper introduces a quantitative model to assess the feasibility and performance of streaming for remote data processing.


<details>
  <summary>Details</summary>
Motivation: Scientific instruments are producing data at rates that exceed local compute capabilities, and traditional file-based systems for remote high-performance computing (HPC) are impractical for real-time applications due to staging and I/O overheads.

Method: The paper presents a quantitative framework and a Streaming Speed Score to evaluate the feasibility of using remote HPC resources for real-time data processing. It factors in key metrics like data generation rate, transfer efficiency, remote processing power, and file I/O overhead.

Result: The methodology is validated through a case study on LCLS-II data, where streaming frameworks show up to 97% reduction in end-to-end processing time compared to file-based methods under high data rates. On the downside, worst-case network congestion severely impacts transfer times.

Conclusion: Streaming frameworks can be significantly more efficient than file-based methods for high data-rate environments, though susceptibility to network congestion must be considered in feasibility assessments.

Abstract: Modern scientific instruments generate data at rates that increasingly exceed
local compute capabilities and, when paired with the staging and I/O overheads
of file-based transfers, also render file-based use of remote HPC resources
impractical for time-sensitive analysis and experimental steering. Real-time
streaming frameworks promise to reduce latency and improve system efficiency,
but lack a principled way to assess their feasibility. In this work, we
introduce a quantitative framework and an accompanying Streaming Speed Score to
evaluate whether remote high-performance computing (HPC) resources can provide
timely data processing compared to local alternatives. Our model incorporates
key parameters including data generation rate, transfer efficiency, remote
processing power, and file input/output overhead to compute total processing
completion time and identify operational regimes where streaming is beneficial.
We motivate our methodology with use cases from facilities such as APS, FRIB,
LCLS-II, and the LHC, and validate our approach through an illustrative case
study based on LCLS-II data. Our measurements show that streaming can achieve
up to 97% lower end-to-end completion time than file-based methods under high
data rates, while worst-case congestion can increase transfer times by over an
order of magnitude, underscoring the importance of tail latency in streaming
feasibility decisions.

</details>


### [121] [A Survey of Recent Advancements in Secure Peer-to-Peer Networks](https://arxiv.org/abs/2509.19539)
*Raj Patel,Umesh Biswas,Surya Kodipaka,Will Carroll,Preston Peranich,Maxwell Young*

Main category: cs.DC

TL;DR: This paper provides an updated review of security advancements in peer-to-peer networks, addressing traditional threats and exploring new challenges driven by emerging technologies.


<details>
  <summary>Details</summary>
Motivation: The existing survey on P2P network security is outdated, and there is a need to examine how emerging trends impact the solutions to classic and new threats.

Method: An extensive review of recent theoretical advancements in P2P network security was conducted, evaluating solutions to known issues and challenges introduced by new technologies.

Result: The research identifies the strengths and weaknesses of recent security solutions for P2P networks and acknowledges how emerging trends shape these developments.

Conclusion: The study emphasizes the ongoing challenges in P2P security and suggests future research directions to tackle threats effectively in light of technological evolution.

Abstract: Peer-to-peer (P2P) networks are a cornerstone of modern computing, and their
security is an active area of research. Many defenses with strong security
guarantees have been proposed; however, the most-recent survey is over a decade
old. This paper delivers an updated review of recent theoretical advances that
address classic threats, such as the Sybil and routing attacks, while
highlighting how emerging trends -- such as machine learning, social networks,
and dynamic systems -- pose new challenges and drive novel solutions. We
evaluate the strengths and weaknesses of these solutions and suggest directions
for future research.

</details>


### [122] [Gyges: Dynamic Cross-Instance Parallelism Transformation for Efficient LLM Inference](https://arxiv.org/abs/2509.19729)
*Haoyu Chen,Xue Li,Kun Qian,Yu Guan,Jin Zhao,Xin Wang*

Main category: cs.DC

TL;DR: The paper introduces Gyges, a method to dynamically adapt parallelism strategies for LLM serving, ensuring better throughput despite handling varying request context lengths.


<details>
  <summary>Details</summary>
Motivation: The paper addresses inefficiencies caused by context length variance in LLM serving, where adapting to large contexts degrades throughput due to GPU parallelism trade-offs.

Method: The authors propose Gyges, which includes (1) a novel KV cache layout, (2) optimized weight padding, and (3) a transformation-aware scheduler to adjust parallelism strategies dynamically.

Result: Gyges achieves a 1.75x-6.57x improvement in throughput compared to existing state-of-the-art methods when tested on real-world datasets.

Conclusion: Dynamic parallelism strategies can significantly enhance throughput in LLM-serving scenarios, solving key challenges with large context lengths using Gyges.

Abstract: Efficiently processing the dynamics of requests, especially the context
length variance, is important in Large Language Model (LLM) serving scenarios.
However, there is an intrinsic trade-off: while leveraging parallelism
strategies, such as Tensor Parallelism (TP), can coordinate multiple GPUs to
accommodate larger context lengths, it inevitably results in degraded overall
throughput. In this paper, we propose Cross-Instance Parallelism Transformation
(Gyges), which adaptively adjusts the parallelism strategies of running
instances to align with the dynamics of incoming requests. We design (1) a
page-friendly, header-centric layout to accelerate KV cache transformations;
(2) dedicated weight padding to accelerate model weight transformations; and
(3) a transformation-aware scheduler to cooperatively schedule requests and
parallelism transformations, optimizing the overall performance. Evaluations
using real-world traces show that Gyges improves throughput by 1.75x-6.57x
compared to state-of-the-art solutions.

</details>


### [123] [BurstEngine: an Efficient Distributed Framework for Training Transformers on Extremely Long Sequences of over 1M Tokens](https://arxiv.org/abs/2509.19836)
*Ao Sun,Weilin Zhao,Xu Han,Cheng Yang,Zhiyuan Liu,Chuan Shi,Maosong sun*

Main category: cs.DC

TL;DR: BurstEngine is presented as a novel framework to efficiently train Large Language Models (LLMs) on extremely long sequences, exceeding 1M tokens. It achieves significant improvements in speed and memory usage compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods like Tensor Parallelism suffer from inefficiencies in FLOP utilization and scalability when handling long sequences and multiple GPUs, especially when sequence lengths surpass 1M tokens.

Method: BurstEngine integrates several innovations such as BurstAttention with optimized communication costs, selective checkpointing, fused loss function, and workload balance optimization for attention masking.

Result: BurstEngine outperforms state-of-the-art methods, achieving a 1.2x speedup and lower memory overhead for training tasks involving sequences longer than 1M tokens.

Conclusion: BurstEngine provides a scalable and memory-efficient solution for training LLMs on extremely long sequence data, backed by strong empirical improvements and public availability of the code.

Abstract: Existing methods for training LLMs on long-sequence data, such as Tensor
Parallelism and Context Parallelism, exhibit low Model FLOPs Utilization as
sequence lengths and number of GPUs increase, especially when sequence lengths
exceed 1M tokens. To address these challenges, we propose BurstEngine, an
efficient framework designed to train LLMs on long-sequence data. BurstEngine
introduces BurstAttention, an optimized distributed attention with lower
communication cost than RingAttention. BurstAttention leverages topology-aware
ring communication to fully utilize network bandwidth and incorporates
fine-grained communication-computation overlap. Furthermore, BurstEngine
introduces sequence-level selective checkpointing and fuses the language
modeling head with the loss function to reduce memory cost. Additionally,
BurstEngine introduces workload balance optimization for various types of
attention masking. By integrating these optimizations, BurstEngine achieves a
$1.2\times$ speedup with much lower memory overhead than the state-of-the-art
baselines when training LLMs on extremely long sequences of over 1M tokens. We
have made our code publicly available on GitHub:
https://github.com/thunlp/BurstEngine.

</details>


### [124] [Characterizing the Performance of Accelerated Jetson Edge Devices for Training Deep Learning Models](https://arxiv.org/abs/2509.20160)
*Prashanthi S. K.,Sai Anuroop Kesanapalli,Yogesh Simmhan*

Main category: cs.DC

TL;DR: This paper studies DNN training on edge devices, focusing on resource utilization and providing insights for optimization.


<details>
  <summary>Details</summary>
Motivation: Edge devices are increasingly used for DNN inferencing, but their ability to handle DNN training is poorly understood. This paper aims to address that gap.

Method: The authors performed experiments on Jetson devices using diverse DNN models and datasets, varying parameters like mini-batch sizes and power modes.

Result: The study reveals resource inter-dependencies, unexpected insights, and provides predictive models for training time and energy efficiency.

Conclusion: This analysis aids in optimizing DNN training on edge devices and selecting suitable hardware, with potential applications in federated learning.

Abstract: Deep Neural Networks (DNNs) have had a significant impact on domains like
autonomous vehicles and smart cities through low-latency inferencing on edge
computing devices close to the data source. However, DNN training on the edge
is poorly explored. Techniques like federated learning and the growing capacity
of GPU-accelerated edge devices like NVIDIA Jetson motivate the need for a
holistic characterization of DNN training on the edge. Training DNNs is
resource-intensive and can stress an edge's GPU, CPU, memory and storage
capacities. Edge devices also have different resources compared to workstations
and servers, such as slower shared memory and diverse storage media. Here, we
perform a principled study of DNN training on individual devices of three
contemporary Jetson device types: AGX Xavier, Xavier NX and Nano for three
diverse DNN model--dataset combinations. We vary device and training parameters
such as I/O pipelining and parallelism, storage media, mini-batch sizes and
power modes, and examine their effect on CPU and GPU utilization, fetch stalls,
training time, energy usage, and variability. Our analysis exposes several
resource inter-dependencies and counter-intuitive insights, while also helping
quantify known wisdom. Our rigorous study can help tune the training
performance on the edge, trade-off time and energy usage on constrained
devices, and even select an ideal edge hardware for a DNN workload, and, in
future, extend to federated learning too. As an illustration, we use these
results to build a simple model to predict the training time and energy per
epoch for any given DNN across different power modes, with minimal additional
profiling.

</details>


### [125] [Pagoda: An Energy and Time Roofline Study for DNN Workloads on Edge Accelerators](https://arxiv.org/abs/2509.20189)
*Prashanthi S. K.,Kunal Kumar Sahoo,Amartya Ranjan Saikia,Pranav Gupta,Atharva Vinay Joshi,Priyanshu Pansari,Yogesh Simmhan*

Main category: cs.DC

TL;DR: The paper introduces analytical models to examine DNN workload performance and energy efficiency on Nvidia Jetson devices and demonstrates optimization techniques.


<details>
  <summary>Details</summary>
Motivation: To address the lack of analysis explaining why edge accelerators perform as they do across various power modes, enabling informed optimizations.

Method: Developed analytical time and energy roofline models to study Nvidia Jetson's power modes and coupled them with FLOP and memory access analysis for DNN workloads.

Result: Insights include counter-intuitive findings like MAXN mode not being most energy-efficient. Optimizations reduce energy consumption by up to 15% with minimal latency impact.

Conclusion: Roofline models help understand and optimize DNN workloads on edge accelerators, improving energy and performance trade-offs.

Abstract: Edge accelerators such as Nvidia Jetsons are becoming an integral part of the
computing continuum, and are often used for DNN inferencing and training.
Nvidia Jetson edge devices have $2000$+ CUDA cores within a $70$W power
envelope and offer $1000$s of power modes to customize CPU, GPU and memory
frequencies. Their widely varying power--performance trade-offs can be
exploited for energy and power-constrained deployments. While data-driven
methods to predict the power and latency of DNN workloads for edge devices
exist, there is a lack of principled study to understand why edge accelerators
and their power modes perform the way they do. We develop a time roofline and a
novel energy roofline model for the Jetson Orin AGX for diverse power modes,
and couple it with an analytical model of the compute (FLOP) and memory access
(bytes) for DNN inference workloads to analyze them from first principles.
These reveal unique, sometimes counter-intuitive, insights into the power and
performance behavior of DNN workloads on edge accelerators, e.g., the default
power mode MAXN is not the most energy efficient and time efficiency implies
energy efficiency for all power modes. We also extend our analytical roofline
models to DNN training. Finally, we apply these methods to tune the power mode
(and hence the roofline) of the edge device to optimize the latency and energy
for DNN inference, with up to $15\%$ lower energy and minimal degradation in
inference time.

</details>


### [126] [Fulcrum: Optimizing Concurrent DNN Training and Inferencing on Edge Accelerators](https://arxiv.org/abs/2509.20205)
*Prashanthi S. K.,Saisamarth Taluri,Pranav Gupta,Amartya Ranjan Saikia,Kunal Kumar Sahoo,Atharva Vinay Joshi,Lakshya Karwa,Kedar Dhule,Yogesh Simmhan*

Main category: cs.DC

TL;DR: The paper addresses efficient time-slicing for GPU sharing between training and inference on Nvidia Jetson edge devices, proposing optimization methods and scheduling strategies.


<details>
  <summary>Details</summary>
Motivation: The rise of GPU-accelerated edge devices for concurrent DNN inference and training, along with privacy concerns, necessitates efficient resource sharing and power-performance optimization.

Method: This paper formulates an optimization problem to interleave workloads and applies multi-dimensional gradient descent (GMD) and active learning (ALS) to identify Pareto-optimal power modes with limited profiling.

Result: The proposed strategies, ALS and GMD, outperform baselines and meet power and latency budgets for over 97% of runs, reaching throughput within 7% of optimal.

Conclusion: An intelligent scheduler combining ALS and GMD achieves efficient utilization of Jetson devices for concurrent DNN tasks, offering a balance of performance and profiling efficiency.

Abstract: The proliferation of GPU accelerated edge devices like Nvidia Jetsons and the
rise in privacy concerns are placing an emphasis on concurrent DNN training and
inferencing on edge devices. Inference and training have different computing
and QoS goals. But edge accelerators like Jetson do not support native GPU
sharing and expose 1000s of power modes. This requires careful time-sharing of
concurrent workloads to meet power--performance goals, while limiting costly
profiling. In this paper, we design an intelligent time-slicing approach for
concurrent DNN training and inferencing on Jetsons. We formulate an
optimization problem to interleave training and inferencing minibatches, and
decide the device power mode and inference minibatch size, while maximizing the
training throughput and staying within latency and power budgets, with modest
profiling costs. We propose GMD, an efficient multi-dimensional gradient
descent search which profiles just $15$ power modes; and ALS, an Active
Learning technique which identifies reusable Pareto-optimal power modes, but
profiles $50$--$150$ power modes. We evaluate these within our Fulcrum
scheduler for $273,000+$ configurations across $15$ DNN workloads. We also
evaluate our strategies on dynamic arrival inference and concurrent inferences.
ALS and GMD outperform simpler and more complex baselines with larger-scale
profiling. Their solutions satisfy the latency and power budget for $>97\%$ of
our runs, and on average are within $7\%$ of the optimal throughput.

</details>


### [127] [An Empirical Analysis of Secure Federated Learning for Autonomous Vehicle Applications](https://arxiv.org/abs/2509.20223)
*Md Jueal Mia,M. Hadi Amini*

Main category: cs.DC

TL;DR: The study explores the use of secure aggregation techniques and multiparty computation to safeguard federated learning for autonomous vehicles against cyber-attacks, ensuring data privacy and model resilience.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address vulnerabilities in federated learning for autonomous vehicle applications, which are susceptible to cyber-attacks like poisoning and inference attacks.

Method: An empirical analysis using transportation image datasets (e.g., LISA traffic light) to test various secure aggregation techniques and multiparty computation under cyber-attack scenarios.

Result: The study investigates the resilience and effectiveness of federated learning security protocols in mitigating misclassification attacks and other vulnerabilities in autonomous vehicle applications.

Conclusion: Secure aggregation and multiparty computation are promising approaches to ensure the robustness and privacy of federated learning models for autonomous vehicles against diverse cyber threats.

Abstract: Federated Learning lends itself as a promising paradigm in enabling
distributed learning for autonomous vehicles applications and ensuring data
privacy while enhancing and refining predictive model performance through
collaborative training on edge client vehicles. However, it remains vulnerable
to various categories of cyber-attacks, necessitating more robust security
measures to effectively mitigate potential threats. Poisoning attacks and
inference attacks are commonly initiated within the federated learning
environment to compromise secure system performance. Secure aggregation can
limit the disclosure of sensitive information from outsider and insider
attackers of the federated learning environment. In this study, our aim is to
conduct an empirical analysis on the transportation image dataset (e.g., LISA
traffic light) using various secure aggregation techniques and multiparty
computation in the presence of diverse categories of cyber-attacks. Multiparty
computation serves as a state-of-the-art security mechanism, offering standard
privacy for secure aggregation of edge autonomous vehicles local model updates
through various security protocols. The presence of adversaries can mislead the
autonomous vehicle learning model, leading to the misclassification of traffic
lights, and resulting in detrimental impacts. This empirical study explores the
resilience of various secure federated learning aggregation techniques and
multiparty computation in safeguarding autonomous vehicle applications against
various cyber threats during both training and inference times.

</details>


### [128] [xGFabric: Coupling Sensor Networks and HPC Facilities with Private 5G Wireless Networks for Real-Time Digital Agriculture](https://arxiv.org/abs/2509.20340)
*Liubov Kurafeeva,Alan Subedi,Ryan Hartung,Michael Fay,Avhishek Biswas,Shantenu Jha,Ozgur O. Kilic,Chandra Krintz,Andre Merzky,Douglas Thain,Mehmet C. Vuran,Rich Wolski*

Main category: cs.DC

TL;DR: This paper introduces xGFabric, a solution combining sensor networks with HPC facilities through private 5G networks for real-time agricultural simulation.


<details>
  <summary>Details</summary>
Motivation: To address the need for coupling low-performance sensor networks with high-performance computing (HPC) for real-time applications in digital agriculture.

Method: The authors created xGFabric, a system leveraging private 5G network slicing to integrate remote sensors with HPC systems effectively.

Result: A prototype implementation of xGFabric successfully demonstrates real-time digital agricultural simulations by connecting sensor networks to HPC through 5G.

Conclusion: xGFabric showcases the potential of private 5G networks for enabling low-latency, reliable connections between distributed sensors and HPC systems for advanced applications like digital agriculture.

Abstract: Advanced scientific applications require coupling distributed sensor networks
with centralized high-performance computing facilities. Citrus Under Protective
Screening (CUPS) exemplifies this need in digital agriculture, where citrus
research facilities are instrumented with numerous sensors monitoring
environmental conditions and detecting protective screening damage. CUPS
demands access to computational fluid dynamics codes for modeling environmental
conditions and guiding real-time interventions like water application or
robotic repairs. These computing domains have contrasting properties: sensor
networks provide low-performance, limited-capacity, unreliable data access,
while high-performance facilities offer enormous computing power through
high-latency batch processing. Private 5G networks present novel capabilities
addressing this challenge by providing low latency, high throughput, and
reliability necessary for near-real-time coupling of edge sensor networks with
HPC simulations. This work presents xGFabric, an end-to-end system coupling
sensor networks with HPC facilities through Private 5G networks. The prototype
connects remote sensors via 5G network slicing to HPC systems, enabling
real-time digital agriculture simulation.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [129] [Wavelet Fourier Diffuser: Frequency-Aware Diffusion Model for Reinforcement Learning](https://arxiv.org/abs/2509.19305)
*Yifu Luo,Yongzhe Chang,Xueqian Wang*

Main category: cs.LG

TL;DR: The paper introduces a novel reinforcement learning approach called WFDiffuser that leverages frequency-domain features to improve trajectory stability and decision-making.


<details>
  <summary>Details</summary>
Motivation: Existing offline reinforcement learning methods focus on time-domain features, often leading to frequency shift and degraded performance.

Method: WFDiffuser combines Discrete Wavelet Transform, Short-Time Fourier Transform, and cross attention mechanisms to integrate frequency-domain features and improve trajectory modeling.

Result: The proposed WFDiffuser significantly mitigates frequency shift and improves both trajectory stability and decision-making, as demonstrated on the D4RL benchmark.

Conclusion: Frequency-domain features are critical in reinforcement learning, and WFDiffuser provides a robust framework to harness them, outperforming existing time-domain-focused methods.

Abstract: Diffusion probability models have shown significant promise in offline
reinforcement learning by directly modeling trajectory sequences. However,
existing approaches primarily focus on time-domain features while overlooking
frequency-domain features, leading to frequency shift and degraded performance
according to our observation. In this paper, we investigate the RL problem from
a new perspective of the frequency domain. We first observe that
time-domain-only approaches inadvertently introduce shifts in the low-frequency
components of the frequency domain, which results in trajectory instability and
degraded performance. To address this issue, we propose Wavelet Fourier
Diffuser (WFDiffuser), a novel diffusion-based RL framework that integrates
Discrete Wavelet Transform to decompose trajectories into low- and
high-frequency components. To further enhance diffusion modeling for each
component, WFDiffuser employs Short-Time Fourier Transform and cross attention
mechanisms to extract frequency-domain features and facilitate cross-frequency
interaction. Extensive experiment results on the D4RL benchmark demonstrate
that WFDiffuser effectively mitigates frequency shift, leading to smoother,
more stable trajectories and improved decision-making performance over existing
methods.

</details>


### [130] [Anti-Money Laundering Systems Using Deep Learning](https://arxiv.org/abs/2509.19359)
*Mashkhal Abdalwahid Sidiq,Yimamu Kirubel Wondaferew*

Main category: cs.LG

TL;DR: This paper presents a deep learning-based approach to enhance Anti-Money Laundering (AML) efforts, overcoming the limitations of traditional rule-based systems.


<details>
  <summary>Details</summary>
Motivation: Traditional AML systems suffer from high false positives and fail to detect complex laundering schemes.

Method: The research employs deep learning techniques combined with link analysis and centrality algorithms (Degree Centrality, Closeness Centrality, Betweenness Centrality, PageRank) to detect money laundering in financial networks.

Result: The proposed system demonstrated the effectiveness of utilizing a Graph Convolutional Network (GCN) model for analyzing financial transactions within their contextual network.

Conclusion: The study establishes that integrating deep learning and centrality algorithms can significantly improve the performance of AML systems, offering a compelling alternative to traditional approaches.

Abstract: In this paper, we focused on using deep learning methods for detecting money
laundering in financial transaction networks, in order to demonstrate that it
can be used as a complement or instead of the more commonly used rule-based
systems and conventional Anti-Money Laundering (AML) systems. The paper
explores the pivotal role played by Anti-Money Laundering (AML) activities in
the global financial industry. It underscores the drawbacks of conventional AML
systems, which exhibit high rates of false positives and lack the
sophistication to uncover intricate money laundering schemes. To tackle these
challenges, the paper proposes an advanced AML system that capitalizes on link
analysis using deep learning techniques. At the heart of this system lies the
utilization of centrality algorithms like Degree Centrality, Closeness
Centrality, Betweenness Centrality, and PageRank. These algorithms enhance the
system's capability to identify suspicious activities by examining the
influence and interconnections within networks of financial transactions. The
significance of Anti-Money Laundering (AML) efforts within the global financial
sector is discussed in this paper. It highlights the limitations of traditional
AML systems. The results showed the practicality and superiority of the new
implementation of the GCN model, which is a preferable method for connectively
structured data, meaning that a transaction or account is analyzed in the
context of its financial environment. In addition, the paper delves into the
prospects of Anti-Money Laundering (AML) efforts, proposing the integration of
emerging technologies such as deep learning and centrality algorithms. This
integration holds promise for enhancing the effectiveness of AML systems by
refining their capabilities.

</details>


### [131] [DeepACTIF: Efficient Feature Attribution via Activation Traces in Neural Sequence Models](https://arxiv.org/abs/2509.19362)
*Benedikt W. Hosp*

Main category: cs.LG

TL;DR: DeepACTIF is a lightweight feature attribution method tailored for real-time applications, addressing inefficiencies in standard methods like Integrated Gradients or SHAP.


<details>
  <summary>Details</summary>
Motivation: Standard feature attribution methods are unsuitable for real-time use in time-series domains due to their high computational cost.

Method: DeepACTIF leverages internal activations of LSTM-based sequence models. It employs an inverse-weighted aggregation scheme to stabilize and emphasize feature importance.

Result: DeepACTIF preserves model accuracy under severe feature reduction and outperforms established methods like SHAP and Integrated Gradients in both accuracy and statistical robustness while reducing computational and memory demands.

Conclusion: DeepACTIF provides a computationally efficient and accurate solution for real-time feature attribution, suitable for edge devices like mobile XR headsets or health monitors.

Abstract: Feature attribution is essential for interpreting deep learning models,
particularly in time-series domains such as healthcare, biometrics, and
human-AI interaction. However, standard attribution methods, such as Integrated
Gradients or SHAP, are computationally intensive and not well-suited for
real-time applications. We present DeepACTIF, a lightweight and
architecture-aware feature attribution method that leverages internal
activations of sequence models to estimate feature importance efficiently.
Focusing on LSTM-based networks, we introduce an inverse-weighted aggregation
scheme that emphasises stability and magnitude of activations across time
steps. Our evaluation across three biometric gaze datasets shows that DeepACTIF
not only preserves predictive performance under severe feature reduction (top
10% of features) but also significantly outperforms established methods,
including SHAP, IG, and DeepLIFT, in terms of both accuracy and statistical
robustness. Using Wilcoxon signed-rank tests and effect size analysis, we
demonstrate that DeepACTIF yields more informative feature rankings with
significantly lower error across all top-k conditions (10 - 40%). Our
experiments demonstrate that DeepACTIF not only reduces computation time and
memory usage by orders of magnitude but also preserves model accuracy when
using only top-ranked features. That makes DeepACTIF a viable solution for
real-time interpretability on edge devices such as mobile XR headsets or
embedded health monitors.

</details>


### [132] [Analyzing the Impact of Credit Card Fraud on Economic Fluctuations of American Households Using an Adaptive Neuro-Fuzzy Inference System](https://arxiv.org/abs/2509.19363)
*Zhuqi Wang,Qinghe Zhang,Zhuopei Cheng*

Main category: cs.LG

TL;DR: The paper proposes an enhanced ANFIS model with wavelet decomposition and temporal attention mechanisms to address credit card fraud, achieving a 17.8% improvement in RMSE over conventional methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the growing threat of credit card fraud in the US, which impacts household economic stability and behavior.

Method: The method involves an enhanced Adaptive Neuro-Fuzzy Inference System (ANFIS) model that integrates multi-resolution wavelet decomposition to detect economic shock signals and a temporal attention encoder to improve long-term anomaly detection. This approach incorporates adaptive Gaussian membership functions and modular training for higher adaptability.

Result: The proposed model achieved a significant reduction of 17.8% in RMSE compared to existing neuro-fuzzy and LSTM-based models.

Conclusion: The enhanced model offers an advanced, more accurate approach to credit card fraud detection by effectively analyzing multi-scale patterns and improving inference capabilities.

Abstract: Credit card fraud is assuming growing proportions as a major threat to the
financial position of American household, leading to unpredictable changes in
household economic behavior. To solve this problem, in this paper, a new hybrid
analysis method is presented by using the Enhanced ANFIS. The model proposes
several advances of the conventional ANFIS framework and employs a
multi-resolution wavelet decomposition module and a temporal attention
mechanism. The model performs discrete wavelet transformations on historical
transaction data and macroeconomic indicators to generate localized economic
shock signals. The transformed features are then fed into a deep fuzzy rule
library which is based on Takagi-Sugeno fuzzy rules with adaptive Gaussian
membership functions. The model proposes a temporal attention encoder that
adaptively assigns weights to multi-scale economic behavior patterns,
increasing the effectiveness of relevance assessment in the fuzzy inference
stage and enhancing the capture of long-term temporal dependencies and
anomalies caused by fraudulent activities. The proposed method differs from
classical ANFIS which has fixed input-output relations since it integrates
fuzzy rule activation with the wavelet basis selection and the temporal
correlation weights via a modular training procedure. Experimental results show
that the RMSE was reduced by 17.8% compared with local neuro-fuzzy models and
conventional LSTM models.

</details>


### [133] [Unsupervised Outlier Detection in Audit Analytics: A Case Study Using USA Spending Data](https://arxiv.org/abs/2509.19366)
*Buhe Li,Berkay Kaplan,Maksym Lazirko,Aleksandr Kogan*

Main category: cs.LG

TL;DR: This study explores unsupervised outlier detection methods using U.S. federal spending data to improve audit analytics.


<details>
  <summary>Details</summary>
Motivation: The need for efficient and accurate anomaly detection methods in large-scale governmental datasets, where traditional auditing approaches often fail.

Method: Using data from DHHS, the study implements multiple outlier algorithms, including HBOS, Robust PCA, MCD, and KNN, and evaluates their performance using metrics like precision, recall, and F1 scores.

Result: Hybrid approaches combining different methods show improved robustness and accuracy in detecting anomalies in financial data.

Conclusion: Unsupervised learning techniques demonstrate substantial potential in enhancing audit quality and efficiency, offering valuable insights for auditors, policymakers, and researchers in financial oversight.

Abstract: This study investigates the effectiveness of unsupervised outlier detection
methods in audit analytics, utilizing USA spending data from the U.S.
Department of Health and Human Services (DHHS) as a case example. We employ and
compare multiple outlier detection algorithms, including Histogram-based
Outlier Score (HBOS), Robust Principal Component Analysis (PCA), Minimum
Covariance Determinant (MCD), and K-Nearest Neighbors (KNN) to identify
anomalies in federal spending patterns. The research addresses the growing need
for efficient and accurate anomaly detection in large-scale governmental
datasets, where traditional auditing methods may fall short. Our methodology
involves data preparation, algorithm implementation, and performance evaluation
using precision, recall, and F1 scores. Results indicate that a hybrid
approach, combining multiple detection strategies, enhances the robustness and
accuracy of outlier identification in complex financial data. This study
contributes to the field of audit analytics by providing insights into the
comparative effectiveness of various outlier detection models and demonstrating
the potential of unsupervised learning techniques in improving audit quality
and efficiency. The findings have implications for auditors, policymakers, and
researchers seeking to leverage advanced analytics in governmental financial
oversight and risk management.

</details>


### [134] [Representation-based Broad Hallucination Detectors Fail to Generalize Out of Distribution](https://arxiv.org/abs/2509.19372)
*Zuzanna Dubanowska,Maciej Żelaszczyk,Michał Brzozowski,Paolo Mandica,Michał Karpowicz*

Main category: cs.LG

TL;DR: This paper examines the inefficacy of current SOTA methods in detecting hallucinations, exposing reliance on spurious correlations and limited out-of-distribution generalization.


<details>
  <summary>Details</summary>
Motivation: To critically analyze and improve the effectiveness and evaluation of hallucination detection methods.

Method: Assessment of hallucination detection performance on RAGTruth dataset and comparison with supervised linear probes.

Result: State-of-the-art methods were found to rely on spurious correlation, perform poorly out-of-distribution, and add complexity without tangible benefits.

Conclusion: Current methods require improvement in evaluation, generalization, and can perform no better than simplified alternatives.

Abstract: We critically assess the efficacy of the current SOTA in hallucination
detection and find that its performance on the RAGTruth dataset is largely
driven by a spurious correlation with data. Controlling for this effect,
state-of-the-art performs no better than supervised linear probes, while
requiring extensive hyperparameter tuning across datasets. Out-of-distribution
generalization is currently out of reach, with all of the analyzed methods
performing close to random. We propose a set of guidelines for hallucination
detection and its evaluation.

</details>


### [135] [Uncertainty Quantification of Large Language Models using Approximate Bayesian Computation](https://arxiv.org/abs/2509.19375)
*Mridul Sharma,Adeetya Patel,Zaneta D' Souza,Samira Abbasgholizadeh Rahimi,Siva Reddy,Sreenath Madathil*

Main category: cs.LG

TL;DR: The paper introduces an Approximate Bayesian Computation (ABC) framework to enhance uncertainty expression in Large Language Models (LLMs), particularly targeting clinical applications.


<details>
  <summary>Details</summary>
Motivation: LLMs are overconfident and poorly calibrated, making them unsuitable for high-stakes domains like clinical diagnostics.

Method: The authors developed a likelihood-free Bayesian inference framework (ABC) that models LLMs as stochastic simulators to infer posterior probabilities.

Result: The ABC-based approach shows improvements in accuracy by up to 46.9%, reduces Brier scores by 74.4%, and enhances calibration metrics such as Expected Calibration Error (ECE) and predictive entropy.

Conclusion: ABC can significantly improve the reliability and calibration of LLMs, making them more suitable for critical applications like medical diagnostics.

Abstract: Despite their widespread applications, Large Language Models (LLMs) often
struggle to express uncertainty, posing a challenge for reliable deployment in
high stakes and safety critical domains like clinical diagnostics. Existing
standard baseline methods such as model logits and elicited probabilities
produce overconfident and poorly calibrated estimates. In this work, we propose
Approximate Bayesian Computation (ABC), a likelihood-free Bayesian inference,
based approach that treats LLMs as a stochastic simulator to infer posterior
distributions over predictive probabilities. We evaluate our ABC approach on
two clinically relevant benchmarks: a synthetic oral lesion diagnosis dataset
and the publicly available GretelAI symptom-to-diagnosis dataset. Compared to
standard baselines, our approach improves accuracy by up to 46.9\%, reduces
Brier scores by 74.4\%, and enhances calibration as measured by Expected
Calibration Error (ECE) and predictive entropy.

</details>


### [136] [Predictive Coding-based Deep Neural Network Fine-tuning for Computationally Efficient Domain Adaptation](https://arxiv.org/abs/2509.20269)
*Matteo Cardoni,Sam Leroux*

Main category: cs.LG

TL;DR: A hybrid methodology combines Backpropagation and Predictive Coding to enable efficient on-device domain adaptation for deep neural networks, addressing challenges in dynamic environments like sensor drift or lighting variations.


<details>
  <summary>Details</summary>
Motivation: Single static models struggle to adapt to changes in input data distribution in dynamic environments, necessitating new methodologies for continual model adaptation.

Method: The paper proposes training a neural network offline using Backpropagation for high initial performance and employing Predictive Coding for efficient online adaptation to dynamic input changes.

Result: Experimental results on MNIST and CIFAR-10 datasets showcase effective adaptation with reduced computational overhead using this hybrid approach.

Conclusion: The hybrid strategy offers a computationally efficient solution for maintaining deep neural network performance in dynamic environments, especially on resource-constrained edge devices.

Abstract: As deep neural networks are increasingly deployed in dynamic, real-world
environments, relying on a single static model is often insufficient. Changes
in input data distributions caused by sensor drift or lighting variations
necessitate continual model adaptation. In this paper, we propose a hybrid
training methodology that enables efficient on-device domain adaptation by
combining the strengths of Backpropagation and Predictive Coding. The method
begins with a deep neural network trained offline using Backpropagation to
achieve high initial performance. Subsequently, Predictive Coding is employed
for online adaptation, allowing the model to recover accuracy lost due to
shifts in the input data distribution. This approach leverages the robustness
of Backpropagation for initial representation learning and the computational
efficiency of Predictive Coding for continual learning, making it particularly
well-suited for resource-constrained edge devices or future neuromorphic
accelerators. Experimental results on the MNIST and CIFAR-10 datasets
demonstrate that this hybrid strategy enables effective adaptation with a
reduced computational overhead, offering a promising solution for maintaining
model performance in dynamic environments.

</details>


### [137] [Solving Freshness in RAG: A Simple Recency Prior and the Limits of Heuristic Trend Detection](https://arxiv.org/abs/2509.19376)
*Matthew Grofsky*

Main category: cs.LG

TL;DR: Two methods applied to tackle temporal failures in RAG systems on cybersecurity data, with mixed success.


<details>
  <summary>Details</summary>
Motivation: To address the issue of temporal failures in RAG systems, particularly focusing on ensuring recency and detecting topic evolution.

Method: Two methods were tested: a simple recency prior to ensure information freshness, and a clustering heuristic to detect topic evolution in cybersecurity data.

Result: The recency prior demonstrated excellent performance with an accuracy of 1.00. However, the clustering heuristic for topic evolution detection failed, achieving only a 0.08 F1-score.

Conclusion: While the recency prior is effective for tackling temporal issues, detecting topic evolution requires more advanced methods than simple heuristics.

Abstract: We address temporal failures in RAG systems using two methods on
cybersecurity data. A simple recency prior achieved an accuracy of 1.00 on
freshness tasks. In contrast, a clustering heuristic for topic evolution failed
(0.08 F1-score), showing trend detection requires methods beyond simple
heuristics.

</details>


### [138] [Learning from Observation: A Survey of Recent Advances](https://arxiv.org/abs/2509.19379)
*Returaj Burnwal,Hriday Mehta,Nirav Pravinbhai Bhatt,Balaraman Ravindran*

Main category: cs.LG

TL;DR: The paper surveys methods for imitation learning specifically focusing on learning from observation (LfO), where only expert state information is available instead of actions.


<details>
  <summary>Details</summary>
Motivation: Expert actions are often impractical to obtain in real-world applications, motivating the need for imitation learning methods that rely solely on expert state visitation data.

Method: The authors propose a framework that categorizes existing LfO approaches based on trajectory construction, assumptions, and design choices, while also exploring connections with related fields like offline RL, model-based RL, and hierarchical RL.

Result: The paper organizes the field of LfO and provides insights into existing methodologies, highlighting their categorization and relationships with other RL research areas.

Conclusion: A clear structure for understanding LfO methods is established, along with identification of open problems and future research opportunities in imitation learning and related fields.

Abstract: Imitation Learning (IL) algorithms offer an efficient way to train an agent
by mimicking an expert's behavior without requiring a reward function. IL
algorithms often necessitate access to state and action information from expert
demonstrations. Although expert actions can provide detailed guidance,
requiring such action information may prove impractical for real-world
applications where expert actions are difficult to obtain. To address this
limitation, the concept of learning from observation (LfO) or state-only
imitation learning (SOIL) has recently gained attention, wherein the imitator
only has access to expert state visitation information. In this paper, we
present a framework for LfO and use it to survey and classify existing LfO
methods in terms of their trajectory construction, assumptions and algorithm's
design choices. This survey also draws connections between several related
fields like offline RL, model-based RL and hierarchical RL. Finally, we use our
framework to identify open problems and suggest future research directions.

</details>


### [139] [TensLoRA: Tensor Alternatives for Low-Rank Adaptation](https://arxiv.org/abs/2509.19391)
*Axel Marmoret,Reda Bensaid,Jonathan Lys,Vincent Gripon,François Leduc-Primeau*

Main category: cs.LG

TL;DR: TensLoRA enhances LoRA by combining updates into higher-order tensors, providing flexible and efficient adaptation for transformers.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve upon LoRA's efficiency limitation by exploring tensor-based methodologies for adaptation in transformers.

Method: The authors propose TensLoRA, a framework that aggregates LoRA updates into higher-order tensors, enabling customizable compression rates tailored for specific tasks.

Result: Experiments on vision and language benchmarks demonstrate TensLoRA's adaptability, occasionally outperforming standard LoRA under similar parameter conditions.

Conclusion: TensLoRA provides a systematic and general formulation for tensor-based adaptations, enhancing parameter efficiency and task customization.

Abstract: Low-Rank Adaptation (LoRA) is widely used to efficiently adapt Transformers
by adding trainable low-rank matrices to attention projections. While
effective, these matrices are considered independent for each attention
projection (Query, Key, and Value) and each layer. Recent extensions have
considered joint, tensor-based adaptations, but only in limited forms and
without a systematic framework. We introduce TensLoRA, a unified framework that
aggregates LoRA updates into higher-order tensors and models a broad family of
tensor-based low-rank adaptations. Our formulation generalizes existing
tensor-based methods and enables mode-specific compression rates, allowing
parameter budgets to be tailored according to the modality and task.
Experiments on vision and language benchmarks reveal that the tensor
construction directly impacts performance, sometimes better than standard LoRA
under similar parameter counts.

</details>


### [140] [OmniFed: A Modular Framework for Configurable Federated Learning from Edge to HPC](https://arxiv.org/abs/2509.19396)
*Sahil Tyagi,Andrei Cozma,Olivera Kotevska,Feiyi Wang*

Main category: cs.LG

TL;DR: OmniFed is a modular framework for Federated Learning that offers flexibility in configuration, privacy options, and communication protocols, simplifying deployment across diverse environments.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for a robust, modular framework to facilitate Federated Learning deployment in situations where data is decentralized, and privacy is paramount.

Method: OmniFed is designed with a modular architecture allowing for customization of topology, orchestration, training logic, and privacy mechanisms, offering support for various protocols, algorithms, and compression strategies.

Result: The framework was evaluated using multiple models and algorithms to measure performance, demonstrating its capabilities for streamlining Federated Learning deployment.

Conclusion: OmniFed unifies different aspects of Federated Learning into a single framework with extension points for customization, making it versatile for edge and HPC applications while preserving core system integrity.

Abstract: Federated Learning (FL) is critical for edge and High Performance Computing
(HPC) where data is not centralized and privacy is crucial. We present OmniFed,
a modular framework designed around decoupling and clear separation of concerns
for configuration, orchestration, communication, and training logic. Its
architecture supports configuration-driven prototyping and code-level
override-what-you-need customization. We also support different topologies,
mixed communication protocols within a single deployment, and popular training
algorithms. It also offers optional privacy mechanisms including Differential
Privacy (DP), Homomorphic Encryption (HE), and Secure Aggregation (SA), as well
as compression strategies. These capabilities are exposed through well-defined
extension points, allowing users to customize topology and orchestration,
learning logic, and privacy/compression plugins, all while preserving the
integrity of the core system. We evaluate multiple models and algorithms to
measure various performance metrics. By unifying topology configuration,
mixed-protocol communication, and pluggable modules in one stack, OmniFed
streamlines FL deployment across heterogeneous environments. Github repository
is available at https://github.com/at-aaims/OmniFed.

</details>


### [141] [TimeMosaic: Temporal Heterogeneity Guided Time Series Forecasting via Adaptive Granularity Patch and Segment-wise Decoding](https://arxiv.org/abs/2509.19406)
*Kuiye Ding,Fanda Fan,Chunyi Hou,Zheya Wang,Lei Wang,Zhengxin Yang,Jianfeng Zhan*

Main category: cs.LG

TL;DR: TimeMosaic, a novel framework for multivariate time series forecasting, tackles challenges in temporal heterogeneity using adaptive patch embedding and segment-wise decoding strategies.


<details>
  <summary>Details</summary>
Motivation: Existing multivariate time series forecasting methods often fail to address the heterogeneity in local temporal dynamics and in decoding strategies, leading to inefficiencies in information-dense regions and inaccuracies in capturing short-term versus long-term complexities.

Method: TimeMosaic introduces adaptive patch embedding to modify segmentation granularity based on local information density while preserving continuity. It also employs segment-wise decoding to adapt predictions to horizon-specific requirements, treating different forecast horizons as separate subtasks.

Result: Evaluations on benchmark datasets show that TimeMosaic consistently improves forecasting accuracy compared to current methodologies. Furthermore, the model trained on a dataset with 321 billion observations achieves performance on par with state-of-the-art forecasting systems.

Conclusion: TimeMosaic effectively addresses the challenges of temporal heterogeneity in multivariate time series forecasting, yielding consistent and competitive results while leveraging adaptive methodologies for improved accuracy.

Abstract: Multivariate time series forecasting is essential in domains such as finance,
transportation, climate, and energy. However, existing patch-based methods
typically adopt fixed-length segmentation, overlooking the heterogeneity of
local temporal dynamics and the decoding heterogeneity of forecasting. Such
designs lose details in information-dense regions, introduce redundancy in
stable segments, and fail to capture the distinct complexities of short-term
and long-term horizons. We propose TimeMosaic, a forecasting framework that
aims to address temporal heterogeneity. TimeMosaic employs adaptive patch
embedding to dynamically adjust granularity according to local information
density, balancing motif reuse with structural clarity while preserving
temporal continuity. In addition, it introduces segment-wise decoding that
treats each prediction horizon as a related subtask and adapts to
horizon-specific difficulty and information requirements, rather than applying
a single uniform decoder. Extensive evaluations on benchmark datasets
demonstrate that TimeMosaic delivers consistent improvements over existing
methods, and our model trained on the large-scale corpus with 321 billion
observations achieves performance competitive with state-of-the-art TSFMs.

</details>


### [142] [Enhancing Credit Default Prediction Using Boruta Feature Selection and DBSCAN Algorithm with Different Resampling Techniques](https://arxiv.org/abs/2509.19408)
*Obu-Amoah Ampomah,Edmund Agyemang,Kofi Acheampong,Louis Agyekum*

Main category: cs.LG

TL;DR: The study assessed machine learning methods for credit default prediction, emphasizing class imbalance solutions like SMOTE, SMOTE-Tomek, and ADASYN, with GBM performing best in combination with Boruta and DBSCAN processes.


<details>
  <summary>Details</summary>
Motivation: Address class imbalance in credit default datasets and improve predictive performance for decision-making in financial systems.

Method: Comparison of resampling techniques (SMOTE, SMOTE-Tomek, and ADASYN) combined with traditional classifiers and ensemble methods, alongside feature selection using Boruta and outlier detection using DBSCAN.

Result: The Boruta+DBSCAN+SMOTE-Tomek+GBM classifier achieved strong metrics (F1-score: 82.56%, G-mean: 82.98%, ROC-AUC: 90.90%, PR-AUC: 91.85%), outperforming other models.

Conclusion: The study highlights SMOTE-Tomek paired with GBM and advanced preprocessing techniques as effective solutions for addressing credit default prediction challenges, supporting future adaptive system development.

Abstract: This study examines credit default prediction by comparing three techniques,
namely SMOTE, SMOTE-Tomek, and ADASYN, that are commonly used to address the
class imbalance problem in credit default situations. Recognizing that credit
default datasets are typically skewed, with defaulters comprising a much
smaller proportion than non-defaulters, we began our analysis by evaluating
machine learning (ML) models on the imbalanced data without any resampling to
establish baseline performance. These baseline results provide a reference
point for understanding the impact of subsequent balancing methods. In addition
to traditional classifiers such as Naive Bayes and K-Nearest Neighbors (KNN),
our study also explores the suitability of advanced ensemble boosting
algorithms, including Extreme Gradient Boosting (XGBoost), AdaBoost, Gradient
Boosting Machines (GBM), and Light GBM for credit default prediction using
Boruta feature selection and DBSCAN-based outlier detection, both before and
after resampling. A real-world credit default data set sourced from the
University of Cleveland ML Repository was used to build ML classifiers, and
their performances were tested. The criteria chosen to measure model
performance are the area under the receiver operating characteristic curve
(ROC-AUC), area under the precision-recall curve (PR-AUC), G-mean, and
F1-scores. The results from this empirical study indicate that the
Boruta+DBSCAN+SMOTE-Tomek+GBM classifier outperformed the other ML models
(F1-score: 82.56%, G-mean: 82.98%, ROC-AUC: 90.90%, PR-AUC: 91.85%) in a credit
default context. The findings establish a foundation for future progress in
creating more resilient and adaptive credit default systems, which will be
essential as credit-based transactions continue to rise worldwide.

</details>


### [143] [Energy Use of AI Inference: Efficiency Pathways and Test-Time Compute](https://arxiv.org/abs/2509.20241)
*Felipe Oviedo,Fiodar Kazhamiaka,Esha Choukse,Allen Kim,Amy Luers,Melanie Nakagawa,Ricardo Bianchini,Juan M. Lavista Ferres*

Main category: cs.LG

TL;DR: The paper introduces a method to estimate per-query energy use of large-scale language models (LLMs) and highlights efficiency strategies for reducing energy demands during AI inference workloads.


<details>
  <summary>Details</summary>
Motivation: The growing need for billions of AI queries and token-intensive workflows creates a pressing requirement for accurate energy consumption estimates to enable capacity planning, emissions tracking, and efficiency optimization.

Method: A bottom-up approach is proposed that calculates energy use per query based on token throughput, deploying realistic workloads on GPUs (H100 nodes) and accounting for practical conditions like PUE and GPU utilization.

Result: Median per-query energy use for frontier models (>200 billion parameters) is estimated at 0.34 Wh, with 15x token scaling scenarios increasing it to 4.32 Wh. Efficiency improvements could lead to 8-20x reductions in energy use.

Conclusion: Implementing efficiency measures at various levels (model, platform, hardware) can significantly decrease AI inference energy demands, aligning them with scaling patterns observed in data centers historically.

Abstract: As AI inference scales to billions of queries and emerging reasoning and
agentic workflows increase token demand, reliable estimates of per-query energy
use are increasingly important for capacity planning, emissions accounting, and
efficiency prioritization. Many public estimates are inconsistent and overstate
energy use, because they extrapolate from limited benchmarks and fail to
reflect efficiency gains achievable at scale. In this perspective, we introduce
a bottom-up methodology to estimate the per-query energy of large-scale LLM
systems based on token throughput. For models running on an H100 node under
realistic workloads, GPU utilization and PUE constraints, we estimate a median
energy per query of 0.34 Wh (IQR: 0.18-0.67) for frontier-scale models (>200
billion parameters). These results are consistent with measurements using
production-scale configurations and show that non-production estimates and
assumptions can overstate energy use by 4-20x. Extending to test-time scaling
scenarios with 15x more tokens per typical query, the median energy rises 13x
to 4.32 Wh, indicating that targeting efficiency in this regime will deliver
the largest fleet-wide savings. We quantify achievable efficiency gains at the
model, serving platform, and hardware levels, finding individual median
reductions of 1.5-3.5x in energy per query, while combined advances can
plausibly deliver 8-20x reductions. To illustrate the system-level impact, we
estimate the baseline daily energy use of a deployment serving 1 billion
queries to be 0.8 GWh/day. If 10% are long queries, demand could grow to 1.8
GWh/day. With targeted efficiency interventions, it falls to 0.9 GWh/day,
similar to the energy footprint of web search at that scale. This echoes how
data centers historically tempered energy growth through efficiency gains
during the internet and cloud build-up.

</details>


### [144] [Analyzing Uncertainty Quantification in Statistical and Deep Learning Models for Probabilistic Electricity Price Forecasting](https://arxiv.org/abs/2509.19417)
*Andreas Lebedev,Abhinav Das,Sven Pappert,Stephan Schlüter*

Main category: cs.LG

TL;DR: The study explores uncertainty quantification in probabilistic forecasting for German electricity prices, comparing statistical methods with deep learning approaches. Results highlight differences in model performances based on metrics.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for precise probabilistic forecasts in energy risk management, aiming to enhance uncertainty quantification methods due to their current limitations in capturing the full scope of uncertainty.

Method: Examines state-of-the-art statistical models (e.g., LEAR and GARCH) and deep learning models (e.g., DDNNs) augmented by techniques like Monte Carlo dropout, ensemble methods, and conformal prediction for electricity price forecasting.

Result: LEAR-based models perform consistently well, DDNNs show improvements with added uncertainty modeling, and conformal prediction is effective for capturing uncertainty.

Conclusion: All models perform competitively, but their efficiency is dependent on chosen forecasting metrics, highlighting the importance of the method and metric pairing in probabilistic forecasting.

Abstract: Precise probabilistic forecasts are fundamental for energy risk management,
and there is a wide range of both statistical and machine learning models for
this purpose. Inherent to these probabilistic models is some form of
uncertainty quantification. However, most models do not capture the full extent
of uncertainty, which arises not only from the data itself but also from model
and distributional choices. In this study, we examine uncertainty
quantification in state-of-the-art statistical and deep learning probabilistic
forecasting models for electricity price forecasting in the German market. In
particular, we consider deep distributional neural networks (DDNNs) and augment
them with an ensemble approach, Monte Carlo (MC) dropout, and conformal
prediction to account for model uncertainty. Additionally, we consider the
LASSO-estimated autoregressive (LEAR) approach combined with quantile
regression averaging (QRA), generalized autoregressive conditional
heteroskedasticity (GARCH), and conformal prediction. Across a range of
performance metrics, we find that the LEAR-based models perform well in terms
of probabilistic forecasting, irrespective of the uncertainty quantification
method. Furthermore, we find that DDNNs benefit from incorporating both data
and model uncertainty, improving both point and probabilistic forecasting.
Uncertainty itself appears to be best captured by the models using conformal
prediction. Overall, our extensive study shows that all models under
consideration perform competitively. However, their relative performance
depends on the choice of metrics for point and probabilistic forecasting.

</details>


### [145] [Probabilistic Runtime Verification, Evaluation and Risk Assessment of Visual Deep Learning Systems](https://arxiv.org/abs/2509.19419)
*Birk Torpmann-Hagen,Pål Halvorsen,Michael A. Riegler,Dag Johansen*

Main category: cs.LG

TL;DR: The paper introduces a methodology to improve the evaluation of deep neural networks by explicitly accounting for distributional shifts during deployment using out-of-distribution detectors and structured binary tree models.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks often underperform in real-world applications due to sensitivity to distributional shifts, which are not accounted for in typical evaluations, inflating performance metrics.

Method: The paper proposes modeling distributional shifts at runtime through probabilities derived from out-of-distribution detectors and structuring conditional probabilities of network correctness in a binary tree to estimate network accuracy and assess risks.

Result: The methodology was tested on five datasets simulating varying distributional shifts and consistently provided more accurate accuracy estimates with errors between 0.01 and 0.1, outperforming conventional evaluation methods.

Conclusion: The approach enhances the reliability and trustworthiness of deep learning systems, especially in safety-critical applications, by enabling more credible performance estimates and comprehensive risk assessments.

Abstract: Despite achieving excellent performance on benchmarks, deep neural networks
often underperform in real-world deployment due to sensitivity to minor, often
imperceptible shifts in input data, known as distributional shifts. These
shifts are common in practical scenarios but are rarely accounted for during
evaluation, leading to inflated performance metrics. To address this gap, we
propose a novel methodology for the verification, evaluation, and risk
assessment of deep learning systems. Our approach explicitly models the
incidence of distributional shifts at runtime by estimating their probability
from outputs of out-of-distribution detectors. We combine these estimates with
conditional probabilities of network correctness, structuring them in a binary
tree. By traversing this tree, we can compute credible and precise estimates of
network accuracy. We assess our approach on five different datasets, with which
we simulate deployment conditions characterized by differing frequencies of
distributional shift. Our approach consistently outperforms conventional
evaluation, with accuracy estimation errors typically ranging between 0.01 and
0.1. We further showcase the potential of our approach on a medical
segmentation benchmark, wherein we apply our methods towards risk assessment by
associating costs with tree nodes, informing cost-benefit analyses and
value-judgments. Ultimately, our approach offers a robust framework for
improving the reliability and trustworthiness of deep learning systems,
particularly in safety-critical applications, by providing more accurate
performance estimates and actionable risk assessments.

</details>


### [146] [A Realistic Evaluation of Cross-Frequency Transfer Learning and Foundation Forecasting Models](https://arxiv.org/abs/2509.19465)
*Kin G. Olivares,Malcolm Wolff,Tatiana Konstantinova,Shankar Ramasubramanian,Andrew Gordon Wilson,Andres Potapczynski,Willa Potosnak,Mengfei Cao,Boris Oreshkin,Dmitry Efimov*

Main category: cs.LG

TL;DR: The paper critiques current Cross-Frequency Transfer Learning (CFTL) benchmarking practices, proposes improvements, and evaluates them on 15 datasets, revealing that statistical models generally outperform existing foundation forecasting models (FFMs) by a significant margin.


<details>
  <summary>Details</summary>
Motivation: Current benchmarking methods for CFTL are inadequate because they often rely on small datasets, poorly compute summary statistics, and fail to properly account for test dataset overlap or use optimal models.

Method: The authors reimplement widely-used neural forecasting networks for the CFTL framework, ensuring no test leakage by pre-training on only proprietary and synthetic data. They evaluate these models on 15 large and diverse public datasets.

Result: Statistical models and their ensembles outperform FFMs by over 8.2% in sCRPS and 20% in MASE across the datasets. However, pre-training on synthetic datasets improves FFM accuracy by approximately 7%.

Conclusion: Improved benchmarking and pre-training practices highlight the underreported strengths of statistical models in forecasting, but also suggest that synthetic dataset pre-training offers moderate benefits for FFMs.

Abstract: Cross-frequency transfer learning (CFTL) has emerged as a popular framework
for curating large-scale time series datasets to pre-train foundation
forecasting models (FFMs). Although CFTL has shown promise, current
benchmarking practices fall short of accurately assessing its performance. This
shortcoming stems from many factors: an over-reliance on small-scale evaluation
datasets; inadequate treatment of sample size when computing summary
statistics; reporting of suboptimal statistical models; and failing to account
for non-negligible risks of overlap between pre-training and test datasets. To
address these limitations, we introduce a unified reimplementation of
widely-adopted neural forecasting networks, adapting them for the CFTL setup;
we pre-train only on proprietary and synthetic data, being careful to prevent
test leakage; and we evaluate on 15 large, diverse public forecast competition
datasets. Our empirical analysis reveals that statistical models' accuracy is
frequently underreported. Notably, we confirm that statistical models and their
ensembles consistently outperform existing FFMs by more than 8.2% in sCRPS, and
by more than 20% MASE, across datasets. However, we also find that synthetic
dataset pre-training does improve the accuracy of a FFM by 7% percent.

</details>


### [147] [THINNs: Thermodynamically Informed Neural Networks](https://arxiv.org/abs/2509.19467)
*Javier Castro,Benjamin Gess*

Main category: cs.LG

TL;DR: This paper introduces THINNs, a thermodynamically consistent extension of Physics-Informed Neural Networks (PINNs), improving their structure with penalty terms linked to large deviation principles.


<details>
  <summary>Details</summary>
Motivation: Physics-Informed Neural Networks (PINNs) often use heuristic penalty terms, which lack thermodynamic consistency in capturing fluctuations in non-equilibrium systems.

Method: The authors propose THINNs, which integrate a large deviation principle into PINNs to devise a physically informed penalization strategy for improbable deviations.

Result: THINNs improve penalization efficiency and demonstrate advantages in capturing thermodynamic consistency through analytical and empirical evaluations.

Conclusion: Integrating large deviation principles into PINNs enhances their ability to model non-equilibrium fluctuating systems, bridging gaps in current penalization strategies.

Abstract: Physics-Informed Neural Networks (PINNs) are a class of deep learning models
aiming to approximate solutions of PDEs by training neural networks to minimize
the residual of the equation. Focusing on non-equilibrium fluctuating systems,
we propose a physically informed choice of penalization that is consistent with
the underlying fluctuation structure, as characterized by a large deviations
principle. This approach yields a novel formulation of PINNs in which the
penalty term is chosen to penalize improbable deviations, rather than being
selected heuristically. The resulting thermodynamically consistent extension of
PINNs, termed THINNs, is subsequently analyzed by establishing analytical a
posteriori estimates, and providing empirical comparisons to established
penalization strategies.

</details>


### [148] [Mamba Modulation: On the Length Generalization of Mamba](https://arxiv.org/abs/2509.19633)
*Peng Lu,Jerry Huang,Qiuhao Zeng,Xinyu Wang,Boxing Wang,Philippe Langlais,Yufei Cui*

Main category: cs.LG

TL;DR: The paper addresses Mamba's performance issues with longer contexts by attributing the problem to state-space dynamics and proposing spectrum scaling.


<details>
  <summary>Details</summary>
Motivation: The quadratic complexity of attention in Transformer models drives the exploration of sub-quadratic alternatives. Mamba, a state-space model, shows promise but fails with longer contexts, underscoring its limitations.

Method: The authors analyzed Mamba's state-space dynamics, particularly the state transition matrix spectrum, and proposed spectrum scaling to adjust the matrices, improving long-context generalization.

Result: Applying spectrum scaling to pre-trained Mamba significantly enhanced its performance in handling longer contexts, outperforming traditional methods that solely modulate time step discretization.

Conclusion: Spectrum scaling provides a novel and effective method for addressing sensitivity to input length in state-space models, opening paths for better long-context generalization.

Abstract: The quadratic complexity of the attention mechanism in Transformer models has
motivated the development of alternative architectures with sub-quadratic
scaling, such as state-space models. Among these, Mamba has emerged as a
leading architecture, achieving state-of-the-art results across a range of
language modeling tasks. However, Mamba's performance significantly
deteriorates when applied to contexts longer than those seen during
pre-training, revealing a sharp sensitivity to context length extension.
Through detailed analysis, we attribute this limitation to the
out-of-distribution behaviour of its state-space dynamics, particularly within
the parameterization of the state transition matrix $\mathbf{A}$. Unlike recent
works which attribute this sensitivity to the vanished accumulation of
discretization time steps, $\exp(-\sum_{t=1}^N\Delta_t)$, we establish a
connection between state convergence behavior as the input length approaches
infinity and the spectrum of the transition matrix $\mathbf{A}$, offering a
well-founded explanation of its role in length extension. Next, to overcome
this challenge, we propose an approach that applies spectrum scaling to
pre-trained Mamba models to enable robust long-context generalization by
selectively modulating the spectrum of $\mathbf{A}$ matrices in each layer. We
show that this can significantly improve performance in settings where simply
modulating $\Delta_t$ fails, validating our insights and providing avenues for
better length generalization of state-space models with structured transition
matrices.

</details>


### [149] [Transformer Modeling for Both Scalability and Performance in Multivariate Time Series](https://arxiv.org/abs/2509.19471)
*Hunjae Lee,Corey Clark*

Main category: cs.LG

TL;DR: DELTAformer is a new transformer design for multivariate time series (MTS) data that improves scalability and performance through constrained inter-variable mixing.


<details>
  <summary>Details</summary>
Motivation: Scalability and indiscriminate inter-variable mixing are challenges in applying transformers to MTS data, leading to noise accumulation, poor alignment between variables, and performance degradation.

Method: The DELTAformer introduces 'delegate tokens' to limit inter-variable information mixing and conducts full inter-temporal modeling, acting as a selective and implicit regularizer for relevant information propagation.

Result: DELTAformer achieves linear scalability with variable count, surpasses standard transformers in benchmarks, and exhibits robust noise resilience in noisy MTS environments.

Conclusion: By leveraging domain-specific challenges, DELTAformer simultaneously achieves linear scaling and improved performance, showcasing state-of-the-art results in MTS tasks.

Abstract: Variable count is among the main scalability bottlenecks for transformer
modeling in multivariate time series (MTS) data. On top of this, a growing
consensus in the field points to indiscriminate inter-variable mixing as a
potential source of noise-accumulation and performance degradation. This is
likely exacerbated by sparsity of informative signals characteristic of many
MTS systems coupled with representational misalignment stemming from
indiscriminate information mixing between (heterogeneous) variables. While
scalability and performance are often seen as competing interests in
transformer design, we show that both can be improved simultaneously in MTS by
strategically constraining the representational capacity of inter-variable
mixing. Our proposed method, transformer with Delegate Token Attention
(DELTAformer), constrains inter-variable modeling through what we call delegate
tokens which are then used to perform full, unconstrained, inter-temporal
modeling. Delegate tokens act as an implicit regularizer that forces the model
to be highly selective about what inter-variable information is allowed to
propagate through the network. Our results show that DELTAformer scales
linearly with variable-count while actually outperforming standard
transformers, achieving state-of-the-art performance across benchmarks and
baselines. In addition, DELTAformer can focus on relevant signals better than
standard transformers in noisy MTS environments and overall exhibit superior
noise-resilience. Overall, results across various experiments confirm that by
aligning our model design to leverage domain-specific challenges in MTS to our
advantage, DELTAformer can simultaneously achieve linear scaling while actually
improving its performance against standard, quadratic transformers.

</details>


### [150] [Constraint-Reduced MILP with Local Outlier Factor Modeling for Plausible Counterfactual Explanations in Credit Approval](https://arxiv.org/abs/2509.19504)
*Trung Nguyen Thanh,Huyen Giang Thi Thu,Tai Le Quy,Ha-Bang Ban*

Main category: cs.LG

TL;DR: This paper refines the DACE framework for counterfactual explanations by employing an efficient Mixed-Integer Linear Programming approach to reduce constraints, resulting in faster computation while maintaining high explanation quality.


<details>
  <summary>Details</summary>
Motivation: Counterfactual explanations are helpful for actionable insights in ML models, but current plausible methods suffer from high computational costs due to numerous constraints. The paper addresses this inefficiency.

Method: The authors refine the optimization of the local outlier factor (LOF) component in the DACE framework by introducing a Mixed-Integer Linear Programming formulation and applying it to a linear SVM classifier.

Result: The experimental results demonstrate significantly faster solving times without compromising the quality of counterfactual explanations.

Conclusion: Efficient LOF modeling can be achieved using the proposed formulation, supporting advancements in counterfactual explanation techniques with reduced computational demands.

Abstract: Counterfactual explanation (CE) is a widely used post-hoc method that
provides individuals with actionable changes to alter an unfavorable prediction
from a machine learning model. Plausible CE methods improve realism by
considering data distribution characteristics, but their optimization models
introduce a large number of constraints, leading to high computational cost. In
this work, we revisit the DACE framework and propose a refined Mixed-Integer
Linear Programming (MILP) formulation that significantly reduces the number of
constraints in the local outlier factor (LOF) objective component. We also
apply the method to a linear SVM classifier with standard scaler. The
experimental results show that our approach achieves faster solving times while
maintaining explanation quality. These results demonstrate the promise of more
efficient LOF modeling in counterfactual explanation and data science
applications.

</details>


### [151] [On the Rate of Convergence of Kolmogorov-Arnold Network Regression Estimators](https://arxiv.org/abs/2509.19830)
*Wei Liu,Eleni Chatzi,Zhilu Lai*

Main category: cs.LG

TL;DR: This paper investigates the theoretical convergence of Kolmogorov-Arnold Networks (KANs) for approximating multivariate functions using B-splines, achieving optimal convergence rates and verifying them through simulations.


<details>
  <summary>Details</summary>
Motivation: To establish theoretical guarantees for KANs and demonstrate their effectiveness as structured alternatives in function approximation.

Method: The paper proves convergence rates using mathematical analysis within Sobolev spaces and provides guidelines for B-spline parameter selection, along with simulation studies to validate theoretical findings.

Result: KANs using B-splines are shown to achieve minimax-optimal convergence rates of $O(n^{-2r/(2r+1)})$, confirmed experimentally through simulations.

Conclusion: KANs are theoretically validated for use in nonparametric regression, offering a structured framework for function approximation with practical guideline derivations for implementation.

Abstract: Kolmogorov-Arnold Networks (KANs) offer a structured and interpretable
framework for multivariate function approximation by composing univariate
transformations through additive or multiplicative aggregation. This paper
establishes theoretical convergence guarantees for KANs when the univariate
components are represented by B-splines. We prove that both additive and hybrid
additive-multiplicative KANs attain the minimax-optimal convergence rate
$O(n^{-2r/(2r+1)})$ for functions in Sobolev spaces of smoothness $r$. We
further derive guidelines for selecting the optimal number of knots in the
B-splines. The theory is supported by simulation studies that confirm the
predicted convergence rates. These results provide a theoretical foundation for
using KANs in nonparametric regression and highlight their potential as a
structured alternative to existing methods.

</details>


### [152] [Frame-based Equivariant Diffusion Models for 3D Molecular Generation](https://arxiv.org/abs/2509.19506)
*Mohan Guo,Cong Liu,Patrick Forré*

Main category: cs.LG

TL;DR: The paper introduces a scalable and physically grounded paradigm for molecular generation, combining deterministic E(3)-equivariance with flexible computational methods.


<details>
  <summary>Details</summary>
Motivation: To overcome the trade-off faced by molecular generation methods between strict equivariance and computational scalability.

Method: The paper proposes a frame-based diffusion paradigm, exploring three variants (GFD, LFD, IFD) paired with EdgeDiT for enhanced expressivity.

Result: On the QM9 dataset, GFD with EdgeDiT achieves state-of-the-art performance in test metrics, surpassing all equivariant baselines with faster sampling.

Conclusion: Frame-based diffusion successfully balances scalability, flexibility, and symmetry enforcement, solidifying its role in molecular generation with superior results.

Abstract: Recent methods for molecular generation face a trade-off: they either enforce
strict equivariance with costly architectures or relax it to gain scalability
and flexibility. We propose a frame-based diffusion paradigm that achieves
deterministic E(3)-equivariance while decoupling symmetry handling from the
backbone. Building on this paradigm, we investigate three variants: Global
Frame Diffusion (GFD), which assigns a shared molecular frame; Local Frame
Diffusion (LFD), which constructs node-specific frames and benefits from
additional alignment constraints; and Invariant Frame Diffusion (IFD), which
relies on pre-canonicalized invariant representations. To enhance expressivity,
we further utilize EdgeDiT, a Diffusion Transformer with edge-aware attention.
  On the QM9 dataset, GFD with EdgeDiT achieves state-of-the-art performance,
with a test NLL of -137.97 at standard scale and -141.85 at double scale,
alongside atom stability of 98.98%, and molecular stability of 90.51%. These
results surpass all equivariant baselines while maintaining high validity and
uniqueness and nearly 2x faster sampling compared to EDM. Altogether, our study
establishes frame-based diffusion as a scalable, flexible, and physically
grounded paradigm for molecular generation, highlighting the critical role of
global structure preservation.

</details>


### [153] [Metriplectic Conditional Flow Matching for Dissipative Dynamics](https://arxiv.org/abs/2509.19526)
*Ali Baheri,Lars Lindemann*

Main category: cs.LG

TL;DR: MCFM integrates dissipative dynamics into neural modeling while preserving first principles, optimizing short transitions without destabilizing long rollouts.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the issue of instability and energy injection in neural surrogates for modeling dissipative dynamics, especially over long rollouts.

Method: MCFM employs conditional flow matching on short transitions for training, alongside an inference method combining symplectic updates and proximal metric steps for energy-preservation.

Result: MCFM was tested on a controlled mechanical benchmark, showing superior energy stability and accuracy in phase portraits compared to unconstrained neural models.

Conclusion: MCFM offers a robust solution for modeling dissipative dynamics with guarantees of conservation, monotonic dissipation, and stable rollouts between discrete and continuous time.

Abstract: Metriplectic conditional flow matching (MCFM) learns dissipative dynamics
without violating first principles. Neural surrogates often inject energy and
destabilize long-horizon rollouts; MCFM instead builds the
conservative-dissipative split into both the vector field and a structure
preserving sampler. MCFM trains via conditional flow matching on short
transitions, avoiding long rollout adjoints. In inference, a Strang-prox scheme
alternates a symplectic update with a proximal metric step, ensuring discrete
energy decay; an optional projection enforces strict decay when a trusted
energy is available. We provide continuous and discrete time guarantees linking
this parameterization and sampler to conservation, monotonic dissipation, and
stable rollouts. On a controlled mechanical benchmark, MCFM yields phase
portraits closer to ground truth and markedly fewer energy-increase and
positive energy rate events than an equally expressive unconstrained neural
flow, while matching terminal distributional fit.

</details>


### [154] [Pure Exploration via Frank-Wolfe Self-Play](https://arxiv.org/abs/2509.19901)
*Xinyu Liu,Chao Qin,Wei You*

Main category: cs.LG

TL;DR: The paper addresses structured stochastic multi-armed bandit problems for hypothesis identification, introduces a game-theoretic optimization reformulation, and proposes a method (FWSP) for efficient convergence through both theoretical and numerical support.


<details>
  <summary>Details</summary>
Motivation: The motivation is to efficiently solve the problem of pure exploration in structured stochastic multi-armed bandits, which involves identifying the correct hypothesis from a finite set of alternatives, despite complex structural constraints.

Method: The authors reformulate the optimization as a two-player zero-sum game with a mixed strategy for the skeptic, leading to the development of a saddle-point problem solved by a method called Frank-Wolfe Self-Play (FWSP). They employ differential-inclusion arguments and Lyapunov functions for convergence analysis.

Result: The method addresses challenges like nonsmoothness and nonunique optima, proving convergence to the optimal game value for linear-bandit hypothesis identification. Further, it supports theoretical findings with a learning algorithm and numerical experiments showing vanishing duality gaps.

Conclusion: The FWSP framework successfully navigates structural pathologies in bandit problems, providing a theoretical and practical approach for efficient hypothesis identification with convergence guarantees and empirical validation.

Abstract: We study pure exploration in structured stochastic multi-armed bandits,
aiming to efficiently identify the correct hypothesis from a finite set of
alternatives. For a broad class of tasks, asymptotic analyses reduce to a
maximin optimization that admits a two-player zero-sum game interpretation
between an experimenter and a skeptic: the experimenter allocates measurements
to rule out alternatives while the skeptic proposes alternatives. We
reformulate the game by allowing the skeptic to adopt a mixed strategy,
yielding a concave-convex saddle-point problem. This viewpoint leads to
Frank-Wolfe Self-Play (FWSP): a projection-free, regularization-free,
tuning-free method whose one-hot updates on both sides match the bandit
sampling paradigm. However, structural constraints introduce sharp pathologies
that complicate algorithm design and analysis: our linear-bandit case study
exhibits nonunique optima, optimal designs with zero mass on the best arm,
bilinear objectives, and nonsmoothness at the boundary. We address these
challenges via a differential-inclusion argument, proving convergence of the
game value for best-arm identification in linear bandits. Our analysis proceeds
through a continuous-time limit: a differential inclusion with a Lyapunov
function that decays exponentially, implying a vanishing duality gap and
convergence to the optimal value. Although Lyapunov analysis requires
differentiability of the objective, which is not guaranteed on the boundary, we
show that along continuous trajectories the algorithm steers away from
pathological nonsmooth points and achieves uniform global convergence to the
optimal game value. We then embed the discrete-time updates into a perturbed
flow and show that the discrete game value also converges. Building on FWSP, we
further propose a learning algorithm based on posterior sampling. Numerical
experiments demonstrate a vanishing duality gap.

</details>


### [155] [DAWM: Diffusion Action World Models for Offline Reinforcement Learning via Action-Inferred Transitions](https://arxiv.org/abs/2509.19538)
*Zongyue Li,Xiao Han,Yusong Li,Niklas Strauss,Matthias Schubert*

Main category: cs.LG

TL;DR: DAWM is a diffusion-based world model designed to generate state-reward trajectories for offline RL. It solves limitations of existing models by incorporating an inverse dynamics model (IDM).


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing diffusion-based models in offline RL, which do not directly generate actions and struggle with one-step TD learning compatibility.

Method: DAWM generates future state-reward trajectories conditioned on current state, action, and return-to-go. It uses an inverse dynamics model for efficient action inference and modular training.

Result: DAWM, when combined with offline RL algorithms like TD3BC and IQL, significantly outperforms prior diffusion-based baselines on the D4RL benchmark.

Conclusion: DAWM offers a computationally efficient and effective way to integrate diffusion-based methods with standard offline RL algorithms, improving performance across tasks.

Abstract: Diffusion-based world models have demonstrated strong capabilities in
synthesizing realistic long-horizon trajectories for offline reinforcement
learning (RL). However, many existing methods do not directly generate actions
alongside states and rewards, limiting their compatibility with standard
value-based offline RL algorithms that rely on one-step temporal difference
(TD) learning. While prior work has explored joint modeling of states, rewards,
and actions to address this issue, such formulations often lead to increased
training complexity and reduced performance in practice. We propose
\textbf{DAWM}, a diffusion-based world model that generates future state-reward
trajectories conditioned on the current state, action, and return-to-go, paired
with an inverse dynamics model (IDM) for efficient action inference. This
modular design produces complete synthetic transitions suitable for one-step
TD-based offline RL, enabling effective and computationally efficient training.
Empirically, we show that conservative offline RL algorithms such as TD3BC and
IQL benefit significantly from training on these augmented trajectories,
consistently outperforming prior diffusion-based baselines across multiple
tasks in the D4RL benchmark.

</details>


### [156] [How deep is your network? Deep vs. shallow learning of transfer operators](https://arxiv.org/abs/2509.19930)
*Mohammad Tabish,Benedict Leimkuhler,Stefan Klus*

Main category: cs.LG

TL;DR: RaNNDy, a randomized neural network approach, learns transfer operators and spectral decompositions efficiently by training only the output layer while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in deep learning such as sensitivity to hyperparameters and slow convergence, while reducing training time and computational resources for learning transfer operators.

Method: RaNNDy uses neural networks with randomly initialized hidden layer weights, trains only the output layer, and computes a closed-form solution to directly obtain eigenfunctions of the operator.

Result: RaNNDy effectively learns spectral properties of dynamical operators, like Koopman and Perron-Frobenius operators, with numerical demonstrations on stochastic systems, protein folding, and quantum oscillators.

Conclusion: The method achieves a balance between efficiency and accuracy, with ensemble learning for uncertainty estimation, making it well-suited for analyzing complex dynamical systems.

Abstract: We propose a randomized neural network approach called RaNNDy for learning
transfer operators and their spectral decompositions from data. The weights of
the hidden layers of the neural network are randomly selected and only the
output layer is trained. The main advantage is that without a noticeable
reduction in accuracy, this approach significantly reduces the training time
and resources while avoiding common problems associated with deep learning such
as sensitivity to hyperparameters and slow convergence. Additionally, the
proposed framework allows us to compute a closed-form solution for the output
layer which directly represents the eigenfunctions of the operator. Moreover,
it is possible to estimate uncertainties associated with the computed spectral
properties via ensemble learning. We present results for different dynamical
operators, including Koopman and Perron-Frobenius operators, which have
important applications in analyzing the behavior of complex dynamical systems,
and the Schr\"odinger operator. The numerical examples, which highlight the
strengths but also weaknesses of the proposed framework, include several
stochastic dynamical systems, protein folding processes, and the quantum
harmonic oscillator.

</details>


### [157] [Learning Dynamics of Deep Learning -- Force Analysis of Deep Neural Networks](https://arxiv.org/abs/2509.19554)
*Yi Ren*

Main category: cs.LG

TL;DR: The thesis introduces a novel force-inspired framework to analyze how training examples influence each other during deep learning processes, offering insights into model behavior.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the need for better interpretability of deep learning models by exploring how training examples interact during the learning process.

Method: Inspired by force analysis, the study decomposes the influence of one example on another into similarity and updating force, and applies this framework across various real-world systems.

Result: The approach provides insights into learning paths of examples, the efficacy of LLM fine-tuning methods, and why structured patterns are easier to learn by deep models.

Conclusion: This method enriches our understanding of deep learning behaviors and offers new strategies to refine training processes, although it is still in the development stage.

Abstract: This thesis explores how deep learning models learn over time, using ideas
inspired by force analysis. Specifically, we zoom in on the model's training
procedure to see how one training example affects another during learning, like
analyzing how forces move objects. We break this influence into two parts: how
similar the two examples are, and how strong the updating force is. This
framework helps us understand a wide range of the model's behaviors in
different real systems. For example, it explains why certain examples have
non-trivial learning paths, why (and why not) some LLM finetuning methods work,
and why simpler, more structured patterns tend to be learned more easily. We
apply this approach to various learning tasks and uncover new strategies for
improving model training. While the method is still developing, it offers a new
way to interpret models' behaviors systematically.

</details>


### [158] [Learnable Sampler Distillation for Discrete Diffusion Models](https://arxiv.org/abs/2509.19962)
*Feiyang Fu,Tongxian Guo,Zhaoqiang Liu*

Main category: cs.LG

TL;DR: This paper introduces Learnable Sampler Distillation (LSD) and LSD+ to speed up discrete diffusion models (DDMs) while maintaining high generation quality, reducing sampling steps significantly.


<details>
  <summary>Details</summary>
Motivation: The inefficiency in sampling of discrete diffusion models (DDMs) limits their practical use, as larger step sizes degrade generation quality due to compounded prediction and discretization errors.

Method: LSD trains a student sampler with fewer steps to align its intermediate score trajectory with a high-quality teacher sampler using adaptive coefficients. LSD+ extends this by learning non-uniform step schedules.

Result: Experiments on text, image, and synthetic data show LSD and LSD+ outperform existing samplers in both sampling quality and step efficiency.

Conclusion: LSD and LSD+ represent significant advancements for DDMs, offering accelerated sampling without sacrificing generation fidelity, thus broadening their practical application.

Abstract: Discrete diffusion models (DDMs) have shown powerful generation ability for
discrete data modalities like text and molecules. However, their practical
application is hindered by inefficient sampling, requiring a large number of
sampling steps. Accelerating DDMs by using larger step sizes typically
introduces significant problems in generation quality, as it amplifies the
impact of both the compounding decoding error due to factorized predictions and
discretization error from numerical approximations, leading to a significant
decrease in sampling quality. To address these challenges, we propose learnable
sampler distillation (LSD), a novel approach to train fast and high-fidelity
samplers for DDMs. LSD employs a distillation approach where a student sampler
with a few steps learns to align its intermediate score trajectory with that of
a high-quality teacher sampler with numerous steps. This alignment is achieved
by optimizing learnable sampler coefficients that adaptively adjust sampling
dynamics. Additionally, we further propose LSD+, which also learns time
schedules that allocate steps non-uniformly. Experiments across text
generation, image generation, and synthetic tasks demonstrate that our proposed
approaches outperform existing samplers for DDMs, achieving substantially
higher sampling quality with significantly fewer sampling steps. Our code is
available at
\href{https://github.com/feiyangfu/LSD}{https://github.com/feiyangfu/LSD}.

</details>


### [159] [A Foundation Chemical Language Model for Comprehensive Fragment-Based Drug Discovery](https://arxiv.org/abs/2509.19586)
*Alexander Ho,Sukyeong Lee,Francis T. F. Tsai*

Main category: cs.LG

TL;DR: FragAtlas-62M is a GPT-2 based model trained on over 62 million molecules, effectively generating novel and chemically valid fragments while maintaining high accuracy and releasing resources for public use.


<details>
  <summary>Details</summary>
Motivation: To achieve unprecedented coverage of fragment chemical space and provide tools for accelerating research and application in molecular science.

Method: A GPT-2 based model with 42.7M parameters, trained on the ZINC-22 fragment dataset, validated on descriptors and fingerprint methods.

Result: The model generated chemically valid fragments with 99.90% validity, closely matching training distributions, retaining 53.6% known fragments, and producing 22% novel structures.

Conclusion: FragAtlas-62M is a significant advancement for molecular research, offering high validity and novel fragment generation capabilities, complemented with accessible resources for broad adoption.

Abstract: We introduce FragAtlas-62M, a specialized foundation model trained on the
largest fragment dataset to date. Built on the complete ZINC-22 fragment subset
comprising over 62 million molecules, it achieves unprecedented coverage of
fragment chemical space. Our GPT-2 based model (42.7M parameters) generates
99.90% chemically valid fragments. Validation across 12 descriptors and three
fingerprint methods shows generated fragments closely match the training
distribution (all effect sizes < 0.4). The model retains 53.6% of known ZINC
fragments while producing 22% novel structures with practical relevance. We
release FragAtlas-62M with training code, preprocessed data, documentation, and
model weights to accelerate adoption.

</details>


### [160] [Staying on the Manifold: Geometry-Aware Noise Injection](https://arxiv.org/abs/2509.20201)
*Albert Kjøller Jacobsen,Johanna Marie Gegenfurtner,Georgios Arvanitidis*

Main category: cs.LG

TL;DR: The paper explores geometry-aware input noise techniques to improve generalization and robustness in training models, focusing on manifold structures rather than ambient noise.


<details>
  <summary>Details</summary>
Motivation: Enhance regularization and generalization performance in models by considering the inherent manifold structure of input data rather than relying on ambient noise.

Method: Develop methods that generate input noise aligned with data manifold geometry, including projecting Gaussian noise onto tangent spaces, mapping via geodesic curves, and Brownian motion along manifolds.

Result: Geometry-aware noise demonstrates better generalization and robustness, particularly on curved manifolds, while maintaining performance on simpler manifolds.

Conclusion: Geometry-aware noise provides a structured approach to input regularization, resulting in smoother and more generalizable models particularly in complex data landscapes.

Abstract: It has been shown that perturbing the input during training implicitly
regularises the gradient of the learnt function, leading to smoother models and
enhancing generalisation. However, previous research mostly considered the
addition of ambient noise in the input space, without considering the
underlying structure of the data. In this work, we propose several methods of
adding geometry-aware input noise that accounts for the lower dimensional
manifold the input space inhabits. We start by projecting ambient Gaussian
noise onto the tangent space of the manifold. In a second step, the noise
sample is mapped on the manifold via the associated geodesic curve. We also
consider Brownian motion noise, which moves in random steps along the manifold.
We show that geometry-aware noise leads to improved generalization and
robustness to hyperparameter selection on highly curved manifolds, while
performing at least as well as training without noise on simpler manifolds. Our
proposed framework extends to learned data manifolds.

</details>


### [161] [Modular Machine Learning with Applications to Genetic Circuit Composition](https://arxiv.org/abs/2509.19601)
*Jichi Wang,Eduardo D. Sontag,Domitilla Del Vecchio*

Main category: cs.LG

TL;DR: This paper presents a modular learning framework that uses prior knowledge of a system's compositional structure to identify module input/output functions with less training data, offering potential improvements for synthetic biology and multi-module systems.


<details>
  <summary>Details</summary>
Motivation: In systems composed of many modules with unknown input/output functions, leveraging knowledge of the composition architecture can reduce training data needs and is critical for designing new systems.

Method: The paper introduces modular identifiability and applies a neural network framework that incorporates compositional structure to learn module input/output functions with theoretical guarantees.

Result: Computational studies show that neural networks using compositional structure can predict system outputs even for inputs outside the training data distribution, unlike structure-agnostic neural networks.

Conclusion: The proposed framework reduces experimental data needs, facilitates module identification, and enables the design of synthetic biological circuits and other complex multi-module systems.

Abstract: In several applications, including in synthetic biology, one often has
input/output data on a system composed of many modules, and although the
modules' input/output functions and signals may be unknown, knowledge of the
composition architecture can significantly reduce the amount of training data
required to learn the system's input/output mapping. Learning the modules'
input/output functions is also necessary for designing new systems from
different composition architectures. Here, we propose a modular learning
framework, which incorporates prior knowledge of the system's compositional
structure to (a) identify the composing modules' input/output functions from
the system's input/output data and (b) achieve this by using a reduced amount
of data compared to what would be required without knowledge of the
compositional structure. To achieve this, we introduce the notion of modular
identifiability, which allows recovery of modules' input/output functions from
a subset of the system's input/output data, and provide theoretical guarantees
on a class of systems motivated by genetic circuits. We demonstrate the theory
on computational studies showing that a neural network (NNET) that accounts for
the compositional structure can learn the composing modules' input/output
functions and predict the system's output on inputs outside of the training set
distribution. By contrast, a neural network that is agnostic of the structure
is unable to predict on inputs that fall outside of the training set
distribution. By reducing the need for experimental data and allowing module
identification, this framework offers the potential to ease the design of
synthetic biological circuits and of multi-module systems more generally.

</details>


### [162] [A Recovery Guarantee for Sparse Neural Networks](https://arxiv.org/abs/2509.20323)
*Sara Fridovich-Keil,Mert Pilanci*

Main category: cs.LG

TL;DR: The paper presents theoretical guarantees and empirical validation for sparse recovery in ReLU-based neural networks using an efficient iterative hard thresholding algorithm.


<details>
  <summary>Details</summary>
Motivation: Sparse recovery in neural networks is critical for memory efficiency and performance optimization, and the study aims to establish theoretical guarantees for this process.

Method: The authors analyze sparse two-layer scalar-output networks and use iterative hard thresholding algorithms for weight recovery, providing theoretical backing and experimental validation.

Result: The algorithm recovers sparse network weights using linear memory and delivers competitive experimental performance, often exceeding memory-intensive iterative magnitude pruning.

Conclusion: Sparse recovery in ReLU neural networks can be effectively achieved with an iterative hard thresholding algorithm, ensuring both theoretical guarantees and practical performance advantages.

Abstract: We prove the first guarantees of sparse recovery for ReLU neural networks,
where the sparse network weights constitute the signal to be recovered.
Specifically, we study structural properties of the sparse network weights for
two-layer, scalar-output networks under which a simple iterative hard
thresholding algorithm recovers these weights exactly, using memory that grows
linearly in the number of nonzero weights. We validate this theoretical result
with simple experiments on recovery of sparse planted MLPs, MNIST
classification, and implicit neural representations. Experimentally, we find
performance that is competitive with, and often exceeds, a high-performing but
memory-inefficient baseline based on iterative magnitude pruning.

</details>


### [163] [Improved Therapeutic Antibody Reformatting through Multimodal Machine Learning](https://arxiv.org/abs/2509.19604)
*Jiayi Xin,Aniruddh Raghu,Nick Bhattacharya,Adam Carr,Melanie Montgomery,Hunter Elliott*

Main category: cs.LG

TL;DR: The paper proposes a machine learning framework to predict the success of converting antibodies from one format to another, addressing engineering challenges in therapeutic antibody design.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in therapeutic antibody design, where functional and stability issues arise when converting antibodies into novel formats.

Method: The study utilizes a machine learning framework integrating antibody sequence and structural context. It also evaluates models under realistic deployment scenarios and compares performance of large pre-trained protein language models to domain-tailored multimodal representations.

Result: Large pretrained language models fail to outperform simpler, tailored multimodal models, especially in challenging scenarios like generalizing to a new starting antibody. Their multimodal model achieves high predictive accuracy.

Conclusion: Domain-tailored multimodal representations outperform pretrained models in novel scenarios, enabling effective candidate prioritization and reducing experimental efforts.

Abstract: Modern therapeutic antibody design often involves composing multi-part
assemblages of individual functional domains, each of which may be derived from
a different source or engineered independently. While these complex formats can
expand disease applicability and improve safety, they present a significant
engineering challenge: the function and stability of individual domains are not
guaranteed in the novel format, and the entire molecule may no longer be
synthesizable. To address these challenges, we develop a machine learning
framework to predict "reformatting success" -- whether converting an antibody
from one format to another will succeed or not. Our framework incorporates both
antibody sequence and structural context, incorporating an evaluation protocol
that reflects realistic deployment scenarios. In experiments on a real-world
antibody reformatting dataset, we find the surprising result that large
pretrained protein language models (PLMs) fail to outperform simple,
domain-tailored, multimodal representations. This is particularly evident in
the most difficult evaluation setting, where we test model generalization to a
new starting antibody. In this challenging "new antibody, no data" scenario,
our best multimodal model achieves high predictive accuracy, enabling
prioritization of promising candidates and reducing wasted experimental effort.

</details>


### [164] [Adaptive von Mises-Fisher Likelihood Loss for Supervised Deep Time Series Hashing](https://arxiv.org/abs/2509.19625)
*Juan Manuel Perez,Kevin Garcia,Brooklyn Berry,Dongjin Song,Yifeng Gao*

Main category: cs.LG

TL;DR: The paper presents a novel deep hashing method using von Mises-Fisher distributions for enhanced time series indexing. It maps data onto a hyperspherical space to reduce information loss and maximize separation between semantic classes.


<details>
  <summary>Details</summary>
Motivation: Deep learning-based hashing methods aim to improve time series indexing by focusing on semantic meanings rather than raw similarity. Traditional methods face challenges due to the information loss during discretization.

Method: The paper proposes a von Mises-Fisher (vMF) hashing loss, mapping data to a hyperspherical space while modeling semantic classes as distinct vMF distributions. The objective is to maximize the margin between classes.

Result: Experimental findings illustrate that the vMF hashing model achieves superior performance compared to existing methods in efficiently indexing time series data.

Conclusion: The method successfully addresses information loss in traditional deep hashing and demonstrates improved accuracy and efficiency in time series data mining, suggesting its potential for broader applications.

Abstract: Indexing time series by creating compact binary representations is a
fundamental task in time series data mining. Recently, deep learning-based
hashing methods have proven effective for indexing time series based on
semantic meaning rather than just raw similarity. The purpose of deep hashing
is to map samples with the same semantic meaning to identical binary hash
codes, enabling more efficient search and retrieval. Unlike other supervised
representation learning methods, supervised deep hashing requires a
discretization step to convert real-valued representations into binary codes,
but this can induce significant information loss. In this paper, we propose a
von Mises-Fisher (vMF) hashing loss. The proposed deep hashing model maps data
to an M-dimensional hyperspherical space to effectively reduce information loss
and models each data class as points following distinct vMF distributions. The
designed loss aims to maximize the separation between each modeled vMF
distribution to provide a better way to maximize the margin between each
semantically different data sample. Experimental results show that our method
outperforms existing baselines. The implementation is publicly available at
https://github.com/jmpq97/vmf-hashing

</details>


### [165] [TIMED: Adversarial and Autoregressive Refinement of Diffusion-Based Time Series Generation](https://arxiv.org/abs/2509.19638)
*MohammadReza EskandariNasab,Shah Muhammad Hamdi,Soukaina Filali Boubrahimi*

Main category: cs.LG

TL;DR: TIMED is a generative framework that improves synthetic time series generation using diffusion models, autoregressive supervision, adversarial training, and feature alignment methods.


<details>
  <summary>Details</summary>
Motivation: The scarcity, noise, and cost associated with collecting real time series data make high-quality synthetic generation an essential and challenging task.

Method: TIMED combines denoising diffusion probabilistic models, autoregressive networks, adversarial critics, and Maximum Mean Discrepancy (MMD) loss to jointly train masked attention architectures for modeling time series.

Result: Experimental evaluations across various benchmarks show that TIMED surpasses state-of-the-art models in producing more realistic and temporally coherent synthetic time series.

Conclusion: By leveraging diverse components tailored for both unconditional and conditional dynamics, TIMED effectively addresses challenges in synthetic time series generation.

Abstract: Generating high-quality synthetic time series is a fundamental yet
challenging task across domains such as forecasting and anomaly detection,
where real data can be scarce, noisy, or costly to collect. Unlike static data
generation, synthesizing time series requires modeling both the marginal
distribution of observations and the conditional temporal dependencies that
govern sequential dynamics. We propose TIMED, a unified generative framework
that integrates a denoising diffusion probabilistic model (DDPM) to capture
global structure via a forward-reverse diffusion process, a supervisor network
trained with teacher forcing to learn autoregressive dependencies through
next-step prediction, and a Wasserstein critic that provides adversarial
feedback to ensure temporal smoothness and fidelity. To further align the real
and synthetic distributions in feature space, TIMED incorporates a Maximum Mean
Discrepancy (MMD) loss, promoting both diversity and sample quality. All
components are built using masked attention architectures optimized for
sequence modeling and are trained jointly to effectively capture both
unconditional and conditional aspects of time series data. Experimental results
across diverse multivariate time series benchmarks demonstrate that TIMED
generates more realistic and temporally coherent sequences than
state-of-the-art generative models.

</details>


### [166] [Toward Scalable and Structured Global Station Weather Forecasting](https://arxiv.org/abs/2509.19648)
*Hongyi Chen,Xiucheng Li,Xinyang Chen,Yun Cheng,Jing Li,Kehai Chen,Liqiang Nie*

Main category: cs.LG

TL;DR: This paper introduces a new model for weather forecasting that improves accuracy by addressing spatial correlations often ignored in traditional methods.


<details>
  <summary>Details</summary>
Motivation: Existing global weather forecasting methods fail to capture the intrinsic spatial interconnections of the global weather system, which limits prediction accuracy.

Method: The authors propose a Spatial Structured Attention Block that creates and processes subgraphs to model both local and global spatial correlations, and integrate it into a scalable multiscale spatiotemporal forecasting model.

Result: The proposed model achieves up to a 16.8% improvement in performance compared to conventional forecasting methods, with low computational costs.

Conclusion: The approach effectively balances scalability, structured spatial modeling, and implementation simplicity, making it a significant improvement over existing methods.

Abstract: Global Station Weather Forecasting (GSWF) is a key meteorological research
area, critical to energy, aviation, and agriculture. Existing time series
forecasting methods often ignore or unidirectionally model spatial correlation
when conducting large-scale global station forecasting. This contradicts the
intrinsic nature underlying observations of the global weather system, limiting
forecast performance. To address this, we propose a novel Spatial Structured
Attention Block in this paper. It partitions the spatial graph into a set of
subgraphs and instantiates Intra-subgraph Attention to learn local spatial
correlation within each subgraph, and aggregates nodes into subgraph
representations for message passing among the subgraphs via Inter-subgraph
Attention -- considering both spatial proximity and global correlation.
Building on this block, we develop a multiscale spatiotemporal forecasting
model by progressively expanding subgraph scales. The resulting model is both
scalable and able to produce structured spatial correlation, and meanwhile, it
is easy to implement. The experimental results show that it can achieve
performance improvements up to 16.8% over time series forecasting baselines at
low running costs.

</details>


### [167] [Symbol-Temporal Consistency Self-supervised Learning for Robust Time Series Classification](https://arxiv.org/abs/2509.19654)
*Kevin Garcia,Cassandra Garza,Brooklyn Berry,Yifeng Gao*

Main category: cs.LG

TL;DR: This paper introduces a self-supervised learning framework leveraging bag-of-symbol representation to enhance pattern recognition in noisy time series data within digital health. Results show its effectiveness under data distribution shifts.


<details>
  <summary>Details</summary>
Motivation: Time series data in digital health is often noisy and subject to concept drift, posing challenges for building generalizable deep learning models.

Method: The authors propose a self-supervised learning framework that incorporates bag-of-symbol representation, known for its resistance to data warping, location shifts, and noise.

Result: The framework significantly improves performance in scenarios with considerable data distribution shifts.

Conclusion: Bag-of-symbol representation aids deep learning in acquiring robust representations, paving the way for more reliable time series analysis in digital health domains.

Abstract: The surge in the significance of time series in digital health domains
necessitates advanced methodologies for extracting meaningful patterns and
representations. Self-supervised contrastive learning has emerged as a
promising approach for learning directly from raw data. However, time series
data in digital health is known to be highly noisy, inherently involves concept
drifting, and poses a challenge for training a generalizable deep learning
model. In this paper, we specifically focus on data distribution shift caused
by different human behaviors and propose a self-supervised learning framework
that is aware of the bag-of-symbol representation. The bag-of-symbol
representation is known for its insensitivity to data warping, location shifts,
and noise existed in time series data, making it potentially pivotal in guiding
deep learning to acquire a representation resistant to such data shifting. We
demonstrate that the proposed method can achieve significantly better
performance where significant data shifting exists.

</details>


### [168] [Consistent Estimation of Numerical Distributions under Local Differential Privacy by Wavelet Expansion](https://arxiv.org/abs/2509.19661)
*Puning Zhao,Zhikun Zhang,Bo Sun,Li Shen,Liang Zhang,Shaowei Wang,Zhe Liu*

Main category: cs.LG

TL;DR: The paper introduces a method to estimate numerical data distributions under Local Differential Privacy (LDP) using wavelet expansions, focusing on improving accuracy at a macroscopic level.


<details>
  <summary>Details</summary>
Motivation: Existing methods for LDP distribution estimation work well for categorical data but perform poorly on numerical data, as they fail to prevent probability mass from being misplaced far from the true values.

Method: The authors propose estimating the coefficients of wavelet series under LDP, prioritizing the estimation of low-order coefficients to enhance macroscopic accuracy of distribution.

Result: Theoretical guarantees are established for the proposed method, and experiments indicate that it significantly outperforms existing approaches using Wasserstein and KS distances as evaluation metrics.

Conclusion: Wavelet expansions provide a more accurate and effective approach for estimating numerical distributions under LDP compared to existing methods, ensuring better alignment with the ground truth.

Abstract: Distribution estimation under local differential privacy (LDP) is a
fundamental and challenging task. Significant progresses have been made on
categorical data. However, due to different evaluation metrics, these methods
do not work well when transferred to numerical data. In particular, we need to
prevent the probability mass from being misplaced far away. In this paper, we
propose a new approach that express the sample distribution using wavelet
expansions. The coefficients of wavelet series are estimated under LDP. Our
method prioritizes the estimation of low-order coefficients, in order to ensure
accurate estimation at macroscopic level. Therefore, the probability mass is
prevented from being misplaced too far away from its ground truth. We establish
theoretical guarantees for our methods. Experiments show that our wavelet
expansion method significantly outperforms existing solutions under Wasserstein
and KS distances.

</details>


### [169] [Revisiting Performance Claims for Chest X-Ray Models Using Clinical Context](https://arxiv.org/abs/2509.19671)
*Andrew Wang,Jiashuo Zhang,Michael Oberst*

Main category: cs.LG

TL;DR: The paper evaluates clinical vision models for Chest X-Ray diagnosis by incorporating context from prior discharge summaries and highlights potential shortcomings in their diagnostic performance.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for Chest X-Ray diagnosis models lack a deeper evaluation involving clinical context, which may hinder their reliability in real-world clinical settings.

Method: The authors use prior discharge summaries to derive a 'pre-test' probability for Chest X-Ray labels, evaluating state-of-the-art models based on this contextual clinical knowledge.

Result: Findings show that models perform better on cases with low pre-test probability and worse on high pre-test probability cases. Additionally, models rely heavily on pre-test probability shortcuts, affecting their diagnostic accuracy on balanced datasets.

Conclusion: Incorporating clinical context via discharge summaries provides critical insights into model evaluation, promoting more nuanced and rigorous assessments for clinical utility.

Abstract: Public healthcare datasets of Chest X-Rays (CXRs) have long been a popular
benchmark for developing computer vision models in healthcare. However, strong
average-case performance of machine learning (ML) models on these datasets is
insufficient to certify their clinical utility. In this paper, we use clinical
context, as captured by prior discharge summaries, to provide a more holistic
evaluation of current ``state-of-the-art'' models for the task of CXR
diagnosis. Using discharge summaries recorded prior to each CXR, we derive a
``prior'' or ``pre-test'' probability of each CXR label, as a proxy for
existing contextual knowledge available to clinicians when interpreting CXRs.
Using this measure, we demonstrate two key findings: First, for several
diagnostic labels, CXR models tend to perform best on cases where the pre-test
probability is very low, and substantially worse on cases where the pre-test
probability is higher. Second, we use pre-test probability to assess whether
strong average-case performance reflects true diagnostic signal, rather than an
ability to infer the pre-test probability as a shortcut. We find that
performance drops sharply on a balanced test set where this shortcut does not
exist, which may indicate that much of the apparent diagnostic power derives
from inferring this clinical context. We argue that this style of analysis,
using context derived from clinical notes, is a promising direction for more
rigorous and fine-grained evaluation of clinical vision models.

</details>


### [170] [C${}^2$Prompt: Class-aware Client Knowledge Interaction for Federated Continual Learning](https://arxiv.org/abs/2509.19674)
*Kunlun Xu,Yibo Feng,Jiangmeng Li,Yongsheng Qi,Jiahuan Zhou*

Main category: cs.LG

TL;DR: This paper proposes a novel method (C${}^2$Prompt) to address temporal and spatial forgetting in Federated Continual Learning by improving class-wise knowledge coherence.


<details>
  <summary>Details</summary>
Motivation: Address challenges in Federated Continual Learning, specifically temporal and spatial forgetting, due to insufficient class-wise knowledge coherence during prompt communication.

Method: Introduced Class-aware Client Knowledge Interaction (C${}^2$Prompt), consisting of a Local Class Distribution Compensation (LCDC) mechanism and Class-aware Prompt Aggregation (CPA) to enhance class-wise knowledge coherence.

Result: The proposed method achieves state-of-the-art performance on multiple Federated Continual Learning benchmarks.

Conclusion: C${}^2$Prompt effectively addresses the class-wise knowledge coherence problems in Federated Continual Learning, thereby mitigating both temporal and spatial forgetting.

Abstract: Federated continual learning (FCL) tackles scenarios of learning from
continuously emerging task data across distributed clients, where the key
challenge lies in addressing both temporal forgetting over time and spatial
forgetting simultaneously. Recently, prompt-based FCL methods have shown
advanced performance through task-wise prompt communication.In this study, we
underscore that the existing prompt-based FCL methods are prone to class-wise
knowledge coherence between prompts across clients. The class-wise knowledge
coherence includes two aspects: (1) intra-class distribution gap across
clients, which degrades the learned semantics across prompts, (2) inter-prompt
class-wise relevance, which highlights cross-class knowledge confusion. During
prompt communication, insufficient class-wise coherence exacerbates knowledge
conflicts among new prompts and induces interference with old prompts,
intensifying both spatial and temporal forgetting. To address these issues, we
propose a novel Class-aware Client Knowledge Interaction (C${}^2$Prompt) method
that explicitly enhances class-wise knowledge coherence during prompt
communication. Specifically, a local class distribution compensation mechanism
(LCDC) is introduced to reduce intra-class distribution disparities across
clients, thereby reinforcing intra-class knowledge consistency. Additionally, a
class-aware prompt aggregation scheme (CPA) is designed to alleviate
inter-class knowledge confusion by selectively strengthening class-relevant
knowledge aggregation. Extensive experiments on multiple FCL benchmarks
demonstrate that C${}^2$Prompt achieves state-of-the-art performance. Our
source code is available at
https://github.com/zhoujiahuan1991/NeurIPS2025-C2Prompt

</details>


### [171] [A Unified Noise-Curvature View of Loss of Trainability](https://arxiv.org/abs/2509.19698)
*Gunbir Singh Baveja,Mark Schmidt*

Main category: cs.LG

TL;DR: This paper addresses the loss of trainability (LoT) in continual learning by proposing criteria and a scheduling method to improve training stability and accuracy.


<details>
  <summary>Details</summary>
Motivation: The study aims to overcome the loss of trainability in continual learning tasks where accuracy deteriorates despite sufficient model capacity and adequate supervision.

Method: The authors propose two predictive criteria: a gradient-noise bound (accounting for batch size) and a curvature volatility-bound. These form a per-layer threshold used to guide learning-rate scheduling.

Result: Results show that the proposed scheduler improves training stability and accuracy with tailored learning rate trajectories across techniques like concatenated ReLU, Wasserstein regularization, and L2 weight decay.

Conclusion: The findings suggest that a per-layer scheduler based on predictive bounds can effectively mitigate LoT, providing stability and enhancing continual learning performance.

Abstract: Loss of trainability (LoT) in continual learning occurs when gradient steps
no longer yield improvement as tasks evolve, so accuracy stalls or degrades
despite adequate capacity and supervision. We analyze LoT incurred with Adam
through an optimization lens and find that single indicators such as Hessian
rank, sharpness level, weight or gradient norms, gradient-to-parameter ratios,
and unit-sign entropy are not reliable predictors. Instead we introduce two
complementary criteria: a batch-size-aware gradient-noise bound and a curvature
volatility-controlled bound that combine into a per-layer predictive threshold
that anticipates trainability behavior. Using this threshold, we build a simple
per-layer scheduler that keeps each layers effective step below a safe limit,
stabilizing training and improving accuracy across concatenated ReLU (CReLU),
Wasserstein regularization, and L2 weight decay, with learned learning-rate
trajectories that mirror canonical decay.

</details>


### [172] [Linear Transformers Implicitly Discover Unified Numerical Algorithms](https://arxiv.org/abs/2509.19702)
*Patrick Lutz,Aditya Gangrade,Hadi Daneshmand,Venkatesh Saligrama*

Main category: cs.LG

TL;DR: The paper trains a linear attention transformer on masked-block matrix completion tasks and uncovers that the transformer implicitly discovers a unified iterative solver across various computational regimes.


<details>
  <summary>Details</summary>
Motivation: To investigate whether a transformer model can independently learn an iterative solving method for matrix completion tasks without explicit guidance or handcrafted rules.

Method: The transformer was trained on millions of masked-low rank matrix completion tasks, using input-output pairs and a mean-squared loss, without any explicit mathematical rules or iterations provided.

Result: The trained transformer was able to implicitly discover a parameter-free update rule that performs well across full visibility, rank-limited updates, and distributed computation, achieving second-order convergence and adaptation in multiple contexts.

Conclusion: Transformers trained on matrix completion tasks reveal in-context learning capabilities, implicitly discovering adaptive iterative solvers for prediction, estimation, and Nyström extrapolation without external guidance.

Abstract: We train a linear attention transformer on millions of masked-block matrix
completion tasks: each prompt is masked low-rank matrix whose missing block may
be (i) a scalar prediction target or (ii) an unseen kernel slice of Nystr\"om
extrapolation. The model sees only input-output pairs and a mean-squared loss;
it is given no normal equations, no handcrafted iterations, and no hint that
the tasks are related. Surprisingly, after training, algebraic unrolling
reveals the same parameter-free update rule across three distinct computational
regimes (full visibility, rank-limited updates, and distributed computation).
We prove that this rule achieves second-order convergence on full-batch
problems, cuts distributed iteration complexity, and remains accurate with
rank-limited attention. Thus, a transformer trained solely to patch missing
blocks implicitly discovers a unified, resource-adaptive iterative solver
spanning prediction, estimation, and Nystr\"om extrapolation, highlighting a
powerful capability of in-context learning.

</details>


### [173] [Causal Machine Learning for Surgical Interventions](https://arxiv.org/abs/2509.19705)
*J. Ben Tamo,Nishant S. Chouhan,Micky C. Nnamdi,Yining Yuan,Shreya S. Chivilkar,Wenqi Shi,Steven W. Hwang,B. Randall Brenn,May D. Wang*

Main category: cs.LG

TL;DR: The study presents X-MultiTask, a meta-learning framework for estimating individualized treatment effects (ITEs) in surgical decision-making, with high accuracy demonstrated on spinal datasets.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional statistical methods in handling complex and heterogeneous data for accurate ITE estimation in critical surgical settings such as spinal surgeries.

Method: The paper introduces X-MultiTask, a multi-task meta-learning framework that treats each surgical decision as a distinct task and learns shared representations across tasks. It incorporates inverse probability weighting (IPW) to ensure causal validity.

Result: On two datasets, X-MultiTask achieves superior performance, with high AUC and low errors ($\epsilon_{NN-PEHE}$ and $\epsilon_{ATE}$), outperforming baseline methods in treatment effect estimation.

Conclusion: X-MultiTask demonstrates its effectiveness in advancing personalized surgical care by providing robust, patient-specific causal estimates. It has significant potential to improve surgical decision-making and patient outcomes.

Abstract: Surgical decision-making is complex and requires understanding causal
relationships between patient characteristics, interventions, and outcomes. In
high-stakes settings like spinal fusion or scoliosis correction, accurate
estimation of individualized treatment effects (ITEs) remains limited due to
the reliance on traditional statistical methods that struggle with complex,
heterogeneous data. In this study, we develop a multi-task meta-learning
framework, X-MultiTask, for ITE estimation that models each surgical decision
(e.g., anterior vs. posterior approach, surgery vs. no surgery) as a distinct
task while learning shared representations across tasks. To strengthen causal
validity, we incorporate the inverse probability weighting (IPW) into the
training objective. We evaluate our approach on two datasets: (1) a public
spinal fusion dataset (1,017 patients) to assess the effect of anterior vs.
posterior approaches on complication severity; and (2) a private AIS dataset
(368 patients) to analyze the impact of posterior spinal fusion (PSF) vs.
non-surgical management on patient-reported outcomes (PROs). Our model achieves
the highest average AUC (0.84) in the anterior group and maintains competitive
performance in the posterior group (0.77). It outperforms baselines in
treatment effect estimation with the lowest overall $\epsilon_{\text{NN-PEHE}}$
(0.2778) and $\epsilon_{\text{ATE}}$ (0.0763). Similarly, when predicting PROs
in AIS, X-MultiTask consistently shows superior performance across all domains,
with $\epsilon_{\text{NN-PEHE}}$ = 0.2551 and $\epsilon_{\text{ATE}}$ = 0.0902.
By providing robust, patient-specific causal estimates, X-MultiTask offers a
powerful tool to advance personalized surgical care and improve patient
outcomes. The code is available at https://github.com/Wizaaard/X-MultiTask.

</details>


### [174] [Cuffless Blood Pressure Prediction from Speech Sentences using Deep Learning Methods](https://arxiv.org/abs/2509.19750)
*Kainat*

Main category: cs.LG

TL;DR: A novel BERT-based AI analyzes speech to predict blood pressure noninvasively, showing high accuracy and potential in health monitoring.


<details>
  <summary>Details</summary>
Motivation: Improve noninvasive blood pressure tracking to overcome limitations of traditional cuff-based methods and to enhance cardiovascular health monitoring.

Method: Speech signals were analyzed using a fine-tuned BERT regression model with features extracted from recordings of 95 participants.

Result: The model achieved a mean absolute error of 1.36 mmHg for SBP and 1.24 mmHg for DBP with R-scores of 0.99 and 0.94 respectively, demonstrating strong accuracy and minimal overfitting.

Conclusion: This speech-analysis-based method provides a reliable, discomfort-free solution for blood pressure monitoring, offering significant potential for telemedicine and cardiovascular care.

Abstract: This research presents a novel method for noninvasive arterial blood pressure
ABP prediction using speech signals employing a BERT based regression model
Arterial blood pressure is a vital indicator of cardiovascular health and
accurate monitoring is essential in preventing hypertension related
complications Traditional cuff based methods often yield inconsistent results
due to factors like whitecoat and masked hypertension Our approach leverages
the acoustic characteristics of speech capturing voice features to establish
correlations with blood pressure levels Utilizing advanced deep learning
techniques we analyze speech signals to extract relevant patterns enabling real
time monitoring without the discomfort of conventional methods In our study we
employed a dataset comprising recordings from 95 participants ensuring diverse
representation The BERT model was fine tuned on extracted features from speech
leading to impressive performance metrics achieving a mean absolute error MAE
of 136 mmHg for systolic blood pressure SBP and 124 mmHg for diastolic blood
pressure DBP with R scores of 099 and 094 respectively These results indicate
the models robustness in accurately predicting blood pressure levels
Furthermore the training and validation loss analysis demonstrates effective
learning and minimal overfitting Our findings suggest that integrating deep
learning with speech analysis presents a viable alternative for blood pressure
monitoring paving the way for improved applications in telemedicine and remote
health monitoring By providing a user friendly and accurate method for blood
pressure assessment this research has significant implications for enhancing
patient care and proactive management of cardiovascular health

</details>


### [175] [Frictional Q-Learning](https://arxiv.org/abs/2509.19771)
*Hyunwoo Kim,Hyo Kyung Lee*

Main category: cs.LG

TL;DR: The paper introduces Frictional Q-learning for off-policy reinforcement learning, taking inspiration from the physics of static friction to manage extrapolation error.


<details>
  <summary>Details</summary>
Motivation: The researchers aimed to address the challenge of extrapolation error in off-policy reinforcement learning by drawing a parallel to static friction in classical mechanics.

Method: The Frictional Q-learning algorithm constrains an agent’s action space to stick closer to behavior seen in the replay buffer while distancing itself from unsupported actions, inspired by static friction.

Result: The algorithm demonstrated robust training and competitive performance on standard continuous control benchmarks.

Conclusion: The approach simplifies batch-constrained reinforcement learning while offering a novel way to understand and control extrapolation error in continuous control tasks.

Abstract: We draw an analogy between static friction in classical mechanics and
extrapolation error in off-policy RL, and use it to formulate a constraint that
prevents the policy from drifting toward unsupported actions. In this study, we
present Frictional Q-learning, a deep reinforcement learning algorithm for
continuous control, which extends batch-constrained reinforcement learning. Our
algorithm constrains the agent's action space to encourage behavior similar to
that in the replay buffer, while maintaining a distance from the manifold of
the orthonormal action space. The constraint preserves the simplicity of
batch-constrained, and provides an intuitive physical interpretation of
extrapolation error. Empirically, we further demonstrate that our algorithm is
robustly trained and achieves competitive performance across standard
continuous control benchmarks.

</details>


### [176] [Sobolev acceleration for neural networks](https://arxiv.org/abs/2509.19773)
*Jong Kwon Oh,Hanbaek Lyu,Hwijae Son*

Main category: cs.LG

TL;DR: The paper explores Sobolev training, showing its advantage in accelerating convergence and improving generalization by providing a theoretical framework proving these benefits for ReLU networks.


<details>
  <summary>Details</summary>
Motivation: To address the lack of deep theoretical understanding of Sobolev training, especially its convergence acceleration mechanisms.

Method: The authors analyzed Sobolev training under a student-teacher framework with shallow architectures and Gaussian inputs. They derived explicit formulas for gradients and Hessians and assessed improvements in the loss landscape and gradient-flow rates.

Result: Sobolev training helps improve conditioning of the loss and accelerates gradient-flow convergence. Numerical experiments further demonstrated its effectiveness in modern deep learning tasks.

Conclusion: This work offers a theoretical basis for Sobolev training's benefits and confirms its utility in both theoretical and practical deep learning applications.

Abstract: Sobolev training, which integrates target derivatives into the loss
functions, has been shown to accelerate convergence and improve generalization
compared to conventional $L^2$ training. However, the underlying mechanisms of
this training method remain only partially understood. In this work, we present
the first rigorous theoretical framework proving that Sobolev training
accelerates the convergence of Rectified Linear Unit (ReLU) networks. Under a
student-teacher framework with Gaussian inputs and shallow architectures, we
derive exact formulas for population gradients and Hessians, and quantify the
improvements in conditioning of the loss landscape and gradient-flow
convergence rates. Extensive numerical experiments validate our theoretical
findings and show that the benefits of Sobolev training extend to modern deep
learning tasks.

</details>


### [177] [PPGFlowECG: Latent Rectified Flow with Cross-Modal Encoding for PPG-Guided ECG Generation and Cardiovascular Disease Detection](https://arxiv.org/abs/2509.19774)
*Xiaocheng Fang,Jiarui Jin,Haoyu Wang,Che Liu,Jieyi Cai,Guangkun Nie,Jun Li,Hongyan Li,Shenda Hong*

Main category: cs.LG

TL;DR: The study introduces PPGFlowECG, a framework to translate PPG signals into ECG signals for real-world cardiovascular screening, demonstrating improved fidelity and diagnostic reliability using a large clinical dataset.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address the limitations of ECG, which requires specialized equipment and personnel, and PPG, which lacks diagnostic definitiveness, by creating a method to bridge the gap between these two cardiac monitoring techniques.

Method: The authors propose PPGFlowECG, a two-stage framework composed of a CardioAlign Encoder aligning PPG and ECG in a shared latent space and a latent rectified flow to generate high-quality ECGs. The method is tested on the MCMED dataset.

Result: The approach effectively translates PPG signals into high-fidelity ECG signals and improves the detection of cardiovascular diseases. Cardiologist-led evaluations confirm the reliability of the generated ECGs.

Conclusion: PPGFlowECG offers a promising solution for non-invasive, continuous cardiovascular monitoring, enabling effective PPG-to-ECG translation and enhancing diagnostic reliability for CVD detection.

Abstract: In clinical practice, electrocardiography (ECG) remains the gold standard for
cardiac monitoring, providing crucial insights for diagnosing a wide range of
cardiovascular diseases (CVDs). However, its reliance on specialized equipment
and trained personnel limits feasibility for continuous routine monitoring.
Photoplethysmography (PPG) offers accessible, continuous monitoring but lacks
definitive electrophysiological information, preventing conclusive diagnosis.
Generative models present a promising approach to translate PPG into clinically
valuable ECG signals, yet current methods face substantial challenges,
including the misalignment of physiological semantics in generative models and
the complexity of modeling in high-dimensional signals. To this end, we propose
PPGFlowECG, a two-stage framework that aligns PPG and ECG in a shared latent
space via the CardioAlign Encoder and employs latent rectified flow to generate
ECGs with high fidelity and interpretability. To the best of our knowledge,
this is the first study to experiment on MCMED, a newly released clinical-grade
dataset comprising over 10 million paired PPG-ECG samples from more than
118,000 emergency department visits with expert-labeled cardiovascular disease
annotations. Results demonstrate the effectiveness of our method for PPG-to-ECG
translation and cardiovascular disease detection. Moreover, cardiologist-led
evaluations confirm that the synthesized ECGs achieve high fidelity and improve
diagnostic reliability, underscoring our method's potential for real-world
cardiovascular screening.

</details>


### [178] [Faster, Smaller, and Smarter: Task-Aware Expert Merging for Online MoE Inference](https://arxiv.org/abs/2509.19781)
*Ziyi Han,Xutong Liu,Ruiting Zhou,Xiangxiang Dai,John C. S. Lui*

Main category: cs.LG

TL;DR: The paper introduces the novel Tanbr algorithm, a tree-structured adaptive neural bandit router to enable efficient online inference in Sparse Mixture of Experts models, optimizing routing without explicit task information.


<details>
  <summary>Details</summary>
Motivation: Sparse Mixture of Experts models pose challenges for online inference due to their large model size and complex expert routing, especially in edge networks where task-specific information is often unavailable.

Method: The proposed Tanbr algorithm estimates task distribution over time using historical data and employs a binary tree structure to refine merging weights in a continuous space. It further utilizes a neural bandit approach to learn optimal expert merging without relying on explicit task tags.

Result: Experiments demonstrate substantial improvements: reduced inference latency by 45%, decreased memory usage by 25%, and competitive accuracy levels compared to state-of-the-art alternatives.

Conclusion: Tanbr proves to be an effective solution for overcoming routing challenges in Sparse Mixture of Experts models during online inference, showing scalability and reliability in resource-constrained settings.

Abstract: Sparse Mixture of Experts (SMoE) has become a preferred architecture for
scaling Transformer capacity without increasing computational cost, as it
activates only a small subset of experts for each input. However, deploying
such an approach for \textit{online inference} remains challenging due to the
large size of a full SMoE model and the complexity of expert routing,
especially in resource-constrained edge networks. Moreover, during the online
inference, task information is often unavailable, making the task-level routing
error-prone. In this work, we propose a novel tree-structured adaptive neural
bandit router, \texttt{Tanbr}, to enable efficient and reliable online MoE
inference. Instead of relying on explicit task tags, \texttt{Tanbr} estimates
the task distribution over time from historical data and uses it to guide
task-aware expert merging within a given pre-trained MoE. To handle the large
continuous space of merging weights, \texttt{Tanbr} employs a binary tree to
progressively partition the space and generate finer candidate weights. It then
applies a neural bandit to learn the non-linear mapping from merging weight to
model performance and decides optimal expert merging. We prove that
\texttt{Tanbr} achieves a sublinear regret bound of {\small
$\mathcal{O}(\sqrt{T} \log(T))$} over {\small $T$} rounds, despite operating
over a continuous decision space, matching regret bounds compared to existing
methods. Extensive experiments show that \texttt{Tanbr} reduces inference
latency by at least {\small $45\%$} and memory usage by up to {\small $25\%$},
while maintaining a high accuracy compared to many state-of-the-art methods.

</details>


### [179] [RDAR: Reward-Driven Agent Relevance Estimation for Autonomous Driving](https://arxiv.org/abs/2509.19789)
*Carlo Bosio,Greg Woelki,Noureldin Hendy,Nicholas Roy,Byungsoo Kim*

Main category: cs.LG

TL;DR: This paper introduces RDAR, a method to optimize autonomous driving systems by focusing only on relevant agents in a scene, significantly reducing computational complexity while preserving driving performance.


<details>
  <summary>Details</summary>
Motivation: Human drivers naturally focus on a few relevant agents while driving, but autonomous systems lack this ability, leading to unnecessary computational overhead when processing all agents in a scene.

Method: The authors propose RDAR, which learns the relevance of each agent in a traffic scene through a Markov Decision Process that applies binary masking to exclude irrelevant agents. The method uses a pre-trained behavior model as the baseline.

Result: RDAR demonstrates comparable driving performance (progress, safety, and general behavior) while processing far fewer agents than current state-of-the-art behavior models, validated on a large-scale driving dataset.

Conclusion: RDAR effectively improves efficiency in autonomous driving systems by intelligently selecting relevant agents, showing the potential to maintain performance while reducing computational demands.

Abstract: Human drivers focus only on a handful of agents at any one time. On the other
hand, autonomous driving systems process complex scenes with numerous agents,
regardless of whether they are pedestrians on a crosswalk or vehicles parked on
the side of the road. While attention mechanisms offer an implicit way to
reduce the input to the elements that affect decisions, existing attention
mechanisms for capturing agent interactions are quadratic, and generally
computationally expensive. We propose RDAR, a strategy to learn per-agent
relevance -- how much each agent influences the behavior of the controlled
vehicle -- by identifying which agents can be excluded from the input to a
pre-trained behavior model. We formulate the masking procedure as a Markov
Decision Process where the action consists of a binary mask indicating agent
selection. We evaluate RDAR on a large-scale driving dataset, and demonstrate
its ability to learn an accurate numerical measure of relevance by achieving
comparable driving performance, in terms of overall progress, safety and
performance, while processing significantly fewer agents compared to a state of
the art behavior model.

</details>


### [180] [VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2509.19803)
*Guochao Jiang,Wenfeng Feng,Guofeng Quan,Chuzhan Hao,Yuewei Zhang,Guohua Liu,Hao Wang*

Main category: cs.LG

TL;DR: This paper introduces VCRL, a novel curriculum reinforcement learning framework that dynamically adjusts training sample difficulty based on reward variance, enhancing mathematical reasoning in LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing reinforcement learning methods for LLMs in mathematical reasoning tasks overlook the importance of training with samples of varying difficulty levels, which is contrary to the natural human cognitive process.

Method: The authors propose VCRL, using the variance of rollout group's rewards to identify sample difficulty. Samples are dynamically selected for training to match their difficulty levels with the learning progress.

Result: Experiments conducted on five mathematical benchmarks and two models demonstrated that VCRL surpasses existing reinforcement learning baselines for LLMs.

Conclusion: VCRL effectively aligns LLM training with a human-like learning progression, improving performance on mathematical reasoning tasks.

Abstract: Policy-based reinforcement learning currently plays an important role in
improving LLMs on mathematical reasoning tasks. However, existing rollout-based
reinforcement learning methods (GRPO, DAPO, GSPO, etc.) fail to explicitly
consider LLMs' learning ability for samples of different difficulty levels,
which is contrary to the human cognitive process of mathematical reasoning
tasks from easy to difficult. Intuitively, we find that the variance of the
rollout group's reward in RLVR partly reflects the difficulty of the current
sample for LLMs. Samples that are too easy or too difficult have a lower
variance, while samples with moderate difficulty have a higher variance. Based
on this, we propose VCRL, a curriculum reinforcement learning framework that
dynamically controls the difficulty of training samples based on the variance
of group rewards. Experiments on five mathematical benchmarks and two models
reveal the advantages of VCRL over the current LLM RL baselines.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [181] [Multi-population Ensemble Genetic Programming via Cooperative Coevolution and Multi-view Learning for Classification](https://arxiv.org/abs/2509.19339)
*Mohammad Sadegh Khorshidi,Navid Yazdanjue,Hassan Gharoun,Mohammad Reza Nikoo,Fang Chen,Amir H. Gandomi*

Main category: cs.NE

TL;DR: The paper introduces MEGP, a framework combining cooperative coevolution and multiview learning for classification tasks in complex feature spaces, demonstrating substantial performance improvements and diversity retention.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address classification challenges in high-dimensional, heterogeneous feature spaces by developing a framework that promotes cooperation, diversity, and interpretability.

Method: MEGP uses cooperative coevolution to divide and process multiple feature subsets in parallel, employs a softmax-based gene aggregation layer for decision fusion, and utilizes a hybrid selection mechanism for balanced dynamics.

Result: MEGP outperformed baseline models in experiments across eight benchmark datasets, achieving significant gains in metrics like Log-Loss, Precision, Recall, F1 score, and AUC.

Conclusion: MEGP offers a scalable and interpretable evolutionary learning framework by integrating population-based optimization and multiview representation learning, advancing evolutionary machine learning principles.

Abstract: This paper introduces Multi-population Ensemble Genetic Programming (MEGP), a
computational intelligence framework that integrates cooperative coevolution
and the multiview learning paradigm to address classification challenges in
high-dimensional and heterogeneous feature spaces. MEGP decomposes the input
space into conditionally independent feature subsets, enabling multiple
subpopulations to evolve in parallel while interacting through a dynamic
ensemble-based fitness mechanism. Each individual encodes multiple genes whose
outputs are aggregated via a differentiable softmax-based weighting layer,
enhancing both model interpretability and adaptive decision fusion. A hybrid
selection mechanism incorporating both isolated and ensemble-level fitness
promotes inter-population cooperation while preserving intra-population
diversity. This dual-level evolutionary dynamic facilitates structured search
exploration and reduces premature convergence. Experimental evaluations across
eight benchmark datasets demonstrate that MEGP consistently outperforms a
baseline GP model in terms of convergence behavior and generalization
performance. Comprehensive statistical analyses validate significant
improvements in Log-Loss, Precision, Recall, F1 score, and AUC. MEGP also
exhibits robust diversity retention and accelerated fitness gains throughout
evolution, highlighting its effectiveness for scalable, ensemble-driven
evolutionary learning. By unifying population-based optimization, multi-view
representation learning, and cooperative coevolution, MEGP contributes a
structurally adaptive and interpretable framework that advances emerging
directions in evolutionary machine learning.

</details>


### [182] [Fully Tensorized GPU-accelerated Multi-population Evolutionary Algorithm for Constrained Multiobjective Optimization Problems](https://arxiv.org/abs/2509.19821)
*Weixiong Huang,Rui Wang,Wenhua Li,Sheng Qi,Tianyu Luo,Delong Chen,Tao Zhang,Ling Wang*

Main category: cs.NE

TL;DR: The paper proposes GMPEA, a GPU-accelerated algorithm designed to solve time-sensitive constrained multiobjective optimization problems efficiently.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of existing CMOEAs, such as inefficiency, complexity, and slow convergence, especially in time-sensitive optimization tasks.

Method: Developed GMPEA, a GPU-accelerated, decomposition-based multi-population evolutionary algorithm designed to parallelize the entire workflow for computational speed and performance.

Result: GMPEA achieved competitive outcomes on benchmark tests and Weapon Target Assignment Problems. It notably outperformed existing algorithms under strict time constraints.

Conclusion: GMPEA demonstrates significant advantages in solving real-world, time-sensitive CMOPs due to its parallelized and efficient design.

Abstract: Real world constrained multiobjective optimization problems (CMOPs) are
prevalent and often come with stringent time-sensitive requirements. However,
most contemporary constrained multiobjective evolutionary algorithms (CMOEAs)
suffer from a number of drawbacks, including complex designs, low computational
efficiency, and long convergence times, which are particularly pronounced when
addressing time-sensitive CMOPs. Although research on accelerating evolutionary
algorithms using GPU parallelism has advanced, existing CMOEAs still face
significant limitations within GPU frameworks. To overcome these challenges,
this paper proposes a GPU-accelerated multi-population evolutionary algorithm,
termed GMPEA. We first systematically analyze the performance bottlenecks of
representative CMOEAs when implemented in a GPU environment. To address the
trade-off between computational speed and solution performance, GMPEA
introduces a decomposition-based multi-population approach that is fully
parallelized across its entire workflow. We conducted comparative experiments
on various benchmark tests and real world applications: the Weapon Target
Assignment Problems. The results demonstrate that GMPEA achieves competitive
performance even without time constraints, while its computational speed
significantly surpasses that of the compared algorithms. More critically, under
a strict time limit, the performance of GMPEA drastically outperforms its
counterparts. This work provides compelling evidence of GMPEA's superiority in
solving time-sensitive CMOPs.

</details>


### [183] [Projective Kolmogorov Arnold Neural Networks (P-KANs): Entropy-Driven Functional Space Discovery for Interpretable Machine Learning](https://arxiv.org/abs/2509.20049)
*Alastair Poole,Stig McArthur,Saravan Kumar*

Main category: cs.NE

TL;DR: Projective Kolmogorov-Arnold Networks (P-KANs) mitigate inefficiencies in Kolmogorov-Arnold Networks using entropy-minimization techniques to discover interpretable and reduced-parameter functional representations.


<details>
  <summary>Details</summary>
Motivation: Current Kolmogorov-Arnold Networks are inefficient due to redundancy in high-dimensional spline parameter spaces, which leads to issues like overfitting and poor generalization.

Method: P-KANs introduce a training framework that utilizes entropy-minimization and sparse dictionary learning to guide edge function discovery while maintaining flexibility in functional spaces and introducing "gravitational" terms for optimal convergence.

Result: P-KANs achieved up to 80% reduction in parameters, improved robustness to noise, and demonstrated successful applications in industrial fiber placement prediction.

Conclusion: P-KANs provide a more efficient and interpretable alternative to standard KANs, enabling lower-parameter and mixed functional representations with enhanced scientific interpretability.

Abstract: Kolmogorov-Arnold Networks (KANs) relocate learnable nonlinearities from
nodes to edges, demonstrating remarkable capabilities in scientific machine
learning and interpretable modeling. However, current KAN implementations
suffer from fundamental inefficiencies due to redundancy in high-dimensional
spline parameter spaces, where numerous distinct parameterisations yield
functionally equivalent behaviors. This redundancy manifests as a "nuisance
space" in the model's Jacobian, leading to susceptibility to overfitting and
poor generalization. We introduce Projective Kolmogorov-Arnold Networks
(P-KANs), a novel training framework that guides edge function discovery
towards interpretable functional representations through entropy-minimisation
techniques from signal analysis and sparse dictionary learning. Rather than
constraining functions to predetermined spaces, our approach maintains spline
space flexibility while introducing "gravitational" terms that encourage
convergence towards optimal functional representations. Our key insight
recognizes that optimal representations can be identified through entropy
analysis of projection coefficients, compressing edge functions to
lower-parameter projective spaces (Fourier, Chebyshev, Bessel). P-KANs
demonstrate superior performance across multiple domains, achieving up to 80%
parameter reduction while maintaining representational capacity, significantly
improved robustness to noise compared to standard KANs, and successful
application to industrial automated fiber placement prediction. Our approach
enables automatic discovery of mixed functional representations where different
edges converge to different optimal spaces, providing both compression benefits
and enhanced interpretability for scientific machine learning applications.

</details>


### [184] [Biologically Plausible Learning via Bidirectional Spike-Based Distillation](https://arxiv.org/abs/2509.20284)
*Changze Lv,Yifei Wang,Yanxun Zhang,Yiyang Lu,Jingwen Xu,Di Yu,Xin Du,Xuanjing Huang,Xiaoqing Zheng*

Main category: cs.NE

TL;DR: The paper introduces Bidirectional Spike-based Distillation (BSD), a biologically plausible learning algorithm, that achieves competitive performance with classical error backpropagation.


<details>
  <summary>Details</summary>
Motivation: To develop biologically plausible learning algorithms using spikes that address limitations of previous methods, such as inability to represent negative values effectively.

Method: The BSD algorithm trains a feedforward and a backward spiking network together, framing learning as a transformation between two spiking representations for tasks like perception, decision-making, and memory recall.

Result: BSD demonstrated strong performance across diverse benchmarks like image recognition, generation, and sequential regression, performing comparably to classical error backpropagation.

Conclusion: The findings highlight BSD as a step forward in achieving biologically plausible, spike-driven learning mechanisms in neural networks.

Abstract: Developing biologically plausible learning algorithms that can achieve
performance comparable to error backpropagation remains a longstanding
challenge. Existing approaches often compromise biological plausibility by
entirely avoiding the use of spikes for error propagation or relying on both
positive and negative learning signals, while the question of how spikes can
represent negative values remains unresolved. To address these limitations, we
introduce Bidirectional Spike-based Distillation (BSD), a novel learning
algorithm that jointly trains a feedforward and a backward spiking network. We
formulate learning as a transformation between two spiking representations
(i.e., stimulus encoding and concept encoding) so that the feedforward network
implements perception and decision-making by mapping stimuli to actions, while
the backward network supports memory recall by reconstructing stimuli from
concept representations. Extensive experiments on diverse benchmarks, including
image recognition, image generation, and sequential regression, show that BSD
achieves performance comparable to networks trained with classical error
backpropagation. These findings represent a significant step toward
biologically grounded, spike-driven learning in neural networks.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [185] [Are We Scaling the Right Thing? A System Perspective on Test-Time Scaling](https://arxiv.org/abs/2509.19645)
*Youpeng Zhao,Jinpeng LV,Di Wu,Jun Wang,Christopher Gooley*

Main category: cs.PF

TL;DR: This paper critiques existing test-time scaling (TTS) approaches for large language models (LLMs) and advocates for a system-driven analysis considering practical performance metrics beyond compute optimality.


<details>
  <summary>Details</summary>
Motivation: To address the gap in existing test-time scaling analyses that ignore system-level metrics like latency and cost, focusing instead on compute-optimal but possibly impractical solutions.

Method: The authors evaluate the impact of optimizations, such as tensor parallelism and speculative decoding, while considering practical system metrics like latency and cost-per-token.

Result: The study exposes the limitations of existing TTS methods, particularly their lack of system-awareness, and underscores the need for a holistic approach to scaling evaluations.

Conclusion: The work emphasizes the necessity of shifting from compute-centric to system-aware evaluations to better understand scaling laws and optimize real-world LLM inference.

Abstract: Test-time scaling (TTS) has recently emerged as a promising direction to
exploit the hidden reasoning capabilities of pre-trained large language models
(LLMs). However, existing scaling methods narrowly focus on the compute-optimal
Pareto-frontier, ignoring the simple fact that compute-optimal is not always
system-optimal. In this work, we propose a system-driven perspective on TTS,
analyzing how reasoning models scale against practical metrics, such as latency
and cost-per-token. By evaluating the impact of popular optimizations such as
tensor parallelism and speculative decoding, our preliminary analysis reveals
the limitations of current methods and calls for a paradigm shift toward
holistic, system-aware evaluations that capture the true essence of scaling
laws at inference time.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [186] [Macro-embedding Compiler Intermediate Languages in Racket](https://arxiv.org/abs/2509.19607)
*William J. Bowman*

Main category: cs.PL

TL;DR: The paper introduces a macro-embedding framework in Racket for simplifying the creation and testing of compilers' intermediate languages, facilitating code reuse and modularity.


<details>
  <summary>Details</summary>
Motivation: To address challenges in implementing a family of intermediate languages with interoperability between high- and low-level languages while enabling code reuse and modularity.

Method: The authors designed and implemented a macro-embedding approach in Racket for the intermediate languages, leveraging local macro expansions and interoperability features.

Result: The framework enabled significant code reuse and simplified the implementation of language semantics, interoperability, and extensions within intermediate languages.

Conclusion: This approach showcases the advantages of language-oriented programming for creating modular and interoperable compiler infrastructures, reducing development complexity.

Abstract: We present the design and implementation of a macro-embedding of a family of
compiler intermediate languages, from a Scheme-like language to x86-64, into
Racket. This embedding is used as part of a testing framework for a compilers
course to derive interpreters for all the intermediate languages. The embedding
implements features including safe, functional abstractions as well as unsafe
assembly features, and the interactions between the two at various intermediate
stages.
  This paper aims to demonstrate language-oriented techniques and abstractions
for implementing (1) a large family of languages and (2) interoperability
between low- and high-level languages. The primary strength of this approach is
the high degree of code reuse and interoperability compared to implementing
each interpreter separately. The design emphasizes modularity and
compositionality of an open set of language features by local macro expansion
into a single host language, rather than implementing a language pre-defined by
a closed set of features. This enables reuse from both the host language
(Racket) and between intermediate languages, and enables interoperability
between high- and low-level features, simplifying development of the
intermediate language semantics. It also facilitates extending or redefining
individual language features in intermediate languages, and exposing multiple
interfaces to the embedded languages.

</details>


### [187] [Compilation as Multi-Language Semantics](https://arxiv.org/abs/2509.19613)
*William J. Bowman*

Main category: cs.PL

TL;DR: This work explores using multi-language semantics to uniformly model a compiler as a reduction system, addressing compiler correctness and interoperability without relying on separate syntactic translations.


<details>
  <summary>Details</summary>
Motivation: The goal is to address the challenge of verified and secure compilation, particularly focusing on improving interoperability between programs written in different languages.

Method: A unified approach to model a compiler as a reduction system on open terms using multi-language semantics, integrating compilation and interoperability semantics.

Result: Key results include defining both ahead-of-time (AOT) and just-in-time (JIT) compilation through multi-language reduction, along with insights such as confluence (compiler correctness) and subject reduction (type preservation).

Conclusion: This approach reduces redundancy, offers semantic insights into compilation processes, and simplifies secure compilation proofs by leveraging multi-language reduction properties like confluence and subject reduction.

Abstract: Modeling interoperability between programs in different languages is a key
problem when modeling verified and secure compilation, which has been
successfully addressed using multi-language semantics. Unfortunately, existing
models of compilation using multi-language semantics define two variants of
each compiler pass: a syntactic translation on open terms to model compilation,
and a run-time translation of closed terms at multi-language boundaries to
model interoperability.
  In this talk, I discuss work-in-progress approach to uniformly model a
compiler entirely as a reduction system on open term in a multi-language
semantics, rather than as a syntactic translation. This simultaneously defines
the compiler and the interoperability semantics, reducing duplication. It also
provides interesting semantic insights. Normalization of the cross-language
redexes performs ahead-of-time (AOT) compilation. Evaluation in the
multi-language models just-in-time (JIT) compilation. Confluence of
multi-language reduction implies compiler correctness, and part of the secure
compilation proof (full abstraction), enabling focus on the difficult part of
the proof. Subject reduction of the multi-language reduction implies
type-preservation of the compiler.

</details>


### [188] [The Syntax and Semantics of einsum](https://arxiv.org/abs/2509.20020)
*Maurice Wenig,Paul G. Rump,Mark Blacher,Joachim Giesen*

Main category: cs.PL

TL;DR: This paper focuses on providing a theoretical foundation and unification of the einsum tensor notation used in various scientific and computational frameworks.


<details>
  <summary>Details</summary>
Motivation: While einsum notation is widely used in Python frameworks and other programming languages for tensor expressions, it lacks theoretical grounding and standardization, hindering systematic optimization and formal reasoning.

Method: The paper offers a formal definition of the einsum language, discusses tensor expression terminology, and proves key equivalence rules for tensor expressions to showcase their application and relevance.

Result: The authors successfully establish crucial equivalence rules for tensor expressions based on a formal description of the einsum notation, indicating its theoretical and practical viability.

Conclusion: The formalization of the einsum language provides a unified theoretical basis, ensuring its optimization and consistency across platforms and fostering more systematic reasoning and application.

Abstract: In 2011, einsum was introduced to NumPy as a practical and convenient
notation for tensor expressions in machine learning, quantum circuit
simulation, and other fields. It has since been implemented in additional
Python frameworks such as PyTorch and TensorFlow, as well as in other
programming languages such as Julia. Despite its practical success, the einsum
notation still lacks a solid theoretical basis, and is not unified across the
different frameworks, limiting opportunities for formal reasoning and
systematic optimization. In this work, we discuss the terminology of tensor
expressions and provide a formal definition of the einsum language. Based on
this definition, we formalize and prove important equivalence rules for tensor
expressions and highlight their relevance in practical applications.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [189] [HUNT: High-Speed UAV Navigation and Tracking in Unstructured Environments via Instantaneous Relative Frames](https://arxiv.org/abs/2509.19452)
*Alessandro Saviolo,Jeffrey Mao,Giuseppe Loianno*

Main category: cs.RO

TL;DR: This paper introduces HUNT, a high-speed UAV navigation and tracking framework that enables robust search and tracking in unstructured environments using relative navigation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome challenges in search and rescue operations requiring UAVs to navigate unstructured, unknown environments at high speed and track targets without global localization.

Method: The paper presents HUNT, a unified real-time relative navigation framework that uses onboard instantaneous observables (e.g., attitude, altitude, velocity) for reactive high-speed flight and target tracking.

Result: The framework was tested in outdoor environments like dense forests and search-and-rescue scenarios, demonstrating robust autonomy where traditional global-based methods fail.

Conclusion: HUNT seamlessly transitions between search and tracking phases, enabling UAVs to operate effectively in complex, degraded sensing scenarios.

Abstract: Search and rescue operations require unmanned aerial vehicles to both
traverse unknown unstructured environments at high speed and track targets once
detected. Achieving both capabilities under degraded sensing and without global
localization remains an open challenge. Recent works on relative navigation
have shown robust tracking by anchoring planning and control to a visible
detected object, but cannot address navigation when no target is in the field
of view. We present HUNT (High-speed UAV Navigation and Tracking), a real-time
framework that unifies traversal, acquisition, and tracking within a single
relative formulation. HUNT defines navigation objectives directly from onboard
instantaneous observables such as attitude, altitude, and velocity, enabling
reactive high-speed flight during search. Once a target is detected, the same
perception-control pipeline transitions seamlessly to tracking. Outdoor
experiments in dense forests, container compounds, and search-and-rescue
operations with vehicles and mannequins demonstrate robust autonomy where
global methods fail.

</details>


### [190] [ROPA: Synthetic Robot Pose Generation for RGB-D Bimanual Data Augmentation](https://arxiv.org/abs/2509.19454)
*Jason Chen,I-Chun Arthur Liu,Gaurav Sukhatme,Daniel Seita*

Main category: cs.RO

TL;DR: The paper proposes a method, ROPA, to generate synthetic data for training bimanual manipulation robots effectively using RGB-D inputs through imitation learning without expensive real-world data collection.


<details>
  <summary>Details</summary>
Motivation: Collecting diverse and precise real-world data for bimanual manipulation tasks is costly and time-consuming, limiting scalability. Existing augmentation techniques lack effective solutions for RGB-D, eye-to-hand setups.

Method: The proposed ROPA method fine-tunes Stable Diffusion to synthesize RGB and RGB-D observations, generating novel robot poses along with corresponding joint-space action labels. Constrained optimization ensures physical consistency via gripper-to-object contact constraints.

Result: ROPA is evaluated on 5 simulated and 3 real-world tasks, with results across 2625 simulation trials and 300 real-world trials demonstrating that it outperforms existing baselines and ablations.

Conclusion: ROPA provides a scalable solution for RGB and RGB-D data augmentation in eye-to-hand bimanual manipulation setups, enhancing training efficiency and performance for robot manipulation policies.

Abstract: Training robust bimanual manipulation policies via imitation learning
requires demonstration data with broad coverage over robot poses, contacts, and
scene contexts. However, collecting diverse and precise real-world
demonstrations is costly and time-consuming, which hinders scalability. Prior
works have addressed this with data augmentation, typically for either
eye-in-hand (wrist camera) setups with RGB inputs or for generating novel
images without paired actions, leaving augmentation for eye-to-hand
(third-person) RGB-D training with new action labels less explored. In this
paper, we propose Synthetic Robot Pose Generation for RGB-D Bimanual Data
Augmentation (ROPA), an offline imitation learning data augmentation method
that fine-tunes Stable Diffusion to synthesize third-person RGB and RGB-D
observations of novel robot poses. Our approach simultaneously generates
corresponding joint-space action labels while employing constrained
optimization to enforce physical consistency through appropriate
gripper-to-object contact constraints in bimanual scenarios. We evaluate our
method on 5 simulated and 3 real-world tasks. Our results across 2625
simulation trials and 300 real-world trials demonstrate that ROPA outperforms
baselines and ablations, showing its potential for scalable RGB and RGB-D data
augmentation in eye-to-hand bimanual manipulation. Our project website is
available at: https://ropaaug.github.io/.

</details>


### [191] [Self-evolved Imitation Learning in Simulated World](https://arxiv.org/abs/2509.19460)
*Yifan Ye,Jun Cen,Jing Chen,Zhihe Lu*

Main category: cs.RO

TL;DR: This paper introduces Self-Evolved Imitation Learning (SEIL), a framework for improving imitation learning with limited data by generating and refining demonstrations through simulation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to reduce the cost and labor involved in collecting large-scale expert demonstrations necessary for training generalist agents across multiple tasks in imitation learning.

Method: SEIL uses interactions in simulators to iteratively refine few-shot models by generating new demonstrations. It employs dual-level augmentation (model-level using EMA and environment-level adjustments) and uses a selector to curate high-quality trajectories for further training.

Result: SEIL achieves state-of-the-art performance on the LIBERO benchmark with fewer training examples, demonstrating its effectiveness in few-shot imitation learning scenarios.

Conclusion: SEIL offers an efficient and innovative approach to imitation learning, reducing supervision requirements while maintaining competitive performance.

Abstract: Imitation learning has been a trend recently, yet training a generalist agent
across multiple tasks still requires large-scale expert demonstrations, which
are costly and labor-intensive to collect. To address the challenge of limited
supervision, we propose Self-Evolved Imitation Learning (SEIL), a framework
that progressively improves a few-shot model through simulator interactions.
The model first attempts tasksin the simulator, from which successful
trajectories are collected as new demonstrations for iterative refinement. To
enhance the diversity of these demonstrations, SEIL employs dual-level
augmentation: (i) Model-level, using an Exponential Moving Average (EMA) model
to collaborate with the primary model, and (ii) Environment-level, introducing
slight variations in initial object positions. We further introduce a
lightweight selector that filters complementary and informative trajectories
from the generated pool to ensure demonstration quality. These curated samples
enable the model to achieve competitive performance with far fewer training
examples. Extensive experiments on the LIBERO benchmark show that SEIL achieves
a new state-of-the-art performance in few-shot imitation learning scenarios.
Code is available at https://github.com/Jasper-aaa/SEIL.git.

</details>


### [192] [CU-Multi: A Dataset for Multi-Robot Collaborative Perception](https://arxiv.org/abs/2509.19463)
*Doncey Albin,Daniel McGann,Miles Mena,Annika Thomas,Harel Biggie,Xuefei Sun,Steve McGuire,Jonathan P. How,Christoffer Heckman*

Main category: cs.RO

TL;DR: The paper introduces CU-Multi, a new multi-robot dataset addressing current challenges in Collaborative SLAM (C-SLAM) benchmarking.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of existing multi-robot datasets, which often fall short in trajectory overlap, loop closures, and standardization, thus hindering reproducible evaluations.

Method: The authors collected CU-Multi dataset over multiple days at outdoor sites, featuring synchronized multi-robot runs, dense semantic annotations, and various sensing modalities like RGB-D sensing, RTK GPS, and semantic LiDAR.

Result: They successfully created a dataset with significant trajectory overlap, refined ground-truth odometry, and semantic annotations to enable better C-SLAM benchmarking.

Conclusion: CU-Multi establishes a strong foundation for standardized evaluations in multi-robot collaborative perception, addressing existing benchmarking gaps.

Abstract: A central challenge for multi-robot systems is fusing independently gathered
perception data into a unified representation. Despite progress in
Collaborative SLAM (C-SLAM), benchmarking remains hindered by the scarcity of
dedicated multi-robot datasets. Many evaluations instead partition single-robot
trajectories, a practice that may only partially reflect true multi-robot
operations and, more critically, lacks standardization, leading to results that
are difficult to interpret or compare across studies. While several multi-robot
datasets have recently been introduced, they mostly contain short trajectories
with limited inter-robot overlap and sparse intra-robot loop closures. To
overcome these limitations, we introduce CU-Multi, a dataset collected over
multiple days at two large outdoor sites on the University of Colorado Boulder
campus. CU-Multi comprises four synchronized runs with aligned start times and
controlled trajectory overlap, replicating the distinct perspectives of a robot
team. It includes RGB-D sensing, RTK GPS, semantic LiDAR, and refined
ground-truth odometry. By combining overlap variation with dense semantic
annotations, CU-Multi provides a strong foundation for reproducible evaluation
in multi-robot collaborative perception tasks.

</details>


### [193] [Crater Observing Bio-inspired Rolling Articulator (COBRA)](https://arxiv.org/abs/2509.19473)
*Adarsh Salagame,Henry Noyes,Alireza Ramezani,Eric Sihite,Arash Kalantari*

Main category: cs.RO

TL;DR: NASA proposes COBRA, a snake-style robot, to tackle lunar crater terrain challenges.


<details>
  <summary>Details</summary>
Motivation: Navigate harsh terrains in lunar craters to access water ice for human sustainability.

Method: Designed COBRA with dual locomotion modes: slithering and tumbling, equipped with advanced sensors.

Result: Simulation and experimental tests validate COBRA's adaptability and robustness in extreme terrains.

Conclusion: COBRA is robust, efficient, and could revolutionize lunar exploration.

Abstract: NASA aims to establish a sustainable human basecamp on the Moon as a stepping
stone for future missions to Mars and beyond. The discovery of water ice on the
Moon's craters located in permanently shadowed regions, which can provide
drinking water, oxygen, and rocket fuel, is therefore of critical importance.
However, current methods to access lunar ice deposits are limited. While rovers
have been used to explore the lunar surface for decades, they face significant
challenges in navigating harsh terrains, such as permanently shadowed craters,
due to the high risk of immobilization. This report introduces COBRA (Crater
Observing Bio-inspired Rolling Articulator), a multi-modal snake-style robot
designed to overcome mobility challenges in Shackleton Crater's rugged
environment. COBRA combines slithering and tumbling locomotion to adapt to
various crater terrains. In snake mode, it uses sidewinding to traverse flat or
low inclined surfaces, while in tumbling mode, it forms a circular barrel by
linking its head and tail, enabling rapid movement with minimal energy on steep
slopes. Equipped with an onboard computer, stereo camera, inertial measurement
unit, and joint encoders, COBRA facilitates real-time data collection and
autonomous operation. This paper highlights COBRAs robustness and efficiency in
navigating extreme terrains through both simulations and experimental
validation.

</details>


### [194] [OmniVLA: An Omni-Modal Vision-Language-Action Model for Robot Navigation](https://arxiv.org/abs/2509.19480)
*Noriaki Hirose,Catherine Glossop,Dhruv Shah,Sergey Levine*

Main category: cs.RO

TL;DR: This paper introduces OmniVLA, an omni-modal goal conditioning framework for robotic navigation that integrates vision, language, and action for flexibility and robustness.


<details>
  <summary>Details</summary>
Motivation: Existing robotic navigation systems are limited by their reliance on single-modal goal specifications, making them less adaptable to real-world scenarios where multiple modalities often coexist.

Method: The paper proposes a training framework utilizing a vision-language-action backbone, randomized modality fusion, and three primary goal modalities (2D poses, egocentric images, natural language) to train the OmniVLA model.

Result: The OmniVLA model demonstrates strong generalization to unseen environments, robustness with limited modalities, and the ability to follow new natural language instructions. It outperforms specialized baselines across multiple modalities.

Conclusion: OmniVLA represents a significant step towards omni-modal robotic navigation, offering flexibility, scalability, and a foundation for further extension to new modalities and tasks.

Abstract: Humans can flexibly interpret and compose different goal specifications, such
as language instructions, spatial coordinates, or visual references, when
navigating to a destination. In contrast, most existing robotic navigation
policies are trained on a single modality, limiting their adaptability to
real-world scenarios where different forms of goal specification are natural
and complementary. In this work, we present a training framework for robotic
foundation models that enables omni-modal goal conditioning for vision-based
navigation. Our approach leverages a high-capacity vision-language-action (VLA)
backbone and trains with three primary goal modalities: 2D poses, egocentric
images, and natural language, as well as their combinations, through a
randomized modality fusion strategy. This design not only expands the pool of
usable datasets but also encourages the policy to develop richer geometric,
semantic, and visual representations. The resulting model, OmniVLA, achieves
strong generalization to unseen environments, robustness to scarce modalities,
and the ability to follow novel natural language instructions. We demonstrate
that OmniVLA outperforms specialist baselines across modalities and offers a
flexible foundation for fine-tuning to new modalities and tasks. We believe
OmniVLA provides a step toward broadly generalizable and flexible navigation
policies, and a scalable path for building omni-modal robotic foundation
models. We present videos showcasing OmniVLA performance and will release its
checkpoints and training code on our project page.

</details>


### [195] [Supercomputing for High-speed Avoidance and Reactive Planning in Robots](https://arxiv.org/abs/2509.19486)
*Kieran S. Lachmansingh,José R. González-Estrada,Ryan E. Grant,Matthew K. X. J. Pan*

Main category: cs.RO

TL;DR: The paper introduces SHARP, demonstrating the use of high-performance computing (HPC) to achieve millisecond-scale responsiveness for robotic control.


<details>
  <summary>Details</summary>
Motivation: Robotic systems require fast responsiveness in shared human-robot workspaces, but onboard processors are limited in size, power, and cost.

Method: The paper evaluates SHARP via a parallelized multi-goal A* search implemented with MPI on local and remote HPC clusters, in a stress-test involving a 7-DOF robot avoiding foam projectiles.

Result: The system achieved mean planning latencies of 22.9 ms (local) and 30.0 ms (remote, ~300 km away), with avoidance success rates of 84% and 88%, respectively.

Conclusion: SHARP highlights the feasibility of HPC for reactive robotics, suggesting hybrid control architectures with onboard safety mechanisms and offloaded HPC planning for dynamic environments.

Abstract: This paper presents SHARP (Supercomputing for High-speed Avoidance and
Reactive Planning), a proof-of-concept study demonstrating how high-performance
computing (HPC) can enable millisecond-scale responsiveness in robotic control.
While modern robots face increasing demands for reactivity in human--robot
shared workspaces, onboard processors are constrained by size, power, and cost.
Offloading to HPC offers massive parallelism for trajectory planning, but its
feasibility for real-time robotics remains uncertain due to network latency and
jitter. We evaluate SHARP in a stress-test scenario where a 7-DOF manipulator
must dodge high-speed foam projectiles. Using a parallelized multi-goal A*
search implemented with MPI on both local and remote HPC clusters, the system
achieves mean planning latencies of 22.9 ms (local) and 30.0 ms (remote, ~300
km away), with avoidance success rates of 84% and 88%, respectively. These
results show that when round-trip latency remains within the
tens-of-milliseconds regime, HPC-side computation is no longer the bottleneck,
enabling avoidance well below human reaction times. The SHARP results motivate
hybrid control architectures: low-level reflexes remain onboard for safety,
while bursty, high-throughput planning tasks are offloaded to HPC for
scalability. By reporting per-stage timing and success rates, this study
provides a reproducible template for assessing real-time feasibility of
HPC-driven robotics. Collectively, SHARP reframes HPC offloading as a viable
pathway toward dependable, reactive robots in dynamic environments.

</details>


### [196] [A Bimanual Gesture Interface for ROS-Based Mobile Manipulators Using TinyML and Sensor Fusion](https://arxiv.org/abs/2509.19521)
*Najeeb Ahmed Bhuiyan,M. Nasimul Huq,Sakib H. Chowdhury,Rahul Mangharam*

Main category: cs.RO

TL;DR: The paper introduces a dual-hand gesture interface using TinyML, spectral analysis, and sensor fusion to improve the efficiency and coordination of mobile manipulators.


<details>
  <summary>Details</summary>
Motivation: Address gaps in reliability, efficiency, and intuitiveness in gesture-based control systems for mobile manipulators.

Method: Utilized accelerometer, flex sensors, IMU signals processed through TinyML, spectral analysis, and sensor fusion within a ROS framework to enable simultaneous navigation and manipulation.

Result: Achieved real-time, low-power gesture recognition for controlling a 7-DOF Kinova Gen3 manipulator, improving coordination and efficiency over sequential methods.

Conclusion: The proposed scalable, cost-effective, and open-source framework advances Human-Robot Interaction for industrial and assistive applications, with significant deployment potential.

Abstract: Gesture-based control for mobile manipulators faces persistent challenges in
reliability, efficiency, and intuitiveness. This paper presents a dual-hand
gesture interface that integrates TinyML, spectral analysis, and sensor fusion
within a ROS framework to address these limitations. The system uses left-hand
tilt and finger flexion, captured using accelerometer and flex sensors, for
mobile base navigation, while right-hand IMU signals are processed through
spectral analysis and classified by a lightweight neural network. This pipeline
enables TinyML-based gesture recognition to control a 7-DOF Kinova Gen3
manipulator. By supporting simultaneous navigation and manipulation, the
framework improves efficiency and coordination compared to sequential methods.
Key contributions include a bimanual control architecture, real-time low-power
gesture recognition, robust multimodal sensor fusion, and a scalable ROS-based
implementation. The proposed approach advances Human-Robot Interaction (HRI)
for industrial automation, assistive robotics, and hazardous environments,
offering a cost-effective, open-source solution with strong potential for
real-world deployment and further optimization.

</details>


### [197] [Bioinspired SLAM Approach for Unmanned Surface Vehicle](https://arxiv.org/abs/2509.19522)
*Fabio Coelho,Joao Victor T. Borges,Paulo Padrao,Jose Fuentes,Ramon R. Costa,Liu Hsu,Leonardo Bobadilla*

Main category: cs.RO

TL;DR: OpenRatSLAM2 enhances the RatSLAM framework for low-cost visual-inertial SLAM in GPS-denied environments, particularly applied to USVs.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore bioinspired SLAM solutions using computational models of the rodent hippocampus to address challenges in creating cost-effective and reliable SLAM systems for GPS-denied scenarios.

Method: The research employs a ROS2-based architecture applied to waterway datasets, leveraging the Hausdorff distance to benchmark trajectory estimation against ground truth data.

Result: The algorithm successfully generates a semimetric map with error margins acceptable for most robotic applications, demonstrating practical deployment on USVs.

Conclusion: OpenRatSLAM2 proves to be a viable tool for GPS-denied SLAM applications, particularly in USV environments, while providing insights into parameter tuning and bioinspired methodologies.

Abstract: This paper presents OpenRatSLAM2, a new version of OpenRatSLAM - a
bioinspired SLAM framework based on computational models of the rodent
hippocampus. OpenRatSLAM2 delivers low-computation-cost visual-inertial based
SLAM, suitable for GPS-denied environments. Our contributions include a
ROS2-based architecture, experimental results on new waterway datasets, and
insights into system parameter tuning. This work represents the first known
application of RatSLAM on USVs. The estimated trajectory was compared with
ground truth data using the Hausdorff distance. The results show that the
algorithm can generate a semimetric map with an error margin acceptable for
most robotic applications.

</details>


### [198] [Real-Time Reinforcement Learning for Dynamic Tasks with a Parallel Soft Robot](https://arxiv.org/abs/2509.19525)
*James Avtges,Jake Ketchum,Millicent Schlafly,Helena Young,Taekyoung Kim,Allison Pinosky,Ryan L. Truby,Todd D. Murphey*

Main category: cs.RO

TL;DR: The researchers introduced a curriculum-based RL approach to enable real-time learning of dynamic balancing tasks for soft robots, showcasing robustness even under actuator damage.


<details>
  <summary>Details</summary>
Motivation: To address challenges in controlling soft robots due to their nonlinear responses, hysteresis, and risk of actuator damage, as well as inefficiencies in episodic RL training methods.

Method: Using Reinforcement Learning (RL) with curriculum learning on a soft robotic Stewart platform constructed of 3D-printed actuators, allowing real-time hardware training, even in adverse conditions.

Result: The RL approach successfully learned dynamic balancing tasks in real-time with training lasting only 15 minutes. It performed reliably even with actuators disabled or physically damaged.

Conclusion: Real-time RL methods with curriculum learning improve the robustness and real-world applicability of soft robotic systems, enabling more adaptive and capable hardware.

Abstract: Closed-loop control remains an open challenge in soft robotics. The nonlinear
responses of soft actuators under dynamic loading conditions limit the use of
analytic models for soft robot control. Traditional methods of controlling soft
robots underutilize their configuration spaces to avoid nonlinearity,
hysteresis, large deformations, and the risk of actuator damage. Furthermore,
episodic data-driven control approaches such as reinforcement learning (RL) are
traditionally limited by sample efficiency and inconsistency across
initializations. In this work, we demonstrate RL for reliably learning control
policies for dynamic balancing tasks in real-time single-shot hardware
deployments. We use a deformable Stewart platform constructed using parallel,
3D-printed soft actuators based on motorized handed shearing auxetic (HSA)
structures. By introducing a curriculum learning approach based on expanding
neighborhoods of a known equilibrium, we achieve reliable single-deployment
balancing at arbitrary coordinates. In addition to benchmarking the performance
of model-based and model-free methods, we demonstrate that in a single
deployment, Maximum Diffusion RL is capable of learning dynamic balancing after
half of the actuators are effectively disabled, by inducing buckling and by
breaking actuators with bolt cutters. Training occurs with no prior data, in as
fast as 15 minutes, with performance nearly identical to the fully-intact
platform. Single-shot learning on hardware facilitates soft robotic systems
reliably learning in the real world and will enable more diverse and capable
soft robots.

</details>


### [199] [Autonomous Elemental Characterization Enabled by a Low Cost Robotic Platform Built Upon a Generalized Software Architecture](https://arxiv.org/abs/2509.19541)
*Xuan Cao,Yuxin Wu,Michael L. Whittaker*

Main category: cs.RO

TL;DR: The paper introduces a dual-layer software architecture to facilitate robotic automation in scientific laboratories, demonstrated using a low-cost robotic system for material sample characterization.


<details>
  <summary>Details</summary>
Motivation: Robotic automation in scientific laboratories lags due to lack of generalized methodologies and expensive hardware. The paper aims to address this gap by developing a cost-effective, generalizable approach for laboratory automation.

Method: The authors propose a dual-layer design combining Socket.IO and ROS action server architecture, a web-based interface, and a ROS Behavior Tree for planning. For demonstration, they used a low-cost, open-source robot integrating a handheld laser-induced breakdown spectroscopy device with a 3D adapter for automated 2D chemical mapping.

Result: They showcased the automation by creating a dense hyperspectral map (1071 points) of a core sample at a rate of 1520 bits per second, demonstrating effective and precise chemical characterization.

Conclusion: The proposed system enhances laboratory automation in a cost-effective manner, improving chemical quantification and integration with field measurements, especially for analyzing materials like those used in lithium battery supply chains.

Abstract: Despite the rapidly growing applications of robots in industry, the use of
robots to automate tasks in scientific laboratories is less prolific due to
lack of generalized methodologies and high cost of hardware. This paper focuses
on the automation of characterization tasks necessary for reducing cost while
maintaining generalization, and proposes a software architecture for building
robotic systems in scientific laboratory environment. A dual-layer (Socket.IO
and ROS) action server design is the basic building block, which facilitates
the implementation of a web-based front end for user-friendly operations and
the use of ROS Behavior Tree for convenient task planning and execution. A
robotic platform for automating mineral and material sample characterization is
built upon the architecture, with an open source, low-cost three-axis computer
numerical control gantry system serving as the main robot. A handheld laser
induced breakdown spectroscopy (LIBS) analyzer is integrated with a 3D printed
adapter, enabling automated 2D chemical mapping. We demonstrate the utility of
automated chemical mapping by scanning of the surface of a spodumene-bearing
pegmatite core sample with a 1071-point dense hyperspectral map acquired at a
rate of 1520 bits per second. Automated LIBS scanning enables controlled
chemical quantification in the laboratory that complements field-based
measurements acquired with the same handheld device, linking resource
exploration and processing steps in the supply chain for lithium-based battery
materials.

</details>


### [200] [RoMoCo: Robotic Motion Control Toolbox for Reduced-Order Model-Based Locomotion on Bipedal and Humanoid Robots](https://arxiv.org/abs/2509.19545)
*Min Dai,Aaron D. Ames*

Main category: cs.RO

TL;DR: RoMoCo is an open-source C++ toolbox for creating and evaluating reduced-order model planners and controllers for bipedal and humanoid robots.


<details>
  <summary>Details</summary>
Motivation: To provide a unified and modular solution for rapid prototyping and benchmarking of robot locomotion planners and controllers.

Method: A consistent API integrating state-of-the-art planning and whole-body control techniques, applicable to diverse robotic platforms through reduced-order models.

Result: Successfully demonstrated through simulations on Cassie, Unitree H1, and G1 robots, with hardware validations on Cassie and G1.

Conclusion: RoMoCo offers flexibility, performance, and real-world efficacy for designing locomotion controllers across different robot platforms.

Abstract: We present RoMoCo, an open-source C++ toolbox for the synthesis and
evaluation of reduced-order model-based planners and whole-body controllers for
bipedal and humanoid robots. RoMoCo's modular architecture unifies
state-of-the-art planners and whole-body locomotion controllers under a
consistent API, enabling rapid prototyping and reproducible benchmarking. By
leveraging reduced-order models for platform-agnostic gait generation, RoMoCo
enables flexible controller design across diverse robots. We demonstrate its
versatility and performance through extensive simulations on the Cassie,
Unitree H1, and G1 robots, and validate its real-world efficacy with hardware
experiments on the Cassie and G1 humanoids.

</details>


### [201] [AnySafe: Adapting Latent Safety Filters at Runtime via Safety Constraint Parameterization in the Latent Space](https://arxiv.org/abs/2509.19555)
*Sankalp Agrawal,Junwon Seo,Kensuke Nakamura,Ran Tian,Andrea Bajcsy*

Main category: cs.RO

TL;DR: This paper introduces a method to create adaptable safety filters for vision-based control tasks that change based on user-defined constraints during runtime, even for unknown scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing safety filters for vision-based tasks assume static, predefined constraints and lack adaptability across different scenarios.

Method: Using latent-space similarity measures and conformal calibration, the approach trains safety filters in a simulated world model to condition on encoded safety constraints from user-defined images.

Result: The proposed method enables adaptable, runtime safety filtering without compromising overall system performance, as demonstrated in both simulation and hardware experiments with a Franka manipulator.

Conclusion: The method provides a flexible safety solution for vision-based systems, offering runtime adaptability to arbitrary user-defined safety constraints while maintaining performance.

Abstract: Recent works have shown that foundational safe control methods, such as
Hamilton-Jacobi (HJ) reachability analysis, can be applied in the latent space
of world models. While this enables the synthesis of latent safety filters for
hard-to-model vision-based tasks, they assume that the safety constraint is
known a priori and remains fixed during deployment, limiting the safety
filter's adaptability across scenarios. To address this, we propose
constraint-parameterized latent safety filters that can adapt to user-specified
safety constraints at runtime. Our key idea is to define safety constraints by
conditioning on an encoding of an image that represents a constraint, using a
latent-space similarity measure. The notion of similarity to failure is aligned
in a principled way through conformal calibration, which controls how closely
the system may approach the constraint representation. The parameterized safety
filter is trained entirely within the world model's imagination, treating any
image seen by the model as a potential test-time constraint, thereby enabling
runtime adaptation to arbitrary safety constraints. In simulation and hardware
experiments on vision-based control tasks with a Franka manipulator, we show
that our method adapts at runtime by conditioning on the encoding of
user-specified constraint images, without sacrificing performance. Video
results can be found on https://any-safe.github.io

</details>


### [202] [Agentic Scene Policies: Unifying Space, Semantics, and Affordances for Robot Action](https://arxiv.org/abs/2509.19571)
*Sacha Morin,Kumaraditya Gupta,Mahtab Sandhu,Charlie Gauthier,Francesco Argenziano,Kirsty Ellis,Liam Paull*

Main category: cs.RO

TL;DR: Agentic Scene Policies (ASP) is introduced to better interpret and execute open-ended natural language queries in robotics, outperforming existing Vision-Language-Action models (VLAs).


<details>
  <summary>Details</summary>
Motivation: Enhancing robotic systems' ability to effectively interpret and execute complex natural language instructions, especially in novel settings where end-to-end VLAs show limitations.

Method: ASP uses modern scene representations as a queryable interface, enabling advanced semantic, spatial, and affordance-based querying. It reasons explicitly about object affordances to guide language-conditioned robot policies.

Result: ASP outperforms VLAs in tabletop manipulation tasks, handles room-scale queries using affordance-guided navigation, and scales up scene representations effectively.

Conclusion: ASP demonstrates a scalable and effective alternative to VLAs for handling open-ended, language-conditioned tasks in robotic systems, leveraging explicit scene reasoning and affordance-based queries.

Abstract: Executing open-ended natural language queries is a core problem in robotics.
While recent advances in imitation learning and vision-language-actions models
(VLAs) have enabled promising end-to-end policies, these models struggle when
faced with complex instructions and new scenes. An alternative is to design an
explicit scene representation as a queryable interface between the robot and
the world, using query results to guide downstream motion planning. In this
work, we present Agentic Scene Policies (ASP), an agentic framework that
leverages the advanced semantic, spatial, and affordance-based querying
capabilities of modern scene representations to implement a capable
language-conditioned robot policy. ASP can execute open-vocabulary queries in a
zero-shot manner by explicitly reasoning about object affordances in the case
of more complex skills. Through extensive experiments, we compare ASP with VLAs
on tabletop manipulation problems and showcase how ASP can tackle room-level
queries through affordance-guided navigation, and a scaled-up scene
representation. (Project page:
https://montrealrobotics.ca/agentic-scene-policies.github.io/)

</details>


### [203] [Chasing Stability: Humanoid Running via Control Lyapunov Function Guided Reinforcement Learning](https://arxiv.org/abs/2509.19573)
*Zachary Olkin,Kejun Li,William D. Compton,Aaron D. Ames*

Main category: cs.RO

TL;DR: This paper proposes CLF-RL, a method that integrates control Lyapunov functions and optimized dynamic trajectories into RL training to achieve precise, robust humanoid running control systems.


<details>
  <summary>Details</summary>
Motivation: To enable humanoid robots to achieve dynamic behaviors like running, which is challenging due to the need for both precision and robustness in nonlinear dynamics control.

Method: The authors embed control Lyapunov functions and optimized dynamic reference trajectories into reinforcement learning training to shape reward functions without heuristic tuning, ensuring certifiable stability.

Result: The CLF-RL trained policies achieve reliable running behavior both on a treadmill and outdoors, exhibit robustness to disturbances, and accurately track global references using only onboard sensors.

Conclusion: The proposed method advances humanoid running control by grounding RL training in nonlinear control principles, paving the way for integration into fully autonomous systems.

Abstract: Achieving highly dynamic behaviors on humanoid robots, such as running,
requires controllers that are both robust and precise, and hence difficult to
design. Classical control methods offer valuable insight into how such systems
can stabilize themselves, but synthesizing real-time controllers for nonlinear
and hybrid dynamics remains challenging. Recently, reinforcement learning (RL)
has gained popularity for locomotion control due to its ability to handle these
complex dynamics. In this work, we embed ideas from nonlinear control theory,
specifically control Lyapunov functions (CLFs), along with optimized dynamic
reference trajectories into the reinforcement learning training process to
shape the reward. This approach, CLF-RL, eliminates the need to handcraft and
tune heuristic reward terms, while simultaneously encouraging certifiable
stability and providing meaningful intermediate rewards to guide learning. By
grounding policy learning in dynamically feasible trajectories, we expand the
robot's dynamic capabilities and enable running that includes both flight and
single support phases. The resulting policy operates reliably on a treadmill
and in outdoor environments, demonstrating robustness to disturbances applied
to the torso and feet. Moreover, it achieves accurate global reference tracking
utilizing only on-board sensors, making a critical step toward integrating
these dynamic motions into a full autonomy stack.

</details>


### [204] [Terra: Hierarchical Terrain-Aware 3D Scene Graph for Task-Agnostic Outdoor Mapping](https://arxiv.org/abs/2509.19579)
*Chad R. Samuelson,Abigail Austin,Seth Knoop,Blake Romrell,Gabriel R. Slade,Timothy W. McLain,Joshua G. Mangelson*

Main category: cs.RO

TL;DR: This paper focuses on integrating terrain-aware scene graphs for outdoor robotic navigation by fusing geometric mapping with semantic understanding.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional geometric mapping methods which lack semantic understanding necessary for high-level robotic reasoning.

Method: The paper proposes generating terrain-aware place nodes and hierarchical organization for outdoor environments, combining indoor scene graph techniques with outdoor mapping methods.

Result: Evaluation showed comparable performance to state-of-the-art methods in object retrieval and superior accuracy in region classification, while maintaining memory efficiency.

Conclusion: The proposed approach effectively supports diverse robotic tasks by creating lightweight terrain-aware maps suitable for autonomous outdoor operations.

Abstract: Outdoor intelligent autonomous robotic operation relies on a sufficiently
expressive map of the environment. Classical geometric mapping methods retain
essential structural environment information, but lack a semantic understanding
and organization to allow high-level robotic reasoning. 3D scene graphs (3DSGs)
address this limitation by integrating geometric, topological, and semantic
relationships into a multi-level graph-based map. Outdoor autonomous operations
commonly rely on terrain information either due to task-dependence or the
traversability of the robotic platform. We propose a novel approach that
combines indoor 3DSG techniques with standard outdoor geometric mapping and
terrain-aware reasoning, producing terrain-aware place nodes and hierarchically
organized regions for outdoor environments. Our method generates a
task-agnostic metric-semantic sparse map and constructs a 3DSG from this map
for downstream planning tasks, all while remaining lightweight for autonomous
robotic operation. Our thorough evaluation demonstrates our 3DSG method
performs on par with state-of-the-art camera-based 3DSG methods in object
retrieval and surpasses them in region classification while remaining memory
efficient. We demonstrate its effectiveness in diverse robotic tasks of object
retrieval and region monitoring in both simulation and real-world environments.

</details>


### [205] [From Space to Time: Enabling Adaptive Safety with Learned Value Functions via Disturbance Recasting](https://arxiv.org/abs/2509.19597)
*Sander Tonkens,Nikhil Uday Shinde,Azra Begzadić,Michael C. Yip,Jorge Cortés,Sylvia L. Herbert*

Main category: cs.RO

TL;DR: The paper proposes SPACE2TIME, a method for safe and adaptive deployment of safety filters for autonomous systems under unknown spatially-varying disturbances.


<details>
  <summary>Details</summary>
Motivation: To address challenges in ensuring the safe operation of autonomous systems under complex, uncertain environmental disturbances, especially for high-dimensional systems.

Method: Reparameterizing spatial variations in disturbances as temporal variations, allowing the use of offline precomputed value functions during real-time operation.

Result: Validated through quadcopter simulations and experiments, showing significant improvements in safety and performance compared to baseline approaches.

Conclusion: SPACE2TIME provides an effective solution for enhancing the safety and adaptability of autonomous systems in challenging, disturbance-prone environments.

Abstract: The widespread deployment of autonomous systems in safety-critical
environments such as urban air mobility hinges on ensuring reliable,
performant, and safe operation under varying environmental conditions. One such
approach, value function-based safety filters, minimally modifies a nominal
controller to ensure safety. Recent advances leverage offline learned value
functions to scale these safety filters to high-dimensional systems. However,
these methods assume detailed priors on all possible sources of model mismatch,
in the form of disturbances in the environment -- information that is rarely
available in real world settings. Even in well-mapped environments like urban
canyons or industrial sites, drones encounter complex, spatially-varying
disturbances arising from payload-drone interaction, turbulent airflow, and
other environmental factors. We introduce SPACE2TIME, which enables safe and
adaptive deployment of offline-learned safety filters under unknown,
spatially-varying disturbances. The key idea is to reparameterize spatial
variations in disturbance as temporal variations, enabling the use of
precomputed value functions during online operation. We validate SPACE2TIME on
a quadcopter through extensive simulations and hardware experiments,
demonstrating significant improvement over baselines.

</details>


### [206] [Look as You Leap: Planning Simultaneous Motion and Perception for High-DOF Robots](https://arxiv.org/abs/2509.19610)
*Qingxi Meng,Emiliano Flores,Carlos Quintero-Peña,Peizhu Qian,Zachary Kingston,Shannan K. Hamlin,Vaibhav Unhelkar,Lydia E. Kavraki*

Main category: cs.RO

TL;DR: This paper presents a GPU-parallelized probabilistic roadmap planner (PS-PRM) for high-degree-of-freedom robots to perform simultaneous navigation and perception tasks in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the need for robots to effectively achieve simultaneous navigation and perception tasks, especially in dynamic, human-centered environments where these objectives often conflict.

Method: The proposed method combines a neural surrogate model to estimate perception quality and GPU-based parallelism within a probabilistic roadmap planner to handle dynamic environments and high DoFs efficiently.

Result: Experiments show that the PS-PRM outperforms baseline methods in both static and dynamic environments, tested on high-DoF robots in both simulations and real-world scenarios.

Conclusion: The PS-PRM method enhances the ability of robots to navigate and perceive effectively in dynamic environments, improving their operation in human-centered settings like homes and hospitals.

Abstract: In this work, we address the problem of planning robot motions for a
high-degree-of-freedom (DoF) robot that effectively achieves a given perception
task while the robot and the perception target move in a dynamic environment.
Achieving navigation and perception tasks simultaneously is challenging, as
these objectives often impose conflicting requirements. Existing methods that
compute motion under perception constraints fail to account for obstacles, are
designed for low-DoF robots, or rely on simplified models of perception.
Furthermore, in dynamic real-world environments, robots must replan and react
quickly to changes and directly evaluating the quality of perception (e.g.,
object detection confidence) is often expensive or infeasible at runtime. This
problem is especially important in human-centered environments such as homes
and hospitals, where effective perception is essential for safe and reliable
operation. To address these challenges, we propose a GPU-parallelized
perception-score-guided probabilistic roadmap planner with a neural surrogate
model (PS-PRM). The planner explicitly incorporates the estimated quality of a
perception task into motion planning for high-DoF robots. Our method uses a
learned model to approximate perception scores and leverages GPU parallelism to
enable efficient online replanning in dynamic settings. We demonstrate that our
planner, evaluated on high-DoF robots, outperforms baseline methods in both
static and dynamic environments in both simulation and real-robot experiments.

</details>


### [207] [EgoBridge: Domain Adaptation for Generalizable Imitation from Egocentric Human Data](https://arxiv.org/abs/2509.19626)
*Ryan Punamiya,Dhruv Patel,Patcharapong Aphiwetsa,Pranav Kuppili,Lawrence Y. Zhu,Simar Kareer,Judy Hoffman,Danfei Xu*

Main category: cs.RO

TL;DR: This paper proposes EgoBridge, a framework to bridge the gap in policy latent spaces for imitation learning between humans and robots by employing Optimal Transport for feature alignment.


<details>
  <summary>Details</summary>
Motivation: To overcome domain differences in appearance, modalities, and kinematics between humans and robots hindering effective imitation learning.

Method: The EgoBridge framework uses Optimal Transport to align latent spaces and actions between humans and robots while preserving task-relevant features for policy learning.

Result: EgoBridge improves the policy success rate by 44% over baselines in various manipulation tasks and generalizes to new tasks with human data.

Conclusion: EgoBridge effectively bridges the domain gap, enabling better imitation learning and generalization from human experience data to robotic manipulation.

Abstract: Egocentric human experience data presents a vast resource for scaling up
end-to-end imitation learning for robotic manipulation. However, significant
domain gaps in visual appearance, sensor modalities, and kinematics between
human and robot impede knowledge transfer. This paper presents EgoBridge, a
unified co-training framework that explicitly aligns the policy latent spaces
between human and robot data using domain adaptation. Through a measure of
discrepancy on the joint policy latent features and actions based on Optimal
Transport (OT), we learn observation representations that not only align
between the human and robot domain but also preserve the action-relevant
information critical for policy learning. EgoBridge achieves a significant
absolute policy success rate improvement by 44% over human-augmented
cross-embodiment baselines in three real-world single-arm and bimanual
manipulation tasks. EgoBridge also generalizes to new objects, scenes, and
tasks seen only in human data, where baselines fail entirely. Videos and
additional information can be found at https://ego-bridge.github.io

</details>


### [208] [Minimalistic Autonomous Stack for High-Speed Time-Trial Racing](https://arxiv.org/abs/2509.19636)
*Mahmoud Ali,Hassan Jardali,Youwei Yu,Durgakant Pushp,Lantao Liu*

Main category: cs.RO

TL;DR: The paper introduces a streamlined autonomous racing stack designed for high-speed time trials, requiring minimal track time for real-world validation. It achieved impressive results, including a top speed of 206 km/h within just 11 hours of practice.


<details>
  <summary>Details</summary>
Motivation: Autonomous racing faces challenges due to limited access to real-world testing tracks, which slows down validation and development of racing stacks.

Method: The researchers developed a minimalistic autonomous racing stack focused on rapid deployment and efficient integration, minimizing the reliance on extensive on-track testing.

Result: The stack was tested on speedways, achieving a top speed of 206 km/h after 325 km of practice driving within only 11 hours. Performance metrics, including tracking accuracy, vehicle dynamics, and safety, were analyzed.

Conclusion: The study demonstrates the feasibility of quickly developing and deploying an efficient autonomous racing system even with restricted access to testing facilities, offering valuable insights for similar projects.

Abstract: Autonomous racing has seen significant advancements, driven by competitions
such as the Indy Autonomous Challenge (IAC) and the Abu Dhabi Autonomous Racing
League (A2RL). However, developing an autonomous racing stack for a full-scale
car is often constrained by limited access to dedicated test tracks,
restricting opportunities for real-world validation. While previous work
typically requires extended development cycles and significant track time, this
paper introduces a minimalistic autonomous racing stack for high-speed
time-trial racing that emphasizes rapid deployment and efficient system
integration with minimal on-track testing. The proposed stack was validated on
real speedways, achieving a top speed of 206 km/h within just 11 hours'
practice run on the track with 325 km in total. Additionally, we present the
system performance analysis, including tracking accuracy, vehicle dynamics, and
safety considerations, offering insights for teams seeking to rapidly develop
and deploy an autonomous racing stack with limited track access.

</details>


### [209] [RoboSSM: Scalable In-context Imitation Learning via State-Space Models](https://arxiv.org/abs/2509.19658)
*Youngju Yoo,Jiaheng Hu,Yifeng Zhu,Bo Liu,Qiang Liu,Roberto Martín-Martín,Peter Stone*

Main category: cs.RO

TL;DR: The paper introduces RoboSSM, a scalable approach for in-context imitation learning using state-space models. It addresses the shortcomings of Transformer-based models in handling long prompts efficiently.


<details>
  <summary>Details</summary>
Motivation: To overcome the computational limitations and performance issues of Transformer-based in-context imitation learning methods when dealing with long input prompts.

Method: The authors replace Transformers with the Longhorn state-space model (SSM), which provides linear-time inference and robust extrapolation. Experiments are run on the LIBERO benchmark to compare the model with Transformer-based baselines.

Result: RoboSSM demonstrates effective extrapolation for varying numbers of demonstrations, strong performance on new tasks, and robustness in long tasks. It outperforms Transformer-based models in these scenarios.

Conclusion: State-space models like RoboSSM provide a scalable and efficient alternative to Transformers for in-context imitation learning, especially in long-context and few-shot adaptation scenarios.

Abstract: In-context imitation learning (ICIL) enables robots to learn tasks from
prompts consisting of just a handful of demonstrations. By eliminating the need
for parameter updates at deployment time, this paradigm supports few-shot
adaptation to novel tasks. However, recent ICIL methods rely on Transformers,
which have computational limitations and tend to underperform when handling
longer prompts than those seen during training. In this work, we introduce
RoboSSM, a scalable recipe for in-context imitation learning based on
state-space models (SSM). Specifically, RoboSSM replaces Transformers with
Longhorn -- a state-of-the-art SSM that provides linear-time inference and
strong extrapolation capabilities, making it well-suited for long-context
prompts. We evaluate our approach on the LIBERO benchmark and compare it
against strong Transformer-based ICIL baselines. Experiments show that RoboSSM
extrapolates effectively to varying numbers of in-context demonstrations,
yields high performance on unseen tasks, and remains robust in long-horizon
scenarios. These results highlight the potential of SSMs as an efficient and
scalable backbone for ICIL. Our code is available at
https://github.com/youngjuY/RoboSSM.

</details>


### [210] [Memory-Augmented Potential Field Theory: A Framework for Adaptive Control in Non-Convex Domains](https://arxiv.org/abs/2509.19672)
*Dongzhe Zheng,Wenjie Mei*

Main category: cs.RO

TL;DR: This paper introduces a framework combining stochastic optimal control with memory to improve navigation in non-convex landscapes.


<details>
  <summary>Details</summary>
Motivation: Stochastic optimal control methods often fail in non-convex scenarios because they cannot utilize historical data to escape local optima.

Method: The paper proposes Memory-Augmented Potential Field Theory, which constructs dynamic fields encoding historical experience to guide optimization.

Result: A Memory-Augmented Model Predictive Path Integral controller was developed, showing enhanced performance in challenging non-convex environments.

Conclusion: The proposed approach generalizes experience-based learning for control systems, particularly robots, without relying on extensive offline training or domain-specific knowledge.

Abstract: Stochastic optimal control methods often struggle in complex non-convex
landscapes, frequently becoming trapped in local optima due to their inability
to learn from historical trajectory data. This paper introduces
Memory-Augmented Potential Field Theory, a unified mathematical framework that
integrates historical experience into stochastic optimal control. Our approach
dynamically constructs memory-based potential fields that identify and encode
key topological features of the state space, enabling controllers to
automatically learn from past experiences and adapt their optimization
strategy. We provide a theoretical analysis showing that memory-augmented
potential fields possess non-convex escape properties, asymptotic convergence
characteristics, and computational efficiency. We implement this theoretical
framework in a Memory-Augmented Model Predictive Path Integral (MPPI)
controller that demonstrates significantly improved performance in challenging
non-convex environments. The framework represents a generalizable approach to
experience-based learning within control systems (especially robotic dynamics),
enhancing their ability to navigate complex state spaces without requiring
specialized domain knowledge or extensive offline training.

</details>


### [211] [Formal Safety Verification and Refinement for Generative Motion Planners via Certified Local Stabilization](https://arxiv.org/abs/2509.19688)
*Devesh Nath,Haoran Yin,Glen Chou*

Main category: cs.RO

TL;DR: The paper introduces a verification method for generative motion planners (GMPs) using neural network tools to ensure safety and dynamic feasibility. They solve scalability issues by using a small tracking controller to certify closed-loop safety.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of verifying the outputs of learning-based generative motion planners, which often involve large neural networks that exceed the capabilities of current verification tools.

Method: The method involves stabilizing GMP's sampled references with a small neural tracking controller, applying neural network verification techniques to the closed-loop system, and building a library of verified references for safer deployment.

Result: Simulation and hardware tests show enhanced safety for ground robots, quadcopters, and differential-drive robots across diverse types of motion planners like diffusion and vision-language models.

Conclusion: The proposed approach effectively improves the safety of generative motion planners without requiring retraining, demonstrating scalability and application across diverse systems.

Abstract: We present a method for formal safety verification of learning-based
generative motion planners. Generative motion planners (GMPs) offer advantages
over traditional planners, but verifying the safety and dynamic feasibility of
their outputs is difficult since neural network verification (NNV) tools scale
only to a few hundred neurons, while GMPs often contain millions. To preserve
GMP expressiveness while enabling verification, our key insight is to imitate
the GMP by stabilizing references sampled from the GMP with a small neural
tracking controller and then applying NNV to the closed-loop dynamics. This
yields reachable sets that rigorously certify closed-loop safety, while the
controller enforces dynamic feasibility. Building on this, we construct a
library of verified GMP references and deploy them online in a way that
imitates the original GMP distribution whenever it is safe to do so, improving
safety without retraining. We evaluate across diverse planners, including
diffusion, flow matching, and vision-language models, improving safety in
simulation (on ground robots and quadcopters) and on hardware
(differential-drive robot).

</details>


### [212] [Diffusion-Based Impedance Learning for Contact-Rich Manipulation Tasks](https://arxiv.org/abs/2509.19696)
*Noah Geiger,Tamim Asfour,Neville Hogan,Johannes Lachner*

Main category: cs.RO

TL;DR: The paper introduces a framework called Diffusion-Based Impedance Learning, which integrates learning-based motion generation and physical interaction control using a Transformer-based Diffusion Model. Results show high accuracy and adaptability in robotic tasks.


<details>
  <summary>Details</summary>
Motivation: The paper studies the challenge of integrating learning methods optimized for motion generation with physical interaction techniques such as Impedance Control, which demands fine-tuned parameters for effective physical task execution.

Method: The authors utilize a Transformer-based Diffusion Model with a novel SLERP-based quaternion noise scheduler to reconstruct task trajectories and adapt impedance parameters using an energy-based estimator. The approach incorporates cross-attention to external forces and directional rules for impedance adjustment.

Result: The framework demonstrated sub-millimeter positional accuracy and sub-degree rotational accuracy in robotic operations, successfully handling parkour traversal and achieving a perfect success rate in peg insertion tasks with minimal training samples.

Conclusion: The proposed method bridges the gap between learning-based trajectory generation and physical interaction control, offering a scalable and precise solution for robotic systems. The publicly available code encourages further development in Physical AI.

Abstract: Learning methods excel at motion generation in the information domain but are
not primarily designed for physical interaction in the energy domain. Impedance
Control shapes physical interaction but requires task-aware tuning by selecting
feasible impedance parameters. We present Diffusion-Based Impedance Learning, a
framework that combines both domains. A Transformer-based Diffusion Model with
cross-attention to external wrenches reconstructs a simulated Zero-Force
Trajectory (sZFT). This captures both translational and rotational task-space
behavior. For rotations, we introduce a novel SLERP-based quaternion noise
scheduler that ensures geometric consistency. The reconstructed sZFT is then
passed to an energy-based estimator that updates stiffness and damping
parameters. A directional rule is applied that reduces impedance along non task
axes while preserving rigidity along task directions. Training data were
collected for a parkour scenario and robotic-assisted therapy tasks using
teleoperation with Apple Vision Pro. With only tens of thousands of samples,
the model achieved sub-millimeter positional accuracy and sub-degree rotational
accuracy. Its compact model size enabled real-time torque control and
autonomous stiffness adaptation on a KUKA LBR iiwa robot. The controller
achieved smooth parkour traversal within force and velocity limits and 30/30
success rates for cylindrical, square, and star peg insertions without any
peg-specific demonstrations in the training data set. All code for the
Transformer-based Diffusion Model, the robot controller, and the Apple Vision
Pro telemanipulation framework is publicly available. These results mark an
important step towards Physical AI, fusing model-based control for physical
interaction with learning-based methods for trajectory generation.

</details>


### [213] [TopoCut: Learning Multi-Step Cutting with Spectral Rewards and Discrete Diffusion Policies](https://arxiv.org/abs/2509.19712)
*Liquan Wang,Jiangjie Bian,Eric Heiden,Animesh Garg*

Main category: cs.RO

TL;DR: TopoCut introduces a benchmark for robotic cutting tasks, combining a detailed simulation environment, advanced reward modeling, and an integrated learning pipeline.


<details>
  <summary>Details</summary>
Motivation: Robotic cutting of deformable objects is challenging due to complex behaviors, perception difficulties, and a lack of effective evaluation methods.

Method: TopoCut integrates a high-fidelity simulation, a robust reward design system based on eigenanalysis, and a learning pipeline using topology-aware embeddings for particle-based goal-conditioned policy learning.

Result: TopoCut enables trajectory generation, scalable learning, precise evaluation, and generalization across diverse object shapes, sizes, and cutting objectives.

Conclusion: TopoCut provides a comprehensive framework to address challenges in robotic cutting tasks, presenting strong learning and evaluation capabilities while promoting generalization.

Abstract: Robotic manipulation tasks involving cutting deformable objects remain
challenging due to complex topological behaviors, difficulties in perceiving
dense object states, and the lack of efficient evaluation methods for cutting
outcomes. In this paper, we introduce TopoCut, a comprehensive benchmark for
multi-step robotic cutting tasks that integrates a cutting environment and
generalized policy learning. TopoCut is built upon three core components: (1)
We introduce a high-fidelity simulation environment based on a particle-based
elastoplastic solver with compliant von Mises constitutive models, augmented by
a novel damage-driven topology discovery mechanism that enables accurate
tracking of multiple cutting pieces. (2) We develop a comprehensive reward
design that integrates the topology discovery with a pose-invariant spectral
reward model based on Laplace-Beltrami eigenanalysis, facilitating consistent
and robust assessment of cutting quality. (3) We propose an integrated policy
learning pipeline, where a dynamics-informed perception module predicts
topological evolution and produces particle-wise, topology-aware embeddings to
support PDDP (Particle-based Score-Entropy Discrete Diffusion Policy) for
goal-conditioned policy learning. Extensive experiments demonstrate that
TopoCut supports trajectory generation, scalable learning, precise evaluation,
and strong generalization across diverse object geometries, scales, poses, and
cutting goals.

</details>


### [214] [Towards Autonomous Robotic Electrosurgery via Thermal Imaging](https://arxiv.org/abs/2509.19725)
*Naveed D. Riaziat,Joseph Chen,Axel Krieger,Jeremy D. Brown*

Main category: cs.RO

TL;DR: ThERMO, a technique for electrosurgery, uses thermal imaging to optimize tool velocity, reducing tissue damage and improving cutting outcomes.


<details>
  <summary>Details</summary>
Motivation: Current electrosurgery lacks a quantifiable method to optimize cutting velocity, leading to risks of thermal injury and reliance on constant velocity, which fails under varying conditions.

Method: ThERMO utilizes thermal imaging feedback and optimization algorithms to dynamically adjust the tool velocity, reducing thermal injury and balancing cutting force.

Result: ThERMO achieved a threefold improvement in cut success rate, halved peak cutting forces, and demonstrated robustness to environmental disturbances compared to constant velocity methods.

Conclusion: ThERMO presents a promising approach in autonomous electrosurgery, mitigating tissue damage, improving success rates, and adapting to diverse surgical conditions.

Abstract: Electrosurgery is a surgical technique that can improve tissue cutting by
reducing cutting force and bleeding. However, electrosurgery adds a risk of
thermal injury to surrounding tissue. Expert surgeons estimate desirable
cutting velocities based on experience but have no quantifiable reference to
indicate if a particular velocity is optimal. Furthermore, prior demonstrations
of autonomous electrosurgery have primarily used constant tool velocity, which
is not robust to changes in electrosurgical tissue characteristics, power
settings, or tool type. Thermal imaging feedback provides information that can
be used to reduce thermal injury while balancing cutting force by controlling
tool velocity. We introduce Thermography for Electrosurgical Rate Modulation
via Optimization (ThERMO) to autonomously reduce thermal injury while balancing
cutting force by intelligently controlling tool velocity. We demonstrate ThERMO
in tissue phantoms and compare its performance to the constant velocity
approach. Overall, ThERMO improves cut success rate by a factor of three and
can reduce peak cutting force by a factor of two. ThERMO responds to varying
environmental disturbances, reduces damage to tissue, and completes cutting
tasks that would otherwise result in catastrophic failure for the constant
velocity approach.

</details>


### [215] [Simultaneous estimation of contact position and tool shape with high-dimensional parameters using force measurements and particle filtering](https://arxiv.org/abs/2509.19732)
*Kyo Kutsuzawa,Mitsuhiro Hayashibe*

Main category: cs.RO

TL;DR: The paper introduces a method for estimating both the contact position and tool shape during contact tasks, leveraging a particle filter to manage high-dimensional tool parameters.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for contact position estimation often require pre-known tool geometry or are limited to specific tool shapes, leading to inefficiencies in diverse scenarios.

Method: The authors utilize a particle filter to estimate high-dimensional tool shapes and contact positions simultaneously, avoiding direct handling of complex parameter spaces.

Result: The proposed method is validated through simulations and experiments, demonstrating improved accuracy in estimating contact positions while also determining tool shapes.

Conclusion: The technique advances contact state estimation, enabling more accurate and simultaneous determination of contact positions and tool shapes without prior knowledge of tool geometry.

Abstract: Estimating the contact state between a grasped tool and the environment is
essential for performing contact tasks such as assembly and object
manipulation. Force signals are valuable for estimating the contact state, as
they can be utilized even when the contact location is obscured by the tool.
Previous studies proposed methods for estimating contact positions using
force/torque signals; however, most methods require the geometry of the tool
surface to be known. Although several studies have proposed methods that do not
require the tool shape, these methods require considerable time for estimation
or are limited to tools with low-dimensional shape parameters. Here, we propose
a method for simultaneously estimating the contact position and tool shape,
where the tool shape is represented by a grid, which is high-dimensional (more
than 1000 dimensional). The proposed method uses a particle filter in which
each particle has individual tool shape parameters, thereby to avoid directly
handling a high-dimensional parameter space. The proposed method is evaluated
through simulations and experiments using tools with curved shapes on a plane.
Consequently, the proposed method can estimate the shape of the tool
simultaneously with the contact positions, making the contact-position
estimation more accurate.

</details>


### [216] [Trajectory Planning Using Safe Ellipsoidal Corridors as Projections of Orthogonal Trust Regions](https://arxiv.org/abs/2509.19734)
*Akshay Jaitly,Jon Arrizabalaga,Guanrui Li*

Main category: cs.RO

TL;DR: The paper introduces a new approach to trajectory planning in robotics by representing trajectories in a convex Cartesian product of balls to address scalability and time allocation challenges in complex environments.


<details>
  <summary>Details</summary>
Motivation: Existing planners for collision-free trajectory planning struggle with scaling in complex environments and require explicit time allocation for trajectory segments.

Method: The paper parameterizes trajectories using convex Cartesian products of balls, avoiding explicit time allocations, and develops a specialized solver for the Orthogonal Trust Region Problem (Orth-TRP) to enable efficient optimization.

Result: Their approach demonstrated smoother trajectories and lower runtimes compared to state-of-the-art planners, particularly in complex environments.

Conclusion: The proposed method addresses scalability and time allocation issues effectively, with demonstrated efficiency and smoothness, making it promising for complex robotic environments.

Abstract: Planning collision free trajectories in complex environments remains a core
challenge in robotics. Existing corridor based planners which rely on
decomposition of the free space into collision free subsets scale poorly with
environmental complexity and require explicit allocations of time windows to
trajectory segments. We introduce a new trajectory parameterization that
represents trajectories in a nonconvex collision free corridor as being in a
convex cartesian product of balls. This parameterization allows us to decouple
problem size from geometric complexity of the solution and naturally avoids
explicit time allocation by allowing trajectories to evolve continuously inside
ellipsoidal corridors. Building on this representation, we formulate the
Orthogonal Trust Region Problem (Orth-TRP), a specialized convex program with
separable block constraints, and develop a solver that exploits this parallel
structure and the unique structure of each parallel subproblem for efficient
optimization. Experiments on a quadrotor trajectory planning benchmark show
that our approach produces smoother trajectories and lower runtimes than
state-of-the-art corridor based planners, especially in highly complicated
environments.

</details>


### [217] [Beyond Human Demonstrations: Diffusion-Based Reinforcement Learning to Generate Data for VLA Training](https://arxiv.org/abs/2509.19752)
*Rushuai Yang,Hangxing Wei,Ran Zhang,Zhiyuan Feng,Xiaoyu Chen,Tong Li,Chuheng Zhang,Li Zhao,Jiang Bian,Xiu Su,Yi Chen*

Main category: cs.RO

TL;DR: The paper introduces a modified diffusion algorithm to enhance reinforcement learning (RL) for generating high-quality trajectories, improving vision-language-action (VLA) model training without expensive human demonstration data.


<details>
  <summary>Details</summary>
Motivation: The high cost and effort of acquiring large-scale human demonstration data for VLA models limit their scalability, necessitating an autonomous alternative for generating demonstrations.

Method: The authors propose a modified diffusion policy optimization algorithm that leverages the high expressiveness of diffusion models and iterative denoising to generate high-quality, smooth, and diverse trajectories.

Result: Using the LIBERO benchmark with 130 long-horizon manipulation tasks, the diffusion-based RL-generated trajectories outperformed human and Gaussian RL-generated data in smoothness and quality, achieving an 81.9% success rate in VLA training.

Conclusion: Diffusion RL offers a scalable and effective alternative to human and traditional RL demonstration data, enabling high-quality training for VLA models with superior results.

Abstract: Vision-language-action (VLA) models have shown strong generalization across
tasks and embodiments; however, their reliance on large-scale human
demonstrations limits their scalability owing to the cost and effort of manual
data collection. Reinforcement learning (RL) offers a potential alternative to
generate demonstrations autonomously, yet conventional RL algorithms often
struggle on long-horizon manipulation tasks with sparse rewards. In this paper,
we propose a modified diffusion policy optimization algorithm to generate
high-quality and low-variance trajectories, which contributes to a diffusion
RL-powered VLA training pipeline. Our algorithm benefits from not only the high
expressiveness of diffusion models to explore complex and diverse behaviors but
also the implicit regularization of the iterative denoising process to yield
smooth and consistent demonstrations. We evaluate our approach on the LIBERO
benchmark, which includes 130 long-horizon manipulation tasks, and show that
the generated trajectories are smoother and more consistent than both human
demonstrations and those from standard Gaussian RL policies. Further, training
a VLA model exclusively on the diffusion RL-generated data achieves an average
success rate of 81.9%, which outperforms the model trained on human data by
+5.3% and that on Gaussian RL-generated data by +12.6%. The results highlight
our diffusion RL as an effective alternative for generating abundant,
high-quality, and low-variance demonstrations for VLA models.

</details>


### [218] [DynaFlow: Dynamics-embedded Flow Matching for Physically Consistent Motion Generation from State-only Demonstrations](https://arxiv.org/abs/2509.19804)
*Sowoo Lee,Dongyun Kang,Jaehyun Park,Hae-Won Park*

Main category: cs.RO

TL;DR: DynaFlow is a framework integrating a differentiable simulator with a flow matching model to produce physically consistent state trajectories from state-only demonstrations, with real-world validation on a quadruped robot.


<details>
  <summary>Details</summary>
Motivation: There is a need to bridge the gap between kinematic demonstrations and real-world executable motions, ensuring physical consistency in state trajectories and enabling better deployment of robotic motion learning from state-only datasets.

Method: DynaFlow embeds a differentiable simulator into a flow matching model, enabling trajectory generation in action space that maps to dynamically feasible state trajectories. The framework learns end-to-end from state-only demonstrations.

Result: DynaFlow demonstrates effectiveness through quantitative evaluations and physical deployment on a Go1 quadruped robot. The robot reproduces diverse dataset gaits, executes long-horizon motions, and converts infeasible kinematic demonstrations into feasible, stylistic behaviors.

Conclusion: DynaFlow produces physically consistent, deployable motions directly from state-only demonstrations, providing an effective solution for connecting kinematic data with real-world robotic applications.

Abstract: This paper introduces DynaFlow, a novel framework that embeds a
differentiable simulator directly into a flow matching model. By generating
trajectories in the action space and mapping them to dynamically feasible state
trajectories via the simulator, DynaFlow ensures all outputs are physically
consistent by construction. This end-to-end differentiable architecture enables
training on state-only demonstrations, allowing the model to simultaneously
generate physically consistent state trajectories while inferring the
underlying action sequences required to produce them. We demonstrate the
effectiveness of our approach through quantitative evaluations and showcase its
real-world applicability by deploying the generated actions onto a physical Go1
quadruped robot. The robot successfully reproduces diverse gait present in the
dataset, executes long-horizon motions in open-loop control and translates
infeasible kinematic demonstrations into dynamically executable, stylistic
behaviors. These hardware experiments validate that DynaFlow produces
deployable, highly effective motions on real-world hardware from state-only
demonstrations, effectively bridging the gap between kinematic data and
real-world execution.

</details>


### [219] [Where Did I Leave My Glasses? Open-Vocabulary Semantic Exploration in Real-World Semi-Static Environments](https://arxiv.org/abs/2509.19851)
*Benjamin Bogenberger,Oliver Harrison,Orrin Dahanaggamaarachchi,Lukas Brunke,Jingxing Qian,Siqi Zhou,Angela P. Schoellig*

Main category: cs.RO

TL;DR: The paper introduces an innovative semantic exploration system for robots in semi-static environments, aiming to adapt to changes and enable efficient object-goal navigation.


<details>
  <summary>Details</summary>
Motivation: To address the lack of persistent object-level instance tracking in existing semantic exploration research, which limits robots’ ability to adapt to semi-static environmental changes.

Method: The system builds a probabilistic model for object stationarity, actively explores areas with potential changes, and integrates semantic richness with LLM-based reasoning for efficient map maintenance and navigation.

Result: The system detects 95% of map changes, achieves near-perfect mapping precision (within 2% of a fully rebuilt map), improves exploration efficiency by over 29%, and completes object-goal navigation 14% faster than competing strategies.

Conclusion: The proposed system significantly enhances robot adaptability in semi-static environments through systematic mapping, active exploration, and semantic reasoning, making it more practical for real-world applications.

Abstract: Robots deployed in real-world environments, such as homes, must not only
navigate safely but also understand their surroundings and adapt to environment
changes. To perform tasks efficiently, they must build and maintain a semantic
map that accurately reflects the current state of the environment. Existing
research on semantic exploration largely focuses on static scenes without
persistent object-level instance tracking. A consistent map is, however,
crucial for real-world robotic applications where objects in the environment
can be removed, reintroduced, or shifted over time. In this work, to close this
gap, we propose an open-vocabulary, semantic exploration system for semi-static
environments. Our system maintains a consistent map by building a probabilistic
model of object instance stationarity, systematically tracking semi-static
changes, and actively exploring areas that have not been visited for a
prolonged period of time. In addition to active map maintenance, our approach
leverages the map's semantic richness with LLM-based reasoning for
open-vocabulary object-goal navigation. This enables the robot to search more
efficiently by prioritizing contextually relevant areas. We evaluate our
approach across multiple real-world semi-static environments. Our system
detects 95% of map changes on average, improving efficiency by more than 29% as
compared to random and patrol baselines. Overall, our approach achieves a
mapping precision within 2% of a fully rebuilt map while requiring
substantially less exploration and further completes object goal navigation
tasks about 14% faster than the next-best tested strategy (coverage
patrolling). A video of our work can be found at
http://tiny.cc/sem-explor-semi-static .

</details>


### [220] [SAGE:State-Aware Guided End-to-End Policy for Multi-Stage Sequential Tasks via Hidden Markov Decision Process](https://arxiv.org/abs/2509.19853)
*BinXu Wu,TengFei Zhang,Chen Yang,JiaHao Wen,HaoCheng Li,JingTian Ma,Zhen Chen,JingYuan Wang*

Main category: cs.RO

TL;DR: The paper introduces SAGE, an imitation learning framework enhancing robotic manipulation tasks by addressing state ambiguities. It achieves significant success in real-world experiments with minimal manual labeling.


<details>
  <summary>Details</summary>
Motivation: Robotic manipulation tasks frequently encounter state ambiguity, where similar observations can lead to incorrect actions, necessitating solutions for improved task performance.

Method: The paper models tasks using a Hidden Markov Decision Process (HMDP) and introduces a state transition network for hidden state inference combined with a state-aware action policy to produce accurate actions.

Result: SAGE achieved 100% success in real-world experiments for complex robotic tasks, outclassing baseline models, and demonstrated significant efficiency requiring minimal manual labeling (13%).

Conclusion: SAGE effectively resolves state ambiguity in sequential robotic manipulation tasks, demonstrating strong performance and requiring minimal annotation effort.

Abstract: Multi-stage sequential (MSS) robotic manipulation tasks are prevalent and
crucial in robotics. They often involve state ambiguity, where visually similar
observations correspond to different actions. We present SAGE, a state-aware
guided imitation learning framework that models tasks as a Hidden Markov
Decision Process (HMDP) to explicitly capture latent task stages and resolve
ambiguity. We instantiate the HMDP with a state transition network that infers
hidden states, and a state-aware action policy that conditions on both
observations and hidden states to produce actions, thereby enabling
disambiguation across task stages. To reduce manual annotation effort, we
propose a semi-automatic labeling pipeline combining active learning and soft
label interpolation. In real-world experiments across multiple complex MSS
tasks with state ambiguity, SAGE achieved 100% task success under the standard
evaluation protocol, markedly surpassing the baselines. Ablation studies
further show that such performance can be maintained with manual labeling for
only about 13% of the states, indicating its strong effectiveness.

</details>


### [221] [D3Grasp: Diverse and Deformable Dexterous Grasping for General Objects](https://arxiv.org/abs/2509.19892)
*Keyu Wang,Bingcong Lu,Zhengxue Cheng,Hengdi Zhang,Li Song*

Main category: cs.RO

TL;DR: D3Grasp is a reinforcement learning framework integrating visual and tactile inputs to achieve robust, diverse, and adaptable dexterous grasping for both rigid and deformable objects.


<details>
  <summary>Details</summary>
Motivation: Dexterous grasping, especially for deformable and diverse objects, faces challenges due to high-dimensional action spaces and perceptual uncertainty.

Method: The paper introduces a unified multimodal representation combining visual and tactile inputs, uses an asymmetric reinforcement learning architecture leveraging privileged information during training, and designs a strategy to ensure kinematically feasible grasps.

Result: D3Grasp achieves a 95.1% average success rate in real-world tests, surpassing previous methods for both rigid and deformable object handling.

Conclusion: The framework significantly advances dexterous grasping capabilities, enhancing generalization, adaptability, and real-world applicability under uncertainty.

Abstract: Achieving diverse and stable dexterous grasping for general and deformable
objects remains a fundamental challenge in robotics, due to high-dimensional
action spaces and uncertainty in perception. In this paper, we present D3Grasp,
a multimodal perception-guided reinforcement learning framework designed to
enable Diverse and Deformable Dexterous Grasping. We firstly introduce a
unified multimodal representation that integrates visual and tactile perception
to robustly grasp common objects with diverse properties. Second, we propose an
asymmetric reinforcement learning architecture that exploits privileged
information during training while preserving deployment realism, enhancing both
generalization and sample efficiency. Third, we meticulously design a training
strategy to synthesize contact-rich, penetration-free, and kinematically
feasible grasps with enhanced adaptability to deformable and contact-sensitive
objects. Extensive evaluations confirm that D3Grasp delivers highly robust
performance across large-scale and diverse object categories, and substantially
advances the state of the art in dexterous grasping for deformable and
compliant objects, even under perceptual uncertainty and real-world
disturbances. D3Grasp achieves an average success rate of 95.1% in real-world
trials,outperforming prior methods on both rigid and deformable objects
benchmarks.

</details>


### [222] [GUIDE: A Diffusion-Based Autonomous Robot Exploration Framework Using Global Graph Inference](https://arxiv.org/abs/2509.19916)
*Zijun Che,Yinghong Zhang,Shengyi Liang,Boyu Zhou,Jun Ma,Jinni Zhou*

Main category: cs.RO

TL;DR: The paper presents GUIDE, a framework for efficient autonomous indoor exploration using a graph-based representation and diffusion policy for better decision-making.


<details>
  <summary>Details</summary>
Motivation: To improve autonomous exploration in structured and complex indoor environments, which is currently hindered by limitations in modeling unobserved spaces and planning efficient paths.

Method: The proposed GUIDE framework integrates a region-evaluation global graph representation with a diffusion policy network. The graph accounts for observed and predicted unexplored areas, and the diffusion policy generates foresighted actions with reduced denoising.

Result: GUIDE achieves up to 18.3% faster coverage completion and a 34.9% reduction in redundant movements in both simulations and real-world tests.

Conclusion: The GUIDE framework demonstrates significant improvements over existing methods in terms of exploration efficiency and path planning, indicating its potential as a transformational tool in autonomous navigation.

Abstract: Autonomous exploration in structured and complex indoor environments remains
a challenging task, as existing methods often struggle to appropriately model
unobserved space and plan globally efficient paths. To address these
limitations, we propose GUIDE, a novel exploration framework that
synergistically combines global graph inference with diffusion-based
decision-making. We introduce a region-evaluation global graph representation
that integrates both observed environmental data and predictions of unexplored
areas, enhanced by a region-level evaluation mechanism to prioritize reliable
structural inferences while discounting uncertain predictions. Building upon
this enriched representation, a diffusion policy network generates stable,
foresighted action sequences with significantly reduced denoising steps.
Extensive simulations and real-world deployments demonstrate that GUIDE
consistently outperforms state-of-the-art methods, achieving up to 18.3% faster
coverage completion and a 34.9% reduction in redundant movements.

</details>


### [223] [Robot Trajectron V2: A Probabilistic Shared Control Framework for Navigation](https://arxiv.org/abs/2509.19954)
*Pinhao Song,Yurui Du,Ophelie Saussus,Sofie De Schrijver,Irene Caprara,Peter Janssen,Renaud Detry*

Main category: cs.RO

TL;DR: This paper introduces RT-V2, a probabilistic control system for safe and effective human-robot interaction, especially in assistive navigation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve intent prediction and enhance safety and efficiency in shared human-robot interaction by addressing the noisy and uncertain nature of human user input.

Method: RT-V2 combines a prior intent model (using RNNs and CVAEs) with posterior updates to account for real-time user input and context. The approach integrates probabilistic modeling, reinforcement learning, and safe optimization techniques.

Result: Experiments on synthetic benchmarks, human studies, and brain-machine interfaces demonstrate superior performance of RT-V2 in intent estimation, safety, and navigation efficiency compared to the state of the art.

Conclusion: RT-V2 is a principled, generalizable shared-control framework that balances user autonomy and assistive intervention, offering significant potential for diverse assistive technologies.

Abstract: We propose a probabilistic shared-control solution for navigation, called
Robot Trajectron V2 (RT-V2), that enables accurate intent prediction and safe,
effective assistance in human-robot interaction. RT-V2 jointly models a user's
long-term behavioral patterns and their noisy, low-dimensional control signals
by combining a prior intent model with a posterior update that accounts for
real-time user input and environmental context. The prior captures the
multimodal and history-dependent nature of user intent using recurrent neural
networks and conditional variational autoencoders, while the posterior
integrates this with uncertain user commands to infer desired actions. We
conduct extensive experiments to validate RT-V2 across synthetic benchmarks,
human-computer interaction studies with keyboard input, and brain-machine
interface experiments with non-human primates. Results show that RT-V2
outperforms the state of the art in intent estimation, provides safe and
efficient navigation support, and adequately balances user autonomy with
assistive intervention. By unifying probabilistic modeling, reinforcement
learning, and safe optimization, RT-V2 offers a principled and generalizable
approach to shared control for diverse assistive technologies.

</details>


### [224] [Generalist Robot Manipulation beyond Action Labeled Data](https://arxiv.org/abs/2509.19958)
*Alexander Spiridonov,Jan-Nico Zaech,Nikolay Nikolov,Luc Van Gool,Danda Pani Paudel*

Main category: cs.RO

TL;DR: The paper introduces a method to enhance robot manipulation by learning efficiently from unlabeled human and robot demonstration videos, using 3D point clouds and a self-supervised dynamics predictor.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of scaling high-quality, action-labeled robot demonstration data, which existing robot manipulation methods require but are costly and difficult to obtain.

Method: The method uses dense 3D point clouds at hand or gripper locations, a self-supervised 3D dynamics predictor, and minimal labeled data to tune the predictor into an action predictor.

Result: The method improves generalist robot performance, enables out-of-action generalization, and supports learning tasks from unlabeled demonstrations in both real-world and simulated settings.

Conclusion: The proposed approach allows robots to generalize better and learn new tasks efficiently without requiring extensive labeled demonstration data.

Abstract: Recent advances in generalist robot manipulation leverage pre-trained
Vision-Language Models (VLMs) and large-scale robot demonstrations to tackle
diverse tasks in a zero-shot manner. A key challenge remains: scaling
high-quality, action-labeled robot demonstration data, which existing methods
rely on for robustness and generalization. To address this, we propose a method
that benefits from videos without action labels - featuring humans and/or
robots in action - enhancing open-vocabulary performance and enabling
data-efficient learning of new tasks. Our method extracts dense, dynamic 3D
point clouds at the hand or gripper location and uses a proposed 3D dynamics
predictor for self-supervision. This predictor is then tuned to an action
predictor using a smaller labeled dataset for action alignment. We show that
our method not only learns from unlabeled human and robot demonstrations -
improving downstream generalist robot policies - but also enables robots to
learn new tasks without action labels (i.e., out-of-action generalization) in
both real-world and simulated settings.

</details>


### [225] [An effective control of large systems of active particles: An application to evacuation problem](https://arxiv.org/abs/2509.19972)
*Albina Klepach,Egor E. Nuzhin,Alexey A. Tsukanov,Nikolay V. Brilliantov*

Main category: cs.RO

TL;DR: The paper proposes a novel leader-based control strategy using reinforcement learning and artificial forces for guiding active particles, aimed at effective crowd evacuation.


<details>
  <summary>Details</summary>
Motivation: The paper addresses challenges in managing large systems of active particles across multiple domains, focusing on scalability, robustness, and removing the need for individualized agent control.

Method: A leader-based control strategy is developed that combines reinforcement learning with artificial forces within the generalized Vicsek model.

Result: The results demonstrate that the proposed approach achieves robust and efficient evacuation strategies, outperforming straightforward reinforcement learning methods.

Conclusion: The method provides advancements in large-scale particle control, with publicly accessible code for further studies and applications.

Abstract: Manipulation of large systems of active particles is a serious challenge
across diverse domains, including crowd management, control of robotic swarms,
and coordinated material transport. The development of advanced control
strategies for complex scenarios is hindered, however, by the lack of
scalability and robustness of the existing methods, in particular, due to the
need of an individual control for each agent. One possible solution involves
controlling a system through a leader or a group of leaders, which other agents
tend to follow. Using such an approach we develop an effective control strategy
for a leader, combining reinforcement learning (RL) with artificial forces
acting on the system. To describe the guidance of active particles by a leader
we introduce the generalized Vicsek model. This novel method is then applied to
the problem of the effective evacuation by a robot-rescuer (leader) of large
groups of people from hazardous places. We demonstrate, that while a
straightforward application of RL yields suboptimal results, even for advanced
architectures, our approach provides a robust and efficient evacuation
strategy. The source code supporting this study is publicly available at:
https://github.com/cinemere/evacuation.

</details>


### [226] [Lidar-based Tracking of Traffic Participants with Sensor Nodes in Existing Urban Infrastructure](https://arxiv.org/abs/2509.20009)
*Simon Schäfer,Bassam Alrifaee,Ehsan Hashemi*

Main category: cs.RO

TL;DR: This study introduces a scalable, lidar-only roadside tracking framework optimized for urban environments, ensuring real-time processing on CPU-only edge hardware without relying on GPUs.


<details>
  <summary>Details</summary>
Motivation: Traditional remote sensing systems are expensive and resource-intensive, especially in urban areas with unpredictable environmental challenges, creating the need for scalable, efficient, and cost-effective tracking solutions.

Method: The approach integrates a lidar sensor with an edge computing unit, executing extended Kalman filtering for state updates, Bayesian updates for dimension estimation, a footprint-based lookup table for class updates, and bounding-box consistency for existence estimation.

Result: The solution achieves real-time performance, processing within 100 milliseconds for 99.88% of messages, with impressive detection rates and robustness against challenges like wind and vibration.

Conclusion: This lidar-based framework enables scalable, cost-effective, and privacy-respecting urban deployments through integration with existing city infrastructure, minimizing deployment costs.

Abstract: This paper presents a lidar-only state estimation and tracking framework,
along with a roadside sensing unit for integration with existing urban
infrastructure. Urban deployments demand scalable, real-time tracking
solutions, yet traditional remote sensing remains costly and computationally
intensive, especially under perceptually degraded conditions. Our sensor node
couples a single lidar with an edge computing unit and runs a computationally
efficient, GPU-free observer that simultaneously estimates object state, class,
dimensions, and existence probability. The pipeline performs: (i) state updates
via an extended Kalman filter, (ii) dimension estimation using a 1D
grid-map/Bayesian update, (iii) class updates via a lookup table driven by the
most probable footprint, and (iv) existence estimation from track age and
bounding-box consistency. Experiments in dynamic urban-like scenes with diverse
traffic participants demonstrate real-time performance and high precision: The
complete end-to-end pipeline finishes within \SI{100}{\milli\second} for
\SI{99.88}{\%} of messages, with an excellent detection rate. Robustness is
further confirmed under simulated wind and sensor vibration. These results
indicate that reliable, real-time roadside tracking is feasible on CPU-only
edge hardware, enabling scalable, privacy-friendly deployments within existing
city infrastructure. The framework integrates with existing poles, traffic
lights, and buildings, reducing deployment costs and simplifying large-scale
urban rollouts and maintenance efforts.

</details>


### [227] [MARG: MAstering Risky Gap Terrains for Legged Robots with Elevation Mapping](https://arxiv.org/abs/2509.20036)
*Yinzhao Dong,Ji Ma,Liu Zhao,Wanyue Li,Peng Lu*

Main category: cs.RO

TL;DR: The paper introduces MARG, a DRL controller integrating terrain information and proprioception to enhance locomotion stability in risky terrains. It uses privileged information during training and a terrain map generation model with one LiDAR for zero-shot policy transfer.


<details>
  <summary>Details</summary>
Motivation: Despite advancements in quadrupedal locomotion, current controllers struggle with risky gap terrains due to challenges in perceiving and navigating complex environments. Existing perception-based methods are also resource-intensive.

Method: The researchers developed MARG, a DRL-based controller using simulated privileged information and a terrain map generation model with one LiDAR. Three foot-related rewards were included to improve safe foothold navigation.

Result: MARG demonstrated stability across various risky terrain tasks in experimental testing.

Conclusion: MARG offers a stable and resource-efficient approach to locomotion in complex terrains, leveraging terrain mapping and proprioception for improved robotic performance.

Abstract: Deep Reinforcement Learning (DRL) controllers for quadrupedal locomotion have
demonstrated impressive performance on challenging terrains, allowing robots to
execute complex skills such as climbing, running, and jumping. However,
existing blind locomotion controllers often struggle to ensure safety and
efficient traversal through risky gap terrains, which are typically highly
complex, requiring robots to perceive terrain information and select
appropriate footholds during locomotion accurately. Meanwhile, existing
perception-based controllers still present several practical limitations,
including a complex multi-sensor deployment system and expensive computing
resource requirements. This paper proposes a DRL controller named MAstering
Risky Gap Terrains (MARG), which integrates terrain maps and proprioception to
dynamically adjust the action and enhance the robot's stability in these tasks.
During the training phase, our controller accelerates policy optimization by
selectively incorporating privileged information (e.g., center of mass,
friction coefficients) that are available in simulation but unmeasurable
directly in real-world deployments due to sensor limitations. We also designed
three foot-related rewards to encourage the robot to explore safe footholds.
More importantly, a terrain map generation (TMG) model is proposed to reduce
the drift existing in mapping and provide accurate terrain maps using only one
LiDAR, providing a foundation for zero-shot transfer of the learned policy. The
experimental results indicate that MARG maintains stability in various risky
terrain tasks.

</details>


### [228] [LLM Trainer: Automated Robotic Data Generating via Demonstration Augmentation using LLMs](https://arxiv.org/abs/2509.20070)
*Abraham George,Amir Barati Farimani*

Main category: cs.RO

TL;DR: LLM Trainer automates robot dataset generation by leveraging LLMs for transforming a small number of human demonstrations into reusable, adaptable trajectories.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of creating large robot datasets for imitation learning from limited human demonstrations, aiming to improve efficiency and usability.

Method: The approach involves offline annotation (extracting keyframes and pose-object relations) and online retargeting (adapting annotations to new scenes). Probabilistic Thompson Sampling further optimizes the annotation.

Result: The method outperforms expert baselines in data annotation success, enables robot demonstrations across tasks, combines feed-forward plans with feedback controllers, and demonstrates effectiveness on hardware.

Conclusion: This pipeline augments robotic imitation learning by automating dataset generation, reducing human effort, and validating its application with successful results on hardware systems.

Abstract: We present LLM Trainer, a fully automated pipeline that leverages the world
knowledge of Large Language Models (LLMs) to transform a small number of human
demonstrations (as few as one) into a large robot dataset for imitation
learning. Our approach decomposes demonstration generation into two steps: (1)
offline demonstration annotation that extracts keyframes, salient objects, and
pose-object relations; and (2) online keypose retargeting that adapts those
keyframes to a new scene, given an initial observation. Using these modified
keypoints, our system warps the original demonstration to generate a new
trajectory, which is then executed, and the resulting demo, if successful, is
saved. Because the annotation is reusable across scenes, we use Thompson
sampling to optimize the annotation, significantly improving generation success
rate. We evaluate our method on a range of tasks, and find that our data
annotation method consistently outperforms expert-engineered baselines. We
further show an ensemble policy that combines the optimized LLM feed-forward
plan with a learned feedback imitation learning controller. Finally, we
demonstrate hardware feasibility on a Franka Emika Panda robot. For additional
materials and demonstration videos, please see the project website:
https://sites.google.com/andrew.cmu.edu/llm-trainer

</details>


### [229] [Queryable 3D Scene Representation: A Multi-Modal Framework for Semantic Reasoning and Robotic Task Planning](https://arxiv.org/abs/2509.20077)
*Xun Li,Rodrigo Santa Cruz,Mingze Xi,Hu Zhang,Madhawa Perera,Ziwei Wang,Ahalya Ravendran,Brandon J. Matthews,Feng Xu,Matt Adcock,Dadong Wang,Jiajun Liu*

Main category: cs.RO

TL;DR: The paper introduces the 3D Queryable Scene Representation (3D QSR) framework, enabling robots to interpret and interact meaningfully with 3D environments for executing high-level human instructions.


<details>
  <summary>Details</summary>
Motivation: Robots must achieve comprehensive scene understanding to interpret high-level instructions and perform tasks in complex 3D environments.

Method: The proposed framework unifies three 3D representations (novel view rendering from panoptic reconstruction, precise geometry from 3D point clouds, and 3D scene graphs). It incorporates vision-language models to link multimodal object embeddings and enables robotic task planning by combining spatial and semantic reasoning.

Result: The framework was evaluated using both Unity-simulated scenarios and a real wet lab environment, demonstrating its ability to bridge abstract human instructions with robotic task planning in complex 3D settings.

Conclusion: The 3D QSR framework enhances scene understanding, enabling robots to precisely execute tasks by integrating spatial and semantic data into task planning.

Abstract: To enable robots to comprehend high-level human instructions and perform
complex tasks, a key challenge lies in achieving comprehensive scene
understanding: interpreting and interacting with the 3D environment in a
meaningful way. This requires a smart map that fuses accurate geometric
structure with rich, human-understandable semantics. To address this, we
introduce the 3D Queryable Scene Representation (3D QSR), a novel framework
built on multimedia data that unifies three complementary 3D representations:
(1) 3D-consistent novel view rendering and segmentation from panoptic
reconstruction, (2) precise geometry from 3D point clouds, and (3) structured,
scalable organization via 3D scene graphs. Built on an object-centric design,
the framework integrates with large vision-language models to enable semantic
queryability by linking multimodal object embeddings, and supporting
object-level retrieval of geometric, visual, and semantic information. The
retrieved data are then loaded into a robotic task planner for downstream
execution. We evaluate our approach through simulated robotic task planning
scenarios in Unity, guided by abstract language instructions and using the
indoor public dataset Replica. Furthermore, we apply it in a digital duplicate
of a real wet lab environment to test QSR-supported robotic task planning for
emergency response. The results demonstrate the framework's ability to
facilitate scene understanding and integrate spatial and semantic reasoning,
effectively translating high-level human instructions into precise robotic task
planning in complex 3D environments.

</details>


### [230] [DB-TSDF: Directional Bitmask-based Truncated Signed Distance Fields for Efficient Volumetric Mapping](https://arxiv.org/abs/2509.20081)
*Jose E. Maese,Luis Merino,Fernando Caballero*

Main category: cs.RO

TL;DR: This paper introduces a CPU-based volumetric mapping framework using TSDF that processes LiDAR data efficiently, maintaining constant runtime regardless of resolution, with accuracy comparable to GPU-accelerated methods.


<details>
  <summary>Details</summary>
Motivation: To develop a high-resolution volumetric mapping framework suitable for efficient real-time 3D reconstruction without relying on GPU acceleration.

Method: A directional bitmask-based integration scheme is utilized to fuse LiDAR point-cloud data incrementally into voxel grids, ensuring consistent TSDF representation with constant processing time per cloud.

Result: The proposed system achieves real-time processing performance while matching the accuracy of advanced mapping methods, demonstrated through experiments with real-world datasets.

Conclusion: The framework successfully provides competitive real-time, high-resolution mapping capabilities using CPU resources, proving to be an efficient alternative to GPU-based approaches.

Abstract: This paper presents a high-efficiency, CPU-only volumetric mapping framework
based on a Truncated Signed Distance Field (TSDF). The system incrementally
fuses raw LiDAR point-cloud data into a voxel grid using a directional
bitmask-based integration scheme, producing dense and consistent TSDF
representations suitable for real-time 3D reconstruction. A key feature of the
approach is that the processing time per point-cloud remains constant,
regardless of the voxel grid resolution, enabling high resolution mapping
without sacrificing runtime performance. In contrast to most recent TSDF/ESDF
methods that rely on GPU acceleration, our method operates entirely on CPU,
achieving competitive results in speed. Experiments on real-world open datasets
demonstrate that the generated maps attain accuracy on par with contemporary
mapping techniques.

</details>


### [231] [Orbital Stabilization and Time Synchronization of Unstable Periodic Motions in Underactuated Robots](https://arxiv.org/abs/2509.20082)
*Surov Maksim*

Main category: cs.RO

TL;DR: This paper proposes a control methodology combining time-varying LQR and sliding-mode control to achieve orbital stabilization and time synchronization in underactuated robotic systems, validated on Butterfly robots.


<details>
  <summary>Details</summary>
Motivation: Underactuated robotic systems face challenges in achieving synchronization and stabilization of periodic trajectories.

Method: The paper revises the transverse linearization framework to incorporate time-desynchronization dynamics, using time-varying LQR and sliding-mode control for stabilization.

Result: The approach is tested experimentally with centralized and decentralized control strategies on a group of six Butterfly robots.

Conclusion: The methodology successfully achieves orbital stabilization and time synchronization in underactuated robotic systems.

Abstract: This paper presents a control methodology for achieving orbital stabilization
with simultaneous time synchronization of periodic trajectories in
underactuated robotic systems. The proposed approach extends the classical
transverse linearization framework to explicitly incorporate
time-desynchronization dynamics. To stabilize the resulting extended transverse
dynamics, we employ a combination of time-varying LQR and sliding-mode control.
The theoretical results are validated experimentally through the implementation
of both centralized and decentralized control strategies on a group of six
Butterfly robots.

</details>


### [232] [C-3TO: Continuous 3D Trajectory Optimization on Neural Euclidean Signed Distance Fields](https://arxiv.org/abs/2509.20084)
*Guillermo Gil,Jose Antonio Cobano,Luis Merino,Fernando Caballero*

Main category: cs.RO

TL;DR: The paper introduces a framework (C-3TO) for continuous 3D trajectory optimization in cluttered environments using neural Euclidean Signed Distance Fields (ESDFs).


<details>
  <summary>Details</summary>
Motivation: The research aims to improve the efficiency and accuracy of trajectory optimization in 3D cluttered environments and addresses limitations of prior ESDF-based methods that rely on discretized grids.

Method: The proposed approach utilizes a neural ESDF for smooth trajectory representation with fifth-order polynomials and employs a two-stage nonlinear optimization pipeline.

Result: C-3TO generates collision-aware and dynamically feasible 3D trajectories, showing adaptability to various environmental and user-defined constraints.

Conclusion: The framework establishes a reliable ground for safe and efficient aerial robotic local replanning with generalizability for diverse applications.

Abstract: This paper introduces a novel framework for continuous 3D trajectory
optimization in cluttered environments, leveraging online neural Euclidean
Signed Distance Fields (ESDFs). Unlike prior approaches that rely on
discretized ESDF grids with interpolation, our method directly optimizes smooth
trajectories represented by fifth-order polynomials over a continuous neural
ESDF, ensuring precise gradient information throughout the entire trajectory.
The framework integrates a two-stage nonlinear optimization pipeline that
balances efficiency, safety and smoothness. Experimental results demonstrate
that C-3TO produces collision-aware and dynamically feasible trajectories.
Moreover, its flexibility in defining local window sizes and optimization
parameters enables straightforward adaptation to diverse user's needs without
compromising performance. By combining continuous trajectory parameterization
with a continuously updated neural ESDF, C-3TO establishes a robust and
generalizable foundation for safe and efficient local replanning in aerial
robotics.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [233] [Automated Insertion of Flushes and Fences for Persistency](https://arxiv.org/abs/2509.19459)
*Yutong Guo,Weiyu Luo,Brian Demsky*

Main category: cs.SE

TL;DR: PMRobust is a compiler that ensures code using persistent memory is free of missing flush and fence bugs by automatically inserting these operations, achieving minimal performance overhead.


<details>
  <summary>Details</summary>
Motivation: Manual handling of flush and fence operations in persistent memory is error-prone. Developers often struggle to ensure data persistence beyond crashes.

Method: PMRobust employs innovative static analysis with optimizations tailored for newly allocated objects to automatically insert missing flush and fence operations.

Result: The evaluation of PMRobust on persistent memory libraries and data structures revealed a minimal overhead of 0.26% compared to manually written flush and fence operations.

Conclusion: PMRobust effectively eliminates missing flush and fence bugs with minimal performance impact, simplifying developers' effort to achieve persistence in memory systems.

Abstract: CXL shared memory and persistent memory allow the contents of memory to
persist beyond crashes. Stores to persistent or CXL memory are typically not
immediately made persistent; developers must manually flush the corresponding
cache lines to force the data to be written to the underlying storage.
Correctly using flush and fence operations is known to be challenging. While
state-of-the-art tools can find missing flush instructions, they often require
bug-revealing test cases. No existing tools can ensure the absence of missing
flush bugs.
  In this paper, we present PMRobust, a compiler that automatically inserts
flush and fence operations to ensure that code using persistent memory is free
from missing flush and fence bugs. PMRobust employs a novel static analysis
with optimizations that target newly allocated objects. We have evaluated
PMRobust on persistent memory libraries and several persistent memory data
structures and measured a geometric mean overhead of 0.26% relative to the
original benchmarks with hand-placed flush and fence operations.

</details>


### [234] [Semantic-Aware Fuzzing: An Empirical Framework for LLM-Guided, Reasoning-Driven Input Mutation](https://arxiv.org/abs/2509.19533)
*Mengdi Lu,Steven Ding,Furkan Alaca,Philippe Charland*

Main category: cs.SE

TL;DR: The paper proposes blending reasoning from large language models (LLMs) with traditional fuzzing tools to improve mutation quality in testing software vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: Traditional fuzzing techniques lack deep semantic reasoning and struggle with complex protocol constraints, necessitating advanced methods.

Method: The authors integrate reasoning LLMs into the AFL++ fuzzing framework using a microservices setup, testing different prompt approaches (zero-shot and few-shot) alongside several reasoning LLMs.

Result: Experiments reveal that prompt engineering and LLM selection significantly affect mutation quality, with Deepseek identified as the top-performing model under prompt-only conditions.

Conclusion: Enhancing fuzzing with reasoning LLMs holds promise for deeper, constraint-aware code mutation, but issues like latency and scalability require further research.

Abstract: Security vulnerabilities in Internet-of-Things devices, mobile platforms, and
autonomous systems remain critical. Traditional mutation-based fuzzers -- while
effectively explore code paths -- primarily perform byte- or bit-level edits
without semantic reasoning. Coverage-guided tools such as AFL++ use
dictionaries, grammars, and splicing heuristics to impose shallow structural
constraints, leaving deeper protocol logic, inter-field dependencies, and
domain-specific semantics unaddressed. Conversely, reasoning-capable large
language models (LLMs) can leverage pretraining knowledge to understand input
formats, respect complex constraints, and propose targeted mutations, much like
an experienced reverse engineer or testing expert. However, lacking ground
truth for "correct" mutation reasoning makes supervised fine-tuning
impractical, motivating explorations of off-the-shelf LLMs via prompt-based
few-shot learning. To bridge this gap, we present an open-source microservices
framework that integrates reasoning LLMs with AFL++ on Google's FuzzBench,
tackling asynchronous execution and divergent hardware demands (GPU- vs.
CPU-intensive) of LLMs and fuzzers. We evaluate four research questions: (R1)
How can reasoning LLMs be integrated into the fuzzing mutation loop? (R2) Do
few-shot prompts yield higher-quality mutations than zero-shot? (R3) Can prompt
engineering with off-the-shelf models improve fuzzing directly? and (R4) Which
open-source reasoning LLMs perform best under prompt-only conditions?
Experiments with Llama3.3, Deepseek-r1-Distill-Llama-70B, QwQ-32B, and Gemma3
highlight Deepseek as the most promising. Mutation effectiveness depends more
on prompt complexity and model choice than shot count. Response latency and
throughput bottlenecks remain key obstacles, offering directions for future
work.

</details>


### [235] [Reverse Engineering User Stories from Code using Large Language Models](https://arxiv.org/abs/2509.19587)
*Mohamed Ouf,Haoyu Li,Michael Zhang,Mariam Guizani*

Main category: cs.SE

TL;DR: This paper examines the potential of large language models (LLMs) to generate user stories from source code, achieving promising results with smaller models using illustrative examples.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of missing or outdated user stories in legacy systems by exploring if LLMs can automate their recovery from source code.

Method: The authors evaluate five state-of-the-art LLMs on 1,750 annotated C++ code snippets, employing six different prompting strategies to study their effectiveness.

Result: The models achieve an average F1 score of 0.8 for code up to 200 NLOC. Smaller models performed comparably to larger models when given a single illustrative example, while structured reasoning yielded only minor improvements.

Conclusion: The findings suggest LLMs are effective in recovering user stories, with smaller models performing well under the right prompting. This offers a scalable, automated solution for user story generation in agile development.

Abstract: User stories are essential in agile development, yet often missing or
outdated in legacy and poorly documented systems. We investigate whether large
language models (LLMs) can automatically recover user stories directly from
source code and how prompt design impacts output quality. Using 1,750 annotated
C++ snippets of varying complexity, we evaluate five state-of-the-art LLMs
across six prompting strategies. Results show that all models achieve, on
average, an F1 score of 0.8 for code up to 200 NLOC. Our findings show that a
single illustrative example enables the smallest model (8B) to match the
performance of a much larger 70B model. In contrast, structured reasoning via
Chain-of-Thought offers only marginal gains, primarily for larger models.

</details>


### [236] [Assertion Messages with Large Language Models (LLMs) for Code](https://arxiv.org/abs/2509.19673)
*Ahmed Aljohani,Anamul Haque Mollah,Hyunsook Do*

Main category: cs.SE

TL;DR: The paper evaluates four LLMs for generating assertion messages in unit tests, finding that Codestral-22B performs best but still below human-written messages.


<details>
  <summary>Details</summary>
Motivation: Assertion messages are critical for understanding test failures but are often omitted; evaluating LLM performance in generating these messages is needed.

Method: The study evaluates four Fill-in-the-Middle LLMs on 216 Java test methods with human evaluation and an ablation study on the role of descriptive comments.

Result: Codestral-22B achieved the best score of 2.76/5, improving to 2.97 with test comments, though all models fell short of the human benchmark of 3.24.

Conclusion: Current LLMs can replicate linguistic patterns but have limitations in context-aware assertion message generation, paving the way for future improvements.

Abstract: Assertion messages significantly enhance unit tests by clearly explaining the
reasons behind test failures, yet they are frequently omitted by developers and
automated test-generation tools. Despite recent advancements, Large Language
Models (LLMs) have not been systematically evaluated for their ability to
generate informative assertion messages. In this paper, we introduce an
evaluation of four state-of-the-art Fill-in-the-Middle (FIM) LLMs -
Qwen2.5-Coder-32B, Codestral-22B, CodeLlama-13B, and StarCoder - on a dataset
of 216 Java test methods containing developer-written assertion messages. We
find that Codestral-22B achieves the highest quality score of 2.76 out of 5
using a human-like evaluation approach, compared to 3.24 for manually written
messages. Our ablation study shows that including descriptive test comments
further improves Codestral's performance to 2.97, highlighting the critical
role of context in generating clear assertion messages. Structural analysis
demonstrates that all models frequently replicate developers' preferred
linguistic patterns. We discuss the limitations of the selected models and
conventional text evaluation metrics in capturing diverse assertion message
structures. Our benchmark, evaluation results, and discussions provide an
essential foundation for advancing automated, context-aware generation of
assertion messages in test code. A replication package is available at
https://doi.org/10.5281/zenodo.15293133

</details>


### [237] [Intuition to Evidence: Measuring AI's True Impact on Developer Productivity](https://arxiv.org/abs/2509.19708)
*Anand Kumar,Vishal Khare,Deepak Sharma,Satyam Kumar,Vijay Saini,Anshul Yadav,Sachendra Jain,Ankit Rana,Pratham Verma,Vaibhav Meena,Avinash Edubilli*

Main category: cs.SE

TL;DR: The paper evaluates the impact of an AI tool (DeputyDev) on enterprise-scale software development, showcasing its productivity improvements and practical challenges.


<details>
  <summary>Details</summary>
Motivation: To assess the real-world impact and acceptance of AI-assisted tools in enterprise software development environments.

Method: 300 engineers used DeputyDev, an in-house AI tool, over a year, with its integration into their workflows analyzed through rigorous cohort analysis.

Result: Notable findings include a 31.8% reduction in PR cycle time, strong user satisfaction, usage peaking at 83% by month 6, a 28% increase in code shipment volume, and top adopters achieving 61% more code pushed to production.

Conclusion: AI tools like DeputyDev show significant potential in enhancing enterprise software development productivity but also come with deployment challenges in real-world settings.

Abstract: We present a comprehensive real-world evaluation of AI-assisted software
development tools deployed at enterprise scale. Over one year, 300 engineers
across multiple teams integrated an in-house AI platform (DeputyDev) that
combines code generation and automated review capabilities into their daily
workflows. Through rigorous cohort analysis, our study demonstrates
statistically significant productivity improvements, including an overall 31.8%
reduction in PR review cycle time.
  Developer adoption was strong, with 85% satisfaction for code review features
and 93% expressing a desire to continue using the platform. Adoption patterns
showed systematic scaling from 4% engagement in month 1 to 83% peak usage by
month 6, stabilizing at 60% active engagement. Top adopters achieved a 61%
increase in code volume pushed to production, contributing to approximately 30
to 40% of code shipped to production through this tool, accounting for an
overall 28% increase in code shipment volume.
  Unlike controlled benchmark evaluations, our longitudinal analysis provides
empirical evidence from production environments, revealing both the
transformative potential and practical deployment challenges of integrating AI
into enterprise software development workflows.

</details>


### [238] [The Cream Rises to the Top: Efficient Reranking Method for Verilog Code Generation](https://arxiv.org/abs/2509.20215)
*Guang Yang,Wei Zheng,Xiang Chen,Yifan Sun,Fengji Zhang,Terry Yue Zhuo*

Main category: cs.SE

TL;DR: The paper addresses LLMs' challenges in Verilog generation and proposes VCD-RNK, a model for effectively reranking Verilog code candidates.


<details>
  <summary>Details</summary>
Motivation: To provide hardware engineers with a trustworthy Verilog code generation solution, overcoming the uncertainty of multiple candidates provided by LLMs.

Method: VCD-RNK uses Verilog-specific reasoning by distilling expert knowledge via semantic analysis, test case generation, and functional correctness assessment, avoiding computationally intensive methods.

Result: VCD-RNK delivers efficient Verilog code reranking by simulating necessary reasoning processes, enhancing reliability and reducing computation.

Conclusion: The proposed VCD-RNK model effectively bridges the gap by addressing semantic alignment challenges, offering a reliable and efficient solution for Verilog code generation.

Abstract: LLMs face significant challenges in Verilog generation due to limited
domain-specific knowledge. While sampling techniques improve pass@k metrics,
hardware engineers need one trustworthy solution rather than uncertain
candidates. To bridge this gap, we formulate it as a semantic alignment problem
between requirements and Verilog implementations, and propose VCD-RNK, a
discriminator model tailored for efficient Verilog code reranking.
Specifically, VCD-RNKincorporates Verilog-specific reasoning by distilling
expert knowledge across three dimensions: code semantic analysis, test case
generation, and functional correctness assessment. By explicitly simulating the
above reasoning processes during inference, VCD-RNK effectively avoids
computationally intensive test execution in existing methods.

</details>


### [239] [Beyond Language Barriers: Multi-Agent Coordination for Multi-Language Code Generation](https://arxiv.org/abs/2509.19918)
*Micheline Bénédicte Moumoula,Serge Lionel Nikiema,Albérick Euraste Djire,Abdoul Kader Kabore,Jacques Klein,Tegawendé F. Bissyande*

Main category: cs.SE

TL;DR: XL-CoGen enhances cross-language code generation by leveraging a coordinated multi-agent system with bridging languages, significantly improving translation and repair accuracy, especially in underrepresented programming languages.


<details>
  <summary>Details</summary>
Motivation: To address the uneven proficiency of LLMs in generating high-quality code for underrepresented programming languages and to exploit recurring patterns across languages.

Method: Uses a coordinated multi-agent architecture combining intermediate representation, translation, and automated repair with a data-driven approach for selecting bridging languages based on transfer matrices of translation success.

Result: Achieves up to 13 percentage points improvement over fine-tuned baselines and 30 percentage points over existing single-language methods, demonstrating significant performance gains in cross-language code generation.

Conclusion: The system successfully enhances LLMs' cross-language adaptability by leveraging compatibility-guided knowledge transfer and iterative error correction, emphasizing the potential of shared patterns across languages.

Abstract: Producing high-quality code across multiple programming languages is
increasingly important as today's software systems are built on heterogeneous
stacks. Large language models (LLMs) have advanced the state of automated
programming, yet their proficiency varies sharply between languages, especially
those with limited training data such as Rust, Perl, OCaml, and Erlang. Many
current solutions including language-specific fine-tuning, multi-agent
orchestration, transfer learning, and intermediate-representation pipelines
still approach each target language in isolation, missing opportunities to
share knowledge or exploit recurring cross-language patterns.
  XL-CoGen tackles this challenge with a coordinated multi-agent architecture
that integrates intermediate representation, code generation, translation, and
automated repair. Its distinguishing feature is a data-driven mechanism for
selecting bridging languages: empirically derived transfer matrices identify
the best intermediate languages based on demonstrated translation success
rather than raw generation accuracy. The system performs early output
validation, iteratively corrects errors, and reuses intermediate artifacts as
contextual scaffolds for subsequent translations.
  Extensive experiments show that XL-CoGen yields notable improvements with 13
percentage-point gains over the strongest fine-tuned baseline and as much as 30
percentage points over existing single-language multi-agent methods. Ablation
studies further demonstrate that compatibility-guided bridging significantly
outperforms LLM-based heuristics, confirming the value of cumulative
cross-language knowledge transfer.

</details>


### [240] [Demystifying the Evolution of Neural Networks with BOM Analysis: Insights from a Large-Scale Study of 55,997 GitHub Repositories](https://arxiv.org/abs/2509.20010)
*Xiaoning Ren,Yuhang Ye,Xiongfei Wu,Yueming Wu,Yinxing Xue*

Main category: cs.SE

TL;DR: The paper introduces Neural Network Bill of Material (NNBOM), a dataset aimed at analyzing the evolution of NN software based on curated repositories.


<details>
  <summary>Details</summary>
Motivation: The study aims to address gaps in evolutionary analysis tools for neural network (NN) software, which traditional methods cannot handle effectively due to distinct reuse patterns and structures.

Method: The researchers created a large-scale NNBOM database from thousands of PyTorch GitHub repositories and analyzed NN software evolution trends using this dataset.

Result: A detailed empirical study was conducted, revealing insights into software scale, component reuse, and inter-domain dependencies in NN software.

Conclusion: NNBOM provides a powerful framework for NN software evolution analysis, demonstrated through applications like evolution analyzers and component assessors.

Abstract: Neural networks have become integral to many fields due to their exceptional
performance. The open-source community has witnessed a rapid influx of neural
network (NN) repositories with fast-paced iterations, making it crucial for
practitioners to analyze their evolution to guide development and stay ahead of
trends. While extensive research has explored traditional software evolution
using Software Bill of Materials (SBOMs), these are ill-suited for NN software,
which relies on pre-defined modules and pre-trained models (PTMs) with distinct
component structures and reuse patterns. Conceptual AI Bills of Materials
(AIBOMs) also lack practical implementations for large-scale evolutionary
analysis. To fill this gap, we introduce the Neural Network Bill of Material
(NNBOM), a comprehensive dataset construct tailored for NN software. We create
a large-scale NNBOM database from 55,997 curated PyTorch GitHub repositories,
cataloging their TPLs, PTMs, and modules. Leveraging this database, we conduct
a comprehensive empirical study of neural network software evolution across
software scale, component reuse, and inter-domain dependency, providing
maintainers and developers with a holistic view of its long-term trends.
Building on these findings, we develop two prototype applications,
\textit{Multi repository Evolution Analyzer} and \textit{Single repository
Component Assessor and Recommender}, to demonstrate the practical value of our
analysis.

</details>


### [241] [V-GameGym: Visual Game Generation for Code Large Language Models](https://arxiv.org/abs/2509.20136)
*Wei Zhang,Jack Yang,Renshuai Tao,Lingzheng Chai,Shawn Guo,Jiajun Wu,Xiaoming Chen,Ganqu Cui,Ning Ding,Xander Xu,Hu Wei,Bowen Zhou*

Main category: cs.SE

TL;DR: The paper introduces V-GameGym, a benchmark for evaluating code large language models (LLMs) in visual game development, addressing playability, visual aesthetics, and user engagement.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for code LLMs focus on syntax and execution, overlooking essential metrics for visual game development, such as playability and user engagement.

Method: The authors developed V-GameGym, consisting of 2,219 samples from 100 thematic clusters, curated using a clustering-based methodology, and introduced a multimodal evaluation framework for visual code synthesis.

Result: V-GameGym bridges the gap between accurate code generation and practical game development by quantifying quality metrics for visual programming and interactive elements.

Conclusion: V-GameGym establishes a robust framework for assessing the practical capabilities of code LLMs in comprehensive visual game development tasks.

Abstract: Code large language models have demonstrated remarkable capabilities in
programming tasks, yet current benchmarks primarily focus on single modality
rather than visual game development. Most existing code-related benchmarks
evaluate syntax correctness and execution accuracy, overlooking critical
game-specific metrics such as playability, visual aesthetics, and user
engagement that are essential for real-world deployment. To address the gap
between current LLM capabilities in algorithmic problem-solving and competitive
programming versus the comprehensive requirements of practical game
development, we present V-GameGym, a comprehensive benchmark comprising 2,219
high-quality samples across 100 thematic clusters derived from real-world
repositories, adopting a novel clustering-based curation methodology to ensure
both diversity and structural completeness. Further, we introduce a multimodal
evaluation framework with an automated LLM-driven pipeline for visual code
synthesis using complete UI sandbox environments. Our extensive analysis
reveals that V-GameGym effectively bridges the gap between code generation
accuracy and practical game development workflows, providing quantifiable
quality metrics for visual programming and interactive element generation.

</details>


### [242] [Enhancing Requirement Traceability through Data Augmentation Using Large Language Models](https://arxiv.org/abs/2509.20149)
*Jianzhang Zhang,Jialong Zhou,Nan Niu,Chuang Liu*

Main category: cs.SE

TL;DR: The study tackles the issue of data scarcity in requirements traceability by using large language models (LLMs) for data augmentation, achieving notable improvements in model performance.


<details>
  <summary>Details</summary>
Motivation: Existing automated traceability methods face challenges due to limited training data and difficulty in bridging the semantic gap between requirements and code.

Method: The researchers utilized prompt-based techniques with four LLMs (Gemini 1.5 Pro, Claude 3, GPT-3.5, and GPT-4) for zero-shot and few-shot trace link generation. They also optimized the encoder component of the tracing model for better adaptability.

Result: Experimental results showed up to 28.59% improvement in the F1 score, highlighting the effectiveness of the augmented datasets and optimized encoder.

Conclusion: The study demonstrates that LLMs combined with prompt-based data augmentation and encoder optimization can significantly improve requirements traceability, offering a practical solution to the data scarcity problem.

Abstract: Requirements traceability is crucial in software engineering to ensure
consistency between requirements and code. However, existing automated
traceability methods are constrained by the scarcity of training data and
challenges in bridging the semantic gap between artifacts. This study aims to
address the data scarcity problem in requirements traceability by employing
large language models (LLMs) for data augmentation. We propose a novel approach
that utilizes prompt-based techniques with LLMs to generate augmented
requirement-to-code trace links, thereby enhancing the training dataset. Four
LLMs (Gemini 1.5 Pro, Claude 3, GPT-3.5, and GPT-4) were used, employing both
zero-shot and few-shot templates. Moreover, we optimized the encoder component
of the tracing model to improve its efficiency and adaptability to augmented
data. The key contributions of this paper are: (1) proposing and evaluating
four prompt templates for data augmentation; (2) providing a comparative
analysis of four LLMs for generating trace links; (3) enhancing the model's
encoder for improved adaptability to augmented datasets. Experimental results
show that our approach significantly enhances model performance, achieving an
F1 score improvement of up to 28.59%, thus demonstrating its effectiveness and
potential for practical application.

</details>


### [243] [Benchmarking Web API Integration Code Generation](https://arxiv.org/abs/2509.20172)
*Daniel Maninger,Leon Chemnitz,Amir Molzam Sharifloo,Jannis Brugger,Mira Mezini*

Main category: cs.SE

TL;DR: The paper investigates the challenges of generating correct web API invocation code using large language models (LLMs) and reveals that current open-source models perform poorly, solving less than 40% of tasks.


<details>
  <summary>Details</summary>
Motivation: To address the unexplored area of how effective large language models (LLMs) are in generating web API integration code, a critical aspect of software development.

Method: Presented a dataset and evaluation pipeline to assess the ability of open-source LLMs in generating correct web API invocation code, followed by experiments on several models.

Result: The experiments showed that open-source LLMs face significant challenges, including hallucinated endpoints and incorrect argument usage, with none solving over 40% of the tasks.

Conclusion: Current open-source LLMs are not effective at reliably generating correct web API invocation code, highlighting the need for improved methods to address these limitations.

Abstract: API integration is a cornerstone of our digital infrastructure, enabling
software systems to connect and interact. However, as shown by many studies,
writing or generating correct code to invoke APIs, particularly web APIs, is
challenging. Although large language models~(LLMs) have become popular in
software development, their effectiveness in automating the generation of web
API integration code remains unexplored. In order to address this, we present a
dataset and evaluation pipeline designed to assess the ability of LLMs to
generate web API invocation code. Our experiments with several open-source LLMs
reveal that generating API invocations poses a significant challenge, resulting
in hallucinated endpoints, incorrect argument usage, and other errors. None of
the evaluated open-source models were able to solve more than 40% of the tasks.

</details>


### [244] [Confidentiality-Preserving Verifiable Business Processes through Zero-Knowledge Proofs](https://arxiv.org/abs/2509.20300)
*Jannis Kiesel,Jonathan Heiss*

Main category: cs.SE

TL;DR: The paper presents a zero-knowledge proof (ZKP)-based approach enabling verifiable business process execution while ensuring confidentiality of data, using a system architecture and prototype integrating ZK virtual machines.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of ensuring process integrity in inter-organizational processes without disclosing confidential business information.

Method: The authors integrate ZK virtual machines (zkVMs) into business process management engines via a system architecture and prototype, enabling proof compositions for chained verifications and examining ZKP proving variants within process models.

Result: A demonstration was conducted using product carbon footprinting, showcasing organizations verifying process integrity while maintaining data confidentiality, with an evaluation proving feasibility and automation of process verification.

Conclusion: The approach successfully enables confidential and verifiable business process execution, with potential for practical integration throughout the BPM lifecycle.

Abstract: Ensuring the integrity of business processes without disclosing confidential
business information is a major challenge in inter-organizational processes.
This paper introduces a zero-knowledge proof (ZKP)-based approach for the
verifiable execution of business processes while preserving confidentiality. We
integrate ZK virtual machines (zkVMs) into business process management engines
through a comprehensive system architecture and a prototypical implementation.
Our approach supports chained verifiable computations through proof
compositions. On the example of product carbon footprinting, we model
sequential footprinting activities and demonstrate how organizations can prove
and verify the integrity of verifiable processes without exposing sensitive
information. We assess different ZKP proving variants within process models for
their efficiency in proving and verifying, and discuss the practical
integration of ZKPs throughout the Business Process Management (BPM) lifecycle.
Our experiment-driven evaluation demonstrates the automation of process
verification under given confidentiality constraints.

</details>


### [245] [Protocol Testing with I/O Grammars](https://arxiv.org/abs/2509.20308)
*Alexander Liggesmeyer,José Antonio Zamudio Amaya,Andreas Zeller*

Main category: cs.SE

TL;DR: This paper introduces I/O grammars for testing protocols by generating inputs and validating outputs within a unified framework, improving over random-based methods.


<details>
  <summary>Details</summary>
Motivation: The goal is to address the challenge of generating syntactically and semantically valid inputs while ensuring output correctness, particularly in protocol testing.

Method: Proposes I/O grammars to fully specify the syntax and semantics of protocol interactions, and integrates input generation and output checking with the FANDANGO framework.

Result: I/O grammars enabled efficient and accurate specification and output validation of protocols (like DNS, FTP, SMTP) while achieving quicker and more systematic coverage compared to random techniques.

Conclusion: The approach provides a versatile and systematic framework for protocol testing, outperforming existing tools and improving input and response space coverage significantly.

Abstract: Generating software tests faces two fundamental problems. First, one needs to
_generate inputs_ that are syntactically and semantically correct, yet
sufficiently diverse to cover behavior. Second, one needs an _oracle_ to _check
outputs_ whether a test case is correct or not. Both problems become apparent
in _protocol testing_, where inputs are messages exchanged between parties, and
outputs are the responses of these parties.
  In this paper, we propose a novel approach to protocol testing that combines
input generation and output checking in a single framework. We introduce _I/O
grammars_ as the first means to _completely_ specify the syntax and semantics
of protocols, including messages, states, and interactions. Our implementation,
based on the FANDANGO framework, takes a single I/O grammar, and can act as a
_test generator_, as a _mock object_, and as an _oracle_ for a _client_, a
_server_, or both (or actually any number of parties), a versatility not found
in any existing tool or formalism. User-defined _constraints}_can have the
generator focus on arbitrary protocol features; $k$-path guidance
systematically covers states, messages, responses, and value alternatives in a
unified fashion.
  We evaluate the effectiveness of our approach by applying it to several
protocols, including DNS, FTP, and SMTP. We demonstrate that I/O grammars can
specify advanced protocol features correctly and completely, while also
enabling output validation of the programs under test. In its evaluation, we
find that systematic coverage of the I/O grammar results in much quicker
coverage of the input and response spaces (and thus functionality) compared to
the random-based state-of-the-art approaches.

</details>


### [246] [Developer Productivity With and Without GitHub Copilot: A Longitudinal Mixed-Methods Case Study](https://arxiv.org/abs/2509.20353)
*Viktoria Stray,Elias Goldmann Brandtzæg,Viggo Tellefsen Wivestad,Astri Barbala,Nils Brede Moe*

Main category: cs.SE

TL;DR: This paper studies GitHub Copilot's impact on developer activity and productivity through data analysis, surveys, and interviews, finding higher activity levels for Copilot users but no significant post-adoption changes.


<details>
  <summary>Details</summary>
Motivation: Understand the real-world impact of GitHub Copilot on developer activity and subjective productivity.

Method: A mixed-methods case study, including analysis of 26,317 commits from 703 repositories, surveys, and interviews.

Result: Copilot users were more active than non-users but showed no statistically significant post-adoption change in commit activity.

Conclusion: The subjective perceived productivity increase does not align with measurable changes in commit-based activity after adopting Copilot.

Abstract: This study investigates the real-world impact of the generative AI (GenAI)
tool GitHub Copilot on developer activity and perceived productivity. We
conducted a mixed-methods case study in NAV IT, a large public sector agile
organization. We analyzed 26,317 unique non-merge commits from 703 of NAV IT's
GitHub repositories over a two-year period, focusing on commit-based activity
metrics from 25 Copilot users and 14 non-users. The analysis was complemented
by survey responses on their roles and perceived productivity, as well as 13
interviews. Our analysis of activity metrics revealed that individuals who used
Copilot were consistently more active than non-users, even prior to Copilot's
introduction. We did not find any statistically significant changes in
commit-based activity for Copilot users after they adopted the tool, although
minor increases were observed. This suggests a discrepancy between changes in
commit-based metrics and the subjective experience of productivity.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [247] [The Impact of Structural Changes on Learning Capacity in the Fly Olfactory Neural Circuit](https://arxiv.org/abs/2509.19351)
*Katherine Xie,Gabriel Koch Ocker*

Main category: q-bio.NC

TL;DR: The study investigates the role of specific neural circuits in the Drosophila mushroom body in olfactory learning and classification, emphasizing Kenyon cells' connection to MB output neurons.


<details>
  <summary>Details</summary>
Motivation: To examine how structural changes and connectivity within the Kenyon cell (KC) to mushroom body output neuron (MBON) circuit impact the ability of MBONs to classify odors.

Method: The researchers built a neural network model reflecting the circuitry between projection neurons (PNs), KCs, and MBONs, trained with ten artificial odor input classes, and conducted experiments with KC ablation, pruning, and rewiring.

Result: MBONs connected to fewer KCs showed worse odor classification, and ablation of mature KCs had a bigger impact on learning capacity than immature ones; findings from pruning experiments were consistent with ablation studies.

Conclusion: The research enhances understanding of olfactory neuroplasticity and learning processes, with applications in artificial intelligence and potential implications for treating neurodegenerative diseases.

Abstract: The Drosophila mushroom body (MB) is known to be involved in olfactory
learning and memory; the synaptic plasticity of the Kenyon cell (KC) to
mushroom body output neuron (MBON) synapses plays a key role in the learning
process. Previous research has focused on projection neuron (PN) to Kenyon cell
(KC) connectivity within the MB; we examine how perturbations to the mushroom
body circuit structure and changes in connectivity, specifically within the KC
to mushroom body output neuron (MBON) neural circuit, affect the MBONs' ability
to distinguish between odor classes. We constructed a neural network that
incorporates the connectivity between PNs, KCs, and MBONs. To train our model,
we generated ten artificial input classes, which represent the projection
neuron activity in response to different odors. We collected data on the number
of KC-to-MBON connections, MBON error rates, and KC-to-MBON synaptic weights,
among other metrics. We observed that MBONs with very few presynaptic KCs
consistently performed worse than others in the odor classification task. The
developmental types of KCs also played a significant role in each MBON's
output. We performed random and targeted KC ablation and observed that ablating
developmentally mature KCs had a greater negative impact on MBONs' learning
capacity than ablating immature KCs. Random and targeted pruning of KC-MBON
synaptic connections yielded results largely consistent with the ablation
experiments. To further explore the various types of KCs, we also performed
rewiring experiments in the PN to KC circuit. Our study furthers our
understanding of olfactory neuroplasticity and provides important clues to
understanding learning and memory in general. Understanding how the olfactory
circuits process and learn can also have potential applications in artificial
intelligence and treatments for neurodegenerative diseases.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [248] [Anchored Langevin Algorithms](https://arxiv.org/abs/2509.19455)
*Mert Gurbuzbalaban,Hoang M. Nguyen,Xicheng Zhang,Lingjiong Zhu*

Main category: stat.ML

TL;DR: Anchored Langevin dynamics is introduced to address limitations of standard Langevin algorithms, accommodating non-differentiable targets and heavy-tailed distributions.


<details>
  <summary>Details</summary>
Motivation: Standard first-order Langevin algorithms are effective for high-dimensional sampling but face challenges with non-differentiable components and heavy-tailed distributions.

Method: The new approach replaces the original potential with a smooth reference potential and incorporates multiplicative scaling modifications to Langevin diffusion.

Result: Non-asymptotic guarantees in 2-Wasserstein distance to target distributions are established, alongside numerical experiments demonstrating theory and performance.

Conclusion: Anchored Langevin dynamics unifies and extends standard methods to sample from broader classes of target distributions effectively.

Abstract: Standard first-order Langevin algorithms such as the unadjusted Langevin
algorithm (ULA) are obtained by discretizing the Langevin diffusion and are
widely used for sampling in machine learning because they scale to high
dimensions and large datasets. However, they face two key limitations: (i) they
require differentiable log-densities, excluding targets with non-differentiable
components; and (ii) they generally fail to sample heavy-tailed targets. We
propose anchored Langevin dynamics, a unified approach that accommodates
non-differentiable targets and certain classes of heavy-tailed distributions.
The method replaces the original potential with a smooth reference potential
and modifies the Langevin diffusion via multiplicative scaling. We establish
non-asymptotic guarantees in the 2-Wasserstein distance to the target
distribution and provide an equivalent formulation derived via a random time
change of the Langevin diffusion. We provide numerical experiments to
illustrate the theory and practical performance of our proposed approach.

</details>


### [249] [Stochastic Path Planning in Correlated Obstacle Fields](https://arxiv.org/abs/2509.19559)
*Li Zhou,Elvan Ceyhan*

Main category: stat.ML

TL;DR: This paper presents a framework for navigation in environments with obstacles of uncertain status, using Bayesian updates and a two-stage learning method.


<details>
  <summary>Details</summary>
Motivation: The need to navigate efficiently and accurately in environments with spatially correlated obstacles and limited sensors motivates the study.

Method: The proposed method involves using Bayesian belief updates for better efficiency, alongside a two-stage learning framework combining offline and online components optimized for exploration and adaptation.

Result: The study demonstrates improvements in navigation performance through empirical testing across different obstacle densities and sensor constraints.

Conclusion: This framework offers a robust and adaptable approach for navigation in challenging and uncertain environments.

Abstract: We introduce the Stochastic Correlated Obstacle Scene (SCOS) problem, a
navigation setting with spatially correlated obstacles of uncertain blockage
status, realistically constrained sensors that provide noisy readings and
costly disambiguation. Modeling the spatial correlation with Gaussian Random
Field (GRF), we develop Bayesian belief updates that refine blockage
probabilities, and use the posteriors to reduce search space for efficiency. To
find the optimal traversal policy, we propose a novel two-stage learning
framework. An offline phase learns a robust base policy via optimistic policy
iteration augmented with information bonus to encourage exploration in
informative regions, followed by an online rollout policy with periodic base
updates via a Bayesian mechanism for information adaptation. This framework
supports both Monte Carlo point estimation and distributional reinforcement
learning (RL) to learn full cost distributions, leading to stronger uncertainty
quantification. We establish theoretical benefits of correlation-aware updating
and convergence property under posterior sampling. Comprehensive empirical
evaluations across varying obstacle densities, sensor capabilities demonstrate
consistent performance gains over baselines. This framework addresses
navigation challenges in environments with adversarial interruptions or
clustered natural hazards.

</details>


### [250] [MAGIC: Multi-task Gaussian process for joint imputation and classification in healthcare time series](https://arxiv.org/abs/2509.19577)
*Dohyun Ku,Catherine D. Chong,Visar Berisha,Todd J. Schwedt,Jing Li*

Main category: stat.ML

TL;DR: The paper introduces MAGIC, a framework for handling time series in healthcare by addressing data misalignment and sparsity challenges, and it outperforms traditional methods in two medical applications.


<details>
  <summary>Details</summary>
Motivation: Healthcare time series analysis faces critical issues like time misalignment and data sparsity, impeding effective diagnosis and patient management.

Method: MAGIC combines imputation and prediction within a hierarchical multi-task Gaussian process and functional logistic regression, using Taylor expansion approximations and an EM algorithm for parameter optimization.

Result: MAGIC demonstrated superior predictive accuracy in predicting post-traumatic headache improvement and ICU in-hospital mortality compared to existing methods.

Conclusion: MAGIC enables real-time, accurate decision-making in healthcare contexts, significantly aiding early patient assessment and treatment planning.

Abstract: Time series analysis has emerged as an important tool for improving patient
diagnosis and management in healthcare applications. However, these
applications commonly face two critical challenges: time misalignment and data
sparsity. Traditional approaches address these issues through a two-step
process of imputation followed by prediction. We propose MAGIC (Multi-tAsk
Gaussian Process for Imputation and Classification), a novel unified framework
that simultaneously performs class-informed missing value imputation and label
prediction within a hierarchical multi-task Gaussian process coupled with
functional logistic regression. To handle intractable likelihood components,
MAGIC employs Taylor expansion approximations with bounded error analysis, and
parameter estimation is performed using EM algorithm with block coordinate
optimization supported by convergence analysis. We validate MAGIC through two
healthcare applications: prediction of post-traumatic headache improvement
following mild traumatic brain injury and prediction of in-hospital mortality
within 48 hours after ICU admission. In both applications, MAGIC achieves
superior predictive accuracy compared to existing methods. The ability to
generate real-time and accurate predictions with limited samples facilitates
early clinical assessment and treatment planning, enabling healthcare providers
to make more informed treatment decisions.

</details>


### [251] [Diffusion and Flow-based Copulas: Forgetting and Remembering Dependencies](https://arxiv.org/abs/2509.19707)
*David Huk,Theodoros Damoulas*

Main category: stat.ML

TL;DR: The paper introduces diffusion- and flow-based methods to enhance copula models for high-dimensional and complex dependencies, outperforming existing models while simplifying scalability.


<details>
  <summary>Details</summary>
Motivation: Current copula models struggle with restrictive assumptions and scalability challenges for multimodal and high-dimensional dependencies.

Method: The authors design two processes based on diffusions and flows to forget and then learn inter-variable dependencies, with one method focused on density estimation and the other on sampling.

Result: The methods demonstrate superior performance compared to state-of-the-art copula approaches in scientific and image datasets, accurately modeling high-dimensional dependencies.

Conclusion: This work increases the utility and scalability of copula models, supporting broader applications and enabling use on more complex datasets.

Abstract: Copulas are a fundamental tool for modelling multivariate dependencies in
data, forming the method of choice in diverse fields and applications. However,
the adoption of existing models for multimodal and high-dimensional
dependencies is hindered by restrictive assumptions and poor scaling. In this
work, we present methods for modelling copulas based on the principles of
diffusions and flows. We design two processes that progressively forget
inter-variable dependencies while leaving dimension-wise distributions
unaffected, provably defining valid copulas at all times. We show how to obtain
copula models by learning to remember the forgotten dependencies from each
process, theoretically recovering the true copula at optimality. The first
instantiation of our framework focuses on direct density estimation, while the
second specialises in expedient sampling. Empirically, we demonstrate the
superior performance of our proposed methods over state-of-the-art copula
approaches in modelling complex and high-dimensional dependencies from
scientific datasets and images. Our work enhances the representational power of
copula models, empowering applications and paving the way for their adoption on
larger scales and more challenging domains.

</details>


### [252] [Convex Regression with a Penalty](https://arxiv.org/abs/2509.19788)
*Eunji Lim*

Main category: stat.ML

TL;DR: The paper proposes a new convex regression estimator that avoids boundary overfitting by penalizing subgradients and bounding squared errors.


<details>
  <summary>Details</summary>
Motivation: The prevalent convex regression estimators often overfit at the boundary of domains, creating practical challenges in real-world applications such as queueing systems.

Method: The proposed method minimizes a penalty on subgradients while enforcing an upper bound on the sum of squared errors, where the bound is estimated from data.

Result: The paper proves uniform almost sure consistency as well as convergence rates of the estimator and its subgradients, demonstrating its effectiveness empirically in modeling waiting times in queues.

Conclusion: The novel estimator successfully reduces overfitting in convex regression and is supported by theoretical guarantees and practical utility in applications.

Abstract: A common way to estimate an unknown convex regression function $f_0: \Omega
\subset \mathbb{R}^d \rightarrow \mathbb{R}$ from a set of $n$ noisy
observations is to fit a convex function that minimizes the sum of squared
errors. However, this estimator is known for its tendency to overfit near the
boundary of $\Omega$, posing significant challenges in real-world applications.
In this paper, we introduce a new estimator of $f_0$ that avoids this
overfitting by minimizing a penalty on the subgradient while enforcing an upper
bound $s_n$ on the sum of squared errors. The key advantage of this method is
that $s_n$ can be directly estimated from the data. We establish the uniform
almost sure consistency of the proposed estimator and its subgradient over
$\Omega$ as $n \rightarrow \infty$ and derive convergence rates. The
effectiveness of our estimator is illustrated through its application to
estimating waiting times in a single-server queue.

</details>


### [253] [High-Dimensional Statistical Process Control via Manifold Fitting and Learning](https://arxiv.org/abs/2509.19820)
*Burak I. Tas,Enrique del Castillo*

Main category: stat.ML

TL;DR: This paper presents two Statistical Process Control (SPC) methods for monitoring high-dimensional, non-linear industrial processes: one based on manifold fitting and the other on manifold learning.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity and high-dimensionality of industrial processes necessitate advanced SPC methods that capitalize on underlying nonlinear, lower-dimensional data structures.

Method: The paper proposes two SPC frameworks: one leveraging manifold fitting with a scalar distribution-free control chart to monitor deviations, and the other using traditional dimensionality reduction before monitoring the lower-dimensional representations.

Result: The manifold-fitting approach demonstrated competitive or superior fault detection compared to traditional methods, supported by experiments on synthetic and real-world datasets, including the Tennessee Eastman Process and electrical commutators.

Conclusion: Manifold-fitting proves to be both effective and practically applicable for high-dimensional SPC, detecting anomalies with high accuracy in real-world industrial scenarios.

Abstract: We address the Statistical Process Control (SPC) of high-dimensional, dynamic
industrial processes from two complementary perspectives: manifold fitting and
manifold learning, both of which assume data lies on an underlying nonlinear,
lower dimensional space. We propose two distinct monitoring frameworks for
online or 'phase II' Statistical Process Control (SPC). The first method
leverages state-of-the-art techniques in manifold fitting to accurately
approximate the manifold where the data resides within the ambient
high-dimensional space. It then monitors deviations from this manifold using a
novel scalar distribution-free control chart. In contrast, the second method
adopts a more traditional approach, akin to those used in linear dimensionality
reduction SPC techniques, by first embedding the data into a lower-dimensional
space before monitoring the embedded observations. We prove how both methods
provide a controllable Type I error probability, after which they are
contrasted for their corresponding fault detection ability. Extensive numerical
experiments on a synthetic process and on a replicated Tennessee Eastman
Process show that the conceptually simpler manifold-fitting approach achieves
performance competitive with, and sometimes superior to, the more classical
lower-dimensional manifold monitoring methods. In addition, we demonstrate the
practical applicability of the proposed manifold-fitting approach by
successfully detecting surface anomalies in a real image dataset of electrical
commutators.

</details>


### [254] [Geometric Autoencoder Priors for Bayesian Inversion: Learn First Observe Later](https://arxiv.org/abs/2509.19929)
*Arnaud Vadeboncoeur,Gregory Duthé,Mark Girolami,Eleni Chatzi*

Main category: stat.ML

TL;DR: The paper introduces GABI, a new framework for geometry-aware Bayesian inversion using generative models to provide well-calibrated uncertainty quantification (UQ) in systems with complex geometries.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of recovering physical system behaviors from limited and noisy observations, especially in systems with complex geometries that prohibit standard Bayesian UQ approaches.

Method: The authors propose GABI, which combines geometry-aware generative models as latent priors with Bayesian inversion, enabling flexible integration of observations with geometry-conditioned priors. The framework is computationally efficient, leveraging Approximate Bayesian Computation (ABC) sampling, and is architecture-agnostic.

Result: The GABI framework achieved predictive accuracy similar to supervised learning in applicable settings, and robust, well-calibrated UQ performance on complex geometry-based problems in various engineering examples.

Conclusion: GABI offers a versatile, geometry-aware foundation model for UQ that requires no governing equations or prior knowledge, making it adaptable and effective for a broad range of engineering applications.

Abstract: Uncertainty Quantification (UQ) is paramount for inference in engineering
applications. A common inference task is to recover full-field information of
physical systems from a small number of noisy observations, a usually highly
ill-posed problem. Critically, engineering systems often have complicated and
variable geometries prohibiting the use of standard Bayesian UQ. In this work,
we introduce Geometric Autoencoders for Bayesian Inversion (GABI), a framework
for learning geometry-aware generative models of physical responses that serve
as highly informative geometry-conditioned priors for Bayesian inversion.
Following a ''learn first, observe later'' paradigm, GABI distills information
from large datasets of systems with varying geometries, without requiring
knowledge of governing PDEs, boundary conditions, or observation processes,
into a rich latent prior. At inference time, this prior is seamlessly combined
with the likelihood of the specific observation process, yielding a
geometry-adapted posterior distribution. Our proposed framework is architecture
agnostic. A creative use of Approximate Bayesian Computation (ABC) sampling
yields an efficient implementation that utilizes modern GPU hardware. We test
our method on: steady-state heat over rectangular domains; Reynold-Averaged
Navier-Stokes (RANS) flow around airfoils; Helmholtz resonance and source
localization on 3D car bodies; RANS airflow over terrain. We find: the
predictive accuracy to be comparable to deterministic supervised learning
approaches in the restricted setting where supervised learning is applicable;
UQ to be well calibrated and robust on challenging problems with complex
geometries. The method provides a flexible geometry-aware
train-once-use-anywhere foundation model which is independent of any particular
observation process.

</details>


### [255] [BioBO: Biology-informed Bayesian Optimization for Perturbation Design](https://arxiv.org/abs/2509.19988)
*Yanke Li,Tianyu Cui,Tommaso Mansi,Mangal Prakash,Rui Liao*

Main category: stat.ML

TL;DR: Biology-Informed Bayesian Optimization (BioBO) enhances Bayesian optimization by integrating multimodal gene embeddings and enrichment analysis for better surrogate modeling and acquisition strategies in genomic experiments.


<details>
  <summary>Details</summary>
Motivation: Current Bayesian optimization approaches often overlook the domain-specific biological knowledge needed for efficiently identifying genetic interventions in genomics, making advanced strategies necessary.

Method: BioBO merges Bayesian optimization with multimodal gene embeddings and enrichment analysis, creating biologically grounded priors and acquisition functions to balance exploration and exploitation.

Result: BioBO boosts labeling efficiency by 25-40% and surpasses conventional BO in identifying optimal genetic perturbations. Additionally, it provides pathway-level explanations for perturbation selections.

Conclusion: BioBO is an effective and interpretable tool for gene prioritization in genomic perturbation experiments, linking genetic interventions to regulatory mechanisms and improving experimental efficiency.

Abstract: Efficient design of genomic perturbation experiments is crucial for
accelerating drug discovery and therapeutic target identification, yet
exhaustive perturbation of the human genome remains infeasible due to the vast
search space of potential genetic interactions and experimental constraints.
Bayesian optimization (BO) has emerged as a powerful framework for selecting
informative interventions, but existing approaches often fail to exploit
domain-specific biological prior knowledge. We propose Biology-Informed
Bayesian Optimization (BioBO), a method that integrates Bayesian optimization
with multimodal gene embeddings and enrichment analysis, a widely used tool for
gene prioritization in biology, to enhance surrogate modeling and acquisition
strategies. BioBO combines biologically grounded priors with acquisition
functions in a principled framework, which biases the search toward promising
genes while maintaining the ability to explore uncertain regions. Through
experiments on established public benchmarks and datasets, we demonstrate that
BioBO improves labeling efficiency by 25-40%, and consistently outperforms
conventional BO by identifying top-performing perturbations more effectively.
Moreover, by incorporating enrichment analysis, BioBO yields pathway-level
explanations for selected perturbations, offering mechanistic interpretability
that links designs to biologically coherent regulatory circuits.

</details>


### [256] [First-Extinction Law for Resampling Processes](https://arxiv.org/abs/2509.20101)
*Matteo Benati,Alessandro Londei,Denise Lanzieri,Vittorio Loreto*

Main category: stat.ML

TL;DR: This study reduces the computational complexity of calculating extinction times in resampling processes, providing a linear-cost expression validated through simulations.


<details>
  <summary>Details</summary>
Motivation: Extinction times in resampling processes are crucial for understanding various dynamics but are computationally expensive to analyze using previous methods.

Method: The authors treat multinomial updates as independent square-root diffusions with zero drift, deriving a closed-form law for the first-extinction time and proving it matches known results while significantly reducing complexity.

Result: The derived expression replaces exponential-cost computations with a linear-cost formula, validated through simulations, and demonstrates predictive power in modeling collapse scenarios.

Conclusion: The paper offers a computationally efficient framework for resampling extinction dynamics and suggests broader applications in understanding such processes.

Abstract: Extinction times in resampling processes are fundamental yet often
intractable, as previous formulas scale as $2^M$ with the number of states $M$
present in the initial probability distribution. We solve this by treating
multinomial updates as independent square-root diffusions of zero drift,
yielding a closed-form law for the first-extinction time. We prove that the
mean coincides exactly with the Wright-Fisher result of Baxter et al., thereby
replacing exponential-cost evaluations with a linear-cost expression, and we
validate this result through extensive simulations. Finally, we demonstrate
predictive power for model collapse in a simple self-training setup: the onset
of collapse coincides with the resampling-driven first-extinction time computed
from the model's initial stationary distribution. These results hint to a
unified view of resampling extinction dynamics.

</details>


### [257] [Error Propagation in Dynamic Programming: From Stochastic Control to Option Pricing](https://arxiv.org/abs/2509.20239)
*Andrea Della Vecchia,Damir Filipović*

Main category: stat.ML

TL;DR: The paper formulates a discrete-time stochastic optimal control (SOC) problem, leveraging nonparametric regression in RKHSs and Monte Carlo methods for value function approximation, and rigorously assessing error propagation backward in time, with applications to American options pricing.


<details>
  <summary>Details</summary>
Motivation: To analyze foundational theoretical and methodological aspects of SOC in discrete time with a focus on convergence and error propagation, and demonstrate its application in financial contexts.

Method: Combines dynamic programming with nonparametric regression within RKHSs (using KRR) and Monte Carlo subsampling to approximate the value function, providing error decomposition and backward propagation analysis.

Result: The paper offers a rigorous mathematical framework and error analysis for SOC problems, showing how to control error terms backward in time and applying it specifically to the pricing of American options.

Conclusion: The analysis provides a generalized and systematic approach to discrete-time SOC with practical financial applications, offering deep insights into error control and propagation in such problems.

Abstract: This paper investigates theoretical and methodological foundations for
stochastic optimal control (SOC) in discrete time. We start formulating the
control problem in a general dynamic programming framework, introducing the
mathematical structure needed for a detailed convergence analysis. The
associate value function is estimated through a sequence of approximations
combining nonparametric regression methods and Monte Carlo subsampling. The
regression step is performed within reproducing kernel Hilbert spaces (RKHSs),
exploiting the classical KRR algorithm, while Monte Carlo sampling methods are
introduced to estimate the continuation value. To assess the accuracy of our
value function estimator, we propose a natural error decomposition and
rigorously control the resulting error terms at each time step. We then analyze
how this error propagates backward in time-from maturity to the initial stage-a
relatively underexplored aspect of the SOC literature. Finally, we illustrate
how our analysis naturally applies to a key financial application: the pricing
of American options.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [258] [LLMs as verification oracles for Solidity](https://arxiv.org/abs/2509.19153)
*Massimo Bartoletti,Enrico Lipparini,Livio Pompianu*

Main category: cs.CR

TL;DR: This paper evaluates the effectiveness of GPT-5, a reasoning-oriented language model, as a verification oracle for smart contracts.


<details>
  <summary>Details</summary>
Motivation: Many existing tools for ensuring the correctness of smart contracts are limited by their complexity and narrow capabilities, while most real-world exploits result from errors in business logic.

Method: The authors systematically evaluate GPT-5's performance on a dataset of verification tasks, comparing it to formal verification tools, and analyze its practical effectiveness through a mix of quantitative and qualitative methods.

Result: The study demonstrates that reasoning-oriented LLMs, like GPT-5, are surprisingly effective at reasoning about contract-specific properties and performing verification tasks.

Conclusion: GPT-5 shows promise as a new tool for secure smart contract development and auditing, indicating a potential integration of AI with formal methods in this domain.

Abstract: Ensuring the correctness of smart contracts is critical, as even subtle flaws
can lead to severe financial losses. While bug detection tools able to spot
common vulnerability patterns can serve as a first line of defense, most
real-world exploits and losses stem from errors in the contract business logic.
Formal verification tools such as SolCMC and the Certora Prover address this
challenge, but their impact remains limited by steep learning curves and
restricted specification languages. Recent works have begun to explore the use
of large language models (LLMs) for security-related tasks such as
vulnerability detection and test generation. Yet, a fundamental question
remains open: can LLMs serve as verification oracles, capable of reasoning
about arbitrary contract-specific properties? In this paper, we provide the
first systematic evaluation of GPT-5, a state-of-the-art reasoning LLM, in this
role. We benchmark its performance on a large dataset of verification tasks,
compare its outputs against those of established formal verification tools, and
assess its practical effectiveness in real-world auditing scenarios. Our study
combines quantitative metrics with qualitative analysis, and shows that recent
reasoning-oriented LLMs can be surprisingly effective as verification oracles,
suggesting a new frontier in the convergence of AI and formal methods for
secure smart contract development and auditing.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [259] [Ising dynamics on multilayer networks with heterogeneous layers](https://arxiv.org/abs/2509.20216)
*Suman S. Kulkarni,Christopher W. Lynn,Mason A. Porter,Dani S. Bassett*

Main category: physics.soc-ph

TL;DR: This paper studies Ising dynamics on multilayer networks, highlighting the impact of network structure, interlayer coupling strength, and layer heterogeneity on its dynamics.


<details>
  <summary>Details</summary>
Motivation: To understand how heterogeneity and structural characteristics across layers in multilayer networks influence the overall dynamics in various systems.

Method: The authors use numerical simulations and a mean-field approximation to analyze Ising dynamics across different two-layer multilayer networks (both synthetic and empirical).

Result: Interlayer coupling and structural heterogeneity significantly affect both steady and metastable states. Peripheral node interlayer edges influence the dynamics more than core node edges in synthetic core-periphery networks.

Conclusion: The study demonstrates that heterogeneity across layers in multilayer networks plays a critical role in shaping overall network behavior, with insights applicable to biological and social systems.

Abstract: Multilayer networks provide a framework to study complex systems with
multiple types of interactions, multiple dynamical processes, and/or multiple
subsystems. When studying a dynamical process on a multilayer network, it is
important to consider how both layer structure and heterogeneity across layers
impacts the overall dynamics. As a concrete example, we study Ising dynamics on
multilayer networks and investigate how network structure affects its
qualitative features. We focus primarily on multiplex networks, which are
multilayer networks in which interlayer edges occur only between manifestations
of the same entity on different layers, although we also consider one empirical
example with a more general multilayer structure. We use numerical simulations
and a mean-field approximation to examine the steady-state behavior of the
Ising dynamics as a function of temperature (which is a key model parameter)
for a variety of two-layer multilayer networks from both models and empirical
data. We examine both the steady-state behavior and a metastable state in which
the two layers are anti-aligned, and we explore the effects of interlayer
coupling strength and structural heterogeneity. In synthetic multilayer
networks with core--periphery structure, we show that interlayer edges that
involve peripheral nodes can exert more influence than interlayer edges that
involve only core nodes. Finally, we consider empirical multilayer networks
from biological and social systems. Our work illustrates how heterogeneity
across the layers of a multilayer network influences dynamics on the whole
network.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [260] [Chiseling: Powerful and Valid Subgroup Selection via Interactive Machine Learning](https://arxiv.org/abs/2509.19490)
*Nathan Cheng,Asher Spector,Lucas Janson*

Main category: stat.ME

TL;DR: The paper introduces "chiseling," a method for controlled subgroup selection in regression and causal inference that offers inferential guarantees while being interactive and efficient.


<details>
  <summary>Details</summary>
Motivation: There is a need for methods to reliably identify subgroups with desired properties (e.g., positive average treatment effects) that go beyond the limitations of current approaches lacking inferential guarantees or sacrificing efficiency.

Method: The proposed chiseling method iteratively refines and tests subgroups by shrinking them in directions based on points outside the subgroup, allowing analysts to utilize prior knowledge or machine learning, while ensuring inferential validity under minimal assumptions.

Result: Chiseling demonstrates superior performance in identifying subgroups with desired properties compared to existing methods, showing effectiveness across simulated and real datasets.

Conclusion: Chiseling offers a novel, flexible, and efficient approach to controlled subgroup selection with strong inferential guarantees, enabling better subgroup identification for regression and causal inference tasks.

Abstract: In regression and causal inference, controlled subgroup selection aims to
identify, with inferential guarantees, a subgroup (defined as a subset of the
covariate space) on which the average response or treatment effect is above a
given threshold. E.g., in a clinical trial, it may be of interest to find a
subgroup with a positive average treatment effect. However, existing methods
either lack inferential guarantees, heavily restrict the search for the
subgroup, or sacrifice efficiency by naive data splitting. We propose a novel
framework called chiseling that allows the analyst to interactively refine and
test a candidate subgroup by iteratively shrinking it. The sole restriction is
that the shrinkage direction only depends on the points outside the current
subgroup, but otherwise the analyst may leverage any prior information or
machine learning algorithm. Despite this flexibility, chiseling controls the
probability that the discovered subgroup is null (e.g., has a non-positive
average treatment effect) under minimal assumptions: for example, in randomized
experiments, this inferential validity guarantee holds under only bounded
moment conditions. When applied to a variety of simulated datasets and a real
survey experiment, chiseling identifies substantially better subgroups than
existing methods with inferential guarantees.

</details>


### [261] [Hierarchical Bayesian Operator-induced Symbolic Regression Trees for Structural Learning of Scientific Expressions](https://arxiv.org/abs/2509.19710)
*Somjit Roy,Pritam Dey,Debdeep Pati,Bani K. Mallick*

Main category: stat.ME

TL;DR: The paper introduces a Bayesian framework for symbolic regression to uncover scientific laws, distinguishing itself with principled uncertainty quantification and robust performance compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing symbolic regression methods rely on heuristic algorithms or black-box approaches that often lack interpretability, require low-noise data, and fail to provide principled uncertainty quantification.

Method: The authors develop a hierarchical Bayesian framework for symbolic regression with ensembles of tree-structured expressions, employing Markov chain Monte Carlo for posterior inference and a marginal posterior-based selection adhering to Occam’s window principle.

Result: The framework demonstrates strong performance in recovering scientific laws, outperforming state-of-the-art methods on simulated data, canonical Feynman equations, and single-atom catalysis datasets.

Conclusion: The Bayesian approach to symbolic regression ensures predictive accuracy, structural parsimony, and theoretical rigor, making it robust and interpretable for uncovering scientific laws.

Abstract: The advent of Scientific Machine Learning has heralded a transformative era
in scientific discovery, driving progress across diverse domains. Central to
this progress is uncovering scientific laws from experimental data through
symbolic regression. However, existing approaches are dominated by heuristic
algorithms or data-hungry black-box methods, which often demand low-noise
settings and lack principled uncertainty quantification. Motivated by
interpretable Statistical Artificial Intelligence, we develop a hierarchical
Bayesian framework for symbolic regression that represents scientific laws as
ensembles of tree-structured symbolic expressions endowed with a regularized
tree prior. This coherent probabilistic formulation enables full posterior
inference via an efficient Markov chain Monte Carlo algorithm, yielding a
balance between predictive accuracy and structural parsimony. To guide symbolic
model selection, we develop a marginal posterior-based criterion adhering to
the Occam's window principle and further quantify structural fidelity to ground
truth through a tailored expression-distance metric. On the theoretical front,
we establish near-minimax rate of Bayesian posterior concentration, providing
the first rigorous guarantee in context of symbolic regression. Empirical
evaluation demonstrates robust performance of our proposed methodology against
state-of-the-art competing modules on a simulated example, a suite of canonical
Feynman equations, and single-atom catalysis dataset.

</details>


### [262] [Statistical Inference Leveraging Synthetic Data with Distribution-Free Guarantees](https://arxiv.org/abs/2509.20345)
*Meshi Bashari,Yonghoon Lee,Roy Maor Lotan,Edgar Dobriban,Yaniv Romano*

Main category: stat.ME

TL;DR: The paper introduces the GESPI framework to improve statistical inference by safely combining real and synthetic data.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of integrating synthetic data with real data while ensuring statistical inference remains reliable and efficient.

Method: The GESPI wraps around existing inference methods, adaptively using synthetic data when it is high-quality and defaulting to real data when synthetic data is unreliable, without any distributional assumptions.

Result: GESPI enhances statistical power, adheres to error bounds, and seamlessly integrates with existing statistical procedures, demonstrated through tasks like AlphaFold predictions and reasoning model evaluations.

Conclusion: The framework offers a robust solution for leveraging synthetic data in statistical inference, boosting efficiency and accuracy without compromising reliability.

Abstract: The rapid proliferation of high-quality synthetic data -- generated by
advanced AI models or collected as auxiliary data from related tasks --
presents both opportunities and challenges for statistical inference. This
paper introduces a GEneral Synthetic-Powered Inference (GESPI) framework that
wraps around any statistical inference procedure to safely enhance sample
efficiency by combining synthetic and real data. Our framework leverages
high-quality synthetic data to boost statistical power, yet adaptively defaults
to the standard inference method using only real data when synthetic data is of
low quality. The error of our method remains below a user-specified bound
without any distributional assumptions on the synthetic data, and decreases as
the quality of the synthetic data improves. This flexibility enables seamless
integration with conformal prediction, risk control, hypothesis testing, and
multiple testing procedures, all without modifying the base inference method.
We demonstrate the benefits of our method on challenging tasks with limited
labeled data, including AlphaFold protein structure prediction, and comparing
large reasoning models on complex math problems.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [263] [A Novel Short-Term Anomaly Prediction for IIoT with Software Defined Twin Network](https://arxiv.org/abs/2509.20068)
*Bilal Dalgic,Betul Sen,Muge Erel-Ozcevik*

Main category: cs.NI

TL;DR: The paper presents a novel framework integrating Software-Defined Network (SDN) and Digital Twin (DT) paradigms in Industrial IoT (IIoT) environments for short-term anomaly detection.


<details>
  <summary>Details</summary>
Motivation: Existing approaches lack implementation details for utilizing SDN-based DT and time-aware model training to efficiently predict IIoT threats.

Method: A new SD-TWIN framework coupled with time-aware labeling and evaluation of machine learning models, specifically deploying a GPU-accelerated LightGBM model for real-time detection.

Result: The deployment demonstrated that the GPU-accelerated LightGBM model performs effectively with high recall and strong classification capabilities for detecting anomalies.

Conclusion: Integrating SDN-based DT paradigms along with intelligent time-aware techniques can achieve efficient and secure monitoring for IIoT systems, particularly in addressing short-term anomalies.

Abstract: Secure monitoring and dynamic control in an IIoT environment are major
requirements for current development goals. We believe that dynamic, secure
monitoring of the IIoT environment can be achieved through integration with the
Software-Defined Network (SDN) and Digital Twin (DT) paradigms. The current
literature lacks implementation details for SDN-based DT and time-aware
intelligent model training for short-term anomaly detection against IIoT
threats. Therefore, we have proposed a novel framework for short-term anomaly
detection that uses an SDN-based DT. Using a comprehensive dataset, time-aware
labeling of features, and a comprehensive evaluation of various machine
learning models, we propose a novel SD-TWIN-based anomaly detection algorithm.
According to the performance of a new real-time SD-TWIN deployment, the GPU-
accelerated LightGBM model is particularly effective, achieving a balance of
high recall and strong classification performance.

</details>


### [264] [Radio Propagation Modelling: To Differentiate or To Deep Learn, That Is The Question](https://arxiv.org/abs/2509.19337)
*Stefanos Bakirtzis,Paul Almasan,José Suárez-Varela,Gabriel O. Ferreira,Michail Kalntis,André Felipe Zanella,Ian Wassell,Andra Lutu*

Main category: cs.NI

TL;DR: The paper evaluates differentiable ray tracing against deep learning models for large-scale radio coverage emulation using real-world data. The results favor deep learning in accuracy and adaptability.


<details>
  <summary>Details</summary>
Motivation: To validate the scalability and practical benefits of differentiable ray tracing in production-grade networks and provide guidance for mobile network operators and the research community.

Method: Comparing differentiable ray tracing and deep learning models using extensive real-world data covering 13 cities and over 10,000 antennas.

Result: Differentiable ray-tracing simulators have reduced the efficiency-accuracy gap but struggle to generalize at scale, whereas DL models achieve up to 3 dB accuracy gains and adapt faster across various deployments.

Conclusion: DL models outperform differentiable ray tracing in accuracy and adaptability, making them more suitable for large-scale and real-time applications in radio network emulation.

Abstract: Differentiable ray tracing has recently challenged the status quo in radio
propagation modelling and digital twinning. Promising unprecedented speed and
the ability to learn from real-world data, it offers a real alternative to
conventional deep learning (DL) models. However, no experimental evaluation on
production-grade networks has yet validated its assumed scalability or
practical benefits. This leaves mobile network operators (MNOs) and the
research community without clear guidance on its applicability. In this paper,
we fill this gap by employing both differentiable ray tracing and DL models to
emulate radio coverage using extensive real-world data collected from the
network of a major MNO, covering 13 cities and more than 10,000 antennas. Our
results show that, while differentiable ray-tracing simulators have contributed
to reducing the efficiency-accuracy gap, they struggle to generalize from
real-world data at a large scale, and they remain unsuitable for real-time
applications. In contrast, DL models demonstrate higher accuracy and faster
adaptation than differentiable ray-tracing simulators across urban, suburban,
and rural deployments, achieving accuracy gains of up to 3 dB. Our experimental
results aim to provide timely insights into a fundamental open question with
direct implications on the wireless ecosystem and future research.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [265] [Impact of RHIs and ipSIC on Active RIS-NOMA Systems with Low-Precision ADCs](https://arxiv.org/abs/2509.19383)
*Qianqian Li,Hua Li,Shiya Hao,Lintao Li,Xiaoming Dai*

Main category: eess.SP

TL;DR: A study on ARIS-assisted NOMA system with low-precision ADCs, analyzing outage probability and system performance under impairments.


<details>
  <summary>Details</summary>
Motivation: To improve communication performance for systems using low-precision ADCs and intelligent surfaces, addressing weaknesses like hardware impairments and imperfect interference cancellation.

Method: Analytical approximation of outage probability, high-SNR asymptotic analysis, and simulation of system throughput and diversity order.

Result: ARIS-NOMA outperforms PRIS-NOMA by lowering outage probability and increasing throughput with optimized power usage and reflecting elements.

Conclusion: ARIS-NOMA offers a viable solution for mitigating low-precision ADC impacts while achieving better performance compared to passive systems.

Abstract: This study evaluates the performance of an active reconfigurable intelligent
surface (ARIS)-assisted non-orthogonal multiple access (NOMA) system employing
low-precision analog-to-digital converters (ADCs). Analytical approximations
for the outage probability (OP) are derived, considering residual hardware
impairments (RHIs) and imperfect successive interference cancellation (ipSIC).
Additionally, we analyze the asymptotic OP, system throughput, and diversity
order at high signal-to-noise ratios (SNRs). Simulation results demonstrate
that the proposed quantized ARIS-NOMA system outperforms its passive
counterpart (PRIS-NOMA), achieving lower OP and higher throughput with reduced
transmit power requirements and fewer reflecting elements. Moreover, the outage
performance of both quantized ARIS-NOMA and PRIS-NOMA systems demonstrates
significant improvement as the number of reflecting elements increases. The
negative impacts of low-precision ADCs can be effectively mitigated by
optimizing transmit power and scaling the number of reflecting elements.

</details>


### [266] [Low-Cost Sensor Fusion Framework for Organic Substance Classification and Quality Control Using Classification Methods](https://arxiv.org/abs/2509.19367)
*Borhan Uddin Chowdhury,Damian Valles,Md Raf E Ul Shougat*

Main category: eess.SP

TL;DR: The paper introduces a low-cost sensor-fusion framework utilizing Arduino Mega 2560 with gas sensors to classify and control the quality of organic substances, achieving up to 94% accuracy via machine learning models.


<details>
  <summary>Details</summary>
Motivation: To develop a non-destructive and efficient method for quality control and classification of organic substances using a cost-effective hardware platform.

Method: Data from sensors on the Arduino Mega 2560 were collected for ten organic classes. The data underwent preprocessing (correlation analysis, PCA/LDA), and several machine learning models (SVM, DT, RF, ANN, ensemble classifiers) were implemented and tested.

Result: The best-performing models, including Random Forest, ensemble, and ANN, achieved 93-94% classification accuracy on the organic substances dataset.

Conclusion: Low-cost platforms like Arduino Mega 2560, combined with sensor fusion and machine learning, can provide reliable tools for rapid organic compound identification and quality control.

Abstract: We present a sensor-fusion framework for rapid, non-destructive
classification and quality control of organic substances, built on a standard
Arduino Mega 2560 microcontroller platform equipped with three commercial
environmental and gas sensors. All data used in this study were generated
in-house: sensor outputs for ten distinct classes - including fresh and expired
samples of apple juice, onion, garlic, and ginger, as well as cinnamon and
cardamom - were systematically collected and labeled using this hardware setup,
resulting in a unique, application-specific dataset. Correlation analysis was
employed as part of the preprocessing pipeline for feature selection. After
preprocessing and dimensionality reduction (PCA/LDA), multiple supervised
learning models - including Support Vector Machine (SVM), Decision Tree (DT),
and Random Forest (RF), each with hyperparameter tuning, as well as an
Artificial Neural Network (ANN) and an ensemble voting classifier - were
trained and cross-validated on the collected dataset. The best-performing
models, including tuned Random Forest, ensemble, and ANN, achieved test
accuracies in the 93 to 94 percent range. These results demonstrate that
low-cost, multisensory platforms based on the Arduino Mega 2560, combined with
advanced machine learning and correlation-driven feature engineering, enable
reliable identification and quality control of organic compounds.

</details>


### [267] [Hybrid Pipeline SWD Detection in Long-Term EEG Signals](https://arxiv.org/abs/2509.19387)
*Antonio Quintero Rincon,Nicolas Masino,Veronica Marsico,Hadj Batatia*

Main category: eess.SP

TL;DR: Providing automated screening of spike-and-wave discharges, this paper introduces lightweight computational methods yielding robust results.


<details>
  <summary>Details</summary>
Motivation: Manual identification of spike-and-wave discharges in long EEG recordings is time-consuming and prone to errors.

Method: A hybrid pipeline combines analytical feature extraction (two-sided moving-average filter and statistical descriptors) with a single-hidden-layer ANN for classification.

Result: The method achieved 98% sensitivity, 96.2% specificity, and 97.2% overall accuracy on multi-patient EEG data.

Conclusion: Combining statistical features with a simple ANN provides an accurate, real-time, and patient-specific tool for SWD detection.

Abstract: Spike-and-wave discharges (SWDs) are the electroencephalographic hallmark of
absence epilepsy, yet their manual identification in multi-day recordings
remains labour-intensive and error-prone. We present a lightweight hybrid
pipeline that couples analytical features with a shallow artificial neural
network (ANN) for accurate, patient-specific SWD detection in long-term,
monopolar EEG. A two-sided moving-average (MA) filter first suppresses the
high-frequency components of normal background activity. The residual signal is
then summarised by the mean and the standard deviation of its normally
distributed samples, yielding a compact, two-dimensional feature vector for
every 20s window. These features are fed to a single-hidden-layer ANN trained
via back-propagation to classify each window as SWD or non-SWD. The method was
evaluated on 780 channels sampled at 256 Hz from 12 patients, comprising 392
annotated SWD events. It correctly detected 384 events (sensitivity: 98%) while
achieving a specificity of 96.2 % and an overall accuracy of 97.2%. Because
feature extraction is analytic, and the classifier is small, the pipeline runs
in real-time and requires no manual threshold tuning. These results indicate
that normal-distribution descriptors combined with a modest ANN provide an
effective and computationally inexpensive solution for automated SWD screening
in extended EEG recordings.

</details>


### [268] [Generalized Nonnegative Structured Kruskal Tensor Regression](https://arxiv.org/abs/2509.19900)
*Xinjue Wang,Esa Ollila,Sergiy A. Vorobyov,Ammar Mian*

Main category: eess.SP

TL;DR: The paper presents a new tensor regression framework, NS-KTR, with hybrid regularization and nonnegativity constraints, achieving better performance and interpretability, especially for hyperspectral image analysis.


<details>
  <summary>Details</summary>
Motivation: The authors aim to improve interpretability and performance in tensor regression models, particularly for applications involving structurally heterogeneous multidimensional data.

Method: The paper introduces NS-KTR, incorporating mode-specific hybrid regularizers (fused LASSO, total variation, ridge) and nonnegativity constraints, using an ADMM-based algorithm for parameter estimation.

Result: Experiments show that NS-KTR consistently surpasses conventional tensor regression methods in analyzing synthetic and real hyperspectral datasets.

Conclusion: NS-KTR effectively preserves tensor structure while maintaining physical interpretability, making it ideal for signal processing and hyperspectral image use cases.

Abstract: This paper introduces Generalized Nonnegative Structured Kruskal Tensor
Regression (NS-KTR), a novel tensor regression framework that enhances
interpretability and performance through mode-specific hybrid regularization
and nonnegativity constraints. Our approach accommodates both linear and
logistic regression formulations for diverse response variables while
addressing the structural heterogeneity inherent in multidimensional tensor
data. We integrate fused LASSO, total variation, and ridge regularizers, each
tailored to specific tensor modes, and develop an efficient alternating
direction method of multipliers (ADMM) based algorithm for parameter
estimation. Comprehensive experiments on synthetic signals and real
hyperspectral datasets demonstrate that NS-KTR consistently outperforms
conventional tensor regression methods. The framework's ability to preserve
distinct structural characteristics across tensor dimensions while ensuring
physical interpretability makes it especially suitable for applications in
signal processing and hyperspectral image analysis.

</details>


### [269] [A Federated Fine-Tuning Paradigm of Foundation Models in Heterogenous Wireless Networks](https://arxiv.org/abs/2509.19306)
*Jingyi Wang,Zhongyuan Zhao,Qingtian Wang,Zexu Li,Yue Wang,Tony Q. S. Quek*

Main category: eess.SP

TL;DR: The paper addresses the performance issues in federated fine-tuning of foundation models in heterogeneous wireless networks. It proposes an online optimization framework to improve accuracy and energy efficiency using LoRA and federated learning techniques.


<details>
  <summary>Details</summary>
Motivation: Device heterogeneity and resource constraints in wireless networks pose challenges to federated fine-tuning, necessitating strategies to maintain low latency and high performance.

Method: A switching-based federated fine-tuning framework is introduced, involving LoRA modules, theoretical risk gap analysis, and optimization of model switching, power control, and bandwidth allocation via an online algorithm.

Result: Simulations on SST-2 and QNLI datasets show improvements in test accuracy and energy efficiency, validating the proposed approach.

Conclusion: The proposed framework effectively mitigates the challenges of device heterogeneity and resource constraints in wireless networks, enhancing the performance of federated fine-tuning in edge intelligence systems.

Abstract: Edge intelligence has emerged as a promising strategy to deliver low-latency
and ubiquitous services for mobile devices. Recent advances in fine-tuning
mechanisms of foundation models have enabled edge intelligence by integrating
low-rank adaptation (LoRA) with federated learning. However, in wireless
networks, the device heterogeneity and resource constraints on edge devices
pose great threats to the performance of federated fine-tuning. To tackle these
issues, we propose to optimize federated fine-tuning in heterogenous wireless
networks via online learning. First, the framework of switching-based federated
fine-tuning in wireless networks is provided. The edge devices switches to LoRA
modules dynamically for federated fine-tuning with base station to jointly
mitigate the impact of device heterogeneity and transmission unreliability.
Second, a tractable upper bound on the inference risk gap is derived based on
theoretical analysis. To improve the generalization capability, we formulate a
non-convex mixed-integer programming problem with long-term constraints, and
decouple it into model switching, transmit power control, and bandwidth
allocation subproblems. An online optimization algorithm is developed to solve
the problems with polynomial computational complexity. Finally, the simulation
results on the SST-2 and QNLI data sets demonstrate the performance gains in
test accuracy and energy efficiency.

</details>


### [270] [E2E Learning Massive MIMO for Multimodal Semantic Non-Orthogonal Transmission and Fusion](https://arxiv.org/abs/2509.19312)
*Minghui Wu,Zhen Gao*

Main category: eess.SP

TL;DR: The paper presents an end-to-end uplink-downlink CSI fusion precoding network to enhance efficiency in massive MIMO systems.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of high-dimensional downlink CSI that complicates real-time channel acquisition and precoding in massive MIMO systems.

Method: The authors propose a neural architecture involving a projection network for CSI-RS design, UE-based compression/quantization, and a fusion precoding network at the BS combining feedback-driven and SRS-driven precoders.

Result: Simulation results demonstrate that the proposed approach outperforms traditional methods by leveraging SRS-derived data and UE feedback.

Conclusion: The integrated E2E framework optimizes precoding in massive MIMO, leading to superior spectral efficiency compared to conventional techniques.

Abstract: Massive multiple-input multiple-output (MIMO) promises high spectral
efficiency but also leads to high-dimensional downlink channel state
information (CSI), which complicates real-time channel acquisition and
precoding. To address this, we propose an end-to-end (E2E) uplink-downlink CSI
fusion precoding network that jointly models downlink CSI reference signal
(CSI-RS) design, CSI feedback, and base-station (BS) precoding within a single
E2E neural architecture. Concretely, a projection network built on the MAXIM
architecture takes uplink sounding reference signals (SRS) as input and outputs
frequency-, beam-, and port-domain projection matrices for designing downlink
CSI-RS. User equipment (UE) then compresses/quantizes the resulting CSI-RS
observations and feeds back a compact representation. At the base station (BS),
two complementary branches produce candidate precoders: one is a feedback-only
precoding network driven by quantized downlink observations, and the other is
an SRS-only precoding network driven by uplink SRS. These candidate precoders
are subsequently combined by a fusion precoding network to yield the final
transmit precoder. All the modules are trained with a
spectral-efficiency-oriented loss under a three-stage schedule. Simulation
results show that the proposed approach effectively harnesses both SRS-derived
information and UE feedback, achieving markedly better performance than
conventional baselines.

</details>


### [271] [Advancing Few-Shot Pediatric Arrhythmia Classification with a Novel Contrastive Loss and Multimodal Learning](https://arxiv.org/abs/2509.19315)
*Yiqiao Chen,Zijian Huang,Zhenghui Feng*

Main category: eess.SP

TL;DR: This paper introduces a deep learning framework to improve detection and classification of pediatric arrhythmias using multimodal ECG and IEGM signals, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Pediatric arrhythmias pose risks like sudden cardiac death but their automated classification faces challenges such as class imbalance and complex signal properties. Enhancing detection methods is crucial for early screening and effective clinical actions.

Method: The framework combines dual-branch convolutional encoders for ECG and IEGM signals, semantic attention for cross-modal feature alignment, a lightweight Transformer encoder for global dependency modeling, and a novel Adaptive Global Class-Aware Contrastive Loss (AGCACL) for improved class representation.

Result: The proposed framework achieved top-notch performance metrics like 97.76% Top-1 Accuracy and significant improvements in macro evaluation measures over the strongest baseline by up to +19.82 percentage points in Macro Recall.

Conclusion: This framework enhances detectability and robustness for minority arrhythmia classes, offering significant potential for clinical applications in early diagnosis and management of pediatric heart disease.

Abstract: Pediatric arrhythmias are a major risk factor for disability and sudden
cardiac death, yet their automated classification remains challenging due to
class imbalance, few-shot categories, and complex signal characteristics, which
severely limit the efficiency and reliability of early screening and clinical
intervention. To address this problem, we propose a multimodal end-to-end deep
learning framework that combines dual-branch convolutional encoders for ECG and
IEGM, semantic attention for cross-modal feature alignment, and a lightweight
Transformer encoder for global dependency modeling. In addition, we introduce a
new contrastive loss fucntion named Adaptive Global Class-Aware Contrastive
Loss (AGCACL) to enhance intra-class compactness and inter-class separability
through class prototypes and a global similarity matrix. To the best of our
knowledge, this is the first systematic study based on the Leipzig Heart Center
pediatric/congenital ECG+IEGM dataset, for which we also provide a complete and
reproducible preprocessing pipeline. Experimental results demonstrate that the
proposed method achieves the overall best performance on this dataset,
including 97.76\% Top-1 Accuracy, 94.08\% Macro Precision, 91.97\% Macro
Recall, 92.97\% Macro F1, and 92.36\% Macro F2, with improvements of +13.64,
+15.96, +19.82, and +19.44 percentage points over the strongest baseline in
Macro Precision/Recall/F1/F2, respectively. These findings indicate that the
framework significantly improves the detectability and robustness for minority
arrhythmia classes, offering potential clinical value for rhythm screening,
pre-procedural assessment, and postoperative follow-up in pediatric and
congenital heart disease populations.

</details>


### [272] [Human Activity Recognition Based on Electrocardiogram Data Only](https://arxiv.org/abs/2509.19328)
*Sina Montazeri,Waltenegus Dargie,Yunhe Feng,Kewei Sha*

Main category: eess.SP

TL;DR: This paper presents a significant advancement in activity recognition by demonstrating the ability to classify six distinct activities using only ECG data, with over 94% accuracy for seen subjects.


<details>
  <summary>Details</summary>
Motivation: To develop a more efficient method for human activity recognition without relying on resource-intensive IMUs, addressing the limitations of existing ECG-based methods which are either supplemental or limited in scope.

Method: The paper introduces three deep learning models: (1) CNN with Squeeze-and-Excitation blocks, (2) ResNet with dilated convolutions, and (3) a CNNTransformer hybrid model. These are tested on ECG data collected from 54 subjects to classify six physical activities.

Result: All three models achieved over 94% accuracy for seen subjects. For unseen subjects, the CNNTransformer hybrid model achieved the highest accuracy of 72%, showing potential for further improvement with a larger training dataset.

Conclusion: This study establishes the feasibility of ECG-only activity classification across multiple physical activities, proposing a path for next-gen wearables combining cardiac monitoring and activity recognition without motion sensors.

Abstract: Human activity recognition is critical for applications such as early
intervention and health analytics. Traditional activity recognition relies on
inertial measurement units (IMUs), which are resource intensive and require
calibration. Although electrocardiogram (ECG)-based methods have been explored,
these have typically served as supplements to IMUs or have been limited to
broad categorical classification such as fall detection or active vs. inactive
in daily activities. In this paper, we advance the field by demonstrating, for
the first time, robust recognition of activity only with ECG in six distinct
activities, which is beyond the scope of previous work. We design and evaluate
three new deep learning models, including a CNN classifier with
Squeeze-and-Excitation blocks for channel-wise feature recalibration, a ResNet
classifier with dilated convolutions for multiscale temporal dependency
capture, and a novel CNNTransformer hybrid combining convolutional feature
extraction with attention mechanisms for long-range temporal relationship
modeling. Tested on data from 54 subjects for six activities, all three models
achieve over 94% accuracy for seen subjects, while CNNTransformer hybrid
reaching the best accuracy of 72% for unseen subjects, a result that can be
further improved by increasing the training population. This study demonstrates
the first successful ECG-only activity classification in multiple physical
activities, offering significant potential for developing next-generation
wearables capable of simultaneous cardiac monitoring and activity recognition
without additional motion sensors.

</details>


### [273] [LibEMER: A novel benchmark and algorithms library for EEG-based Multimodal Emotion Recognition](https://arxiv.org/abs/2509.19330)
*Zejun Liu,Yunshan Chen,Chengxi Xie,Huan Liu*

Main category: eess.SP

TL;DR: The paper presents LibEMER, an open-source evaluation framework for EEG-based multimodal emotion recognition, addressing major limitations in the field like lack of implementations and standard benchmarks.


<details>
  <summary>Details</summary>
Motivation: The field of EEG-based multimodal emotion recognition suffers from a lack of open-source implementations, standardized benchmarks, and in-depth discussions on challenges and future directions.

Method: LibEMER, a unified open-source evaluation framework, provides reproducible PyTorch implementations, standardized preprocessing and modeling protocols, and supports unbiased performance assessments on three public datasets for two learning tasks.

Result: An accessible and standardized library called LibEMER, which can fairly evaluate multimodal emotion recognition methods, is presented to enhance transparency and comparability.

Conclusion: LibEMER addresses critical gaps in EEG-based multimodal emotion recognition by fostering reproducibility, transparency, and providing a unified platform for future research exploration.

Abstract: EEG-based multimodal emotion recognition(EMER) has gained significant
attention and witnessed notable advancements, the inherent complexity of human
neural systems has motivated substantial efforts toward multimodal approaches.
However, this field currently suffers from three critical limitations: (i) the
absence of open-source implementations. (ii) the lack of standardized and
transparent benchmarks for fair performance analysis. (iii) in-depth discussion
regarding main challenges and promising research directions is a notable
scarcity. To address these challenges, we introduce LibEMER, a unified
evaluation framework that provides fully reproducible PyTorch implementations
of curated deep learning methods alongside standardized protocols for data
preprocessing, model realization, and experimental setups. This framework
enables unbiased performance assessment on three widely-used public datasets
across two learning tasks. The open-source library is publicly accessible at:
https://anonymous.4open.science/r/2025ULUIUBUEUMUEUR485384

</details>


### [274] [Holographic Transformers for Complex-Valued Signal Processing: Integrating Phase Interference into Self-Attention](https://arxiv.org/abs/2509.19331)
*Enhao Huang,Zhiyu Zhang,Tianxiang Xu,Chunshu Xia,Kaichun Hu,Yuchen Yang,Tongtong Pan,Dong Dong,Zhan Qin*

Main category: eess.SP

TL;DR: The paper introduces the Holographic Transformer, a physics-inspired model that incorporates wave interference principles in self-attention to enhance complex-valued signal processing.


<details>
  <summary>Details</summary>
Motivation: Most deep learning models overlook the phase element in complex-valued signals, leading to inconsistent modeling that undermines effective signal representation.

Method: The Holographic Transformer integrates wave interference into self-attention, modulates interactions via relative phase, maintains amplitude-phase consistency, and employs a dual-headed decoder to prevent phase collapse.

Result: Experiments reveal improved classification accuracy, F1 scores, reduced regression error, and increased robustness to phase perturbations in PolSAR image classification and wireless channel prediction.

Conclusion: Incorporating physical consistency into attention improves complex-valued learning and provides a coherent, unified framework for signal modeling based on physics principles.

Abstract: Complex-valued signals encode both amplitude and phase, yet most deep models
treat attention as real-valued correlation, overlooking interference effects.
We introduce the Holographic Transformer, a physics-inspired architecture that
incorporates wave interference principles into self-attention. Holographic
attention modulates interactions by relative phase and coherently superimposes
values, ensuring consistency between amplitude and phase. A dual-headed decoder
simultaneously reconstructs the input and predicts task outputs, preventing
phase collapse when losses prioritize magnitude over phase. We demonstrate that
holographic attention implements a discrete interference operator and maintains
phase consistency under linear mixing. Experiments on PolSAR image
classification and wireless channel prediction show strong performance,
achieving high classification accuracy and F1 scores, low regression error, and
increased robustness to phase perturbations. These results highlight that
enforcing physical consistency in attention leads to generalizable improvements
in complex-valued learning and provides a unified, physics-based framework for
coherent signal modeling. The code is available at
https://github.com/EonHao/Holographic-Transformers.

</details>


### [275] [CSIYOLO: An Intelligent CSI-based Scatter Sensing Framework for Integrated Sensing and Communication Systems](https://arxiv.org/abs/2509.19335)
*Xudong Zhang,Jingbo Tan,Zhizhen Ren,Jintao Wang,Yihua Ma,Jian Song*

Main category: eess.SP

TL;DR: This paper introduces CSIYOLO, a framework for scatter localization using estimated Channel State Information (CSI) from a single base station, achieving superior localization accuracy without requiring waveform modification.


<details>
  <summary>Details</summary>
Motivation: To address the lack of compatibility and low accuracy limitations in current ISAC methods for scatter sensing, aiming for applications in areas like autonomous driving and low-altitude navigation.

Method: The framework uses an anchor-based scatter parameter detection inspired by YOLO architecture and a CSI-based localization algorithm. It employs an extendable network structure and a noise injection training strategy for enhanced robustness and efficiency.

Result: Experiments reveal that CSIYOLO significantly surpasses existing methods in scatter localization accuracy under different numbers of scatters and estimation errors.

Conclusion: CSIYOLO seamlessly integrates with existing communication systems, offering high-accuracy scatter localization with low implementation complexities.

Abstract: ISAC is regarded as a promising technology for next-generation communication
systems, enabling simultaneous data transmission and target sensing. Among
various tasks in ISAC, scatter sensing plays a crucial role in exploiting the
full potential of ISAC and supporting applications such as autonomous driving
and low-altitude economy. However, most existing methods rely on either
waveform and hardware modifications or traditional signal processing schemes,
leading to poor compatibility with current communication systems and limited
sensing accuracy. To address these challenges, we propose CSIYOLO, a framework
that performs scatter localization only using estimated CSI from a single base
station-user equipment pair. This framework comprises two main components:
anchor-based scatter parameter detection and CSI-based scatter localization.
First, by formulating scatter parameter extraction as an image detection
problem, we propose an anchor-based scatter parameter detection method inspired
by You Only Look Once architectures. After that, a CSI-based localization
algorithm is derived to determine scatter locations with extracted parameters.
Moreover, to improve localization accuracy and implementation efficiency, we
design an extendable network structure with task-oriented optimizations,
enabling multi-scale anchor detection and better adaptation to CSI
characteristics. A noise injection training strategy is further designed to
enhance robustness against channel estimation errors. Since the proposed
framework operates solely on estimated CSI without modifying waveforms or
signal processing pipelines, it can be seamlessly integrated into existing
communication systems as a plugin. Experiments show that our proposed method
can significantly outperform existing methods in scatter localization accuracy
with relatively low complexities under varying numbers of scatters and
estimation errors.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [276] [Digital Signal Processing from Classical Coherent Systems to Continuous-Variable QKD: A Review of Cross-Domain Techniques, Applications, and Challenges](https://arxiv.org/abs/2509.20141)
*Davi Juvêncio Gomes de Sousa,Caroline da Silva Morais Alves,Valéria Loureiro da Silva,Nelson Alves Ferreira Neto*

Main category: quant-ph

TL;DR: This systematic review explores how digital signal processing (DSP) techniques optimized for optical communications can be applied to continuous-variable quantum key distribution (CV-QKD), highlighting adaptations, challenges, and opportunities in integrating classical and quantum systems.


<details>
  <summary>Details</summary>
Motivation: To investigate how DSP advancements in optical communications contribute to the performance and scalability of CV-QKD, aiming to overcome technical barriers and facilitate innovations in quantum communication.

Method: The APISSER methodology, adapted from PRISMA protocol, was used to conduct a structured search and analysis of 220 publications from IEEE Xplore and Web of Science databases between 2021-2025, focusing on six research questions concerning DSP techniques in CV-QKD.

Result: Classical DSP algorithms, such as Kalman filtering and adaptive equalization, were successfully modified for CV-QKD. Additionally, recent DSP innovations, like neural equalization and probabilistic shaping, show promise, though challenges persist, especially in signal robustness under ultra-low SNR conditions.

Conclusion: Integrating classical DSP with CV-QKD enhances system performance but requires tailored adaptations due to quantum constraints. There is promise in emerging techniques, yet the field must address critical barriers for scalability and robustness in quantum communications.

Abstract: This systematic review investigates the application of digital signal
processing (DSP) techniques -- originally developed for coherent optical
communication systems to continuous-variable quantum key distribution (CV-QKD).
The convergence of these domains has enabled significant advances in CV-QKD
performance, particularly in phase synchronization, polarization tracking, and
excess noise mitigation. To provide a comprehensive and reproducible synthesis
of this emerging field, we employed the APISSER methodology, a task-oriented
framework adapted from the PRISMA protocol. A structured search across IEEE
Xplore and Web of Science databases (2021-2025) yielded 220 relevant
publications, which were screened, classified, and analyzed to address six
research questions. Our findings highlight that many classical DSP algorithms,
such as Kalman filtering, carrier recovery, adaptive equalization, and
machine-learning-assisted signal estimation, have been successfully adapted to
the quantum regime, often requiring modifications to meet security and noise
constraints. We also identify a range of recent DSP innovations in coherent
optical communication systems with high potential for future CV-QKD
integration, including neural equalization, probabilistic shaping, and joint
retiming-equalization filters. Despite these advances, challenges remain in
achieving robust phase tracking under ultra-low Signal-to-Noise Ratio (SNR)
conditions, real-time polarization compensation, and secure co-existence with
classical channels. This review maps current trends, technical barriers, and
emerging opportunities at the intersection of signal processing for quantum and
classical communication, supporting the development of scalable and resilient
CV-QKD systems.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [277] [HPL-MxP Benchmark: Mixed-Precision Algorithms, Iterative Refinement, and Scalable Data Generation](https://arxiv.org/abs/2509.19618)
*Jack Dongarra,Piotr Luszczek*

Main category: math.NA

TL;DR: HPL-MxP benchmark combines mixed-precision LU factorization and GMRES-based iterative refinement to evaluate numerical stability and supercomputing performance.


<details>
  <summary>Details</summary>
Motivation: To develop a benchmark capable of assessing Exascale-level throughput and reliable evaluation of AI hardware accelerators.

Method: Introduce mixed-precision LU factorization, GMRES iterative refinement, scalable matrix generation, and diagonal scaling analysis.

Result: Achieved Exascale-level compute throughput; demonstrated impact of diagonal scaling on solution quality.

Conclusion: HPL-MxP is viable for benchmarking supercomputers and offers potential for increased adoption in AI hardware evaluation.

Abstract: We present a mixed-precision benchmark called HPL-MxP that uses both a
lower-precision LU factorization with a non-stationary iterative refinement
based on GMRES. We evaluate the numerical stability of one of the methods of
generating the input matrix in a scalable fashion and show how the diagonal
scaling affects the solution quality in terms of the backward-error. Some of
the performance results at large scale supercomputing installations produced
Exascale-level compute throughput numbers thus proving the viability of the
proposed benchmark for evaluating such machines. We also present the potential
of the benchmark to continue increasing its use with proliferation of hardware
accelerators for AI workloads whose reliable evaluation continues to pose a
particular challenge for the users.

</details>
