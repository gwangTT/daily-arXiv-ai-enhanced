<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 10]
- [cs.AR](#cs.AR) [Total: 4]
- [cs.CL](#cs.CL) [Total: 10]
- [cs.CV](#cs.CV) [Total: 10]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.LG](#cs.LG) [Total: 13]
- [cs.NE](#cs.NE) [Total: 5]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.RO](#cs.RO) [Total: 10]
- [cs.SE](#cs.SE) [Total: 10]
- [q-bio.NC](#q-bio.NC) [Total: 3]
- [stat.ML](#stat.ML) [Total: 7]
- [q-fin.MF](#q-fin.MF) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.CG](#cs.CG) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Agentic Business Process Management Systems](https://arxiv.org/abs/2601.18833)
*Marlon Dumas,Fredrik Milani,David Chapela-Campa*

Main category: cs.AI

TL;DR: This paper discusses how Generative and Agentic AI can shift Business Process Management (BPM) from automation to autonomy using process mining, proposing an Agentic BPM System (A-BPMS) with enhanced autonomy, reasoning, and learning.


<details>
  <summary>Details</summary>
Motivation: To explore and leverage advances in Generative and Agentic AI to enhance BPM by transitioning from traditional design-driven approaches to data-driven autonomy, enabling better process optimization and governance.

Method: Proposes an architectural vision of Agentic BPM Systems (A-BPMS) that integrate process mining, agent autonomy, reasoning, and learning, supporting processes ranging from human-driven to fully autonomous.

Result: The paper theorizes that A-BPMS will redefine BPM by enabling AI agents to sense, reason, and act on process states for maintaining and optimizing performance.

Conclusion: A-BPMS will transform BPM by shifting the paradigm to autonomous, data-driven process management, spanning human-driven to fully autonomous workflows, and enhancing governance and optimization.

Abstract: Since the early 90s, the evolution of the Business Process Management (BPM) discipline has been punctuated by successive waves of automation technologies. Some of these technologies enable the automation of individual tasks, while others focus on orchestrating the execution of end-to-end processes. The rise of Generative and Agentic Artificial Intelligence (AI) is opening the way for another such wave. However, this wave is poised to be different because it shifts the focus from automation to autonomy and from design-driven management of business processes to data-driven management, leveraging process mining techniques. This position paper, based on a keynote talk at the 2025 Workshop on AI for BPM, outlines how process mining has laid the foundations on top of which agents can sense process states, reason about improvement opportunities, and act to maintain and optimize performance. The paper proposes an architectural vision for Agentic Business Process Management Systems (A-BPMS): a new class of platforms that integrate autonomy, reasoning, and learning into process management and execution. The paper contends that such systems must support a continuum of processes, spanning from human-driven to fully autonomous, thus redefining the boundaries of process automation and governance.

</details>


### [2] [LLM Driven Design of Continuous Optimization Problems with Controllable High-level Properties](https://arxiv.org/abs/2601.18846)
*Urban Skvorc,Niki van Stein,Moritz Seiler,Britta Grimme,Thomas Bäck,Heike Trautmann*

Main category: cs.AI

TL;DR: The paper introduces a method to design optimisation problems with specific landscape characteristics using LLMs embedded in an evolutionary framework.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of structural diversity in current benchmark suites like BBOB for continuous black-box optimisation.

Method: The LLaMEA framework guides large language models to generate optimisation problem codes based on desired properties, scoring outputs using ELA-based predictors and fitness-sharing mechanisms that increase population diversity.

Result: Generated functions exhibit intended structural traits, verified through analysis and visual inspection, and expand BBOB's instance space.

Conclusion: This approach provides a diverse, interpretable, and reproducible library of benchmark problems for landscape analysis and algorithm selection.

Abstract: Benchmarking in continuous black-box optimisation is hindered by the limited structural diversity of existing test suites such as BBOB. We explore whether large language models embedded in an evolutionary loop can be used to design optimisation problems with clearly defined high-level landscape characteristics. Using the LLaMEA framework, we guide an LLM to generate problem code from natural-language descriptions of target properties, including multimodality, separability, basin-size homogeneity, search-space homogeneity and globallocal optima contrast. Inside the loop we score candidates through ELA-based property predictors. We introduce an ELA-space fitness-sharing mechanism that increases population diversity and steers the generator away from redundant landscapes. A complementary basin-of-attraction analysis, statistical testing and visual inspection, verifies that many of the generated functions indeed exhibit the intended structural traits. In addition, a t-SNE embedding shows that they expand the BBOB instance space rather than forming an unrelated cluster. The resulting library provides a broad, interpretable, and reproducible set of benchmark problems for landscape analysis and downstream tasks such as automated algorithm selection.

</details>


### [3] [Explainable Uncertainty Quantification for Wastewater Treatment Energy Prediction via Interval Type-2 Neuro-Fuzzy System](https://arxiv.org/abs/2601.18897)
*Qusai Khaled,Bahjat Mallak,Uzay Kaymak,Laura Genga*

Main category: cs.AI

TL;DR: This study introduces the IT2-ANFIS model for wastewater treatment energy forecasting, offering explainable uncertainty quantification alongside comparable predictive performance to existing models.


<details>
  <summary>Details</summary>
Motivation: Energy consumption in wastewater treatment is high, and accurate forecasting is crucial for optimization and sustainability. Current machine learning lacks explainable uncertainty quantification needed for critical decision-making.

Method: The authors propose an Interval Type-2 Adaptive Neuro-Fuzzy Inference System (IT2-ANFIS) that explains uncertainty across feature, rule, and instance levels through interpretable fuzzy rule structures.

Result: IT2-ANFIS is tested on real-world data from Melbourne Water's Eastern Treatment Plant, demonstrating reduced prediction variance, comparable accuracy to first order ANFIS, and improved explainability of uncertainty.

Conclusion: IT2-ANFIS enhances energy forecast models by providing interpretable uncertainties, making it suitable for safety-critical applications in wastewater treatment.

Abstract: Wastewater treatment plants consume 1-3% of global electricity, making accurate energy forecasting critical for operational optimization and sustainability. While machine learning models provide point predictions, they lack explainable uncertainty quantification essential for risk-aware decision-making in safety-critical infrastructure. This study develops an Interval Type-2 Adaptive Neuro-Fuzzy Inference System (IT2-ANFIS) that generates interpretable prediction intervals through fuzzy rule structures. Unlike black-box probabilistic methods, the proposed framework decomposes uncertainty across three levels: feature-level, footprint of uncertainty identify which variables introduce ambiguity, rule-level analysis reveals confidence in local models, and instance-level intervals quantify overall prediction uncertainty. Validated on Melbourne Water's Eastern Treatment Plant dataset, IT2-ANFIS achieves comparable predictive performance to first order ANFIS with substantially reduced variance across training runs, while providing explainable uncertainty estimates that link prediction confidence directly to operational conditions and input variables.

</details>


### [4] [Neural Theorem Proving for Verification Conditions: A Real-World Benchmark](https://arxiv.org/abs/2601.18944)
*Qiyuan Xu,Xiaokun Luan,Renxi Wang,Joshua Ong Jun Leang,Peixin Wang,Haonan Li,Wenda Li,Conrad Watt*

Main category: cs.AI

TL;DR: This paper introduces NTP4VC, a benchmark leveraging real-world Verification Conditions (VCs) for analyzing the potential of Neural Theorem Proving in program verification.


<details>
  <summary>Details</summary>
Motivation: Automated theorem proving faces challenges with hard Verification Conditions (VCs) in program verification, necessitating manual intervention. There is a gap in applying machine learning to address this bottleneck.

Method: The authors present NTP4VC, a multi-language benchmark for VC proving using projects like Linux. Generated test cases are evaluated with large language models, including fine-tuned ones for theorem proving.

Result: Evaluation with LLMs shows potential for automatic VC proving but reveals limits in their application to program verification.

Conclusion: NTP4VC benchmark serves as a foundational tool to explore machine learning in VC proving, highlighting challenges and research opportunities in program verification.

Abstract: Theorem proving is fundamental to program verification, where the automated proof of Verification Conditions (VCs) remains a primary bottleneck. Real-world program verification frequently encounters hard VCs that existing Automated Theorem Provers (ATPs) cannot prove, leading to a critical need for extensive manual proofs that burden practical application. While Neural Theorem Proving (NTP) has achieved significant success in mathematical competitions, demonstrating the potential of machine learning approaches to formal reasoning, its application to program verification--particularly VC proving--remains largely unexplored. Despite existing work on annotation synthesis and verification-related theorem proving, no benchmark has specifically targeted this fundamental bottleneck: automated VC proving. This work introduces Neural Theorem Proving for Verification Conditions (NTP4VC), presenting the first real-world multi-language benchmark for this task. From real-world projects such as Linux and Contiki-OS kernel, our benchmark leverages industrial pipelines (Why3 and Frama-C) to generate semantically equivalent test cases across formal languages of Isabelle, Lean, and Rocq. We evaluate large language models (LLMs), both general-purpose and those fine-tuned for theorem proving, on NTP4VC. Results indicate that although LLMs show promise in VC proving, significant challenges remain for program verification, highlighting a large gap and opportunity for future research.

</details>


### [5] [RIFT: Reordered Instruction Following Testbed To Evaluate Instruction Following in Singular Multistep Prompt Structures](https://arxiv.org/abs/2601.18924)
*Andrew Jaffe,Noah Reicin,Jinho D. Choi*

Main category: cs.AI

TL;DR: The paper introduces RIFT, a benchmark to study how well large language models (LLMs) maintain instruction flow under different structures, revealing significant performance drops in non-linear situations.


<details>
  <summary>Details</summary>
Motivation: To investigate the underexplored ability of LLMs to follow instructions with varying structural ordering, independently of task complexity.

Method: The researchers developed RIFT using rephrased Jeopardy! question-answer pairs to evaluate LLMs on linear versus jumping (non-sequential) prompt structures.

Result: Across 10,000 tests over six state-of-the-art open-source LLMs, accuracy under jumping prompts dropped by up to 72%, revealing LLMs' strong dependence on positional continuity.

Conclusion: Current LLM architectures are limited by a reliance on sequential patterns rather than reasoning skills, impacting their effectiveness in workflows requiring non-linear control flows.

Abstract: Large Language Models (LLMs) are increasingly relied upon for complex workflows, yet their ability to maintain flow of instructions remains underexplored. Existing benchmarks conflate task complexity with structural ordering, making it difficult to isolate the impact of prompt topology on performance. We introduce RIFT, Reordered Instruction Following Testbed, to assess instruction following by disentangling structure from content. Using rephrased Jeopardy! question-answer pairs, we test LLMs across two prompt structures: linear prompts, which progress sequentially, and jumping prompts, which preserve identical content but require non-sequential traversal. Across 10,000 evaluations spanning six state-of-the-art open-source LLMs, accuracy dropped by up to 72% under jumping conditions (compared to baseline), revealing a strong dependence on positional continuity. Error analysis shows that approximately 50% of failures stem from instruction-order violations and semantic drift, indicating that current architectures internalize instruction following as a sequential pattern rather than a reasoning skill. These results reveal structural sensitivity as a fundamental limitation in current architectures, with direct implications for applications requiring non-sequential control flow such as workflow automation and multi-agent systems.

</details>


### [6] [More at Stake: How Payoff and Language Shape LLM Agent Strategies in Cooperation Dilemmas](https://arxiv.org/abs/2601.19082)
*Trung-Kiet Huynh,Dao-Sy Duy-Minh,Thanh-Bang Cao,Phong-Hao Le,Hong-Dan Nguyen,Nguyen Lam Phu Quy,Minh-Luan Nguyen-Vo,Hong-Phat Pham,Pham Phu Hoa,Thien-Kim Than,Chi-Nguyen Tran,Huy Tran,Gia-Thoai Tran-Le,Alessio Buscemi,Le Hong Trang,The Anh Han*

Main category: cs.AI

TL;DR: The paper analyzes how language models (LLMs) behave in social dilemmas by testing their strategies in repeated payoff-scaled Prisoner's Dilemma games, revealing patterns influenced by incentives, language, and linguistic framing.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the strategic behavior of LLMs, given their growing role as autonomous agents, for safety, effective coordination, and for managing AI-driven systems.

Method: The study uses a modified Prisoner's Dilemma setting, incorporating varying payoff magnitudes and linguistic contexts. It trains supervised classifiers on benchmark strategies to evaluate LLM behavior.

Result: Results show that LLMs exhibit sensitivity to incentives, conditionality in strategy, and behavioral variations across languages, with linguistic framing having as much or more impact than model architecture in some cases.

Conclusion: The findings suggest the need for robust frameworks to audit LLMs in strategic settings and provide insights for AI governance, emphasizing cooperation biases and their role in multi-agent system design.

Abstract: As LLMs increasingly act as autonomous agents in interactive and multi-agent settings, understanding their strategic behavior is critical for safety, coordination, and AI-driven social and economic systems. We investigate how payoff magnitude and linguistic context shape LLM strategies in repeated social dilemmas, using a payoff-scaled Prisoner's Dilemma to isolate sensitivity to incentive strength. Across models and languages, we observe consistent behavioral patterns, including incentive-sensitive conditional strategies and cross-linguistic divergence. To interpret these dynamics, we train supervised classifiers on canonical repeated-game strategies and apply them to LLM decisions, revealing systematic, model- and language-dependent behavioral intentions, with linguistic framing sometimes matching or exceeding architectural effects. Our results provide a unified framework for auditing LLMs as strategic agents and highlight cooperation biases with direct implications for AI governance and multi-agent system design.

</details>


### [7] [Uncertainty-Aware 3D Emotional Talking Face Synthesis with Emotion Prior Distillation](https://arxiv.org/abs/2601.19112)
*Nanhan Shen,Zhilei Liu*

Main category: cs.AI

TL;DR: UA-3DTalk addresses limitations in 3D emotional talking face synthesis with better audio-vision emotion alignment, improved emotional micro-expression control, and adaptive multi-view fusion strategies through uncertainty modeling.


<details>
  <summary>Details</summary>
Motivation: Existing methods face challenges in emotion alignment, micro-expression control, and handling uncertainty in multi-view fusion strategies.

Method: UA-3DTalk introduces three modules: Prior Extraction for audio feature disentanglement, Emotion Distillation with multi-modal fusion and 4D Gaussian encoding, and Uncertainty-based Deformation for adaptive fusion using uncertainty blocks.

Result: UA-3DTalk outperforms state-of-the-art methods like DEGSTalk and EDTalk in emotion alignment (E-FID by 5.2%), lip synchronization (SyncC by 3.1%), and rendering quality (LPIPS improvement of 0.015).

Conclusion: UA-3DTalk is a significant advancement in 3D emotional talking face synthesis, enhancing alignment, expression control, and rendering quality through innovative methodologies.

Abstract: Emotional Talking Face synthesis is pivotal in multimedia and signal processing, yet existing 3D methods suffer from two critical challenges: poor audio-vision emotion alignment, manifested as difficult audio emotion extraction and inadequate control over emotional micro-expressions; and a one-size-fits-all multi-view fusion strategy that overlooks uncertainty and feature quality differences, undermining rendering quality. We propose UA-3DTalk, Uncertainty-Aware 3D Emotional Talking Face Synthesis with emotion prior distillation, which has three core modules: the Prior Extraction module disentangles audio into content-synchronized features for alignment and person-specific complementary features for individualization; the Emotion Distillation module introduces a multi-modal attention-weighted fusion mechanism and 4D Gaussian encoding with multi-resolution code-books, enabling fine-grained audio emotion extraction and precise control of emotional micro-expressions; the Uncertainty-based Deformation deploys uncertainty blocks to estimate view-specific aleatoric (input noise) and epistemic (model parameters) uncertainty, realizing adaptive multi-view fusion and incorporating a multi-head decoder for Gaussian primitive optimization to mitigate the limitations of uniform-weight fusion. Extensive experiments on regular and emotional datasets show UA-3DTalk outperforms state-of-the-art methods like DEGSTalk and EDTalk by 5.2% in E-FID for emotion alignment, 3.1% in SyncC for lip synchronization, and 0.015 in LPIPS for rendering quality. Project page: https://mrask999.github.io/UA-3DTalk

</details>


### [8] [Exploring Weaknesses in Function Call Models via Reinforcement Learning: An Adversarial Data Augmentation Approach](https://arxiv.org/abs/2601.19122)
*Weiran Guo,Bing Bo,Shaoxiang Wu,Jingsheng Yang*

Main category: cs.AI

TL;DR: The paper introduces an adversarial data augmentation method using reinforcement learning to improve the function call capabilities and robustness of Large Language Models (LLMs).


<details>
  <summary>Details</summary>
Motivation: Existing methods lack targeted design and are constrained by fixed patterns, limiting the enhancement of LLMs' function call capabilities.

Method: The paper proposes an adversarial data augmentation method where a query model trained with reinforcement learning generates challenging queries, and both query and function call models are trained in a zero-sum game framework.

Result: The method identifies and addresses weaknesses, making function call models more robust and capable.

Conclusion: The proposed approach systematically improves the robustness of function call models by addressing their limitations and enhancing their interaction with external tools.

Abstract: Function call capabilities have become crucial for Large Language Models (LLMs), enabling them to interact more effectively with external tools and APIs. Existing methods for improving the function call capabilities of LLMs rely on data obtained either through manual annotation or automated generation by models, and use this data to finetune the LLMs. However, these methods often lack targeted design and are constrained by fixed patterns and data distributions, which limits their effectiveness in enhancing the generalization and robustness of function call LLMs. To address this limitation, we propose a novel adversarial data augmentation method that employs reinforcement learning to systematically identify and target the weaknesses of function call LLMs. Our training framework introduces a query model trained with reinforcement learning (RL) to generate adversarial queries that are specifically designed to challenge function call (FC) models. This approach adopts a zero sum game formulation, where the query model and the FC model engage in iterative alternating training. Overall, our method advances the development of more robust FC models and provides a systematic way to identify and correct weaknesses in the ability of LLMs to interact with external tools.

</details>


### [9] [Length-Adaptive Interest Network for Balancing Long and Short Sequence Modeling in CTR Prediction](https://arxiv.org/abs/2601.19142)
*Zhicheng Zhang,Zhaocheng Du,Jieming Zhu,Jiwei Tang,Fengyuan Lu,Wang Jiaheng,Song-Li Wu,Qianhui Zhu,Jingyu Li,Hai-Tao Zheng,Zhenhua Dong*

Main category: cs.AI

TL;DR: The paper addresses bias in recommendation systems caused by behavior sequence length heterogeneity using LAIN, a framework improving performance across short- and long-sequence users.


<details>
  <summary>Details</summary>
Motivation: Existing recommendation systems struggle to balance modeling for user interactions of varying sequence lengths, with long sequences offering more context and short ones suffering from degraded performance due to attention polarization and length imbalance in training data.

Method: LAIN introduces three components: a Spectral Length Encoder for length representation, Length-Conditioned Prompting for contextual cues adjustment, and Length-Modulated Attention to tune attention sharpness according to sequence length.

Result: The LAIN framework shows consistent performance improvement with up to 1.15% AUC gain and 2.25% log loss reduction across multiple benchmarks and models, enhancing accuracy for short-sequence users without compromising long-sequence effectiveness.

Conclusion: LAIN provides an efficient and deployable solution to address length-induced bias in sequential recommendation systems.

Abstract: User behavior sequences in modern recommendation systems exhibit significant length heterogeneity, ranging from sparse short-term interactions to rich long-term histories. While longer sequences provide more context, we observe that increasing the maximum input sequence length in existing CTR models paradoxically degrades performance for short-sequence users due to attention polarization and length imbalance in training data. To address this, we propose LAIN(Length-Adaptive Interest Network), a plug-and-play framework that explicitly incorporates sequence length as a conditioning signal to balance long- and short-sequence modeling. LAIN consists of three lightweight components: a Spectral Length Encoder that maps length into continuous representations, Length-Conditioned Prompting that injects global contextual cues into both long- and short-term behavior branches, and Length-Modulated Attention that adaptively adjusts attention sharpness based on sequence length. Extensive experiments on three real-world benchmarks across five strong CTR backbones show that LAIN consistently improves overall performance, achieving up to 1.15% AUC gain and 2.25% log loss reduction. Notably, our method significantly improves accuracy for short-sequence users without sacrificing longsequence effectiveness. Our work offers a general, efficient, and deployable solution to mitigate length-induced bias in sequential recommendation.

</details>


### [10] [TS-Debate: Multimodal Collaborative Debate for Zero-Shot Time Series Reasoning](https://arxiv.org/abs/2601.19151)
*Patara Trirat,Jin Myung Kwak,Jay Heo,Heejun Lee,Sung Ju Hwang*

Main category: cs.AI

TL;DR: The paper introduces TS-Debate, an innovative multi-agent framework specializing in time series reasoning by assigning expert agents to specific modalities and enhancing reasoning via a structured debate protocol.


<details>
  <summary>Details</summary>
Motivation: To address the challenges LLMs face in time series reasoning, such as numeric fidelity, modality interference, and integrated multimodal reasoning.

Method: TS-Debate employs specialized expert agents for textual, visual, and numerical signals, utilizing a structured debate protocol, verification mechanisms, and domain elicitation to improve zero-shot reasoning.

Result: TS-Debate outperforms competitive baselines across diverse benchmark tasks, demonstrating superior fidelity and integration in multimodal reasoning.

Conclusion: The approach mitigates key weaknesses in LLM-based time series analysis by ensuring modality specialization and addressing numeric hallucinations without requiring task-specific fine-tuning.

Abstract: Recent progress at the intersection of large language models (LLMs) and time series (TS) analysis has revealed both promise and fragility. While LLMs can reason over temporal structure given carefully engineered context, they often struggle with numeric fidelity, modality interference, and principled cross-modal integration. We present TS-Debate, a modality-specialized, collaborative multi-agent debate framework for zero-shot time series reasoning. TS-Debate assigns dedicated expert agents to textual context, visual patterns, and numerical signals, preceded by explicit domain knowledge elicitation, and coordinates their interaction via a structured debate protocol. Reviewer agents evaluate agent claims using a verification-conflict-calibration mechanism, supported by lightweight code execution and numerical lookup for programmatic verification. This architecture preserves modality fidelity, exposes conflicting evidence, and mitigates numeric hallucinations without task-specific fine-tuning. Across 20 tasks spanning three public benchmarks, TS-Debate achieves consistent and significant performance improvements over strong baselines, including standard multimodal debate in which all agents observe all inputs.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [11] [M$^{\text{2}}$XFP: A Metadata-Augmented Microscaling Data Format for Efficient Low-bit Quantization](https://arxiv.org/abs/2601.19213)
*Weiming Hu,Zihan Zhang,Haoyan Zhang,Chen Zhang,Cong Guo,Yu Feng,Tianchi Hu,Guanglin Li,Guipeng Hu,Junsong Wang,Jingwen Leng*

Main category: cs.AR

TL;DR: The paper presents a co-designed algorithm and hardware solution that improves the accuracy and efficiency of low-bit Microscaling (MX) formats for large language models.


<details>
  <summary>Details</summary>
Motivation: To address the accuracy degradation in existing low-bit Microscaling formats like MXFP4 caused by shared scaling factors and ensure efficiency in large language models.

Method: Proposed flexible metadata-based algorithm combined with an online quantization and simple encoding method, supported by a lightweight hardware unit integrated into accelerators.

Result: The method reduces accuracy loss by 70.63% relative to MXFP4 and 37.30% compared to NVFP4, enhances speed by up to 1.91x, and achieves 1.75x energy savings on large language model benchmarks.

Conclusion: The co-design successfully recovers accuracy while maintaining high bit efficiency, providing significant improvements in processing speed and energy efficiency for large language models.

Abstract: Existing low-bit Microscaling (MX) formats, such as MXFP4, often suffer from substantial accuracy degradation due to the use of a shared scaling factor with the Power-of-Two format. In this work, we explore strategies that introduce minimal metadata to recover accuracy lost during quantization while maintaining high bit efficiency across a wide range of large language models. We propose a complete algorithm-hardware co-design based on flexible metadata, featuring an online quantization with simple encoding. To support the proposed method efficiently, we implement a lightweight hardware unit and integrate it into the accelerator. Evaluation results demonstrate that our method substantially narrows the accuracy gap, achieving on average a 70.63% reduction in accuracy loss compared to MXFP4 and a 37.30% reduction relative to the latest NVFP4 on LLM benchmarks. Furthermore, our design delivers up to 1.91$\times$ speedup and 1.75$\times$ energy savings over state-of-the-art accelerators. Our code is available at https://github.com/SJTU-ReArch-Group/M2XFP_ASPLOS26.

</details>


### [12] [A Reconfigurable Framework for AI-FPGA Agent Integration and Acceleration](https://arxiv.org/abs/2601.19263)
*Aybars Yunusoglu,Talha Coskun,Hiruna Vishwamith,Murat Isik,I. Can Dikmen*

Main category: cs.AR

TL;DR: The paper introduces AI FPGA Agent, a framework that facilitates integrating and accelerating AI inference on FPGAs, achieving substantial latency and energy efficiency improvements.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address inefficiencies in CPUs and GPUs for real-time AI tasks and to leverage FPGAs for better performance and energy efficiency, despite the challenges of complex hardware-software integration.

Method: The authors propose the AI FPGA Agent framework, which includes a runtime software agent for dynamic AI model partitioning and a hardware accelerator core optimized for quantized arithmetic, automating the hardware-software co-design process.

Result: The framework achieved over 10x latency reduction compared to CPUs, 2-3x higher energy efficiency than GPUs, and maintained classification accuracy with a deviation of less than 0.2%.

Conclusion: AI FPGA Agent demonstrates significant potential for scalable and energy-efficient AI deployment by simplifying the FPGA implementation challenges and optimizing hardware performance for real-time environments.

Abstract: Artificial intelligence (AI) is increasingly deployed in real-time and energy-constrained environments, driving demand for hardware platforms that can deliver high performance and power efficiency. While central processing units (CPUs) and graphics processing units (GPUs) have traditionally served as the primary inference engines, their general-purpose nature often leads to inefficiencies under strict latency or power budgets. Field-Programmable Gate Arrays (FPGAs) offer a promising alternative by enabling custom-tailored parallelism and hardware-level optimizations. However, mapping AI workloads to FPGAs remains challenging due to the complexity of hardware-software co-design and data orchestration. This paper presents AI FPGA Agent, an agent-driven framework that simplifies the integration and acceleration of deep neural network inference on FPGAs. The proposed system employs a runtime software agent that dynamically partitions AI models, schedules compute-intensive layers for hardware offload, and manages data transfers with minimal developer intervention. The hardware component includes a parameterizable accelerator core optimized for high-throughput inference using quantized arithmetic. Experimental results demonstrate that the AI FPGA Agent achieves over 10x latency reduction compared to CPU baselines and 2-3x higher energy efficiency than GPU implementations, all while preserving classification accuracy within 0.2% of full-precision references. These findings underscore the potential of AI-FPGA co-design for scalable, energy-efficient AI deployment.

</details>


### [13] [GenPairX: A Hardware-Algorithm Co-Designed Accelerator for Paired-End Read Mapping](https://arxiv.org/abs/2601.19384)
*Julien Eudine,Chu Li,Zhuo Cheng,Renzo Andri,Can Firtina,Mohammad Sadrosadati,Nika Mansouri Ghiasi,Konstantina Koliogeorgi,Anirban Nag,Arash Tavakkol,Haiyu Mao,Onur Mutlu,Shai Bergman,Ji Zhang*

Main category: cs.AR

TL;DR: This paper proposes GenPairX, a hardware-algorithm co-designed accelerator that significantly improves paired-end read mapping by combining advanced filtering and lightweight alignment techniques.


<details>
  <summary>Details</summary>
Motivation: Read mapping for genome sequencing is computationally expensive, especially for paired-end reads, which require higher accuracy and advanced analysis. Prior approaches like filters and hardware accelerators are limited in effectiveness and fail to sufficiently enhance paired-end read mapping.

Method: The paper introduces GenPairX, which leverages a new filtering algorithm that jointly evaluates read pairs, a lightweight alignment algorithm replacing dynamic programming, and specialized hardware mechanisms to enhance efficiency.

Result: GenPairX achieves up to 1575x higher throughput per watt compared to CPUs and 1.43x compared to existing accelerators, with no loss in accuracy.

Conclusion: GenPairX significantly reduces the computational bottlenecks in paired-end read mapping, improving performance and throughput without sacrificing accuracy, and offers a promising advancement in genome sequencing technology.

Abstract: Genome sequencing has become a central focus in computational biology. A genome study typically begins with sequencing, which produces millions to billions of short DNA fragments known as reads. Read mapping aligns these reads to a reference genome. Read mapping for short reads comes in two forms: single-end and paired-end, with the latter being more prevalent due to its higher accuracy and support for advanced analysis. Read mapping remains a major performance bottleneck in genome analysis due to expensive dynamic programming. Prior efforts have attempted to mitigate this cost by employing filters to identify and potentially discard computationally expensive matches and leveraging hardware accelerators to speed up the computations. While partially effective, these approaches have limitations. In particular, existing filters are often ineffective for paired-end reads, as they evaluate each read independently and exhibit relatively low filtering ratios. In this work, we propose GenPairX, a hardware-algorithm co-designed accelerator that efficiently minimizes the computational load of paired-end read mapping while enhancing the throughput of memory-intensive operations. GenPairX introduces: (1) a novel filtering algorithm that jointly considers both reads in a pair to improve filtering effectiveness, and a lightweight alignment algorithm to replace most of the computationally expensive dynamic programming operations, and (2) two specialized hardware mechanisms to support the proposed algorithms. Our evaluations show that GenPairX delivers substantial performance improvements over state-of-the-art solutions, achieving 1575x and 1.43x higher throughput per watt compared to leading CPU-based and accelerator-based read mappers, respectively, all without compromising accuracy.

</details>


### [14] [Veri-Sure: A Contract-Aware Multi-Agent Framework with Temporal Tracing and Formal Verification for Correct RTL Code Generation](https://arxiv.org/abs/2601.19747)
*Jiale Liu,Taiyu Zhou,Tianqi Jiang*

Main category: cs.AR

TL;DR: The paper presents Veri-Sure, a multi-agent framework designed to enhance Register-Transfer Level (RTL) design accuracy using robust verification techniques, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in silicon-grade correctness for RTL designs, including limited test coverage, debugging-induced errors, and intent misalignment across agents.

Method: Veri-Sure uses a design contract to unify agent intent and applies a static dependency slicing-based patching mechanism for precise repairs. It incorporates a multi-branch verification pipeline with temporal analysis and formal verification.

Result: The proposed framework achieves superior performance in RTL code generation compared to standalone LLMs and earlier agentic systems, as demonstrated on the extended VerilogEval-v2-EXT benchmark.

Conclusion: Veri-Sure significantly improves functional correctness in RTL code generation using a novel multi-agent approach and rigorous verification methods.

Abstract: In the rapidly evolving field of Electronic Design Automation (EDA), the deployment of Large Language Models (LLMs) for Register-Transfer Level (RTL) design has emerged as a promising direction. However, silicon-grade correctness remains bottlenecked by: (i) limited test coverage and reliability of simulation-centric evaluation, (ii) regressions and repair hallucinations introduced by iterative debugging, and (iii) semantic drift as intent is reinterpreted across agent handoffs. In this work, we propose Veri-Sure, a multi-agent framework that establishes a design contract to align agents' intent and uses a patching mechanism guided by static dependency slicing to perform precise, localized repairs. By integrating a multi-branch verification pipeline that combines trace-driven temporal analysis with formal verification consisting of assertion-based checking and boolean equivalence proofs, Veri-Sure enables functional correctness beyond pure simulations. We also introduce VerilogEval-v2-EXT, extending the original benchmark with 53 more industrial-grade design tasks and stratified difficulty levels, and show that Veri-Sure achieves state-of-the-art verified-correct RTL code generation performance, surpassing standalone LLMs and prior agentic systems.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [15] [Language Family Matters: Evaluating LLM-Based ASR Across Linguistic Boundaries](https://arxiv.org/abs/2601.18899)
*Yuchen Zhang,Ravi Shekhar,Haralambos Mouratidis*

Main category: cs.CL

TL;DR: The paper proposes a novel connector-sharing strategy for multilingual Automatic Speech Recognition (ASR) systems using language family memberships, showing improved efficiency and generalization.


<details>
  <summary>Details</summary>
Motivation: ASR systems powered by Large Language Models (LLMs) typically train separate connectors for each language, ignoring linguistic similarities. This approach is resource-intensive and inefficient.

Method: This study introduces a connector-sharing strategy based on membership in linguistic families. Instead of separate connectors, one is shared across a family. The method was tested on two multilingual LLMs and real-world corpora.

Result: The results indicate that the proposed connectors reduce parameter counts, enhance cross-domain generalization, and provide better resource efficiency.

Conclusion: Family-based connectors offer a scalable, efficient solution for multilingual ASR deployment while maintaining strong performance.

Abstract: Large Language Model (LLM)-powered Automatic Speech Recognition (ASR) systems achieve strong performance with limited resources by linking a frozen speech encoder to a pretrained LLM via a lightweight connector. Prior work trains a separate connector per language, overlooking linguistic relatedness. We propose an efficient and novel connector-sharing strategy based on linguistic family membership, enabling one connector per family, and empirically validate its effectiveness across two multilingual LLMs and two real-world corpora spanning curated and crowd-sourced speech. Our results show that family-based connectors reduce parameter count while improving generalization across domains, offering a practical and scalable strategy for multilingual ASR deployment.

</details>


### [16] [Self-Aware Knowledge Probing: Evaluating Language Models' Relational Knowledge through Confidence Calibration](https://arxiv.org/abs/2601.18901)
*Christopher Kissling,Elena Merdjanovska,Alan Akbik*

Main category: cs.CL

TL;DR: The paper introduces a new framework to evaluate language model reliability in relational knowledge by looking at their confidence in three dimensions: intrinsic confidence, structural consistency, and semantic grounding. It finds most models to be overconfident, especially those with a masking objective.


<details>
  <summary>Details</summary>
Motivation: Existing methods to probe language models' knowledge focus on metrics like accuracy, neglecting how reliably their confidence scores are calibrated. These gaps in evaluation motivate the study.

Method: A novel calibration probing framework is proposed, evaluating language models across three dimensions of confidence: intrinsic confidence, structural consistency, and semantic grounding. It is applied to ten causal and six masked language models.

Result: Findings show that models tend to be overconfident, particularly those trained with a masking objective. Best calibration is observed when inconsistency due to rephrasing is considered. However, models fail to represent linguistic confidence semantics well.

Conclusion: The study highlights significant overconfidence in language models and the inadequacy of their semantic confidence representations, calling for better calibration approaches in model development.

Abstract: Knowledge probing quantifies how much relational knowledge a language model (LM) has acquired during pre-training. Existing knowledge probes evaluate model capabilities through metrics like prediction accuracy and precision. Such evaluations fail to account for the model's reliability, reflected in the calibration of its confidence scores. In this paper, we propose a novel calibration probing framework for relational knowledge, covering three modalities of model confidence: (1) intrinsic confidence, (2) structural consistency and (3) semantic grounding. Our extensive analysis of ten causal and six masked language models reveals that most models, especially those pre-trained with the masking objective, are overconfident. The best-calibrated scores come from confidence estimates that account for inconsistencies due to statement rephrasing. Moreover, even the largest pre-trained models fail to encode the semantics of linguistic confidence expressions accurately.

</details>


### [17] [Flatter Tokens are More Valuable for Speculative Draft Model Training](https://arxiv.org/abs/2601.18902)
*Jiaming Fan,Daming Cao,Xiangzhong Luo,Jiale Fu,Chonghan Liu,Xu Yang*

Main category: cs.CL

TL;DR: The paper proposes a data-centric approach to improve training efficiency for Speculative Decoding by filtering training data based on a flatness metric.


<details>
  <summary>Details</summary>
Motivation: To enhance Speculative Decoding efficiency while reducing the reliance on large datasets for training draft models.

Method: Introduce 'flatness' metric to evaluate token significance and implement Sample-level-flatness-based Dataset Distillation (SFDD) to optimize training data before training draft models.

Result: SFDD achieves over 2× training speedup using half the data, with inference speedup remaining close to the full-dataset performance baseline.

Conclusion: The study presents an effective data-centric strategy to improve Speculative Decoding training efficiency without significant performance trade-offs.

Abstract: Speculative Decoding (SD) is a key technique for accelerating Large Language Model (LLM) inference, but it typically requires training a draft model on a large dataset. We approach this problem from a data-centric perspective, finding that not all training samples contribute equally to the SD acceptance rate. Specifically, our theoretical analysis and empirical validation reveals that tokens inducing flatter predictive distributions from the target model are more valuable than those yielding sharply peaked distributions. Based on this insight, we propose flatness, a new metric to quantify this property, and develop the Sample-level-flatness-based Dataset Distillation (SFDD) approach, which filters the training data to retain only the most valuable samples. Experiments on the EAGLE framework demonstrate that SFDD can achieve over 2$\times$ training speedup using only 50% of the data, while keeping the final model's inference speedup within 4% of the full-dataset baseline. This work introduces an effective, data-centric approach that substantially improves the training efficiency for Speculative Decoding. Our code is available at https://anonymous.4open.science/r/Flatness.

</details>


### [18] [LLMs versus the Halting Problem: Revisiting Program Termination Prediction](https://arxiv.org/abs/2601.18987)
*Oren Sultan,Jordi Armengol-Estape,Pascal Kesseli,Julien Vanegue,Dafna Shahaf,Yossi Adi,Peter O'Hearn*

Main category: cs.CL

TL;DR: The paper studies the ability of LLMs to predict program termination, showing promising results but with limitations like performance drop for longer programs and lack of proof validation.


<details>
  <summary>Details</summary>
Motivation: The authors aim to explore whether modern large language models can effectively predict program termination, a traditionally undecidable problem addressed via approximations in automatic verification tools.

Method: The authors evaluate several LLMs on a collection of C programs from the SV-Comp 2025 Termination category and compare their performance against existing verification tools.

Result: LLMs like GPT-5 and Claude Sonnet-4.5 show high performance, ranking just below top verification tools; however, they struggle with providing valid proofs and longer program scenarios.

Conclusion: LLMs show promise in predicting program termination, but their limitations highlight the need for further research in this domain and their broader applications to undecidable problems.

Abstract: Determining whether a program terminates is a central problem in computer science. Turing's foundational result established the Halting Problem as undecidable, showing that no algorithm can universally determine termination for all programs and inputs. Consequently, automatic verification tools approximate termination, sometimes failing to prove or disprove; these tools rely on problem-specific architectures and abstractions, and are usually tied to particular programming languages. Recent success and progress in large language models (LLMs) raises the following question: can LLMs reliably predict program termination? In this work, we evaluate LLMs on a diverse set of C programs from the Termination category of the International Competition on Software Verification (SV-Comp) 2025. Our results suggest that LLMs perform remarkably well at predicting program termination, where GPT-5 and Claude Sonnet-4.5 would rank just behind the top-ranked tool (using test-time-scaling), and Code World Model (CWM) would place just behind the second-ranked tool. While LLMs are effective at predicting program termination, they often fail to provide a valid witness as a proof. Moreover, LLMs performance drops as program length increases. We hope these insights motivate further research into program termination and the broader potential of LLMs for reasoning about undecidable problems.

</details>


### [19] [BabyReasoningBench: Generating Developmentally-Inspired Reasoning Tasks for Evaluating Baby Language Models](https://arxiv.org/abs/2601.18933)
*Kaustubh D. Dhole*

Main category: cs.CL

TL;DR: This paper introduces BabyReasoningBench, a benchmark of 19 reasoning tasks based on developmental psychology paradigms to test reasoning abilities in baby language models trained on child-directed speech.


<details>
  <summary>Details</summary>
Motivation: The motivation is to analyze reasoning abilities of language models trained on child-like data, as traditional benchmarks assume adult-like capabilities, which are unsuitable for these models.

Method: The authors designed BabyReasoningBench, covering reasoning tasks like theory of mind and causal inference, and tested GPT-2 based baby models trained on child-directed texts.

Result: Baby language models exhibited low but varied performance in reasoning tasks, showing progress in some areas like causal reasoning while struggling with belief attribution and pragmatics.

Conclusion: A developmental psychology-inspired benchmark like BabyReasoningBench sheds light on the reasoning limitations and capabilities under child-like training conditions.

Abstract: Traditional evaluations of reasoning capabilities of language models are dominated by adult-centric benchmarks that presuppose broad world knowledge, complex instruction following, and mature pragmatic competence. These assumptions are mismatched to baby language models trained on developmentally plausible input such as child-directed speech and early-childhood narratives, and they obscure which reasoning abilities (if any) emerge under such constraints. We introduce BabyReasoningBench, a GPT-5.2 generated benchmark of 19 reasoning tasks grounded in classic paradigms from developmental psychology, spanning theory of mind, analogical and relational reasoning, causal inference and intervention selection, and core reasoning primitives that are known to be confounded by memory and pragmatics. We find that two GPT-2 based baby language models (pretrained on 10M and 100M of child-directed speech text) show overall low but uneven performance, with dissociations across task families: scaling improves several causal and physical reasoning tasks, while belief attribution and pragmatics-sensitive tasks remain challenging. BabyReasoningBench provides a developmentally grounded lens for analyzing what kinds of reasoning are supported by child-like training distributions, and for testing mechanistic hypotheses about how such abilities emerge.

</details>


### [20] [Malicious Repurposing of Open Science Artefacts by Using Large Language Models](https://arxiv.org/abs/2601.18998)
*Zahra Hashemi,Zhiqiang Zhong,Jun Pang,Wei Zhao*

Main category: cs.CL

TL;DR: The paper analyzes the potential misuse of large language models (LLMs) by introducing a pipeline to identify vulnerabilities in open science artefacts, demonstrating their capacity to generate harmful research proposals.


<details>
  <summary>Details</summary>
Motivation: With rapid advancements in LLMs, there is a need to address how these models might exploit open science artefacts for malicious research purposes.

Method: The study introduces a pipeline that bypasses LLM safeguards and repurposes open science artefacts to evaluate their vulnerabilities, using an assessment framework covering harmfulness, feasibility, and technical soundness.

Result: Their findings reveal that LLMs can produce harmful proposals using open science artefacts, but they demonstrate inconsistencies in evaluating such outcomes across different LLMs.

Conclusion: LLMs are currently unreliable at evaluating malicious proposals, necessitating human assessment for robust dual-use risk evaluation.

Abstract: The rapid evolution of large language models (LLMs) has fuelled enthusiasm about their role in advancing scientific discovery, with studies exploring LLMs that autonomously generate and evaluate novel research ideas. However, little attention has been given to the possibility that such models could be exploited to produce harmful research by repurposing open science artefacts for malicious ends. We fill the gap by introducing an end-to-end pipeline that first bypasses LLM safeguards through persuasion-based jailbreaking, then reinterprets NLP papers to identify and repurpose their artefacts (datasets, methods, and tools) by exploiting their vulnerabilities, and finally assesses the safety of these proposals using our evaluation framework across three dimensions: harmfulness, feasibility of misuse, and soundness of technicality. Overall, our findings demonstrate that LLMs can generate harmful proposals by repurposing ethically designed open artefacts; however, we find that LLMs acting as evaluators strongly disagree with one another on evaluation outcomes: GPT-4.1 assigns higher scores (indicating greater potential harms, higher soundness and feasibility of misuse), Gemini-2.5-pro is markedly stricter, and Grok-3 falls between these extremes. This indicates that LLMs cannot yet serve as reliable judges in a malicious evaluation setup, making human evaluation essential for credible dual-use risk assessment.

</details>


### [21] [FROST: Filtering Reasoning Outliers with Attention for Efficient Reasoning](https://arxiv.org/abs/2601.19001)
*Haozheng Luo,Zhuolin Jiang,Md Zahid Hasan,Yan Chen,Soumalya Sarkar*

Main category: cs.CL

TL;DR: FROST introduces an efficient attention-aware reasoning approach that prunes uncritical reasoning paths, enhancing accuracy and reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Current reasoning models often include uncritical paths, causing inefficiencies and inaccuracies in decision-making. This paper aims to streamline reasoning models by leveraging attention mechanisms to eliminate irrelevant reasoning outliers.

Method: The authors propose FROST, an attention-based mechanism, which identifies and prunes reasoning outliers using attention weights. The framework ensures relevant reasoning paths are preserved by eliminating outliers at the sentence level.

Result: FROST achieves a 69.68% reduction in token usage and a 26.70% improvement in accuracy over the base model, along with notable improvements in attention outlier metrics (15.97% reduction in infinity norm and 91.09% reduction in average kurtosis).

Conclusion: FROST significantly enhances reasoning efficiency and accuracy while lowering computational costs by optimizing attention mechanisms to prune extraneous reasoning paths.

Abstract: We propose FROST, an attention-aware method for efficient reasoning. Unlike traditional approaches, FROST leverages attention weights to prune uncritical reasoning paths, yielding shorter and more reliable reasoning trajectories. Methodologically, we introduce the concept of reasoning outliers and design an attention-based mechanism to remove them. Theoretically, FROST preserves and enhances the model's reasoning capacity while eliminating outliers at the sentence level. Empirically, we validate FROST on four benchmarks using two strong reasoning models (Phi-4-Reasoning and GPT-OSS-20B), outperforming state-of-the-art methods such as TALE and ThinkLess. Notably, FROST achieves an average 69.68% reduction in token usage and a 26.70% improvement in accuracy over the base model. Furthermore, in evaluations of attention outlier metrics, FROST reduces the maximum infinity norm by 15.97% and the average kurtosis by 91.09% compared to the base model. Code is available at https://github.com/robinzixuan/FROST

</details>


### [22] [Optimizing Conversational Quality in Spoken Dialogue Systems with Reinforcement Learning from AI Feedback](https://arxiv.org/abs/2601.19063)
*Siddhant Arora,Jinchuan Tian,Jiatong Shi,Hayato Futami,Yosuke Kashiwagi,Emiru Tsunoo,Shinji Watanabe*

Main category: cs.CL

TL;DR: This paper introduces a multi-reward reinforcement learning framework for speech-in/speech-out systems that improves semantic, audio, and emotion quality in duplex dialogue models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of multi-dimensional conversational quality rewards in existing reinforcement learning systems for spoken dialogue, particularly in duplex systems that generate responses incrementally.

Method: The paper develops a multi-reward framework that integrates semantic, audio-quality, and emotion-consistency rewards, applying turn-level preference sampling and blockwise decoding to align with duplex models.

Result: Experiments demonstrate that single-reward RL improves specific metrics, while multi-reward training consistently enhances both semantic quality and audio naturalness.

Conclusion: The study underscores the necessity of multi-reward frameworks for improving conversational quality in practical speech dialogue systems, and releases a dataset to advance reproducible research.

Abstract: Reinforcement learning from human or AI feedback (RLHF/RLAIF) for speech-in/speech-out dialogue systems (SDS) remains underexplored, with prior work largely limited to single semantic rewards applied at the utterance level. Such setups overlook the multi-dimensional and multi-modal nature of conversational quality, which encompasses semantic coherence, audio naturalness, speaker consistency, emotion alignment, and turn-taking behavior. Moreover, they are fundamentally mismatched with duplex spoken dialogue systems that generate responses incrementally, where agents must make decisions based on partial utterances. We address these limitations with the first multi-reward RLAIF framework for SDS, combining semantic, audio-quality, and emotion-consistency rewards. To align utterance-level preferences with incremental, blockwise decoding in duplex models, we apply turn-level preference sampling and aggregate per-block log-probabilities within a single DPO objective. We present the first systematic study of preference learning for improving SDS quality in both multi-turn Chain-of-Thought and blockwise duplex models, and release a multi-reward DPO dataset to support reproducible research. Experiments show that single-reward RLAIF selectively improves its targeted metric, while joint multi-reward training yields consistent gains across semantic quality and audio naturalness. These results highlight the importance of holistic, multi-reward alignment for practical conversational SDS.

</details>


### [23] [PsyProbe: Proactive and Interpretable Dialogue through User State Modeling for Exploratory Counseling](https://arxiv.org/abs/2601.19096)
*Sohhyung Park,Hyunji Kang,Sungzoon Cho,Dongil Kim*

Main category: cs.CL

TL;DR: The paper introduces PsyProbe, a proactive mental health dialogue system that systematically models user states and generates therapeutic exploration questions, evaluated to be effective in real-world counseling.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of reactive mental health dialogue systems by creating a proactive system that systematically tracks user psychological states for effective counseling.

Method: Developed PsyProbe, a system using the PPPPPI framework for user state tracking, and features modules like State Builder, Memory Construction, Strategy Planner, and Response Generator for precise and proactive dialogue.

Result: PsyProbe was evaluated with 27 participants in real-world scenarios, outperforming baselines in automatic, user, and expert evaluations, and showing comparable effectiveness to professional counselors.

Conclusion: PsyProbe's systematic psychological state modeling and proactive questioning significantly enhance engagement and therapeutic exploration, validating its potential for mental health counseling.

Abstract: Recent advances in large language models have enabled mental health dialogue systems, yet existing approaches remain predominantly reactive, lacking systematic user state modeling for proactive therapeutic exploration. We introduce PsyProbe, a dialogue system designed for the exploration phase of counseling that systematically tracks user psychological states through the PPPPPI framework (Presenting, Predisposing, Precipitating, Perpetuating, Protective, Impact) augmented with cognitive error detection. PsyProbe combines State Builder for extracting structured psychological profiles, Memory Construction for tracking information gaps, Strategy Planner for Motivational Interviewing behavioral codes, and Response Generator with Question Ideation and Critic/Revision modules to generate contextually appropriate, proactive questions. We evaluate PsyProbe with 27 participants in real-world Korean counseling scenarios, including automatic evaluation across ablation modes, user evaluation, and expert evaluation by a certified counselor. The full PsyProbe model consistently outperforms baseline and ablation modes in automatic evaluation. User evaluation demonstrates significantly increased engagement intention and improved naturalness compared to baseline. Expert evaluation shows that PsyProbe substantially improves core issue understanding and achieves question rates comparable to professional counselors, validating the effectiveness of systematic state modeling and proactive questioning for therapeutic exploration.

</details>


### [24] [Leveraging Sentence-oriented Augmentation and Transformer-Based Architecture for Vietnamese-Bahnaric Translation](https://arxiv.org/abs/2601.19124)
*Tan Sang Nguyen,Quoc Nguyen Pham,Tho Quan*

Main category: cs.CL

TL;DR: This paper explores the use of Neural Machine Translation (NMT) to improve Vietnamese-to-Bahnaric language translation, addressing resource constraints and applying flexible augmentation techniques.


<details>
  <summary>Details</summary>
Motivation: The paper aims to preserve and promote the Bahnaric language, which is culturally vital but faces challenges in accessibility and communication due to limited resources.

Method: The study utilizes advanced NMT methods with two domain-specific augmentation strategies. These strategies are adaptable across NMT models and bypass complex preprocessing, additional training systems, or extra data.

Result: The techniques enhance translation accuracy and fluency between Vietnamese and Bahnaric, contributing to language revival and accessibility for Bahnaric speakers.

Conclusion: The proposed NMT strategies support the linguistic preservation of the Bahnaric language and ensure its accessibility without requiring extensive resources.

Abstract: The Bahnar people, an ethnic minority in Vietnam with a rich ancestral heritage, possess a language of immense cultural and historical significance. The government places a strong emphasis on preserving and promoting the Bahnaric language by making it accessible online and encouraging communication across generations. Recent advancements in artificial intelligence, such as Neural Machine Translation (NMT), have brought about a transformation in translation by improving accuracy and fluency. This, in turn, contributes to the revival of the language through educational efforts, communication, and documentation. Specifically, NMT is pivotal in enhancing accessibility for Bahnaric speakers, making information and content more readily available. Nevertheless, the translation of Vietnamese into Bahnaric faces practical challenges due to resource constraints, especially given the limited resources available for the Bahnaric language. To address this, we employ state-of-the-art techniques in NMT along with two augmentation strategies for domain-specific Vietnamese-Bahnaric translation task. Importantly, both approaches are flexible and can be used with various neural machine translation models. Additionally, they do not require complex data preprocessing steps, the training of additional systems, or the acquisition of extra data beyond the existing training parallel corpora.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [25] [Dynamic Mask-Based Backdoor Attack Against Vision AI Models: A Case Study on Mushroom Detection](https://arxiv.org/abs/2601.18845)
*Zeineb Dridi,Jihen Bennaceur,Amine Ben Hassouna*

Main category: cs.CV

TL;DR: This paper introduces a dynamic mask-based backdoor attack method for object detection models, using dataset poisoning and trigger embedding while achieving high accuracy and attack success rates.


<details>
  <summary>Details</summary>
Motivation: To highlight the vulnerability of object detection models to dynamic mask-based backdoor attacks and the risks of outsourcing practices in critical domains.

Method: The authors use a dataset poisoning technique incorporating SAM, an advanced image segmentation model, to dynamically place triggers and implement their novel backdoor attack on a mushroom detection dataset.

Result: Experiments demonstrate the method's high accuracy on clean data and effective attack performance on poisoned data using YOLOv7, outperforming traditional backdoor methods.

Conclusion: Dynamic and stealthy backdoor attacks present significant threats to deep learning models, necessitating urgent development of robust protection measures.

Abstract: Deep learning has revolutionized numerous tasks within the computer vision field, including image classification, image segmentation, and object detection. However, the increasing deployment of deep learning models has exposed them to various adversarial attacks, including backdoor attacks. This paper presents a novel dynamic mask-based backdoor attack method, specifically designed for object detection models. We exploit a dataset poisoning technique to embed a malicious trigger, rendering any models trained on this compromised dataset vulnerable to our backdoor attack. We particularly focus on a mushroom detection dataset to demonstrate the practical risks posed by such attacks on critical real-life domains. Our work also emphasizes the importance of creating a detailed backdoor attack scenario to illustrate the significant risks associated with the outsourcing practice. Our approach leverages SAM, a recent and powerful image segmentation AI model, to create masks for dynamic trigger placement, introducing a new and stealthy attack method. Through extensive experimentation, we show that our sophisticated attack scenario maintains high accuracy on clean data with the YOLOv7 object detection model while achieving high attack success rates on poisoned samples. Our approach surpasses traditional methods for backdoor injection, which are based on static and consistent patterns. Our findings underscore the urgent need for robust countermeasures to protect deep learning models from these evolving adversarial threats.

</details>


### [26] [Audio-Driven Talking Face Generation with Blink Embedding and Hash Grid Landmarks Encoding](https://arxiv.org/abs/2601.18849)
*Yuhui Zhang,Hui Yu,Wei Liang,Sunjie Zhang*

Main category: cs.CV

TL;DR: The paper introduces a method to improve the fidelity of talking face models using technologies like blink embedding, hash grid landmarks encoding, and neural radiance fields.


<details>
  <summary>Details</summary>
Motivation: To address the persistent challenge of accurately and efficiently capturing mouth movements in talking portraits.

Method: The proposed method involves using blink embedding, hash grid landmarks, and a Dynamic Landmark Transformer that integrates facial features and audio residuals into neural radiance fields.

Result: Experimental results demonstrate that this method outperforms existing approaches in generating high-fidelity talking portraits.

Conclusion: The method offers a more lifelike representation of faces, significantly enhancing facial animation accuracy and efficiency.

Abstract: Dynamic Neural Radiance Fields (NeRF) have demonstrated considerable success in generating high-fidelity 3D models of talking portraits. Despite significant advancements in the rendering speed and generation quality, challenges persist in accurately and efficiently capturing mouth movements in talking portraits. To tackle this challenge, we propose an automatic method based on blink embedding and hash grid landmarks encoding in this study, which can substantially enhance the fidelity of talking faces. Specifically, we leverage facial features encoded as conditional features and integrate audio features as residual terms into our model through a Dynamic Landmark Transformer. Furthermore, we employ neural radiance fields to model the entire face, resulting in a lifelike face representation. Experimental evaluations have validated the superiority of our approach to existing methods.

</details>


### [27] [SelfieAvatar: Real-time Head Avatar reenactment from a Selfie Video](https://arxiv.org/abs/2601.18851)
*Wei Liang,Hui Yu,Derui Ding,Rachael E. Jack,Philippe G. Schyns*

Main category: cs.CV

TL;DR: The paper introduces a method for detailed head avatar reenactment using a single selfie video, overcoming the limitations of existing methods by combining 3DMMs and StyleGAN-based generators.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in current head avatar reenactment methods, such as real-time head and background capture limitations, fine-grained detail loss, and reliance on extensive training data.

Method: Combining 3D Morphable Models with a StyleGAN-based generator, introducing a detailed reconstruction model with mixed loss functions for capturing high-frequency details.

Result: The proposed method achieves superior self-reenactment and cross-reenactment tasks, producing head avatars with rich textures and high fidelity.

Conclusion: The method effectively enhances head avatar reenactment performance using only a simple selfie video, outperforming existing approaches in detail and quality.

Abstract: Head avatar reenactment focuses on creating animatable personal avatars from monocular videos, serving as a foundational element for applications like social signal understanding, gaming, human-machine interaction, and computer vision. Recent advances in 3D Morphable Model (3DMM)-based facial reconstruction methods have achieved remarkable high-fidelity face estimation. However, on the one hand, they struggle to capture the entire head, including non-facial regions and background details in real time, which is an essential aspect for producing realistic, high-fidelity head avatars. On the other hand, recent approaches leveraging generative adversarial networks (GANs) for head avatar generation from videos can achieve high-quality reenactments but encounter limitations in reproducing fine-grained head details, such as wrinkles and hair textures. In addition, existing methods generally rely on a large amount of training data, and rarely focus on using only a simple selfie video to achieve avatar reenactment. To address these challenges, this study introduces a method for detailed head avatar reenactment using a selfie video. The approach combines 3DMMs with a StyleGAN-based generator. A detailed reconstruction model is proposed, incorporating mixed loss functions for foreground reconstruction and avatar image generation during adversarial training to recover high-frequency details. Qualitative and quantitative evaluations on self-reenactment and cross-reenactment tasks demonstrate that the proposed method achieves superior head avatar reconstruction with rich and intricate textures compared to existing approaches.

</details>


### [28] [Weakly supervised framework for wildlife detection and counting in challenging Arctic environments: a case study on caribou (Rangifer tarandus)](https://arxiv.org/abs/2601.18891)
*Ghazaleh Serati,Samuel Foucher,Jerome Theau*

Main category: cs.CV

TL;DR: The paper introduces a method for improved detection of caribou in Arctic aerial imagery through weakly supervised pretraining, addressing challenges like background heterogeneity and class imbalance.


<details>
  <summary>Details</summary>
Motivation: Caribou populations are declining in the Arctic, necessitating scalable, accurate monitoring systems to guide conservation efforts and policies.

Method: The paper proposes a weakly supervised patch-level pretraining approach for a detection network (HerdNet), enabling improved caribou detection in challenging imagery conditions using coarse labels.

Result: The method achieved significant improvements in detection accuracy (F1 scores > 90%) on multi-year datasets, surpassing generic weight initialization approaches.

Conclusion: Weakly supervised pretraining enhances detection in large-scale, challenging datasets, demonstrating its potential to address limited labeled data, though some limitations like false positives and negatives remain.

Abstract: Caribou across the Arctic has declined in recent decades, motivating scalable and accurate monitoring approaches to guide evidence-based conservation actions and policy decisions. Manual interpretation from this imagery is labor-intensive and error-prone, underscoring the need for automatic and reliable detection across varying scenes. Yet, such automatic detection is challenging due to severe background heterogeneity, dominant empty terrain (class imbalance), small or occluded targets, and wide variation in density and scale. To make the detection model (HerdNet) more robust to these challenges, a weakly supervised patch-level pretraining based on a detection network's architecture is proposed. The detection dataset includes five caribou herds distributed across Alaska. By learning from empty vs. non-empty labels in this dataset, the approach produces early weakly supervised knowledge for enhanced detection compared to HerdNet, which is initialized from generic weights. Accordingly, the patch-based pretrain network attained high accuracy on multi-herd imagery (2017) and on an independent year's (2019) test sets (F1: 93.7%/92.6%, respectively), enabling reliable mapping of regions containing animals to facilitate manual counting on large aerial imagery. Transferred to detection, initialization from weakly supervised pretraining yielded consistent gains over ImageNet weights on both positive patches (F1: 92.6%/93.5% vs. 89.3%/88.6%), and full-image counting (F1: 95.5%/93.3% vs. 91.5%/90.4%). Remaining limitations are false positives from animal-like background clutter and false negatives related to low animal density occlusions. Overall, pretraining on coarse labels prior to detection makes it possible to rely on weakly-supervised pretrained weights even when labeled data are limited, achieving results comparable to generic-weight initialization.

</details>


### [29] [RealStats: A Rigorous Real-Only Statistical Framework for Fake Image Detection](https://arxiv.org/abs/2601.18900)
*Haim Zisman,Uri Shaham*

Main category: cs.CV

TL;DR: The paper proposes a statistically grounded framework for detecting AI-generated images using interpretable probability scores and an ensemble of training-free statistics.


<details>
  <summary>Details</summary>
Motivation: To enhance robustness and interpretability in detecting fake images generated by advancing generative models.

Method: Introduces a statistically grounded detection framework using p-values over multiple test statistics and classical ensembling techniques, avoiding training on specific datasets.

Result: Proposed a generic and flexible method capable of detecting fake images reliably across diverse situations.

Conclusion: The method offers a robust, interpretable, and adaptable solution for detecting AI-generated images in the face of evolving generative technologies.

Abstract: As generative models continue to evolve, detecting AI-generated images remains a critical challenge. While effective detection methods exist, they often lack formal interpretability and may rely on implicit assumptions about fake content, potentially limiting robustness to distributional shifts. In this work, we introduce a rigorous, statistically grounded framework for fake image detection that focuses on producing a probability score interpretable with respect to the real-image population. Our method leverages the strengths of multiple existing detectors by combining training-free statistics. We compute p-values over a range of test statistics and aggregate them using classical statistical ensembling to assess alignment with the unified real-image distribution. This framework is generic, flexible, and training-free, making it well-suited for robust fake image detection across diverse and evolving settings.

</details>


### [30] [On the Role of Depth in Surgical Vision Foundation Models: An Empirical Study of RGB-D Pre-training](https://arxiv.org/abs/2601.18929)
*John J. Han,Adam Schmidt,Muhammad Abdullah Jamal,Chinedu Nwoye,Anita Rau,Jie Ying Wu,Omid Mohareri*

Main category: cs.CV

TL;DR: The paper investigates the benefits of incorporating depth information into vision foundation models (VFMs) for surgical scene understanding, comparing multimodal (RGB-D) versus unimodal (RGB) pre-training.


<details>
  <summary>Details</summary>
Motivation: To explore how multimodal or geometry-aware pre-training using depth information can enhance VFMs for complex surgical environments, addressing limitations of unimodal RGB-based approaches.

Method: The study compared eight ViT-based VFMs pre-trained using robotic surgical images paired with depth maps, under frozen-backbone and end-to-end fine-tuning protocols, across eight surgical datasets.

Result: Models with explicit geometric tokenization outperformed unimodal baselines significantly, showing improved data efficiency (performing better with 25% of labeled data) without requiring runtime inference changes.

Conclusion: Multimodal pre-training, incorporating depth information, proves effective for improving surgical vision systems, suggesting a pathway for more capable models without altering inference setups.

Abstract: Vision foundation models (VFMs) have emerged as powerful tools for surgical scene understanding. However, current approaches predominantly rely on unimodal RGB pre-training, overlooking the complex 3D geometry inherent to surgical environments. Although several architectures support multimodal or geometry-aware inputs in general computer vision, the benefits of incorporating depth information in surgical settings remain underexplored. We conduct a large-scale empirical study comparing eight ViT-based VFMs that differ in pre-training domain, learning objective, and input modality (RGB vs. RGB-D). For pre-training, we use a curated dataset of 1.4 million robotic surgical images paired with depth maps generated from an off-the-shelf network. We evaluate these models under both frozen-backbone and end-to-end fine-tuning protocols across eight surgical datasets spanning object detection, segmentation, depth estimation, and pose estimation. Our experiments yield several consistent findings. Models incorporating explicit geometric tokenization, such as MultiMAE, substantially outperform unimodal baselines across all tasks. Notably, geometric-aware pre-training enables remarkable data efficiency: models fine-tuned on just 25% of labeled data consistently surpass RGB-only models trained on the full dataset. Importantly, these gains require no architectural or runtime changes at inference; depth is used only during pre-training, making adoption straightforward. These findings suggest that multimodal pre-training offers a viable path towards building more capable surgical vision systems.

</details>


### [31] [Smart Split-Federated Learning over Noisy Channels for Embryo Image Segmentation](https://arxiv.org/abs/2601.18948)
*Zahra Hafezi Kafshgari,Ivan V. Bajic,Parvaneh Saeedi*

Main category: cs.CV

TL;DR: The paper studies the impact of noise in communication channels within Split-Federated Learning and introduces a smart averaging strategy to improve noise tolerance.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of channel noise in Split-Federated learning and to enhance its resilience, ensuring both computational efficiency and model accuracy.

Method: The authors present a smart averaging strategy to handle noise in the communication channels of Split-Federated learning.

Result: The results demonstrate that the proposed strategy tolerates significantly higher noise levels—two orders of magnitude stronger—while preserving model accuracy during embryo image segmentation.

Conclusion: The proposed smart averaging strategy improves Split-Federated learning's resistance to channel noise without compromising model performance.

Abstract: Split-Federated (SplitFed) learning is an extension of federated learning that places minimal requirements on the clients computing infrastructure, since only a small portion of the overall model is deployed on the clients hardware. In SplitFed learning, feature values, gradient updates, and model updates are transferred across communication channels. In this paper, we study the effects of noise in the communication channels on the learning process and the quality of the final model. We propose a smart averaging strategy for SplitFed learning with the goal of improving resilience against channel noise. Experiments on a segmentation model for embryo images shows that the proposed smart averaging strategy is able to tolerate two orders of magnitude stronger noise in the communication channels compared to conventional averaging, while still maintaining the accuracy of the final model.

</details>


### [32] [Pay Attention to Where You Look](https://arxiv.org/abs/2601.18970)
*Alex Beriand,JhihYang Wu,Daniel Brignac,Natnael Daba,Abhijit Mahalanobis*

Main category: cs.CV

TL;DR: The paper introduces a camera-weighting mechanism to improve few-shot Novel View Synthesis (NVS) by adjusting input view importance based on relevance, enhancing synthesis quality and realism.


<details>
  <summary>Details</summary>
Motivation: Few-shot NVS assumes equal importance for input views, which often leads to suboptimal synthesis results.

Method: The paper introduces a camera-weighting mechanism with two approaches: a deterministic scheme using geometric properties and a learning scheme based on cross-attention for adaptive weighting.

Result: Adaptive camera weighting significantly enhances the accuracy and realism of novel view synthesis results.

Conclusion: Integrating the proposed camera-weighting mechanism into NVS frameworks improves synthesis quality, marking a step toward more accurate photorealistic image generation.

Abstract: Novel view synthesis (NVS) has advanced with generative modeling, enabling photorealistic image generation. In few-shot NVS, where only a few input views are available, existing methods often assume equal importance for all input views relative to the target, leading to suboptimal results.
  We address this limitation by introducing a camera-weighting mechanism that adjusts the importance of source views based on their relevance to the target. We propose two approaches: a deterministic weighting scheme leveraging geometric properties like Euclidean distance and angular differences, and a cross-attention-based learning scheme that optimizes view weighting. Additionally, models can be further trained with our camera-weighting scheme to refine their understanding of view relevance and enhance synthesis quality. This mechanism is adaptable and can be integrated into various NVS algorithms, improving their ability to synthesize high-quality novel views. Our results demonstrate that adaptive view weighting enhances accuracy and realism, offering a promising direction for improving NVS.

</details>


### [33] [FreeOrbit4D: Training-Free Arbitrary Camera Redirection for Monocular Videos via Geometry-Complete 4D Reconstruction](https://arxiv.org/abs/2601.18993)
*Wei Cao,Hao Zhang,Fengrui Tian,Yulun Wu,Yingying Li,Shenlong Wang,Ning Yu,Yaoyao Liu*

Main category: cs.CV

TL;DR: FreeOrbit4D offers a training-free framework for camera redirection, overcoming geometric ambiguities in large-angle viewpoint changes with a geometry-complete 4D proxy.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenging problem of camera redirection for large-angle viewpoint changes, which suffer from geometric ambiguity and temporal inconsistency due to the partial spatial and temporal information captured by a single monocular video.

Method: FreeOrbit4D decouples the monocular video into static background and incomplete foreground point clouds, reconstructs geometry-complete foregrounds using a multi-view diffusion model, aligns these reconstructions with dense pixel-synchronized correspondences, and employs them as structural grounding for a video diffusion model.

Result: Experimental evaluations demonstrate that FreeOrbit4D generates more faithful redirected videos under large-angle trajectories compared to existing methods, overcoming previously observed inconsistencies.

Conclusion: FreeOrbit4D advances camera redirection by introducing a geometry-complete 4D proxy, enabling robust redirections and benefiting practical applications such as edit propagation and 4D data generation.

Abstract: Camera redirection aims to replay a dynamic scene from a single monocular video under a user-specified camera trajectory. However, large-angle redirection is inherently ill-posed: a monocular video captures only a narrow spatio-temporal view of a dynamic 3D scene, providing highly partial observations of the underlying 4D world. The key challenge is therefore to recover a complete and coherent representation from this limited input, with consistent geometry and motion. While recent diffusion-based methods achieve impressive results, they often break down under large-angle viewpoint changes far from the original trajectory, where missing visual grounding leads to severe geometric ambiguity and temporal inconsistency. To address this, we present FreeOrbit4D, an effective training-free framework that tackles this geometric ambiguity by recovering a geometry-complete 4D proxy as structural grounding for video generation. We obtain this proxy by decoupling foreground and background reconstructions: we unproject the monocular video into a static background and geometry-incomplete foreground point clouds in a unified global space, then leverage an object-centric multi-view diffusion model to synthesize multi-view images and reconstruct geometry-complete foreground point clouds in canonical object space. By aligning the canonical foreground point cloud to the global scene space via dense pixel-synchronized 3D--3D correspondences and projecting the geometry-complete 4D proxy onto target camera viewpoints, we provide geometric scaffolds that guide a conditional video diffusion model. Extensive experiments show that FreeOrbit4D produces more faithful redirected videos under challenging large-angle trajectories, and our geometry-complete 4D proxy further opens a potential avenue for practical applications such as edit propagation and 4D data generation. Project page and code will be released soon.

</details>


### [34] [Anatomically-aware conformal prediction for medical image segmentation with random walks](https://arxiv.org/abs/2601.18997)
*Mélanie Gaillochet,Christian Desrosiers,Hervé Lombaert*

Main category: cs.CV

TL;DR: The paper introduces Random-Walk Conformal Prediction (RW-CP) to improve spatial coherence in medical image segmentation, achieving better anatomical validity and segmentation quality.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of anatomically meaningful and stable uncertainty quantification in medical image segmentation, as standard methods often result in fragmented and clinically less useful outcomes.

Method: RW-CP improves prediction intervals by constructing a k-nearest neighbor graph from pre-trained vision foundation model features, applying random walks to diffuse uncertainty and enforce spatial coherence.

Result: RW-CP improves prediction set quality by up to 35.4% compared to standard CP methods, with proven marginal coverage and enhanced segmentation stability.

Conclusion: RW-CP provides an effective and anatomically coherent uncertainty quantification framework for medical image segmentation while being model-agnostic and maintaining statistical rigor.

Abstract: The reliable deployment of deep learning in medical imaging requires uncertainty quantification that provides rigorous error guarantees while remaining anatomically meaningful. Conformal prediction (CP) is a powerful distribution-free framework for constructing statistically valid prediction intervals. However, standard applications in segmentation often ignore anatomical context, resulting in fragmented, spatially incoherent, and over-segmented prediction sets that limit clinical utility. To bridge this gap, this paper proposes Random-Walk Conformal Prediction (RW-CP), a model-agnostic framework which can be added on top of any segmentation method. RW-CP enforces spatial coherence to generate anatomically valid sets. Our method constructs a k-nearest neighbour graph from pre-trained vision foundation model features and applies a random walk to diffuse uncertainty. The random walk diffusion regularizes the non-conformity scores, making the prediction sets less sensitive to the conformal calibration parameter $λ$, ensuring more stable and continuous anatomical boundaries. RW-CP maintains rigorous marginal coverage while significantly improving segmentation quality. Evaluations on multi-modal public datasets show improvements of up to $35.4\%$ compared to standard CP baselines, given an allowable error rate of $α=0.1$.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [35] [Trustworthy Scheduling for Big Data Applications](https://arxiv.org/abs/2601.18983)
*Dimitrios Tomaras,Vana Kalogeraki,Dimitrios Gunopulos*

Main category: cs.DC

TL;DR: The paper introduces X-Sched, a middleware for containerized environments using explainable counterfactual explanations with machine learning to optimize resource configurations and improve SLO compliance.


<details>
  <summary>Details</summary>
Motivation: Existing schedulers for containerized environments lack transparency in decision-making and actionable guidance for meeting Service Level Objectives (SLOs).

Method: X-Sched integrates counterfactual explanations and machine learning (e.g., Random Forests) to identify optimal task execution configurations.

Result: Experimental results demonstrate the efficacy and practicality of X-Sched in improving resource and time performance goals based on real-world data.

Conclusion: X-Sched provides users with explainable and actionable insights for resource configurations, bridging the transparency gap in traditional schedulers while achieving performance objectives.

Abstract: Recent advances in modern containerized execution environments have resulted in substantial benefits in terms of elasticity and more efficient utilization of computing resources. Although existing schedulers strive to optimize performance metrics like task execution times and resource utilization, they provide limited transparency into their decision-making processes or the specific actions developers must take to meet Service Level Objectives (SLOs). In this work, we propose X-Sched, a middleware that uses explainability techniques to generate actionable guidance on resource configurations that makes task execution in containerized environments feasible, under resource and time constraints. X-Sched addresses this gap by integrating counterfactual explanations with advanced machine learning models, such as Random Forests, to efficiently identify optimal configurations. This approach not only ensures that tasks are executed in line with performance goals but also gives users clear, actionable insights into the rationale behind scheduling decisions. Our experimental results validated with data from real-world execution environments, illustrate the efficiency, benefits and practicality of our approach.

</details>


### [36] [Axe: A Simple Unified Layout Abstraction for Machine Learning Compilers](https://arxiv.org/abs/2601.19092)
*Bohan Hou,Hongyi Jin,Guanjie Wang,Jinqi Chen,Yaxing Cai,Lijie Yang,Zihao Ye,Yaoyao Ding,Ruihang Lai,Tianqi Chen*

Main category: cs.DC

TL;DR: Axe Layout introduces a unified, hardware-aware abstraction for deep learning workloads, optimizing placements across devices and accelerators, with results comparable to hand-tuned kernels.


<details>
  <summary>Details</summary>
Motivation: Scaling deep learning requires optimized coordination of data and compute placement across devices, memory hierarchies, and heterogeneous hardware efficiently.

Method: Axe Layout employs a hardware-aware abstraction linking logical tensors to physical devices via named axes, unifying operations like tiling, sharding, and replication. A custom DSL and compiler utilize this for fine-grained control combining threads and collective operators.

Result: Performance results using Axe Layout showcase efficiency close to optimized hand-crafted kernels, tested on GPUs, multi-device setups, and accelerator platforms.

Conclusion: The framework simplifies complexity in managing deep learning workloads on advanced hardware setups while maintaining high performance.

Abstract: Scaling modern deep learning workloads demands coordinated placement of data and compute across device meshes, memory hierarchies, and heterogeneous accelerators. We present Axe Layout, a hardware-aware abstraction that maps logical tensor coordinates to a multi-axis physical space via named axes. Axe unifies tiling, sharding, replication, and offsets across inter-device distribution and on-device layouts, enabling collective primitives to be expressed consistently from device meshes to threads. Building on Axe, we design a multi-granularity, distribution-aware DSL and compiler that composes thread-local control with collective operators in a single kernel. Experiments show that our unified approach can bring performance close to hand-tuned kernels on across latest GPU devices and multi-device environments and accelerator backends.

</details>


### [37] [KUBEDIRECT: Unleashing the Full Power of the Cluster Manager for Serverless Computing](https://arxiv.org/abs/2601.19160)
*Sheng Qi,Zhiquan Zhang,Xuanzhe Liu,Xin Jin*

Main category: cs.DC

TL;DR: KUBEDIRECT is a Kubernetes-based cluster manager designed to improve the efficiency of scaling FaaS instances by bypassing the API Server bottleneck, achieving significant latency reduction while maintaining compatibility.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the bottleneck in state exchange via API Server during FaaS instance scaling on Kubernetes. Existing solutions introduce inefficiency or lack compatibility.

Method: The method involves bypassing the API Server in Kubernetes using direct message passing and a hierarchical write-back cache, leveraging the narrow waist of abstractions to guarantee consistency without centralized coordination.

Result: KUBEDIRECT reduces serving latency by 26.7x over Knative and achieves performance on par with Dirigent, a state-of-the-art, clean-slate platform.

Conclusion: KUBEDIRECT effectively optimizes FaaS scaling with minimal code changes, ensures compatibility with Kubernetes, and provides high performance comparable to advanced clean-slate platforms.

Abstract: FaaS platforms rely on cluster managers like Kubernetes for resource management. Kubernetes is popular due to its state-centric APIs that decouple the control plane into modular controllers. However, to scale out a burst of FaaS instances, message passing becomes the primary bottleneck as controllers have to exchange extensive state through the API Server. Existing solutions opt for a clean-slate redesign of cluster managers, but at the expense of compatibility with existing ecosystem and substantial engineering effort.
  We present KUBEDIRECT, a Kubernetes-based cluster manager for FaaS. We find that there exists a common narrow waist across FaaS platform that allows us to achieve both efficiency and external compatibility. Our insight is that the sequential structure of the narrow waist obviates the need for a single source of truth, allowing us to bypass the API Server and perform direct message passing for efficiency. However, our approach introduces a set of ephemeral states across controllers, making it challenging to enforce end-to-end semantics due to the absence of centralized coordination. KUBEDIRECT employs a novel state management scheme that leverages the narrow waist as a hierarchical write-back cache, ensuring consistency and convergence to the desired state. KUBEDIRECT can seamlessly integrate with Kubernetes, adding ~150 LoC per controller. Experiments show that KUBEDIRECT reduces serving latency by 26.7x over Knative, and has similar performance as the state-of-the-art clean-slate platform Dirigent.

</details>


### [38] [Revisiting Parameter Server in LLM Post-Training](https://arxiv.org/abs/2601.19362)
*Xinyi Wan,Penghui Qi,Guangxing Huang,Chaoyi Ruan,Min Lin,Jialin Li*

Main category: cs.DC

TL;DR: The paper introduces On-Demand Communication (ODC), enhancing training efficiency under imbalanced workloads during large language model (LLM) post-training, resulting in up to 36% speed improvement over Fully Sharded Data Parallel (FSDP).


<details>
  <summary>Details</summary>
Motivation: Current data parallel training methods are inefficient under imbalanced workloads like those caused by high variance in sequence lengths in LLM post-training. This inefficiency calls for a revised approach to eliminate synchronization bottlenecks and improve utilization.

Method: The authors propose On-Demand Communication (ODC), which adapts the parameter server model into FSDP by leveraging direct point-to-point communications instead of collective operations. This reduces synchronization barriers and decouples device workloads, leading to better balancing at the minibatch level.

Result: ODC consistently enhances device utilization and training throughput across LLM post-training tasks, achieving up to 36% speedup compared to standard FSDP.

Conclusion: ODC is proven to be a highly effective solution for handling imbalanced workloads in LLM post-training, significantly improving training speed and efficiency. The implementation is made open-source for further use and development.

Abstract: Modern data parallel (DP) training favors collective communication over parameter servers (PS) for its simplicity and efficiency under balanced workloads. However, the balanced workload assumption no longer holds in large language model (LLM) post-training due to the high variance in sequence lengths. Under imbalanced workloads, collective communication creates synchronization barriers, leading to under-utilization of devices with smaller workloads. This change in training dynamics calls for a revisit of the PS paradigm for its robustness to such imbalance. We propose \textbf{On-Demand Communication (ODC)}, which adapts PS into Fully Sharded Data Parallel (FSDP) by replacing collective all-gather and reduce-scatter with direct point-to-point communication. Compared to FSDP, ODC reduces the synchronization barrier from once per layer to once per minibatch and decouples the workload on each device so that faster workers are not stalled. It also enables simpler and more effective load balancing at the minibatch level. Across diverse LLM post-training tasks, ODC consistently improves device utilization and training throughput, achieving up to a 36\% speedup over standard FSDP. These results demonstrate that ODC is a superior fit for the prevalent imbalanced workloads in LLM post-training. Our implementation of ODC and integration with FSDP is open-sourced at https://github.com/sail-sg/odc.

</details>


### [39] [Modular Foundation Model Inference at the Edge: Network-Aware Microservice Optimization](https://arxiv.org/abs/2601.19563)
*Juan Zhu,Zixin Wang,Shenghui Song,Jun Zhang,Khaled Ben Letaief*

Main category: cs.DC

TL;DR: The paper introduces a microservice-based framework designed to run resource-intensive foundation models effectively across edge and cloud, prioritizing real-time responsiveness and privacy.


<details>
  <summary>Details</summary>
Motivation: Foundation models demand powerful infrastructures but face limitations due to cloud dependency, including latency and privacy concerns.

Method: The framework combines long-term, static placement of core services with dynamic, low-complexity orchestration of light services, leveraging optimization techniques to maintain QoS.

Result: Simulations show the system achieves 84% on-time task completion on average with efficient deployment costs and scalability.

Conclusion: The proposed approach advances the deployment of foundation models by balancing real-time performance, robustness, and resource efficiency across cloud and edge environments.

Abstract: Foundation models (FMs) unlock unprecedented multimodal and multitask intelligence, yet their cloud-centric deployment precludes real-time responsiveness and compromises user privacy. Meanwhile, monolithic execution at the edge remains infeasible under stringent resource limits and uncertain network dynamics. To bridge this gap, we propose a microservice-based FM inference framework that exploits the intrinsic functional asymmetry between heavyweight core services and agile light services. Our two-tier deployment strategy ensures robust Quality of Service (QoS) under resource contention. Specifically, core services are placed statically via a long-term network-aware integer program with sparsity constraints to form a fault-tolerant backbone. On the other hand, light services are orchestrated dynamically by a low-complexity online controller that integrates effective capacity theory with Lyapunov optimization, providing probabilistic latency guarantees under real-time workload fluctuations. Simulations demonstrate that our framework achieves over 84% average on-time task completion with moderate deployment costs and maintains strong robustness as the system load scales.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [40] [NavFormer: IGRF Forecasting in Moving Coordinate Frames](https://arxiv.org/abs/2601.18800)
*Yoontae Hwang,Dongwoo Lee,Minseok Choi,Yong Sup Ihn,Daham Kim,Deok-Young Lee*

Main category: cs.LG

TL;DR: NavFormer improves forecasting of invariant IGRF total intensity using a rotation-invariant module. Better results are shown across diverse training and transfer scenarios.


<details>
  <summary>Details</summary>
Motivation: The goal is to achieve robust forecasting of IGRF total intensity, even when measurements are affected by sensor movements and attitude changes.

Method: NavFormer uses rotation-invariant scalar features and a Canonical SPD module. This module stabilizes window-level Gram matrices to prevent spectral instabilities and discontinuities.

Result: NavFormer outperforms strong baselines in standard, few-shot, and zero-shot training/transfer scenarios in five flight experiments.

Conclusion: NavFormer demonstrates significant improvements in robustness and accuracy for magnetometer-based IGRF forecasting, with publicly available code for further application and validation.

Abstract: Triad magnetometer components change with sensor attitude even when the IGRF total intensity target stays invariant. NavFormer forecasts this invariant target with rotation invariant scalar features and a Canonical SPD module that stabilizes the spectrum of window level second moments of the triads without sign discontinuities. The module builds a canonical frame from a Gram matrix per window and applies state dependent spectral scaling in the original coordinates. Experiments across five flights show lower error than strong baselines in standard training, few shot training, and zero shot transfer. The code is available at: https://anonymous.4open.science/r/NavFormer-Robust-IGRF-Forecasting-for-Autonomous-Navigators-0765

</details>


### [41] [Latent Structural Similarity Networks for Unsupervised Discovery in Multivariate Time Series](https://arxiv.org/abs/2601.18803)
*Olusegun Owoeye*

Main category: cs.LG

TL;DR: The paper introduces a task-agnostic framework for constructing a relational hypothesis graph for multivariate time series data without making assumptions on linearity or objectives.


<details>
  <summary>Details</summary>
Motivation: To address the need for a method to explore candidate relationships in multivariate time series data without being task-specific or constrained by assumptions like linearity and stationarity.

Method: Uses an unsupervised sequence-to-sequence autoencoder to generate sequence representations, aggregates them into embeddings, and constructs a sparse similarity network through latent-space thresholding.

Result: The framework effectively identifies latent similarities and coherent network structures in a real-world dataset of cryptocurrency returns, reflecting underlying relationships.

Conclusion: The proposed layer provides a task-agnostic abstraction to analyze multivariate time series, enabling the discovery of relationships without requiring specific downstream objectives.

Abstract: This paper proposes a task-agnostic discovery layer for multivariate time series that constructs a relational hypothesis graph over entities without assuming linearity, stationarity, or a downstream objective. The method learns window-level sequence representations using an unsupervised sequence-to-sequence autoencoder, aggregates these representations into entity-level embeddings, and induces a sparse similarity network by thresholding a latent-space similarity measure. This network is intended as an analyzable abstraction that compresses the pairwise search space and exposes candidate relationships for further investigation, rather than as a model optimized for prediction, trading, or any decision rule. The framework is demonstrated on a challenging real-world dataset of hourly cryptocurrency returns, illustrating how latent similarity induces coherent network structure; a classical econometric relation is also reported as an external diagnostic lens to contextualize discovered edges.

</details>


### [42] [Variational Quantum Circuit-Based Reinforcement Learning for Dynamic Portfolio Optimization](https://arxiv.org/abs/2601.18811)
*Vincent Gurgul,Ying Chen,Stefan Lessmann*

Main category: cs.LG

TL;DR: This paper introduces Quantum Reinforcement Learning (QRL) for dynamic portfolio optimization using Variational Quantum Circuits. It achieves results comparable to classical deep RL models with fewer parameters and shows robustness across market regimes. However, deployment is limited by latency issues.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the dynamic portfolio optimization problem in financial markets using quantum technology, introducing potential efficiencies and robustness compared to classical deep reinforcement learning models.

Method: Quantum analogues of classical Deep Deterministic Policy Gradient and Deep Q-Network algorithms were implemented, leveraging Variational Quantum Circuits for quantum reinforcement learning.

Result: Quantum agents achieved comparable or superior risk-adjusted performance to classical RL models, with significantly fewer parameters, and demonstrated reduced variability across varying market conditions.

Conclusion: QRL theoretically holds promise for competitive dynamic decision-making in financial markets. Practical deployment faces latency challenges but might gain advantage as technological and infrastructural barriers decrease.

Abstract: This paper presents a Quantum Reinforcement Learning (QRL) solution to the dynamic portfolio optimization problem based on Variational Quantum Circuits. The implemented QRL approaches are quantum analogues of the classical neural-network-based Deep Deterministic Policy Gradient and Deep Q-Network algorithms. Through an empirical evaluation on real-world financial data, we show that our quantum agents achieve risk-adjusted performance comparable to, and in some cases exceeding, that of classical Deep RL models with several orders of magnitude more parameters. In addition to improved parameter efficiency, quantum agents exhibit reduced variability across market regimes, indicating robust behaviour under changing conditions. However, while quantum circuit execution is inherently fast at the hardware level, practical deployment on cloud-based quantum systems introduces substantial latency, making end-to-end runtime currently dominated by infrastructural overhead and limiting practical applicability. Taken together, our results suggest that QRL is theoretically competitive with state-of-the-art classical reinforcement learning and may become practically advantageous as deployment overheads diminish. This positions QRL as a promising paradigm for dynamic decision-making in complex, high-dimensional, and non-stationary environments such as financial markets. The complete codebase is released as open source at: https://github.com/VincentGurgul/qrl-dpo-public

</details>


### [43] [VAE with Hyperspherical Coordinates: Improving Anomaly Detection from Hypervolume-Compressed Latent Space](https://arxiv.org/abs/2601.18823)
*Alejandro Ascarate,Leo Lebrat,Rodrigo Santa Cruz,Clinton Fookes,Olivier Salvado*

Main category: cs.LG

TL;DR: The paper improves VAE's ability for unsupervised and out-of-distribution (OOD) anomaly detection by reformulating latent variables with hyperspherical coordinates.


<details>
  <summary>Details</summary>
Motivation: Detecting out-of-distribution data with VAEs is hindered due to challenges in high-dimensional latent spaces, such as the exponential growth of hypervolume and statistical issues related to hyperspherical geometry.

Method: Proposing hyperspherical latent variable formulation for VAEs to compress latent vectors toward specific directions, enhancing the expressiveness of the approximate posterior.

Result: The proposed approach significantly outperforms existing methods in detecting anomalies over both complex real-world datasets (Mars Rover and galaxy imagery) and standard benchmarks (Cifar10 and subsets of ImageNet).

Conclusion: This hyperspherical approach enhances VAE's generative and anomaly-detection capabilities by leveraging high-dimensional statistical insights, yielding state-of-the-art results.

Abstract: Variational autoencoders (VAE) encode data into lower-dimensional latent vectors before decoding those vectors back to data. Once trained, one can hope to detect out-of-distribution (abnormal) latent vectors, but several issues arise when the latent space is high dimensional. This includes an exponential growth of the hypervolume with the dimension, which severely affects the generative capacity of the VAE. In this paper, we draw insights from high dimensional statistics: in these regimes, the latent vectors of a standard VAE are distributed on the `equators' of a hypersphere, challenging the detection of anomalies. We propose to formulate the latent variables of a VAE using hyperspherical coordinates, which allows compressing the latent vectors towards a given direction on the hypersphere, thereby allowing for a more expressive approximate posterior. We show that this improves both the fully unsupervised and OOD anomaly detection ability of the VAE, achieving the best performance on the datasets we considered, outperforming existing methods. For the unsupervised and OOD modalities, respectively, these are: i) detecting unusual landscape from the Mars Rover camera and unusual Galaxies from ground based imagery (complex, real world datasets); ii) standard benchmarks like Cifar10 and subsets of ImageNet as the in-distribution (ID) class.

</details>


### [44] [Is Finer Better? The Limits of Microscaling Formats in Large Language Models](https://arxiv.org/abs/2601.19026)
*Andrea Fasoli,Monodeep Kar,Chi-Chun Liu,Swagath Venkataramani,Viji Srinivasan,Leland Chang,Naigang Wang*

Main category: cs.LG

TL;DR: Microscaling data formats use per-block tensor quantization for model compression but face accuracy degradation when block size decreases below a threshold. This study identifies the anomaly and proposes a novel hardware-friendly format.


<details>
  <summary>Details</summary>
Motivation: The study aims to explore and address the unexpected degradation in quantized model output when block sizes in microscaling data formats are below a certain threshold, clashing with expectations of improved accuracy.

Method: The authors adopted experimental and theoretical approaches, analyzing tensor distribution behaviors in large language models and laying a framework to explain the anomaly with significant alignment between experimental data and theory.

Result: The anomaly is linked to narrow tensor distributions and limited dynamic range of quantized scales. A new data format, FP8 unsigned E5M3 (UE5M3), is proposed to mitigate this issue.

Conclusion: The proposed UE5M3 format improves performance by eliminating global scaling operations while maintaining comparability to conventional formats for efficient tensor quantization and hardware implementation.

Abstract: Microscaling data formats leverage per-block tensor quantization to enable aggressive model compression with limited loss in accuracy. Unlocking their potential for efficient training and inference necessitates hardware-friendly implementations that handle matrix multiplications in a native format and adopt efficient error-mitigation strategies. Herein, we report the emergence of a surprising behavior associated with microscaling quantization, whereas the output of a quantized model degrades as block size is decreased below a given threshold. This behavior clashes with the expectation that a smaller block size should allow for a better representation of the tensor elements. We investigate this phenomenon both experimentally and theoretically, decoupling the sources of quantization error behind it. Experimentally, we analyze the distributions of several Large Language Models and identify the conditions driving the anomalous behavior. Theoretically, we lay down a framework showing remarkable agreement with experimental data from pretrained model distributions and ideal ones. Overall, we show that the anomaly is driven by the interplay between narrow tensor distributions and the limited dynamic range of the quantized scales. Based on these insights, we propose the use of FP8 unsigned E5M3 (UE5M3) as a novel hardware-friendly format for the scales in FP4 microscaling data types. We demonstrate that UE5M3 achieves comparable performance to the conventional FP8 unsigned E4M3 scales while obviating the need of global scaling operations on weights and activations.

</details>


### [45] [IPBC: An Interactive Projection-Based Framework for Human-in-the-Loop Semi-Supervised Clustering of High-Dimensional Data](https://arxiv.org/abs/2601.18828)
*Mohammad Zare*

Main category: cs.LG

TL;DR: This paper presents IPBC, a framework for effective clustering in high-dimensional datasets using interactive human-guided modifications and nonlinear visual projections.


<details>
  <summary>Details</summary>
Motivation: Clustering high-dimensional data is challenging due to weak distance metrics and the collapse/overlap of clusters in low-dimensional embeddings. Current techniques lack meaningful interactivity and interpretability.

Method: The IPBC framework integrates nonlinear projections with interactive feedback (e.g., must-link/cannot-link constraints), enabling users to iteratively refine the cluster structure. It maps refinements back to the original feature space for explainability.

Result: Experiments indicate that minimal user interaction significantly improves clustering quality, using benchmark datasets for validation.

Conclusion: IPBC enhances clustering through collaboration between machine learning and human insight, improving both robustness and interpretability in high-dimensional data analysis.

Abstract: High-dimensional datasets are increasingly common across scientific and industrial domains, yet they remain difficult to cluster effectively due to the diminishing usefulness of distance metrics and the tendency of clusters to collapse or overlap when projected into lower dimensions. Traditional dimensionality reduction techniques generate static 2D or 3D embeddings that provide limited interpretability and do not offer a mechanism to leverage the analyst's intuition during exploration. To address this gap, we propose Interactive Project-Based Clustering (IPBC), a framework that reframes clustering as an iterative human-guided visual analysis process. IPBC integrates a nonlinear projection module with a feedback loop that allows users to modify the embedding by adjusting viewing angles and supplying simple constraints such as must-link or cannot-link relationships. These constraints reshape the objective of the projection model, gradually pulling semantically related points closer together and pushing unrelated points further apart. As the projection becomes more structured and expressive through user interaction, a conventional clustering algorithm operating on the optimized 2D layout can more reliably identify distinct groups. An additional explainability component then maps each discovered cluster back to the original feature space, producing interpretable rules or feature rankings that highlight what distinguishes each cluster. Experiments on various benchmark datasets show that only a small number of interactive refinement steps can substantially improve cluster quality. Overall, IPBC turns clustering into a collaborative discovery process in which machine representation and human insight reinforce one another.

</details>


### [46] [Native LLM and MLLM Inference at Scale on Apple Silicon](https://arxiv.org/abs/2601.19139)
*Wayner Barrios*

Main category: cs.LG

TL;DR: The paper introduces vllm-mlx, a framework designed for efficient machine learning inference on Apple Silicon, offering significant performance improvements for both text and multimodal models.


<details>
  <summary>Details</summary>
Motivation: The widespread use of Apple Silicon for machine learning demands optimized inference solutions, as existing tools are either not natively optimized or fail to support multimodal workloads effectively.

Method: The proposed vllm-mlx is built natively on MLX, incorporates continuous batching for text models, and introduces content-based prefix caching for multimodal models to eliminate redundant vision encoding.

Result: For text models, the framework achieves up to 87% higher throughput than llama.cpp and scales aggregate throughput by 4.3x under concurrent requests. For multimodal models, it reduces latency significantly, achieving up to 28x speedup using caching techniques.

Conclusion: vllm-mlx demonstrates considerable performance improvements on Apple Silicon, enabling efficient machine learning inference for both text and multimodal applications, and has been made open source for wider use.

Abstract: The growing adoption of Apple Silicon for machine learning development has created demand for efficient inference solutions that leverage its unique unified memory architecture. However, existing tools either lack native optimization (PyTorch MPS) or focus solely on text models (llama.cpp), leaving multimodal workloads underserved. We present vllm-mlx, a framework for efficient LLM and MLLM inference on Apple Silicon built natively on MLX. For text models, we achieve 21% to 87% higher throughput than llama.cpp across models ranging from Qwen3-0.6B to Nemotron-30B, while providing continuous batching that scales to 4.3x aggregate throughput at 16 concurrent requests. For multimodal models, we introduce content-based prefix caching that eliminates redundant vision encoding by identifying identical images through content hashing, regardless of input format. Our evaluation on Apple M4 Max demonstrates throughput of up to 525 tokens per second on text models and 28x speedup on repeated image queries, reducing multimodal latency from 21.7 seconds to under 1 second. Video analysis with up to 64 frames achieves 24.7x cache speedup. We release our implementation as open source to support efficient inference on consumer Apple hardware.

</details>


### [47] [CP Loss: Channel-wise Perceptual Loss for Time Series Forecasting](https://arxiv.org/abs/2601.18829)
*Yaohua Zha,Chunlin Fan,Peiyuan Liu,Yong Jiang,Tao Dai,Hai Wu,Shu-Tao Xia*

Main category: cs.LG

TL;DR: The paper introduces Channel-wise Perceptual Loss (CP Loss), a technique to enhance multi-channel time-series forecasting by learning channel-specific perceptual spaces for better capturing unique dynamics.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the limitations of channel-agnostic loss functions like MSE, which fail to capture the specific characteristics of individual channels in multi-channel time-series data.

Method: The method involves designing a learnable channel-wise filter to generate multi-scale representations for each channel. This filter is optimized jointly with the forecasting model to explicitly align the perceptual spaces with the prediction task. Losses are computed in the proposed perceptual spaces.

Result: The proposed CP Loss enabled the forecasting model to capture channel-specific dynamics such as fluctuations and trend shifts, improving prediction performance over existing methods.

Conclusion: The study concludes that incorporating channel-specific perceptual losses significantly enhances the ability to forecast heterogeneous multi-channel data. The code is made accessible for reproducibility.

Abstract: Multi-channel time-series data, prevalent across diverse applications, is characterized by significant heterogeneity in its different channels. However, existing forecasting models are typically guided by channel-agnostic loss functions like MSE, which apply a uniform metric across all channels. This often leads to fail to capture channel-specific dynamics such as sharp fluctuations or trend shifts. To address this, we propose a Channel-wise Perceptual Loss (CP Loss). Its core idea is to learn a unique perceptual space for each channel that is adapted to its characteristics, and to compute the loss within this space. Specifically, we first design a learnable channel-wise filter that decomposes the raw signal into disentangled multi-scale representations, which form the basis of our perceptual space. Crucially, the filter is optimized jointly with the main forecasting model, ensuring that the learned perceptual space is explicitly oriented towards the prediction task. Finally, losses are calculated within these perception spaces to optimize the model. Code is available at https://github.com/zyh16143998882/CP_Loss.

</details>


### [48] [How Much Temporal Modeling is Enough? A Systematic Study of Hybrid CNN-RNN Architectures for Multi-Label ECG Classification](https://arxiv.org/abs/2601.18830)
*Alireza Jafari,Fatemeh Jafari*

Main category: cs.LG

TL;DR: The study evaluates various deep learning architectures for multi-label ECG classification and finds that combining CNN with a single BiLSTM layer offers the best balance between performance and model complexity.


<details>
  <summary>Details</summary>
Motivation: Accurately classifying ECG signals is challenging due to multiple cardiac conditions, class imbalance, and long-range temporal dependencies. This study seeks to determine the necessity of complex recurrent architectures.

Method: The study systematically compares CNNs with recurrent configurations like LSTM, GRU, BiLSTM, and stacked variants on the PTB-XL dataset, using performance metrics like Hamming loss and macro-AUPRC.

Result: A CNN integrated with a single BiLSTM layer achieves superior performance metrics, including Hamming loss (0.0338) and subset accuracy (0.5723), compared to deeper recurrent models.

Conclusion: Adding more recurrent depth often leads to diminishing returns and overfitting. Structuring architectures based on ECG signals' temporal patterns is crucial for clinically effective models.

Abstract: Accurate multi-label classification of electrocardiogram (ECG) signals remains challenging due to the coexistence of multiple cardiac conditions, pronounced class imbalance, and long-range temporal dependencies in multi-lead recordings. Although recent studies increasingly rely on deep and stacked recurrent architectures, the necessity and clinical justification of such architectural complexity have not been rigorously examined. In this work, we perform a systematic comparative evaluation of convolutional neural networks (CNNs) combined with multiple recurrent configurations, including LSTM, GRU, Bidirectional LSTM (BiLSTM), and their stacked variants, for multi-label ECG classification on the PTB-XL dataset comprising 23 diagnostic categories. The CNN component serves as a morphology-driven baseline, while recurrent layers are progressively integrated to assess their contribution to temporal modeling and generalization performance. Experimental results indicate that a CNN integrated with a single BiLSTM layer achieves the most favorable trade-off between predictive performance and model complexity. This configuration attains superior Hamming loss (0.0338), macro-AUPRC (0.4715), micro-F1 score (0.6979), and subset accuracy (0.5723) compared with deeper recurrent combinations. Although stacked recurrent models occasionally improve recall for specific rare classes, our results provide empirical evidence that increasing recurrent depth yields diminishing returns and may degrade generalization due to reduced precision and overfitting. These findings suggest that architectural alignment with the intrinsic temporal structure of ECG signals, rather than increased recurrent depth, is a key determinant of robust performance and clinically relevant deployment.

</details>


### [49] [The Geometric Reasoner: Manifold-Informed Latent Foresight Search for Long-Context Reasoning](https://arxiv.org/abs/2601.18832)
*Ren Zhuang,Ben Wang,Shuifa Sun*

Main category: cs.LG

TL;DR: The Geometric Reasoner (TGR) is a training-free framework for efficient long chain-of-thought reasoning, improving trajectory coverage with minimal computational cost.


<details>
  <summary>Details</summary>
Motivation: Overcoming the trade-off between computational cost and coverage quality in scaling test-time compute during long chain-of-thought reasoning.

Method: Uses manifold-informed latent foresight search with soft geometric regularizers and chunk-wise memory resets to optimize trajectory exploration under strict memory constraints.

Result: TGR enhances trajectory coverage and robustness on math and code benchmarks, boosting AUC by up to 13 points with only 1.1-1.3 times overhead.

Conclusion: TGR provides an effective solution for improving chain-of-thought reasoning coverage without demanding high computational resources or training requirements.

Abstract: Scaling test-time compute enhances long chain-of-thought (CoT) reasoning, yet existing approaches face a fundamental trade-off between computational cost and coverage quality: either incurring high training expense or yielding redundant trajectories. We introduce The Geometric Reasoner (TGR), a training-free framework that performs manifold-informed latent foresight search under strict memory bounds. At each chunk boundary, TGR scores candidate latent anchors via a lightweight look-ahead estimate combined with soft geometric regularizers that encourage smooth trajectories and diverse exploration. Chunk-wise KV cache resets keep memory linear in chunk length. On challenging math and code benchmarks, TGR improves robust trajectory coverage, measured by the area under the Pass@$k$ curve (AUC), by up to 13 points on Qwen3-8B, with negligible overhead of about 1.1--1.3 times.

</details>


### [50] [Time series forecasting with Hahn Kolmogorov-Arnold networks](https://arxiv.org/abs/2601.18837)
*Md Zahidul Hasan,A. Ben Hamza,Nizar Bouguila*

Main category: cs.LG

TL;DR: The paper introduces HaKAN, a lightweight model for multivariate time series forecasting using Kolmogorov-Arnold Networks (KANs) with Hahn polynomial-based activations, surpassing state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in Transformers' quadratic complexity and MLPs' spectral bias in long-term time series forecasting.

Method: HaKAN leverages learnable Hahn polynomial-based activation functions, integrates channel independence, patching, and uses Hahn-KAN blocks with residual connections to capture global and local temporal patterns.

Result: The model demonstrates superior performance compared to recent state-of-the-art methods across various forecasting benchmarks, validated through ablation studies.

Conclusion: HaKAN offers a lightweight, interpretable solution for multivariate time series forecasting, effectively addressing challenges of previous models while achieving consistent improvement.

Abstract: Recent Transformer- and MLP-based models have demonstrated strong performance in long-term time series forecasting, yet Transformers remain limited by their quadratic complexity and permutation-equivariant attention, while MLPs exhibit spectral bias. We propose HaKAN, a versatile model based on Kolmogorov-Arnold Networks (KANs), leveraging Hahn polynomial-based learnable activation functions and providing a lightweight and interpretable alternative for multivariate time series forecasting. Our model integrates channel independence, patching, a stack of Hahn-KAN blocks with residual connections, and a bottleneck structure comprised of two fully connected layers. The Hahn-KAN block consists of inter- and intra-patch KAN layers to effectively capture both global and local temporal patterns. Extensive experiments on various forecasting benchmarks demonstrate that our model consistently outperforms recent state-of-the-art methods, with ablation studies validating the effectiveness of its core components.

</details>


### [51] [Knowledge-Aware Evolution for Streaming Federated Continual Learning with Category Overlap and without Task Identifiers](https://arxiv.org/abs/2601.19788)
*Sixing Tan,Xianmin Liu*

Main category: cs.LG

TL;DR: The paper proposes FedKACE, a method for addressing challenges in federated continual learning under streaming scenarios with overlapping categories and no task identifiers.


<details>
  <summary>Details</summary>
Motivation: To address issues in federated continual learning settings where current approaches struggle with non-stationary data streaming with overlapping categories and absent task identifiers, leading to confusion in knowledge retention and task assignments.

Method: Introduces FedKACE with three components: an adaptive model-switching mechanism, a gradient-balanced replay scheme, and a kernel spectral boundary buffer to improve continuous learning and knowledge retention.

Result: FedKACE demonstrates superior performance in handling overlapping-class scenarios and preserving prior knowledge in experiments and regret analysis across multiple scenarios.

Conclusion: FedKACE effectively balances the trade-off between personalization and generalization while optimizing knowledge retention and adaptability in streaming federated continual learning settings.

Abstract: Federated Continual Learning (FCL) leverages inter-client collaboration to balance new knowledge acquisition and prior knowledge retention in non-stationary data. However, existing batch-based FCL methods lack adaptability to streaming scenarios featuring category overlap between old and new data and absent task identifiers, leading to indistinguishability of old and new knowledge, uncertain task assignments for samples, and knowledge confusion.To address this, we propose streaming federated continual learning setting: per federated learning (FL) round, clients process streaming data with disjoint samples and potentially overlapping categories without task identifiers, necessitating sustained inference capability for all prior categories after each FL round.Next, we introduce FedKACE: 1) an adaptive inference model switching mechanism that enables unidirectional switching from local model to global model to achieve a trade-off between personalization and generalization; 2) a adaptive gradient-balanced replay scheme that reconciles new knowledge learning and old knowledge retention under overlapping-class scenarios; 3) a kernel spectral boundary buffer maintenance that preserves high-information and high-boundary-influence samples to optimize cross-round knowledge retention. Experiments across multiple scenarios and regret analysis demonstrate the effectiveness of FedKACE.

</details>


### [52] [Analysis of Control Bellman Residual Minimization for Markov Decision Problem](https://arxiv.org/abs/2601.18840)
*Donghwan Lee,Hyukjun Yang*

Main category: cs.LG

TL;DR: This paper focuses on control Bellman residual minimization for policy optimization in Markov decision processes.


<details>
  <summary>Details</summary>
Motivation: Bellman residual minimization for policy optimization has been underexplored compared to dynamic programming, despite its potential benefits like stable convergence.

Method: The authors establish foundational results for using Bellman residual minimization in control tasks related to policy optimization.

Result: The paper advances understanding and applicability of Bellman residual minimization in control tasks.

Conclusion: This work highlights the potential of Bellman residual minimization for policy optimization and provides a basis for further research in this area.

Abstract: Markov decision problems are most commonly solved via dynamic programming. Another approach is Bellman residual minimization, which directly minimizes the squared Bellman residual objective function. However, compared to dynamic programming, this approach has received relatively less attention, mainly because it is often less efficient in practice and can be more difficult to extend to model-free settings such as reinforcement learning. Nonetheless, Bellman residual minimization has several advantages that make it worth investigating, such as more stable convergence with function approximation for value functions. While Bellman residual methods for policy evaluation have been widely studied, methods for policy optimization (control tasks) have been scarcely explored. In this paper, we establish foundational results for the control Bellman residual minimization for policy optimization.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [53] [HEATACO: Heatmap-Guided Ant Colony Decoding for Large-Scale Travelling Salesman Problems](https://arxiv.org/abs/2601.19041)
*Bo-Cheng Lin,Yi Mei,Mengjie Zhang*

Main category: cs.NE

TL;DR: The paper introduces HeatACO, a Max-Min Ant System-based non-autoregressive decoding method for solving large-scale Travelling Salesman Problems using heatmaps, improving the trade-off between computational efficiency and tour quality.


<details>
  <summary>Details</summary>
Motivation: Solving large-scale Travelling Salesman Problems (TSPs) using heatmap-based solvers is hampered by errors in conventional greedy decoding and the computational expense of accurate methods like MCTS.

Method: The authors propose HeatACO that integrates pheromone updates and heatmaps as soft edge priors to guide probabilistic tour construction, correcting local mis-rankings via global feedback under feasibility constraints.

Result: HeatACO achieves high-quality tours with competitive gaps of 0.11%/0.23%/1.15% on TSP500/TSP1K/TSP10K datasets, offering seconds-to-minutes CPU decoding on fixed heatmaps, outperforming previous methods.

Conclusion: Heatmaps serve as a reliable prior, but their generalization under distribution shifts remains a challenge. Iterative improvements to heatmap accuracy may lead to better decoding performance in large TSPs.

Abstract: Heatmap-based non-autoregressive solvers for large-scale Travelling Salesman Problems output dense edge-probability scores, yet final performance largely hinges on the decoder that must satisfy degree-2 constraints and form a single Hamiltonian tour. Greedy commitment can cascade into irreparable mistakes at large $N$, whereas MCTS-guided local search is accurate but compute-heavy and highly engineered. We instead treat the heatmap as a soft edge prior and cast decoding as probabilistic tour construction under feasibility constraints, where the key is to correct local mis-rankings via inexpensive global coordination. Based on this view, we introduce HeatACO, a plug-and-play Max-Min Ant System decoder whose transition policy is softly biased by the heatmap while pheromone updates provide lightweight, instance-specific feedback to resolve global conflicts; optional 2-opt/3-opt post-processing further improves tour quality. On TSP500/1K/10K, using heatmaps produced by four pretrained predictors, HeatACO+2opt achieves gaps down to 0.11%/0.23%/1.15% with seconds-to-minutes CPU decoding for fixed heatmaps, offering a better quality--time trade-off than greedy decoding and published MCTS-based decoders. Finally, we find the gains track heatmap reliability: under distribution shift, miscalibration and confidence collapse bound decoding improvements, suggesting heatmap generalisation is a primary lever for further progress.

</details>


### [54] [ROIDS: Robust Outlier-Aware Informed Down-Sampling](https://arxiv.org/abs/2601.19477)
*Alina Geiger,Martin Briesch,Dominik Sobania,Franz Rothlauf*

Main category: cs.NE

TL;DR: This paper introduces ROIDS (Robust Outlier-Aware Informed Down-Sampling) to improve symbolic regression by addressing issues of overfitting to outliers in the IDS methodology.


<details>
  <summary>Details</summary>
Motivation: IDS is inconsistent in handling problems with outliers, causing overfitting in symbolic regression as it selects subsets that favor outliers.

Method: ROIDS improves upon IDS by excluding outliers during the informed down-sampling process, maintaining IDS's advantages without overfitting.

Result: ROIDS outperforms IDS on synthetic and real-world problems with outliers, achieving success on over 80% of real-world benchmarks and the best average rank among baseline methods.

Conclusion: ROIDS is a robust and reliable method for down-sampling in symbolic regression, especially when datasets include outliers.

Abstract: Informed down-sampling (IDS) is known to improve performance in symbolic regression when combined with various selection strategies, especially tournament selection. However, recent work found that IDS's gains are not consistent across all problems. Our analysis reveals that IDS performance is worse for problems containing outliers. IDS systematically favors including outliers in subsets which pushes GP towards finding solutions that overfit to outliers. To address this, we introduce ROIDS (Robust Outlier-Aware Informed Down-Sampling), which excludes potential outliers from the sampling process of IDS. With ROIDS it is possible to keep the advantages of IDS without overfitting to outliers and to compete on a wide range of benchmark problems. This is also reflected in our experiments in which ROIDS shows the desired behavior on all studied benchmark problems. ROIDS consistently outperforms IDS on synthetic problems with added outliers as well as on a wide range of complex real-world problems, surpassing IDS on over 80% of the real-world benchmark problems. Moreover, compared to all studied baseline approaches, ROIDS achieves the best average rank across all tested benchmark problems. This robust behavior makes ROIDS a reliable down-sampling method for selection in symbolic regression, especially when outliers may be included in the data set.

</details>


### [55] [Posterior Distribution-assisted Evolutionary Dynamic Optimization as an Online Calibrator for Complex Social Simulations](https://arxiv.org/abs/2601.19481)
*Peng Yang,Zhenhua Yang,Boquan Jiang,Chenkai Wang,Ke Tang,Xin Yao*

Main category: cs.NE

TL;DR: The paper proposes an online calibration method for simulators of dynamic social systems using a pretrained posterior model, improving the performance of Evolutionary Dynamic Optimization methods.


<details>
  <summary>Details</summary>
Motivation: To address the inadequacy of existing Evolutionary Dynamic Optimization methods in handling online calibration for simulators of complex and evolving social systems.

Method: The authors propose a pretrained posterior model for learning and fine-tuning the posterior distributions of the parameters and observational data, enabling change detection and adaptation during optimization.

Result: Tests on economic and financial simulators show that the method enhances Evolutionary Dynamic Optimization performance by accurately fitting to real system changes.

Conclusion: This approach establishes a new, effective pathway for online calibration, making Evolutionary Dynamic Optimization feasible for real-world social system simulations.

Abstract: The calibration of simulators for complex social systems aims to identify the optimal parameter that drives the output of the simulator best matching the target data observed from the system. As many social systems may change internally over time, calibration naturally becomes an online task, requiring parameters to be updated continuously to maintain the simulator's fidelity. In this work, the online setting is first formulated as a dynamic optimization problem (DOP), requiring the search for a sequence of optimal parameters that fit the simulator to real system changes. However, in contrast to traditional DOP formulations, online calibration explicitly incorporates the observational data as the driver of environmental dynamics. Due to this fundamental difference, existing Evolutionary Dynamic Optimization (EDO) methods, despite being extensively studied for black-box DOPs, are ill-equipped to handle such a scenario. As a result, online calibration problems constitute a new set of challenging DOPs. Here, we propose to explicitly learn the posterior distributions of the parameters and the observational data, thereby facilitating both change detection and environmental adaptation of existing EDOs for this scenario. We thus present a pretrained posterior model for implementation, and fine-tune it during the optimization. Extensive tests on both economic and financial simulators verify that the posterior distribution strongly promotes EDOs in such DOPs widely existed in social science.

</details>


### [56] [Rethinking Intelligence: Brain-like Neuron Network](https://arxiv.org/abs/2601.19508)
*Weifeng Liu*

Main category: cs.NE

TL;DR: The paper introduces Brain-like Neural Network (BNN) and its implementation, LuminaNet, which autonomously modifies its architecture for self-evolution, proving superior performance in image classification and text generation tasks.


<details>
  <summary>Details</summary>
Motivation: To rethink the formation and evolution of intelligence and develop neural networks that mimic biological neural systems more closely.

Method: LuminaNet, designed based on the BNN paradigm, eliminates convolutions and self-attention, achieving autonomous dynamic architectural modifications.

Result: LuminaNet improves accuracy significantly in CIFAR-10, reaching notable performance against benchmarks like LeNet-5 and AlexNet. It also performs well in the TinyStories task with reduced computational cost and memory usage.

Conclusion: LuminaNet demonstrates the capability for self-evolution in artificial neural networks, suggesting a viable alternative to traditional models while providing efficiency advantages.

Abstract: Since their inception, artificial neural networks have relied on manually designed architectures and inductive biases to better adapt to data and tasks. With the rise of deep learning and the expansion of parameter spaces, they have begun to exhibit brain-like functional behaviors. Nevertheless, artificial neural networks remain fundamentally different from biological neural systems in structural organization, learning mechanisms, and evolutionary pathways.
  From the perspective of neuroscience, we rethink the formation and evolution of intelligence and proposes a new neural network paradigm, Brain-like Neural Network (BNN). We further present the first instantiation of a BNN termed LuminaNet that operates without convolutions or self-attention and is capable of autonomously modifying its architecture. We conduct extensive experiments demonstrating that LuminaNet can achieve self-evolution through dynamic architectural changes. On the CIFAR-10, LuminaNet achieves top-1 accuracy improvements of 11.19%, 5.46% over LeNet-5 and AlexNet, respectively, outperforming MLP-Mixer, ResMLP, and DeiT-Tiny among MLP/ViT architectures. On the TinyStories text generation task, LuminaNet attains a perplexity of 8.4, comparable to a single-layer GPT-2-style Transformer, while reducing computational cost by approximately 25% and peak memory usage by nearly 50%. Code and interactive structures are available at https://github.com/aaroncomo/LuminaNet.

</details>


### [57] [Tournament Informed Adversarial Quality Diversity](https://arxiv.org/abs/2601.19562)
*Timothée Anne,Noah Syrkis,Meriem Elhosni,Florian Turati,Alexandre Manai,Franck Legendre,Alain Jaquier,Sebastian Risi*

Main category: cs.NE

TL;DR: The paper improves Generational Adversarial MAP-Elites (GAME) for adversarial problems by introducing tournament-based methods and evaluates them in three games.


<details>
  <summary>Details</summary>
Motivation: Classical quality diversity (QD) fails in adversarial problems because solution evaluation depends on opponent-side dependencies. The paper aims to improve QD's application to these problems by addressing these limitations in measures and task selection.

Method: The paper uses an inter-variants tournament mechanism for fair comparison and proposes two task selection methods informed by the tournament outcomes.

Result: Experimental results in Pong, Cat-and-mouse, and Pursuers-and-evaders games show that their tournament-informed task selection improves both quality and diversity in adversarial settings.

Conclusion: Tournament-informed metrics and task selection effectively address the limitations of classical QD in adversarial problems, advancing the field with measurable quality and diversity improvements.

Abstract: Quality diversity (QD) is a branch of evolutionary computation that seeks high-quality and behaviorally diverse solutions to a problem. While adversarial problems are common, classical QD cannot be easily applied to them, as both the fitness and the behavior depend on the opposing solutions. Recently, Generational Adversarial MAP-Elites (GAME) has been proposed to coevolve both sides of an adversarial problem by alternating the execution of a multi-task QD algorithm against previous elites, called tasks. The original algorithm selects new tasks based on a behavioral criterion, which may lead to undesired dynamics due to inter-side dependencies. In addition, comparing sets of solutions cannot be done directly using classical QD measures due to side dependencies. In this paper, we (1) use an inter-variants tournament to compare the sets of solutions, ensuring a fair comparison, with 6 measures of quality and diversity, and (2) propose two tournament-informed task selection methods to promote higher quality and diversity at each generation. We evaluate the variants across three adversarial problems: Pong, a Cat-and-mouse game, and a Pursuers-and-evaders game. We show that the tournament-informed task selection method leads to higher adversarial quality and diversity. We hope that this work will help further advance adversarial quality diversity. Code, videos, and supplementary material are available at https://github.com/Timothee-ANNE/GAME_tournament_informed.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [58] [Refactoring and Equivalence in Rust: Expanding the REM Toolchain with a Novel Approach to Automated Equivalence Proofs](https://arxiv.org/abs/2601.19207)
*Matthew Britton,Sasha Pak,Alex Potanin*

Main category: cs.PL

TL;DR: The paper presents REM2.0, a toolchain for Rust that enables fast and advanced extract-function refactoring while supporting optional robust verification of equivalence proofs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in automated extract-function refactoring for Rust, caused by unique ownership, borrowing, and type features, with existing tools being slow, limited, or providing insufficient guarantees.

Method: REM2.0 operates on top of rust-analyzer as a persistent daemon, offers low-latency refactorings via a VSCode interface, includes a feature to repair lifetime and signature issues, and integrates with CHARON and AENEAS for optional verification.

Result: REM2.0 achieves 100% compatibility with the original artifact while drastically reducing latency, handles complex extractions in GitHub examples, and successfully generates equivalence proofs for supported benchmarks.

Conclusion: The approach demonstrates that rust-analyzer-based tools can offer efficient and feature-rich refactoring solutions for Rust, with optional formal verification ensuring behavior preservation.

Abstract: Refactoring tools are central to modern development, with extract-function refactorings used heavily in day-to-day work. For Rust, however, ownership, borrowing, and advanced type features make automated extract-function refactoring challenging. Existing tools either rely on slow compiler-based analysis, support only restricted language fragments, or provide little assurance beyond "it still compiles." This paper presents REM2.0, a new extract-function and verification toolchain for Rust. REM2.0 works atop rust-analyzer as a persistent daemon, providing low-latency refactorings with a VSCode front-end. It adds a repairer that automatically adjusts lifetimes and signatures when extraction exposes borrow-checker issues, and an optional verification pipeline connecting to CHARON and AENEAS to generate Coq equivalence proofs for a supported Rust subset. The architecture is evaluated on three benchmark suites. On the original REM artefact, REM2.0 achieves 100% compatibility while reducing latency from ~1000ms to single-digit milliseconds in the daemon. On 40 feature-focused extractions from 20 highly starred GitHub repositories, REM2.0 handles most examples involving async/await, const fn, non-local control flow, generics, and higher-ranked trait bounds. On twenty verification benchmarks, the CHARON/AENEAS pipeline constructs end-to-end equivalence proofs for cases within its current subset. Overall, results show that a rust-analyzer-based design can provide fast, feature-rich extract-function refactoring for real Rust programs, while opt-in verification delivers machine-checked behaviour preservation.

</details>


### [59] [For Generalised Algebraic Theories, Two Sorts Are Enough](https://arxiv.org/abs/2601.19426)
*Samy Avrillon,Ambrus Kaposi,Ambroise Lafont,Niyousha Najmaei,Johann Rosain*

Main category: cs.PL

TL;DR: The paper introduces a reduction method for generalised algebraic theories (GATs), showing that any GAT can be simplified to have only two sorts while preserving key properties.


<details>
  <summary>Details</summary>
Motivation: The motivation is to simplify the structure of generalised algebraic theories for better understanding and implementation. This simplification aims to eliminate complexities such as sort equalities and interleaving between sorts and operations.

Method: The authors use a semantic approach based on Uemura's bi-initial characterization in a 2-category of finitely complete categories to reduce GATs to two sorts. This reduction maintains a strict correspondence between the models of the original and simplified GATs.

Result: The reduced GAT is simpler, lacking sort equalities and interleaved sorts and operations. Models of the original GAT can be converted to and from the reduced GAT seamlessly, preserving identity roundtrip transformations.

Conclusion: This reduction improves theoretical simplicity and practical implementation of GATs, such as for QIITs, especially in contexts like Cubical Agda that have restrictions on constructors.

Abstract: Generalised algebraic theories (GATs) allow multiple sorts indexed over each other. For example, the theories of categories or Martin-L{ö}f type theories form GATs. Categories have two sorts, objects and morphisms, and the latter are double-indexed over the former. Martin-L{ö}f type theory has four sorts: contexts, substitutions, types and terms. For example, types are indexed over contexts, and terms are indexed over both contexts and types. In this paper we show that any GAT can be reduced to a GAT with only two sorts, and there is a section-retraction correspondence (formally, a strict coreflection) between models of the original and the reduced GAT. In particular, any model of the original GAT can be turned into a model of the reduced (two-sorted) GAT and back, and this roundtrip is the identity.
  The reduced GAT is simpler than the original GAT in the following aspects: it does not have sort equalities; it does not have interleaved sorts and operations; if the original GAT did not have interleaved sorts and operations, then the reduced GAT won't have operations interleaved between different sorts. In a type-theoretic metatheory, the initial algebra of a GAT is called a quotient inductive-inductive type (QIIT). Our reduction provides a way to implement QIITs with sort equalities or interleaved constructors which are not allowed by Cubical Agda. An instance of our reduction is the well-known method of reducing mutual inductive types to a single indexed family. Our approach is semantic in that it does not rely on a syntactic description of GATs, but instead, on Uemura's bi-initial characterisation of the category of (finite) GATs in the 2-category of finitely complete categories with a chosen exponentiable morphism.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [60] [Learning the Pareto Space of Multi-Objective Autonomous Driving: A Modular, Data-Driven Approach](https://arxiv.org/abs/2601.18913)
*Mohammad Elayan,Wissam Kontar*

Main category: cs.RO

TL;DR: This study proposes a framework to analyze the trade-offs among safety, efficiency, and interaction in autonomous vehicle (AV) operations using naturalistic trajectory data. It identifies optimal states through Pareto dominance.


<details>
  <summary>Details</summary>
Motivation: Designing AVs requires balancing safety, efficiency, and interaction to ensure effective real-world behavior. This paper aims to empirically understand and optimize these trade-offs.

Method: The approach uses kinematic and positional data from AV datasets and applies Pareto dominance to assess and visualize balanced performance across safety, efficiency, and interaction.

Result: Findings showed that only 0.23% of AV driving instances were Pareto-optimal, with interaction having the highest potential for improvement.

Conclusion: The framework provides a modular, minimally invasive method to study multi-objective trade-offs in AVs, offering potential for application in other contexts to improve performance surfaces.

Abstract: Balancing safety, efficiency, and interaction is fundamental to designing autonomous driving agents and to understanding autonomous vehicle (AV) behavior in real-world operation. This study introduces an empirical learning framework that derives these trade-offs directly from naturalistic trajectory data. A unified objective space represents each AV timestep through composite scores of safety, efficiency, and interaction. Pareto dominance is applied to identify non-dominated states, forming an empirical frontier that defines the attainable region of balanced performance.
  The proposed framework was demonstrated using the Third Generation Simulation (TGSIM) datasets from Foggy Bottom and I-395. Results showed that only 0.23\% of AV driving instances were Pareto-optimal, underscoring the rarity of simultaneous optimization across objectives. Pareto-optimal states showed notably higher mean scores for safety, efficiency, and interaction compared to non-optimal cases, with interaction showing the greatest potential for improvement.
  This minimally invasive and modular framework, which requires only kinematic and positional data, can be directly applied beyond the scope of this study to derive and visualize multi-objective learning surfaces

</details>


### [61] [DeFM: Learning Foundation Representations from Depth for Robotics](https://arxiv.org/abs/2601.18923)
*Manthan Patel,Jonas Frey,Mayank Mittal,Fan Yang,Alexander Hansson,Amir Bar,Cesar Cadena,Marco Hutter*

Main category: cs.RO

TL;DR: This paper introduces DeFM, a self-supervised foundation model designed for depth images used in robotics, achieving state-of-the-art performance across tasks without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: The authors address the lack of foundation models specifically tailored for depth modality in robotics, as most previous work focuses on RGB images despite depth sensors being widely used.

Method: DeFM uses a DINO-style self-distillation objective on a 60M-depth image dataset and introduces a novel input normalization strategy to learn geometric and semantic representations. Compact models for resource-constrained systems are also developed.

Result: DeFM achieves state-of-the-art performance across various benchmarks including classification, segmentation, navigation, locomotion, and manipulation, generalizing well from simulation to real-world scenarios.

Conclusion: DeFM demonstrates the capability of depth-specific representation learning for robotics and provides pretrained models for immediate use in diverse robotic tasks, filling a gap in depth-based learning approaches.

Abstract: Depth sensors are widely deployed across robotic platforms, and advances in fast, high-fidelity depth simulation have enabled robotic policies trained on depth observations to achieve robust sim-to-real transfer for a wide range of tasks. Despite this, representation learning for depth modality remains underexplored compared to RGB, where large-scale foundation models now define the state of the art. To address this gap, we present DeFM, a self-supervised foundation model trained entirely on depth images for robotic applications. Using a DINO-style self-distillation objective on a curated dataset of 60M depth images, DeFM learns geometric and semantic representations that generalize to diverse environments, tasks, and sensors. To retain metric awareness across multiple scales, we introduce a novel input normalization strategy. We further distill DeFM into compact models suitable for resource-constrained robotic systems. When evaluated on depth-based classification, segmentation, navigation, locomotion, and manipulation benchmarks, DeFM achieves state-of-the-art performance and demonstrates strong generalization from simulation to real-world environments. We release all our pretrained models, which can be adopted off-the-shelf for depth-based robotic learning without task-specific fine-tuning. Webpage: https://de-fm.github.io/

</details>


### [62] [Fauna Sprout: A lightweight, approachable, developer-ready humanoid robot](https://arxiv.org/abs/2601.18963)
*Fauna Robotics,:,Diego Aldarondo,Ana Pervan,Daniel Corbalan,Dave Petrillo,Bolun Dai,Aadhithya Iyer,Nina Mortensen,Erik Pearson,Sridhar Pandian Arunachalam,Emma Reznick,David Weis,Jacob Davison,Samuel Patterson,Tess Carella,Michael Suguitan,David Ye,Oswaldo Ferro,Nilesh Suriyarachchi,Spencer Ling,Erik Su,Daniel Giebisch,Peter Traver,Sam Fonseca,Mack Mor,Rohan Singh,Sertac Guven,Kangni Liu,Yaswanth Kumar Orru,Ashiq Rahman Anwar Batcha,Shruthi Ravindranath,Silky Arora,Hugo Ponte,Dez Hernandez,Utsav Chaudhary,Zack Walker,Michael Kelberman,Ivan Veloz,Christina Santa Lucia,Kat Casale,Helen Han,Michael Gromis,Michael Mignatti,Jason Reisman,Kelleher Guerin,Dario Narvaez,Christopher Anderson,Anthony Moschella,Robert Cochran,Josh Merel*

Main category: cs.RO

TL;DR: The paper introduces Sprout, a safe, accessible humanoid platform built for operating in human environments, with features supporting safety, social interaction, and developer accessibility.


<details>
  <summary>Details</summary>
Motivation: Current humanoid robotics suffer from a lack of platforms that are safe, user-friendly, and deployable in human environments.

Method: The authors developed Sprout, a humanoid system with compliant control, limited joint torques, soft exteriors, an expressive head, integrated grippers, and VR-based teleoperation, all within a user-friendly stack.

Result: Sprout offers a safer, more expressive, and accessible platform for humanoid robotics, enabling easier deployment in shared human spaces.

Conclusion: Sprout addresses the limitations of existing humanoid systems by providing a practical platform that fosters embodied intelligence development in real-world environments.

Abstract: Recent advances in learned control, large-scale simulation, and generative models have accelerated progress toward general-purpose robotic controllers, yet the field still lacks platforms suitable for safe, expressive, long-term deployment in human environments. Most existing humanoids are either closed industrial systems or academic prototypes that are difficult to deploy and operate around people, limiting progress in robotics. We introduce Sprout, a developer platform designed to address these limitations through an emphasis on safety, expressivity, and developer accessibility. Sprout adopts a lightweight form factor with compliant control, limited joint torques, and soft exteriors to support safe operation in shared human spaces. The platform integrates whole-body control, manipulation with integrated grippers, and virtual-reality-based teleoperation within a unified hardware-software stack. An expressive head further enables social interaction -- a domain that remains underexplored on most utilitarian humanoids. By lowering physical and technical barriers to deployment, Sprout expands access to capable humanoid platforms and provides a practical basis for developing embodied intelligence in real human environments.

</details>


### [63] [A Switching Nonlinear Model Predictive Control Strategy for Safe Collision Handling by an Underwater Vehicle-Manipulator System](https://arxiv.org/abs/2601.18971)
*Ioannis G. Polyzos,Konstantinos J. Kyriakopoulos*

Main category: cs.RO

TL;DR: This paper presents a Nonlinear Model Predictive Control (NMPC) strategy for safe collision handling in underwater robot systems, allowing both collision avoidance and controlled manipulator engagement.


<details>
  <summary>Details</summary>
Motivation: The development of control strategies to address potential collisions for Underwater Vehicle-Manipulator Systems (UVMS) in autonomous underwater tasks is crucial as these systems operate in complex environments.

Method: The researchers developed a switching Nonlinear Model Predictive Control (NMPC) algorithm that either avoids collisions or uses the manipulator to safely manage collisions by deflecting the UVMS.

Result: Through virtual experiments, the proposed algorithm successfully demonstrated its ability to both detect and manage potential collisions, utilizing the manipulator to protect the vehicle from damage during unavoidable impacts.

Conclusion: The NMPC strategy offers a robust solution for collision management in UVMS, ensuring safe operation during active underwater interventions, paving the way for further autonomy in challenging environments.

Abstract: For active intervention tasks in underwater environments, the use of autonomous vehicles is just now emerging as an active area of research. During operation, for various reasons, the robot might find itself on a collision course with an obstacle in its environment. In this paper, a switching Nonlinear Model Predictive Control (NMPC) strategy is proposed to safely handle collisions for an Underwater Vehicle-Manipulator System (UVMS). When avoiding the collision is impossible, the control algorithm takes advantage of the manipulator, using it to push against the obstacle, and deflect away from the collision. Virtual experiments are performed to demonstrate the algorithm's capability to successfully detect collisions and either avoid them, or use the manipulator to handle them appropriately without damaging sensitive areas of the vehicle.

</details>


### [64] [Neuromorphic BrailleNet: Accurate and Generalizable Braille Reading Beyond Single Characters through Event-Based Optical Tactile Sensing](https://arxiv.org/abs/2601.19079)
*Naqash Afzal,Niklas Funk,Erik Helmut,Jan Peters,Benjamin Ward-Cherrier*

Main category: cs.RO

TL;DR: This paper introduces a continuous Braille recognition system using neuromorphic tactile sensors, achieving high accuracy and robustness compared to conventional methods.


<details>
  <summary>Details</summary>
Motivation: Motivated by limitations in conventional robotic Braille readers, such as slow and disruptive scanning methods, and issues in vision-based alternatives that face computation inefficiencies and challenges under real-world conditions.

Method: The system uses an event-based tactile sensor (Evetac) for dynamic contact event encoding, coupled with spatiotemporal segmentation and a lightweight ResNet-based classifier for robust character recognition across varying depths and speeds.

Result: The method achieves >=98% accuracy on Braille character recognition at standard depths, demonstrates word-level accuracy over 90% on physical Braille boards, and generalizes well across scanning speeds and layouts.

Conclusion: Neuromorphic tactile sensing proves to be an efficient and accurate approach for robotic Braille reading, with strong potential for broader applications in tactile perception for assistive and robotic technologies.

Abstract: Conventional robotic Braille readers typically rely on discrete, character-by-character scanning, limiting reading speed and disrupting natural flow. Vision-based alternatives often require substantial computation, introduce latency, and degrade in real-world conditions. In this work, we present a high accuracy, real-time pipeline for continuous Braille recognition using Evetac, an open-source neuromorphic event-based tactile sensor. Unlike frame-based vision systems, the neuromorphic tactile modality directly encodes dynamic contact events during continuous sliding, closely emulating human finger-scanning strategies. Our approach combines spatiotemporal segmentation with a lightweight ResNet-based classifier to process sparse event streams, enabling robust character recognition across varying indentation depths and scanning speeds. The proposed system achieves near-perfect accuracy (>=98%) at standard depths, generalizes across multiple Braille board layouts, and maintains strong performance under fast scanning. On a physical Braille board containing daily-living vocabulary, the system attains over 90% word-level accuracy, demonstrating robustness to temporal compression effects that challenge conventional methods. These results position neuromorphic tactile sensing as a scalable, low latency solution for robotic Braille reading, with broader implications for tactile perception in assistive and robotic applications.

</details>


### [65] [SimTO: A simulation-based topology optimization framework for bespoke soft robotic grippers](https://arxiv.org/abs/2601.19098)
*Kurt Enkera,Josh Pinskier,Marcus Gallagher,David Howard*

Main category: cs.RO

TL;DR: The paper introduces SimTO, a framework for topology optimization of soft robotic grippers designed for feature-rich objects, with customizable morphology tailored to object geometry.


<details>
  <summary>Details</summary>
Motivation: Existing soft robotic grippers struggle with feature-rich objects that lack optimal contact surfaces, making them difficult to grasp and prone to damage.

Method: SimTO uses a contact-based physics simulator to automatically extract load cases for topology optimization, eliminating manual load specification and enabling customized gripper design.

Result: Numerical results show that designs created by SimTO are highly specialized for complex objects and also generalize well to unseen ones.

Conclusion: SimTO demonstrates that automated load extraction and topology optimization can achieve high-resolution, tailored soft grippers for safe and effective handling of complex objects.

Abstract: Soft robotic grippers are essential for grasping delicate, geometrically complex objects in manufacturing, healthcare and agriculture. However, existing grippers struggle to grasp feature-rich objects with high topological variability, including gears with sharp tooth profiles on automotive assembly lines, corals with fragile protrusions, or vegetables with irregular branching structures like broccoli. Unlike simple geometric primitives such as cubes or spheres, feature-rich objects lack a clear "optimal" contact surface, making them both difficult to grasp and susceptible to damage when grasped by existing gripper designs. Safe handling of such objects therefore requires specialized soft grippers whose morphology is tailored to the object's features. Topology optimization offers a promising approach for producing specialized grippers, but its utility is limited by the requirement for pre-defined load cases. For soft grippers interacting with feature-rich objects, these loads arise from hundreds of unpredictable gripper-object contact forces during grasping and are unknown a priori. To address this problem, we introduce SimTO, a framework that enables high-resolution topology optimization by automatically extracting load cases from a contact-based physics simulator, eliminating the need for manual load specification. Given an arbitrary feature-rich object, SimTO produces highly customized soft grippers with fine-grained morphological features tailored to the object geometry. Numerical results show our designs are not only highly specialized to feature-rich objects, but also generalize to unseen objects.

</details>


### [66] [Agree to Disagree: Consensus-Free Flocking under Constraints](https://arxiv.org/abs/2601.19119)
*Peter Travis Jardine,Sidney Givigi*

Main category: cs.RO

TL;DR: This paper presents a method for collective robot flocking that adapts inter-agent distance parameters in scenarios with partially-aligned or conflicting goals, without requiring global communication.


<details>
  <summary>Details</summary>
Motivation: Robots often operate without guarantees of trust or secure communication within multi-agent systems, and traditional flocking assumes uniform inter-agent distances, limiting flexibility in diverse settings.

Method: The authors introduced a constrained collective potential function that allows negotiation of inter-agent distance parameters based solely on local observations, eliminating the need for global information or communication.

Result: The presented approach demonstrates robustness and effectiveness in semi-trust scenarios through simulations, enabling agents to manage partially-aligned or conflicting goals.

Conclusion: This research proposes a more flexible and robust flocking framework that supports negotiation of inter-agent distances using local data, addressing challenges in trust and communication for robot swarm systems.

Abstract: Robots sometimes have to work together with a mixture of partially-aligned or conflicting goals. Flocking - coordinated motion through cohesion, alignment, and separation - traditionally assumes uniform desired inter-agent distances. Many practical applications demand greater flexibility, as the diversity of types and configurations grows with the popularity of multi-agent systems in society. Moreover, agents often operate without guarantees of trust or secure communication. Motivated by these challenges we update well-established frameworks by relaxing this assumption of shared inter-agent distances and constraints. Through a new form of constrained collective potential function, we introduce a solution that permits negotiation of these parameters. In the spirit of the traditional flocking control canon, this negotiation is achieved purely through local observations and does not require any global information or inter-agent communication. The approach is robust to semi-trust scenarios, where neighbouring agents pursue conflicting goals. We validate the effectiveness of the approach through a series of simulations.

</details>


### [67] [Robust Out-of-Order Retrieval for Grid-Based Storage at Maximum Capacity](https://arxiv.org/abs/2601.19144)
*Tzvika Geft,William Zhang,Jingjin Yu,Kostas Bekris*

Main category: cs.RO

TL;DR: The paper presents a framework to optimize the efficiency of automated storage systems by minimizing load relocations under uncertain retrieval sequences, achieving robust storage arrangements.


<details>
  <summary>Details</summary>
Motivation: To improve operational efficiency in automated storage systems while addressing uncertainties in retrieval sequences, a common challenge in logistics such as last-mile distribution centers and shipyards.

Method: The framework focuses on a grid-based storage system and incorporates k-bounded perturbations to model retrieval sequence uncertainty. It uses mathematical proofs and efficient solving techniques to minimize or eliminate relocations.

Result: The framework efficiently handles uncertainty, eliminates relocations for k up to half the grid width, and reduces them by over 50% for k up to the full grid width during experiments.

Conclusion: This framework enhances storage system efficiency under uncertainty, making it practical for logistics applications where retrieval sequences may change.

Abstract: This paper proposes a framework for improving the operational efficiency of automated storage systems under uncertainty. It considers a 2D grid-based storage for uniform-sized loads (e.g., containers, pallets, or totes), which are moved by a robot (or other manipulator) along a collision-free path in the grid. The loads are labeled (i.e., unique) and must be stored in a given sequence, and later be retrieved in a different sequence -- an operational pattern that arises in logistics applications, such as last-mile distribution centers and shipyards. The objective is to minimize the load relocations to ensure efficient retrieval. A previous result guarantees a zero-relocation solution for known storage and retrieval sequences, even for storage at full capacity, provided that the side of the grid through which loads are stored/retrieved is at least 3 cells wide. However, in practice, the retrieval sequence can change after the storage phase. To address such uncertainty, this work investigates \emph{$k$-bounded perturbations} during retrieval, under which any two loads may depart out of order if they are originally at most $k$ positions apart. We prove that a $Θ(k)$ grid width is necessary and sufficient for eliminating relocations at maximum capacity. We also provide an efficient solver for computing a storage arrangement that is robust to such perturbations. To address the higher-uncertainty case where perturbations exceed $k$, a strategy is introduced to effectively minimize relocations. Extensive experiments show that, for $k$ up to half the grid width, the proposed storage-retrieval framework essentially eliminates relocations. For $k$ values up to the full grid width, relocations are reduced by $50\%+$.

</details>


### [68] [iFAN Ecosystem: A Unified AI, Digital Twin, Cyber-Physical Security, and Robotics Environment for Advanced Nuclear Simulation and Operations](https://arxiv.org/abs/2601.19234)
*Youndo Do,Chad Meece,Marc Zebrowitz,Spencer Banks,Myeongjun Choi,Xiaoxu Diao,Kai Tan,Michael Doran,Jason Reed,Fan Zhang*

Main category: cs.RO

TL;DR: The paper develops the Immersive Framework for Advanced Nuclear (iFAN) ecosystem, a virtual testbed for nuclear facilities using AI, robotics, and cybersecurity technologies.


<details>
  <summary>Details</summary>
Motivation: Current nuclear facilities lack dedicated virtual testbeds to evaluate and deploy advanced technologies such as AI, cyber-physical security, and robotic operations.

Method: Introduced the iFAN ecosystem, a digital twin framework designed with a realistic 3D environment, physics-based simulations, and features like virtual reality, reinforcement learning, and real-time data exchange.

Result: The ecosystem provides a high-fidelity virtual testbed enabling pre-deployment verification for plant operation, security, and robotic functionalities.

Conclusion: The iFAN ecosystem is a secure and versatile solution for validating autonomous and cyber-resilient nuclear operations with potential applications in operational scenarios.

Abstract: As nuclear facilities experience digital transformation and advanced reactor development, AI integration, cyber-physical security, and other emerging technologies such as autonomous robot operations are increasingly developed. However, evaluation and deployment is challenged by the lack of dedicated virtual testbeds. The Immersive Framework for Advanced Nuclear (iFAN) ecosystem is developed, a comprehensive digital twin framework with a realistic 3D environment with physics-based simulations. The iFAN ecosystem serves as a high-fidelity virtual testbed for plant operation, cybersecurity, physical security, and robotic operation, as it provides real-time data exchange for pre-deployment verification. Core features include virtual reality, reinforcement learning, radiation simulation, and cyber-physical security. In addition, the paper investigates various applications through potential operational scenarios. The iFAN ecosystem provides a versatile and secure architecture for validating the next generation of autonomous and cyber-resilient nuclear operations.

</details>


### [69] [Tactile Memory with Soft Robot: Robust Object Insertion via Masked Encoding and Soft Wrist](https://arxiv.org/abs/2601.19275)
*Tatsuya Kamijo,Mai Nishimura,Cristian C. Beltran-Hernandez,Nodoka Shibasaki,Masashi Hamaya*

Main category: cs.RO

TL;DR: This paper introduces TaMeSo-bot, a tactile memory-integrated system with a soft robot wrist and MAT$^	ext{3}$ transformer model for robust manipulation in contact-rich tasks, achieving superior performance in real-world peg-in-hole tasks.


<details>
  <summary>Details</summary>
Motivation: To enable safe and flexible manipulation under uncertainty by mimicking human tactile memory and improving robot capabilities for contact-rich tasks.

Method: The TaMeSo-bot system combines a soft wrist for safe contact exploration and a Masked Tactile Trajectory Transformer (MAT$^	ext{3}$) model that learns spatiotemporal sensory representations through masked-token prediction.

Result: Real-robot experiments on peg-in-hole tasks under various conditions demonstrate high success rates and adaptability to unseen scenarios, outperforming baselines.

Conclusion: The approach establishes that integrating tactile memory and spatiotemporal modeling enhances robot manipulation capabilities, paving the way for safe and adaptive tactile-based robotics.

Abstract: Tactile memory, the ability to store and retrieve touch-based experience, is critical for contact-rich tasks such as key insertion under uncertainty. To replicate this capability, we introduce Tactile Memory with Soft Robot (TaMeSo-bot), a system that integrates a soft wrist with tactile retrieval-based control to enable safe and robust manipulation. The soft wrist allows safe contact exploration during data collection, while tactile memory reuses past demonstrations via retrieval for flexible adaptation to unseen scenarios. The core of this system is the Masked Tactile Trajectory Transformer (MAT$^\text{3}$), which jointly models spatiotemporal interactions between robot actions, distributed tactile feedback, force-torque measurements, and proprioceptive signals. Through masked-token prediction, MAT$^\text{3}$ learns rich spatiotemporal representations by inferring missing sensory information from context, autonomously extracting task-relevant features without explicit subtask segmentation. We validate our approach on peg-in-hole tasks with diverse pegs and conditions in real-robot experiments. Our extensive evaluation demonstrates that MAT$^\text{3}$ achieves higher success rates than the baselines over all conditions and shows remarkable capability to adapt to unseen pegs and conditions.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [70] [Automated structural testing of LLM-based agents: methods, framework, and case studies](https://arxiv.org/abs/2601.18827)
*Jens Kohl,Otto Kruse,Youssef Mostafa,Andre Luckow,Karsten Schroer,Thomas Riedl,Ryan French,David Katz,Manuel P. Luitz,Tanrajbir Takher,Ken E. Friedl,Céline Laurent-Winter*

Main category: cs.SE

TL;DR: The paper introduces a structural testing methodology for LLM-based agents, improving automation, defect detection, and testing efficiency.


<details>
  <summary>Details</summary>
Motivation: LLM-based agents interact unsupervised, needing extensive and efficient testing beyond manual, expensive current approaches.

Method: Utilize traces via OpenTelemetry for agent trajectory tracking, employ mocking for reproducible behavior, and add automated test verifications using assertions.

Result: Demonstrated improved testing practices like regression testing, multi-language testing, automated workflows, and faster defect analysis.

Conclusion: Structural testing methods increase testing efficiency, cut costs, enhance automation, and integrate software engineering best practices for LLM-based agents.

Abstract: LLM-based agents are rapidly being adopted across diverse domains. Since they interact with users without supervision, they must be tested extensively. Current testing approaches focus on acceptance-level evaluation from the user's perspective. While intuitive, these tests require manual evaluation, are difficult to automate, do not facilitate root cause analysis, and incur expensive test environments. In this paper, we present methods to enable structural testing of LLM-based agents. Our approach utilizes traces (based on OpenTelemetry) to capture agent trajectories, employs mocking to enforce reproducible LLM behavior, and adds assertions to automate test verification. This enables testing agent components and interactions at a deeper technical level within automated workflows. We demonstrate how structural testing enables the adaptation of software engineering best practices to agents, including the test automation pyramid, regression testing, test-driven development, and multi-language testing. In representative case studies, we demonstrate automated execution and faster root-cause analysis. Collectively, these methods reduce testing costs and improve agent quality through higher coverage, reusability, and earlier defect detection. We provide an open source reference implementation on GitHub.

</details>


### [71] [Reducing False Positives in Static Bug Detection with LLMs: An Empirical Study in Industry](https://arxiv.org/abs/2601.18844)
*Xueying Du,Jiayi Feng,Yi Zou,Wei Xu,Jie Ma,Wei Zhang,Sisi Liu,Xin Peng,Yiling Lou*

Main category: cs.SE

TL;DR: The paper evaluates large language models (LLMs) in reducing false positive alerts in enterprise-level static analysis tools (SATs), using Tencent's dataset.


<details>
  <summary>Details</summary>
Motivation: Despite the effectiveness of SATs in enhancing software quality, high false positive rates limit their practical utility in industrial settings. The work aims to assess LLMs for reducing such inefficiencies.

Method: Datasets from Tencent's enterprise SATs were examined with LLM-based approaches to classify 433 alarms (328 false positives and 105 true positives) and further analyzed alongside developer feedback.

Result: LLM-based and hybrid techniques eliminated 94-98% of false positives with low costs, offering significant advantages in efficiency and financial savings when compared to manual reviews.

Conclusion: LLMs demonstrate significant potential for improving SATs by reducing false positives in industrial contexts, though certain limitations remain to be addressed for broader applicability.

Abstract: Static analysis tools (SATs) are widely adopted in both academia and industry for improving software quality, yet their practical use is often hindered by high false positive rates, especially in large-scale enterprise systems. These false alarms demand substantial manual inspection, creating severe inefficiencies in industrial code review. While recent work has demonstrated the potential of large language models (LLMs) for false alarm reduction on open-source benchmarks, their effectiveness in real-world enterprise settings remains unclear. To bridge this gap, we conduct the first comprehensive empirical study of diverse LLM-based false alarm reduction techniques in an industrial context at Tencent, one of the largest IT companies in China. Using data from Tencent's enterprise-customized SAT on its large-scale Advertising and Marketing Services software, we construct a dataset of 433 alarms (328 false positives, 105 true positives) covering three common bug types. Through interviewing developers and analyzing the data, our results highlight the prevalence of false positives, which wastes substantial manual effort (e.g., 10-20 minutes of manual inspection per alarm). Meanwhile, our results show the huge potential of LLMs for reducing false alarms in industrial settings (e.g., hybrid techniques of LLM and static analysis eliminate 94-98% of false positives with high recall). Furthermore, LLM-based techniques are cost-effective, with per-alarm costs as low as 2.1-109.5 seconds and $0.0011-$0.12, representing orders-of-magnitude savings compared to manual review. Finally, our case analysis further identifies key limitations of LLM-based false alarm reduction in industrial settings.

</details>


### [72] [MulVul: Retrieval-augmented Multi-Agent Code Vulnerability Detection via Cross-Model Prompt Evolution](https://arxiv.org/abs/2601.18847)
*Zihan Wu,Jie Xu,Yun Peng,Chun Yong Chong,Xiaohua Jia*

Main category: cs.SE

TL;DR: The paper introduces 'MulVul', a multi-agent framework for vulnerability detection to address challenges of heterogeneity in patterns and unscalable manual prompt engineering. It uses retrieval-augmented detection and automated prompt generation through cross-model prompt evolution.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) are limited in detecting vulnerabilities due to the heterogeneous nature of vulnerability patterns and the impracticality of manually creating prompts for numerous categories.

Method: The proposed MulVul framework employs a Router agent to categorize inputs into coarse types and Detector agents for fine classification. These agents use retrieval tools for evidence gathering. It also introduces Cross-Model Prompt Evolution for automatically creating effective, specialized prompts.

Result: MulVul achieved 34.79% Macro-F1 on 130 CWE types, outperforming baselines by 41.5%. Cross-model prompt evolution improved performance by 51.6% over manual prompt engineering.

Conclusion: MulVul effectively addresses the limitations of LLMs in vulnerability detection with its multi-agent approach and automated prompt optimization, significantly improving detection accuracy.

Abstract: Large Language Models (LLMs) struggle to automate real-world vulnerability detection due to two key limitations: the heterogeneity of vulnerability patterns undermines the effectiveness of a single unified model, and manual prompt engineering for massive weakness categories is unscalable.
  To address these challenges, we propose \textbf{MulVul}, a retrieval-augmented multi-agent framework designed for precise and broad-coverage vulnerability detection. MulVul adopts a coarse-to-fine strategy: a \emph{Router} agent first predicts the top-$k$ coarse categories and then forwards the input to specialized \emph{Detector} agents, which identify the exact vulnerability types. Both agents are equipped with retrieval tools to actively source evidence from vulnerability knowledge bases to mitigate hallucinations.
  Crucially, to automate the generation of specialized prompts, we design \emph{Cross-Model Prompt Evolution}, a prompt optimization mechanism where a generator LLM iteratively refines candidate prompts while a distinct executor LLM validates their effectiveness. This decoupling mitigates the self-correction bias inherent in single-model optimization.
  Evaluated on 130 CWE types, MulVul achieves 34.79\% Macro-F1, outperforming the best baseline by 41.5\%. Ablation studies validate cross-model prompt evolution, which boosts performance by 51.6\% over manual prompts by effectively handling diverse vulnerability patterns.

</details>


### [73] [Towards Safety-Compliant Transformer Architectures for Automotive Systems](https://arxiv.org/abs/2601.18850)
*Sven Kirchner,Nils Purschke,Chengdong Wu,Alois Knoll*

Main category: cs.SE

TL;DR: This paper develops a safety-focused framework for integrating Transformer-based architectures into automotive systems by leveraging multimodal sensor inputs for robustness and fault tolerance.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of safely integrating Transformer-based architectures into critical applications like autonomous driving, where reliability and functional safety are paramount.

Method: Proposes an architecture with independent modality-specific encoders that fuse into a shared latent space, designed to maintain functionality even when one sensor modality fails.

Result: The approach achieves consistent scene understanding by embedding redundancy and diversity in representations, demonstrating robustness and compatibility with established safety practices.

Conclusion: This framework enhances the reliability of Transformer models in safety-critical systems, providing a path toward certifiable AI in autonomous vehicles.

Abstract: Transformer-based architectures have shown remarkable performance in vision and language tasks but pose unique challenges for safety-critical applications. This paper presents a conceptual framework for integrating Transformers into automotive systems from a safety perspective. We outline how multimodal Foundation Models can leverage sensor diversity and redundancy to improve fault tolerance and robustness. Our proposed architecture combines multiple independent modality-specific encoders that fuse their representations into a shared latent space, supporting fail-operational behavior if one modality degrades. We demonstrate how different input modalities could be fused in order to maintain consistent scene understanding. By structurally embedding redundancy and diversity at the representational level, this approach bridges the gap between modern deep learning and established functional safety practices, paving the way for certifiable AI systems in autonomous driving.

</details>


### [74] [The Opaque Pointer Design Pattern in Python: Towards a Pythonic PIMPL for Modularity, Encapsulation, and Stability](https://arxiv.org/abs/2601.19065)
*Antonios Saravanos,John Pazarzis,Stavros Zervoudakis,Dongnanzi Zheng*

Main category: cs.SE

TL;DR: The paper revisits and applies the C++ PIMPL idiom in Python to ensure stable APIs while enhancing modularity and maintainability.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge Python libraries face in maintaining stable public APIs amidst internal changes and evolving dependencies while avoiding reliance on unintended “reachable internals.”

Method: The authors reinterpret the PIMPL idiom in Python, situating it within encapsulation techniques and demonstrating its application to isolate dependencies, enable lazy imports, and allow runtime backend selection.

Result: The study identifies PIMPL-like structures already used in Python ecosystems and demonstrates how this idiom effectively tackles risks of API instability and improves long-term library maintenance.

Conclusion: The PIMPL idiom, adapted to Python, proves to be a useful pattern for stable, maintainable libraries, with practical guidance provided for its appropriate and effective use.

Abstract: Python libraries often need to maintain a stable public API even as internal implementations evolve, gain new backends, or depend on heavy optional libraries. In Python, where internal objects are easy to inspect and import, users can come to rely on "reachable internals" that were never intended to be public, making refactoring risky and slowing long-term maintenance. This paper revisits the pointer-to-implementation (PIMPL) idiom from C++ and reinterprets it as a Pythonic pattern of opaque delegation: a small public object (or module) that delegates its behavior to a separate implementation object treated as internal. We situate this pattern within a broader taxonomy of encapsulation techniques in Python, relate it to existing practices such as module-level indirection, facade objects, and backend dispatch, and identify PIMPL-like structures already used in the standard library and the scientific Python ecosystem. We then show how a Pythonic PIMPL can be used in existing codebases to isolate heavy dependencies, support lazy imports, and enable runtime selection of alternative backends without changing the public API. Finally, we discuss the benefits and trade-offs of the approach and offer practical guidance on when the pattern is appropriate and how to apply it in large, long-lived Python libraries.

</details>


### [75] [Tricky$^2$: Towards a Benchmark for Evaluating Human and LLM Error Interactions](https://arxiv.org/abs/2601.18949)
*Cole Granger,Dipin Khati,Daniel Rodriguez-Cardenas,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: This paper introduces Tricky$^2$, a dataset combining human-written and LLM-generated bugs in C++, Python, and Java programs to study mixed-origin error behaviors.


<details>
  <summary>Details</summary>
Motivation: To understand how human-introduced bugs and LLM-generated bugs interact, and to improve robustness and reliability in hybrid human-machine code.

Method: Created Tricky$^2$ by augmenting the existing TrickyBugs dataset with bugs generated by GPT-5 and OpenAI-oss-20b, using a taxonomy-guided prompting framework.

Result: Generated a dataset containing human-only, LLM-only, and mixed-origin bugs, along with small-scale baseline evaluations for classification, localization, and repair tasks.

Conclusion: Tricky$^2$ enables detailed analysis of mixed-origin code errors and enhances research in detecting, classifying, and repairing bugs in hybrid human-machine code.

Abstract: Large language models (LLMs) are increasingly integrated into software development workflows, yet they often introduce subtle logic or data-misuse errors that differ from human bugs. To study how these two error types interact, we construct Tricky$^2$, a hybrid dataset that augments the existing TrickyBugs corpus of human-written defects with errors injected by both GPT-5 and OpenAI-oss-20b across C++, Python, and Java programs. Our approach uses a taxonomy-guided prompting framework to generate machine-originated bugs while preserving original human defects and program structure. The resulting corpus spans human-only, LLM-only, and human+LLM splits, enabling analysis of mixed-origin error behavior, multi-bug repair robustness, and reliability in hybrid human-machine code. This paper outlines the dataset construction pipeline and illustrates its use through small-scale baseline evaluations of classification, localization, and repair tasks.

</details>


### [76] [Dynamic Cogeneration of Bug Reproduction Test in Agentic Program Repair](https://arxiv.org/abs/2601.19066)
*Runxiang Cheng,Michele Tufano,José Cambronero,Renyao Wei,Sherry Shi,Grant Uy,Pat Rondon,Franjo Ivančić*

Main category: cs.SE

TL;DR: This paper investigates cogeneration strategies in agentic Automated Program Repair (APR), where fixes and Bug Reproduction Tests (BRTs) are generated simultaneously, showing it improves efficiency and confidence in patches.


<details>
  <summary>Details</summary>
Motivation: Developers desire Bug Reproduction Tests (BRTs) alongside fixes, but current APR systems often generate them separately or focus only on fixes.

Method: The study evaluates cogeneration strategies on 120 human-reported bugs, develops patch selectors considering test changes, and analyzes failures to improve APR agent behavior.

Result: Cogeneration generates BRTs for as many bugs as dedicated agents without reducing plausible fix generation, simplifying APR pipeline coordination and reducing engineering effort.

Conclusion: Cogeneration in agentic APR supports the generation of both fixes and BRTs effectively, streamlining workflows and maintaining patch quality.

Abstract: Bug Reproduction Tests (BRTs) have been used in many agentic Automated Program Repair (APR) systems, primarily for validating promising fixes and aiding fix generation. In practice, when developers submit a patch, they often implement the BRT alongside the fix. Our experience deploying agentic APR reveals that developers similarly desire a BRT within AI-generated patches to increase their confidence. However, canonical APR systems tend to generate BRTs and fixes separately, or focus on producing only the fix in the final patch. In this paper, we study agentic APR in the context of cogeneration, where the APR agent is instructed to generate both a fix and a BRT in the same patch. We evaluate the effectiveness of different cogeneration strategies on 120 human-reported bugs at Google and characterize different cogeneration strategies by their influence on APR agent behavior. We develop and evaluate patch selectors that account for test change information to select patches with plausible fixes (and plausible BRTs). Finally, we analyze the root causes of failed cogeneration trajectories. Importantly, we show that cogeneration allows the APR agent to generate BRTs for at least as many bugs as a dedicated BRT agent, without compromising the generation rate of plausible fixes, thereby reducing engineering effort in maintaining and coordinating separate generation pipelines for fix and BRT at scale.

</details>


### [77] [HalluJudge: A Reference-Free Hallucination Detection for Context Misalignment in Code Review Automation](https://arxiv.org/abs/2601.19072)
*Kla Tantithamthavorn,Hong Yi Lin,Patanamon Thongtanunam,Wachiraphan Charoenwet,Minwoo Jeong,Ming Wu*

Main category: cs.SE

TL;DR: The paper presents HalluJudge, a framework for detecting hallucinations in LLM-generated code review comments by assessing context alignment, showing effective results with high accuracy and low cost.


<details>
  <summary>Details</summary>
Motivation: LLMs in code review automation suffer from hallucinations, which hinder their practical adoption. An effective method to detect and reduce hallucinations is needed to build trust in these tools.

Method: The authors introduce HalluJudge, implementing strategies like direct assessment and multi-branch reasoning (Tree-of-Thoughts) to evaluate hallucinations in code review comments. They tested these strategies in real-world, large-scale software projects.

Result: HalluJudge achieves a high F1 score of 0.85 and an average cost of $0.009 per assessment. It also aligns with developer preferences 67% of the time during real-world evaluations.

Conclusion: HalluJudge proves to be a cost-effective solution for supporting LLM adoption by mitigating hallucinations in code review comments, thereby increasing developer trust and workflow reliability.

Abstract: Large Language models (LLMs) have shown strong capabilities in code review automation, such as review comment generation, yet they suffer from hallucinations -- where the generated review comments are ungrounded in the actual code -- poses a significant challenge to the adoption of LLMs in code review workflows. To address this, we explore effective and scalable methods for a hallucination detection in LLM-generated code review comments without the reference. In this work, we design HalluJudge that aims to assess the grounding of generated review comments based on the context alignment. HalluJudge includes four key strategies ranging from direct assessment to structured multi-branch reasoning (e.g., Tree-of-Thoughts). We conduct a comprehensive evaluation of these assessment strategies across Atlassian's enterprise-scale software projects to examine the effectiveness and cost-efficiency of HalluJudge. Furthermore, we analyze the alignment between HalluJudge's judgment and developer preference of the actual LLM-generated code review comments in the real-world production. Our results show that the hallucination assessment in HalluJudge is cost-effective with an F1 score of 0.85 and an average cost of $0.009. On average, 67% of the HalluJudge assessments are aligned with the developer preference of the actual LLM-generated review comments in the online production. Our results suggest that HalluJudge can serve as a practical safeguard to reduce developers' exposure to hallucinated comments, fostering trust in AI-assisted code reviews.

</details>


### [78] [Hybrid Fault-Driven Mutation Testing for Python](https://arxiv.org/abs/2601.19088)
*Saba Alimadadi,Golnaz Gharachorlu*

Main category: cs.SE

TL;DR: This paper introduces PyTation, a mutation testing tool for Python that generates more diverse mutants by incorporating seven novel mutation operators inspired by Python anti-patterns, and evaluates its effectiveness on open-source projects.


<details>
  <summary>Details</summary>
Motivation: Existing mutation testing techniques fail to effectively capture common faults in dynamically typed languages like Python due to the lack of tailored operators for these languages.

Method: The authors developed seven new mutation operators reflecting Python-specific anti-patterns, used a hybrid static and dynamic analysis approach to minimize equivalent mutants, and implemented these ideas in a tool called PyTation, which was evaluated on 13 open-source Python applications.

Result: PyTation created unique mutants that complemented general-purpose mutation tools, demonstrated distinct behavior during tests, revealed gaps in high-coverage test suites, and had low equivalent mutants, cross-kill rates, and test overlaps.

Conclusion: PyTation broadens the fault model in mutation testing for Python, effectively improving the assessment of test suite adequacy with an innovative approach.

Abstract: Mutation testing is an effective technique for assessing the effectiveness of test suites by systematically injecting artificial faults into programs. However, existing mutation testing techniques fall short in capturing many types of common faults in dynamically typed languages like Python. In this paper, we introduce a novel set of seven mutation operators that are inspired by prevalent anti-patterns in Python programs, designed to complement the existing general-purpose operators and broaden the spectrum of simulated faults. We propose a mutation testing technique that utilizes a hybrid of static and dynamic analyses to mutate Python programs based on these operators while minimizing equivalent mutants. We implement our approach in a tool called PyTation and evaluate it on 13 open-source Python applications. Our results show that PyTation generates mutants that complement those from general-purpose tools, exhibiting distinct behaviour under test execution and uncovering inadequacies in high-coverage test suites. We further demonstrate that PyTation produces a high proportion of unique mutants, a low cross-kill rate, and a low test overlap ratio relative to baseline tools, highlighting its novel fault model. PyTation also incurs few equivalent mutants, aided by dynamic analysis heuristics.

</details>


### [79] [Reward Engineering for Reinforcement Learning in Software Tasks](https://arxiv.org/abs/2601.19100)
*Md Rayhanul Masud,Azmine Toushik Wasi,Salman Rahman,Md Rizwan Parvez*

Main category: cs.SE

TL;DR: The paper surveys reward engineering for reinforcement learning in software tasks like code generation and optimization, outlining existing methods and challenges.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning faces challenges in software tasks due to the difficulty in designing effective reward signals, as rewards often rely on proxies rather than clear numeric objectives.

Method: The authors provide a systematic review of literature, categorizing reward design along three dimensions and summarizing approaches used for reinforcement learning in software tasks.

Result: The paper identifies and organizes reward design approaches, presenting a comprehensive overview of the methods applied to RL in software engineering.

Conclusion: The study highlights the fragmented nature of reward design research, offers structure to the field, and proposes challenges and recommendations for future work in reinforcement learning for software tasks.

Abstract: Reinforcement learning is increasingly used for code-centric tasks. These tasks include code generation, summarization, understanding, repair, testing, and optimization. This trend is growing faster with large language models and autonomous agents. A key challenge is how to design reward signals that make sense for software. In many RL problems, the reward is a clear number. In software, this is often not possible. The goal is rarely a single numeric objective. Instead, rewards are usually proxies. Common proxies check if the code compiles, passes tests, or satisfies quality metrics. Many reward designs have been proposed for code-related tasks. However, the work is scattered across areas and papers. There is no single survey that brings these approaches together and shows the full landscape of reward design for RL in software. In this survey, we provide the first systematic and comprehensive review of reward engineering for RL in software tasks. We focus on existing methods and techniques. We structure the literature along three complementary dimensions, summarizing the reward-design choices within each. We conclude with challenges and recommendations in the reward design space for SE tasks.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [80] [Schema-based active inference supports rapid generalization of experience and frontal cortical coding of abstract structure](https://arxiv.org/abs/2601.18946)
*Toon Van de Maele,Tim Verbelen,Dileep George,Giovanni Pezzulo*

Main category: q-bio.NC

TL;DR: This paper presents S-HAI, a computational framework to simulate schema-based learning and generalization.


<details>
  <summary>Details</summary>
Motivation: To understand the computational principles and neural mechanisms behind schema formation and its role in cognition.

Method: The S-HAI framework integrates hierarchical active inference and predictive processing to model schema-dependent behaviors and neural mechanisms through simulations.

Result: S-HAI reproduced behaviors like schema-based generalization in spatial tasks and neural patterns seen in medial prefrontal cortex and hippocampus.

Conclusion: S-HAI provides a mechanistic link between schema-based behavior, neuroscience data, and theory, pointing to hierarchical predictive processing across cortical and hippocampal circuits as key to schema generalization.

Abstract: Schemas -- abstract relational structures that capture the commonalities across experiences -- are thought to underlie humans' and animals' ability to rapidly generalize knowledge, rebind new experiences to existing structures, and flexibly adapt behavior across contexts. Despite their central role in cognition, the computational principles and neural mechanisms supporting schema formation and use remain elusive. Here, we introduce schema-based hierarchical active inference (S-HAI), a novel computational framework that combines predictive processing and active inference with schema-based mechanisms. In S-HAI, a higher-level generative model encodes abstract task structure, while a lower-level model encodes spatial navigation, with the two levels linked by a grounding likelihood that maps abstract goals to physical locations. Through a series of simulations, we show that S-HAI reproduces key behavioral signatures of rapid schema-based generalization in spatial navigation tasks, including the ability to flexibly remap abstract schemas onto novel contexts, resolve goal ambiguity, and balance reuse versus accommodation of novel mappings. Crucially, S-HAI also reproduces prominent neural codes reported in rodent medial prefrontal cortex during a schema-dependent navigation and decision task, including task-invariant goal-progress cells, goal-identity cells, and goal-and-spatially conjunctive cells, as well as place-like codes at the lower level. Taken together, these results provide a mechanistic account of schema-based learning and inference that bridges behavior, neural data, and theory. More broadly, our findings suggest that schema formation and generalization may arise from predictive processing principles implemented hierarchically across cortical and hippocampal circuits, enabling the generalization of experience.

</details>


### [81] [Smooth embeddings in contracting recurrent networks driven by regular dynamics: A synthesis for neural representation](https://arxiv.org/abs/2601.19019)
*Vikas N. O'Reilly-Shah,Alessandro Maria Selvitella*

Main category: q-bio.NC

TL;DR: This paper addresses how trained recurrent neural networks (RNNs) create smooth, topology-preserving latent representations when predicting time-series data, specifically focusing on recurrent networks handling regular dynamical structures.


<details>
  <summary>Details</summary>
Motivation: The authors aim to understand when and why trained RNNs develop latent embeddings that preserve the dynamical structure of the input, providing clarity and testable predictions about their representational capabilities.

Method: The framework combines insights: synchronization and embedding theory for contracting reservoirs, regularity mechanisms ensuring smooth synchronization maps, and a base-system perspective treating the input's invariant manifold as the driver.

Result: The research synthesizes diverse empirical findings and theoretical results into unified expectations for when RNN representations preserve topological properties, including insights into state dimensionality scaling with intrinsic input dynamics.

Conclusion: RNNs trained for time-series prediction can form reliable, topology-preserving latent representations under specific conditions, with implications for their design and analysis in processing regular dynamical systems.

Abstract: Recurrent neural networks trained for time-series prediction often develop latent trajectories that preserve qualitative structure of the dynamical systems generating their inputs. Recent empirical work has documented topology-preserving latent organization in trained recurrent models, and recent theoretical results in reservoir computing establish conditions under which the synchronization map is an embedding. Here we synthesize these threads into a unified account of when contracting recurrent networks yield smooth, topology-preserving internal representations for a broad and biologically relevant class of inputs: regular dynamics on invariant circles and tori.
  Our contribution is an integrated framework that assembles (i) generalized synchronization and embedding guarantees for contracting reservoirs, (ii) regularity mechanisms ensuring differentiability of the synchronization map under mild constraints, and (iii) a base-system viewpoint in which the invariant manifold generating the input stream is treated as the driving system. In this regular setting, the conditions commonly viewed as restrictive in chaotic-attractor analyses become mild and readily satisfied by standard contractive architectures. The framework clarifies how representational content in recurrent circuits is inherently historical: the network state encodes finite windows of input history rather than instantaneous stimuli.
  By consolidating disparate empirical and theoretical results under common assumptions, the synthesis yields concrete, testable expectations about when prediction-trained recurrent circuits should (or should not) form smooth latent embeddings and how required state dimension scales with the intrinsic dimension of the driving dynamics.

</details>


### [82] [Stroboscopic motion reversals in delay-coupled neural fields](https://arxiv.org/abs/2601.19125)
*Noah Parks,Zachary P Kilpatrick*

Main category: q-bio.NC

TL;DR: The paper presents a delay-coupled neural field model to explain visual illusions like stroboscopic percepts and the wagon-wheel illusion, highlighting the role of signal propagation delays.


<details>
  <summary>Details</summary>
Motivation: The authors aim to understand and mechanistically explain visual illusions by leveraging neural dynamics and how delays in neural activity shape perceptual outcomes.

Method: A delay-coupled neural field model was introduced, which uses ring-like neural modules representing angular preferences with localized and delayed coupling. Asymptotic reductions simplify the system into delay differential equations to analyze dynamics.

Result: The study found that neural delays create multiple traveling bump solutions with quantized speeds, influenced by external pulsed inputs that mimic observed perceptual transitions in visual illusions.

Conclusion: The research elucidates how neural delays contribute to organizing visual perception into distinct dynamical states, offering a mechanistic understanding of stroboscopic visual illusions.

Abstract: Visual illusions provide a window into the mechanisms underlying visual processing, and dynamical neural circuit models offer a natural framework for proposing and testing theories of their emergence. We propose and analyze a delay-coupled neural field model that explains stroboscopic percepts arising from the subsampling of a moving, often rotating, stimulus, such as the wagon-wheel illusion. Motivated by the role of activity propagation delays in shaping visual percepts, we study neural fields with both uniform and spatially dependent delays, representing the finite time required for signals to travel along axonal projections. Each module is organized as a ring of neurons encoding angular preference, with instantaneous local coupling and delayed long-range coupling strongest between neurons with similar preference. We show that delays generate a family of coexisting traveling bump solutions with distinct, quantized propagation speeds. Using interface-based asymptotic methods, we reduce the neural field dynamics to a low-dimensional system of coupled delay differential equations, enabling a detailed analysis of speed selection, stability, entrainment, and state transitions. Regularly pulsed inputs induce transitions between distinct speed states, including motion opposite to the forcing direction, capturing key features of visual aliasing and stroboscopic motion reversal. These results demonstrate how delayed neural interactions organize perception into discrete dynamical states and provide a mechanistic explanation for stroboscopic visual illusions.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [83] [Statistical Inference for Explainable Boosting Machines](https://arxiv.org/abs/2601.18857)
*Haimo Fang,Kevin Tan,Jonathan Pipping,Giles Hooker*

Main category: stat.ML

TL;DR: The paper presents a new approach for explainable boosting machines (EBMs), enabling efficient statistical inference, confidence intervals, and overcoming dimensionality issues.


<details>
  <summary>Details</summary>
Motivation: To address the computational intensity of bootstrapping in EBMs for uncertainty quantification, making it difficult to determine the importance of features.

Method: Introduces Boulevard regularization using a moving average, converging to feature-wise kernel ridge regression and providing minimax-optimal MSE for fitting Lipschitz GAMs. Constructs prediction and confidence intervals efficiently.

Result: Achieves minimax-optimal mean squared error rate, asymptotically normal predictions, and constructs prediction/confidence intervals without depending on dataset size during runtime.

Conclusion: The proposed approach enhances explainability, uncertainty quantification, and efficiency for EBMs, avoiding computational intensity and dimensionality challenges.

Abstract: Explainable boosting machines (EBMs) are popular "glass-box" models that learn a set of univariate functions using boosting trees. These achieve explainability through visualizations of each feature's effect. However, unlike linear model coefficients, uncertainty quantification for the learned univariate functions requires computationally intensive bootstrapping, making it hard to know which features truly matter. We provide an alternative using recent advances in statistical inference for gradient boosting, deriving methods for statistical inference as well as end-to-end theoretical guarantees. Using a moving average instead of a sum of trees (Boulevard regularization) allows the boosting process to converge to a feature-wise kernel ridge regression. This produces asymptotically normal predictions that achieve the minimax-optimal mean squared error for fitting Lipschitz GAMs with $p$ features at rate $O(pn^{-2/3})$, successfully avoiding the curse of dimensionality. We then construct prediction intervals for the response and confidence intervals for each learned univariate function with a runtime independent of the number of datapoints, enabling further explainability within EBMs.

</details>


### [84] [Implicit Q-Learning and SARSA: Liberating Policy Control from Step-Size Calibration](https://arxiv.org/abs/2601.18907)
*Hwanwoo Kim,Eric Laber*

Main category: stat.ML

TL;DR: The paper proposes adaptive variants of Q-learning and SARSA using implicit methods to automatically adjust step-sizes, ensuring stability and reducing sensitivity to manual calibration.


<details>
  <summary>Details</summary>
Motivation: Step-size calibration in Q-learning and SARSA is critical as improper values can cause instability or slow learning.

Method: Implicit methods reformulate iterative updates as fixed-point equations, enabling adaptive step-size adjustment inversely scaled with feature norms.

Result: Empirical validation shows these implicit methods reduce sensitivity to step-sizes, maintain stability for broad step-size ranges, and outperform standard methods in discrete and continuous state benchmarks.

Conclusion: Implicit Q-learning and SARSA provide stable performance and wider applicability, allowing for larger step-sizes and eliminating the need for manual tuning of step-sizes.

Abstract: Q-learning and SARSA are foundational reinforcement learning algorithms whose practical success depends critically on step-size calibration. Step-sizes that are too large can cause numerical instability, while step-sizes that are too small can lead to slow progress. We propose implicit variants of Q-learning and SARSA that reformulate their iterative updates as fixed-point equations. This yields an adaptive step-size adjustment that scales inversely with feature norms, providing automatic regularization without manual tuning. Our non-asymptotic analyses demonstrate that implicit methods maintain stability over significantly broader step-size ranges. Under favorable conditions, it permits arbitrarily large step-sizes while achieving comparable convergence rates. Empirical validation across benchmark environments spanning discrete and continuous state spaces shows that implicit Q-learning and SARSA exhibit substantially reduced sensitivity to step-size selection, achieving stable performance with step-sizes that would cause standard methods to fail.

</details>


### [85] [Collaborative Compressors in Distributed Mean Estimation with Limited Communication Budget](https://arxiv.org/abs/2601.18950)
*Harsh Vardhan,Arya Mazumdar*

Main category: stat.ML

TL;DR: This paper introduces four collaborative compression schemes for distributed high-dimensional mean estimation, optimizing communication under limited knowledge of vector correlations.


<details>
  <summary>Details</summary>
Motivation: Address the communication constraints in distributed optimization where traditional methods fail to utilize vector similarities due to independent compression.

Method: Proposed four collaborative compression schemes that exploit vector similarities without requiring prior knowledge of correlations, and analyzed these based on their $l_2$, $l_infty$, and cosine estimation errors.

Result: The schemes provide significant communication savings while maintaining performance, with error analysis showing a dependence on vector similarity.

Conclusion: The proposed schemes offer computationally efficient solutions for communication-constrained environments, agnostically leveraging vector similarities while providing robust error analysis.

Abstract: Distributed high dimensional mean estimation is a common aggregation routine used often in distributed optimization methods. Most of these applications call for a communication-constrained setting where vectors, whose mean is to be estimated, have to be compressed before sharing. One could independently encode and decode these to achieve compression, but that overlooks the fact that these vectors are often close to each other. To exploit these similarities, recently Suresh et al., 2022, Jhunjhunwala et al., 2021, Jiang et al, 2023, proposed multiple correlation-aware compression schemes. However, in most cases, the correlations have to be known for these schemes to work. Moreover, a theoretical analysis of graceful degradation of these correlation-aware compression schemes with increasing dissimilarity is limited to only the $\ell_2$-error in the literature. In this paper, we propose four different collaborative compression schemes that agnostically exploit the similarities among vectors in a distributed setting. Our schemes are all simple to implement and computationally efficient, while resulting in big savings in communication. The analysis of our proposed schemes show how the $\ell_2$, $\ell_\infty$ and cosine estimation error varies with the degree of similarity among vectors.

</details>


### [86] [Convergence of Muon with Newton-Schulz](https://arxiv.org/abs/2601.19156)
*Gyu Yeol Kim,Min-hwan Oh*

Main category: stat.ML

TL;DR: The paper analyzes the Muon method with momentum orthogonalization via Newton-Schulz steps, showing its convergence rate and efficiency compared to exact SVD-based methods.


<details>
  <summary>Details</summary>
Motivation: To bridge the theory-practice gap in the Muon optimization method and justify the use of Newton-Schulz steps for efficient orthogonalization in practical applications.

Method: The study performs theoretical analysis comparing Muon using Newton-Schulz steps against an idealized version with SVD-based polar factorization, examining convergence rates and efficiency.

Result: Muon with Newton-Schulz matches the behavior of SVD-based polar methods efficiently in less computation time, and the constant factor of its convergence improves with steps and polynomial degree.

Conclusion: The practical design of Muon using Newton-Schulz is justified by these results, as it balances computational efficiency with theoretical performance equivalence.

Abstract: We analyze Muon as originally proposed and used in practice -- using the momentum orthogonalization with a few Newton-Schulz steps. The prior theoretical results replace this key step in Muon with an exact SVD-based polar factor. We prove that Muon with Newton-Schulz converges to a stationary point at the same rate as the SVD-polar idealization, up to a constant factor for a given number $q$ of Newton-Schulz steps. We further analyze this constant factor and prove that it converges to 1 doubly exponentially in $q$ and improves with the degree of the polynomial used in Newton-Schulz for approximating the orthogonalization direction. We also prove that Muon removes the typical square-root-of-rank loss compared to its vector-based counterpart, SGD with momentum. Our results explain why Muon with a few low-degree Newton-Schulz steps matches exact-polar (SVD) behavior at a much faster wall-clock time and explain how much momentum matrix orthogonalization via Newton-Schulz benefits over the vector-based optimizer. Overall, our theory justifies the practical Newton-Schulz design of Muon, narrowing its practice-theory gap.

</details>


### [87] [Double Fairness Policy Learning: Integrating Action Fairness and Outcome Fairness in Decision-making](https://arxiv.org/abs/2601.19186)
*Zeyu Bian,Lan Wang,Chengchun Shi,Zhengling Qi*

Main category: stat.ML

TL;DR: The paper introduces a double fairness learning (DFL) framework addressing action fairness, outcome fairness, and value maximization in policy learning.


<details>
  <summary>Details</summary>
Motivation: Fairness in trustworthy machine learning requires exploration in policy learning due to its interventional nature and distinct fairness targets.

Method: The authors propose a double fairness learning framework using multi-objective optimization and lexicographic weighted Tchebyshev methods to balance fairness and value maximization.

Result: DFL enhances action and outcome fairness while slightly reducing value, as demonstrated by simulations and practical applications.

Conclusion: DFL offers an effective approach to integrate fairness into policy learning, balancing objectives with theoretical guarantees.

Abstract: Fairness is a central pillar of trustworthy machine learning, especially in domains where accuracy- or profit-driven optimization is insufficient. While most fairness research focuses on supervised learning, fairness in policy learning remains less explored. Because policy learning is interventional, it induces two distinct fairness targets: action fairness (equitable action assignments) and outcome fairness (equitable downstream consequences). Crucially, equalizing actions does not generally equalize outcomes when groups face different constraints or respond differently to the same action. We propose a novel double fairness learning (DFL) framework that explicitly manages the trade-off among three objectives: action fairness, outcome fairness, and value maximization. We integrate fairness directly into a multi-objective optimization problem for policy learning and employ a lexicographic weighted Tchebyshev method that recovers Pareto solutions beyond convex settings, with theoretical guarantees on the regret bounds. Our framework is flexible and accommodates various commonly used fairness notions. Extensive simulations demonstrate improved performance relative to competing methods. In applications to a motor third-party liability insurance dataset and an entrepreneurship training dataset, DFL substantially improves both action and outcome fairness while incurring only a modest reduction in overall value.

</details>


### [88] [Regularized $f$-Divergence Kernel Tests](https://arxiv.org/abs/2601.19755)
*Mónica Ribero,Antonin Schrab,Arthur Gretton*

Main category: stat.ML

TL;DR: The paper introduces a framework for constructing kernel-based two-sample tests using the $f$-divergences family, with adaptivity, theoretical guarantees, and application focus.


<details>
  <summary>Details</summary>
Motivation: To improve two-sample testing by leveraging $f$-divergences and addressing needs in sensitive contexts like differential privacy auditing and machine unlearning evaluation.

Method: The proposed method uses kernel methods to estimate the witness function of a regularized variational representation of $f$-divergences, with adaptivity in hyperparameters such as kernel bandwidth and regularization.

Result: Experiments show diverse $f$-divergences detect different localized differences, aiding sensitivity in applications such as unlearning failure evaluation.

Conclusion: The framework effectively leverages $f$-divergences for diverse statistical testing and specific applications like privacy auditing and unlearning evaluation, offering localized insights.

Abstract: We propose a framework to construct practical kernel-based two-sample tests from the family of $f$-divergences. The test statistic is computed from the witness function of a regularized variational representation of the divergence, which we estimate using kernel methods. The proposed test is adaptive over hyperparameters such as the kernel bandwidth and the regularization parameter. We provide theoretical guarantees for statistical test power across our family of $f$-divergence estimates. While our test covers a variety of $f$-divergences, we bring particular focus to the Hockey-Stick divergence, motivated by its applications to differential privacy auditing and machine unlearning evaluation. For two-sample testing, experiments demonstrate that different $f$-divergences are sensitive to different localized differences, illustrating the importance of leveraging diverse statistics. For machine unlearning, we propose a relative test that distinguishes true unlearning failures from safe distributional variations.

</details>


### [89] [Revisiting Incremental Stochastic Majorization-Minimization Algorithms with Applications to Mixture of Experts](https://arxiv.org/abs/2601.19811)
*TrungKhang Tran,TrungTin Nguyen,Gersende Fort,Tung Doan,Hien Duy Nguyen,Binh T. Nguyen,Florence Forbes,Christopher Drovandi*

Main category: stat.ML

TL;DR: This paper revisits and analyzes the incremental stochastic Majorization-Minimization (MM) algorithm, showing its broader applicability and superior performance compared to popular stochastic optimization methods.


<details>
  <summary>Details</summary>
Motivation: Standard batch-mode algorithms are impractical for high-volume streaming data, leading to interest in incremental stochastic methods for flexibility and efficiency.

Method: The paper generalizes the incremental stochastic Expectation-Maximization algorithm into the MM framework, proving theoretical guarantees of convergence and demonstrating its advantages empirically.

Result: The MM approach outperforms other stochastic optimizers like SGD, RMSProp, and Adam, validated on synthetic and real-world datasets, including a bioinformatics case study.

Conclusion: Incremental stochastic MM demonstrates significant promise for heterogeneous data modeling and inspires further algorithm development in machine learning.

Abstract: Processing high-volume, streaming data is increasingly common in modern statistics and machine learning, where batch-mode algorithms are often impractical because they require repeated passes over the full dataset. This has motivated incremental stochastic estimation methods, including the incremental stochastic Expectation-Maximization (EM) algorithm formulated via stochastic approximation. In this work, we revisit and analyze an incremental stochastic variant of the Majorization-Minimization (MM) algorithm, which generalizes incremental stochastic EM as a special case. Our approach relaxes key EM requirements, such as explicit latent-variable representations, enabling broader applicability and greater algorithmic flexibility. We establish theoretical guarantees for the incremental stochastic MM algorithm, proving consistency in the sense that the iterates converge to a stationary point characterized by a vanishing gradient of the objective. We demonstrate these advantages on a softmax-gated mixture of experts (MoE) regression problem, for which no stochastic EM algorithm is available. Empirically, our method consistently outperforms widely used stochastic optimizers, including stochastic gradient descent, root mean square propagation, adaptive moment estimation, and second-order clipped stochastic optimization. These results support the development of new incremental stochastic algorithms, given the central role of softmax-gated MoE architectures in contemporary deep neural networks for heterogeneous data modeling. Beyond synthetic experiments, we also validate practical effectiveness on two real-world datasets, including a bioinformatics study of dent maize genotypes under drought stress that integrates high-dimensional proteomics with ecophysiological traits, where incremental stochastic MM yields stable gains in predictive performance.

</details>


<div id='q-fin.MF'></div>

# q-fin.MF [[Back]](#toc)

### [90] [Prediction Markets as Bayesian Inverse Problems: Uncertainty Quantification, Identifiability, and Information Gain from Price-Volume Histories under Latent Types](https://arxiv.org/abs/2601.18815)
*Juan Pablo Madrigal-Cianci,Camilo Monsalve Maya,Lachlan Breakey*

Main category: q-fin.MF

TL;DR: The paper models prediction markets as Bayesian inverse problems, utilizing price and volume data to infer event outcomes while addressing uncertainty quantification, identifiability, and stability.


<details>
  <summary>Details</summary>
Motivation: The aim is to better understand how prediction markets aggregate dispersed private information into observable market histories, despite the noise and strategic behavior involved.

Method: The paper formulates a Bayesian inverse framework with a mechanism-agnostic observation model in log-odds space, addressing informed/uninformed trading, noise, and adversarial flow. Identifiability, uncertainty quantification, and stability are analyzed using Kullback-Leibler criteria and statistical bounds.

Result: The framework provides diagnostics for understanding when market histories are informative versus ill-posed, validated with synthetic data regarding posterior concentration rates and identifiability thresholds.

Conclusion: The Bayesian inverse formulation offers a robust tool to analyze the informativeness and reliability of prediction markets, addressing challenges like type-composition confounding and stability against perturbations.

Abstract: Prediction markets are often described as mechanisms that ``aggregate information'' into prices, yet the mapping from dispersed private information to observed market histories is typically noisy, endogenous, and shaped by heterogeneous and strategic participation. This paper formulates prediction markets as Bayesian inverse problems in which the unknown event outcome \(Y\in\{0,1\}\) is inferred from an observed history of market-implied probabilities and traded volumes. We introduce a mechanism-agnostic observation model in log-odds space in which price increments conditional on volume arise from a latent mixture of trader types. The resulting likelihood class encompasses informed and uninformed trading, heavy-tailed microstructure noise, and adversarial or manipulative flow, while requiring only price and volume as observables.
  Within this framework we define posterior uncertainty quantification for \(Y\), provide identifiability and well-posedness criteria in terms of Kullback--Leibler separation between outcome-conditional increment laws, and derive posterior concentration statements and finite-sample error bounds under general regularity assumptions. We further study stability of posterior odds to perturbations of the observed price--volume path and define realized and expected information gain via the posterior-vs-prior KL divergence and mutual information. The inverse-problem formulation yields explicit diagnostics for regimes in which market histories are informative and stable versus regimes in which inference is ill-posed due to type-composition confounding or outcome--nuisance symmetries.
  Extensive experiments on synthetic data validate our theoretical predictions regarding posterior concentration rates and identifiability thresholds.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [91] [Accelerating radio astronomy imaging with RICK: a step towards SKA-Mid and SKA-Low](https://arxiv.org/abs/2601.19714)
*Giovanni Lacopo,Emanuele De Rubeis,Claudio Gheller,Giuliano Taffoni,Luca Tornatore*

Main category: astro-ph.IM

TL;DR: The paper introduces RICK 2.0, a scalable and high-performance imaging software utilizing the HeFFTe library to address computational challenges posed by large data volumes from modern radio interferometers.


<details>
  <summary>Details</summary>
Motivation: To address the computational challenges posed by increasing data volumes from telescopes, such as SKA precursors, and overcome scalability issues in imaging pipelines.

Method: Introducing RICK 2.0 with a novel architecture leveraging HeFFTe library for distributed Fast Fourier Transforms to enable portability across HPC architectures and improve computational efficiency.

Result: Validated against observational data from MeerKAT and LOFAR, demonstrating substantial performance advantages, especially on GPUs, and scalability in high-resolution imaging scenarios.

Conclusion: RICK 2.0 reduces communication overhead significantly compared to its predecessors, providing a scalable and efficient imaging solution for modern and upcoming radio interferometers like SKA.

Abstract: The data volumes generated by modern radio interferometers, such as the SKA precursors, present significant computational challenges for imaging pipelines. Addressing the need for high-performance, portable, and scalable software, we present RICK 2.0 (Radio Imaging Code Kernels). This work introduces a novel implementation that leverages the HeFFTe library for distributed Fast Fourier Transforms, ensuring portability across diverse HPC architectures, including multi-core CPUs and accelerators. We validate RICK's correctness and performance against real observational data from both MeerKAT and LOFAR. Our results demonstrate that the HeFFTe-based implementation offers substantial performance advantages, particularly when running on GPUs, and scales effectively with large pixel resolutions and a high number of frequency planes. This new architecture overcomes the critical scaling limitations identified in previous work (Paper II, Paper III), where communication overheads consumed up to 96% of the runtime due to the necessity of communicating the entire grid. This new RICK version drastically reduces this communication impact, representing a scalable and efficient imaging solution ready for the SKA era.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [92] [Convex Hull 3D Filtering with GPU Ray Tracing and Tensor Cores](https://arxiv.org/abs/2601.19647)
*Roberto Carrasco,Enzo Meneses,Hector Ferrada,Cristobal A. Navarro,Nancy Hitschfeld*

Main category: cs.CG

TL;DR: Proposes a preprocessing filter for convex hull computation utilizing ray tracing and tensor core technologies to significantly enhance speed and energy efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing the growing demand for efficient and sustainable geometric computations under strict time constraints in real-time applications like simulations and gaming.

Method: Developed a preprocessing filter using ray tracing and tensor core technologies to build a delimiter polyhedron that discards unnecessary points based on Manhattan distances.

Result: The proposed filter accelerates convex hull computation by up to 200x compared to CPU parallel implementations while also reducing GPU energy consumption.

Conclusion: The method achieves significant performance gains and energy efficiency, demonstrating scalable geometric computation suitable for high-performance real-time systems.

Abstract: In recent years, applications such as real-time simulations, autonomous systems, and video games increasingly demand the processing of complex geometric models under stringent time constraints. Traditional geometric algorithms, including the convex hull, are subject to these challenges. A common approach to improve performance is scaling computational resources, which often results in higher energy consumption. Given the growing global concern regarding sustainable use of energy, this becomes a critical limitation. This work presents a 3D preprocessing filter for the convex hull algorithm using ray tracing and tensor core technologies. The filter builds a delimiter polyhedron based on Manhattan distances that discards points from the original set. The filter is evaluated on two point distributions: uniform and sphere. Experimental results show that the proposed filter, combined with convex hull construction, accelerates the computation of the 3D convex hull by up to $200 \times$ with respect to a CPU parallel implementation. This research demonstrates that geometric algorithms can be accelerated through massive parallelism while maintaining efficient energy utilization. Beyond execution time and speedup evaluation, we also analyze GPU energy consumption, showing that the proposed preprocessing filter not only reduces the computational workload but also achieves performance gains with controlled energy usage. These results highlight the dual benefit of the method in terms of both speed and energy efficiency, reinforcing its applicability in modern high-performance scenarios.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [93] [Decentralized Nonsmooth Nonconvex Optimization with Client Sampling](https://arxiv.org/abs/2601.19381)
*Xinyan Chen,Weiguo Gao,Luo Luo*

Main category: math.OC

TL;DR: The paper proposes an efficient stochastic method for decentralized nonsmooth and nonconvex optimization of Lipschitz continuous functions with improved complexities for sampling, computation, and communication.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of achieving efficient decentralized optimization in nonsmooth and nonconvex scenarios, particularly focusing on reducing sampling, computation, and communication complexities.

Method: A stochastic first-order method with client sampling technique is developed to reach the $(δ, ε)$-Goldstein stationary point with optimal sample and communication complexities. The method is also extended to zeroth-order optimization.

Result: The proposed method achieves optimal sample complexity and sharper communication complexity as compared to existing approaches. Numerical experiments validate its practical advantages.

Conclusion: This work contributes an optimized decentralized approach to challenging nonsmooth nonconvex optimization tasks, providing theoretical and empirical evidence of its superiority.

Abstract: This paper considers decentralized nonsmooth nonconvex optimization problem with Lipschitz continuous local functions. We propose an efficient stochastic first-order method with client sampling, achieving the $(δ,ε)$-Goldstein stationary point with the overall sample complexity of ${\mathcal O}(δ^{-1}ε^{-3})$, the computation rounds of ${\mathcal O}(δ^{-1}ε^{-3})$, and the communication rounds of ${\tilde{\mathcal O}}(γ^{-1/2}δ^{-1}ε^{-3})$, where $γ$ is the spectral gap of the mixing matrix for the network. Our results achieve the optimal sample complexity and the sharper communication complexity than existing methods. We also extend our ideas to zeroth-order optimization. Moreover, the numerical experiments show the empirical advantage of our methods.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [94] [Configurable p-Neurons Using Modular p-Bits](https://arxiv.org/abs/2601.18943)
*Saleh Bunaiyan,Mohammad Alsharif,Abdelrahman S. Abdelrahman,Hesham ElSawy,Suraj S. Cheema,Suhaib A. Fahmy,Kerem Y. Camsari,Feras Al-Dirini*

Main category: cs.ET

TL;DR: Probabilistic neurons (p-neurons) are introduced with configurable probabilistic activation functions, demonstrating significant hardware savings.


<details>
  <summary>Details</summary>
Motivation: To explore diverse probabilistic activation functions for improved neural networks performance and efficiency.

Method: Re-engineering p-bits into modular p-neurons with configurable probabilistic activation functions, implemented in spintronic designs and tested on FPGA.

Result: Wide and tunable activation functions with 10x hardware resource savings in FPGA implementations compared to traditional digital p-bits.

Conclusion: Modular p-neurons provide versatile probabilistic activation functions and efficient computation mechanisms, paving the way for resource-efficient neural network designs.

Abstract: Probabilistic bits (p-bits) have recently been employed in neural networks (NNs) as stochastic neurons with sigmoidal probabilistic activation functions. Nonetheless, there remain a wealth of other probabilistic activation functions that are yet to be explored. Here we re-engineer the p-bit by decoupling its stochastic signal path from its input data path, giving rise to a modular p-bit that enables the realization of probabilistic neurons (p-neurons) with a range of configurable probabilistic activation functions, including a probabilistic version of the widely used Logistic Sigmoid, Tanh and Rectified Linear Unit (ReLU) activation functions. We present spintronic (CMOS + sMTJ) designs that show wide and tunable probabilistic ranges of operation. Finally, we experimentally implement digital-CMOS versions on an FPGA, with stochastic unit sharing, and demonstrate an order of magnitude (10x) saving in required hardware resources compared to conventional digital p-bit implementations.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [95] [In-Network Collective Operations: Game Changer or Challenge for AI Workloads?](https://arxiv.org/abs/2601.19132)
*Torsten Hoefler,Mikhail Khalilov,Josiah Clark,Surendra Anubolu,Mohan Kalkunte,Karen Schramm,Eric Spada,Duncan Roweth,Keith Underwood,Adrian Caulfield,Abdul Kabbani,Amirreza Rastegari*

Main category: cs.NI

TL;DR: The paper discusses in-network collective operations (INC) and their potential to accelerate AI workloads, focusing on two types: Edge-INC and Core-INC.


<details>
  <summary>Details</summary>
Motivation: To explore and introduce INC as a means to bridge networking and AI fields, and accelerate AI collective operations while addressing obstacles in their adoption.

Method: Discussed Edge-INC and Core-INC systems, identified six key obstacles to adoption, and outlined their potential performance benefits.

Result: Provided detailed analysis of INC opportunities and challenges, fostering accessibility for non-experts and connecting AI with networking.

Conclusion: INC presents promising acceleration for AI workloads, though its adoption faces critical challenges, and future developments and applications hold potential.

Abstract: This paper summarizes the opportunities of in-network collective operations (INC) for accelerated collective operations in AI workloads. We provide sufficient detail to make this important field accessible to non-experts in AI or networking, fostering a connection between these communities. Consider two types of INC: Edge-INC, where the system is implemented at the node level, and Core-INC, where the system is embedded within network switches. We outline the potential performance benefits as well as six key obstacles in the context of both Edge-INC and Core-INC that may hinder their adoption. Finally, we present a set of predictions for the future development and application of INC.

</details>
